<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-204 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-204</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-204</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-10.html">extraction-schema-10</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <p><strong>Paper ID:</strong> paper-9815f586f329a098df83ca872799880bd5cb1a15</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9815f586f329a098df83ca872799880bd5cb1a15" target="_blank">Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts is presented, and it is found that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e204.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e204.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-source coherent counter-memory receptiveness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>High receptiveness to a single coherent/generated counter-memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When a single, coherent piece of external evidence (counter-memory) that contradicts a model's parametric memory is presented as the only evidence, LLMs frequently adopt the counter-evidence and change their answer, often with high token-level confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (ChatGPT, GPT-4, PaLM2, Qwen-7B, Llama2-7B, Llama2-70B, Vicuna-7B, Vicuna-33B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Closed-book QA (PopQA, StrategyQA) with a single external evidence passage provided (counter-memory only).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Contradictory factual passages (coherent narrative supporting counter-answer)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Synthetic/generated by ChatGPT (generation-based counter-memory)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>contradictory</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Closed-book baseline accuracies reported (e.g., ChatGPT PopQA 44.6% correct, StrategyQA 67.4%; GPT-4 PopQA 50.8%, StrategyQA 77.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>With a single coherent generated counter-memory, many LLMs switch to the counter-answer; generation-based counter-memory yields much higher adoption of counter-answers than entity-substitution (Figure 2). Llama2-7B sampled cases show very high token probabilities for the chosen counter-answer (see Appendix A.2; reported >~90% of sampled examples had >95% token probability for the chosen answer).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed — adding a single coherent counter-memory often causes models to adopt the counter-answer (positive if counter-memory correct / negative when it is disinformation); overall increases likelihood of switching away from parametric memory and often increases model confidence in the chosen (possibly incorrect) answer.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Coherent and convincing external text persuades models (sycophancy to coherent context); models weigh surface-level coherence and direct entailment strongly when only one piece of evidence is present.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Contradictory but coherent single-source evidence is frequently persuasive: LLMs accept convincingly written counter-memory and will change answers even when this conflicts with parametric memory; this makes LLMs vulnerable to synthetic disinformation if presented alone.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e204.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e204.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entity-substitution stubbornness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stubbornness to incoherent entity-substitution counter-memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When counter-memory is constructed via word-level entity substitution (incoherent context), LLMs tend to ignore that evidence and stick to their parametric memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (ChatGPT, GPT-4, PaLM2, Llama2-7B, Llama2-70B, Vicuna variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Closed-book QA where evidence is an entity-substituted passage (heuristic counter-memory).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Contradictory factual passage with incoherence due to word-level substitutions</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Synthetic (entity substitution applied to parametric memory)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>contradictory</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Closed-book baseline as above (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>With substitution-based counter-memory as sole evidence, many models (especially larger ones) still output the memory answer; Figure 2 shows high rates of memorization (Mem-Ans.) under substitution-based counter-memory.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>negative/neutral — evidence is ineffective at changing answers because incoherence makes it unpersuasive; adding such evidence does not reduce parametric-memory-driven responses.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Models detect low coherence / contextual mismatch in substituted passages and rely on stored parametric memory; larger models' stronger memorization and sensitivity to context may make them more immune to incoherent evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Heuristic, low-coherence counter-memory (entity substitution) is unlikely to change model answers; prior reports of stubbornness may be due to such low-quality counter-evidence rather than an inherent refusal to update beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e204.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e204.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-source confirmation bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Confirmation bias when both parametric-memory and counter-memory evidence are present</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When both supportive (parametric memory) and contradictory (counter-memory) evidence pieces are presented together, LLMs tend to prefer evidence consistent with their internal parametric memory and stick to their original answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (notably GPT-4, ChatGPT, PaLM2, Llama2-7B, others)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evidence-based QA with multiple evidence sources (both parametric-memory passages and counter-memory passages) presented simultaneously.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Mixed: supporting and contradictory passages</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Parametric memory elicited from the model + generated counter-memory (or human-written sources in some experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Closed-book baselines as in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Memorization ratio increases substantially when parametric-memory evidence is present alongside counter-memory; compared to single-source counter-memory, multi-source setups show models favoring memory-aligned answers (Table 6 and discussion). Example: GPT-4 shows up to ~80% memorization ratio on most popular questions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>negative (for unbiased adoption of external evidence) — adding supportive parametric-memory evidence reduces adoption of contradictory external evidence and increases reliance on internal memory; accuracy can improve or worsen depending on which side is correct.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Confirmation bias: models favor evidence consistent with stronger internal beliefs (parametric memory), particularly when multiple signals align with stored beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In multi-source contexts, LLMs demonstrate a strong confirmation bias and preferentially select answers backed by internal memory-consistent evidence rather than integrating conflicting sources objectively.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e204.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e204.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Popularity effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stronger confirmation bias for popular entities</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs demonstrate a greater tendency to rely on parametric memory for facts about more popular entities, making them less receptive to contradictory external evidence for highly popular items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (highlighted), ChatGPT, Llama2 variants, others</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evidence preference analysis stratified by entity popularity on PopQA.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Mixed (parametric-memory and counter-memory passages)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Model-elicited parametric memory + generated counter-memory</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed (contradictory vs aligned depending on passage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Baseline closed-book accuracy (by model).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Memorization ratio increases with entity popularity; GPT-4 reportedly achieves ~80% memorization ratio for the most popular questions (Figure 3 / text).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>negative for overturning incorrect parametric memory — higher popularity makes external contradictory evidence less effective.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>More frequent exposure during pretraining yields stronger internalized beliefs for popular entities, which in turn produce stronger inertia against contradictory external evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Popularity of topics amplifies confirmation bias: models cling more to parametric memory for popular entities, reducing the corrective power of external evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e204.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e204.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Order sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity to the order of evidence presentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs' preference between conflicting evidence pieces depends on the order in which evidence is presented; different models show different directional biases (favoring first or later evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT, GPT-4, PaLM2, Llama2-7B (evaluated explicitly in Table 5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evidence-based QA with controlled first-evidence placement (parametric-memory first vs counter-memory first vs random).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Mixed (supporting and contradictory passages)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Model-elicited parametric memory + generated counter-memory</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Closed-book baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Memorization ratio varies by evidence order: e.g., ChatGPT memorization ratio when parametric memory is first = 46.7% vs counter-memory first = 40.1% (Table 5); other models show >5% fluctuations and some (PaLM2, Llama2-7B) exhibit >30% change depending on order.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed — evidence order can materially alter whether a model follows parametric memory or external evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Prompt/evidence order sensitivity: LLMs weight earlier or later context unequally depending on model and alignment, possibly due to attention/window heuristics or learned positional/contextual priors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Order of evidence presentation significantly alters model decisions; some models favor early evidence while others favor later evidence, which is undesirable for consistent evidence orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e204.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e204.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Quantity/majority effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Models follow the majority of presented evidence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When multiple pieces of evidence are presented, LLMs tend to choose the answer backed by the majority of evidence; increasing the proportion of parametric-memory aligned evidence raises memorization ratio markedly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (ChatGPT, GPT-4, PaLM2, Qwen-7B, Llama2 variants, Vicuna variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-evidence QA experiments varying the number/proportion of supporting vs contradictory evidence pieces.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Multiple passages (parametric-memory and counter-memory) of varied counts</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Model-generated + human-written (Wikipedia) evidence in some experiments</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed (varied proportions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Closed-book baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Memorization ratio increases with the proportion of parametric-memory evidence: e.g., ChatGPT memorization ratio by proportion (from Table 6): ~30.0% at 1/3 parametric evidence, 43.0% at 1/2, 86.2% at 2/3, and 99.8% at 100% parametric evidence (PopQA columns).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive for the majority side — models tend to align with the side that has more evidence; can be harmful if majority evidence is incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>A heuristic 'herd' or majority-weighting effect where the model aggregates counts/strength of corroborating passages and favors the majority-supported answer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Quantity matters: more supporting passages for one answer strongly bias models toward that answer; models do not robustly evaluate intrinsic credibility but rely on counts/aggregated support.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e204.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e204.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Irrelevant-evidence distraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs can be distracted by irrelevant evidence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Irrelevant retrieved passages can cause LLMs to produce irrelevant or incorrect answers and reduce reliance on their parametric memory, especially as the quantity of irrelevant evidence increases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT, Llama2-7B (detailed in Table 7), others evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>QA with only irrelevant evidence, and mixed relevant+irrelevant evidence experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Irrelevant passages (retrieved by Sentence-BERT) and mixtures with relevant passages</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Human-written corpus (Wikipedia) filtered for irrelevance via retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>neutral/irrelevant</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Without relevant evidence, models sometimes abstain or use parametric memory (baseline uncertain rates in Table B.4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>With only irrelevant evidence ChatGPT often outputs 'Irrelevant answers' (e.g., with 1 irrelevant passage ChatGPT produced Irr-Ans 18.2% w/o relevant evidence; Llama2-7B produced Irr-Ans 92.4% in similar settings). Adding irrelevant evidence alongside relevant evidence reduces correct reliance on relevant evidence as irrelevant count increases (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>negative — irrelevant evidence can distract models into producing irrelevant answers and lowers reliance on correct memory or correct relevant evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Models can be misled by topically or lexically similar but non-informative contexts; retrieval noise degrades evidence integration and increases impulsive reliance on surface cues.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Irrelevant retrieved passages harm model performance: models can be distracted into irrelevant outputs and their ability to filter irrelevant evidence diminishes as more irrelevant items are present (notably severe in Llama2-7B).</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e204.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e204.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fragmentation and length effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Failure to integrate fragmented evidence and asymmetric sensitivity to evidence length</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs perform worse when evidence is split into fragments (reduced integration) and show asymmetry: short counter-evidence is less persuasive, while short evidence aligned with parametric memory is often still accepted.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (fragmentation experiments), multiple models for length tests (Table A.1)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-fragment evidence (StrategyQA facts separated) and experiments replacing full evidence with short answers (length ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Fragmented sub-facts vs whole merged evidence; very short evidence (answer-only) vs full passages</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Human-written StrategyQA facts and generated passages</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Baseline closed-book.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Fragmenting evidence caused ChatGPT to change its preference to the other intact evidence in 38.2% of examples. Shortening counter-memory to just the counter-answer drastically reduces adoption of counter-answer (PopQA Table A.1: Counter-answer-as-evidence led to Ctr-Ans 18.8% vs standard Ctr-Ans 56.7%). Shortening parametric-memory to its answer does not reduce adoption much (Mem-Ans 43.9% when short vs 42.7% standard).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed — fragmentation reduces persuasive power of the originally chosen evidence and can flip preferences; short counter-evidence is less effective while short memory-aligned evidence remains persuasive.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>LLMs have limited ability to integrate multiple fragmented sub-evidence; persuasive strength is correlated with length/coherence for novel counter-evidence but internal memory can be triggered by short cues.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Presentation format matters: coherent, consolidated passages are more persuasive; fragmenting evidence undermines its influence and counter-evidence needs more content to persuade than memory-aligned evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e204.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e204.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model confidence analyses</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token-probability evidence of high model confidence in chosen answers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Across scenarios where models choose either memory-aligned or counter-evidence-aligned answers, token-level probabilities indicate LLMs are often highly confident (>95% token probability) in their chosen option.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B (explicit probability analyses), other models evaluated qualitatively</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Probability/confidence analysis for first token of chosen answer in multiple-evidence and single-evidence settings (normalized over the 3 MC choices).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Single counter-memory or multi-source evidence</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Generated counter-memory / parametric-memory evidence</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>contradictory or mixed depending on experiment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Baseline confidence not explicitly tabulated; closed-book outputs exist.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>In single-source (counter-memory only) sampled 1000 examples where Llama2-7B picked counter-answer, Llama2-7B shows high confidence; reported that ~91.3% of cases had >=95% token probability for the chosen token in sampled counter-answer cases. In multi-source (2 supportive + 2 contradictory) sampled 1000 instances where model favored memory answer, ~96.3% of examples had >=95% token probability for the chosen token (Appendix A.2).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>adding convincing evidence (either side) increases model's token-level confidence in the chosen answer; thus evidence often increases certainty even when the chosen answer may be incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Once a side (memory or external evidence) is preferred, the model's generation probability distribution concentrates strongly on that choice, reflecting high internal certainty rather than calibrated uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs not only change answers under evidence but do so with high confidence (high token probabilities), indicating overconfidence and potential lack of calibrated uncertainty in the face of conflicting evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Entity-based knowledge conflicts in question answering <em>(Rating: 2)</em></li>
                <li>When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories <em>(Rating: 2)</em></li>
                <li>Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence <em>(Rating: 2)</em></li>
                <li>Large language models can be easily distracted by irrelevant context <em>(Rating: 2)</em></li>
                <li>Can retriever-augmented language models reason? the blame game between the retriever and the language model <em>(Rating: 1)</em></li>
                <li>How context affects language models' factual predictions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-204",
    "paper_id": "paper-9815f586f329a098df83ca872799880bd5cb1a15",
    "extraction_schema_id": "extraction-schema-10",
    "extracted_data": [
        {
            "name_short": "Single-source coherent counter-memory receptiveness",
            "name_full": "High receptiveness to a single coherent/generated counter-memory",
            "brief_description": "When a single, coherent piece of external evidence (counter-memory) that contradicts a model's parametric memory is presented as the only evidence, LLMs frequently adopt the counter-evidence and change their answer, often with high token-level confidence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (ChatGPT, GPT-4, PaLM2, Qwen-7B, Llama2-7B, Llama2-70B, Vicuna-7B, Vicuna-33B)",
            "model_size": null,
            "task_description": "Closed-book QA (PopQA, StrategyQA) with a single external evidence passage provided (counter-memory only).",
            "evidence_type": "Contradictory factual passages (coherent narrative supporting counter-answer)",
            "evidence_source": "Synthetic/generated by ChatGPT (generation-based counter-memory)",
            "parametric_knowledge_alignment": "contradictory",
            "performance_without_evidence": "Closed-book baseline accuracies reported (e.g., ChatGPT PopQA 44.6% correct, StrategyQA 67.4%; GPT-4 PopQA 50.8%, StrategyQA 77.3%).",
            "performance_with_evidence": "With a single coherent generated counter-memory, many LLMs switch to the counter-answer; generation-based counter-memory yields much higher adoption of counter-answers than entity-substitution (Figure 2). Llama2-7B sampled cases show very high token probabilities for the chosen counter-answer (see Appendix A.2; reported &gt;~90% of sampled examples had &gt;95% token probability for the chosen answer).",
            "evidence_effect": "mixed — adding a single coherent counter-memory often causes models to adopt the counter-answer (positive if counter-memory correct / negative when it is disinformation); overall increases likelihood of switching away from parametric memory and often increases model confidence in the chosen (possibly incorrect) answer.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Coherent and convincing external text persuades models (sycophancy to coherent context); models weigh surface-level coherence and direct entailment strongly when only one piece of evidence is present.",
            "key_findings": "Contradictory but coherent single-source evidence is frequently persuasive: LLMs accept convincingly written counter-memory and will change answers even when this conflicts with parametric memory; this makes LLMs vulnerable to synthetic disinformation if presented alone.",
            "counterintuitive_behavior": true,
            "uuid": "e204.0"
        },
        {
            "name_short": "Entity-substitution stubbornness",
            "name_full": "Stubbornness to incoherent entity-substitution counter-memory",
            "brief_description": "When counter-memory is constructed via word-level entity substitution (incoherent context), LLMs tend to ignore that evidence and stick to their parametric memory.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (ChatGPT, GPT-4, PaLM2, Llama2-7B, Llama2-70B, Vicuna variants)",
            "model_size": null,
            "task_description": "Closed-book QA where evidence is an entity-substituted passage (heuristic counter-memory).",
            "evidence_type": "Contradictory factual passage with incoherence due to word-level substitutions",
            "evidence_source": "Synthetic (entity substitution applied to parametric memory)",
            "parametric_knowledge_alignment": "contradictory",
            "performance_without_evidence": "Closed-book baseline as above (see Table 1).",
            "performance_with_evidence": "With substitution-based counter-memory as sole evidence, many models (especially larger ones) still output the memory answer; Figure 2 shows high rates of memorization (Mem-Ans.) under substitution-based counter-memory.",
            "evidence_effect": "negative/neutral — evidence is ineffective at changing answers because incoherence makes it unpersuasive; adding such evidence does not reduce parametric-memory-driven responses.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Models detect low coherence / contextual mismatch in substituted passages and rely on stored parametric memory; larger models' stronger memorization and sensitivity to context may make them more immune to incoherent evidence.",
            "key_findings": "Heuristic, low-coherence counter-memory (entity substitution) is unlikely to change model answers; prior reports of stubbornness may be due to such low-quality counter-evidence rather than an inherent refusal to update beliefs.",
            "counterintuitive_behavior": false,
            "uuid": "e204.1"
        },
        {
            "name_short": "Multi-source confirmation bias",
            "name_full": "Confirmation bias when both parametric-memory and counter-memory evidence are present",
            "brief_description": "When both supportive (parametric memory) and contradictory (counter-memory) evidence pieces are presented together, LLMs tend to prefer evidence consistent with their internal parametric memory and stick to their original answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (notably GPT-4, ChatGPT, PaLM2, Llama2-7B, others)",
            "model_size": null,
            "task_description": "Evidence-based QA with multiple evidence sources (both parametric-memory passages and counter-memory passages) presented simultaneously.",
            "evidence_type": "Mixed: supporting and contradictory passages",
            "evidence_source": "Parametric memory elicited from the model + generated counter-memory (or human-written sources in some experiments)",
            "parametric_knowledge_alignment": "mixed",
            "performance_without_evidence": "Closed-book baselines as in Table 1.",
            "performance_with_evidence": "Memorization ratio increases substantially when parametric-memory evidence is present alongside counter-memory; compared to single-source counter-memory, multi-source setups show models favoring memory-aligned answers (Table 6 and discussion). Example: GPT-4 shows up to ~80% memorization ratio on most popular questions.",
            "evidence_effect": "negative (for unbiased adoption of external evidence) — adding supportive parametric-memory evidence reduces adoption of contradictory external evidence and increases reliance on internal memory; accuracy can improve or worsen depending on which side is correct.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Confirmation bias: models favor evidence consistent with stronger internal beliefs (parametric memory), particularly when multiple signals align with stored beliefs.",
            "key_findings": "In multi-source contexts, LLMs demonstrate a strong confirmation bias and preferentially select answers backed by internal memory-consistent evidence rather than integrating conflicting sources objectively.",
            "counterintuitive_behavior": true,
            "uuid": "e204.2"
        },
        {
            "name_short": "Popularity effect",
            "name_full": "Stronger confirmation bias for popular entities",
            "brief_description": "LLMs demonstrate a greater tendency to rely on parametric memory for facts about more popular entities, making them less receptive to contradictory external evidence for highly popular items.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (highlighted), ChatGPT, Llama2 variants, others",
            "model_size": null,
            "task_description": "Evidence preference analysis stratified by entity popularity on PopQA.",
            "evidence_type": "Mixed (parametric-memory and counter-memory passages)",
            "evidence_source": "Model-elicited parametric memory + generated counter-memory",
            "parametric_knowledge_alignment": "mixed (contradictory vs aligned depending on passage)",
            "performance_without_evidence": "Baseline closed-book accuracy (by model).",
            "performance_with_evidence": "Memorization ratio increases with entity popularity; GPT-4 reportedly achieves ~80% memorization ratio for the most popular questions (Figure 3 / text).",
            "evidence_effect": "negative for overturning incorrect parametric memory — higher popularity makes external contradictory evidence less effective.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "More frequent exposure during pretraining yields stronger internalized beliefs for popular entities, which in turn produce stronger inertia against contradictory external evidence.",
            "key_findings": "Popularity of topics amplifies confirmation bias: models cling more to parametric memory for popular entities, reducing the corrective power of external evidence.",
            "counterintuitive_behavior": true,
            "uuid": "e204.3"
        },
        {
            "name_short": "Order sensitivity",
            "name_full": "Sensitivity to the order of evidence presentation",
            "brief_description": "LLMs' preference between conflicting evidence pieces depends on the order in which evidence is presented; different models show different directional biases (favoring first or later evidence).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT, GPT-4, PaLM2, Llama2-7B (evaluated explicitly in Table 5)",
            "model_size": null,
            "task_description": "Evidence-based QA with controlled first-evidence placement (parametric-memory first vs counter-memory first vs random).",
            "evidence_type": "Mixed (supporting and contradictory passages)",
            "evidence_source": "Model-elicited parametric memory + generated counter-memory",
            "parametric_knowledge_alignment": "mixed",
            "performance_without_evidence": "Closed-book baselines.",
            "performance_with_evidence": "Memorization ratio varies by evidence order: e.g., ChatGPT memorization ratio when parametric memory is first = 46.7% vs counter-memory first = 40.1% (Table 5); other models show &gt;5% fluctuations and some (PaLM2, Llama2-7B) exhibit &gt;30% change depending on order.",
            "evidence_effect": "mixed — evidence order can materially alter whether a model follows parametric memory or external evidence.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Prompt/evidence order sensitivity: LLMs weight earlier or later context unequally depending on model and alignment, possibly due to attention/window heuristics or learned positional/contextual priors.",
            "key_findings": "Order of evidence presentation significantly alters model decisions; some models favor early evidence while others favor later evidence, which is undesirable for consistent evidence orchestration.",
            "counterintuitive_behavior": true,
            "uuid": "e204.4"
        },
        {
            "name_short": "Quantity/majority effect",
            "name_full": "Models follow the majority of presented evidence",
            "brief_description": "When multiple pieces of evidence are presented, LLMs tend to choose the answer backed by the majority of evidence; increasing the proportion of parametric-memory aligned evidence raises memorization ratio markedly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (ChatGPT, GPT-4, PaLM2, Qwen-7B, Llama2 variants, Vicuna variants)",
            "model_size": null,
            "task_description": "Multi-evidence QA experiments varying the number/proportion of supporting vs contradictory evidence pieces.",
            "evidence_type": "Multiple passages (parametric-memory and counter-memory) of varied counts",
            "evidence_source": "Model-generated + human-written (Wikipedia) evidence in some experiments",
            "parametric_knowledge_alignment": "mixed (varied proportions)",
            "performance_without_evidence": "Closed-book baselines.",
            "performance_with_evidence": "Memorization ratio increases with the proportion of parametric-memory evidence: e.g., ChatGPT memorization ratio by proportion (from Table 6): ~30.0% at 1/3 parametric evidence, 43.0% at 1/2, 86.2% at 2/3, and 99.8% at 100% parametric evidence (PopQA columns).",
            "evidence_effect": "positive for the majority side — models tend to align with the side that has more evidence; can be harmful if majority evidence is incorrect.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "A heuristic 'herd' or majority-weighting effect where the model aggregates counts/strength of corroborating passages and favors the majority-supported answer.",
            "key_findings": "Quantity matters: more supporting passages for one answer strongly bias models toward that answer; models do not robustly evaluate intrinsic credibility but rely on counts/aggregated support.",
            "counterintuitive_behavior": true,
            "uuid": "e204.5"
        },
        {
            "name_short": "Irrelevant-evidence distraction",
            "name_full": "LLMs can be distracted by irrelevant evidence",
            "brief_description": "Irrelevant retrieved passages can cause LLMs to produce irrelevant or incorrect answers and reduce reliance on their parametric memory, especially as the quantity of irrelevant evidence increases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT, Llama2-7B (detailed in Table 7), others evaluated",
            "model_size": null,
            "task_description": "QA with only irrelevant evidence, and mixed relevant+irrelevant evidence experiments.",
            "evidence_type": "Irrelevant passages (retrieved by Sentence-BERT) and mixtures with relevant passages",
            "evidence_source": "Human-written corpus (Wikipedia) filtered for irrelevance via retrieval",
            "parametric_knowledge_alignment": "neutral/irrelevant",
            "performance_without_evidence": "Without relevant evidence, models sometimes abstain or use parametric memory (baseline uncertain rates in Table B.4).",
            "performance_with_evidence": "With only irrelevant evidence ChatGPT often outputs 'Irrelevant answers' (e.g., with 1 irrelevant passage ChatGPT produced Irr-Ans 18.2% w/o relevant evidence; Llama2-7B produced Irr-Ans 92.4% in similar settings). Adding irrelevant evidence alongside relevant evidence reduces correct reliance on relevant evidence as irrelevant count increases (Table 7).",
            "evidence_effect": "negative — irrelevant evidence can distract models into producing irrelevant answers and lowers reliance on correct memory or correct relevant evidence.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Models can be misled by topically or lexically similar but non-informative contexts; retrieval noise degrades evidence integration and increases impulsive reliance on surface cues.",
            "key_findings": "Irrelevant retrieved passages harm model performance: models can be distracted into irrelevant outputs and their ability to filter irrelevant evidence diminishes as more irrelevant items are present (notably severe in Llama2-7B).",
            "counterintuitive_behavior": true,
            "uuid": "e204.6"
        },
        {
            "name_short": "Fragmentation and length effects",
            "name_full": "Failure to integrate fragmented evidence and asymmetric sensitivity to evidence length",
            "brief_description": "LLMs perform worse when evidence is split into fragments (reduced integration) and show asymmetry: short counter-evidence is less persuasive, while short evidence aligned with parametric memory is often still accepted.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (fragmentation experiments), multiple models for length tests (Table A.1)",
            "model_size": null,
            "task_description": "Multi-fragment evidence (StrategyQA facts separated) and experiments replacing full evidence with short answers (length ablation).",
            "evidence_type": "Fragmented sub-facts vs whole merged evidence; very short evidence (answer-only) vs full passages",
            "evidence_source": "Human-written StrategyQA facts and generated passages",
            "parametric_knowledge_alignment": "mixed",
            "performance_without_evidence": "Baseline closed-book.",
            "performance_with_evidence": "Fragmenting evidence caused ChatGPT to change its preference to the other intact evidence in 38.2% of examples. Shortening counter-memory to just the counter-answer drastically reduces adoption of counter-answer (PopQA Table A.1: Counter-answer-as-evidence led to Ctr-Ans 18.8% vs standard Ctr-Ans 56.7%). Shortening parametric-memory to its answer does not reduce adoption much (Mem-Ans 43.9% when short vs 42.7% standard).",
            "evidence_effect": "mixed — fragmentation reduces persuasive power of the originally chosen evidence and can flip preferences; short counter-evidence is less effective while short memory-aligned evidence remains persuasive.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "LLMs have limited ability to integrate multiple fragmented sub-evidence; persuasive strength is correlated with length/coherence for novel counter-evidence but internal memory can be triggered by short cues.",
            "key_findings": "Presentation format matters: coherent, consolidated passages are more persuasive; fragmenting evidence undermines its influence and counter-evidence needs more content to persuade than memory-aligned evidence.",
            "counterintuitive_behavior": true,
            "uuid": "e204.7"
        },
        {
            "name_short": "Model confidence analyses",
            "name_full": "Token-probability evidence of high model confidence in chosen answers",
            "brief_description": "Across scenarios where models choose either memory-aligned or counter-evidence-aligned answers, token-level probabilities indicate LLMs are often highly confident (&gt;95% token probability) in their chosen option.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-7B (explicit probability analyses), other models evaluated qualitatively",
            "model_size": null,
            "task_description": "Probability/confidence analysis for first token of chosen answer in multiple-evidence and single-evidence settings (normalized over the 3 MC choices).",
            "evidence_type": "Single counter-memory or multi-source evidence",
            "evidence_source": "Generated counter-memory / parametric-memory evidence",
            "parametric_knowledge_alignment": "contradictory or mixed depending on experiment",
            "performance_without_evidence": "Baseline confidence not explicitly tabulated; closed-book outputs exist.",
            "performance_with_evidence": "In single-source (counter-memory only) sampled 1000 examples where Llama2-7B picked counter-answer, Llama2-7B shows high confidence; reported that ~91.3% of cases had &gt;=95% token probability for the chosen token in sampled counter-answer cases. In multi-source (2 supportive + 2 contradictory) sampled 1000 instances where model favored memory answer, ~96.3% of examples had &gt;=95% token probability for the chosen token (Appendix A.2).",
            "evidence_effect": "adding convincing evidence (either side) increases model's token-level confidence in the chosen answer; thus evidence often increases certainty even when the chosen answer may be incorrect.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Once a side (memory or external evidence) is preferred, the model's generation probability distribution concentrates strongly on that choice, reflecting high internal certainty rather than calibrated uncertainty.",
            "key_findings": "LLMs not only change answers under evidence but do so with high confidence (high token probabilities), indicating overconfidence and potential lack of calibrated uncertainty in the face of conflicting evidence.",
            "counterintuitive_behavior": true,
            "uuid": "e204.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Entity-based knowledge conflicts in question answering",
            "rating": 2
        },
        {
            "paper_title": "When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories",
            "rating": 2
        },
        {
            "paper_title": "Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence",
            "rating": 2
        },
        {
            "paper_title": "Large language models can be easily distracted by irrelevant context",
            "rating": 2
        },
        {
            "paper_title": "Can retriever-augmented language models reason? the blame game between the retriever and the language model",
            "rating": 1
        },
        {
            "paper_title": "How context affects language models' factual predictions",
            "rating": 1
        }
    ],
    "cost": 0.019551,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Adaptive Chameleon or Stubborn Sloth: REVEALING THE BEHAVIOR OF LARGE LANGUAGE MODELS IN KNOWLEDGE CONFLICTS</h1>
<p>Jian Xie ${ }^{\Delta <em>}$ Kai Zhang ${ }^{\Delta </em>}$ Jiangjie Chen ${ }^{\Delta}$ Renze Lou ${ }^{\ominus}$ Yu Su ${ }^{\Delta}$<br>${ }^{1}$ School of Computer Science, Fudan University<br>${ }^{\Delta}$ The Ohio State University ${ }^{\ominus}$ The Pennsylvania State University<br>jianxie22@m.fudan.edu.cn, (zhang.13253, su.809)@osu.edu</p>
<h4>Abstract</h4>
<p>By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory. However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts. We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. Our investigation reveals seemingly contradicting behaviors of LLMs. On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. On the other hand, LLMs also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time. These results pose important implications that are worth careful consideration for the further development and deployment of tool- and retrieval-augmented LLMs. Resources are available at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.</p>
<h2>1 INTRODUCTION</h2>
<p>After pre-training on massive corpora, large language models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Ouyang et al., 2022; OpenAI, 2022; 2023; Zeng et al., 2023; Touvron et al., 2023a) have formed a wealth of parametric memory, such as commonsense and factual knowledge (Petroni et al., 2019; Li et al., 2022; Zhao et al., 2023). However, such parametric memory may be inaccurate or become outdated (Liska et al., 2022; Luu et al., 2022) due to misinformation in the pre-training corpus or the static nature of parametric memory, known to be a major cause for hallucinations (Elazar et al., 2021; Shuster et al., 2021; Ji et al., 2023).
Tool ${ }^{1}$ (Schick et al., 2023; Qin et al., 2023) or retrieval augmentation (Mallen et al., 2022; Shi et al., 2023b; Ram et al., 2023) has emerged as a promising solution by providing external information as new evidence to LLMs, such as ChatGPT Plugins and New Bing. However, external evidence, inevitably, could conflict with LLMs' parametric memory. We refer to external evidence that conflicts with parametric memory as counter-memory. In this paper, we seek to answer the question: how receptive are LLMs to external evidence, especially counter-memory? A solid understanding of this question is an essential stepping stone for wider application of tool-augmented LLMs. Not only does this relate to overcoming the limitations of LLM's static parametric memory, but it is also associated</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>with direct safety concerns. For example, what if a third-party tool, either by the developer or hijacked by attackers, intentionally returns disinformation? Will LLMs be deceived?</p>
<p>We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering counter-memory. A key challenge lies in how to construct the counter-memory. Prior work employs various heuristics, such as negation injection (Niu \&amp; Bansal, 2018; Kassner et al., 2021; Gubelmann \&amp; Handschuh, 2022) and entity substitution (Longpre et al., 2021; Zhou et al., 2023), and finds that language models (both large and small) tend to be stubborn and cling to their parametric memory. However, such heuristic word-level editing results in incoherent counter-memory (see an example in Section 4.1), which may make it trivial for LLMs to detect and thus neglect the constructed counter-memory. It is unclear how the prior conclusions translate to real-world scenarios, where counter-memory is more coherent and convincing.</p>
<p>We propose a systematic framework to elicit the parametric memory of LLMs and construct the corresponding counter-memory. We design a series of checks, such as entailment from parametric memory to the answer, to ensure that the elicited parametric memory is indeed the LLM's internal belief. For the counter-memory, instead of heuristically editing the parametric memory, we instruct an LLM to directly generate a coherent passage that factually conflicts with the parametric memory. After obtaining a large pool of parametric memory and counter-memory pairs, we then examine LLMs' behavior in different knowledge conflict scenarios, including 1) when only counter-memory is present as external evidence and 2) when both parametric memory and counter-memory are present.</p>
<p>Our investigation leads to a series of interesting new findings. We highlight the following:</p>
<ul>
<li>LLMs are highly receptive to external evidence if that is the only evidence, even when it conflicts with their parametric memory. This contradicts the prior wisdom (Longpre et al., 2021), and we attribute this to the more coherent and convincing counter-memory constructed through our framework. On the other hand, this also suggests that LLMs may be easily deceived by, e.g., disinformation from malicious (third-party) tools.</li>
<li>However, with both supportive and contradictory evidence to their parametric memory, LLMs show a strong confirmation bias (Nickerson, 1998) and tend to cling to their parametric memory. This reveals a potential challenge for LLMs to unbiasedly orchestrate multiple pieces of conflicting evidence, a common situation encountered by generative search engines.</li>
</ul>
<h1>2 Related Work</h1>
<p>Parametric Memory in Language Models After pre-training, language models have internalized a vast amount of knowledge into their parameters (Roberts et al., 2020; Jiang et al., 2020), also known as parametric memory. Many past studies have explored the elicitation of parametric memory in language models, such as commonsense or factual knowledge probing (Petroni et al., 2019; Lin et al., 2020; Zhang et al., 2021; West et al., 2022; Chen et al., 2023; Wang et al., 2023). Such parametric memory could help solve downstream tasks (Wang et al., 2021; Yu et al., 2023; Sun et al., 2023). However, previous work has discovered that language models only memorize a small portion of the knowledge they have been exposed to during pre-training (Carlini et al., 2021; 2023) due to model's limited memorization abilities. In addition, the parametric memory may become outdated (Lazaridou et al., 2021; De Cao et al., 2021). Such incorrect and outdated parametric memory may show as hallucinations (Elazar et al., 2021; Shuster et al., 2021; Ji et al., 2023). Although some methods are proposed to edit knowledge in language models (Dai et al., 2022; Meng et al., 2022; 2023), they typically require additional modifications on model weights without evaluating the consequences on models' other aspects such as performances and are limited to factual knowledge.</p>
<p>Tool-augmented Language Models To address the limitations of parametric memory, external tools such as retrievers are used to augment language models with up-to-date information, namely tool-augmented (Nakano et al., 2021; Yao et al., 2023; Qin et al., 2023; Schick et al., 2023; Lu et al., 2023) or retrieval-augmented (Guu et al., 2020; Khandelwal et al., 2020; Izacard \&amp; Grave, 2021; Borgeaud et al., 2022; Zhong et al., 2022) language models. Such a framework, which has proven its efficacy in enhancing large language models (Shi et al., 2023b; Ram et al., 2023; Mallen et al., 2022), is adopted in real-world applications such as New Bing and ChatGPT Plugins. Inevitably, the external evidence could conflict with the parametric memory. However, the behavior of LLMs in</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Our framework for simulating knowledge conflicts. In Step 1, we elicit LLMs' memory answer and parametric memory in a closed-book QA fashion. In Step 2, we construct counter-answer to memory answer with heuristic rules, for which ChatGPT generates supporting counter-memory with instructions. To uphold evidence quality, we conduct entailment checking (Step 3) and answer consistency (Step 4) to filter unqualified examples. All experiments are implemented under zero-shot setting to avoid the bias introduced by demonstrations.
knowledge conflict scenarios remains under-explored, and unraveling it holds significance for wider applications of tool-augmented LLMs.</p>
<p>Knowledge Conflict To perform controlled experiments, knowledge conflict is often simulated with counter-memory constructed upon parametric memory. Heuristic counter-memory construction methods such as negation injection (Niu \&amp; Bansal, 2018; Kassner et al., 2021; Petroni et al., 2020; Pan et al., 2021) have been developed. Furthermore, entity substitution (Longpre et al., 2021; Chen et al., 2022; Si et al., 2023; Zhou et al., 2023) replaces all mentions of the answer entity in parametric memory with other entities to construct counter-memory. However, these methods are limited to word-level editing, leading to low overall coherence in the counter-memory. We instead instruct LLMs to generate counter-memory from scratch to ensure high coherence.</p>
<h1>3 EXPERIMENTAL SETUP</h1>
<p>In this section, we describe our framework for eliciting high-quality parametric memory from LLMs and constructing the corresponding counter-memory, as well as the evaluation metrics.</p>
<h3>3.1 DATASETS</h3>
<p>Following prior work (Longpre et al., 2021; Chen et al., 2022), we adopt question answering (QA) task as the testbed for knowledge conflict experiments. In addition to an entity-based QA dataset (POPQA), we include a multi-step reasoning dataset (STRATEGYQA) for diversifying the questions studied in the experiments. Specifically,</p>
<ul>
<li>PopQA (Mallen et al., 2022) is an entity-centric QA dataset that contains 14K questions. Data for POPQA originates from triples in Wikidata. Employing custom templates tailored to relationship types, the authors construct questions through the substitution of the subject within knowledge triples. POPQA defines the popularity of a question based on the monthly Wikipedia page views associated with the entity mentioned in the question.</li>
<li>StrategyQA (Geva et al., 2021) is a multi-step fact reasoning benchmark that necessitates the implicit question decomposition into reasoning steps. The questions are built around Wikipedia terms and cover a wide range of strategies, which demand the model's capability to select and integrate relevant knowledge effectively. The language model is expected to provide a True or False answer.</li>
</ul>
<p>Table 1: The correctness of LLMs responses in closed-book QA fashion (Step 1 in Figure 1). We examine eight LLMs, including three closed-source LLMs and five open-source LLMs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">PopQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">StrategyQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">Wrong</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">Wrong</td>
<td style="text-align: center;">Unknown</td>
</tr>
<tr>
<td style="text-align: left;">Closed-source LLMs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (OpenAI, 2022)</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">1.9</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (OpenAI, 2023)</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">PaLM2 (Anil et al., 2023)</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Open-source LLMs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Qwen-7B (Alibaba, 2023)</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-7B (Touvron et al., 2023b)</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-70B (Touvron et al., 2023b)</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-7B (Zheng et al., 2023)</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-33B (Zheng et al., 2023)</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">0.0</td>
</tr>
</tbody>
</table>
<h1>3.2 Parametric Memory Elicitation</h1>
<p>Step 1 in Figure 1 illustrates how we elicit parametric memory: in a closed-book QA fashion, LLMs recall their parametric memory to answer questions without any external evidence. Specifically, given a question, e.g., "Who is the chief scientist of Google DeepMind", LLMs are instructed to provide an answer "Demis Hassabis" and its supporting background information about how Demis founded and led DeepMind in detail. We cast the detailed background as parametric memory because the answer only represents the conclusion of parametric memory w.r.t. the given question.</p>
<p>Table 1 shows the closed-book results of LLMs on PopQA and StrategyQA. Notably, LLMs may respond with "Unknown" when no evidence is provided in the context, particularly in ChatGPT. Such answer abstention (Rajpurkar et al., 2018) suggests that LLMs fail to recall valid memory associated with the given question, so we discard them. For comprehensiveness, we also keep the examples that LLMs answer incorrectly in the closed-book paradigm because the wrong answer and associated memory are also stored in model parameters.</p>
<h3>3.3 COUNTER-MEMORY CONSTRUCTION</h3>
<p>As depicted in Figure 1, at Step 2, we reframe the memory answer "Demis Hassabis" to a counteranswer (e.g., "Jeff Dean"). Concretely, for PopQA, we substitute the entity in the memory answer with a same-type entity (e.g., from Demis to Jeff); while in StrategyQA, we flip the memory answer (e.g., from positive sentence to negative sentence). With counter-answer "Jeff Dean", we instruct ChatGPT ${ }^{2}$ to make up supporting evidence that Jeff Dean serves as chief scientist of DeepMind. We term such evidence that conflicts with parametric memory as counter-memory.</p>
<p>Since the counter-memory is generated from scratch by powerful generative LLMs, it is more coherent compared to previous word-level editing methods (Longpre et al., 2021; Chen et al., 2022) performed on parametric memory. Both generated parametric memory and counter-memory could serve as external evidence for later experiments on LLMs in knowledge conflicts. Please refer to Appendix B. 1 for more details of evidence construction in each dataset.</p>
<h3>3.4 ANSWER-EVIDENCE Entailment CHECKING</h3>
<p>An ideal piece of evidence should strongly support its answer. For instance, the parametric memory about Demis and DeepMind should clearly support the corresponding memory answer that Demis is the chief scientist of DeepMind. Similarly, counter-memory should clearly support the corresponding counter-answer as well. Therefore, for Step 3 shown in Figure 1, we utilize a natural language inference (NLI) model for support-checking to ensure the evidence indeed entails the answer. Specifically,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Examples in final dataset. Both parametric memory and counter-memory are generated by ChatGPT. They are coherent and present an illusion of correctness even when factually incorrect.</p>
<table>
<thead>
<tr>
<th>Correct Memory</th>
<th></th>
<th>Wrong Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>Question</td>
<td>What is the capital of Kingdom of France?</td>
<td>Who was the producer of The Cardinal?</td>
</tr>
<tr>
<td>Ground Truth</td>
<td>Paris</td>
<td>Otto Preminger</td>
</tr>
<tr>
<td>Parametric Memory</td>
<td>Paris was chosen as the capital of the Kingdom of France due to its central location and strategic importance. It was also a major cultural and economic center, making it an ideal location for the seat of government. Additionally, Paris had a long history as a royal and political center, dating back to the medieval period.</td>
<td>The Cardinal was produced by Martin Ransohoff, who was a prominent film and television producer in the mid-20th century. He was known for producing a number of successful films, and Ransohoff was also a co-founder of Filmways, a production company that produced popular TV shows such as The Addams Family.</td>
</tr>
<tr>
<td>Counter-memory</td>
<td>Nima is the capital of the Kingdom of France. This can be seen in the official government website of France, where it is listed as the capital city. Additionally, Nima is home to the royal palace and the seat of the French government, further solidifying its status as the capital. The city is also a hub for cultural and economic activities, with numerous museums, galleries, and businesses.</td>
<td>Otto Preminger was a prominent film producer in the mid-20th century, known for his work on a number of successful films. One of his most notable productions was the 1963 film The Cardinal, which was directed by him and started Tom Tryon. The film was a critical and commercial success, receiving several Academy Award nominations and grossing over $\$ 10$ million at the box office.</td>
</tr>
</tbody>
</table>
<p>we use the state-of-the-art NLI model DeBERTa-V2 (He et al., 2021) ${ }^{3}$ to determine whether both the parametric memory and counter-memory support their corresponding answers. We only keep the examples where both answers are supported for subsequent experiments.</p>
<p>To ensure the reliability of the selected NLI model, we manually evaluated 200 random examples and observed $99 \%$ accuracy of the model. Please refer to Appendix B. 5 for more details.</p>
<h1>3.5 MEMORY ANSWER CONSISTENCY</h1>
<p>We adopt another check (Step 4 of Figure 1) for further ensuring the data quality. If the parametric memory we elicit is truly the internal belief of an LLM's, presenting it explicitly as evidence should lead the LLM to provide the same answer as in the closed-book setting (Step 1). Therefore, in the evidence-based QA task format, we use the parametric memory as the sole evidence and instruct LLMs to answer the same question again. For example, given the parametric memory about Demis and DeepMind, LLMs should have a consistent response with the previous memory answer, that Demis is the chief scientist of DeepMind.</p>
<p>However, the answer inconsistency results in Table 3 show that LLMs may still change their answers when the parametric memory obtained in Step 1 is explicitly presented as evidence. This suggests that the LLM's internal belief on this parametric memory may not be firm (e.g., there may competing answers that are equally plausible based on the LLM). We filter out such examples to ensure the remaining ones well capture an LLM's firm parametric memory.</p>
<p>After undergoing entailment and answer consistency checks, the remaining examples are likely to represent firm parametric memory and high-quality counter-memory, which lay a solid foundation for subsequent knowledge conflict experiments. Some examples from the final POPQA data are shown in Table 2 and the statistics of the final datasets are shown in Table 4. Please refer to Appendix B. 2 for more details for Step 3 and 4 and examples.</p>
<h3>3.6 Evaluation Metrics</h3>
<p>A single generation from an LLM could contain both the memory answer and the counter-answer, which poses a challenge to automatically determine the exact answer from an LLM. To address this issue, we transform the free-form QA to a multiple-choice QA format by providing a few options as possible answers. This limits the generation space and helps determine the answer provided by LLMs with certainty. Specifically, for each question from both datasets, LLMs are instructed to select one answer from memory answer (Mem-Ans.), counter-answer (Ctr-Ans.), and "Uncertain". Additionally, to quantify the frequency of LLMs sticking to their parametric memory, we adopt the memorization ratio metric (Longpre et al., 2021; Chen et al., 2022):</p>
<p>$$
M_{R}=\frac{f_{m}}{f_{m}+f_{c}}
$$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3: Answer inconsistency rate between closed-book results (Step 1) and evidence-based QA with parametric memory (Step 4).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">PopQA</th>
<th style="text-align: center;">StrategyQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">$4.7 \%$</td>
<td style="text-align: center;">$3.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$3.9 \%$</td>
<td style="text-align: center;">$2.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM2</td>
<td style="text-align: center;">$8.4 \%$</td>
<td style="text-align: center;">$2.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-7B</td>
<td style="text-align: center;">$5.4 \%$</td>
<td style="text-align: center;">$5.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-7B</td>
<td style="text-align: center;">$4.7 \%$</td>
<td style="text-align: center;">$7.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-70B</td>
<td style="text-align: center;">$2.3 \%$</td>
<td style="text-align: center;">$0.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-7B</td>
<td style="text-align: center;">$12.4 \%$</td>
<td style="text-align: center;">$6.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-33B</td>
<td style="text-align: center;">$16.6 \%$</td>
<td style="text-align: center;">$5.3 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Number of final examples for each LLM. The difference between LLMs is due to their different outputs going through the framework.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">PopQA(#)</th>
<th style="text-align: center;">StrategyQA(#)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">7,947</td>
<td style="text-align: center;">1,245</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">9,544</td>
<td style="text-align: center;">1,356</td>
</tr>
<tr>
<td style="text-align: left;">PaLM2</td>
<td style="text-align: center;">5,256</td>
<td style="text-align: center;">500</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-7B</td>
<td style="text-align: center;">7,204</td>
<td style="text-align: center;">671</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-7B</td>
<td style="text-align: center;">8,027</td>
<td style="text-align: center;">698</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-70B</td>
<td style="text-align: center;">9,314</td>
<td style="text-align: center;">822</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-7B</td>
<td style="text-align: center;">4,170</td>
<td style="text-align: center;">559</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-33B</td>
<td style="text-align: center;">3,787</td>
<td style="text-align: center;">775</td>
</tr>
</tbody>
</table>
<p>where $f_{m}$ is the frequency of memory answer and $f_{c}$ is that of counter-answer. Higher memorization ratios signify LLMs relying more on their parametric memory, while lower ratios indicate more frequent adoption of the counter-memory.</p>
<h1>4 EXPERIMENTS</h1>
<h3>4.1 Single-SOURCE Evidence</h3>
<p>We experiment with LLMs in the single-source evidence setting where counter-memory is the sole evidence presented to LLMs. Such knowledge conflict happens when LLMs are augmented with tools returning single external evidence such as Wikipedia API (Yao et al., 2023). In particular, for counter-memory construction, we would apply 1) the entity substitution counter-memory method, a widely-applied strategy in previous work, and 2) our generation-based method.</p>
<p>LLMs are stubborn when encountering entity substitution-based counter-memory. Following previous work (Longpre et al., 2021; Chen et al., 2022), we substitute the exactly matched ground truth entity mentions in the parametric memory with a random entity of the same type. The counter-memory is then used as the sole evidence for LLMs to answer the question. Here is an example:</p>
<p>Evidence: Washington D.C. London, USA's capital, has the Washington Monument.
Question: What is the capital city of USA? Answer by ChatGPT: Washington D.C.
Figure 2 shows the results with this approach on PopQA dataset. Observably, although the instruction clearly guides LLMs to answer questions based on the given counter-memory, LLMs still stick to their parametric memory instead, especially for three closed-sourced LLMs (ChatGPT, GPT-4, and PaLM2). This observation is aligned with previous work (Longpre et al., 2021). The reasons may stem from the incoherence of the evidence built with substitution: In the given example, although "Washington D.C." is successfully substituted by "London", the context containing Washington Monument and USA still highly correlate with the original entity, impeding LLMs to generate London as the answer. Furthermore, when comparing Llama2-7B and Vicuna-7B to their larger counterparts in the same series (i.e., Llama2-70B and Vicuna-33B), we observe that the larger LLMs are more inclined to insist on their parametric memory. We suppose that larger LLMs, due to their enhanced memorization and reasoning capabilities, are more sensitive to incoherent sentences.</p>
<p>LLMs are highly receptive to generated coherent counter-memory. To alleviate the incoherence issue of the above counter-memory, we instruct LLMs to directly generate coherent countermemory following the steps aforementioned (Figure 1). Figure 2 shows the experimental results with generation-based counter-memory, from which we can have the following observations:
First, LLMs are actually highly receptive to external evidence if it is presented in a coherent way, even though it conflicts with their parametric memory. This contradicts the prior conclusion (Longpre et al., 2021) and the observation with entity substitution counter-memory shown in Figure 2. Such high receptiveness in turn shows that the counter-memory constructed through our framework is indeed more coherent and convincing. We manually check 50 stubborn (i.e., "Mem-Ans.") cases and</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Answer distributions of entity substitution-based (Subs.) and generation-based (Gen.) counter-memory as the single evidence. Mem-Ans. and Ctr-Ans. refers to memory answer and counter-answer, respectively.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Memorization ratio of LLMs answering questions from different popularity categories. Higher memorization ratio indicates LLMs rely more on their parametric memory and generate the memory answer. We choose four widely-used LLMs as experimental objects.
find that most of them are due to hard-to-override commonsense or lack of strong direct conflicts. Detailed analyses can be found in Appendix B.3.</p>
<p>Second, many of the generated counter-memory are disinformation that misleads LLMs to the wrong answer. Concerningly, LLMs appear to be susceptible to and can be easily deceived by such disinformation. Exploring methods to prevent LLMs from such attacks when using external tools warrants significant attention in future research.</p>
<p>Third, the effectiveness of our generated counter-memory also shows that LLMs can generate convincing dis- or misinformation, sufficient to mislead even themselves. This raises concerns about the potential misuse of LLMs.</p>
<h1>4.2 Multi-SOURCE Evidence</h1>
<p>Multi-source evidence is a setting where multiple pieces of evidence that either supports or conflicts with the parametric memory are presented to LLMs. Such knowledge conflicts can happen frequently, e.g., when LLMs are augmented with search engines having diverse or even web-scale information sources. We study the evidence preference of LLMs from different aspects of evidence, including popularity, order, and quantity. By default, the order of evidence is randomized in all experiments in Section 4.2, if not specified otherwise.</p>
<p>LLMs exhibit stronger confirmation bias in more popular knowledge. Step 5 in Figure 1 illustrates how we instruct LLMs to answer questions when both parametric memory and countermemory are presented as evidence. Figure 3 shows the memorization ratio of different LLMs w.r.t. the question popularity on POPQA.</p>
<p>Table 5: Memorization ratio of LLMs with different evidence orders.</p>
<table>
<thead>
<tr>
<th>First Evidence</th>
<th>PopQA</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>StrategyQA</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>ChatGPT</td>
<td>GPT-4</td>
<td>PaLM2</td>
<td>Llama2-7B</td>
<td>ChatGPT</td>
<td>GPT-4</td>
<td>PaLM2</td>
<td>Llama2-7B</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Parametric Memory</td>
<td>46.7</td>
<td>60.9</td>
<td>38.6</td>
<td>33.3</td>
<td>59.5</td>
<td>73.6</td>
<td>43.6</td>
<td>84.0</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Random</td>
<td>43.0</td>
<td>61.9</td>
<td>56.8</td>
<td>58.4</td>
<td>50.1</td>
<td>71.7</td>
<td>55.3</td>
<td>84.5</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Counter-memory</td>
<td>40.1</td>
<td>62.7</td>
<td>72.2</td>
<td>82.8</td>
<td>42.2</td>
<td>70.5</td>
<td>76.9</td>
<td>86.2</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 6: Memorization ratio of LLMs under varying proportions of parametric memory in all the available evidence, e.g., $1 / 3$ means one piece of parametric memory and two pieces of counter-memory.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>PopQA</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>StrategyQA</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$\begin{gathered} \sigma / 2 \ (0 \%) \end{gathered}$</td>
<td>$\begin{gathered} 1 / 3 \ (33 \%) \end{gathered}$</td>
<td>$\begin{gathered} 1 / 2 \ (50 \%) \end{gathered}$</td>
<td>$\begin{gathered} 2 / 4 \ (50 \%) \end{gathered}$</td>
<td>$\begin{gathered} 2 / 3 \ (67 \%) \end{gathered}$</td>
<td>$\begin{gathered} 2 / 2 \ (100 \%) \end{gathered}$</td>
<td>$\begin{gathered} \sigma / 2 \ (0 \%) \end{gathered}$</td>
<td>$\begin{gathered} 1 / 3 \ (33 \%) \end{gathered}$</td>
<td>$\begin{gathered} 1 / 2 \ (50 \%) \end{gathered}$</td>
<td>$\begin{gathered} 2 / 4 \ (50 \%) \end{gathered}$</td>
<td>$\begin{gathered} 2 / 3 \ (67 \%) \end{gathered}$</td>
</tr>
<tr>
<td>Closed-source LLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ChatGPT</td>
<td>3.7</td>
<td>30.0</td>
<td>43.0</td>
<td>63.3</td>
<td>86.2</td>
<td>99.8</td>
<td>2.6</td>
<td>26.8</td>
<td>50.0</td>
<td>48.9</td>
<td>72.6</td>
</tr>
<tr>
<td>GPT-4</td>
<td>8.9</td>
<td>50.3</td>
<td>65.4</td>
<td>75.4</td>
<td>91.0</td>
<td>99.8</td>
<td>13.0</td>
<td>46.0</td>
<td>72.8</td>
<td>72.9</td>
<td>88.7</td>
</tr>
<tr>
<td>PaLM2</td>
<td>15.8</td>
<td>15.8</td>
<td>56.8</td>
<td>53.9</td>
<td>69.9</td>
<td>89.5</td>
<td>18.1</td>
<td>52.9</td>
<td>55.3</td>
<td>65.2</td>
<td>71.5</td>
</tr>
<tr>
<td>Open-source LLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Qwen-7B</td>
<td>2.3</td>
<td>32.5</td>
<td>52.3</td>
<td>63.0</td>
<td>80.4</td>
<td>99.2</td>
<td>9.5</td>
<td>55.1</td>
<td>56.8</td>
<td>67.6</td>
<td>76.3</td>
</tr>
<tr>
<td>Llama2-7B</td>
<td>2.6</td>
<td>34.6</td>
<td>58.4</td>
<td>65.1</td>
<td>83.7</td>
<td>91.7</td>
<td>11.5</td>
<td>70.8</td>
<td>84.5</td>
<td>84.1</td>
<td>89.1</td>
</tr>
<tr>
<td>Llama2-70B</td>
<td>3.0</td>
<td>21.6</td>
<td>58.4</td>
<td>62.9</td>
<td>72.9</td>
<td>96.0</td>
<td>11.6</td>
<td>48.7</td>
<td>57.8</td>
<td>70.8</td>
<td>80.7</td>
</tr>
<tr>
<td>Vicuna-7B</td>
<td>1.7</td>
<td>29.5</td>
<td>45.9</td>
<td>56.2</td>
<td>74.6</td>
<td>98.6</td>
<td>44.9</td>
<td>86.1</td>
<td>87.0</td>
<td>88.6</td>
<td>89.8</td>
</tr>
<tr>
<td>Vicuna-33B</td>
<td>4.6</td>
<td>49.5</td>
<td>51.7</td>
<td>75.7</td>
<td>87.7</td>
<td>99.1</td>
<td>32.1</td>
<td>52.0</td>
<td>53.1</td>
<td>54.7</td>
<td>59.3</td>
</tr>
</tbody>
</table>
<p>First, compared with when only the generated counter-memory is presented as evidence (singlesource), both LLMs demonstrate significantly higher memorization ratios when parametric memory is also provided as evidence (multi-source), especially in the case of GPT-4. In other words, when faced with conflicting evidence, LLMs often prefer the evidence consistent with their internal belief (parametric memory) over the conflicting evidence (counter-memory), demonstrating a strong confirmation bias (Nickerson, 1998). Such properties could hinder the unbiased use of external evidence in tool-augmented LLMs.</p>
<p>Second, for questions regarding more popular entities, LLMs demonstrate a stronger confirmation bias. In particular, GPT-4 shows an $80 \%$ memorization ratio for the most popular questions. This may suggest that LLMs form a stronger belief in facts concerning more popular entities, possibly because they have seen these facts and entities more often during pre-training, which leads to a stronger confirmation bias.</p>
<p>LLMs demonstrate a noticeable sensitivity to the evidence order. Previous work has shown a tendency in tool-augmented language models to select evidence presented in the top place (BehnamGhader et al., 2022) and the order sensitivity in LLMs (Lu et al., 2022). To demystify the impact of the evidence-presenting order in LLMs, we respectively put parametric memory and counter-memory as the first evidence in multi-source settings. As a reference, the results of first evidence randomly selected from the two are also reported in Table 5. In line with the popularity experiment, we use the same LLMs.</p>
<p>We observe that, with the exception of GPT-4, other models demonstrated pronounced order sensitivity, with fluctuations exceeding 5\%. It's especially concerning that the variations in PaLM2 and Llama27B surpassed 30\%. When evidence is presented first, ChatGPT tends to favor it; however, PaLM2 and Llama2-7B lean towards later pieces of evidence. Such order sensitivity for evidence in the context may not be a desirable property for tool-augmented LLMs. By default, the order of evidence is randomized in other experiments in this section.
LLMs follow the herd and choose the side with more evidence. In addition to LLM-generated evidence (parametric memory and counter-memory), we also extend to human-crafted ones such as Wikipedia. These highly credible and accessible human-written texts are likely to be retrieved as evidence by real-world search engine tools. We adopt Wikipedia passages from PopQA and manually annotated facts from StrategyQA with post-processing to ensure that the ground truth answer can indeed be deduced. Please refer to Appendix B. 4 for more processing details.</p>
<p>To balance the quantity of evidence supporting memory answer and counter-answer, we create additional evidence through the method mentioned in Section 3.3, with the goal of achieving a</p>
<p>Table 7: Answer distribution of ChatGPT and Llama2-7B under different quantities of relevant (i.e., parametric memory and counter-memory) and irrelevant evidence (Irr.). In this setting, LLMs may generate irrelevant answers (Irr-Ans.). "w/ Relevant Evidence" means that we provide both a parametric memory and a counter-memory as evidence. Under the setting of 'w/o relevant evidence', the notation "-" indicates no counter-answers, consistent with the premise of lacking counter-memory.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Irr.( $#$ )</th>
<th style="text-align: center;">w/o Relevant Evidence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">w/ Relevant Evidence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mem-Ans.</td>
<td style="text-align: center;">Ctr-Ans.</td>
<td style="text-align: center;">Irr-Ans.</td>
<td style="text-align: center;">Uncertain</td>
<td style="text-align: center;">Mem-Ans.</td>
<td style="text-align: center;">Ctr-Ans.</td>
<td style="text-align: center;">Irr-Ans.</td>
<td style="text-align: center;">Uncertain</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">2.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">4.3</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-7B</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">0.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">0.2</td>
</tr>
</tbody>
</table>
<p>balanced 2:2 split at most between parametric memory and counter-memory evidence. Table 6 shows the memorization ratio under different proportions between parametric memory-aligned evidence and counter-memory. We have three main observations: 1) LLMs generally provide answers backed by the majority of evidence. The higher the proportion of evidence supporting a particular answer, the more likely LLMs will return that answer. 2) The confirmation bias becomes increasingly obvious with a rise in the quantity of parametric memory evidence, despite maintaining a consistent relative proportion (e.g., $1 / 2$ vs. $2 / 4$ ). 3) Compared to other LLMs, GPT-4 and Vicuna-33B are less receptive to counter-memory across all proportions of evidence. Particularly, regardless of more pieces of evidence supporting the counter-answer (ratio $1 / 3$ ), these two models still noticeably cling to their parametric memory. These observations once again signify the confirmation bias in LLMs.</p>
<p>LLMs can be distracted by irrelevant evidences. We further experiment on more complicated knowledge conflict scenario. We are interested in this question: Tools such as search engine may return irrelevant evidence - What if irrelevant evidence is presented to LLMs? When irrelevant evidence is presented, LLMs are expected to 1) abstain if no evidence clearly supports any answer and 2) ignore irrelevant evidence and answer based on the relevant ones. To set up, we regard top-ranked irrelevant passages retrieved by Sentence-BERT embeddings ${ }^{4}$ (Reimers \&amp; Gurevych, 2019) as irrelevant evidence (i.e., sentences unrelated to the entities shown in the question). The experimental results on PopQA are presented in Table 7. We find that: 1) With only irrelevant evidence provided, LLMs can be distracted by them, delivering irrelevant answers. And this issue is particularly concerning in Llama2-7B. Meanwhile, as more irrelevant evidence is introduced, LLMs become less likely to answer based on their parametric memory. 2) With both relevant and irrelevant evidence provided, LLMs can filter out the irrelevant ones to a certain extent. This observation aligns with the study by Shi et al. (2023a) on how LLMs might be distracted by irrelevant context in mathematics problems. Furthermore, we find that as the quantity of irrelevant evidence increases, such an ability diminishes, especially in the case of Llama2-7B.</p>
<h1>5 CONCLUSION</h1>
<p>In this work, we propose a systematic framework to elicit the parametric memory of LLMs, construct counterpart counter-memory, and design a series of checks to entire their quality. With these parametric memory and counter-memory as external evidence, we simulate comprehensive scenarios as controlled experiments to unravel the behaviors of LLMs in knowledge conflicts. We find that LLMs are highly receptive to counter-memory when it is the only evidence presented in a coherent way. However, LLMs also demonstrate a strong confirmation bias toward parametric memory when both supportive and contradictory evidence to their parametric memory are present. In addition, we show that LLMs' evidence preference is influenced by the popularity, order, and quantity of evidence, none of which may be a desired property for tool-augmented LLMs. Finally, the effectiveness of our framework also demonstrates that LLMs can generate convincing misinformation, which poses potential ethical risks. We hope our work provides a solid evaluation testbed and useful insights for understanding, improving, and deploying tool-augmented LLMs in the future.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>ETHICS STATEMENT</h1>
<p>Our study highlights a serious concern: LLMs can be instructed to make up coherent and convincing fake information. This underscores the potential misuse of these models if left unchecked. As researchers, it is our duty to address this pressing issue. The risks associated with the misuse of LLMs demand robust safeguards and prevention measures, requiring concerted effort from the wider research community. To this end, we commit to careful distribution of the data generated through our research, ensuring it serves strictly for research purposes. Our goal is to mitigate the risks while maximizing the benefits offered by LLMs.</p>
<h2>REPRODUCIbILITY STATEMENT</h2>
<p>Our experiments utilize three closed-sourced LLMs accessed via API, as well as five open-sourced LLMs. We have increased reproducibility by including the prompts used in our experiments in Appendix C. As for the versions of the closed-sourced LLMs, we used ChatGPT-0301, GPT-4-0314, and Chat-Bison-001 of PaLM2 in all our tests.</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>The authors would like to thank colleagues from the OSU NLP group for their constructive feedback and manual evaluations. The authors would also like to thank Siyu Yuan, Wei Shi, and Jiayi Fu from Fudan University as well as the anonymous reviewers for their valuable comments. This research was sponsored in part by Cisco and YS's startup funds.</p>
<h2>REFERENCES</h2>
<p>Alibaba. Qwen, 2023. URL https://github.com/QwenLM/Qwen-7B/blob/main/tech_memo.md.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>AutoGPT. Autogpt, 2023. URL https://github.com/Significant-Gravitas/AutoGPT.
Parishad BehnamGhader, Santiago Miret, and Siva Reddy. Can retriever-augmented language models reason? the blame game between the retriever and the language model. arXiv preprint arXiv:2212.09146, 2022. URL https://arxiv.org/abs/2212.09146.</p>
<p>Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In Proceedings of ICML, 2022. URL https://proceedings.mlr.press/v162/borgeaud22a.html.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Proceedings of NeurIPS, 2020. URL https://papers.nips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.</p>
<p>Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In Proceedings of USENIX Security Symposium, 2021. URL https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting.</p>
<p>Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. In Proceedings of ICLR, 2023. URL https://openreview.net/forum?id=TatRHT_1cK.</p>
<p>Hung-Ting Chen, Michael Zhang, and Eunsol Choi. Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence. In Proceedings of EMNLP, pp. 2292-2307, 2022. URL https://aclanthology.org/2022.emnlp-main.146.</p>
<p>Jiangjie Chen, Wei Shi, Ziquan Fu, Sijie Cheng, Lei Li, and Yanghua Xiao. Say what you mean! large language models speak too positively about negative commonsense knowledge. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9890-9908, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.acl-long. 550.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. URL https: //arxiv.org/abs/2204.02311.</p>
<p>Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. In Proceedings of ACL, 2022. URL https://aclanthology.org/2022. acl-long. 581 .</p>
<p>Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In Proceedings of EMNLP, 2021. URL https://aclanthology.org/2021.emnlp-main.522.</p>
<p>Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. Transactions of ACL, 2021. URL https://aclanthology.org/2021.tacl-1.60/.</p>
<p>Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate text with citations. arXiv preprint arXiv:2305.14627, 2023.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of ACL, 2021. URL https://aclanthology.org/2021.tacl-1.21/.</p>
<p>Reto Gubelmann and Siegfried Handschuh. Context matters: A pragmatic study of plms' negation understanding. In Proceedings of ACL, 2022. URL https://aclanthology.org/2022.acl-long.315/.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In Proceedings of ICML, pp. 3929-3938, 2020. URL https://dl.acm. org/doi/abs/10.5555/3524938.3525306.</p>
<p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. In Proceedings of ICLR, 2021. URL https://openreview.net/forum?id= XPZIaotutsD.</p>
<p>Gautier Izacard and Édouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of EACL, 2021. URL https://aclanthology.org/2021. eacl-main. 74 .</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 2023. URL https://dl.acm.org/doi/10.1145/3571730.</p>
<p>Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of ACL, 2020. URL https://aclanthology.org/2020.tacl-1.28.</p>
<p>Nora Kassner, Oyvind Tafjord, Hinrich Schütze, and Peter Clark. Beliefbank: Adding memory to a pre-trained language model for a systematic notion of belief. In Proceedings of EMNLP, 2021. URL https://aclanthology.org/2021.emnlp-main.697.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In Proceedings of ICLR, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.</p>
<p>Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d'Autume, Tomáš Kočiský, Sebastian Ruder, Dani Yogatama, Kris Cao, Susannah Young, and Phil Blunsom. Mind the gap: Assessing temporal generalization in neural language models. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Proceedings of NeurIPS, 2021. URL https://openreview.net/forum?id= 73OmmrCfSyy.</p>
<p>Xiang Lorraine Li, Adhiguna Kuncoro, Jordan Hoffmann, Cyprien de Masson d'Autume, Phil Blunsom, and Aida Nematzadeh. A systematic investigation of commonsense knowledge in large language models. In Proceedings of EMNLP, 2022. URL https://aclanthology.org/2022. emnlp-main.812/.</p>
<p>Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models. In Proceedings of EMNLP, 2020. URL https://aclanthology.org/2020.emnlp-main.557.</p>
<p>Adam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal, D'Autume Cyprien De Masson, Tim Scholtes, Manzil Zaheer, Susannah Young, et al. Streamingqa: A benchmark for adaptation to new knowledge over time in question answering models. In Proceedings of ICML, 2022. URL https://proceedings.mlr.press/v162/liska22a/liska22a.pdf.</p>
<p>Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based knowledge conflicts in question answering. In Proceedings of EMNLP, 2021. URL https://aclanthology.org/2021.emnlp-main.565.</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. arXiv preprint arXiv:2304.09842, 2023. URL https://arxiv.org/abs/2304.09842.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of ACL, 2022. URL https://aclanthology.org/2022.acl-long.556.</p>
<p>Kelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, and Noah A Smith. Time waits for no one! analysis and challenges of temporal misalignment. In Proceedings of NAACL, 2022. URL https://aclanthology.org/2022.naacl-main.435/.</p>
<p>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511, 2022. URL https://arxiv.org/abs/ 2212.10511.</p>
<p>Kevin Meng, David Bau, Alex J Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. In Proceedings of NeurIPS, 2022. URL https://openreview.net/forum?id= -h6WAS6eE4.</p>
<p>Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in a transformer. In Proceedings of ICLR, 2023. URL https://openreview.net/forum?id= MkbcAHIYgyS.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. URL https: //arxiv.org/abs/2112.09332.</p>
<p>Raymond S Nickerson. Confirmation bias: A ubiquitous phenomenon in many guises. Review of general psychology, 2(2):175-220, 1998. URL https://journals.sagepub.com/doi/abs/10.1037/ 1089-2680.2.2.175?journalCode=rgpa.</p>
<p>Tong Niu and Mohit Bansal. Adversarial over-sensitivity and over-stability strategies for dialogue models. In Proceedings of CoNLL, 2018. URL https://aclanthology.org/K18-1047/.</p>
<p>OpenAI. Chatgpt, 2022. URL https://openai.com/blog/chatgpt.
OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https://arxiv.org/abs/ 2303.08774.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In Proceedings of NeurIPS, 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html.</p>
<p>Liangming Pan, Wenhu Chen, Min-Yen Kan, and William Yang Wang. Contraqa: Question answering under contradicting contexts. arXiv preprint arXiv:2110.07803, 2021. URL https://arxiv.org/abs/ 2110.07803 .</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of EMNLP-IJCNLP, 2019. URL https://aclanthology.org/D19-1250.</p>
<p>Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models' factual predictions. In Proceedings of $A K B C, 2020$. URL https://openreview.net/forum?id=025X0zPfn.</p>
<p>Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint arXiv:2304.08354, 2023. URL https://arxiv.org/abs/2304.08354.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for squad. In Proceedings of ACL, 2018. URL https://arxiv.org/abs/1806.03822.</p>
<p>Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083, 2023. URL https://arxiv.org/abs/2302.00083.</p>
<p>Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of EMNLP, 2019. URL https://arxiv.org/abs/1908.10084.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? In Proceedings of EMNLP, 2020. URL https://aclanthology.org/ 2020.emnlp-main.437/.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. URL https://arxiv.org/abs/2302.04761.</p>
<p>Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. arXiv preprint arXiv:2302.00093, 2023a. URL https://arxiv.org/abs/2302.00093.</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023b. URL https://arxiv.org/abs/2302.00083.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation. In Findings of EMNLP, 2021. URL https://aclanthology. org/2021.findings-emnlp.320/.</p>
<p>Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber, and Lijuan Wang. Prompting GPT-3 to be reliable. In Proceedings of ICLR, 2023. URL https://openreview.net/forum?id=98p5x51L5af.</p>
<p>Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language models. In Proceedings of ICLR, 2023. URL https://openreview.net/forum?id=-cqvvvb-NkI.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. URL https: //arxiv.org/abs/2302.13971.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, et al. Freshllms: Refreshing large language models with search engine augmentation. arXiv preprint arXiv:2310.03214, 2023.</p>
<p>Boshi Wang, Xiang Yue, and Huan Sun. Can chatgpt defend the truth? automatic dialectical evaluation elicits llms' deficiencies in reasoning. arXiv preprint arXiv:2305.13160, 2023.</p>
<p>Cunxiang Wang, Pai Liu, and Yue Zhang. Can generative pre-trained language models serve as knowledge bases for closed-book qa? In Proceedings of ACL-IJCNLP, 2021. URL https: //aclanthology.org/2021.acl-long.251/.</p>
<p>Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. Symbolic knowledge distillation: from general language models to commonsense models. In Proceedings of NAACL, 2022. URL https://aclanthology.org/2022. naacl-main. 341 .</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In Proceedings of ICLR, 2023. URL https://arxiv.org/abs/2210.03629.</p>
<p>Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large language models are strong context generators. In Proceedings of ICLR, 2023. URL https://openreview.net/forum?id= fB0hRu9GZUS.</p>
<p>Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. Automatic evaluation of attribution by large language models. arXiv preprint arXiv:2305.06311, 2023. URL https: //arxiv.org/abs/2305.06311.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130b: An open bilingual pre-trained model. In Proceedings of ICLR, 2023. URL https://openreview.net/forum?id=-Aw0rrrPUF.</p>
<p>Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramèr, and Nicholas Carlini. Counterfactual memorization in neural language models. arXiv preprint arXiv:2112.12938, 2021. URL https://arxiv.org/abs/2112.12938.</p>
<p>Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, and Joseph E Gonzalez. The wisdom of hindsight makes language models better instruction followers. arXiv preprint arXiv:2302.05206, 2023.</p>
<p>Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al. Retrieving multimodal information for augmented generation: A survey. arXiv preprint arXiv:2303.10868, 2023. URL https://arxiv.org/ abs/2303.10868.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.</p>
<p>Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation. In Proceedings of EMNLP, 2022. URL https://aclanthology.org/2022.emnlp-main. 382.</p>
<p>Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. Context-faithful prompting for large language models. arXiv preprint arXiv:2303.11315, 2023. URL https://arxiv.org/abs/2303.11315.</p>
<h1>APPENDIX</h1>
<p>Within this supplementary material, we elaborate on the following aspects:</p>
<ul>
<li>Appendix A: Discussions</li>
<li>Appendix B: Experimental Setup Details</li>
<li>Appendix C: Prompts List</li>
</ul>
<h2>A Discussions</h2>
<h2>A. 1 Broader Impact and Potential Solutions</h2>
<p>We observe two behaviors of LLMs in knowledge conflict: (1) high receptiveness to single external evidence and (2) confirmation bias to multiple pieces of external evidence, and we will discuss its impact and potential solutions in detail.</p>
<p>Firstly, high receptiveness is a two-sided coin. On one side, it implies that remedying the outdated or incorrect parametric knowledge of LLMs can be effectively achieved (Zheng et al., 2023; Vu et al., 2023), which is beneficial to methodologies such as retrieval-augmented generation. On the other side, as LLMs are increasingly connected with external tools, such as ChatGPT Plugins and recent language agents like AutoGPT (AutoGPT, 2023), the high receptiveness to external input raises concerns - LLMs can be easily deceived by misleading or manipulative information from malicious third-party tools.</p>
<p>Confirmation bias is a highly undesired property, especially for generative search engines or similar applications (e.g., multi-document summarization) of LLMs where orchestrating multiple pieces of potentially contradicting information in an unbiased way is important.</p>
<p>In terms of potential solutions, for the risks due to high receptiveness, a validation and monitoring system should be employed to prevent improper information by third-party tools from being presented to LLMs. For confirmation bias, depending on the deployment scenarios, further alignment through fine-tuning or reinforcement learning from human feedback (RLHF) Ouyang et al. (2022); Zhang et al. (2023) to reduce the bias could be a promising direction. Finally, from a generative search engine perspective, citing the sources for the answer and letting users be more informed and judge the final answer can be a more reliable way (Yue et al., 2023; Gao et al., 2023).</p>
<h2>A. 2 Additional Knowledge Conflict Discussion</h2>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure A.1: We report the changes in the ChatGPT's evidence preference before and after fragmenting the evidence. OthersToWhole means ChatGPT now favors the entire evidence supporting a different answer, which is inconsistent with its preference before fragmentation.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure A.2: The answer distribution of ChatGPT under different length ratios between parametric memory and counter-memory.</p>
<p>LLMs barely consider short counter-memory, while they adopt parametric memory of any length. As a proxy of convincing degree, the length of evidence may affect the preference of LLMs. To verify it, we categorize the examples based on the length ratio between parametric memory and</p>
<p>Table A.1: Answer distribution of ChatGPT when given extremely short evidence (i.e., answer presented as evidence). Memory Answer and Counter-answer indicates parametric memory and counter-memory are replaced by their corresponding answer, respectively. Standard denotes both pieces of evidence are intact.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Evidence</th>
<th style="text-align: center;">Mem-Ans.</th>
<th style="text-align: center;">Ctr-Ans.</th>
<th style="text-align: center;">Uncertain</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Memory Answer</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">1.2</td>
</tr>
<tr>
<td style="text-align: left;">Standard</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr>
<td style="text-align: left;">Counter-answer</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">1.3</td>
</tr>
</tbody>
</table>
<p>counter-memory, i.e., $&lt;0.8,&gt;1.2$, and $[0.8,1.2]$, which are distinguishable in the data samples. ${ }^{5}$ Figure A. 2 shows the answer distribution within each category. It is evident that ChatGPT tends to adopt the longer side, especially in STRATEGYQA, where longer evidence generally indicates more reasoning steps.</p>
<p>To explore the largest impact of evidence length, we further explore the scenarios with extremely short evidence. Specifically, we present the answer as evidence to LLMs directly and investigate whether they adopt such a short evidence without any concrete explanations. We alternately replace either parametric memory or counter-memory with their respective supporting answers, while keeping the other one intact. This results in memory answer vs. counter-memory and counter-answer vs. parametric memory. Table A. 1 shows the results of PopQA: shorter counter-memory evidence (counter-answer) is less likely to be considered by LLMs ( $56.7 \%$ to $18.8 \%$ ). However, shortening parametric memory evidence into memory answer does not affect the preferences of LLMs much; interestingly, it is even more favored by LLMs ( $42.7 \%$ to $43.9 \%$ ). In other words, persuading LLMs to embrace counter-memory needs informative and solid evidence. In contrast, short evidence that aligns with parametric memory is acceptable enough by LLMs as the associated memory is encoded in the parameters already. This observation indicates the parametric memory we elicit could well be the firm beliefs of LLMs. More importantly, this unequal receptiveness to evidence further highlights the presence of strong confirmation bias in LLMs, a potentially significant limitation when they are used in tool-augmented applications.</p>
<p>LLMs demonstrate a deficiency in information integration. In real-world scenarios, a complex query may require fragmented evidence gathered from different sources to have the final answer. As a multi-step reasoning dataset, STRATEGYQA provides multiple separate pieces of evidence related to sub-questions. Therefore, we take StrategyQA as an ideal sample dataset for such exploration. In the standard mode, we merge these facts to construct an intact piece of evidence. However, in this setting, we treat each fact as an individual piece of evidence, without any consolidation. The results in Figure A. 1 clearly show: after the original evidence (parametric memory or counter-memory) used by ChatGPT is fragmented, ChatGPT shifts to consider the other intact evidence (counter-memory or parametric memory) in $38.2 \%$ examples, indicating the limited abilities of LLMs to integrate fragments of evidence. This observation also suggests that the same external evidence in different formats (fragmented or whole) may have different effects on LLMs in the tool-augmented systems. Therefore, from the perspective of external tools, it is worth exploring the presentation of evidence in an easy-to-use format for LLMs in the future.</p>
<p>LLMs are confident in their response. Beyond observing textual responses, we also investigate how confident the LLMs are in their responses. With Llama2-7B as a case study, we report the log probabilities for the token it generates, after normalizing over all three tokens representing memory answer, counter-answer, and uncertain. Specifically, we mainly explore two scenarios:</p>
<ul>
<li>Firstly, in the single-source setting where counter-memory is presented as the sole evidence, we sampled 1,000 examples that Llama2-7B gives a counter-answer. In Figure A.3, Llama27B shows high confidence when generating the counter-answer and $91.3 \%$ of examples have a memory answer probability of $95 \%$ or greater. This demonstrates the high receptiveness to the external evidence, even when it conflicts with LLM's parametric memory.</li>
<li>Secondly, in the multi-source scenario where two supportive and two contradictory pieces of evidence are presented, we sample 1,000 instances that Llama2-7B favors the counter-</li>
</ul>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure A.3: Normalized log probability for the first token of counter-answer when counter-memory is the only external evidence presented to Llama2-7B.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure A.4: Normalized log probability for the first token of memory answer when four evidence (two supportive and two contradictory to the parametric memory) are presented.
answer. Figure A. 4 shows that Llama2-7B is confident in its memory answer response, based on the token log probability. For instance, $96.3 \%$ of the examples show a log probability of $95 \%$ or greater for the counter-answer. Both the high frequency ( $65 \%$ in Table 6) and the high confidence of using memory-aligned evidence indicate the potential confirmation bias of LLMs.</p>
<h1>B EXPERIMENTAL SETUP DETAILS</h1>
<h2>B. 1 COUNTER-MEMORY CONSTRUCTION DETAILS</h2>
<p>To construct high-quality counter-memory, we incorporate ChatGPT as a generator to produce text at a human-written level. Specifically, we first reframe the memory answer to construct the counter-answer. For different datasets, we utilize different strategies.</p>
<p>Due to the PopQA is a entity-centric QA dataset, we adopt the following principles: (i) If the memory answer is wrong, we directly adopt the triplets provided by PopQA. (ii) If the memory answer is right, we substitute the object entities in the triplets with those of the same relation from the ground truth (the objects within the same relationship category are of consistent entity types). Filters are applied based on exact matching to prevent any overlap between the selected entities and the candidate ground truth. Subsequently, we use a template to generate claims in a natural language format based on the triplets.</p>
<p>Considering that the output of StrategyQA is "True" or "False", it cannot be directly used as a claim. Therefore, we employ ChatGPT to generate two claims corresponding to "True" and "False", respectively. Based on the output, the generated claims are dynamically classified as memory answer and counter-answer. To ensure high-quality and control format, we adopt the in-context learning strategy and use three demonstrations.</p>
<p>After obtaining the counter-answer, we instruct the ChatGPT to generate the counter-memory.</p>
<h2>B. 2 DATASET DETAILS</h2>
<p>The dataset scale at each step are presented in the Table B.3. We also report the inconsistency type distribution in Table B.5. And some examples of answer inconsistency on LLMs are presented in Table B.6. In Table B.7, we show more examples in the final datasets.</p>
<h2>B. 3 EXAMPLES of Stubbornness in Response to Parametric MEMORY</h2>
<p>In Table B.8, we present some examples which are stubborn to give memory answer even only the counter-memory evidence given. Upon manually scrutinizing 50 randomly selected samples,</p>
<p>Table B.2: Human-written templates for POPQA counter-answer construction. [subj] and [obj] denote subject and object entity in triplet, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Relationship</th>
<th style="text-align: left;">Template</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">occupation</td>
<td style="text-align: left;">[subj]'s occupation is [obj].</td>
</tr>
<tr>
<td style="text-align: left;">place of birth</td>
<td style="text-align: left;">[subj] was born in [obj].</td>
</tr>
<tr>
<td style="text-align: left;">genre</td>
<td style="text-align: left;">The genre of [subj] is [obj].</td>
</tr>
<tr>
<td style="text-align: left;">father</td>
<td style="text-align: left;">[obj] is the father of [subj].</td>
</tr>
<tr>
<td style="text-align: left;">country</td>
<td style="text-align: left;">[subj] is in [obj].</td>
</tr>
<tr>
<td style="text-align: left;">producer</td>
<td style="text-align: left;">[obj] is the producer of [subj].</td>
</tr>
<tr>
<td style="text-align: left;">director</td>
<td style="text-align: left;">[obj] is the director of [subj].</td>
</tr>
<tr>
<td style="text-align: left;">capital of</td>
<td style="text-align: left;">[subj] is the capital of [obj].</td>
</tr>
<tr>
<td style="text-align: left;">screenwriter</td>
<td style="text-align: left;">[obj] was the screenwriter for [subj].</td>
</tr>
<tr>
<td style="text-align: left;">composer</td>
<td style="text-align: left;">[obj] was the composer of [subj].</td>
</tr>
<tr>
<td style="text-align: left;">color</td>
<td style="text-align: left;">The color of [subj] is [obj].</td>
</tr>
<tr>
<td style="text-align: left;">religion</td>
<td style="text-align: left;">[obj] is the religion of [subj].</td>
</tr>
<tr>
<td style="text-align: left;">sport</td>
<td style="text-align: left;">[subj] plays [obj].</td>
</tr>
<tr>
<td style="text-align: left;">author</td>
<td style="text-align: left;">[obj] is the author of [subj].</td>
</tr>
<tr>
<td style="text-align: left;">mother</td>
<td style="text-align: left;">[obj] is the mother of [subj].</td>
</tr>
<tr>
<td style="text-align: left;">capital</td>
<td style="text-align: left;">[obj] is the capital of [subj].</td>
</tr>
</tbody>
</table>
<p>Table B.3: The dataset scale at each step. "Illegal" indicates that the output format is not as expected (i.e. output the answer and supporting reason at the same time).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;">PaLM2</th>
<th style="text-align: center;">Qwen-7B</th>
<th style="text-align: center;">Llama2-7B</th>
<th style="text-align: center;">Llama2-70B</th>
<th style="text-align: center;">Vicuna-7B</th>
<th style="text-align: center;">Vicuna-33B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">POPQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Initial</td>
<td style="text-align: center;">14,267</td>
<td style="text-align: center;">14,267</td>
<td style="text-align: center;">14,267</td>
<td style="text-align: center;">14,267</td>
<td style="text-align: center;">14,267</td>
<td style="text-align: center;">14,267</td>
<td style="text-align: center;">14,267</td>
<td style="text-align: center;">14,267</td>
</tr>
<tr>
<td style="text-align: left;">Absention / Illeagal</td>
<td style="text-align: center;">12,435</td>
<td style="text-align: center;">14,194</td>
<td style="text-align: center;">12,476</td>
<td style="text-align: center;">12,759</td>
<td style="text-align: center;">14,197</td>
<td style="text-align: center;">14,175</td>
<td style="text-align: center;">13,185</td>
<td style="text-align: center;">14,219</td>
</tr>
<tr>
<td style="text-align: left;">Parametric Memory Entail</td>
<td style="text-align: center;">9,359</td>
<td style="text-align: center;">11,776</td>
<td style="text-align: center;">8,963</td>
<td style="text-align: center;">10,372</td>
<td style="text-align: center;">12,332</td>
<td style="text-align: center;">12,828</td>
<td style="text-align: center;">9,164</td>
<td style="text-align: center;">9,177</td>
</tr>
<tr>
<td style="text-align: left;">Answer-consistency Filter</td>
<td style="text-align: center;">8,920</td>
<td style="text-align: center;">11,437</td>
<td style="text-align: center;">7,836</td>
<td style="text-align: center;">9,905</td>
<td style="text-align: center;">11,733</td>
<td style="text-align: center;">12,444</td>
<td style="text-align: center;">7,915</td>
<td style="text-align: center;">7,624</td>
</tr>
<tr>
<td style="text-align: left;">Counter-memory Entail</td>
<td style="text-align: center;">7,949</td>
<td style="text-align: center;">9,544</td>
<td style="text-align: center;">5,256</td>
<td style="text-align: center;">7,204</td>
<td style="text-align: center;">8,027</td>
<td style="text-align: center;">9,314</td>
<td style="text-align: center;">4,170</td>
<td style="text-align: center;">3,787</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Initial</td>
<td style="text-align: center;">2,290</td>
<td style="text-align: center;">2,290</td>
<td style="text-align: center;">2,290</td>
<td style="text-align: center;">2,290</td>
<td style="text-align: center;">2,290</td>
<td style="text-align: center;">2,290</td>
<td style="text-align: center;">2,290</td>
<td style="text-align: center;">2,290</td>
</tr>
<tr>
<td style="text-align: left;">Absention / Illeagal</td>
<td style="text-align: center;">2,148</td>
<td style="text-align: center;">2,116</td>
<td style="text-align: center;">2,022</td>
<td style="text-align: center;">2,043</td>
<td style="text-align: center;">2,290</td>
<td style="text-align: center;">2,287</td>
<td style="text-align: center;">2,287</td>
<td style="text-align: center;">2,289</td>
</tr>
<tr>
<td style="text-align: left;">Parametric Memory Entail</td>
<td style="text-align: center;">1,698</td>
<td style="text-align: center;">2,015</td>
<td style="text-align: center;">715</td>
<td style="text-align: center;">858</td>
<td style="text-align: center;">898</td>
<td style="text-align: center;">989</td>
<td style="text-align: center;">711</td>
<td style="text-align: center;">979</td>
</tr>
<tr>
<td style="text-align: left;">Answer-consistency Filter</td>
<td style="text-align: center;">1,627</td>
<td style="text-align: center;">1,963</td>
<td style="text-align: center;">542</td>
<td style="text-align: center;">799</td>
<td style="text-align: center;">832</td>
<td style="text-align: center;">981</td>
<td style="text-align: center;">662</td>
<td style="text-align: center;">927</td>
</tr>
<tr>
<td style="text-align: left;">Counter-memory Entail</td>
<td style="text-align: center;">1,245</td>
<td style="text-align: center;">1,356</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">671</td>
<td style="text-align: center;">698</td>
<td style="text-align: center;">822</td>
<td style="text-align: center;">559</td>
<td style="text-align: center;">775</td>
</tr>
</tbody>
</table>
<p>we discover that ambiguity in counter-memory, commonsense question leading to unacceptable counter-memory, or highly suggestive questions, account for 34 of these instances. This implies that only a minimal fraction of LLMs demonstrate stubbornness towards parametric memory, reaffirming that LLMs maintain open in the single source setting.</p>
<h1>B. 4 Process for Human-written Evidence</h1>
<p>Despite the availability of retrieved Wikipedia passages in the POPQA dataset, not all questions have a high-quality inferential passage (i.e., containing the ground truth). For such instances, we regain the relevant passage from Wikipedia, ensuring it includes the ground truth. However, a small portion of data (around 400 instances) lack inferential passages even on Wikipedia. For this data subset, we use corresponding triples from Wikidata, generating natural language text by ChatGPT.</p>
<p>As for StrategyQA, the facts in it are manually written, ensuring each fact supports the ground truth, and therefore require no additional modifications.</p>
<h2>B. 5 Human Evaluation Detail for NLI Model Accuracy</h2>
<p>To ensure the quality of synthesized evidence used in experiments, we use a state-of-the-art natural language inference (NLI) model to filter out the less qualified examples. To estimate the effectiveness of NLI model for this purpose, we randomly sample 200 generated examples and manually annotate</p>
<p>Table B.4: Uncertain answer ratio.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">POPQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">StrategyQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{gathered} \stackrel{0}{2} / 2 \ (0 \%) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 1 / 2 \ (33 \%) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 1 / 2 \ (50 \%) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 2 / 4 \ (50 \%) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 2 / 3 \ (67 \%) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 2 / 3 \ (100 \%) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0 / 2 \ (0 \%) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 1 / 4 \ (33 \%) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 1 / 2 \ (50 \%) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 2 / 4 \ (50 \%) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 2 / 4 \ (67 \%) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 2 / 2 \ (100 \%) \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">Closed-source LLMs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">1.2</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">1.5</td>
</tr>
<tr>
<td style="text-align: center;">PaLM2</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">14.2</td>
</tr>
<tr>
<td style="text-align: center;">Open-source LLMs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Qwen-7B</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-7B</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-70B</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">0.4</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7B</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-33B</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">0.9</td>
</tr>
</tbody>
</table>
<p>Table B.5: Inconsistency type distribution. "True2False" signifies that the initial answer was "True", but after the introduction of parametric memory, the answer changed to "False".</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">True2False(\%)</th>
<th style="text-align: center;">False2True(\%)</th>
<th style="text-align: center;">True2Unknown(\%)</th>
<th style="text-align: center;">False2Unknown(\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">POPQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">6.9</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">PaLM2</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">15.5</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-7B</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">20.4</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-7B</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">17.0</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-70B</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-7B</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">11.3</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-33B</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.2</td>
</tr>
<tr>
<td style="text-align: left;">StrategyQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">41.6</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">PaLM2</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-7B</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-7B</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-70B</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-7B</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-33B</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>whether the generated content (including both parametric memory and counter-memory) entails the corresponding claim (memory answer and counter-answer). The labels are supportive (entailment in the NLI task) or not supportive (either neutral or contradiction in the NLI task). Then we evaluate the state-of-the-art NLI model over this dataset and calculate its accuracy.</p>
<h1>B. 6 Uncertainty Answer Ratio When LLMs Encounter Knowledge Conflict</h1>
<p>In Table B.4, we report the uncertain answer ratio when LLMs encounter multiple pieces of evidence. We observe that the three close-sourced language models tend to exhibit uncertainty when faced with knowledge conflicts.</p>
<h2>B. 7 IrReLeVANT Evidence</h2>
<p>We collect irrelevant evidence for the question from the human-written corpus (i.e., Wikipedia passages provided by POPQA). Specifically, we use SentenceBERT to retrieve the top 3 sentences with the highest similarity to the question. We limit our search to data within the same question type. Note that we exclude any evidence that includes the entity mentioned in the parametric memory or counter-memory, as it would affect the arrangement of our options. The method for constructing options for irrelevant evidence is based on the template provided in the Table B.2.</p>
<h1>B. 8 Fragmented Evidence</h1>
<p>The StrategyQA dataset incorporates human-written facts associated with each sub-question. In the standard mode, we merge these facts to construct an intact piece of evidence. However, in Section A.2, we treat each fact as an individual piece of evidence, without any consolidation.</p>
<h2>C Prompts List</h2>
<p>In Table C.9, we provide a comprehensive list of all the prompts that have been utilized in this study, offering a clear reference for understanding our experimental approach.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Examples of Answer Inconsistency</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ChatGPT on POPQA</td>
<td style="text-align: center;">The first paragraph answers the question and the second paragraph gives the reason. Q: Who is the mother of Eliot Sumner? <br> A: The mother of Eliot Sumner is the actress and musician Trudie Styler. Trudie Styler is the wife of the musician Sting, who is Eliot Sumner's father. She is also a successful actress and producer, known for her work in films such as "The Next Three Days" and "Moon". Additionally, Styler has released several albums as a musician and has been involved in various philanthropic endeavors throughout her career.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">According to the given information and your knowledge, answer the question. Information:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: Who is the mother of Eliot Sumner? <br> A: The information provided does not mention the mother of Eliot Sumner.</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT on StrategyQA</td>
<td style="text-align: center;">In the first paragraph, you are expected to answer the question "True" or "False". And in the second paragraph, you should give the evidence. <br> Q: Could an NBA game be completed within the span of the Six-Day War? <br> A: <br> False. <br> The Six-Day War lasted from June 5 to June 10, 1967, which is only six days. An NBA game typically lasts around two hours, so it would be impossible to complete a full game within the span of the Six-Day War.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">According to the given information and your knowledge, answer the question "True" or "False". <br> Information:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: Could an NBA game be completed within the span of the Six-Day War? <br> A: <br> True.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 on POPQA</td>
<td style="text-align: center;">The first paragraph answers the question and the second paragraph gives the reason. Q: Who is the author of Countdown to Final Crisis? <br> A: The author of Countdown to Final Crisis is Paul Dini. <br> Countdown to Final Crisis is a comic book series published by DC Comics, written by Paul Dini, along with other contributing writers such as Jimmy Palmiotti, Justin Gray, Tony Bedard, and Adam Beechen. The series serves as a lead-up to the Final Crisis event and features various characters from the DC Universe.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">According to the given information and your knowledge, answer the question. Information:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: Who is the mother of Eliot Sumner? <br> A: Paul Dini is the main author of Countdown to Final Crisis, along with contributing writers <br> such as Jimmy Palmiotti, Justin Gray, Tony Bedard, and Adam Beechen.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 on StrategyQA</td>
<td style="text-align: center;">In the first paragraph, you are expected to answer the question "True" or "False". And in the second paragraph, you should give the evidence. <br> Q: Can you only see hippopotamus in Africa? <br> A: <br> True <br> Hippopotamuses are native to Africa, specifically in the sub-Saharan region. They are primarily found in countries like Kenya, Tanzania, Zambia, Zimbabwe, and South Africa. While they can be seen in zoos and wildlife parks around the world, their natural habitat is exclusively in Africa.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">According to the given information and your knowledge, answer the question "True" or "False". <br> Information:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: Can you only see hippopotamus in Africa? <br> A: <br> False</td>
</tr>
</tbody>
</table>
<p>Table B.6: Some examples of answer inconsistency on LLMs. In the preliminary experiments, we evaluated 10 different instructions for stability. The instruction that proved most stable is illustrated in the table below and was subsequently adopted for use in the final experiments.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Consistent results and observations are found in results with other splits.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
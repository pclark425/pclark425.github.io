<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2008 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2008</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2008</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-47.html">extraction-schema-47</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <p><strong>Paper ID:</strong> paper-280000246</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.18880v1.pdf" target="_blank">OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization</a></p>
                <p><strong>Paper Abstract:</strong> Recent large-scale language models (LLMs) with long Chain-of-Thought reasoning-such as DeepSeek-R1-have achieved impressive results on Olympiad-level mathematics benchmarks. However, they often rely on a narrow set of strategies and struggle with problems that require a novel way of thinking. To systematically investigate these limitations, we introduce OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a controlled yet diverse benchmark designed to evaluate three axes of out-of-distribution generalization, inspired by Boden's typology of creativity: (1) Exploratory-applying known problem solving skills to more complex instances within the same problem domain; (2) Compositional-combining distinct reasoning skills, previously learned in isolation, to solve novel problems that require integrating these skills in new and coherent ways; and (3) Transformative-adopting novel, often unconventional strategies by moving beyond familiar approaches to solve problems more effectively. OMEGA consists of programmatically generated training-test pairs derived from templated problem generators across geometry, number theory, algebra, combinatorics, logic, and puzzles, with solutions verified using symbolic, numerical, or graphical methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance degradation as problem complexity increases. Moreover, we fine-tune the Qwen-series models across all generalization settings and observe notable improvements in exploratory generalization, while compositional generalization remains limited and transformative reasoning shows little to no improvement. By isolating and quantifying these fine-grained failures, OMEGA lays the groundwork for advancing LLMs toward genuine mathematical creativity beyond mechanical proficiency.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2008.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2008.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL Generalization Experiments (Qwen2.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning Generalization Experiments with Qwen2.5-7B-Instruct / Qwen2.5-Math-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic RL (GRPO) fine-tuning experiments evaluating exploratory, compositional, and transformative generalization on the OMEGA benchmark using Qwen2.5 models; measures in-distribution (ID) and out-of-distribution (OOD) performance across complexity levels and skill compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>Mathematical reasoning tasks (OMEGA benchmark: arithmetic, algebra, combinatorics, number theory, geometry, logic & puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>Qwen2.5-7B-Instruct and Qwen2.5-Math-7B (base models for RL fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>No explicit multi-stage curriculum was deployed; training data consisted of primitive/problem-family instances limited to low complexity (typically complexity levels 1-2). For compositional settings, training used primitives separately (500 samples per family) and excluded composite tasks; variants included expanding training complexity to levels 1-4 to test transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td>Primitives were trained separately per problem family (compositional runs: 500 samples per skill family; general RL training: 1k problems total). RL reliably improved primitive (isolated skill) accuracy — examples: pattern-matching base 6% → post-RL 22%; polygon rotation base 13% → ≈83% (reported as +≈70 pp). Qwen2.5-Math-7B showed average post-RL gains of +51 percentage points on ID problems and +24 percentage points on OOD problems in explorative settings.</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>Primarily 2-skill compositions (pairwise compositions); no experiments reported with systematically deeper (>2) explicit composition chains.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Compositional settings comprised 7 compositional categories (each combining two distinct skill families) with templated generators; training included multiple diverse parameterizations per primitive (total training sets typically 1,000 problems, split 500/500 for pairwise compositional training).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td>When evaluated on the isolated (trained) primitive families (ID), RL often produced high accuracies (examples: >69% on many primitives; polygon rotation improved from 13% baseline to ≈83% after RL; pattern matching improved from 6% → 22%). Qwen2.5-Math-7B average ID gains after RL: +51 percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Performance on novel compositions (composing trained primitives but not seen jointly) remained poor: typical improvements were small (often +0–+6 percentage points), with many composed OOD tasks remaining near 0% accuracy in the hardest transformative/compositional tests. Example: combining GCD and polynomial-root reasoning showed no improvement after RL in at least one reported case.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td>Across explorative settings: average post-RL ID gain +51 pp vs OOD gain +24 pp → implied generalization gap of ≈27 pp between ID and OOD gains; for compositional settings, gap between primitive mastery (>69% on trained primitives) and composed OOD tasks (≈0–15%) frequently exceeded ~50–60 percentage points (varies by setting).</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td>Evidence indicates a sharp, non-linear degradation when moving from isolated skills to composed tasks: pairwise composition causes large drops (often catastrophic) in performance despite strong primitive mastery; complexity scaling for explorative tasks showed plateauing RL gains and abrupt failures at higher combinatorial complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>No explicit curriculum-vs-baseline (random ordering) controlled comparison was run; ablation on training complexity (training on levels 1-2 vs 1-4) showed little to no transfer to the hardest levels (e.g., arithmetic GCD level 5 remained at 3% despite broader complexity in training).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td>Reported: RL frequently reinforced brittle heuristics and specific patterns within each skill domain rather than learning flexible compositional strategies; optimization sometimes entrenched suboptimal shortcuts (evidence: RL reversed prior OOD gains in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>RL (GRPO) substantially improves performance on isolated, in-distribution primitives (large ID gains and moderate OOD gains for explorative tasks), but these improvements do not translate into reliable compositional generalization: models trained on primitives separately often fail to solve novel compositions of those primitives (typical OOD composition accuracy ≈0–15%). RL can also cause negative transfer (e.g., matrix-rank OOD performance dropped ~30 pp after RL), and expanding the training complexity range yields only modest improvements on higher-complexity targets.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports - provides strong empirical evidence for a compositional generalization gap: primitive mastery (even when substantially improved by RL) does not guarantee ability to compose skills, supporting theories that identify a compositional generalization gap and that simple fine-tuning/curriculum-free RL is insufficient to close it.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2008.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2008.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Compositional Ablation Study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation Study on Skill-Pair Composition and Training Compatibility</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Targeted ablations that systematically replace components of trained skill pairs to test whether compositional gains depend on specific pairings; measures post-RL improvement sensitivity to replacing one or both primitives in the joint training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>Compositional generalization within mathematical reasoning (OMEGA compositional settings)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>Qwen2.5-7B-Instruct (RL fine-tuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>Training jointly on two primitive families (original pairing) versus altered pairings; no staged/adaptive curriculum; ablation replaced one or both primitives while keeping training budget similar.</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td>Original joint training: both skill A and skill B included (500 samples each); ablations replaced one primitive with a nearby alternative (same sample budget) or replaced both components. Reported best gains occurred with original semantically-aligned pairings.</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>Pairwise compositions (2 primitives) only.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Each compositional setting included multiple templated variations for each primitive family; ablations tested nearby alternative skill families to probe alignment dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td>Original aligned pairings produced the largest post-RL compositional gains reported in ablations (+7.5 percentage points and +15 percentage points in two highlighted settings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>When one component was replaced by a nearby skill, gains dropped to modest +2–+5 percentage points; replacing both components typically removed or reversed gains (reported drops of -18 pp and -3 pp in some experiments). Overall composed-task performance remained low relative to trained-primitive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td>Original pairing: compositional OOD gain +7.5 pp and +15 pp in two settings; replacing one component reduced gains by ~5–13 pp; replacing both produced negative or zero net gain (e.g., -18 pp in one ablation). The effective gap between trained-primitive accuracy (>60–80%) and composed-task accuracy often ~50+ pp.</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td>No multi-depth scaling tested beyond pairwise; ablation results imply compositional success is highly sensitive to semantic alignment of primitives and does not scale favorably even for depth=2 when primitive compatibility is reduced.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>No formal curriculum baseline compared; ablations serve as internal controls showing dependency on the specific training composition rather than a general curriculum effect.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td>Findings are consistent with overfitting to co-occurrence patterns or domain-specific heuristics: only semantically-coherent skill pairings yielded post-RL compositional gains, suggesting models learn joint-distribution cues rather than robust composition operators.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Compositional gains after RL depend strongly on the semantic alignment of primitive pairs: original, coherent pairings yielded modest compositional improvements, while replacing one or both primitives dramatically reduced or negated gains. This indicates that RL-facilitated composition is fragile and contingent on joint training distribution coherence rather than a general emergent composition capability.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>nuances - provides nuance to compositional generalization theory: while primitive mastery is necessary, it is not sufficient; successful composition after RL additionally requires compatible, semantically aligned joint training distributions, suggesting structure in the training curriculum matters.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2008.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2008.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curriculum Scaffolding (Proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum Scaffolding (proposed future direction for improving compositional and transformative generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper proposes curriculum scaffolding—dynamically ordering tasks to gradually introduce compositional and transformative challenges alongside explorative ones—as a recommended future approach, but does not experiment with it.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>Proposed methodology for improved generalization in mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>The paper explicitly recommends investigating curriculum scaffolding (dynamically ordered tasks) and meta-reasoning controllers as future work to help bridge the gap between primitive mastery and true compositional/transformative generalization, but presents no experimental data for these approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mention - suggests curriculum learning as a promising remediation to the observed compositional gap but provides no direct evidence in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can models learn skill composition from examples? <em>(Rating: 2)</em></li>
                <li>Compositional generalization by learning analytical expressions <em>(Rating: 2)</em></li>
                <li>Measuring compositional generalization: A comprehensive method on realistic data <em>(Rating: 2)</em></li>
                <li>Faith and fate: Limits of transformers on compositionality <em>(Rating: 1)</em></li>
                <li>GSM-Infinite: How do your LLMs behave over infinitely increasing context length and reasoning complexity? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2008",
    "paper_id": "paper-280000246",
    "extraction_schema_id": "extraction-schema-47",
    "extracted_data": [
        {
            "name_short": "RL Generalization Experiments (Qwen2.5)",
            "name_full": "Reinforcement Learning Generalization Experiments with Qwen2.5-7B-Instruct / Qwen2.5-Math-7B",
            "brief_description": "Systematic RL (GRPO) fine-tuning experiments evaluating exploratory, compositional, and transformative generalization on the OMEGA benchmark using Qwen2.5 models; measures in-distribution (ID) and out-of-distribution (OOD) performance across complexity levels and skill compositions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "Mathematical reasoning tasks (OMEGA benchmark: arithmetic, algebra, combinatorics, number theory, geometry, logic & puzzles)",
            "agent_or_model_name": "Qwen2.5-7B-Instruct and Qwen2.5-Math-7B (base models for RL fine-tuning)",
            "curriculum_structure": "No explicit multi-stage curriculum was deployed; training data consisted of primitive/problem-family instances limited to low complexity (typically complexity levels 1-2). For compositional settings, training used primitives separately (500 samples per family) and excluded composite tasks; variants included expanding training complexity to levels 1-4 to test transfer.",
            "primitive_training_details": "Primitives were trained separately per problem family (compositional runs: 500 samples per skill family; general RL training: 1k problems total). RL reliably improved primitive (isolated skill) accuracy — examples: pattern-matching base 6% → post-RL 22%; polygon rotation base 13% → ≈83% (reported as +≈70 pp). Qwen2.5-Math-7B showed average post-RL gains of +51 percentage points on ID problems and +24 percentage points on OOD problems in explorative settings.",
            "composition_depth_range": "Primarily 2-skill compositions (pairwise compositions); no experiments reported with systematically deeper (&gt;2) explicit composition chains.",
            "compositional_diversity_description": "Compositional settings comprised 7 compositional categories (each combining two distinct skill families) with templated generators; training included multiple diverse parameterizations per primitive (total training sets typically 1,000 problems, split 500/500 for pairwise compositional training).",
            "performance_trained_compositions": "When evaluated on the isolated (trained) primitive families (ID), RL often produced high accuracies (examples: &gt;69% on many primitives; polygon rotation improved from 13% baseline to ≈83% after RL; pattern matching improved from 6% → 22%). Qwen2.5-Math-7B average ID gains after RL: +51 percentage points.",
            "performance_novel_compositions": "Performance on novel compositions (composing trained primitives but not seen jointly) remained poor: typical improvements were small (often +0–+6 percentage points), with many composed OOD tasks remaining near 0% accuracy in the hardest transformative/compositional tests. Example: combining GCD and polynomial-root reasoning showed no improvement after RL in at least one reported case.",
            "generalization_gap_measured": true,
            "generalization_gap_value": "Across explorative settings: average post-RL ID gain +51 pp vs OOD gain +24 pp → implied generalization gap of ≈27 pp between ID and OOD gains; for compositional settings, gap between primitive mastery (&gt;69% on trained primitives) and composed OOD tasks (≈0–15%) frequently exceeded ~50–60 percentage points (varies by setting).",
            "composition_depth_scaling": "Evidence indicates a sharp, non-linear degradation when moving from isolated skills to composed tasks: pairwise composition causes large drops (often catastrophic) in performance despite strong primitive mastery; complexity scaling for explorative tasks showed plateauing RL gains and abrupt failures at higher combinatorial complexity.",
            "curriculum_vs_baseline_comparison": "No explicit curriculum-vs-baseline (random ordering) controlled comparison was run; ablation on training complexity (training on levels 1-2 vs 1-4) showed little to no transfer to the hardest levels (e.g., arithmetic GCD level 5 remained at 3% despite broader complexity in training).",
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": "Reported: RL frequently reinforced brittle heuristics and specific patterns within each skill domain rather than learning flexible compositional strategies; optimization sometimes entrenched suboptimal shortcuts (evidence: RL reversed prior OOD gains in some settings).",
            "negative_transfer_observed": true,
            "key_findings_summary": "RL (GRPO) substantially improves performance on isolated, in-distribution primitives (large ID gains and moderate OOD gains for explorative tasks), but these improvements do not translate into reliable compositional generalization: models trained on primitives separately often fail to solve novel compositions of those primitives (typical OOD composition accuracy ≈0–15%). RL can also cause negative transfer (e.g., matrix-rank OOD performance dropped ~30 pp after RL), and expanding the training complexity range yields only modest improvements on higher-complexity targets.",
            "supports_or_challenges_theory": "supports - provides strong empirical evidence for a compositional generalization gap: primitive mastery (even when substantially improved by RL) does not guarantee ability to compose skills, supporting theories that identify a compositional generalization gap and that simple fine-tuning/curriculum-free RL is insufficient to close it.",
            "uuid": "e2008.0"
        },
        {
            "name_short": "Compositional Ablation Study",
            "name_full": "Ablation Study on Skill-Pair Composition and Training Compatibility",
            "brief_description": "Targeted ablations that systematically replace components of trained skill pairs to test whether compositional gains depend on specific pairings; measures post-RL improvement sensitivity to replacing one or both primitives in the joint training distribution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "Compositional generalization within mathematical reasoning (OMEGA compositional settings)",
            "agent_or_model_name": "Qwen2.5-7B-Instruct (RL fine-tuned variants)",
            "curriculum_structure": "Training jointly on two primitive families (original pairing) versus altered pairings; no staged/adaptive curriculum; ablation replaced one or both primitives while keeping training budget similar.",
            "primitive_training_details": "Original joint training: both skill A and skill B included (500 samples each); ablations replaced one primitive with a nearby alternative (same sample budget) or replaced both components. Reported best gains occurred with original semantically-aligned pairings.",
            "composition_depth_range": "Pairwise compositions (2 primitives) only.",
            "compositional_diversity_description": "Each compositional setting included multiple templated variations for each primitive family; ablations tested nearby alternative skill families to probe alignment dependence.",
            "performance_trained_compositions": "Original aligned pairings produced the largest post-RL compositional gains reported in ablations (+7.5 percentage points and +15 percentage points in two highlighted settings).",
            "performance_novel_compositions": "When one component was replaced by a nearby skill, gains dropped to modest +2–+5 percentage points; replacing both components typically removed or reversed gains (reported drops of -18 pp and -3 pp in some experiments). Overall composed-task performance remained low relative to trained-primitive performance.",
            "generalization_gap_measured": true,
            "generalization_gap_value": "Original pairing: compositional OOD gain +7.5 pp and +15 pp in two settings; replacing one component reduced gains by ~5–13 pp; replacing both produced negative or zero net gain (e.g., -18 pp in one ablation). The effective gap between trained-primitive accuracy (&gt;60–80%) and composed-task accuracy often ~50+ pp.",
            "composition_depth_scaling": "No multi-depth scaling tested beyond pairwise; ablation results imply compositional success is highly sensitive to semantic alignment of primitives and does not scale favorably even for depth=2 when primitive compatibility is reduced.",
            "curriculum_vs_baseline_comparison": "No formal curriculum baseline compared; ablations serve as internal controls showing dependency on the specific training composition rather than a general curriculum effect.",
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": "Findings are consistent with overfitting to co-occurrence patterns or domain-specific heuristics: only semantically-coherent skill pairings yielded post-RL compositional gains, suggesting models learn joint-distribution cues rather than robust composition operators.",
            "negative_transfer_observed": null,
            "key_findings_summary": "Compositional gains after RL depend strongly on the semantic alignment of primitive pairs: original, coherent pairings yielded modest compositional improvements, while replacing one or both primitives dramatically reduced or negated gains. This indicates that RL-facilitated composition is fragile and contingent on joint training distribution coherence rather than a general emergent composition capability.",
            "supports_or_challenges_theory": "nuances - provides nuance to compositional generalization theory: while primitive mastery is necessary, it is not sufficient; successful composition after RL additionally requires compatible, semantically aligned joint training distributions, suggesting structure in the training curriculum matters.",
            "uuid": "e2008.1"
        },
        {
            "name_short": "Curriculum Scaffolding (Proposed)",
            "name_full": "Curriculum Scaffolding (proposed future direction for improving compositional and transformative generalization)",
            "brief_description": "Paper proposes curriculum scaffolding—dynamically ordering tasks to gradually introduce compositional and transformative challenges alongside explorative ones—as a recommended future approach, but does not experiment with it.",
            "citation_title": "",
            "mention_or_use": "mention",
            "study_domain": "Proposed methodology for improved generalization in mathematical reasoning",
            "agent_or_model_name": "",
            "curriculum_structure": null,
            "primitive_training_details": null,
            "composition_depth_range": null,
            "compositional_diversity_description": null,
            "performance_trained_compositions": null,
            "performance_novel_compositions": null,
            "generalization_gap_measured": null,
            "generalization_gap_value": null,
            "composition_depth_scaling": null,
            "curriculum_vs_baseline_comparison": null,
            "adaptive_curriculum_used": null,
            "spurious_correlation_effects": null,
            "negative_transfer_observed": null,
            "key_findings_summary": "The paper explicitly recommends investigating curriculum scaffolding (dynamically ordered tasks) and meta-reasoning controllers as future work to help bridge the gap between primitive mastery and true compositional/transformative generalization, but presents no experimental data for these approaches.",
            "supports_or_challenges_theory": "mention - suggests curriculum learning as a promising remediation to the observed compositional gap but provides no direct evidence in this paper.",
            "uuid": "e2008.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can models learn skill composition from examples?",
            "rating": 2
        },
        {
            "paper_title": "Compositional generalization by learning analytical expressions",
            "rating": 2
        },
        {
            "paper_title": "Measuring compositional generalization: A comprehensive method on realistic data",
            "rating": 2
        },
        {
            "paper_title": "Faith and fate: Limits of transformers on compositionality",
            "rating": 1
        },
        {
            "paper_title": "GSM-Infinite: How do your LLMs behave over infinitely increasing context length and reasoning complexity?",
            "rating": 1
        }
    ],
    "cost": 0.01521275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization
23 Jun 2025</p>
<p>Yiyou Sun 
University of California
Berkeley</p>
<p>Shawn Hu 
Georgia Zhou 
University of California
Berkeley</p>
<p>Ken Zheng 
University of California
Berkeley</p>
<p>Hannaneh Hajishirzi 
Ai2</p>
<p>University of Washington</p>
<p>Nouha Dziri 
Ai2</p>
<p>Dawn Song 
University of California
Berkeley</p>
<p>OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization
23 Jun 20253FCB283892AB39008C16652BB065A62AarXiv:2506.18880v1[cs.CL]
Recent large-scale language models (LLMs) with long Chain-of-Thought reasoning-such as DeepSeek-R1-have achieved impressive results on Olympiad-level mathematics benchmarks.However, they often rely on a narrow set of strategies and struggle with problems that require a novel way of thinking[32].To systematically investigate these limitations, we introduce OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a controlled yet diverse benchmark designed to evaluate three axes of out-of-distribution generalization, inspired by Boden's typology of creativity [4]: (1) Exploratory-applying known problemsolving skills to more complex instances within the same problem domain; (2) Compositional-combining distinct reasoning skills, previously learned in isolation, to solve novel problems that require integrating these skills in new and coherent ways; and (3) Transformative-adopting novel, often unconventional strategies by moving beyond familiar approaches to solve problems more effectively.OMEGA consists of programmatically generated training-test pairs derived from templated problem generators across geometry, number theory, algebra, combinatorics, logic, and puzzles, with solutions verified using symbolic, numerical, or graphical methods.We evaluate frontier (or top-tier) LLMs and observe sharp performance degradation as problem complexity increases.Moreover, we fine-tune the Qwen-series models across all generalization settings and observe notable improvements in exploratory generalization, while compositional generalization remains limited and transformative reasoning shows little to no improvement.By isolating and quantifying these fine-grained failures, OMEGA lays the groundwork for advancing LLMs toward genuine mathematical creativity beyond mechanical proficiency.Our code and dataset is available at https://github.com/sunblaze-ucb/math_ood.* indicates the equal advising role.</p>
<p>Introduction</p>
<p>Large-scale language models (LLMs) with long Chain-of-Thought (CoT) reasoning-such as DeepSeek-R1 [12], OpenAI-o4 [29], and Claude-Sonnet [34]-have recently achieved impressive results on Olympiad-level mathematics benchmarks, fueling optimism that general-purpose LLMs reasoners may soon rival skilled human problem-solvers.However, recent studies reveal that models trained via Supervised Fine-Tuning (SFT) [32] or Reinforcement Learning (RL) [38] often rely on a limited set of strategies-for instance, such as repeating familiar algebra rules or defaulting to coordinate geometry in diagram problems.And thus, they tend to struggle with particularly challenging problems that require novel insights [32].Bridging the gap between following learned reasoning patters and demonstrating true mathematical creativity remains a critical open challenge.</p>
<p>Hint: count subsets with size n can be formed by 3 letters, then subtract the overcounts where any letter exceeds n-1.</p>
<p>(a) Explorative Generalization (more complexity)</p>
<p>Find the number of rectangles that can be formed inside a fixed regular octagon where each side of the rectangle lies on either a side or a diagonal of the dodecagon.</p>
<p>Find the number of rectangles that can be formed inside a fixed regular dodecagon where each side of the rectangle lies on either a side or a diagonal of the dodecagon.</p>
<p>octagon dodecagon</p>
<p>Training Problems Test Problems</p>
<p>What is the highest common factor of 21385 and 2961?</p>
<p>$f(x) = -36x^3 + 272x^2 -412^x -240$.Find real roots of f(x).</p>
<p>Training Problems</p>
<p>Find the number of rectangles that can be formed inside a fixed regular dodecagon where each side of the rectangle lies on either a side or a diagonal of the dodecagon.</p>
<p>Find the greatest common divisor of the two polynomials $f(x) = x^5 + x^3 -x^2 -1$ and $g(x) = x^4 -3x^2 -4$.</p>
<p>Test Problems</p>
<p>(b) Compositional Generalization (c) Transformative Generalization (shifted thinking mode)</p>
<p>How many distinct words of length 5 can be formed from {"c": 3, "a": 2, "t": 2}? (value indicates the number of letter)</p>
<p>How many distinct words of length n can be formed from {"c": n-1, "a": n-1, "b": n-1}?(value indicates the number of letter)</p>
<p>Hint: Use case analysis based on times c appears in the word (|c| = 3, 2, 1).For each case, list all combinations and sum the permutations.</p>
<p>Training Problems Test Problems</p>
<p>(composed strategy)</p>
<p>GCD of integers + Factorize polynomial Generalization increases complexity within the same frame of thinking (e.g., extending geometric reasoning from an octagon to a dodecagon).(b) Compositional Generalization requires integrating multiple learned strategies (e.g., combining GCD and root-finding for polynomials).(c) Transformative Generalization demands a shift in thinking mode (e.g., from fixed-case enumeration to a "clever" solution that requires thinking in a reverse way).</p>
<p>While fully addressing this gap is an ongoing research effort, our works aims to offer novel insights into the generalization limits of frontier LLMs in mathematical reasoning, cutting through the noise to identify what these models can and cannot do.</p>
<p>Existing math datasets are poorly suited for analyzing math skills that RL models can learn.Largescale corpora such as Numina-Math [20], Omni-Math [11], and DeepMath [14] blend a large number of math questions in different topics and complexity levels, making it hard to isolate specific reasoning skills behind a model's success or failure.On the other hand, controlled datasets like GSM-Symbolic [26], GSM-PLUS [22], and GSM-Infinite [42] focus on narrow domains, making the diversity of reasoning problems limited.Earlier resources like the DeepMind Mathematics suite [2] provide synthetic problems spanning broader topics, but were tailored for earlier-generation models and emphasize elementary tasks (e.g., base conversion), which are far below Olympiad-level complexity.As a result, current benchmarks are either too coarse for causal analysis or insufficiently challenging for modern LLMs.We provide a more detailed comparison in Table 1.</p>
<p>To address this gap, we introduce OMEGA, a controlled, yet diverse benchmark designed to probe three axes of Out-of-Distribution (OOD) generalization, inspired by Boden's typology of creativity [4].For each axis, we construct matched training-test pairs that isolate a specific reasoning capability (Figure 1) that span 3 dimensions: (1) Exploratory-assessing whether models can apply known problem-solving skills to more complex instances within the same problem domain.For example, counting rectangles in an octagon (train) versus a dodecagon (test) (Figure 1a); (2) Compositional-evaluating their ability to combine distinct reasoning skills, previously learned in isolation, to solve novel problems that require integrating these skills in new and coherent ways, e.g, finding the GCD of polynomials followed by root-solving (Figure 1b); and (3) Transformative-testing whether models can adopt unconventional strategies by moving beyond familiar approaches to solve problems more effectively.For instance, one can replace brute-force enumeration with a subtractive counting method that overcounts and then removes invalid cases (Figure 1c).</p>
<p>OMEGA's test and train problems are constructed using carefully engineered templates that provide precise control over diversity, complexity, and the specific reasoning strategies required for solutions.</p>
<p>Our framework employs 40 templated problem generators across six mathematical domains: arithmetic, algebra, combinatorics, number theory, geometry, and logic &amp; puzzles, with complexity levels aligned to Olympiad-level problems.All problems are programmatically generated from problem templates, with answers computed via symbolic, numerical, or graphical methods.Each template encodes a distinct reasoning strategy, enabling systematic generalization studies and the construction of compound problems by combining multiple templates.DISTRIBUTION NOTES AIME [3] Human Human High ✗ ✗ 30 Questions per year.GSM8K [6] Human Human Low ✗ ✓ Primitive math-word problems.GSM-Symbolic [26] Program Program Low ✓ ✓ Perturbed math-word problems.</p>
<p>GSM-Infinite [42] Program Program Arbitrary ✓ ✓ Infinitely generable math-word problems.MATH500 [30] Human Human Low ✓ ✗ METAMATH [8] Human/LLM Human/LLM Low ✗ ✗ Based on GSM8K/MATH.BIGMATH [1] Human Human/Filters High ✗ ✗ A mix of many datasets.MATHSCALEQA [33] LLM LLM Low ✗ ✓ 2M generated datapoints.</p>
<p>OPENMATH-INSTRUCT [35] Human/LLM Human/Code/LLM Low ✗ ✗ 1.8M solutions to 14K problems from MATH / GSM8K.DEEPMATH [14] Human  Our empirical results reveal three primary findings about current reasoning models: a) Performance degradation in scaling up complexity.As mathematical task complexity increases, frontier models' performance deteriorates to near-zero despite substantial inference-time compute.The CoT analysis highlights several key insights: (i) models often discover correct solutions early but expend excessive tokens on verification, leading to inefficient computation, and (ii) models frequently fall into error spirals due to overthinking and self-correction mechanisms, compounding early mistakes and abandoning correct reasoning pathways, (iii) lower accuracy on high-complexity problems can stem from the models' reluctance to perform tedious computations, rather than from arithmetic errors; b) Generalization of RL exhibits plateau gain.RL effectively improves model generalization from easy to medium-complexity mathematical problems, especially on familiar (in-domain) tasks, but struggles to achieve significant gains on higher-complexity problems.Performance boosts vary significantly across domains, highlighting the importance of domain-specific knowledge and complexity.c) Struggle of skill integration and creative reasoning in LLMs.Unlike humans who fluidly integrate mastered skills, RL models trained on isolated skills struggle at compositional generalization, and models trained conventionally deteriorate on problems necessitating unconventional thinking.These findings underscore crucial gaps between current LLM reasoning capabilities and the flexible, insightful problem-solving characteristic of human mathematicians, particularly in scenarios demanding genuine mathematical creativity beyond mere pattern recognition.</p>
<p>Beyond highlighting current limitations, we hope this study encourages the community to explore smarter scaling solutions rather than brute-force approaches.Although many of the identified failure cases could potentially be patched through targeted data augmentation or synthetic scaffolding, such short-term fixes may obscure deeper, structural weaknesses in model reasoning.Our objective is not only to expose these limitations, but also to inspire strategies that fundamentally equip models with robust, efficient mathematical reasoning capabilities that should address underlying issues that persist beyond simple dataset patches or model scaling.</p>
<p>OMEGA: Probing the Generalization Limits of LLMs in Math Reasoning</p>
<p>A central goal of mathematical reasoning is not merely to apply memorized procedures but to flexibly adapt, combine, and extend learned strategies.To assess the extent to which LLMs exhibit this capacity, we propose a typology of generalization inspired by Margaret Boden's framework for creativity in cognitive science [4].Specifically, we define three axes of reasoning generalization-exploratory, compositional, and transformative-to probe the limits of these models on controlled out-of-distribution (OOD) cases that range from easier extensions of seen patterns to harder, more unconventional reasoning problems.Assessing performance along these axes requires fine-grained control over the in-distribution training data.
(x) = 2(−3x + 4) 2 + (−3x + 4) + 3 , g(x) = 3x − 1 , x = 1.</p>
<p>Digit Sum</p>
<p>Let N be the 10th smallest 3-digit integer with digit sum divisible by 6 .Find N .</p>
<p>log 10 (answer)</p>
<p>Triple Count How many ordered triples (a, b, c) with a, b, c ≤ 3 2 satisfy −2a 3 − 2b 3 + 2c 3 ≡ 0 (mod 3 2 ) ?</p>
<p>log 10 (answer)</p>
<p>Prime Mod</p>
<p>Let p be the smallest prime for which n 6 + 2 ≡ 0 (mod p 5 ) has a solution; find the minimal n for this p.</p>
<p>log 10 (answer)</p>
<p>Logic &amp; Puzzles</p>
<p>Grid Blocked</p>
<p>In a 4x4 grid, how many different paths are there from the bottom left (0, 0) to the top right (3,3), if you can only move right or up at each step, subject to the constraint that you cannot move through the following cells:</p>
<p>(3, 1), (2, 3), (0, 1), (2, 1) ?grid size</p>
<p>Problem Construction</p>
<p>Training on a heterogeneous mix of unrelated problems obscures the source of generalization.In contrast, restricting training data to instances drawn from a single template ensures that the model learns a well-scoped strategy.2. These problems are calibrated at the knowledge level comparable to the American Invitational Mathematics Examination (AIME) [3], with many serving as crucial sub-components in solving Olympiad-level problems.For instance, the function_intersection problem type represents an essential building block for questions requiring advanced function analysis.</p>
<p>The selection of problem templates involved several critical considerations:</p>
<p>• Single-scope with meaningful variations.Each problem template is designed to focus on a single-scope mathematical strategy while allowing for substantial variations.By single-scope, we mean that the required solution approach is confined within a well-defined framework, enabling controlled studies of specific reasoning patterns.For instance, instead of combining multiple geometric shapes in a single problem generation template, we isolate problem families on different shapes independently.At the same time, we ensure meaningful variation by designing parameters that fundamentally alter solution trajectories when modified.This contrasts with datasets (numerical perturbation) like GSM-PLUS [22], where varying numerical values often preserve the underlying solution path without introducing new reasoning challenges.</p>
<p>• Programmatic generation and solution validation.To ensure scalability, both problem instances and their solutions are programmatically generated.This requirement significantly influenced template selection, especially for geometry problems that demand sophisticated procedural generation.We employed diverse computational methods for solution validation: grid search algorithms for function_intersection problems, exhaustive enumeration for combinatorial tasks, and computer vision techniques-such as cv2.approxPolyDP from OpenCV-to accurately count polygons in rotation problems.</p>
<p>Training and Evaluation Setup for Generalization</p>
<p>Let T = {τ } denote a collection of problem templates, where each template τ defines a family of problem instances P τ = { x τ,θ | θ ∈ Θ τ }, parameterized by a complexity vector θ within a parameter space Θ τ .We define a scalar complexity measure δ : Θ τ → Z + that ranks problems by increasing complexity.For each generalization axis-such as exploratory, compositional, or transformative-we specify a training set by selecting a collection of templates along with particular regions of their parameter spaces.Similarly, a distinct set of templates and parameter regions is chosen for testing separately, depending on the different generalization test settings.For each generalization category and each math domain, we construct: 1) training data, 2) In-distribution (ID) test data, and 3) OOD test data.</p>
<p>Exploratory Generalization</p>
<p>Exploratory generalization assesses whether a model can faithfully extend a single reasoning strategy beyond the range of complexities seen during training.Concretely, the model is exposed to problems drawn from one template τ , all lying within a "low-complexity" regime, and is then evaluated on harder instances from the same family.This axis probes robustness: does the model generalizes the same algorithm to higher complexity problems?or does it merely memorize solutions at a fixed complexity level?</p>
<p>Training and testing data construction.we define a cutoff threshold δ 0 based on a task-specific complexity measure δ, which determines the maximum complexity level included in training.All problem instances with δ ≤ δ 0 are used for training, while those with δ &gt; δ 0 are reserved for testing.</p>
<p>To ensure the setting remains sufficiently challenging, we select δ 0 such that the base model achieves under 50% accuracy on the training data-reflecting the inherent complexity of these reasoning tasks and leaving room for improvement through fine-tuning.All problem templates introduced in Section 2 are suitable for exploratory generalization experiments, as they encompass scalable reasoning tasks.</p>
<p>For each template, we ensure that the complexity scaling aligns with the mathematical intuition of the task, such that increasing δ genuinely demands more sophisticated reasoning steps.</p>
<p>Compositional Generalization</p>
<p>Compositional generalization probes a model's ability to integrate multiple, distinct reasoning strategies.Unlike explorative generalization, which scales a known method to larger instances, compositional generalization requires a fusion of sub-skills synergistically.Figure 2 illustrates two such cases, where solving the target problem hinges on combining finite-case enumeration with piecewise reasoning or geometric layout analysis with nested-pattern counting.Overall, compositional generalization offers a controlled framework for assessing whether a model can go beyond mastering individual reasoning patterns to dynamically combine them-thereby distinguishing shallow, rote learning from genuine skill integration and true task understanding.</p>
<p>To curate meaningful compositional settings, we enforce the following principles: First, cohesive skill integration where the compositional train problems should require true synthesis of multiple reasoning skills rather than superficial concatenation.This ensures that solving the problem depends on the synergistic application of sub-skills, not merely applying them in sequence.Second, complete skill coverage where each reasoning skill involved in the composed test task should be independently represented in the training set.This ensures that success on the test reflects the model's ability to compose familiar strategies, rather than rely on exposure to novel ones.And lastly, nontrivial complexity of train problems where train problem should be sufficiently challenging so that the model actually learns each sub-skill, making any compositional gains observable.The training problems from our templated inventory remain challenging to the base model, even at low complexity levels (1)(2).</p>
<p>Training and testing data construction.Our compositional dataset is structured around seven categories (details in Appendix §A.2), each designed to probe specific combinations of reasoning skills.Within each problem family, we identify a core skill and construct corresponding training examples that isolate and reinforce this skill.To evaluate compositional generalization, we then design test problems that require the synergistic application of two distinct skills-such that the solution cannot be obtained by applying each skill naively, but instead demands their true integration.For instance, as illustrated in Figure 2, one problem family focuses on interpreting polygonal geometry, while another targets counting nested patterns; their composition results in a task that requires counting nested structures within polygons.Each setting includes multiple training instances for individual skills and corresponding test instances that assess the model's ability to combine them effectively.Representative examples are provided in Table 7 and Table 8, with additional information in Appendix A.2.</p>
<p>Transformative Generalization</p>
<p>Transformative generalization poses the greatest challenge: it asks whether a model can abandon a familiar but ultimately ineffective strategy in favor of a qualitatively different and more efficient one.These tasks lie outside the scope of mere extension or composition; they require a "jump out of the box"-a creative reframing that circumvents the limitations of standard tactics.To curate Find the number of rectangles that can be formed inside a fixed regular dodecagon ($12$-gon) where each side of the rectangle lies on either a side or a diagonal of the dodecagon.</p>
<p>Finite-case enumeration</p>
<p>Piecewise function reasoning</p>
<p>In a regular octagon labeled 1-8, draw diagonals from 5 to 3 and from 2 to 7. Rotate the figure 7 steps counterclockwise and overlap it with the original.How many smallest triangular regions are formed?</p>
<p>Geometric layout analysis in polygon</p>
<p>When randomly selecting 4 letters from the multiset {y: 2, f: 3, g: 1} to form a word, what is the expected number of matches of the pattern 'f.*f'?</p>
<p>Counting for nested patterns</p>
<p>Counting for nested rectangles in polygon Finite-case enumeration for parametric functions.</p>
<p>Source Skill Target Skill Test Problems Train Problems
POLYNOMIAL ROOTS • Problem. Solve f (x) = −36x 3 + 272x 2 − 412x − 240. • Tactic learned.
Apply the Rational Root Theorem (enumerate p/q with p | 240, q | 36), test candidates via synthetic division, then factor the cubic.
• Problem. Solve f (x) = x 5 + 10x 3 + 20x − 4. • Needed insight. Substitute x = t + a t to exploit
symmetry, reduce to a quadratic in t 2 , then recover x.
• Problem. With f (x) = |x| − 1 2 and g(x) = |x| − 1 4 , find intersections of y = 4g(f (sin 2πx)), x = 4g(f (cos 3πy)).
• Needed insight.Avoid exhaustive casework;</p>
<p>instead, analyze how "up" and "down" graph segments multiply and intersect, using visual symmetry 2 for efficient counting.Training and testing data construction.Our transformative dataset comprises seven categories (detailed in Appendix §A.3), each specifically designed to evaluate a model's capacity to adopt novel problem-solving approaches.Within each category, training problems are generated from the templates described in Section 2. These training tasks can typically be solved using conventional reasoning strategies of moderate complexity, ensuring that the model thoroughly acquires foundational skills.Conversely, the corresponding test problems are intentionally constructed to render these familiar methods ineffective, compelling the model to devise and employ qualitatively distinct solutions.For instance, as illustrated in</p>
<p>Experiments</p>
<p>Limits of Reasoning Language Models on Increasing Problem Complexity</p>
<p>We evaluate four frontier models-DeepSeek-R1, Claude-3.7-Sonnet,OpenAI-o3-mini and OpenAI-o4-mini3 -across different complexity levels, measuring exact-match accuracy on a heldout set of 100 samples per complexity level.Detailed experimental setup and complexity level descriptions are provided in Appendix B. As the complexity increases, performance degrades and goes to zero.We provide complexity analysis to typical problems to ensure they are within the models' output length as detailed in §D.Chain-of-Thought reasoning patterns analysis.</p>
<p>Reasoning LLMs performance degrades with increasing problem complexity</p>
<p>we observed several key patterns: i) early solution discovery followed by excessive verification: CoT traces in correct answers reveal that models often reach correct solutions relatively early in their responses but then spend substantial additional tokens on verification and double-checking.The yellow "overthinking" regions in Figure 5 show this postsolution elaboration, which remains consistent across most domains, though it can increase with task complexity (e.g., in algebra, up to 3k extra tokens spent on verification) even when the answer has already been found.Spending more tokens to verify an answer can be beneficial, but models must be cautious as excessive elaboration may introduces unnecessary steps, increases compute cost, and can destabilize otherwise correct reasoning.ii) overthinking leads to spiral loops of errors: we noticed that incorrect responses consistently consume more tokens than correct ones across all complexity levels.Response length initially increases with problem complexity, but then drops for some tasks at the highest levels and models abandon systematic reasoning when prob- lems become intractable.To understand the types of reasoning failures models exhibit, we identified two dominant patterns (Figure 4).The first is the correct → incorrect shift, where models initially arrive at the correct answer but then second-guess themselves and revise toward an incorrect one (∼38% of incorrect responses at complexity 1, with similar trends across higher levels).The second is reasoning spirals (wrong → wrong), in which models never reach the correct answer and instead cycle through multiple flawed reasoning paths, making repeated errors without converging.This reveals that CoT with self-correction and backtracking, although significantly beneficial, is not sufficient to counter the snowballing of errors-transformers' autoregressive nature still compounds early mistakes, and CoT overthinking can paradoxically lead models to abandon the correct branch and answer, causing them to fall into spirals of errors.Earlier we observed a steady decline in solution accuracy as the complexity level of our benchmarks rises.A plausible explanation is that harder problems require longer numeric derivations which amplifies the chance of arithmetic slips [32].To disentangle cause from correlation, we zoom in on the Matrix Rank family, whose solution path (Gaussian elimination) is mostly deterministic and whose intermediate results can be easily verified.For every DeepSeek-R1's trajectory that produced an incorrect final answer, we segmented the CoT at each line break and asked O4-mini to label each segment as (i) a conjecture-a speculative statement about the final answer, (ii) a computation-an explicit algebraic or numeric operation, or (iii) other.When a segment was tagged as a computation, we further checked whether its arithmetic was correct.We provide the prompt details in Appendix §B.2.Paradoxically, when the model does compute, it does so more reliably at higher levels which suggests that arithmetic precision may not be the only bottleneck.</p>
<p>Collectively, these patterns show that the accuracy loss at higher complexity can be not only driven by cascading numerical mistakes, but also by the model's reluctance to invest reasoning budget in systematic calculation.Mitigating this issue may therefore require steering mechanisms that incentivize faithful computation rather than merely improving arithmetic skill.To investigate how inference-time compute contributes to solving difficult math problems, we scale the number of candidates at inference time from 1 to 32 for the advanced LLMs and report Pass@k across six graded complexity levels.Figure 7 shows results for the "letter distribution" problems (we provide more problems in Figure 13 in Appendix §C).Results show that increasing the search space improves performance, gradually approaching 100% when the problem complexity is low.However, as the complexity increases, the benefit diminishes, and performance drops to zero at complexity level 6.Notably, this failure is not due to context length limitations-when solving this problem with dynamic programming, the state space remains within the LLM's context window.For example, the level 6 question example in Figure 7 only takes 36 unique states in using a DP solver.This abrupt failure underlines how a seemingly modest increase in combinatorial load can overwhelm current reasoning LLMs, highlighting that increasing the search space cannot necessarily mitigate the fundamental limits of transformers.While brute force helps, there must be smarter scaling approaches so that models learn the underlying algorithms and skills to solve math problems rather than simply relying on increased compute.Due to budget constraints, we limited testing to 64 attempts, but given the zero performance, we speculate that increasing beyond this point would not help.</p>
<p>RL Generalization Experiments</p>
<p>Experimental Setup.We evaluate the impact of RL on the generalization capabilities of the base model, Qwen2.5-7B-Instruct and Qwen2.5-Math-7B 5 , across three distinct generalization paradigms: exploratory, compositional, and transformational.For each generalization type, we apply the GRPO algorithm on 1k training problems and evaluate on corresponding in-domain (ID) and out-of-distribution (OOD) test sets.For exploratory generalization, we train on problems with restricted complexity levels 1 and 2, then evaluate on: (i) ID problems from the same problem family and complexity range (δ ≤ δ 0 = 2), and (ii) OOD problems from the same problem type but with higher complexity (δ &gt; 2).Regarding compositional generalization, for each compositional category C = (S A , S B ), we train the model on problems that involve the individual skills S A and S B separately, but not their combination, then evaluate on: (i) ID problems testing each skill separately (P S A and P S B ), and (ii) OOD problems requiring the integrated composition of both skills (P S A ⊕S B ), where successful solution demands the synergistic combination rather than sequential application of the individual skills.</p>
<p>For transformational generalization, we train on problems with conventional solution approaches, then evaluate on: (i) ID problems solvable using familiar methods from the training distribution, and (ii) OOD problems that appear similar to training data but require unconventional solution strategies.OOD examples-all without any supervised fine-tuning.These significant improvements show that RL with reward-driven exploration alone can lead the model to uncover effective reasoning strategies, even in tasks that demand complex combinatorial reasoning.</p>
<p>However, this performance boost is not uniform across all domains.In geometry, for instance, the base model starts from an even lower baseline (under 15%), and RL training results in comparatively smaller gains (+31 pp on ID and just +8 pp on OOD examples).This disparity raises important questions about domain-specific learning dynamics.One explanation is that the inherent complexity and multimodal nature of geometry (e.g., spatial reasoning, diagram interpretation, and algebraic translation) make it harder for the model to discover effective strategies via reward signals, particularly if the model lacked sufficient pretraining exposure to such content.Recent works [39,27] show that spatial reasoning skills do not emerge automatically from standard training pipelines and require dedicated data and architectures to improve.This suggests that prior knowledge and domain familiarity play a significant role in determining how much RL can improve performance.</p>
<p>Understanding the extent to which domain complexity and prior exposure influence RL effectiveness remains an open question, and we leave a deeper investigation of this phenomenon to future work.</p>
<p>We further investigated whether training RL on a broader range of complexities-specifically levels 1 through 4 rather than only levels 1 and 2-would improve generalization to harder problems.Figure 9 presents these results.In the Arithmetic GCD domain, for example, training with RL on levels 1-2 raises the model's accuracy on level 3 from 6% to 10%, a modest +4 percentage-point gain.However, when we expand training to include levels 1-4 and evaluate on level 5, the accuracy remains stuck at 3%, identical to the un-trained baseline, despite exposure to higher-complexity tasks.Similarly, training only on levels 1-2 also fails to raise performance on level 5, which again stays at 3%.This suggests that RL alone may not be able to push performance beyond the model's base capabilities on level 5, where the base accuracy is already extremely low (3%).Recent work [24] suggests that RL is most effective when the base model initially struggles with a task; conversely, when the base model already exhibits strong performance, RL yields diminishing returns.In our experiments, however, we observe no uniform pattern across different problem types-RL improvements appear task-dependent rather than uniformly correlated with the base model's initial performance.</p>
<p>While RL fine-tuning consistently narrows the gap between in-distribution and OOD performance, our results provide initial insights that RL does not reliably equip the model with the underlying skills required to solve more complex tasks and to transfer reasoning capabilities from easy to hard problems.</p>
<p>Figure 9: Generalization across complexity levels.Models were trained with data up to a certain complexity level (y-axis) and evaluated on problems from levels 1 to 5 (x-axis).Cells marked 'ID' represent in-distribution evaluations where the test complexity level was included in the training set.Our results show that RL generalizes from easy to hard problems with a plateauing gain.We evaluate the model's ability to integrate two distinct reasoning skills learned separately during training to solve novel problems requiring their composition.For each compositional category C i = (S ai , S bi ), we train the model using RL on problems requiring skill S ai in isolation and skill S bi in isolation, but crucially exclude any training examples that require both skills simultaneously.The compositional test (OOD) then measures whether the model can synthesize these separatelyacquired capabilities to solve problems requiring S ai ⊕ S bi .This setup directly tests the fundamental question of whether RL can enable emergent compositional reasoning-the ability to combine known sub-procedures into novel, more complex reasoning strategies without explicit supervision on the composite task.Similarly, for pattern matching problems, the base model starts at 6%, with RL improving accuracy to 22%.This shows that RL is effective at reinforcing specific, isolated reasoning capabilities.It can help models internalize and reliably execute individual skills, even when the base model starts from very low accuracy.However, the magnitude of improvement varies significantly across settings-suggesting that not all skill types benefit equally from RL.Some skills may be easier to reinforce than others.Despite these gains, models struggle when tested on compositions of learned skills.Performance on such integrated tasks shows little to no improvement after RL, with only minor gains (e.g., +6%) in most cases.For example, when tested on problems that combine GCD and polynomial root reasoning, RL yields no improvement.This begs the question: if models have truly learned the underlying components, why can't they combine them effectively?We hypothesize that current RL approaches tend to overfit to specific patterns within each skill domain rather than learning flexible, generalizable OOD reasoning strategies.This contrasts sharply with human reasoning, where mathematicians readily compose learned sub-skills to solve novel problems-for example, in Perelman's proof of the Poincaré Conjecture, which integrates geometric flow theory, Ricci flow, and topological surgery.</p>
<p>Can RL Learn to Compose Math Skills into Integrated Solutions? Strong Performance on Isolated Skills, but Limited Compositional Generalization</p>
<p>To further probe this limitation, we conducted ablation experiments on the strongest-performing compositional settings.We retrained models while systematically altering the composition of skill pairs-replacing one or both components with nearby math skills alternatives.The original skill pairings led to the highest gains after RL fine-tuning (+7.5 pp and +15 pp) as shown in Table11 and</p>
<p>Related Work</p>
<p>OOD Generalization and Compositional Abilities of LLMs.Generalization to out-of-distribution (OOD) data remains a fundamental challenge in large language models (LLMs) and machine learning more broadly, with far-reaching implications for tasks such as mathematical reasoning, physical modeling, and financial forecasting [10,17,21,23,36,37].In practice, many key questions about model performance reduce to whether models can effectively handle test distributions that differ from their training data.Compositional generalization-models' ability to systematically combine learned skills-has also been a long-standing focus in language research [25,16,15,5,18,9].Much of this work has relied on controlled testbeds involving rule-based languages such as SQL or synthetically generated tasks.More recently, [41] extended this line of inquiry to natural language skills, while [7] examined whether LLMs can acquire compositional generalization through tasks like integer multiplication and dynamic programming.Building on this foundation, OMEGA offers a comprehensive benchmark for assessing compositional generalization in mathematical reasoning, spanning a broad range of problem types and solution strategies.</p>
<p>Benchmarking LLMs' Mathematical Abilities.The most common way to evaluate an LLM's math ability is by reporting accuracy on a large collection of questions.They are typically created in a few ways: by hiring humans to write problems (e.g., GSM8K [28], MinervaMath [19]), which allows control over topic and complexity but is costly; by collecting or adapting existing exam questions (e.g., AIME [3], OlympiadBench [13], GaoKao [40]), which ensures quality but limits scale and diversity; or by scraping exam corpora and filtering them with human (e.g., NuminaMath [20], BigMath [1]); or LLM-based verification (e.g., MathScale [33]).Another approach is to generate problems using LLMs with correctness constraints (e.g., MetaMathQA [8], OpenMathInstruct-1 [35]).Some works also modify existing datasets for specific goals, like GSM-Plus [22], GSM-Symbolic [26] and GSM-Infinite [42].Other typical datasets include Math500 [30], AIME [3], MinervaMath [19], and OlympiadBench [13].The detailed comparison of popular math benchmarks is in Table 1.In the recent study [31], evaluations on four synthetic puzzles indicate that LLMs encounter distinct reasoning boundaries as problem complexity escalates.These findings align with our observations presented in Section 3.1.1,where we examine a broader set of mathematical categories across multiple problem families.Beyond evaluating the performance limits of frontier models, our work further investigates the underexplored boundaries of reasoning generalization in LLMs-specifically through explorative, compositional, and transformative perspectives.</p>
<p>A Dataset Details</p>
<p>A.1 Details of Problem Families</p>
<p>To provide full transparency on our templated generators, we include three comprehensive tables in the appendix.Table 4 lists all arithmetic and algebra templates (e.g., linear equations, polynomial roots, function operations), alongside their complexity measures across five calibration levels.Table 5 details the combinatorics and number-theory generators with corresponding size or range metrics at each level.Finally, Table 6 presents our logic &amp; puzzles and geometry templates, again annotated with statement counts or grid sizes for the five levels.Together, these tables document the full set of 41 problem families used in MathOOD, illustrating how each template is systematically calibrated to enforce controlled, domain-specific reasoning strategies.combinatory/prob_gt_n_fixWhat is the probability that, when forming a 4-letter word from {'h' : 2, 'r' : 3, 'q' : 3} and shuffling it, at least one 'r' remains in its original position?</p>
<p>Total letters [</p>
<p>combinatory/prob_eq_n_fixWhat is the probability that, when forming a 2-letter word from {'m' : 2, 'r' : 1, 'o' : 1} and shuffling it, exactly one 'r' remains in its original position?</p>
<p>Total letters [</p>
<p>A.2 Details of Compositional Generalization Problems</p>
<p>Compositional generalization evaluates a model's ability to integrate multiple, distinct reasoning strategies.In contrast to exploratory generalization-which focuses on scaling a single known method to larger instances-compositional generalization requires the synergistic fusion of sub-strategies to solve more complex problems.By the submission deadline, we provide 7 distinct settings to assess compositional performance.Setting 1, illustrated in Figure 1, combines GCD and polynomial root problems.Detailed examples and explanations for the remaining six settings are provided in Table 7 and Table 8. • Composed Problem.Find the number of rectangles that can be formed inside a fixed regular 12-gon where each side of the rectangle lies on either a side or a diagonal of the 12-gon.Note that it is possible for a rectangle to be contained within another rectangle, and that the rectangles may not extend beyond the boundaries of the 12-gon.• Decomposition.After observing the rotational symmetries of the 12-gon and "visualizing" the problem, define the conditions necessary for lines parallel/perpendicular to a specific orientation to form a rectangle.Since a rectangle divided along an line parallel to its sides forms more rectangles, finding the number of total rectangles in such a structure is a combinatorial problem isomorphic to the string problem.• Composed Problem.A circle with radius 4 is moving on the coordinate plane such that its center moves along the curve P (t) = ⟨t, t 2 ⟩ starting at t=0.Find the first value of t for which the circle lies tangent to the x-axis.• Decomposition.Observe that it is sufficient to find a value of t for which the circle's center has a y-coordinate of 4, which reduces to a pure "equation solving" problem. 6  COMP.SETTING 4:  • Composed Problem.There is a collection of 25 indistinguishable white chips and 25 indistinguishable black chips.Find the number of ways to place some of these chips in the 25 unit cells of a 5 × 5 grid such that:</p>
<p>each cell contains at most one chip all chips in the same row and all chips in the same column have the same colour any additional chip placed on the grid would violate one or more of the previous two conditions.• Decomposition.The problem asks to find the number of possible arrangements subject to the named constraints.The first subproblem tests understanding of constraints in a very similar setting.The second subproblem tests the ability to compute the number of cases fitting a particular constraint.</p>
<p>A.3 Details of Transformative Generalization Problems.</p>
<p>Transformative generalization presents the greatest challenge: it tests whether a model can discard a familiar yet ineffective strategy in favor of a qualitatively different and more efficient one.These tasks go beyond simple extension or composition, requiring a "jump out of the box"-a creative reframing or redescription that bypasses the limitations of standard reasoning tactics.By the submission deadline, we include 7 distinct settings to evaluate transformative generalization.Setting 2 (algebra/function_intersection) and Setting 3 (algebra/polynomial_root) are illustrated in Table 3, while Setting 4 (combinatory/prob_no_fix) is visualized in Figure 1.Detailed examples and explanations for the remaining settings are provided in Table 9 and Table 10.
n, eij = 1 if i + j is even 0 if i + j is odd
. Find rank(En).</p>
<p>• Needed insight.Observe that
En = 1 2 11 T + [(−1) i ]i [(−1) j ] T j ,
i.e. a sum of two outer products (each rank 1), so rank(En) = 2 for n ≥ 2 (and
1 if n = 1). TRANSFORMATIVE SETTING 5: FUNC_INTEGRATION • Problem. What is the symbolic integration of the function f (x) = 4 −1(5x 2 + 5x − 2) + 4 − 3?
• Tactic learned.First expand and simplify the algebraic expression to a polynomial, then apply the power-rule integration term by term.</p>
<p>• Problem.Evaluate the indefinite integral
(1+x+x 2 +x 3 +x 4 ) (1−x+x 2 −x 3 +x 4 ) dx.
• Needed insight.Observe that multiplying the two quintic sums collapses all odd-power terms, yielding the even-power polynomial x 8 + x 6 + x 4 +x 2 +1, which can then be integrated directly by the power rule.• Problem.Let circle C1 be positioned in the coordinate plane with a radius of 1. Draw its horizontal diameter and call its endpoints A1 and B1.Draw its vertical diameter and call the higher endpoint D1.Then, let circle C2 be the circle centered at D1 that passes through A1 and B1.Likewise, draw its horizontal diameter and call its endpoints A1 and B1, and draw its vertical diameter and call its higher endpoint D2.Then, repeat this process, constructing a circle C3 centered at D2 that passes through A2 and B2, drawing its horizontal and vertical diameters and constructing points A3, B3, and D3 analogously, and so on until you construct D5.What is the distance between D5 and the center of C1? • Needed insight.There is a pattern to the construction, so that the distance between C1 and Dn is geometric in n, which allows you to avoid actually constructing most of the circles.Training Details.We fine-tune models using the GRPO algorithm implemented in the Open-Instruct framework 8 .The key training parameters are as follows:</p>
<p>B Experiment Details</p>
<p>--beta 0.</p>
<p>B.2 Prompt for Reasoning Trace Step Classification</p>
<p>To systematically analyze the types of reasoning exhibited in model-generated mathematical traces, we employed a structured prompt to guide the annotation of each sentence within the reasoning chain.This prompt instructs the LLM to classify each sentence into one of three categories-conjecture, computation, or other-with further verification for the correctness of computational steps.</p>
<p>The full prompt is as follows:</p>
<p>You are analyzing a sentence from a mathematical reasoning trace.Please classify the following sentence into one of these categories:</p>
<ol>
<li>
<p>"conjecture" -The sentence makes a hypothesis or conjecture about the final answer.Typical examples include "Alternatively, maybe the matrix is singular.","Wait, let's check if the determinant is zero or not.","Alternatively, maybe the problem is from a source where the answer is 14." 2. "computation" -The sentence performs a mathematical computation or calculation.</p>
</li>
<li>
<p>"other" -The sentence is explanation, setup, conclusion, or another type of reasoning.</p>
</li>
</ol>
<p>Original math problem: {original_question} Correct answer: {correct_answer} Sentence to classify: {sentence} If you classify it as "computation", also verify if the computation is correct by doing the calculation yourself.</p>
<p>Please respond in the following JSON format: { "classification": "conjecture|computation|other", "reasoning": "Brief explanation of why you classified it this way.", "computation_correct": true/false/null (only fill if classification is "computation") } This prompt enables fine-grained, reproducible labeling of reasoning steps for downstream analysis.</p>
<p>In our experiments, we applied it to every step separated with ".\n" of the chain-of-thought traces.</p>
<p>C Additional Experiments</p>
<p>Frontier Models' Performance on the Test Problems in Compositional/Transformative Setting.We provide results in Figure 12.In the compositional setting, OpenAI models (particularly o4-mini and o3-mini) demonstrate superior performance on structured problems like matrix rank and polynomial operations, suggesting strong capabilities in combining fundamental mathematical concepts.Claude 3.7 Sonnet and DeepSeek-R1 show more moderate performance in this setting.In the transformative setting, all models struggle with special function intersections and certain polynomial problems.These results highlight both the progress made in LLMs' mathematical reasoning and the remaining challenges in developing models capable enough in different mathematical contexts.</p>
<p>Ablation Study on Disentangling the Role of In-distribution Problem Family in Compositional RL Gain.To better understand under which in-distribution problem family RL improves performance on compositional test problems, we conduct an ablation study (see Table 11 and Table 12) on the two compositional settings (Settings 2 and 5) that showed notable gains after RL fine-tuning according to Figure 11.In these settings, the model was originally trained jointly on two distinct problem families (skill A and skill B), and tested on composite tasks that require integrating both skills.Since not all settings benefited from RL, we hypothesize that the specific choice and compatibility of skill A and skill B may influence whether RL can effectively promote compositional generalization.</p>
<p>To test this hypothesis, we retrain the model in each setting while systematically altering the composition: replacing either skill A or skill B with a nearby alternative, or replacing both.Results show that the original skill A + skill B pairing consistently yields the highest post-RL improvement (+7.5 pp and +15 pp), indicating a strong synergy between the selected task pairs.Replacing just one component reduces gains to a modest +2-5 pp, while replacing both typically eliminates or reverses improvement (-18 pp and -3 pp).These findings suggest that RL is most effective when it can build upon complementary skills already aligned in the joint training distribution-supporting the idea that compositional success depends not just on RL, but on the semantic coherence of the underlying task pair.</p>
<p>Supplementary Analysis on Qwen2.5-Math-7B.As shown in Figure 14, RL fine-tuning consistently improves performance on both in-distribution and explorative generalization tasks, with Qwen2.5-Math-7Bachieving average gains of +51 percentage points on ID problems and +24 percentage points on OOD problems.Notably, the Math-7B model demonstrates particularly strong performance on Logic Zebralogic, reaching 85% ID accuracy and 82% OOD accuracy after RL training-indicating that the specialized mathematical training of the base model synergizes effectively with our RL approach.While Qwen2.5-7B-Instruct generally achieves slightly higher absolute performance (e.g., 95% vs 85% on Logic Zebralogic ID), both models exhibit similar improvement patterns, with consistently larger gains on ID tasks compared to OOD tasks.Interestingly, both Python program.Our analysis proceeds in three steps: (i) we summarize the context window limits of current large-context models; (ii) we provide a representative level-6 problem and a compact dynamic programming (DP) solver; and (iii) we measure the solver's computational footprint and estimate the corresponding token usage.This family generalises classical balls-into-bins counting with (i) multisets of item types and (ii) capacity constraints.Difficulty level k controls the total number of items and the size of the search space; level 6 is the hardest setting used in our experiments.</p>
<p>D.1 Context windows of frontier models</p>
<p>D.3 DP solver and instrumentation</p>
<p>We employ a depth-first DP that memoises states of the form (i, c), where i indexes the current letter type and c is the non-increasing vector of residual capacities.The core Python routine is shown below.Four counters track its execution:</p>
<p>• dp_calls -total invocations of the memoised routine;</p>
<p>• distribution_calls -number of distinct "distribute t items into c" sub-problems generated;</p>
<p>• backtrack_calls -recursive steps inside the enumerator;</p>
<p>• state_transitions -edges explored in the DP graph.</p>
<p>def gen_distributions(total, rem_caps): global distribution_calls distribution_calls += 1 # return all possible distributions of 'total' items into boxes with caps rem_caps</p>
<p>Figure 1 :
1
Figure 1: Examples of training-test pairs designed to test distinct generalization capabilities: (a) Explorative</p>
<p>Form a word by randomly choosing 3
3
letters from the multiset {k: 4, m: 3}, shuffle the letters in the word, what is the probability of at least 3 letter 'k' remains in the same position?How many solutions does the equation (x^2 + 3x + 4)/|x^2 -2x + 4|= -|((2x -1)/(-1x + 2)) -3| -1 have for x in the interval [-10, 10]?Considering the functions f(x) = ax / |x -3| and g(x) = px^2 + qx, where a, p, q can each take integer values from 1 to 5, how many different combinations of parameter values result in at least 1 intersection points in the range [-10, 10]?</p>
<p>Figure 2 :
2
Figure 2: Two examples of compositional generalization in our training/test setup.Each case presents training problems from two separate templates that exercise particular reasoning skills that the model must master, and a test problem that composes the skills.More examples can be found at Appendix A.</p>
<p>•</p>
<p>Problem.Count intersections of f (x) = 2|−2 exp(πx + 2) + 1| − 2 + 3 and g(x) = 3|x + 2| − 3 on [−10, 10].• Tactic learned.Simplify by sign-case analysis, resolve absolute values, and use periodicity to count intersections.</p>
<p>meaningful transformative settings, we enforce the following principles: a) Same problem scope, new insight.Training and test problems share the same template family (e.g., polynomial-root finding or function-intersection), but test instances are specifically designed so that the familiar tactic either fails or becomes intractably cumbersome; b) Necessity of reframing.Solving the test problem must require a novel strategy-such as a symmetry-exploiting substitution or a global geometric argument-rather than exhaustive casework or brute-force enumeration; c) Nontrivial training tasks.The training problems themselves remain sufficiently challenging to ensure the model genuinely learns the familiar tactic before being forced to abandon it.</p>
<p>Figure 3 :
3
Figure 3: Exact-match accuracy of four top-tier LLMs on OMEGA, plotted against increasing complexity levels.As the complexity increases, performance degrades and goes to zero.We provide complexity analysis to typical problems to ensure they are within the models' output length as detailed in §D.</p>
<p>Figure 3
3
Figure3reveals a consistent trend across all models and task types: performance begins near ceiling levels but steadily declines as problem complexity increases.This degradation aligns with the growing number of reasoning steps required, which amplifies the likelihood of error.To justify the evaluation, we provide a complexity analysis in Appendix D, demonstrating that the evaluated problems remain within the models' context length limits.Despite the use of Chain-of-Thought (CoT) traces which enables step-by-step decomposition and self-correction, models still exhibit clear scaling limitations.CoT reasoning remains effective only below a critical complexity threshold, beyond which performance rapidly deteriorates under increased cognitive load.To investigate how CoT reasoning changes under increasing complexity, we analyze the compositional patterns of DeepSeek-R14 CoTs in correct and incorrect responses using O4-mini.Results are shown in Figure5.</p>
<p>Figure 4 :
4
Figure 4: The percentage of incorrect responses exhibiting two distinct error patterns: correct → incorrect shift (blue bars) where models initially provided correct answers but changed to incorrect ones through overthinking, and reasoning spirals (red bars) where models remained in wrong → wrong reasoning chains throughout their response.</p>
<p>Figure 5 :
5
Figure 5: Performance and reasoning patterns across six mathematical task domains showing accuracy degradation and verification behavior as problem complexity increases.Models often reach the correct answer early in the response but continue generating unnecessary verification steps, as shown in the yellow overthinking regions.This behavior increases token usage and can destabilize otherwise correct outputs.Incorrect responses consistently consume more tokens than correct ones.</p>
<p>Figure 6
6
Figure 6 reveals three key trends: a) Shrinking calculation budget.The fraction of tokens devoted to</p>
<p>Figure 7 :
7
Figure 7: Pass@k performance of the advanced LLMs across complexity levels for geometry rotation problems.</p>
<ol>
<li>1 . 3
13
Can More Inference-Time Compute Solve Harder Problems?Helps at Moderate Complexity, but Gains Plateau at Higher Levels</li>
</ol>
<p>Figure 8 :
8
Figure 8: Performance comparison of Qwen2.5-7B-Instructbefore and after RL on OMEGA under the exploratory generalization setting (Section 2.1).Each problem setting is represented by concatenated bars:In-distribution (ID) accuracy (blue) and Out-of-distribution (OOD) accuracy (orange).RL yields strong improvements across most domains on in-distribution tasks; however, gains on out-of-distribution tasks are typically lower and more variable, highlighting the limits of generalization from seen distributions.</p>
<p>Figure 10 :
10
Figure 10: Performance comparison of Qwen2.5-7B-Instruct on OMEGA under the compositional generalization setting.The model's ability to integrate reasoning strategies from two problem families is assessed.For each setting, accuracies are reported on the individual in-distribution problem families (Skill A &amp; Skill B) and their compositional problems.Results are shown before and after RL.RL leads to strong gains on isolated skills, yet these improvements do not reliably carry over to the composed setting.In almost all cases, models that master both skills independently fail to generalize when the solution requires their integration.</p>
<p>Figure 11 :
11
Figure 11: Performance comparison of Qwen2.5-7B-Instruct on OMEGA under the transformational generalization setting.The model's ability to adopt qualitatively new reasoning strategies is evaluated.For each setting, we report accuracies on the source problem family and on the corresponding transformative problems, before and after RL.RL consistently boosts performance on in-distribution tasks, however, accuracy on OOD problems remains near zero across most settings which shows that RL fails to induce new solution strategies.In the matrix rank setting, where the base model showed unusually high OOD performance, RL training reversed this progress-highlighting that optimization can entrench brittle heuristics rather than encourage exploration.</p>
<p>Figure 10 presents
10
Figure 10 presents our findings across five compositional settings.When training RL on individual skills and testing on those same individual skills, models achieve strong performance (often &gt;69% accuracy on S ai and S bi .For instance, in Setting 2, the base model scores only 13% on polygon rotation problems, but RL training boosts performance significantly by nearly 70 percentage points.Similarly, for pattern matching problems, the base model starts at 6%, with RL improving accuracy to 22%.This shows that RL is effective at reinforcing specific, isolated reasoning capabilities.It can help models internalize and reliably execute individual skills, even when the base model starts from very low accuracy.However, the magnitude of improvement varies significantly across settings-suggesting that not all skill types benefit equally from RL.Some skills may be easier to reinforce than others.Despite these gains, models struggle when tested on compositions of learned skills.Performance on such integrated tasks shows little to no improvement after RL, with only minor gains (e.g., +6%) in most cases.For example, when tested on problems that combine GCD and polynomial root reasoning, RL yields no improvement.This begs the question: if models have truly learned the underlying components, why can't they combine them effectively?We hypothesize that current RL approaches tend to overfit to specific patterns within each skill domain rather than learning flexible, generalizable OOD reasoning strategies.This contrasts sharply with human reasoning, where mathematicians readily compose learned sub-skills to solve novel problems-for example, in Perelman's proof of the Poincaré Conjecture, which integrates geometric flow theory, Ricci flow, and topological surgery.</p>
<p>1 − 3 ( 3 Composed 2
1332
Compute the indefinite integral for f (x) = 2(x − 5) 2 − 4(x − 5) + 3. enclosed by f (x) = 3(−e −x −2)−−e −x −2)−3 , g(x) = −3|x + 1| + Number of maximal connected intervals in [−10, 10] where f (x) = −4 −2 sin(πx − Average of all x-coordinates of local minima of f (x) = −3(−2 sin(πx−2)+2)+2 2(−2 sin(πx−2)+2)+1 .maxima of f (x) = 2 cos 3π(|x + 1| + 3) + 3 − 1 in [−10, 10].Integer value (rounded) at which f (x) = x − 5, g(x) = −2|x| − 1 intersect in [−10, 10].Number of intersections of f (x) = −3 cos 2π(2|x+2|+2)+3 +1, g(x) = 4x − 3 in [−5, 5].Number of x-intercepts of f (x) = 3 cos π(−3|x − 2| + 1) − 3 + 3.</p>
<p>COMP•</p>
<p>Example Problem from Domain A. Circle center C, radius 7. G on circle; L midpoint of GC; X midpoint of LC; I midpoint of LX; F is reflection of G across C. Find |IF |. • Example Problem from Domain B. Find the number of intersections of f (x) = −3 cos 2π(2|x + 2| + 2) + 3 + 1, g(x) = 4x − 3 in [−5, 5].</p>
<p>•</p>
<p>Example Problem from Domain A. What is the probability that, when forming a 4-letter word from {'b' : 4, 'i' : 2, 'u' : 2} and shuffling it, no letter remains in its original position?• Example Problem from Domain B. Find the number of intersections of f (x) = −3 cos 2π(2|x + 2| + 2) + 3 + 1, g(x) = 4x − 3 in [−5, 5].• Composed Problem.Considering the functions f (x) = a sin(bπx) and g(x) = p sin(πqx), where a, b, p, q can each take integer values from 1 to 5, how many different combinations of parameter values result in at least 7 intersection points in the range [-10, 10]? • Decomposition.The composed problem requires integrating symbolic reasoning over parameterized trigonometric functions (from Domain B) with combinatorial generalization over multiple configurations (related to Domain A).</p>
<p>Evaluation Protocol.Evaluation uses the same sampling strategy as training.Models are evaluated 200 times throughout training.To account for convergence fluctuations, we report the average performance over the last 5 evaluation checkpoints.Compute Resources.Each RL training run uses 32 NVIDIA H100 GPUs (distributed across 4 nodes) and completes in approximately 12 hours.</p>
<p>Figure 12 :
12
Figure 12: Performance comparison of state-of-the-art LLMs on mathematical reasoning tasks in compositional (left) and transformative (right) settings.</p>
<p>Figure 14 :
14
Figure 14: Comparison of RL fine-tuning effectiveness in the explorative generalization setting between Qwen2.5-Math-7B and Qwen2.5-7B-Instruct.Accuracy on in-distribution (ID) and outof-distribution (OOD) mathematical reasoning tasks before and after RL fine-tuning.Solid bars: Math-7B; hatched bars: Instruct-7B.</p>
<p>Table 1 :
1
A comparison of various evaluation datasets and the methods used to generate them.</p>
<p>Table 2 :
2
Example problem templates across six mathematical domains.For illustration purposes, template content has been shortened.Shaded text indicates programmatically generated variants.Each problem template is associated with a complexity measure δ(θ), reflecting task-specific complexity metrics.
CategoryProblem NameTemplate Example (τ ) with parameter (θ)Complexity(δ(θ))GCDWhat is the greatest common factor of 3450 andlog 10 (answer)Arithmetic24380 ? Prime Factorization What is the second-largest prime factor of 519439 ?log 10 (answer)Mixed OperationsWhat is the value of (-7920)/1320 -2/44*4614 ?number of opera-tionsMatrix RankFindtherankofthematrixsize of the matrix[[5, -14, 6, -1], [-2, -1, 5, -4], [10, -10, -6, 10], [-19, 1, 3, -31]]Linear EquationSolve 5m = -8k -345, -3m + 26 + 119 = -898k + 894knumber of sym-for m.bolsPolynomial RootsSuppose 4160a 3 + 4480a 4 − 585a − 12090 7 a 2 + 1080 7 = 0 , max powerAlgebraFunc Intersectionwhat is a (rational number)? How many timesdothegraphsofnumber of compo-f (x) = 2|(−2 sin(πx + 2) + 1) − 2| + 3andsitionsg(x) = 3|x + 2| − 3 intersect on [−10, 10]?Func AreaFindtheareaboundedbyf</p>
<p>3 , and x = 1.7 .
number of compo-sitionsLetter DistributionDistribute {s:3, g:2, j:2} into 3 identical containersholding [3, 2, 2] letters.Combinatorics
number of letters Pattern Match Randomly select 3 letters from {o:2, x:3} ; expected matches of pattern 'xo+' ?number of letters Prob.(No Fixed) Choose 3 letters from {u:1, f:3, t:2} and shuffle.Probability of no fixed letter positions?number of letters Number Theory</p>
<p>In our work, all training and test problems problems are generated from carefully designed templates to enable precise control over problem structure, diveristy and required reasoning strategies.To do so, we use 40 templated problem generators spanning six mathematical domains: arithmetic, algebra, combinatorics, number theory, geometry, and logic &amp; puzzles.Example problem templates are illustrated in Table</p>
<p>Table 3 :
3
Illustrative training versus test tasks that probe Transformative generalization.Training problems reinforce familiar tactics, but can be over-complicated for test problems where qualitatively different reasoning is required.More examples can be found at Appendix A.
Problem fam-ilyTraining regime (familiar tactic)Transformative test (new tactic required)</p>
<p>Table 3
3
, polynomial-root finding tasks in training might be addressed through straightforward factorization, whereas the test scenarios require employing specialized algebraic substitutions to efficiently determine solutions.Similarly, training instances for function-intersection problems might typically involve direct derivative analysis, whereas the test cases demand recognition of underlying geometric properties to bypass computationally intensive algebra.Each transformative category thus pairs multiple training problems that reinforce established techniques with test problems explicitly designed to challenge the model to surpass these traditional approaches and engage in genuine strategic innovation.Additional examples and detailed explanations are available in Appendix §A.3.</p>
<p>Ratio Reasoning Patterns Analysis Across Difficulty Levels (Problem: Arithmetic Matrix Rank)
1.00.80.4 0.6|Conjecture Steps| / |All Steps| |Computation Steps| / |All Steps| |Correct Steps| / |Computation Steps|0.20.01234 Difficulty Level567Figure 6: Reasoning trace analysis with distribu-tion of two specific types of reasoning steps and
3.1.2Is Lower Accuracy Simply Caused by Errors in Computation?Not Really, LLMs Exhibit Preference for Heuristics Over Direct Computation correctness for the computation step, tested on Matrix Rank problem family.As problem difficulty increases, the model spends less of its CoT on explicit calculations (gold squares) and more on conjectural guesses (pink circles).</p>
<p>Table 12
12
.2.3 Can RL Go Beyond Familiar Skills to Discover New Reasoning Abilities?Learns Familiar Strategies, but Struggles with Unconventional Solution Paths Figure11presents the most challenging test of reasoning flexibility, requiring models to abandon training-observed strategies and discover entirely new solution approaches.The base model performs modestly across most settings, with accuracy typically below 20%.RL training provides substantial benefits on in-domain examples where the solution approach is familiar from training data, and this aligns with our previous generalization tasks (e.g., +56% on matrix rank).observe that it continues to rely on naive solutions, succeeding only on some simple variants of the transformative problems where conventional approaches still apply.Nonetheless, the result highlights both the strengths and limits of RL: it offers meaningful gains when a familiar structure exists but struggles to induce genuinely novel reasoning strategies without prior exposure.This suggests that RL training alone could be insufficient for discovering novel reasoning paradigms, and that such transformational capabilities may require explicit exposure to diverse problem-solving strategies during base model training or supervised fine-tuning.Notably, in the matrix rank setting where the base model achieved decent OOD performance (70%), further RL training actually led to performance deterioration, dropping 30 percentage points.This decline indicates that RL optimization can sometimes reinforce suboptimal patterns learned during training rather than promoting exploration of alternative approaches.
However, performance on OOD transformational problems remains low after RL, often 0%. Theonly notable improvement appears in Setting 7 (+10 pp); however, upon examining the model'strajectories, we
, while altering either component reduced gains substantially, and replacing both often canceled or reversed the benefit.These results suggest that RL can promote compositional generalization, but only when the underlying skills are conceptually aligned and sufficiently reinforced during joint training.3</p>
<p>Table 4 :
4
Problem families (arithmetic and algebra) with sample problems and complexity measures across five levels.
ProblemFamilySample Problem StatementComplexityLv1 Lv2 Lv3 Lv4 Lv5AliasMeasurealgebra/linear_equation Solve 3n − 4t + 1012801=Symbol num-234561012843, −3n + 66 = 4tberalgebra/polynomial_roots Express the second largest root of− 147407 8 as n/m where gcd(n, m) = 1. m 3 − 19331 4 m 2 + 1053 8 m+ 117 2 = 0</p>
<p>Table 5 :
5
Problem families (combinatory and number theory) with sample problems and complexity measures across five levels.
ProblemFamilySample Problem Statement (simplified)ComplexityLv1 Lv2 Lv3 Lv4 Lv5AliasMeasurecombinatory/distributeDivide the letters from {'m' : 2, 'p' :Total letters[4,6] [6,8] [9,10] [11,11][12,12]2, 't' : 2} into 3 distinctively labeled boxeswith sizes [2, 1, 3]. How many ways?combinatory/pattern_matchForm a word by randomly choosing 4 lettersTotal letters[4,6] [6,8] [9,10] [11,12][13,14]from the multiset {'h' : 6, 'u' : 3}. Whatis the expected number of occurrences ofh.*h?</p>
<p>Table 6 :
6
Let N be the number of ordered pairs (a, b) with a, b ≤ 2 4 such that a 2 +b 2 is a multiple of 2 2 .What is N ?Problem families (logic and geometry) with sample problems and complexity measures across five levels.
/prob_no_fix What is the probability that, when formingTotal letters[4,6] [6,8] [8,9] [10,11][11,12]a 4-letter word from {'b' : 4, 'i' : 2, 'u' :2} and shuffling it, no letter remains in itsoriginal position?combinatory/prob_no_letterWhat is the probability that, when forming aTotal letters[4,6] [6,8] [8,9] [10,11][11,12]4-letter word from {'r' : 3, 'x' : 3, 'n' : 2}and shuffling it, no 'x' occupies any of itsoriginal positions?numbertheory/digit_sum Let N be the greatest 4-digit integer suchDigit count23456that both N and its digit-reverse are divisi-ble by 9. What is the digit sum of N ?numbertheory/triple_count Max answer1050100 200 500numbertheory/prime_mod Let N be the tenth smallest 3-digit integerDigit count23456whose digit sum is divisible by 6. What isthe digit count of N ?</p>
<p>.
Grid size45678How many chips placed?geometry/basicStatement1015202530numbergeometry/polygon_chords For a 6-gon with specified diagonals drawn# of diagonals [6,7] [8,9] [10,11][12,13]<a href="2-6,1-4,3-6,5-2,6-4,4-2,3-1">14,15</a>, how manypairs of diagonals are perpendicular?geometry/circleStatement1015202530numberStatement1015202530numbergeometry/triangleStatement1015202530numbergeometry/rotationIn a 10-gon, draw diagonals 5-9 and 8-6,Diagonal num-23456then rotate setup 5 vertices CCW and super-berimpose. Count smallest polygons formed.geometry/polygon_color A 6-gon vertices colored B,B,R,B,B,B inn of n-gon[6,7] [8,9] [10,11][12,13][14,15]order. By rotating, what is the maximumblue vertices landing on originally red posi-tions?
DS = 10.P is midpoint of DS.Rotate S by 7π/12 about P to X. Reflect X over D to Z; reflect D over Z to L. B is midpoint of P Z; F is bisector of ∠SP L; reflect S over F to T .Find |BT |.Circle center C, radius 7. G on circle; L midpoint of GC; X midpoint of LC; I midpoint of LX; F is reflection of G across C. Find |IF |. geometry/polygon_general Square ABCD center T , circumradius 7. Reflect T across B to G. O midpoint of DG; Z midpoint of T A. Find |OZ|.XT = 6.Rotate T by 5π/6 about X to O. Reflect O across XT to V .D is incenter of △T OX; E midpoint of XV .Find |DE|.</p>
<p>Table 7 :
7
Examples (part 1) of training and test tasks that probe Compositional generalization ability of LLM.
Problem familyTraining regime (familiar tactic)Compositional test (combined tactic required)• Example Problem from Domain A. Supposeyou have a 9-gon, with vertices numbered 1through 9 in counterclockwise order. Draw thediagonal from vertex 6 to vertex 4, from vertex 1to vertex 6, and from vertex 3 to vertex 5. Then,rotate the entire setup, including the constructeddiagonals, 8 vertices counterclockwise (so thatCOMP. SETTING 2: GEOMETRY/ROTA-vertex 1 ends up where vertex 9 was), and super-impose it on the original (so that the resultingTION + COMBINATORY/PAT-diagram contains both the original diagonals and the rotated versions of the diagonals). The orig-TERN_MATCHinal 9-gon will be partitioned into a collectionof smaller polygons. How many such polygonswill there be?• Example Problem from Domain B. Form aword by randomly choosing 3 letters from themultiset {y: 2, v: 1, p: 4, z: 4}. What is theexpected number of occurrences of the pattern'p.*p' in each word?</p>
<p>Table 8 :
8
Examples (part 2) of training and test tasks that probe Compositional generalization ability of LLM.
Problem familyTraining regime (familiar tactic)Compositional test (combined tactic required)• Composed Problem. Consider the matrix M =a b c• Example Problem from Domain A.Compute1 a bwhere a, b, and c are integers be-COMP. SETTING 5:the rank of the given 4x4 matrix: ... • Example Problem from Domain B. What is2 1 a tween 3 and 10, inclusive. How many differentARITHMETIC/MA-TRIX_RANK +the probability of such event happening: Form a word by randomly choosing 4 letters from thecombinations of (a, b, c) result in a matrix with rank exactly 3COMBINATO-multiset {j: 4, d: 2, p: 2}, shuffle the letters in• Decomposition. The composed problem re-RY/PROB_NO_FIXthe word, what is the probability of exact 1 letterquires integrating linear algebra reasoning (ma-'p' remains in the same position?trix rank determination) (from Domain B) withcombinatorial generalization over multiple con-figurations (related to Domain A).• Example Problem from Domain A. A 6-gon is• Composed Problem. Each vertex of a regularcolored so that in clockwise order, the verticesoctagon is independently colored either red orare colored as follows: vertex 0 is blue, vertex 1blue with equal probability. The probability thatis blue, vertex 2 is red, vertex 3 is blue, vertex 4the octagon can then be rotated so that all ofCOMP. SETTING 6: GEOMETRY/POLY-GON_COLOR +is blue, vertex 5 is blue. What is the maximum number of blue vertices that can be made to oc-cupy a position where there were originally red vertices by rotating the 6-gon?the blue vertices end up at positions where there were originally red vertices is m n , where m and n are relatively prime positive integers. What is m + n?COMBINATO-• Example Problem from Domain B. What is• Decomposition. The problem is fundamentallyRY/PROB_NO_FIXthe probability of such event happening: Formabout finding the number of cases satisfying aa word by randomly choosing 4 letters from theconstraint. The first subproblem tests under-multiset {j: 4, d: 2, p: 2}, shuffle the letters instanding of the constraint (and the required spa-the word, what is the probability of exact 1 lettertial reasoning). The second subproblem tests the'p' remains in the same position?ability to enumerate cases.• Example Problem from Domain A. Chips, col-ored either black or white, are placed in the 25unit cells of a 5x5 grid such that: a) each cellcontains at most one chip, b) all chips in thesame row and all chips in the same column havethe same colour, c) any additional chip placedon the grid would violate one or more of the pre-COMP. SETTING 7: LOGIC/GRID_CHIP +vious two conditions. Furthermore, we have the following constraints (with the cells 0-indexed): cell (3, 4) is black, cell (2, 0) is white, cell (4, 3)COMBINATO-is black, cell (1, 1) is white, cell (2, 2) is white,RY/PROB_NO_FIXcell (0, 3) is black. How many chips are placedon the grid?• Example Problem from Domain B. What isthe probability of such event happening: Forma word by randomly choosing 4 letters from themultiset {j: 4, d: 2, p: 2}, shuffle the letters inthe word, what is the probability of exact 1 letter'p' remains in the same position?</p>
<p>Table 9 :
9
Examples of training and test tasks that probe Transformative generalization (part 1)
Problem familyTraining regime (familiar tactic)Transformative test (new tactic required)• Problem.Let En be n ×• Problem.  −4 −16 −8 What is the rank of the ma-7 TRANSFORMATIVEtrix: 9 417 106 0−14 −10   item TacticSETTING76−2 −12MATRIX_RANKlearned. Use Gaussian elimination to reducethe matrix to row-echelon form and count thenumber of nonzero pivot rows.</p>
<p>Table 10 :
10
Examples of training and test tasks that probe Transformative generalization (part 2).
Problem familyTraining regime (familiar tactic)Transformative test (new tactic required)TRANSFORMATIVE SETTING 6: LOG-IC/BLOCKED_GRID• Problem. In a 6x6 grid, how many different paths are there from the bottom left (0, 0) to the top right (5, 5), if you can only move right or up at each step, subject to the constraint that you cannot move through the following cells: (3, 3), (2, 1), (3, 4), (3, 1), (0, 5), (5, 0), (2, 0), (0, 4), (2, 5) • Tactic learned. Among possible strategies, plot the cells on the grid, and categorize paths ac-cording to whether they pass above or below a fixed cell. Use combinatorial formulas to easily find the number of paths in each category. For smaller problems, use brute-force search.• Problem. In a 10x10 grid, how many different paths are there from the bottom left (0, 0) to the top right (9, 9), if you can only move right or up at each step, subject to the constraint that you cannot move through the following cells: (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8)? • Needed insight. There is a wall, which vastly simplifies the analysis. The only variation among viable paths is at which "vertical" index we first choose to move right, so there are 10 options.• Problem. Let C be the circle with center Vand radius 6. Point K is on circle C. Let I be themidpoint of segment KV. Point M is the midpointTRANSFORMATIVE SETTING 7:of segment IK. Let L be the midpoint of segment IM. What is the distance between points L and I?GEOMETRY/CIRCLE• Tactic learned. Construct circles, lines, andperpendicular bisectors; find distances betweenrelevant points in the plane using coordinate ge-ometry.</p>
<p>All experiments are conducted using the base model Qwen2.5-7B-Instruct, a strong instruction-tuned large language model.This model serves as the initialization for reinforcement learning (RL) fine-tuning.Explorative problems: 100 test samples from the same problem family within the explorative problems but with higher complexity (level 3).• Compositional and Transformational problems: 20-50 test samples per setting.Although these problems do not have explicit complexity annotations, we adjust key parameters (like from small to large) to ensure the test set spans a range of complexity.
Datasets. The training and evaluation problems for explorative, compositional, and transformationalgeneralization are drawn from the curated problem families described in Appendix A. Unlessotherwise specified, each training set consists of 1,000 problems. For compositional settings wheretraining involves two problem families, we allocate 500 samples per family. To align with theproficiency level of Qwen2.5-7B-Instruct 7 , the training problems are restricted to complexity levels1-2. Evaluation is performed on:• In-distribution (ID) problems: 100 test samples drawn from the same complexity range(1-2) as training, depending on the setup-whether explorative, compositional, or transfor-mational.•
B.1 Experimental SetupModels.</p>
<p>Table 13 :
13
Context-length limits of the models considered in this work.
ModelMaximum tokensReferenceGPT-o3 Mini200 000OpenAI DocsGPT-o4 Mini128 000Addepto BlogClaude 3 Sonnet v3.7≥200 000Anthropic SupportDeepSeek-R1164 000OpenRouter Card
D.2 Representative level-6 problemArrange the letters {o:6, l:1, d:2, y:2, v:3} into five indistinguishable boxes with capacities [2, 2, 2, 5, 3].How many distinct distributions exist?</p>
<p>Versions used: Claude (2025-02-19), o3-mini (2025-01-25), o4-mini (2025-04-16).
Qwen2.5-Math-7B results follow the same patterns as Qwen2.5-7B-Instruct; please refer to Figure14in the Appendix for more details.
Successful RL training requires the base model to achieve nonzero accuracy on the training problems.
https://github.com/allenai/open-instruct
Discussion &amp; ConclusionWe have presented OMEGA, a controlled benchmark designed to isolate and evaluate three axes of out-of-distribution generalization in mathematical reasoning: explorative, compositional, and transformative.By generating matched train-test pairs from template-driven problem families, our framework enables precise analysis of reasoning behaviors and supports infinite-scale, reproducible synthesis.Our empirical study yields three key insights.First, RL fine-tuning delivers substantial gains on both in-distribution and explorative generalization, boosting accuracy on harder instances within known problem domains.Second, despite these improvements, RL's impact on compositional tasks remains modest: models still struggle to integrate multiple learned strategies coherently.Third, RL struggles to induce genuinely new reasoning patterns, showing negligible progress on transformative generalization that requires shifting to novel solution paradigms.These findings underscore a fundamental limitation: while RL can amplify the breadth and depth of problems that LLMs solve, they do not by themselves foster the creative leaps needed for true transformational reasoning.To bridge this gap, future work might explore:• Curriculum scaffolding: dynamically ordering tasks to gradually introduce compositional and transformative challenges alongside explorative ones; • Meta-reasoning controllers: mechanisms that detect when a default strategy stalls and actively search for alternative solution families;By diagnosing where and why current LLMs fail to generalize creatively, OMEGA lays the groundwork for next-generation reasoners that can not only interpolate but also innovate-moving us closer to human-level mathematical problem-solving.Digit length[4,7]D Complexity AnalysisIn this section, we present a complexity analysis for the COMBINATORY/DISTRIBUTION task, which is studied often in the main paper.Our goal is to demonstrate that this problem can be solved within the context-length limits of today's frontier large language models and Qwen-series models.Unlike other tasks, such as function intersection or geometry, where the number of tokens required is difficult to estimate, combinatory distribution problems allow for more precise tracking via simulation using a  for j in range(len(rem_caps)))) count += dp(i + 1, new_caps) return countD.4 Empirical resource usageRunning the solver on the level-6 instance yields the statistics in 14.The backtracking routine dominates runtime with 1059 calls.Conservatively assuming that each backtrack call translates to 20 generated/consumed tokens, the total token demand is 1059 × 20 = 21 180 tokens, well below even the smallest window in Table13D.5 Footprint across difficulty levels (1-5)We measured the average number of backtrack calls on the canonical instance for each lower difficulty.Table15summarises these, along with the corresponding token estimates: Table15: Average backtracking calls and estimated token usage for levels 1-5.Even at level 5-the hardest below level 6-the solver requires only ≈14 K tokens.All levels thus comfortably fit within every model's context window, confirming the practicality of our experiments.D.6 Take-awayEven under pessimistic token-accounting assumptions, level-6 COMBINATORY DISTRIBUTION problems demand fewer than 30 000 tokens of "reasoning budget".All four frontier models listed in Table13therefore possess ample context to solve every instance we evaluate, validating the feasibility of our experimental design.
Big-math: A large-scale, high-quality math dataset for reinforcement learning in language models. Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, Nick Haber, 2025</p>
<p>The mathematics of deepmind models. Alonso Noguer, The Mathematics of DeepMind Models. November 01, 20242024</p>
<p>Art of Problem Solving. 2024 aime ii problems/problem 1. 2024</p>
<p>Creativity and artificial intelligence. Margaret A Boden, Artificial intelligence. 1031-21998</p>
<p>Compositionality and generalization in emergent languages. Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, Marco Baroni, arXiv:2004.091242020arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Yuchen Jiang, Peter Lin, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Sean Sanyal, Xiang Welleck, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, 2023Faith and fate: Limits of transformers on compositionality</p>
<p>. Hugging Face. Metamathqa. 2023</p>
<p>Improving text-to-sql evaluation methodology. Catherine Finegan-Dollak, Jonathan K Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, Dragomir Radev, arXiv:1806.090292018arXiv preprint</p>
<p>Deep learning with long short-term memory networks for financial market predictions. Thomas Fischer, Christopher Krauss, FAU Discussion Papers in Economics. 11/2017, 2017</p>
<p>Omni-math: A universal olympiad level mathematic benchmark for large language models. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, arXiv:2410.079852024arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun, 2024</p>
<p>Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, arXiv:2504.11456Deepmath-103k: A large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. 2025arXiv preprint</p>
<p>Compositionality decomposed: How do neural networks generalise. Dieuwke Hupkes, Verna Dankers, Mathijs Mul, Elia Bruni, Journal of Artificial Intelligence Research. 672020</p>
<p>Measuring compositional generalization: A comprehensive method on realistic data. Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, arXiv:1912.097132019arXiv preprint</p>
<p>Measuring compositional generalization: A comprehensive method on realistic data. Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc Van Zee, Olivier Bousquet, 2020</p>
<p>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. Brenden Lake, Marco Baroni, International conference on machine learning. PMLR2018</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag. Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra2022</p>
<p>. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Costa Shengyi, Kashif Huang, Longhui Rasul, Albert Yu, Ziju Jiang, Zihan Shen, Bin Qin, Li Dong, Yann Zhou, Guillaume Fleureau, Stanislas Lample, Polu, Numinamath, 2024</p>
<p>Adji Bousso Dieng, and Jason Hattrick-Simpers. Probing out-of-distribution generalization in machine learning for materials. Kangming Li, Andre Niyongabo Rubungo, Xiangyun Lei, Daniel Persaud, Kamal Choudhary, Brian Decost, 2024</p>
<p>Gsm-plus: A comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers. Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, Wei Bi, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Towards out-of-distribution generalization: A survey. Jiashuo Liu, Zheyan Shen, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, Peng Cui, 2023</p>
<p>Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, Yi Dong, arXiv:2505.248642025arXiv preprint</p>
<p>Compositional generalization by learning analytical expressions. Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, Dongmei Zhang, Advances in Neural Information Processing Systems. 202033</p>
<p>Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, arXiv:2410.05229Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. 2024arXiv preprint</p>
<p>Beyond lines and circles: Unveiling the geometric reasoning gap in large language models. Spyridon Mouselinos, Henryk Michalewski, Mateusz Malinowski, arXiv:2402.038772024arXiv preprint</p>
<p>. Openai, April 2022</p>
<p>Learning to reason with llms. Openai, September 2024</p>
<p>. OpenAI. Math. 500November 2024</p>
<p>The illusion of thinking: Understanding the strengths and limitations of reasoning models via the lens of problem complexity. Parshin Shojaee, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, Mehrdad Farajtabar, arXiv:2506.069412025arXiv preprint</p>
<p>Climbing the ladder of reasoning: What llms can-and still can't-solve after sft?. Yiyou Sun, Georgia Zhou, Hao Wang, Dacheng Li, Nouha Dziri, Dawn Song, arXiv:2504.117412025arXiv preprint</p>
<p>Mathscale: Scaling instruction tuning for mathematical reasoning. Zhengyang Tang, Xingxing Zhang, Benyou Wang, Furu Wei, 2024</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic Team, </p>
<p>Openmathinstruct-1: A 1.8 million math instruction tuning dataset. Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, Igor Gitman, 2024</p>
<p>Grokked transformers are implicit reasoners: A mechanistic journey to the edge of generalization. Boshi Wang, Xiang Yue, Yu Su, Huan Sun, 2024</p>
<p>Towards a theoretical framework of out-of-distribution generalization. Haotian Ye, Chuanlong Xie, Tianle Cai, Ruichen Li, Zhenguo Li, Liwei Wang, 2021</p>
<p>Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, Gao Huang, arXiv:2504.13837Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?. 2025arXiv preprint</p>
<p>Scaling and beyond: Advancing spatial reasoning in mllms requires new recipes. Huanyu Zhang, Chengzu Li, Wenshan Wu, Shaoguang Mao, Yifan Zhang, Haochen Tian, Ivan Vulić, Zhang Zhang, Liang Wang, Tieniu Tan, arXiv:2504.150372025arXiv preprint</p>
<p>Evaluating the performance of large language models on gaokao benchmark. Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, Xipeng Qiu, 2024</p>
<p>Can models learn skill composition from examples?. Haoyu Zhao, Simran Kaur, Dingli Yu, Anirudh Goyal, Sanjeev Arora, Advances in Neural Information Processing Systems. 202437</p>
<p>Yang Zhou, Hongyi Liu, Zhuoming Chen, Yuandong Tian, Beidi Chen, arXiv:2502.05252Gsm-infinite: How do your llms behave over infinitely increasing context length and reasoning complexity?. 2025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
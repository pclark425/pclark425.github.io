<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1965 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1965</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1965</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-278740010</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.11917v1.pdf" target="_blank">OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> General-purpose robots capable of performing diverse tasks require synergistic reasoning and acting capabilities. However, recent dual-system approaches, which separate high-level reasoning from low-level acting, often suffer from challenges such as limited mutual understanding of capabilities between systems and latency issues. This paper introduces OneTwoVLA, a single unified vision-language-action model that can perform both acting (System One) and reasoning (System Two). Crucially, OneTwoVLA adaptively switches between two modes: explicitly reasoning at critical moments during task execution, and generating actions based on the most recent reasoning at other times. To further unlock OneTwoVLA's reasoning and generalization capabilities, we design a scalable pipeline for synthesizing embodied reasoning-centric vision-language data, used for co-training with robot data. We validate OneTwoVLA's effectiveness through extensive experiments, highlighting its superior performance across four key capabilities: long-horizon task planning, error detection and recovery, natural human-robot interaction, and generalizable visual grounding, enabling the model to perform long-horizon, highly dexterous manipulation tasks such as making hotpot or mixing cocktails.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1965.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1965.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OneTwoVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single unified VLA that jointly performs high-level textual reasoning and low-level action generation, adaptively switching between reasoning mode and acting mode; co-trained with reasoning-enriched robot data and large-scale synthetic vision-language data to improve grounding and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OneTwoVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unified autoregressive vision-language-action model that (1) predicts a decision token ([BOR] to reason or [BOA] to act), (2) when reasoning generates textual 'embodied reasoning' (scene description, high-level plan, history summary, next step), and (3) when acting generates action chunks conditioned on current images, a reference image (I_ref) representing the observation at the latest reasoning time, the latest reasoning content R, language instruction ℓ, and proprioceptive state s_t. Continuous actions are modeled via an action expert (inherited from base VLA π0) trained with a flow-matching loss; the VLM component auto-regressively generates textual reasoning supervised with cross-entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Explicit textual embodied reasoning that names target objects and their locations (scene description + next-step) and supplies this textual grounding to the policy; co-training with synthetic embodied vision-language data to transfer web/common-sense object knowledge into the VLA. Grounding is thus implemented as textual reasoning that links language tokens to visual observations and then conditions action generation on those tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>scene-level (image observations with textual object/location descriptions); no explicit object detector or pixel-level segmentation reported</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit spatial grounding using image observations and textual spatial descriptions (reference image I_ref used to keep reasoning aligned with a particular observation); no explicit 3D coordinates or bounding-box supervision reported</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation (long-horizon manipulation), human-robot interaction, visual grounding, error detection & recovery, generalizable planning</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Tomato-Egg, Hotpot, Cocktail (author-designed long-horizon tasks); Single-Env and Open-World visual grounding benchmarks (authors' evaluation), plus generalizable planning tasks (Get Icy Cola, Empty Plate, Tool Use, Prepare Drinks)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>real-world robot (wrist-mounted fisheye GoPro and multi-camera setups) with some synthetic VL co-training (16,000 synthetic tabletop images augmented with fisheye and gripper compositing)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (task completion) for manipulation and visual grounding; action MSE reported for action fitting comparisons; timing (inference) for computation cost</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Long-horizon tasks: average success rate 87% (Tomato-Egg 85%, Hotpot 80%, Cocktail 95%); Visual grounding Single-Env (OneTwoVLA): 78% success; Visual grounding Open-World (OneTwoVLA without VL co-training): 8% success; Action MSE on validation: π0 had 62% higher MSE than OneTwoVLA (visual grounding experiment) and elsewhere π0 MSE 56% higher (multi-modal actions experiment). Inference acting-mode latency <0.2s; reasoning-mode latency depends on token length (e.g., when generating ~100 reasoning tokens >2s).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Without synthetic VL co-training (OneTwoVLA trained only on robot data): Open-World visual grounding 8% success vs OneTwoVLA-VL 73% (absolute difference +65%); Single-Env: OneTwoVLA 78% vs OneTwoVLA-VL 88% (+10% absolute).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Co-training with embodied VL data yields large absolute improvements in open-world grounding (e.g., +65% SR on Open-World), and modest improvements in single-environment grounding (+10% SR); co-training also improves generalizable planning and enables transfer of commonsense object knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>The paper identifies domain gap and missing web/common-sense object knowledge as major perceptual/grounding bottlenecks: models trained only on robot action data (π0 or OneTwoVLA without VL co-training) often 'aimlessly reach' or fail to identify objects in open-world scenes. Dual-system latency and lack of mutual capability awareness are also noted perceptual/grounding bottlenecks that lead to outdated or infeasible commands.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Reported failure types include: occasional failure to firmly grasp objects (e.g., oil bottle slips, plate drops), failing to grasp the strainer due to misalignment (requiring recovery), misidentifying the location of target ingredients (e.g., confusing bok choy and enoki mushrooms), inaccurate pouring causing spills, and moving toward irrelevant objects when the target is absent from both training datasets. Frequencies are qualitative (described as 'occasionally' or 'sometimes'); quantitative per-failure rates are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Domain gap addressed by scalable synthetic vision-language pipeline: (1) generate diverse tabletop textual descriptions (Gemini 2.5 Pro), (2) synthesize images from descriptions (FLUX.1-dev), (3) augment synthetic images with fisheye distortion and composited robot gripper to better match robot observations, and (4) generate instruction-reasoning annotations with Gemini; then co-train VLA on robot data + synthetic VL data (16k images). This co-training is reported to significantly reduce open-world performance drops.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>OneTwoVLA-VL (co-trained) achieves 73% success on Open-World visual grounding including objects unseen in robot data; OneTwoVLA without VL achieves 8% and π0 3% on the same Open-World. The paper notes OneTwoVLA-VL sometimes even generalizes to objects not present in either dataset, but specific quantitative breakdowns per object-type are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Authors report that co-training with 16,000 synthetic VL images (6k for visual grounding annotations and 10k for long-horizon plans) substantially improves grounding and generalization; no systematic study of scale beyond this dataset size is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Language and visual inputs are fused by a single unified autoregressive VLM that consumes instruction ℓ and the latest reasoning content R (text) together with image observations (I_t and I_ref) to produce either textual reasoning or a [BOA] and then action tokens. The model thus conditions actions on textual reasoning (late fusion via shared transformer context), rather than an external separate planner.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Robot data: authors collected ~2,000 demonstrations across platforms (atomic skills: pick/place/move/open/close/pour) plus task-specific demonstrations (e.g., Tomato-Egg 200, Hotpot 600, Cocktail 300). Co-training with 16k synthetic VL samples enabled strong open-world generalization with the given robot data; no exact samples-vs-performance curve reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Explicit, textual embodied reasoning (scene descriptions + plans + next-step) integrated into a unified VLA substantially improves visual grounding and action quality compared to a flat action-only VLA (π0) or a decoupled dual-system; co-training with synthetic embodied vision-language data bridges domain gap and unlocks open-world grounding (OneTwoVLA-VL: 73% SR Open-World vs OneTwoVLA 8% and π0 3%); major failure modes are perceptual (novel/unseen objects, occlusion, mislocalization) and latency/misalignment in dual-system pipelines; reasoning-mode frequency kept low to retain efficiency, but long reasoning outputs cause inference pauses (seconds).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1965.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1965.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OneTwoVLA-VL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OneTwoVLA co-trained with Vision-Language (synthetic) data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variant of OneTwoVLA co-trained on the authors' 16,000 synthetic embodied vision-language samples (annotated instruction + reasoning) in addition to robot demonstrations, producing large gains in open-world visual grounding and generalizable planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OneTwoVLA-VL (OneTwoVLA + synthetic VL co-training)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same unified VLA architecture as OneTwoVLA but co-trained on combined robot demonstration data and a large synthetic vision-language dataset (16k images annotated with embodied reasoning generated by Gemini 2.5 Pro and images synthesized by FLUX.1-dev), using image augmentations (fisheye, composited gripper) to reduce domain gap.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Co-training transfers commonsense/web knowledge into the VLA: synthetic instruction-reasoning pairs give the model explicit textual labels and spatial descriptions that link language to visual features; the reasoning text is then used as conditioning for action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>scene-level with textual object-level descriptions (no explicit object detectors reported)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit via image observations and textual spatial phrasing in the embodied reasoning annotations; synthetic data includes explicit spatial relations in text to train such mappings</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>visual grounding (open-world), generalizable long-horizon planning, manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open-World visual grounding benchmark (authors' evaluation), plus same long-horizon tasks where co-training enabled improved generalization</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>synthetic tabletop images (16k) used in co-training augmented to resemble robot views; deployed/evaluated on real-world robot setups</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (task completion) for visual grounding and manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Visual grounding: Single-Env 88% success; Open-World 73% success. (Compared to OneTwoVLA without VL: Single-Env 78% and Open-World 8%; π0: 5% and 3% respectively.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>OneTwoVLA without VL co-training: Open-World 8% SR (vs OneTwoVLA-VL 73% SR), Single-Env 78% (vs 88%).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Absolute improvements: +65% SR on Open-World visual grounding and +10% SR on Single-Env visual grounding versus OneTwoVLA without VL co-training.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Synthetic VL co-training designed to address domain gap and missing object knowledge — pipeline includes fisheye distortion and gripper compositing to better match robot observations; paper reports these augmentations help transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Even with VL co-training, certain failure cases persist when encountering objects absent from both the robot and synthetic VL data; no per-case frequency breakdown is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Explicit domain-shift mitigation via synthetic image augmentations (fisheye, composited gripper) plus large-scale synthetic reasoning annotations to transfer web/common-sense knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>OneTwoVLA-VL handles objects unseen in robot data (10 object categories present in VL data) with substantially higher success (73% Open-World), and in some cases generalizes to objects not present in either dataset (reported qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Paper provides a single co-training scale (16k synthetic images) and reports large gains; no multi-scale study.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Same unified autoregressive fusion as OneTwoVLA: textual reasoning + instruction are part of the model context conditioning action outputs; co-training enriches the textual-visual associations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Co-training allowed strong open-world generalization without collecting massive additional robot demonstrations; training set included ~2,000 robot demos plus 16k synthetic images (authors' collected counts), but no formal sample-efficiency curve provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Co-training with high-quality, embodied reasoning-centric synthetic VL data is highly effective at enabling open-world visual grounding and transferring commonsense object knowledge into robot policies; explicit textual reasoning in training enables the model to identify and pick objects unseen in robot data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1965.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1965.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>π0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>π0 (base vision-language-action model used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior state-of-the-art vision-language-action model used as the base VLA and as a main baseline; it predicts actions directly from images and instructions without producing textual embodied reasoning, and is fine-tuned on the authors' robot dataset for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>π0 (baseline VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Flat VLA that directly maps (image observations + instruction) to actions (action tokens) without an explicit textual reasoning step or historical textual context; used both as base model for OneTwoVLA and as a baseline after fine-tuning on the same dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Implicit grounding via end-to-end mapping from images+instruction to actions (no explicit textual reasoning or embodied reasoning content); tends to overfit action shortcuts rather than form explicit object-language mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>scene-level implicit representation learned end-to-end for action prediction</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit in learned visual features; no explicit spatial tokens or reference-image mechanism described</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation, long-horizon tasks (evaluated as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Same long-horizon tasks (Tomato-Egg, Hotpot, Cocktail) and visual grounding benchmarks (Single-Env, Open-World) used as baselines in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>real-world robot observations (same hardware as OneTwoVLA) after fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate, action MSE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Long-horizon tasks average success: 57% (Tomato-Egg 70%, Hotpot 50%, Cocktail 50%); Visual grounding Single-Env 5% success, Open-World 3% success; action MSE on validation reported as 62% higher than OneTwoVLA in the visual grounding experiment (and 56% higher in the multi-modal actions experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>π0 lacks explicit reasoning/historical context and therefore sometimes loses track of task progress (e.g., stays stuck at initial position or repeats picking same item), exhibits aimless reaching when instructions are ambiguous or when target is referred to indirectly, and shows very poor open-world grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Common failures include: getting stuck at initial step on multi-step tasks, repeatedly picking wrong items (e.g., picking beef repeatedly in Hotpot), aimless forward reaches in grounding tasks; quantitatively, extremely low open-world grounding success (3%) and high action MSE relative to OneTwoVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Fine-tuned solely on robot data for experiments; performance collapses under open-world domain shift (very low success on objects not in robot dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Very poor: 3% success on Open-World visual grounding benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>End-to-end mapping from visual features and instruction tokens to action tokens (no separate textual reasoning or late conditioning used).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained/fine-tuned on the same robot datasets as OneTwoVLA for fair comparison; however, lacks the benefit of synthetic VL co-training, resulting in worse open-world generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Flat VLA without explicit reasoning exhibits poor visual grounding and long-horizon task tracking; explicit reasoning (as in OneTwoVLA) improves object identification, reduces action MSE, and substantially improves performance on instruction-following and grounding tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1965.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1965.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual-System VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual-system approach (Gemini 2.5 Pro as System Two + π0 as System One)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-module pipeline where an external VLM (Gemini 2.5 Pro) acts as a high-level System Two to produce decompositions/atomic commands, and π0 is fine-tuned as a low-level System One to execute atomic commands; used as a baseline to expose limitations of decoupled planners and controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dual-System (Gemini 2.5 Pro as System Two + π0 as System One)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline architecture: System Two (large VLM Gemini 2.5 Pro) decomposes high-level instructions into atomic commands or textual plans; System One (π0) consumes atomic commands and generates low-level actions. Systems are separate models communicating via textual intermediate commands.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Grounding is performed via textual atomic commands produced by System Two that the System One attempts to map to actions; thus grounding is an inter-model textual handoff rather than joint conditioning within a single unified model.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>textual atomic commands (plan-level) handed to an action model; the grounding operates at the level of commands-to-actions</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit in the textual atomic commands and the low-level policy's visual features; no explicit shared reference image mechanism reported</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>long-horizon manipulation and instruction decomposition evaluated as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Tomato-Egg, Hotpot, Cocktail (same long-horizon tasks used for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>real-world robots (same hardware used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Long-horizon tasks average success: 63% (Tomato-Egg 55%, Hotpot 70%, Cocktail 65%) according to Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Two major grounding-related issues identified: (1) lack of mutual awareness — System Two may produce atomic commands infeasible for System One (e.g., 'add green onion' when none present), and (2) inference latency of System Two (Gemini 2.5 Pro) causing stale/outdated reasoning that leads System One into out-of-distribution states and failed recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Failure modes include producing infeasible atomic commands due to missing perception of current scene state and delayed updates causing the low-level policy to act on stale plans; these issues contributed to lower success rates relative to OneTwoVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>No explicit domain-shift mitigation beyond using Gemini 2.5 Pro for decomposition and fine-tuning π0 on atomic commands; reported latency and misalignment problems reduce robustness under dynamic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Not specifically reported separately for dual-system beyond aggregate long-horizon success rates; qualitative failure examples include giving commands referencing absent objects.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Late fusion via textual handoff: System Two produces text that System One conditions on; no joint multimodal latent fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Built by pairing large VLM decompositions with action-finetuned π0 using annotated atomic commands; no explicit sample-efficiency numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Dual-system decomposition can achieve reasonable performance but suffers from misalignment between planner and executor (infeasible commands) and latency, motivating the unified OneTwoVLA approach which reduces these grounding failure modes by joint training and shared context.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1965.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1965.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synthetic VL Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scalable synthesized embodied reasoning-centric vision-language data pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-step automated pipeline (Gemini 2.5 Pro -> FLUX.1-dev -> Gemini 2.5 Pro annotations) that generates 16,000 synthetic tabletop images with embodied instruction + reasoning pairs and domain-matching augmentations (fisheye, composited gripper) to co-train VLAs for better grounding and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Synthetic VL pipeline (Gemini 2.5 Pro + FLUX.1-dev)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline: (1) prompt Gemini 2.5 Pro to generate diverse textual tabletop layout descriptions; (2) synthesize corresponding images using FLUX.1-dev; (3) augment images (fisheye distortion, composited robot gripper) and use Gemini 2.5 Pro to generate instruction-reasoning annotation pairs for each synthetic image, producing 16k samples (6k grounding images with 17 instruction-reasoning pairs each, 10k long-horizon planning images with plans).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Provides high-quality textual grounding supervision (object names, spatial relationships, attributes, plans) that the unified VLA learns to map to visual observations; effectively supplies explicit paired (textual reasoning <-> image) supervision used to ground language to visual features in the VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>scene-level synthetic images annotated with per-image multi-type textual references (names, spatial relationships, attributes, semantic features) and multi-step plans</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>textual spatial relations in annotations (e.g., left-back of X, front-right) used to teach spatial grounding; no explicit 3D geometry produced by pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>visual grounding tasks (spatial/attribute/semantic references) and long-horizon planning tasks for co-training</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Synthetic datasets created for Single-Env / Open-World visual grounding and long-horizon planning co-training (authors' dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>synthetic tabletop images augmented to resemble robot camera views (fisheye + gripper compositing)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>indirect: improvements in downstream VLA success rates and grounding generalization when co-trained</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Co-training with 16,000 synthetic VL samples produced large downstream gains: e.g., Open-World grounding success improved from 8% (OneTwoVLA without VL) to 73% (OneTwoVLA-VL).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Without synthetic VL pipeline co-training: OneTwoVLA Open-World grounding 8% SR; with pipeline: 73% SR.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Large absolute improvement on open-world grounding (+65% SR) attributed to the synthetic VL pipeline and its embodied reasoning annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Pipeline was explicitly designed to mitigate perception/grounding bottlenecks caused by domain gap and missing object/semantic knowledge in robot datasets by synthesizing diverse scenes and adding robot-like visual augmentations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Authors note remaining failures when encountering objects absent from both robot and synthetic datasets; pipeline alleviates but does not fully eliminate such novel-object failures.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Primary technique for domain-shift handling in the paper: synthetic-to-real augmentations and co-training to transfer commonsense knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Enables improved performance on objects unseen in robot data but present in synthetic VL data (authors report successful transfer to many such objects; exact per-object numbers limited), enabling OneTwoVLA-VL to reach 73% Open-World SR.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Paper presents one applied scale (16k images) and claims substantial benefit; no multi-scale ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Not a model fusion mechanism per se; the pipeline provides paired textual reasoning that is consumed by unified VLA, enabling textual conditioning of actions.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Authors collected ~2,000 robot demos and used 16k synthetic VL images to achieve open-world generalization; the pipeline intended to produce large-scale VL supervision cheaply compared to manual annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>High-quality synthesized embodied reasoning-centric VL data (with spatial/attribute/semantic annotations and long-horizon plans) is an effective lever to improve VLA grounding and generalization to novel objects/environments when co-trained with robot demonstration data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Rt-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 2)</em></li>
                <li>Palm-e: An embodied multimodal language model <em>(Rating: 2)</em></li>
                <li>Hi robot: Open-ended instruction following with hierarchical vision-languageaction models <em>(Rating: 2)</em></li>
                <li>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning <em>(Rating: 2)</em></li>
                <li>Grounded decoding: Guiding text generation with grounded models for embodied agents <em>(Rating: 2)</em></li>
                <li>Otter: A vision-language-action model with text-aware visual feature extraction <em>(Rating: 1)</em></li>
                <li>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models <em>(Rating: 2)</em></li>
                <li>Open-world object manipulation using pre-trained vision-language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1965",
    "paper_id": "paper-278740010",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "OneTwoVLA",
            "name_full": "OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning",
            "brief_description": "A single unified VLA that jointly performs high-level textual reasoning and low-level action generation, adaptively switching between reasoning mode and acting mode; co-trained with reasoning-enriched robot data and large-scale synthetic vision-language data to improve grounding and generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OneTwoVLA",
            "model_description": "Unified autoregressive vision-language-action model that (1) predicts a decision token ([BOR] to reason or [BOA] to act), (2) when reasoning generates textual 'embodied reasoning' (scene description, high-level plan, history summary, next step), and (3) when acting generates action chunks conditioned on current images, a reference image (I_ref) representing the observation at the latest reasoning time, the latest reasoning content R, language instruction ℓ, and proprioceptive state s_t. Continuous actions are modeled via an action expert (inherited from base VLA π0) trained with a flow-matching loss; the VLM component auto-regressively generates textual reasoning supervised with cross-entropy.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Explicit textual embodied reasoning that names target objects and their locations (scene description + next-step) and supplies this textual grounding to the policy; co-training with synthetic embodied vision-language data to transfer web/common-sense object knowledge into the VLA. Grounding is thus implemented as textual reasoning that links language tokens to visual observations and then conditions action generation on those tokens.",
            "representation_level": "scene-level (image observations with textual object/location descriptions); no explicit object detector or pixel-level segmentation reported",
            "spatial_representation": "implicit spatial grounding using image observations and textual spatial descriptions (reference image I_ref used to keep reasoning aligned with a particular observation); no explicit 3D coordinates or bounding-box supervision reported",
            "embodied_task_type": "object manipulation (long-horizon manipulation), human-robot interaction, visual grounding, error detection & recovery, generalizable planning",
            "embodied_task_name": "Tomato-Egg, Hotpot, Cocktail (author-designed long-horizon tasks); Single-Env and Open-World visual grounding benchmarks (authors' evaluation), plus generalizable planning tasks (Get Icy Cola, Empty Plate, Tool Use, Prepare Drinks)",
            "visual_domain": "real-world robot (wrist-mounted fisheye GoPro and multi-camera setups) with some synthetic VL co-training (16,000 synthetic tabletop images augmented with fisheye and gripper compositing)",
            "performance_metric": "success rate (task completion) for manipulation and visual grounding; action MSE reported for action fitting comparisons; timing (inference) for computation cost",
            "performance_value": "Long-horizon tasks: average success rate 87% (Tomato-Egg 85%, Hotpot 80%, Cocktail 95%); Visual grounding Single-Env (OneTwoVLA): 78% success; Visual grounding Open-World (OneTwoVLA without VL co-training): 8% success; Action MSE on validation: π0 had 62% higher MSE than OneTwoVLA (visual grounding experiment) and elsewhere π0 MSE 56% higher (multi-modal actions experiment). Inference acting-mode latency &lt;0.2s; reasoning-mode latency depends on token length (e.g., when generating ~100 reasoning tokens &gt;2s).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Without synthetic VL co-training (OneTwoVLA trained only on robot data): Open-World visual grounding 8% success vs OneTwoVLA-VL 73% (absolute difference +65%); Single-Env: OneTwoVLA 78% vs OneTwoVLA-VL 88% (+10% absolute).",
            "grounding_improvement": "Co-training with embodied VL data yields large absolute improvements in open-world grounding (e.g., +65% SR on Open-World), and modest improvements in single-environment grounding (+10% SR); co-training also improves generalizable planning and enables transfer of commonsense object knowledge.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "The paper identifies domain gap and missing web/common-sense object knowledge as major perceptual/grounding bottlenecks: models trained only on robot action data (π0 or OneTwoVLA without VL co-training) often 'aimlessly reach' or fail to identify objects in open-world scenes. Dual-system latency and lack of mutual capability awareness are also noted perceptual/grounding bottlenecks that lead to outdated or infeasible commands.",
            "failure_mode_analysis": "Reported failure types include: occasional failure to firmly grasp objects (e.g., oil bottle slips, plate drops), failing to grasp the strainer due to misalignment (requiring recovery), misidentifying the location of target ingredients (e.g., confusing bok choy and enoki mushrooms), inaccurate pouring causing spills, and moving toward irrelevant objects when the target is absent from both training datasets. Frequencies are qualitative (described as 'occasionally' or 'sometimes'); quantitative per-failure rates are not provided.",
            "domain_shift_handling": "Domain gap addressed by scalable synthetic vision-language pipeline: (1) generate diverse tabletop textual descriptions (Gemini 2.5 Pro), (2) synthesize images from descriptions (FLUX.1-dev), (3) augment synthetic images with fisheye distortion and composited robot gripper to better match robot observations, and (4) generate instruction-reasoning annotations with Gemini; then co-train VLA on robot data + synthetic VL data (16k images). This co-training is reported to significantly reduce open-world performance drops.",
            "novel_object_performance": "OneTwoVLA-VL (co-trained) achieves 73% success on Open-World visual grounding including objects unseen in robot data; OneTwoVLA without VL achieves 8% and π0 3% on the same Open-World. The paper notes OneTwoVLA-VL sometimes even generalizes to objects not present in either dataset, but specific quantitative breakdowns per object-type are limited.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Authors report that co-training with 16,000 synthetic VL images (6k for visual grounding annotations and 10k for long-horizon plans) substantially improves grounding and generalization; no systematic study of scale beyond this dataset size is provided.",
            "fusion_mechanism": "Language and visual inputs are fused by a single unified autoregressive VLM that consumes instruction ℓ and the latest reasoning content R (text) together with image observations (I_t and I_ref) to produce either textual reasoning or a [BOA] and then action tokens. The model thus conditions actions on textual reasoning (late fusion via shared transformer context), rather than an external separate planner.",
            "sample_efficiency": "Robot data: authors collected ~2,000 demonstrations across platforms (atomic skills: pick/place/move/open/close/pour) plus task-specific demonstrations (e.g., Tomato-Egg 200, Hotpot 600, Cocktail 300). Co-training with 16k synthetic VL samples enabled strong open-world generalization with the given robot data; no exact samples-vs-performance curve reported.",
            "key_findings_grounding": "Explicit, textual embodied reasoning (scene descriptions + plans + next-step) integrated into a unified VLA substantially improves visual grounding and action quality compared to a flat action-only VLA (π0) or a decoupled dual-system; co-training with synthetic embodied vision-language data bridges domain gap and unlocks open-world grounding (OneTwoVLA-VL: 73% SR Open-World vs OneTwoVLA 8% and π0 3%); major failure modes are perceptual (novel/unseen objects, occlusion, mislocalization) and latency/misalignment in dual-system pipelines; reasoning-mode frequency kept low to retain efficiency, but long reasoning outputs cause inference pauses (seconds).",
            "uuid": "e1965.0"
        },
        {
            "name_short": "OneTwoVLA-VL",
            "name_full": "OneTwoVLA co-trained with Vision-Language (synthetic) data",
            "brief_description": "Variant of OneTwoVLA co-trained on the authors' 16,000 synthetic embodied vision-language samples (annotated instruction + reasoning) in addition to robot demonstrations, producing large gains in open-world visual grounding and generalizable planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OneTwoVLA-VL (OneTwoVLA + synthetic VL co-training)",
            "model_description": "Same unified VLA architecture as OneTwoVLA but co-trained on combined robot demonstration data and a large synthetic vision-language dataset (16k images annotated with embodied reasoning generated by Gemini 2.5 Pro and images synthesized by FLUX.1-dev), using image augmentations (fisheye, composited gripper) to reduce domain gap.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Co-training transfers commonsense/web knowledge into the VLA: synthetic instruction-reasoning pairs give the model explicit textual labels and spatial descriptions that link language to visual features; the reasoning text is then used as conditioning for action generation.",
            "representation_level": "scene-level with textual object-level descriptions (no explicit object detectors reported)",
            "spatial_representation": "implicit via image observations and textual spatial phrasing in the embodied reasoning annotations; synthetic data includes explicit spatial relations in text to train such mappings",
            "embodied_task_type": "visual grounding (open-world), generalizable long-horizon planning, manipulation",
            "embodied_task_name": "Open-World visual grounding benchmark (authors' evaluation), plus same long-horizon tasks where co-training enabled improved generalization",
            "visual_domain": "synthetic tabletop images (16k) used in co-training augmented to resemble robot views; deployed/evaluated on real-world robot setups",
            "performance_metric": "success rate (task completion) for visual grounding and manipulation",
            "performance_value": "Visual grounding: Single-Env 88% success; Open-World 73% success. (Compared to OneTwoVLA without VL: Single-Env 78% and Open-World 8%; π0: 5% and 3% respectively.)",
            "has_grounding_ablation": true,
            "performance_without_grounding": "OneTwoVLA without VL co-training: Open-World 8% SR (vs OneTwoVLA-VL 73% SR), Single-Env 78% (vs 88%).",
            "grounding_improvement": "Absolute improvements: +65% SR on Open-World visual grounding and +10% SR on Single-Env visual grounding versus OneTwoVLA without VL co-training.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Synthetic VL co-training designed to address domain gap and missing object knowledge — pipeline includes fisheye distortion and gripper compositing to better match robot observations; paper reports these augmentations help transfer.",
            "failure_mode_analysis": "Even with VL co-training, certain failure cases persist when encountering objects absent from both the robot and synthetic VL data; no per-case frequency breakdown is provided.",
            "domain_shift_handling": "Explicit domain-shift mitigation via synthetic image augmentations (fisheye, composited gripper) plus large-scale synthetic reasoning annotations to transfer web/common-sense knowledge.",
            "novel_object_performance": "OneTwoVLA-VL handles objects unseen in robot data (10 object categories present in VL data) with substantially higher success (73% Open-World), and in some cases generalizes to objects not present in either dataset (reported qualitatively).",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Paper provides a single co-training scale (16k synthetic images) and reports large gains; no multi-scale study.",
            "fusion_mechanism": "Same unified autoregressive fusion as OneTwoVLA: textual reasoning + instruction are part of the model context conditioning action outputs; co-training enriches the textual-visual associations.",
            "sample_efficiency": "Co-training allowed strong open-world generalization without collecting massive additional robot demonstrations; training set included ~2,000 robot demos plus 16k synthetic images (authors' collected counts), but no formal sample-efficiency curve provided.",
            "key_findings_grounding": "Co-training with high-quality, embodied reasoning-centric synthetic VL data is highly effective at enabling open-world visual grounding and transferring commonsense object knowledge into robot policies; explicit textual reasoning in training enables the model to identify and pick objects unseen in robot data.",
            "uuid": "e1965.1"
        },
        {
            "name_short": "π0",
            "name_full": "π0 (base vision-language-action model used as baseline)",
            "brief_description": "A prior state-of-the-art vision-language-action model used as the base VLA and as a main baseline; it predicts actions directly from images and instructions without producing textual embodied reasoning, and is fine-tuned on the authors' robot dataset for fair comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "π0 (baseline VLA)",
            "model_description": "Flat VLA that directly maps (image observations + instruction) to actions (action tokens) without an explicit textual reasoning step or historical textual context; used both as base model for OneTwoVLA and as a baseline after fine-tuning on the same dataset.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Implicit grounding via end-to-end mapping from images+instruction to actions (no explicit textual reasoning or embodied reasoning content); tends to overfit action shortcuts rather than form explicit object-language mappings.",
            "representation_level": "scene-level implicit representation learned end-to-end for action prediction",
            "spatial_representation": "implicit in learned visual features; no explicit spatial tokens or reference-image mechanism described",
            "embodied_task_type": "object manipulation, long-horizon tasks (evaluated as baseline)",
            "embodied_task_name": "Same long-horizon tasks (Tomato-Egg, Hotpot, Cocktail) and visual grounding benchmarks (Single-Env, Open-World) used as baselines in experiments",
            "visual_domain": "real-world robot observations (same hardware as OneTwoVLA) after fine-tuning",
            "performance_metric": "success rate, action MSE",
            "performance_value": "Long-horizon tasks average success: 57% (Tomato-Egg 70%, Hotpot 50%, Cocktail 50%); Visual grounding Single-Env 5% success, Open-World 3% success; action MSE on validation reported as 62% higher than OneTwoVLA in the visual grounding experiment (and 56% higher in the multi-modal actions experiment).",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "π0 lacks explicit reasoning/historical context and therefore sometimes loses track of task progress (e.g., stays stuck at initial position or repeats picking same item), exhibits aimless reaching when instructions are ambiguous or when target is referred to indirectly, and shows very poor open-world grounding.",
            "failure_mode_analysis": "Common failures include: getting stuck at initial step on multi-step tasks, repeatedly picking wrong items (e.g., picking beef repeatedly in Hotpot), aimless forward reaches in grounding tasks; quantitatively, extremely low open-world grounding success (3%) and high action MSE relative to OneTwoVLA.",
            "domain_shift_handling": "Fine-tuned solely on robot data for experiments; performance collapses under open-world domain shift (very low success on objects not in robot dataset).",
            "novel_object_performance": "Very poor: 3% success on Open-World visual grounding benchmark.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "End-to-end mapping from visual features and instruction tokens to action tokens (no separate textual reasoning or late conditioning used).",
            "sample_efficiency": "Trained/fine-tuned on the same robot datasets as OneTwoVLA for fair comparison; however, lacks the benefit of synthetic VL co-training, resulting in worse open-world generalization.",
            "key_findings_grounding": "Flat VLA without explicit reasoning exhibits poor visual grounding and long-horizon task tracking; explicit reasoning (as in OneTwoVLA) improves object identification, reduces action MSE, and substantially improves performance on instruction-following and grounding tasks.",
            "uuid": "e1965.2"
        },
        {
            "name_short": "Dual-System VLA",
            "name_full": "Dual-system approach (Gemini 2.5 Pro as System Two + π0 as System One)",
            "brief_description": "A two-module pipeline where an external VLM (Gemini 2.5 Pro) acts as a high-level System Two to produce decompositions/atomic commands, and π0 is fine-tuned as a low-level System One to execute atomic commands; used as a baseline to expose limitations of decoupled planners and controllers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Dual-System (Gemini 2.5 Pro as System Two + π0 as System One)",
            "model_description": "Pipeline architecture: System Two (large VLM Gemini 2.5 Pro) decomposes high-level instructions into atomic commands or textual plans; System One (π0) consumes atomic commands and generates low-level actions. Systems are separate models communicating via textual intermediate commands.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Grounding is performed via textual atomic commands produced by System Two that the System One attempts to map to actions; thus grounding is an inter-model textual handoff rather than joint conditioning within a single unified model.",
            "representation_level": "textual atomic commands (plan-level) handed to an action model; the grounding operates at the level of commands-to-actions",
            "spatial_representation": "implicit in the textual atomic commands and the low-level policy's visual features; no explicit shared reference image mechanism reported",
            "embodied_task_type": "long-horizon manipulation and instruction decomposition evaluated as baseline",
            "embodied_task_name": "Tomato-Egg, Hotpot, Cocktail (same long-horizon tasks used for comparison)",
            "visual_domain": "real-world robots (same hardware used in experiments)",
            "performance_metric": "success rate",
            "performance_value": "Long-horizon tasks average success: 63% (Tomato-Egg 55%, Hotpot 70%, Cocktail 65%) according to Table 1.",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Two major grounding-related issues identified: (1) lack of mutual awareness — System Two may produce atomic commands infeasible for System One (e.g., 'add green onion' when none present), and (2) inference latency of System Two (Gemini 2.5 Pro) causing stale/outdated reasoning that leads System One into out-of-distribution states and failed recovery.",
            "failure_mode_analysis": "Failure modes include producing infeasible atomic commands due to missing perception of current scene state and delayed updates causing the low-level policy to act on stale plans; these issues contributed to lower success rates relative to OneTwoVLA.",
            "domain_shift_handling": "No explicit domain-shift mitigation beyond using Gemini 2.5 Pro for decomposition and fine-tuning π0 on atomic commands; reported latency and misalignment problems reduce robustness under dynamic execution.",
            "novel_object_performance": "Not specifically reported separately for dual-system beyond aggregate long-horizon success rates; qualitative failure examples include giving commands referencing absent objects.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Late fusion via textual handoff: System Two produces text that System One conditions on; no joint multimodal latent fusion.",
            "sample_efficiency": "Built by pairing large VLM decompositions with action-finetuned π0 using annotated atomic commands; no explicit sample-efficiency numbers provided.",
            "key_findings_grounding": "Dual-system decomposition can achieve reasonable performance but suffers from misalignment between planner and executor (infeasible commands) and latency, motivating the unified OneTwoVLA approach which reduces these grounding failure modes by joint training and shared context.",
            "uuid": "e1965.3"
        },
        {
            "name_short": "Synthetic VL Pipeline",
            "name_full": "Scalable synthesized embodied reasoning-centric vision-language data pipeline",
            "brief_description": "A three-step automated pipeline (Gemini 2.5 Pro -&gt; FLUX.1-dev -&gt; Gemini 2.5 Pro annotations) that generates 16,000 synthetic tabletop images with embodied instruction + reasoning pairs and domain-matching augmentations (fisheye, composited gripper) to co-train VLAs for better grounding and generalization.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Synthetic VL pipeline (Gemini 2.5 Pro + FLUX.1-dev)",
            "model_description": "Pipeline: (1) prompt Gemini 2.5 Pro to generate diverse textual tabletop layout descriptions; (2) synthesize corresponding images using FLUX.1-dev; (3) augment images (fisheye distortion, composited robot gripper) and use Gemini 2.5 Pro to generate instruction-reasoning annotation pairs for each synthetic image, producing 16k samples (6k grounding images with 17 instruction-reasoning pairs each, 10k long-horizon planning images with plans).",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Provides high-quality textual grounding supervision (object names, spatial relationships, attributes, plans) that the unified VLA learns to map to visual observations; effectively supplies explicit paired (textual reasoning &lt;-&gt; image) supervision used to ground language to visual features in the VLA.",
            "representation_level": "scene-level synthetic images annotated with per-image multi-type textual references (names, spatial relationships, attributes, semantic features) and multi-step plans",
            "spatial_representation": "textual spatial relations in annotations (e.g., left-back of X, front-right) used to teach spatial grounding; no explicit 3D geometry produced by pipeline",
            "embodied_task_type": "visual grounding tasks (spatial/attribute/semantic references) and long-horizon planning tasks for co-training",
            "embodied_task_name": "Synthetic datasets created for Single-Env / Open-World visual grounding and long-horizon planning co-training (authors' dataset)",
            "visual_domain": "synthetic tabletop images augmented to resemble robot camera views (fisheye + gripper compositing)",
            "performance_metric": "indirect: improvements in downstream VLA success rates and grounding generalization when co-trained",
            "performance_value": "Co-training with 16,000 synthetic VL samples produced large downstream gains: e.g., Open-World grounding success improved from 8% (OneTwoVLA without VL) to 73% (OneTwoVLA-VL).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Without synthetic VL pipeline co-training: OneTwoVLA Open-World grounding 8% SR; with pipeline: 73% SR.",
            "grounding_improvement": "Large absolute improvement on open-world grounding (+65% SR) attributed to the synthetic VL pipeline and its embodied reasoning annotations.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Pipeline was explicitly designed to mitigate perception/grounding bottlenecks caused by domain gap and missing object/semantic knowledge in robot datasets by synthesizing diverse scenes and adding robot-like visual augmentations.",
            "failure_mode_analysis": "Authors note remaining failures when encountering objects absent from both robot and synthetic datasets; pipeline alleviates but does not fully eliminate such novel-object failures.",
            "domain_shift_handling": "Primary technique for domain-shift handling in the paper: synthetic-to-real augmentations and co-training to transfer commonsense knowledge.",
            "novel_object_performance": "Enables improved performance on objects unseen in robot data but present in synthetic VL data (authors report successful transfer to many such objects; exact per-object numbers limited), enabling OneTwoVLA-VL to reach 73% Open-World SR.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Paper presents one applied scale (16k images) and claims substantial benefit; no multi-scale ablation.",
            "fusion_mechanism": "Not a model fusion mechanism per se; the pipeline provides paired textual reasoning that is consumed by unified VLA, enabling textual conditioning of actions.",
            "sample_efficiency": "Authors collected ~2,000 robot demos and used 16k synthetic VL images to achieve open-world generalization; the pipeline intended to produce large-scale VL supervision cheaply compared to manual annotation.",
            "key_findings_grounding": "High-quality synthesized embodied reasoning-centric VL data (with spatial/attribute/semantic annotations and long-horizon plans) is an effective lever to improve VLA grounding and generalization to novel objects/environments when co-trained with robot demonstration data.",
            "uuid": "e1965.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 2
        },
        {
            "paper_title": "Palm-e: An embodied multimodal language model",
            "rating": 2
        },
        {
            "paper_title": "Hi robot: Open-ended instruction following with hierarchical vision-languageaction models",
            "rating": 2
        },
        {
            "paper_title": "Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning",
            "rating": 2
        },
        {
            "paper_title": "Grounded decoding: Guiding text generation with grounded models for embodied agents",
            "rating": 2
        },
        {
            "paper_title": "Otter: A vision-language-action model with text-aware visual feature extraction",
            "rating": 1
        },
        {
            "paper_title": "Cot-vla: Visual chain-of-thought reasoning for vision-language-action models",
            "rating": 2
        },
        {
            "paper_title": "Open-world object manipulation using pre-trained vision-language models",
            "rating": 1
        }
    ],
    "cost": 0.02362,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning
17 May 2025</p>
<p>Fanqi Lin 
Tsinghua University
2 Shanghai Qi Zhi Institute</p>
<p>Shanghai Artificial Intelligence Labo-ratory</p>
<p>Ruiqian Nai 
Tsinghua University
2 Shanghai Qi Zhi Institute</p>
<p>Shanghai Artificial Intelligence Labo-ratory</p>
<p>Yingdong Hu 
Tsinghua University
2 Shanghai Qi Zhi Institute</p>
<p>Shanghai Artificial Intelligence Labo-ratory</p>
<p>Jiacheng You 
Tsinghua University
2 Shanghai Qi Zhi Institute</p>
<p>Shanghai Artificial Intelligence Labo-ratory</p>
<p>Junming Zhao 
Tsinghua University
2 Shanghai Qi Zhi Institute</p>
<p>Fudan University
5 SpiritAI</p>
<p>Yang Gao 
Tsinghua University
2 Shanghai Qi Zhi Institute</p>
<p>Shanghai Artificial Intelligence Labo-ratory</p>
<p>OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning
17 May 2025C4ED379316FE54F3858BAC409DE4AEAAarXiv:2505.11917v1[cs.RO]Vision-Language-Action Models, Embodied Reasoning
General-purpose robots capable of performing diverse tasks require synergistic reasoning and acting capabilities.However, recent dual-system approaches, which separate high-level reasoning from low-level acting, often suffer from challenges such as limited mutual understanding of capabilities between systems and latency issues.This paper introduces OneTwoVLA, a single unified vision-language-action model that can perform both acting (System One) and reasoning (System Two).Crucially, OneTwoVLA adaptively switches between two modes: explicitly reasoning at critical moments during task execution, and generating actions based on the most recent reasoning at other times.To further unlock OneTwoVLA's reasoning and generalization capabilities, we design a scalable pipeline for synthesizing embodied reasoning-centric vision-language data, used for co-training with robot data.We validate OneTwoVLA's effectiveness through extensive experiments, highlighting its superior performance across four key capabilities: long-horizon task planning, error detection and recovery, natural human-robot interaction, and generalizable visual grounding, enabling the model to perform long-horizon, highly dexterous manipulation tasks such as making hotpot or mixing cocktails.</p>
<p>OneTwoVLA</p>
<p>Reason Act
ΔT ΔR Grip ΔT ΔR Grip ⋮ ⋮ ⋮ ΔT ΔR Grip Act ΔT ΔR Grip ΔT ΔR Grip ⋮ ⋮ ⋮ ΔT ΔR Grip Act ΔT ΔR Grip ΔT ΔR Grip ⋮ ⋮ ⋮ ΔT ΔR Grip</p>
<p>Reason Act</p>
<p>ΔT ΔR Grip ΔT ΔR Grip ⋮ ⋮ ⋮ ΔT ΔR Grip understanding of the scene and goal (e.g., interpreting the recipe, planning the sequence of steps), while acting corresponds to the physical execution (e.g., chopping, mixing) that grounds abstract reasoning in the real world.This paper aims to imbue robots with a similar synergistic relationship between reasoning and acting.</p>
<p>Current approaches [3][4][5][6][7] often draw inspiration from Kahneman's dual-system framework [8].Typically, a System Two, such as internet-pretrained vision-language models (VLMs) [9,10], is dedicated to slow high-level reasoning, generating intermediate reasoning contents.Meanwhile, a System One, such as vision-language-action models (VLAs) [11][12][13], translates these intermediate contents into precise low-level robot actions.However, this explicit decoupling results in both systems lacking mutual awareness of each other's capabilities; System Two may produce intermediate contents that System One cannot execute [5].Furthermore, in real-world deployment, issues such as latency may cause System Two to respond belatedly, providing outdated or irrelevant guidance.</p>
<p>We argue that achieving stronger reasoning-acting synergy demands a unified model.Indeed, the recent trend towards unifying capabilities within single models is proving crucial for advancing AI [14][15][16][17], and we believe this approach holds particular promise for robot learning.In light of this, we introduce OneTwoVLA, a single unified vision-language-action model capable of both acting (System One) and reasoning (System Two).Importantly, it adaptively determines when to engage each mode.As shown in Fig. 1, OneTwoVLA triggers natural language reasoning at key stepslike completing a subtask, detecting an error, or requiring human input -producing outputs such as scene descriptions, task plans, historical summaries, and next-step instructions.Otherwise, it generates actions informed by its most recent reasoning outputs.A key advantage of this unified model is its natural support for co-training with vision-language data, significantly enhancing reasoning and generalization.To facilitate this, we develop a scalable pipeline for synthesizing high-quality, embodied reasoning-centric vision-language data.</p>
<p>Our extensive experiments validate OneTwoVLA's effectiveness, demonstrating its ability to integrate diverse capabilities within a single model: 1) Long-horizon task planning: OneTwoVLA reasons to formulate, track, and dynamically adjust task plans based on execution feedback, significantly outperforming flat VLA (by 30%) and dual-system VLA (by 24%) baselines.Vision-language co-training further enables generalization to novel task instructions (e.g., planning coffee preparation for "Help me stay awake").2) Error detection and recovery: OneTwoVLA detects execution errors in real time, reasons about corrective strategies, and performs agile recovery actions.3) Natural human-robot interaction: OneTwoVLA adjusts actions immediately upon human intervention and proactively seeks clarification when faced with ambiguity.4) Generalizable visual grounding: OneTwoVLA exhibits superior understanding of spatial relationships, object attributes, and semantic features, even generalizing to objects absent from its robot training data.</p>
<p>Related Work</p>
<p>Vision-Language-Action Models.Initialized from pre-trained vision-language models (VLMs) [9,[18][19][20][21], vision-language-action models (VLAs) [6,[11][12][13][22][23][24][25][26] have emerged as a promising approach for building general-purpose robots.These VLAs, trained on large robot datasets [27][28][29][30][31][32][33][34][35][36][37], can handle a wide range of real-world manipulation tasks.However, these VLAs exhibit limited reasoning capabilities [4,5,13], showing vulnerability when confronted with long-horizon tasks or complex dynamic environments.Furthermore, their generalization capabilities remain constrained, often requiring task-specific fine-tuning [11,12].In contrast, our work enhances reasoning and generalization capabilities through a unified model architecture and a co-training framework.</p>
<p>Reasoning for Robot Control.Previous works [38][39][40][41][42][43][44][45][46] demonstrate that high-level reasoning can enhance low-level policy performance in robot control.In particular, many studies [3-7, 13, 47] explore dual-system frameworks, where a foundation model (e.g., a VLM) serves as System Two to perform high-level reasoning, while a low-level policy operates as System One to generate actions based on reasoning outputs.While this dual-system framework proves effective for accomplishing long-horizon manipulation tasks, it inherently suffers from limitations such as the two systems lack-ing mutual awareness of each other's capabilities [5] as well as latency issues with System Two.Our concurrent work [48] employs a single model to predict a subtask before each action, but this reasoning is simple and information-limited.If this inflexible paradigm generates extensive reasoning at every step, it significantly impacts inference efficiency [49].To address these limitations, we propose a unified model capable of adaptively deciding when to reason versus when to act, allowing for both informative reasoning and efficient execution.</p>
<p>Co-training for Robot Learning.Co-training with data from diverse sources has been shown to benefit robot learning [22,[50][51][52][53][54][55][56][57][58].In particular, several prior works [23,[59][60][61]explore co-training robot policies with action-free vision-language data alongside robot data, demonstrating improvements in policy generalization.However, these methods [23,59,61] typically either rely on existing vision-language datasets, which suffer from limited quality due to their significant domain gap from robot application scenarios; or manually collect vision-language datasets [60], which are inherently limited in size and difficult to scale up.To address these limitations, we propose a scalable pipeline for synthesizing vision-language data rich in embodied reasoning.Our pipeline ensures both high quality and scalability, significantly enhancing policy's reasoning and generalization capabilities.</p>
<p>Method</p>
<p>In this section, we first introduce the framework of OneTwoVLA in Sec.3.1, including its formulation, adaptive inference, and model instantiation.We then describe how we curate robot data to enable synergistic reasoning and acting in Sec.3.2.Finally, we present our scalable pipeline for synthesizing vision-language data enriched with embodied reasoning in Sec.3.3.</p>
<p>Framework of OneTwoVLA</p>
<p>Problem Formulation.The central problem investigated in this work is how to develop a robotic control policy π θ capable of both reasoning and acting, with the critical ability to autonomously decide at each timestep t whether to reason or act.Formally, the policy operates in two modes.When in reasoning mode, the policy takes as input the current image observations from multiple cameras I 1 t , . . ., I n t (denoted as I 1:n t , where n is the number of cameras), the reference images from the latest reasoning timestep I 1 ref , . . ., I n ref (denoted as I 1:n ref , which introduces observation histories to prevent ambiguous states), the language instruction ℓ, and the latest reasoning content R. The policy performs reasoning in the form of textual output, generating updated reasoning content R ∼ π θ (•|I 1:n t , I 1:n ref , ℓ, R).Sec.3.2 provides further details on the specific content of this reasoning process.In acting mode, the policy π additionally incorporates the robot's proprioceptive state s t and generates an action chunk A t based on the latest reasoning content:
A t ∼ π θ (•|I 1:n t , I 1:n ref , ℓ, R, s t ).</p>
<p>Algorithm 1 Inference Pipeline of OneTwoVLA</p>
<p>Require: VLA model π θ , language instruction ℓ 1: t ← 0,
I 1:n ref ← initial image, R ← none 2: while R ̸ = "Task Finished" do 3: DT ∼ π θ .decide(•|I 1:n t , I 1:n ref , ℓ, R) 4: if DT = [BOR] then 5: R ∼ π θ .reason(•|I 1:n t , I 1:n ref , ℓ, R) 6: R ← R, I ref ← It 7: else if DT = [BOA] then 8:
At ∼ π θ .act(•|I 1:n t , I token.Since the model only enters reasoning mode at a few critical steps, the additional inference time incurred is minimal (see Appendix D.3).Conversely, when [BOA] is predicted, the model enters acting mode and directly generates the action chunk A t , with inference efficiency unaffected in this mode.This adaptive framework allows for both informative reasoning and efficient execution, while previous methods suffer from either overly simple reasoning [48] or low inference efficiency [49].Moreover, our framework inherently supports error recovery and human-robot interaction: when the policy detects an error (e.g., failing to grasp an object), it autonomously enters reasoning mode to determine a corrective strategy and execute agile recovery actions.When human interaction occurs, any interaction text will be consistently added to the language instruction ℓ in subsequent steps.Model Instantiation.OneTwoVLA is designed to be general, allowing most existing VLAs to be integrated with minimal modifications.For a specific instance, we employ π 0 [12] as the base VLA, which demonstrates strong performance across various tasks.The visionlanguage model of π 0 auto-regressively generates textual reasoning during inference and is supervised via a crossentropy loss during training.To model complex continuous action distributions, we inherit the action expert architecture from π 0 and train it using a flow matching loss [62,63].</p>
<p>OneTwoVLA's inference flow is detailed in Fig. 2. See Appendix D.2 for more training details.</p>
<p>Curating Robot Data with Embodied Reasoning</p>
<p>Most existing robotic manipulation datasets consist primarily of observation-action pairs and lack associated reasoning information.To address this gap, we introduce a novel robot data format.For a given task, we first collect demonstration trajectories provided by human experts.Subsequently, each trajectory is segmented into a sequence of intervals.There are two types of intervals: reasoning intervals, which capture key steps requiring model reasoning (e.g., upon completing subtasks, detecting errors, or when human interaction is required), which we further annotate with textual reasoning content; and acting intervals, in which the model primarily learns to predict actions based on observations and the latest reasoning content.See Appendix D.1 for more details.</p>
<p>Next, we elaborate on the embodied reasoning content.As shown in Fig. 3 left, it consists of four components: 1) a detailed scene description, primarily focusing on the locations of task-relevant objects; 2) a high-level plan that outlines the sequential steps to accomplish the task; 3) a concise historical summary to keep the model informed about the task's progress; and 4) the immediate next step that the robot needs to execute.This comprehensive reasoning content encourages the model to understand the visual world, learn high-level planning, and track task progress.Furthermore, to equip the policy with error detection and recovery capabilities, we specifically collect and label robot data focused on recovery from failure states.To enable natural human-robot interaction, we annotate certain intervals of the demonstrations with interaction context (e.g., the robot's question and the human's answer shown in Fig. 3 left).</p>
<p>Scalable Synthesis of Vision-Language Data with Embodied Reasoning</p>
<p>The carefully curated robot data described in Sec.3.2 allows the model to directly learn the desired task, but its size scales linearly with the costly human effort, making large dataset creation impractical.To endow our model with stronger generalization and the ability to cope with highly varied scenarios, we leverage off-the-shelf foundation models and design a fully scalable pipeline that synthesizes vision-language data enriched with embodied reasoning.</p>
<p>This pipeline consists of three steps: 1) We prompt Gemini 2.5 Pro [64] to generate diverse textual descriptions of tabletop layouts featuring common household items; 2) Based on these textual descriptions, we employ the text-to-image generation model FLUX.1-dev[65] to synthesize highquality images depicting the tabletop layouts.We further augment the synthetic images by randomly applying fisheye distortion or compositing a robot gripper with adaptive brightness, making the visu-  The generated task instructions fall into two categories: 1) Visual grounding tasks [66][67][68], where the instruction implicitly refers to an object in the image through spatial relationships, attributes, or semantic features.The accompanying reasoning must reveal the object's explicit name and, optionally, its location; 2) Long-horizon tasks, where the instruction describes an extended, multistep objective.The reasoning must supply a high-level, step-by-step plan for completing the task.</p>
<p>Experiments</p>
<p>In this section, we evaluate OneTwoVLA through extensive real-world experiments, demonstrating its superior performance in versatile capabilities: long-horizon task planning, error detection and recovery, natural human-robot interaction, and visual grounding.Additionally, we show that co-training with our synthetic vision-language data enables OneTwoVLA to exhibit generalizable planning behaviors and open-world visual grounding capabilities on unseen scenarios and tasks.</p>
<p>Long-horizon Task Planning</p>
<p>Hardware.We utilize two robot platforms.The primary platform consists of a single 7-DoF Franka arm equipped with a parallel jaw gripper.A wrist-mounted GoPro camera with fisheye lens provides wide field-of-view observations.Most of our experiments are conducted using this setup.Additionally, we employ a dual-arm platform featuring two 6-DoF ARX arms with three cameras (two wrist and one base), primarily for generalizable planning experiments.See Appendix F for further details.</p>
<p>Long-horizon Tasks.We design three challenging long-horizon tasks (shown in Fig. 4, with more details provided in Appendix A), each requiring the robot to understand the scene, plan accordingly, accurately track task progress, and generate precise actions throughout execution.1) Tomato-Egg:</p>
<p>The robot pours oil followed by tomato and egg liquid into a cooking machine.Once cooking completes, it uses a spoon to scoop the scramble onto a plate-a contact-rich action demanding fine precision.2) Hotpot: Four plates containing various food items are presented with varying relative positions.The robot must sequentially dip beef and one vegetable type, precisely place them into a strainer, and finally lift the strainer.3) Cocktail: The robot mixes one of three cocktails (Mojito, Mountain Fuji, or Vodka Sunrise), each requiring 3-4 steps of ingredient pouring.The robot must distinguish between nearly ten visually similar ingredients and pour accurately.</p>
<p>Long-Horizon Planning</p>
<p>Task: Pass me an empty plate.</p>
<p>Task: Give me an icy cola.</p>
<p>Reasoning:</p>
<p>Plan: 1. Open fridge.2. Get icy cola.3. Close fridge.</p>
<p>Reasoning:</p>
<p>Plan: 1. Move grapes.2. Move apple.3. Pick up plate.</p>
<p>Generalizable Planning Baselines.We compare OneTwoVLA with two baselines: 1) a state-of-the-art VLA model π 0 [12].</p>
<p>To ensure fair comparison, we fine-tune π 0 on the same dataset used for training OneTwoVLA; and 2) a dual-system approach inspired by VILA [4], where we employ Gemini 2.5 Pro as the highlevel System Two to decompose complex instructions into sequences of atomic commands.We then annotate our dataset with atomic commands and fine-tune π 0 to act as the low-level System One.</p>
<p>Tomato-Egg Hotpot Cocktail Avg.
OneTwoVLA 85% 80% 95% 87% π0 70% 50% 50% 57%
Dual-System 55% 70% 65% 63%</p>
<p>Table 1: Evaluation results on long-horizon tasks.Each method is tested 20 trials for each task.OneTwoVLA excels in long-horizon task planning compared to baselines.</p>
<p>Experimental Results.As shown in Table 1, OneTwoVLA achieves an average success rate of 87% across the three challenging tasks, outperforming π 0 by 30% and the dual-system approach by 24%.OneTwoVLA consistently generates correct plans, accurately tracks task progress, and outputs precise actions.In contrast, lacking explicit reasoning and historical context, π 0 sometimes loses track of its current step -such as staying stuck at the initial position when preparing Mojito or repeatedly picking up beef in the Hotpot task.We also observe that explicit reasoning facilitates more fine-grained action learning; π 0 sometimes struggles to grasp ingredients precisely in Hotpot task or scoops too lightly in the Tomato-Egg task, whereas OneT-woVLA performs these delicate actions accurately.Regarding the dual-system approach, we found limitations arising from the lack of mutual awareness between the two systems' capabilities.System Two occasionally outputs atomic commands that are infeasible for System One to execute (e.g., instructing to add green onion in the Tomato-Egg task when none is present).Additionally, the significant inference latency of Gemini 2.5 Pro may prevent System Two from promptly updating its reasoning content, causing System One to encounter out-of-distribution states during execution.</p>
<p>Generalizable Planning.We investigate how co-training with large-scale vision-language (VL) data can improve OneTwoVLA's ability to generalize in task planning.Specifically, we collect additional demonstration data for various atomic skills (e.g., pick, place, open, etc.) across two robot platforms.We then co-train OneTwoVLA on these robot data together with the VL data synthesized by the pipeline described in Sec.3.3.During testing, OneTwoVLA receives instructions that never appear in the robot data (such as the task shown in Fig. 4, last column) and still exhibits strong generalization, transferring knowledge from VL data to robot control.For instance, the robot proactively searches for objects not visible (e.g., opening the refrigerator to find icy cola), and handles complex spatial relationships such as occlusion (e.g., first removing fruit from a plate when instructed with "Pass me an empty plate").Furthermore, OneTwoVLA exhibits scene-aware human intent understanding, handling abstract requests such as planning to prepare coffee for "Help me stay awake", kale juice for "I want something healthy", and blue mood cocktail for "I'm feeling down".</p>
<p>Error Detection and Recovery</p>
<p>Recovering from mistakes is a critical capability for general-purpose robots [69][70][71][72][73]. OneT-woVLA can detect errors in real-time, rapidly reason about recovery strategies, and subsequently generate corrective actions learned from collected robot recovery data.For example, in the Hotpot task, the robot occasionally fails to grasp the strainer due to misalignment.In that case, OneT-woVLA reasons to retract, reposition to align with the strainer and try grasping again, subsequently succeeding in lifting it up.In contrast, π 0 frequently ignores errors and continues to lift the gripper despite not having grasped the strainer.In the Tomato-Egg task, sometimes the oil bottle slips from the gripper while pouring.OneTwoVLA recognizes the error, reasons to adjust its grasp for increased firmness and retry the action.However, the dual-system approach fails to respond promptly due to latency issues.System Two only alerts that the oil bottle is not grasped after the robot has already reached the pouring pose, by which time recovery is hard because the robot has entered an out-of-distribution state.</p>
<p>Natural Human-Robot Interaction</p>
<p>To deploy robots in human-centric scenarios, the ability to interact naturally with humans is indispensable [74][75][76][77].Due to its adaptive nature and explicit reasoning process, OneTwoVLA is able to engage with humans in a natural way -seamlessly handling human interventions and proactively seek clarification when faced with ambiguities.For example, in the Hotpot task, when a human interrupts by requesting, "Could you also dip another vegetable for me?" OneTwoVLA immediately responds by clarifying, "Sure!Would you like green bok choy, enoki mushrooms, or cabbage?"In the Cocktail task, when the robot is preparing a Vodka Sunrise and the human interrupts with, "I don't want orange vodka, I want lemon-flavored one," OneTwoVLA immediately reasons that it needs to put down the orange vodka, retrieve the lemon vodka, and generate action sequences that align with the human's intent.In contrast, the dual-system approach frequently loses context during interaction and struggles to maintain a coherent reasoning process, merely picking up the lemon vodka without continuing to prepare the cocktail in the example above.π 0 is unable to engage in such language-based human interaction because it cannot output textual reasoning content.</p>
<p>Enhanced Visual Grounding</p>
<p>Grounding objects in language instructions to the visual world is a prerequisite for robots to accomplish more complex tasks.We categorize visual grounding into three key aspects [66][67][68]78]: spatial relationships, object attributes, and semantic features.To validate OneTwoVLA's effectiveness in these aspects, we design experiments where instruction following requires non-trivial object grounding capabilities.Furthermore, to demonstrate the impact of our synthetic vision-language data, we conduct experiments in open-world settings where diverse items and environments pose additional challenges.The specific experimental settings are described below (shown in Fig. 5):</p>
<p>1) Single-Env: Four objects are randomly arranged on a tabletop in a single environment.We collect 50 picking-up demonstrations for each object using the UMI [79] device, totaling 200 demonstrations.For testing, we perform 40 trials per method in the same environment using the same four objects.2) Open-World: We collect demonstrations in 16 diverse in-the-wild environments, totaling 933 valid demonstrations using the UMI device.Each demonstration involves moving the gripper to a randomly selected object within the scene, collectively including 180 distinct household items.For testing, we evaluate each method across 8 unseen environments, testing 5 times per environment, each time randomly selecting one from 20 objects: 5 objects seen in robot data, 10 objects unseen in robot data but present in synthetic vision-language data, and 5 objects unseen in either dataset.</p>
<p>Reasoning:</p>
<p>Black mouse is for computer tasks.I need to pick up black mouse to the left of cube, which has a arching shape that fits the palm.</p>
<p>Instruction:</p>
<p>"Get the object for computer tasks."</p>
<p>Instruction:</p>
<p>"Hand me the woven object with a wide brim."</p>
<p>Reasoning:</p>
<p>Straw hat is the woven object with a wide brim.I need to pick up straw hat to the left back of teddy bear, which provides sun protection.</p>
<p>Single-Env Open-World</p>
<p>Figure 5: Illustrations of visual grounding tasks.In the Single-Env setting, we provide task instructions that require understanding of spatial relationships, object attributes, or semantic features.In the Open-World setting, we further evaluate the model's generalizable visual grounding capabilities.</p>
<p>In both settings, training and test instructions refer to target objects using their names or through spatial relationships, attributes, or semantic features.Our annotated reasoning explicitly identifies the target object's name and includes additional information about it.We compare three methods: 1) OneTwoVLA-VL: Trained on robot data and 16,000 synthetic vision-language data.2) OneTwoVLA: Trained exclusively on robot data for learning reasoning and acting.3) π 0 : Trained solely on robot data to directly predict actions based on instructions.Appendix A.3 offers more details.</p>
<p>Explicit reasoning facilitates visual grounding.In the Single-Env setting, as shown in Table 2, OneTwoVLA achieves a success rate of 78%, significantly outperforming π 0 , which has a success rate of only 5%.In most cases, OneTwoVLA accurately interprets spatial relationships, object attributes, and semantic features described in the instructions, reasons about the correct object, and then successfully picks up the target object.In stark contrast, π 0 consistently fails to comprehend the instructions, even when the target object is explicitly named.π 0 typically extends the gripper forward aimlessly or randomly picks up the closest object.This clear performance gap demonstrates that explicitly learning to reason helps the model truly understand the visual world rather than attempting to find shortcuts to overfit actions.Moreover, we find that the reasoning content also aids the model in fitting actions, as evidenced by π 0 's action mean squared error (MSE) on the validation set being 62% higher than OneTwoVLA's.</p>
<p>Single-Env Open-World</p>
<p>OneTwoVLA-VL 88% 73%</p>
<p>OneTwoVLA 78% 8% π0 5% 3%</p>
<p>Table 2: Evaluation results on visual grounding tasks.OneTwoVLA exhibits strong visual grounding capabilities, attributed to its explicit reasoning.Moreover, our synthetic visionlanguage data significantly enhances the model's generalization.</p>
<p>Reasoning-centric vision-language data enables generalizable visual grounding.In the Open-World setting, OneTwoVLA-VL achieves a 73% success rate, significantly outperforming both OneTwoVLA and π 0 .In most cases, OneTwoVLA-VL can correctly handle objects unseen in the robot data but present in vision-language (VL) data, effectively transferring commonsense knowledge from VL data to the robot policy.Remarkably, OneTwoVLA-VL generalizes even to novel objects that appear in neither the robot nor VL training data (e.g., Sprite, GoPro).We attribute this exceptional generalization capability to VL data co-training, which better activates web knowledge already encoded in the pretrained vision-language model.In contrast, OneTwoVLA and π 0 frequently exhibit aimless reaching behaviors -even for objects present in the training data -indicating that they merely overfit to action training data without developing genuine understanding of the visual environment in this complex and diverse setting.</p>
<p>Conclusion</p>
<p>In this paper, we present OneTwoVLA, a single unified model capable of both reasoning and acting, and adaptively switching between these two modes.This synergy is enabled by our meticulously designed framework and reasoning-enriched robot data curation.Moreover, we propose a scalable pipeline for synthesizing embodied reasoning-centric vision-language data to further enhance the model's reasoning and generalization capabilities.Extensive experiments demonstrate OneT-woVLA's superior performance across four key abilities: long-horizon task planning, error detection and recovery, natural human-robot interaction, and generalizable visual grounding.</p>
<p>Limitations</p>
<p>Despite the promising results of OneTwoVLA, there are several limitations that future work can address.First, OneTwoVLA's reasoning content is based on our careful manual annotations, while in the field of large language models, reinforcement learning has been widely adopted to enhance reasoning ability [80][81][82].Future work could explore similar approaches to improve the reasoning capabilities of VLA models.Second, although our adaptive framework allows the model to reason only at a few critical steps during task execution, the robot still needs to pause for two to three seconds while reasoning occurs.Future research could explore the design of asynchronous architectures, enabling simultaneous reasoning and action generation.Finally, due to resource constraints, we only investigate the effect of high-quality synthetic vision-language data on VLA reasoning capabilities.Future work could explore the impact of vision-language data from various sources.</p>
<p>A Tasks and Evaluations</p>
<p>In this section, we provide a detailed description of the tasks and evaluations.</p>
<p>A.1 Long-horizon Tasks Fig. 6 shows the complete execution progress of the three long-horizon tasks.Detailed descriptions of these tasks are as follows:</p>
<p>1) Tomato-Egg: The robot first pours oil, then tomato and egg liquid into a cooking machine.Once cooking is finished, the robot picks up a spoon hanging on a rack, scoops out the tomatoegg scramble, transfers it onto a plate, and finally places the spoon into the cooking machine.We observe that sometimes the robot fails to grip the oil bottle firmly enough, causing it to slip from the gripper.We collect dedicated recovery data for re-grasping the oil bottle more securely after it has slipped.This enables the robot to automatically perform this recovery if it encounters a bottle slip during testing.We collect 200 robot demonstrations for this task.</p>
<p>2) Hotpot: Four plates containing beef, green bok choy, enoki mushrooms, and cabbage are placed on a table with randomized relative positions.A hotpot with a strainer is positioned to the right of the plates.For each test, the human instructs the robot to dip beef and one type of vegetable.The robot must accurately pick up the ingredients sequentially, place them in the strainer, wait for them to cook, and then lift the strainer.Notably, for OneTwoVLA and the dual-system approach, in 10 of the experiments, the initial instruction is only to dip the beef.While waiting for the beef to cook, the human interacts with the robot saying,"Could you also dip another vegetable for me?", requiring the robot to ask, "Sure!Would you like green bok choy, enoki mushrooms, or cabbage?"Following the human's specification, the robot then proceeds to dip the requested vegetable.This interaction step is omitted for π 0 due to its lack of text output capabilities.Furthermore, we observe instances where the robot fails to grasp the strainer.To address this, we specifically collect recovery data for correcting misaligned grasps.This enables the robot to automatically perform this recovery if it fails to pick up the strainer during testing.We collect 600 robot demonstrations for this task.</p>
<p>3) Cocktail: The robot is instructed to prepare one of three cocktails: Mojito, Mountain Fuji, or Vodka Sunrise.Each cocktail requires pouring 3-4 different ingredients.For OneTwoVLA and the dual-system approach, in 10 trials, the initial human instruction is general: "Make me a cocktail."</p>
<p>The robot must clarify by asking: "Which cocktail would you like?", and then proceed based on the human's specific cocktail choice.This interaction step is again omitted for π 0 .Additionally, during 3 separate Vodka Sunrise trials, the human interrupts with, "I don't want orange vodka, I want lemon-flavored one," requiring the robot to put down the orange vodka and pick up lemon vodka instead.We collect 100 robot demonstrations for each type of cocktail, totaling 300 demonstrations.We collect 2,000 robot demonstrations using the single-arm Franka system and dual-arm ARX system.Each demonstration belongs to one category of atomic skill, including pick, place, move, open, close, and pour.The task instructions and corresponding reasoning contents for these demonstrations focus on short-horizon atomic skills.Training solely on this data limits the model's generalizable long-horizon planning capabilities.OneTwoVLA overcomes this limitation through co-training with our synthesized embodied reasoning-centric vision-language data, which equips it to generalize to previously unseen tasks.Fig. 7 shows the complete execution progress of these unseen tasks.Detailed descriptions of these tasks are as follows:</p>
<p>A.2 Generalizable Planning Tasks</p>
<p>1) Get Icy Cola: The instruction is "Get me a can of icy cola."The challenge is that a cola can is not directly visible in the scene.The robot must infer that "icy cola" implies the cola is stored in the fridge and therefore plan the necessary steps to open the fridge, locate the cola, and retrieve it.</p>
<p>2) Empty Plate: The instruction is "Pass me an empty plate".However, the plate in the scene is not empty, as it contains apples and grapes.The robot needs to remove each fruit from the plate before finally picking up the empty plate.</p>
<p>3) Tool Use: The instruction is "Pick up the cocoa powder can, which is out of reach".The primary difficulty here is that the target object is not within the robot's direct reach.The robot must recognize the need for a tool (a nearby stick), plan to first grasp the stick, use it to sweep the distant cocoa powder can within reach, and only then proceed to pick up the can.</p>
<p>4) Prepare Drinks: The robot needs to plan and prepare appropriate drinks based on user intent: such as coconut latte for "Help me stay awake," kale juice for "I want something healthy," and a blue mood cocktail for "I'm feeling down."This task requires scene-aware user intent understanding capability.</p>
<p>A.3 Visual Grounding Tasks</p>
<p>Task descriptions can be found in Sec.4.4.In the Single-Env setting, each robot demonstration is paired with 11 instruction-reasoning pairs.These instructions refer to target objects using their names (2 instances), spatial relationships (3 instances), attributes (3 instances), or semantic features (3 instances).In the Open-World setting, each demonstration includes a total of 17 instruction-reasoning pairs, broken down as 2 using direct names, 5 using spatial relationships, 5 using attributes, and 5 using semantic features.All instruction-reasoning pairs are first generated with Gemini 2.5 Pro and then verified by human annotators.</p>
<p>During testing, we evaluate each method 40 times in both settings.This consists of 10 tests for each reference type.Table 3 presents the experimental results broken down by these four types.</p>
<p>Here we list the objects used in visual grounding tasks.The Single-Env task uses four objects: blue cube, eggplant toy, coconut water bottle, and black mouse.For the Open-World task evaluation, we use the following objects (shown in Fig. 8): 1) 5 objects seen in robot data: flower, mouse, cardholder, tissue, and glasses case.</p>
<p>2) 10 objects unseen in robot data but present in synthetic vision-language data: globe, teddy bear, straw hat, binoculars, trowel, croissant, map, magnifying glass, VR headset, lantern.</p>
<p>3) 5 objects unseen in either dataset: GoPro, Sprite, Starbucks Coffee, HDMI cable, Captain America model.</p>
<p>Objects seen in robot data</p>
<p>Objects seen in vision-language data Objects unseen in either dataset   Instruction: Give me the object behind the seashell.</p>
<p>Reasoning: I need to pick up the lantern to the left of the croissant, which is black with a metal structure, and holds light and can be carried.</p>
<p>Instruction: Hand me the object which is black and round.</p>
<p>Reasoning: I need to pick up the alarm clock sitting on the left side of the table.</p>
<p>Instruction: I need something to get a closer look at this fine print.</p>
<p>Reasoning: I need to pick up the magnifying glass, which is to the right of the red pepper, and has a wooden handle and a circular lens.</p>
<p>Table 9: Reasoning examples for Open-World visual grounding.</p>
<p>C Synthetic Vision-Language Data Examples</p>
<p>Our 16,000 synthetic images are entirely annotated by Gemini 2.5 Pro, without any human intervention.For 6,000 of these images, we generate visual grounding tasks.Each of these images is annotated with 17 instruction-reasoning pairs, with the instructions referring to objects using their direct names (2 instances), spatial relationships (5 instances), attributes (5 instances), and semantic features (5 instances).For the remaining 10,000 images, we annotate a long-horizon planning task along with a corresponding high-level, step-by-step plan for task completion.We also attempt to use GPT-4o for annotating our synthetic images but find its spatial understanding to be weak.We therefore use Gemini 2.5 Pro, which demonstrates strong spatial reasoning capabilities.</p>
<p>We present illustrative examples synthesized by our embodied reasoning-centric visual-language data synthesis pipeline.Table 10 and Table 11 show samples of synthesized data for visual grounding and long-horizon tasks, respectively, each including textual descriptions of tabletop layouts, synthesized images corresponding to these descriptions, and the accompanying instruction-reasoning pairs (for visual grounding example, we only show one pair for each of the four reference types).Fig. 11 illustrates the effects of applying fisheye distortion or compositing a robot gripper with adaptive brightness to the synthetic images.</p>
<p>Moreover, Fig. 12, 13, and 14 detail the specific prompts used with Gemini 2.5 Pro throughout our pipeline: Fig. 12 shows the prompt for generating diverse tabletop descriptions, while Fig. 13 and 14 illustrate the prompts for generating visual grounding and long-horizon task instructions and their associated reasoning, respectively.</p>
<p>Tabletop layout description: The entire scene is sharp and focused.From a 45-degree overhead view, observe a medium-sized, rectangular driftwood table with a natural, uneven texture.On it are displayed seashells, a coral sculpture, a nautical map under a magnifying glass, a vintage lantern, and a model sailboat, backed by a beach house and ocean view.</p>
<p>Direct instruction: Can you pass me the map?Reasoning: I need to pick up the map, located towards the center of the table.</p>
<p>Spatial instruction: Get me the object sitting on the back right area of the paper item.</p>
<p>Reasoning: I need to pick up the miniature sailboat model, which is placed on the map near the top right corner of the map.</p>
<p>Attribute instruction: I need the item with the complex branching structure.</p>
<p>Reasoning: I need to pick up the large piece of coral positioned at the left side of the table.</p>
<p>Semantic instruction: Could you pass me the device used for seeing small details much larger?Reasoning: I need to pick up the magnifying glass situated on the map near the bottom right corner of the map.</p>
<p>Tabletop layout description: The entire scene is sharp and focused.From a 45-degree overhead view, observe a large, oval, stone table with a rough texture, set outdoors in a mountain retreat.On it, a hearty meal of grilled trout, wild rice, steamed vegetables, a jug of water, and a lantern.The rugged outdoor setting is enhanced by the surrounding pine trees and the fading twilight.</p>
<p>Direct instruction: Can you hand me the lantern?Reasoning: I need to pick up the black lantern situated at the back left of the rock table.</p>
<p>Spatial instruction: Pass me the item directly to the left of the grilled trouts.</p>
<p>Reasoning: I need to pick up the fork sitting to the immediate left of the grilled trouts.</p>
<p>Attribute instruction: Please give me the plate with the long, silvery food item.</p>
<p>Reasoning: I need to pick up the plate containing the two cooked fish positioned in front of the vegetable bowl.</p>
<p>Semantic instruction: Hand me that grain-based side dish, please.</p>
<p>Reasoning: I need to pick up the plate of wild rice located at the back left, to the left of the vegetable bowl.</p>
<p>Tabletop layout description: The entire scene is sharp and focused.From a 45-degree overhead view, observe a rectangular maple table with a honey finish, placed in a sunny conservatory.A watering can, a terracotta pot with blooming flowers, a gardening book, gloves, and a small birdhouse are charmingly scattered on it.Glass panels reveal a lush garden outside.</p>
<p>Direct instruction: Can you pass me the watering can?</p>
<p>Reasoning: I need to pick up the green watering can positioned on the back left side of the table.</p>
<p>Spatial instruction: I want the item located on the front right side of the table.</p>
<p>Reasoning: I need to pick up the pair of gardening gloves, situated on the front right area of the table surface.</p>
<p>Attribute instruction: I need the object that's open and has pages with text.</p>
<p>Reasoning: I need to pick up the book, which is open and located between the watering can and the brown gloves.</p>
<p>Semantic instruction: Please pass me the item that could provide shelter for small birds.</p>
<p>Reasoning: I need to pick up the wooden birdhouse, sitting on the back right side of the table.Table 10: Examples of synthetic vision-language data for visual grounding tasks.</p>
<p>Tabletop layout description: The entire scene is sharp and focused.From a 45-degree overhead view, directly facing the cluttered table, observe a rustic wooden surface covered in vibrant fresh ingredients like tomatoes, peas, herbs, and spices.A cutting board, knife, spoon, and book are laid out, bathed in bright light, against a backdrop of softly visible shelves with plants.</p>
<p>Instruction: Prepare a Fresh Herb and Tomato Relish.Reasoning: Plan: 1. Chop the tomatoes using the knife and cutting board.2. Chop the fresh basil and parsley.3. Combine the chopped tomatoes, basil, and parsley in a bowl.4. Add the green peas and the dried herbs and spices from the piles on the board to the bowl. 5. Mix all ingredients thoroughly using the wooden spoon.Tabletop layout description: The entire scene is sharp and focused.From a 45-degree overhead view, directly facing the cluttered table, observe a low Japanese tea table with a woven tatami mat beneath it, showcasing a tea set, a small bonsai tree, and a scroll.The scene plays out indoors, in a serene, minimalist room, emphasizing peaceful tranquility.</p>
<p>Instruction: Prepare and serve tea.Reasoning: Plan: 1. Pick up the teacup and place it next to the teapot.2. Fill the teapot with hot water.3. Pour tea from the teapot into the teacup.4. Place the teapot on the table.5. Serve the teacup to the guest.Tabletop layout description: The entire scene is sharp and focused.From a 45-degree overhead view, directly facing the cluttered table, observe a small potted plant beside an empty terracotta pot.A soil bag, a silver trowel, and various tools are scattered across the worn wooden surface.The scene takes place outdoors, in a garden full of flowers, bathed in warm afternoon light.</p>
<p>Instruction: Repot the small plant into the larger terracotta pot.Reasoning: Plan: 1. Remove the plant from its current small pot. 2. Add soil from the bag into the bottom of the larger terracotta pot using the trowel.3. Place the plant in the center of the larger pot. 4. Fill the remaining space in the larger pot with soil from the bag using the trowel.5. Use the trowel to gently firm the soil around the base of the plant.</p>
<p>Tabletop layout description: The entire scene is sharp and focused.From a 45-degree overhead view, directly facing the cluttered table, observe a white folding table at an outdoor market.Assorted fruits, vegetables, and price tags cover the table.The bright, natural light enhances the colors of the produce.</p>
<p>Instruction: Prioritize fresh produce to boost your daily vitamin intake.Reasoning: Plan: 1. Pick up some tomatoes and place them in the basket.2. Pick up some lemons and place them in the basket.3. Pick up some oranges and place them in the basket.4. Carry the basket to checkout.</p>
<p>Table 11: Examples of synthetic vision-language data for long-horizon tasks.Create 30 detailed 50-word prompts that describe scenes from a 45-degree top-down view of a table.The table should have a clear description of its shape, size, texture, and color.On the table, place around five objects, describing each object in detail and their positions relative to each other (e.g., object A is placed above object B).The background and environment should be clearly defined, either indoor or outdoor, and the scene should be rich in detail.Ensure there is no blurriness or out-of-focus areas, and the lighting and atmosphere should enhance the realism.</p>
<p>Please ensure each of the following prompts is unique and creatively different, varying the table, objects, environment (like indoor or outdoor), lighting, and overall atmosphere.</p>
<p>Each prompt should start with "The entire scene is sharp and focused.From a 45-degree overhead view, observe ...", followed by a description of the table's <strong>COLOR</strong> (this could be diverse across different prompts), shape, texture, size, etc.In the provided image, you will notice several items placed on a table.Your task is to come up with 17 different instructions based on these items.These tasks will be categorized into three types based on object properties: spatial, semantic, and attribute.Spatial pertains to the object's position in space (e.g., on top of the plate, to the right of the book, or at the bottom right corner of the table).</p>
<p>Semantic refers to the object's general, high-level meaning (e.g., sushi is a type of Japanese food, a kettle is used for boiling water, a book is meant for reading, etc.).</p>
<p>Attribute is concerned with the object's specific features or characteristics (e.g., a ball is round, a handle is made of wood, etc.).</p>
<p>For the objects on the table in the image, your task is to create 17 instructions, which can either directly ask for an object or describe it using its spatial, semantic, or attribute properties (e.g., "pass me the item on the plate," "give me something that helps with drying hair," or "hand me the yellow object").</p>
<p>Each task is essentially a "pick" task, but the instruction should sound natural and realistic.</p>
<p>After giving the instruction, provide a more specific description that starts with "I need to pick up," and then clearly name the object, possibly with some additional spatial details to help locate it.</p>
<p>When describing a location, try to be as accurate as possible.Avoid using vague descriptions such as "in the middle/center of the table," "near," "beside," or "next to," as these could apply to many objects.Instead, use precise relative positioning, such as "to the left front of an object," "on top of an object," "between object A and object B," "to the right back of an object," or "behind an object."</p>
<p>When giving instructions, avoid mentioning the specific name of the object and instead use pronouns like "item," "object," or "device."</p>
<p>When providing attribute instructions, only list 1 or 2 properties of the object.</p>
<p>Your Tasks: First, generating 2 tasks with direct references to the object name.Then, generate 5 tasks <strong>only</strong> related to spatial properties (focusing on the location of the objects).Next, generate 5 tasks related to semantic properties (focusing on the general meaning or purpose of the objects).Finally, generate 5 tasks related to attribute properties (focusing on specific features of the objects).In the given image, there is a table with several items placed on it in a messy manner.</p>
<p>Your task is to first imagine a long-horizon task based on the items in the image (such as organizing the table, making a sandwich, etc.).This task needs to be relatively long-term, meaning it should require about several steps to complete.</p>
<p>The second step is to provide a plan, where each step is a brief action description (e.g., Pick up sth and place it somewhere, Close sth, Open sth, Move sth to somewhere, etc.).• Predict [BOR] and the updated reasoning content R based on the latest reasoning content R.</p>
<p>• Predict [BOA] and actions based on the updated reasoning content R.</p>
<p>• Predict actions based on the latest reasoning content R without supervising [BOA].This is to prevent incorrect action prediction if the model fails to update the reasoning promptly during deployment.</p>
<p>2) Acting intervals, OneTwoVLA learns to:</p>
<p>• Predict [BOA] and actions based on the latest reasoning content R.</p>
<p>• (Optional) Predict [BOR] based on outdated reasoning without supervising the reasoning content.This is included because we observe that during deployment, the model sometimes fails to enter the reasoning mode.Since predicting decision tokens is essentially a binary classification problem, and acting intervals are typically significantly longer than reasoning intervals, the model predominantly learns to predict [BOA], leading to an imbalanced classification problem.This optional training helps to increase the proportion of [BOR] predictions.</p>
<p>Additionally, it is important to note that reasoning interval during training is designed to encourage the model to learn the reasoning process more effectively.In real-world deployment, the robot only reasons at a small number of steps (rather than continuous intervals), ensuring that the overall operational efficiency is almost unaffected.</p>
<p>D.2 Policy Training</p>
<p>As shown in Sec.3.1, we use π 0 as our base model.For each task, we train the model for 30,000 steps on 8xH100 GPUs, requiring approximately 10 hours.We adopt training hyperparameters from π 0 .We make two modifications to the original π 0 's input.Firstly, we use the current image I t and the reference image I ref as image observations.We incorporate I ref because the textual scene descriptions in reasoning may become outdated as the task progresses (e.g., an object's position described relative to the gripper becomes invalid upon gripper movement).Including I ref , which corresponds to the image observation for the current reasoning content, helps prevent model confusion that might arise from potentially outdated textual descriptions.Second, we input not only the current robot proprioceptive states but also the proprioceptive states from 0.05 and 0.25 seconds earlier.This temporal context allows the model to generate more consistent and smooth actions during execution.</p>
<p>D.3 Deployment</p>
<p>In real-world deployment, we use the temporal ensemble [83] technique to ensure smooth action execution.Specifically, in acting mode, the policy generates temporally overlapping action sequences every 0.2 seconds.At any given timestep, multiple predicted actions are averaged using exponential weighting to determine the actual executed actions.</p>
<p>Table 12 lists the computation time for π 0 , along with the computation time for OneTwoVLA in acting mode for varying input token counts and in reasoning mode for varying output token counts, all of which are tested while processing two image inputs on an NVIDIA 4090 GPU.In acting mode, although OneTwoVLA has additional reasoning content as input and outputs an extra [BOA] compared to π 0 , this has minimal impact on computation time and remains well below 0.2 seconds, thus execution efficiency is not affected in this mode.In reasoning mode, when the reasoning token count is low (less than 20 tokens), execution efficiency is unaffected; however, when reasoning content is lengthy (exceeding 100 tokens), the robot needs to pause for a few seconds.Nevertheless, reasoning only occurs at a few critical moments, resulting in minimal impact on overall execution efficiency.For example, in one trial of the Tomato-Egg task, the entire long-horizon task takes 183 seconds, with reasoning occurring 5 times, totaling 16 seconds of reasoning time, which accounts for 8.7% of the total duration.Similarly, in one trial of the preparing Mountain Fuji task, the entire long-horizon task takes 135 seconds, with reasoning occurring 5 times, totaling 14 seconds of reasoning time, which accounts for 10.4% of the total duration.</p>
<h1></h1>
<p>E.2 OneTwoVLA Produces Reasoning-Compliant Actions</p>
<p>Our experiments show that the actions generated by OneTwoVLA consistently align with its reasoning, even when the reasoning itself is incorrect.This finding is similar to observations in previous work [49].For example, in the Hotpot task, if OneTwoVLA occasionally reasons incorrectly about food locations, it proceeds to reach toward those incorrect positions.Similarly, in the Open-World experiment, OneTwoVLA moves to the object specified in its reasoning, even if that object does not align with the instruction.This indicates that OneTwoVLA's cognition and behavior are highly unified, showcasing synergistic reasoning and acting.Additionally, this interesting phenomenon may indicate that improving the model's reasoning ability (e.g., through additional vision-language data, using more powerful VLM as the base model, or more precise reasoning annotations) may contribute to generating more appropriate actions.</p>
<p>F Hardware Setup</p>
<p>Figure 16: Robot platform overview.We employ two robot platforms: a single-arm Franka system (left) and a dual-arm ARX system (right).</p>
<p>We utilize two robot platforms.The primary platform (Fig. 16, left) is a single 7-DoF Franka arm equipped with a Weiss WSG-50 parallel-jaw gripper.A wrist-mounted GoPro camera with fisheye lens provides wide-angle observations.The arm is mounted on a custom height-adjustable table that can be pushed by a person-while not autonomous, this mobility allows us to evaluate the policy beyond traditional laboratory environments.The action space is 7-dimensional (6-DoF end-effector pose plus gripper width).Expert demonstrations for this platform are collected using UMI [79].</p>
<p>The second platform (Fig. 16, right) features two 6-DoF ARX arms with parallel-jaw grippers and a three-camera system (two wrist-mounted and one base-mounted).It also includes a holonomic wheeled base and a 1-DoF torso lift mechanism, though these components have not yet been utilized in our experiments.The resulting action space is 14-dimensional (2 × 7).Expert demonstrations are collected via teleoperation using a Meta Quest headset.</p>
<p>Figure 2 :
2
Figure 2: Inference flow of OneTwoVLA in two modes.</p>
<p>Figure 3 :
3
Figure 3: Left.Example of robot data with reasoning content.The reasoning content comprises a scene description, a high-level plan, a historical summary, and the next-step instruction.Interaction texts (e.g., the robot question and the human answer) are appended after the instruction.Right.Examples of synthetic embodied reasoning-centric vision-language data.The top two examples illustrate visual grounding tasks, while the bottom two demonstrate long-horizon tasks.More examples are provided in Appendix C. als more closely resemble real robot observations; 3) Finally, we utilize Gemini 2.5 Pro again to generate task instructions and corresponding reasoning contents for each synthesized image.Through this pipeline, we automatically generated 16,000 data samples, with examples shown in Fig. 3 right.</p>
<p>Beef and mushrooms are in strainer.But I fail to grasp the strainer.Plan: 1.✅ Dip beef.2.✅ Dip mushrooms.3. Cook, then lift strainer.Now I need to: Retract, align with strainer, retry to grasp.Plan: 1.✅ Add syrup.2.✅ Add juice.3. Add lemon vodka.Now I need to: Put orange vodka down, add lemon vodka.Oil is located at … Cooking machine is at … Tomatoes and eggs are at … Spoon is at … Plan: 1. Add oil to cooking machine.2. Add tomatoes and eggs.3. Grab spoon and wait for cooking.4. Scoop out dish.Now I need to: Add oil to cooking machine.</p>
<p>Figure 4 :
4
Figure 4: Task illustrations and reasoning examples.In the three leftmost columns, we present three challenging, long-horizon manipulation tasks.Completing these tasks requires not only planning abilities, but also error detection and recovery capabilities, as well as the the ability to interact naturally with humans.In the rightmost column, we demonstrate two tasks drawn from our experiments on generalizable planning.For every task, we include a sample of the model's reasoning content.See Appendix B for additional reasoning examples.</p>
<p>Figure 6 :
6
Figure 6: Execution processes of three long-horizon tasks: Tomato-Egg, Hotpot, and Cocktail (exemplified by Mountain Fuji preparation).</p>
<p>Figure 7 :
7
Figure 7: Execution processes of four generalizable planning tasks: Get Icy Cola, Empty Plate, Prepare Drinks (exemplified by kale juice preparation) and Tool Use.</p>
<p>Fig. 9
9
Fig.9displays the 16 training environments for the Open-World task, while Fig.10shows the 8 evaluation environments.</p>
<p>Figure 8 :
8
Figure 8: Objects for Open-World task evaluation.</p>
<p>Figure 9 :
9
Figure 9: Training environments for Open-World visual grounding task.</p>
<p>Figure 10 :
10
Figure 10: Evaluation environments for Open-World visual grounding task.</p>
<p>Figure 11 :
11
Figure 11: Augmentations for our synthetic images.From left to right: original synthetic images, synthetic images with fisheye distortion, synthetic images with a robot gripper composited with adaptive brightness, and synthetic images with both fisheye distortion and compositing a robot gripper with adaptive brightness.</p>
<p>Use the following format to separate each prompt: <strong>START Prompt <Prompt ID></strong> [Detailed description of the scene] <strong>END Prompt <Prompt ID></strong> Repeat this process for 30 distinct prompts.Request to generate all at once.</p>
<p>Figure 12 :
12
Figure 12: Prompt used to generate tabletop descriptions.</p>
<p>For</p>
<p>each task, follow this format: <strong>Start Task <task id></strong> Instruction: ... I need to pick up ... <strong>End Task <task id></strong> Separate these 4 types of tasks by ### Tasks Related to Spatial Properties ### Tasks Related to Semantic Properties ### Tasks Related to Attribute Properties</p>
<p>Figure 13 :
13
Figure 13: Prompt used to generate visual grounding task instructions and reasoning.</p>
<p>Task** If you cannot think of an interesting task, simply output "Fail to think of a plan."Note that the instruction and plan should be brief and precise.</p>
<p>Figure 14 : 1 )
141
Figure 14: Prompt used to generate long-horizon task instructions and reasoning.</p>
<p>Table 3 :
3
Experimental</p>
<p>results for the visual grounding tasks.Results are broken down by the four instruction reference types: direct names, spatial relationships, object attributes, and semantic features.</p>
<p>Table 12 :
12
input tokens # output tokens computation time Computation times of π0 and OneTwoVLA.π0'sinputtokens consist solely of instruction ℓ.OneTwoVLA's input tokens are typically longer, including instruction and latest reasoning content (ℓ and R).In acting mode (OneTwoVLA-Act rows), OneTwoVLA's output token is a single[BOA].While in reasoning mode (OneTwoVLA-Reason rows), OneTwoVLA outputs [BOR] and updated reasoning content, R. We showcase computation times when its output token length is 20, 100, and 200.
π 0200.082 sOneTwoVLA-Act-202010.102 sOneTwoVLA-Act-20020010.104 sOneTwoVLA-Reason-20200200.853 sOneTwoVLA-Reason-1002001002.346 sOneTwoVLA-Reason-2002002004.361 s
AcknowledgmentsThis work is supported by the National Key R&amp;D Program of China (2022ZD0161700), National Natural Science Foundation of China (62176135), Shanghai Qi Zhi Institute Innovation Program SQZ202306 and the Tsinghua University Dushi Program, the grant of National Natural Science Foundation of China (NSFC) 12201341.We would like to express my sincere gratitude to Tong Zhang, Chuan Wen, Weirui Ye, Weijun Dong, Shengjie Wang, Chengbo Yuan, Boyuan Zheng, Haoxu Huang, Yihang Hu and Yuyang Liu for their valuable discussions throughout this research.My thanks also extend to the ARX team for their prompt assistance with the ARX robot hardware issues.AppendixPlease visit our anonymous website to view robot rollout videos: https://one-two-vla.github.io.4), generalizable planning (Table5), error detection and recovery (Table6), natural human-robot interaction (Table7), Single-Env visual grounding (Table8), and Open-World visual grounding (Table9).Instruction: Pick up the black mouse.Reasoning: I need to pick up the black mouse, which is to the left back of the eggplant, has a smooth, arching shape that fits the palm, and is a key component of a typical office workstation.Instruction: Pick up the object which is to the left back of the coconut water bottle.Reasoning: I need to pick up the blue cube, which is identified by its blue shade, and is a solid form useful for creating stable structures.Instruction: Pick up the object which exhibits a purple coloration.Reasoning: I need to pick up the eggplant, which is to the right of the coconut water bottle, and is a common ingredient in ratatouille.Instruction: Pick up the object which is a packaged drink.Reasoning: I need to pick up the coconut water bottle, which is to the left back of the blue cube, and has a cylindrical form.E Other FindingsInstruction: "Grasp cube."Instruction: "Grasp bottle."Instruction: "Grasp cube."Instruction: "Grasp bottle."Reasoning:There are two cubes on the table.I choose the left one to grasp.Reasoning:There are two bottles on the table.I choose the left one to grasp.Execution: Execution:Reasoning:There are two cubes on the table.I choose the right one to grasp.Execution: Execution:Reasoning:There are two bottles on the table.I choose the right one to grasp.When the instruction doesn't specify grasping the left or right object, OneTwoVLA can reason to grasp either the left or the right object, producing multi-modal actions.E.1 OneTwoVLA Produces Multi-Modal ActionsIn this section, we design experiments to show OneTwoVLA's capability to produce multi-modal actions.Tasks and Evaluations.Two identical cubes are symmetrically placed on a table, each with an identical bottle positioned symmetrically behind it.Using the UMI device, we collect 50 demonstrations for each of these four objects (totaling 200 demonstrations).Each demonstration instruction is either "Grasp the cube" or "Grasp the bottle," without specifying left or right.During testing, the object positions and the robotic gripper's initial pose remain fixed.Each method is tested 20 times per instruction.Comparative Methods. 1) OneTwoVLA: For each demonstration, we explicitly include disambiguating reasoning content (e.g., specifying picking up the left or right object) to resolve the ambiguity.2) π 0 : The model receives the original instruction directly, without explicit disambiguation.Experimental Results.As shown in Fig.15, OneTwoVLA demonstrates multi-modal action capability by alternating between reasoning to grasp objects from either side.Specifically, in the "grasp cube" experiment, OneTwoVLA grasps the left cube 9 times and the right cube 11 times.In the "grasp bottle" experiment, it grasps the left bottle 8 times and the right bottle 12 times.OneT-woVLA achieves this balanced left-right performance because its reasoning process is probabilistic, which means the model can sample different decisions (such as whether to grasp from the left or right) based on predicted token probabilities, much like language models generate varied responses from the same input.In contrast, although flow matching[62,63]or diffusion[84,85]algorithms theoretically enable multi-modality, π 0 consistently selects only the right-side objects, exhibiting only unimodal behavior, similar to observations in some other studies[86].Additionally, the disambiguating reasoning content helps the model fit actions more accurately.This is evidenced by π 0 occasionally failing to grasp the block, while OneTwoVLA consistently achieves precise grasps.Moreover, π 0 's action mean squared error (MSE) on the validation dataset is 56% higher than OneT-woVLA's.This interesting finding suggests that when training on large-scale, variable-quality robot datasets, detailed annotation of reasoning content may enhance action learning.G Failure CasesInstruction: "Prepare a plate of tomato-egg scramble."Instruction: "Dip the beef and the green bok choy."Instruction: "Make me a cup of Vodka Sunrise."Instruction: "Pick up the small basketball toy."Despite the promising performance of OneTwoVLA, it still makes mistakes.Fig.17illustrates the main failure cases of OneTwoVLA.In the Tomato-Egg task, OneTwoVLA occasionally fails to grip the yellow plate containing tomato and egg liquid firmly enough, resulting in the plate being dropped (see the first column in Fig.17).In the Hotpot task, OneTwoVLA sometimes misidentifies the location of the target ingredient.For instance, as shown in the Fig.17second column, the robot is instructed to pick up green bok choy but instead it attempts to pick up enoki mushrooms.The third column of Fig.17shows a case in Cocktail task, where OneTwoVLA fails to pour the orange juice accurately while preparing the Vodka Sunrise, causing the juice to spill.In the Open-world experiments, OneTwoVLA shows vulnerability when encountering objects that are not present in either the robot data or our synthesized vision-language data.For instance, as illustrated in the Fig.17fourth column, the robot consistently moves toward the chessboard despite being instructed to pick up the small basketball toy.We believe that training on larger robot datasets, as well as cotraining with richer vision-language data, can further facilitate OneTwoVLA in learning fine-grained actions and improve generalization capabilities.
The embodied mind: Cognitive science and human experience. J Varela Francisco, T Evan, R Eleanor, 1991</p>
<p>Embodied cognition: A field guide. M L Anderson, Artificial intelligence. 14912003</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. Y Hu, F Lin, T Zhang, L Yi, Y Gao, arXiv:2311.178422023arXiv preprint</p>
<p>Hi robot: Open-ended instruction following with hierarchical vision-languageaction models. L X Shi, B Ichter, M Equi, L Ke, K Pertsch, Q Vuong, J Tanner, A Walling, H Wang, N Fusai, arXiv:2502.194172025arXiv preprint</p>
<p>G R Team, S Abeyruwan, J Ainslie, J.-B Alayrac, M G Arenas, T Armstrong, A Balakrishna, R Baruch, M Bauza, M Blokzijl, arXiv:2503.20020Gemini robotics: Bringing ai into the physical world. 2025arXiv preprint</p>
<p>Helix: A vision-language-action model for generalist humanoid control. 2025</p>
<p>Thinking, fast and slow. D Kahneman, 2011macmillan</p>
<p>L Beyer, A Steiner, A S Pinto, A Kolesnikov, X Wang, D Salz, M Neumann, I Alabdulmohsin, M Tschannen, E Bugliarello, arXiv:2407.07726A versatile 3b vlm for transfer. 2024arXiv preprint</p>
<p>Prismatic vlms: Investigating the design space of visually-conditioned language models. S Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Forty-first International Conference on Machine Learning. 2024</p>
<p>M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, arXiv:2406.09246An open-source vision-language-action model. 2024arXiv preprint</p>
<p>0 : A vision-language-action flow model for general robot control. K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, arXiv:2410.241642024arXiv preprint</p>
<p>J Bjorck, F Castañeda, N Cherniadev, X Da, R Ding, L Fan, Y Fang, D Fox, F Hu, S Huang, arXiv:2503.14734Gr00t n1: An open foundation model for generalist humanoid robots. 2025arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>C Zhou, L Yu, A Babu, K Tirumala, M Yasunaga, L Shamis, J Kahn, X Ma, L Zettlemoyer, O Levy, arXiv:2408.11039Transfusion: Predict the next token and diffuse images with one multimodal model. 2024arXiv preprint</p>
<p>Introducing 4o image generation. Openai, 2025</p>
<p>Experiment with gemini 2.0 flash native image generation. Google, 2025</p>
<p>Pali-3 vision language models: Smaller, faster, stronger. X Chen, X Wang, L Beyer, A Kolesnikov, J Wu, P Voigtlaender, B Mustafa, S Goodman, I Alabdulmohsin, P Padlewski, arXiv:2310.091992023arXiv preprint</p>
<p>Improved baselines with visual instruction tuning. H Liu, C Li, Y Li, Y J Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. P Wang, S Bai, S Tan, S Wang, Z Fan, J Bai, K Chen, X Liu, J Wang, W Ge, arXiv:2409.121912024arXiv preprint</p>
<p>Deepseek-vl: towards real-world vision-language understanding. H Lu, W Liu, B Zhang, B Wang, K Dong, B Liu, J Sun, T Ren, Z Li, H Yang, arXiv:2403.055252024arXiv preprint</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, A Wahid, J Tompson, Q Vuong, T Yu, W Huang, arXiv:2303.033782023arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Fast: Efficient action tokenization for vision-language-action models. K Pertsch, K Stachowicz, B Ichter, D Driess, S Nair, Q Vuong, O Mees, C Finn, S Levine, arXiv:2501.097472025arXiv preprint</p>
<p>Dexvla: Vision-language model with plug-in diffusion expert for general robot control. J Wen, Y Zhu, J Li, Z Tang, C Shen, F Feng, arXiv:2502.058552025arXiv preprint</p>
<p>Otter: A vision-language-action model with text-aware visual feature extraction. H Huang, F Liu, L Fu, T Wu, M Mukadam, J Malik, K Goldberg, P Abbeel, arXiv:2503.037342025arXiv preprint</p>
<p>Roboturk: A crowdsourcing platform for robotic skill learning through imitation. A Mandlekar, Y Zhu, A Garg, J Booher, M Spero, A Tung, J Gao, J Emmons, A Gupta, E Orbay, Conference on Robot Learning. PMLR2018</p>
<p>Robot learning in homes: Improving generalization and reducing dataset bias. A Gupta, A Murali, D P Gandhi, L Pinto, Advances in neural information processing systems. 201831</p>
<p>S Dasari, F Ebert, S Tian, S Nair, B Bucher, K Schmeckpeper, S Singh, S Levine, C Finn, arXiv:1910.11215Robonet: Large-scale multi-robot learning. 2019arXiv preprint</p>
<p>Scaling data-driven robotics with reward sketching and batch reinforcement learning. S Cabi, S G Colmenarejo, A Novikov, K Konyushkova, S Reed, R Jeong, K Zolna, Y Aytar, D Budden, M Vecerik, arXiv:1909.122002019arXiv preprint</p>
<p>Graspnet-1billion: A large-scale benchmark for general object grasping. H.-S Fang, C Wang, M Gou, C Lu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. E Jang, A Irpan, M Khansari, D Kappler, F Ebert, C Lynch, S Levine, C Finn, Conference on Robot Learning. PMLR2022</p>
<p>Bridgedata v2: A dataset for robot learning at scale. H R Walke, K Black, T Z Zhao, Q Vuong, C Zheng, P Hansen-Estruch, A W He, V Myers, M J Kim, M Du, Conference on Robot Learning. PMLR2023</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. A O'neill, A Rehman, A Maddukuri, A Gupta, A Padalkar, A Lee, A Pooley, A Gupta, A Mandlekar, A Jain, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>A Khazatsky, K Pertsch, S Nair, A Balakrishna, S Dasari, S Karamcheti, S Nasiriany, M K Srirama, L Y Chen, K Ellis, arXiv:2403.12945A large-scale in-the-wild robot manipulation dataset. 2024arXiv preprint</p>
<p>Data scaling laws in imitation learning for robotic manipulation. F Lin, Y Hu, P Sheng, C Wen, J You, Y Gao, arXiv:2410.186472024arXiv preprint</p>
<p>Open-world object manipulation using pre-trained vision-language models. A Stone, T Xiao, Y Lu, K Gopalakrishnan, K.-H Lee, Q Vuong, P Wohlhart, S Kirmani, B Zitkovich, F Xia, arXiv:2303.009052023arXiv preprint</p>
<p>Grounded decoding: Guiding text generation with grounded models for embodied agents. W Huang, F Xia, D Shah, D Driess, A Zeng, Y Lu, P Florence, I Mordatch, S Levine, K Hausman, Advances in Neural Information Processing Systems. 202336</p>
<p>B Li, P Wu, P Abbeel, J Malik, arXiv:2310.10645Interactive task planning with language models. 2023arXiv preprint</p>
<p>S Belkhale, T Ding, T Xiao, P Sermanet, Q Vuong, J Tompson, Y Chebotar, D Dwibedi, D Sadigh, arXiv:2403.01823Rt-h: Action hierarchies using language. 2024arXiv preprint</p>
<p>P Liu, Y Orru, J Vakil, C Paxton, N M M Shafiullah, L Pinto, arXiv:2401.12202Ok-robot: What really matters in integrating open-knowledge models for robotics. 2024arXiv preprint</p>
<p>L X Shi, Z Hu, T Z Zhao, A Sharma, K Pertsch, J Luo, S Levine, C Finn, arXiv:2403.12910Yell at your robot: Improving on-the-fly from language corrections. 2024arXiv preprint</p>
<p>P Zhi, Z Zhang, Y Zhao, M Han, Z Zhang, Z Li, Z Jiao, B Jia, S Huang, arXiv:2404.10220Closed-loop open-vocabulary mobile manipulation with gpt-4v. 2024arXiv preprint</p>
<p>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. Q Zhao, Y Lu, M J Kim, Z Fu, Z Zhang, Y Wu, Z Li, Q Ma, S Han, C Finn, arXiv:2503.220202025arXiv preprint</p>
<p>Y Li, Y Deng, J Zhang, J Jang, M Memmel, R Yu, C R Garrett, F Ramos, D Fox, A Li, arXiv:2502.05485Hierarchical action models for open-world robot manipulation. 2025arXiv preprint</p>
<p>Copa: General robotic manipulation through spatial constraints of parts with foundation models. H Huang, F Lin, Y Hu, S Wang, Y Gao, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2024</p>
<p>P Intelligence, K Black, N Brown, J Darpinian, K Dhabalia, D Driess, A Esmail, M Equi, C Finn, N Fusai, arXiv:2504.16054π 0.5 : a vision-language-action model with open-world generalization. 2025arXiv preprint</p>
<p>Robotic control via embodied chain-of-thought reasoning. M Zawalski, W Chen, K Pertsch, O Mees, C Finn, S Levine, arXiv:2407.086932024arXiv preprint</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models. Q Vuong, S Levine, H R Walke, K Pertsch, A Singh, R Doshi, C Xu, J Luo, L Tan, D Shah, Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition@ CoRL2023. 2023</p>
<p>Vision-language foundation models as effective robot imitators. X Li, M Liu, H Zhang, C Yu, J Xu, H Wu, C Cheang, Y Jing, W Zhang, H Liu, arXiv:2311.013782023arXiv preprint</p>
<p>Glso: Grammar-guided latent space optimization for sample-efficient robot design automation. J Hu, J Whitman, H Choset, Conference on Robot Learning. PMLR2023</p>
<p>Rtaffordance: Reasoning about robotic manipulation with affordances. S Nasiriany, S Kirmani, T Ding, L Smith, Y Zhu, D Driess, D Sadigh, T Xiao, CoRL 2024 Workshop on Mastering Robot Manipulation in a World of Abundant Data. 2024</p>
<p>Re-mix: Optimizing data mixtures for large scale imitation learning. J Hejna, C Bhateja, Y Jiang, K Pertsch, D Sadigh, arXiv:2408.140372024arXiv preprint</p>
<p>Pushing the limits of cross-embodiment learning for manipulation and navigation. J Yang, C Glossop, A Bhorkar, D Shah, Q Vuong, C Finn, D Sadigh, S Levine, arXiv:2402.194322024arXiv preprint</p>
<p>Scaling cross-embodied learning: One policy for manipulation, navigation, locomotion and aviation. R Doshi, H Walke, O Mees, S Dasari, S Levine, arXiv:2408.118122024arXiv preprint</p>
<p>Robopoint: A vision-language model for spatial affordance prediction for robotics. W Yuan, J Duan, V Blukis, W Pumacay, R Krishna, A Murali, A Mousavian, D Fox, arXiv:2406.107212024arXiv preprint</p>
<p>Sim-and-real co-training: A simple recipe for vision-based robotic manipulation. A Maddukuri, Z Jiang, L Y Chen, S Nasiriany, Y Xie, Y Fang, W Huang, Z Wang, Z Xu, N Chernyadev, arXiv:2503.243612025arXiv preprint</p>
<p>Embodiedgpt: Vision-language pre-training via embodied chain of thought. Y Mu, Q Zhang, M Hu, W Wang, M Ding, J Jin, B Wang, J Dai, Y Qiao, P Luo, Advances in Neural Information Processing Systems. 202336</p>
<p>M Zhu, Y Zhu, J Li, Z Zhou, J Wen, X Liu, C Shen, Y Peng, F Feng, arXiv:2502.19250Objectvla: End-to-end open-world object manipulation without demonstration. 2025arXiv preprint</p>
<p>Z Zhou, Y Zhu, M Zhu, J Wen, N Liu, Z Xu, W Meng, R Cheng, Y Peng, C Shen, arXiv:2502.14420Unified multimodal understanding and robot control with vision-languageaction model. 2025arXiv preprint</p>
<p>Y Lipman, R T Chen, H Ben-Hamu, M Nickel, M Le, arXiv:2210.02747Flow matching for generative modeling. 2022arXiv preprint</p>
<p>Rectified flow: A marginal preserving approach to optimal transport. Q Liu, arXiv:2209.145772022arXiv preprint</p>
<p>G Deepmind, Gemini 2.5: Our most intelligent ai model. March 2025. 1 May 2025</p>
<p>. B F Labs, 2024</p>
<p>Interactive visual grounding of referring expressions for human-robot interaction. M Shridhar, D Hsu, arXiv:1806.038312018arXiv preprint</p>
<p>Hifi-cs: Towards open vocabulary visual grounding for robotic grasping using vision-language models. V Bhat, P Krishnamurthy, R Karri, F Khorrami, arXiv:2409.104192024arXiv preprint</p>
<p>Gvcci: Lifelong learning of visual grounding for language-guided robotic manipulation. J Kim, G.-C Kang, J Kim, S Shin, B.-T Zhang, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023</p>
<p>Automating robot failure recovery using vision-language models with optimized prompts. H Chen, Y Yao, R Liu, C Liu, J Ichnowski, arXiv:2409.039662024arXiv preprint</p>
<p>Automated behavior tree error recovery framework for robotic systems. R Wu, S Kortik, C H Santos, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Towards automatic error recovery in robot programs. M Gini, G Gini, International Symposium on the Occasion of the 25th Anniversary of McGill University Centre for Intelligent Machines. Springer1983</p>
<p>Error recovery in robot systems. S Srinivas, 1977California Institute of Technology</p>
<p>Reset-free trial-and-error learning for robot damage recovery. K Chatzilygeroudis, V Vassiliades, J.-B Mouret, Robotics and Autonomous Systems. 1002018</p>
<p>Human-robot interaction: status and challenges. T B Sheridan, Human factors. 5842016</p>
<p>Human-robot interaction: a survey. M A Goodrich, A C Schultz, Foundations and trends® in human-computer interaction. 132008</p>
<p>Human-robot interaction. R R Murphy, T Nomura, A Billard, J L Burke, IEEE robotics &amp; automation magazine. 1722010</p>
<p>An atlas of physical human-robot interaction. Mechanism and Machine Theory. A De Santis, B Siciliano, A De Luca, A Bicchi, 200843</p>
<p>Ingress: Interactive visual grounding of referring expressions. M Shridhar, D Mittal, D Hsu, The International Journal of Robotics Research. 392-32020</p>
<p>C Chi, Z Xu, C Pan, E Cousineau, B Burchfiel, S Feng, R Tedrake, S Song, arXiv:2402.10329Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots. 2024arXiv preprint</p>
<p>D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 202235</p>
<p>D M Ziegler, N Stiennon, J Wu, T B Brown, A Radford, D Amodei, P Christiano, G Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019arXiv preprint</p>
<p>Learning fine-grained bimanual manipulation with low-cost hardware. T Z Zhao, V Kumar, S Levine, C Finn, arXiv:2304.137052023arXiv preprint</p>
<p>Denoising diffusion probabilistic models. J Ho, A Jain, P Abbeel, Advances in neural information processing systems. 202033</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. C Chi, Z Xu, S Feng, E Cousineau, Y Du, B Burchfiel, R Tedrake, S Song, The International Journal of Robotics Research. 027836492412736682023</p>
<p>Does diffusion policy produce multi-modal actions?. A Soare, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
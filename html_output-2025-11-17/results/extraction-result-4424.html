<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4424 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4424</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4424</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-270380186</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.06863v1.pdf" target="_blank">Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have the potential to enhance Agent-Based Modeling by better representing complex interdependent cybersecurity systems, improving cybersecurity threat modeling and risk management. However, evaluating LLMs in this context is crucial for legal compliance and effective application development. Existing LLM evaluation frameworks often overlook the human factor and cognitive computing capabilities essential for interdependent cybersecurity. To address this gap, I propose OllaBench, a novel evaluation framework that assesses LLMs' accuracy, wastefulness, and consistency in answering scenario-based information security compliance and non-compliance questions. OllaBench is built on a foundation of 24 cognitive behavioral theories and empirical evidence from 38 peer-reviewed papers. OllaBench was used to evaluate 21 LLMs, including both open-weight and commercial models from OpenAI, Anthropic, Google, Microsoft, Meta and so on. The results reveal that while commercial LLMs have the highest overall accuracy scores, there is significant room for improvement. Smaller low-resolution open-weight LLMs are not far behind in performance, and there are significant differences in token efficiency and consistency among the evaluated models. OllaBench provides a user-friendly interface and supports a wide range of LLM platforms, making it a valuable tool for researchers and solution developers in the field of human-centric interdependent cybersecurity and beyond.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4424.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4424.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OllaBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OllaBench evaluation framework for human-centric interdependent cybersecurity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scenario-driven, theory-grounded evaluation framework that ranks LLMs by accuracy, token-efficiency (wastefulness), and consistency when reasoning about human cognitive-behavioral compliance/non-compliance in information security scenarios; built on a 24-theory knowledge graph and a 10,000-item synthetic scenario benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>OllaBench</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generates synthetic two-person cognitive-behavioral profiles using a knowledge graph (Cybonto-4-IC) grounded on 24 cognitive behavioral theories and 38 empirical papers, then asks each LLM a fixed set of four multiple-choice questions per scenario (Which-Cog-Path, Who-is-Who, Team-Risk, Target-Factor). Responses are compared to reference answers derived deterministically from the scenario generator; scores are aggregated into categorical accuracies, token-based wastefulness, and a consistency statistic derived from fitting a Structural Equation Model (SEM) linking question-type scores to latent constructs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Relevance, alignment, effectiveness, unintended effects (selected formative-evaluation focuses); operationalized as categorical accuracy on four question types, wastefulness (average tokens per wrong answer), and model–knowledge consistency via SEM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Human-centered interdependent cybersecurity / cognitive behavioral reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Behavioral-cognitive construct identification and predictive/team-risk reasoning (applied explanatory/predictive reasoning about human behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Applied to 21 LLMs. Overall accuracy top performers: gemini-1.5-flash-latest (0.5102), gpt-4o (0.4735), claude-3-opus (0.4688), mixtral-8x7b (0.4667), mistral-7b (0.4634). Per-question top scores listed in paper (e.g., Which-Cognitive-Path top: gemini 0.4485, gpt-4o 0.4306). Wastefulness (lower better): best openchat-7b-v3.5-q4 ~6.427 tokens per wrong answer; worst orca2-13b ~402.038 tokens per wrong answer. Consistency: authors report that the most accurate LLMs are the most consistent by SEM; SEM parameter estimates reported in Table II for top models. Detailed full results and benchmark files available on project GitHub.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated/hybrid: evaluation is automated — deterministic reference answers, numeric scoring, token counting, and SEM fitting; no human raters for answer correctness during main experiments (reference answers are programmatically constructed from scenario generation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Design validation via theory-driven grounding: knowledge graph nodes/edges rooted in 24 cognitive-behavioral theories and 38 peer-reviewed papers; correctness labels are derived deterministically from generated scenarios (seeded from validated survey instruments). Consistency assessed via SEM fit to the proposed model; no independent human-judged cross-validation reported beyond theoretical grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Scope limited to scenario-based cognitive-behavioral compliance questions (not general scientific-theory generation); assumes LLMs have indexed the domain theories and instruments; inter-subjective transmission criteria not evaluated; chosen metrics are quantitative-subjective only; formative evaluation choices (relevance/alignment/effectiveness/unintended effects) exclude some domains (e.g., equity, sustainability). No explicit human vs LLM theory-generation comparison was performed.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Standard benchmark: 10,000 synthetic two-person cognitive-behavioral scenarios (each with 4 multiple-choice questions) generated from Cybonto-4-IC knowledge graph and verbal description libraries; dataset and generator code available on GitHub.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity", 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4424.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4424.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Categorical Accuracy (A_cat)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Categorical accuracy per question category</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Accuracy metric computed per question-type category (Which-Cog-Path, Who-is-Who, Team-Risk, Target-Factor) as number of correct responses divided by number of questions in that category; overall accuracy is the mean of category accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Categorical Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each category cat with n_cat questions and r_cat correct responses, A_cat = r_cat / n_cat. Overall Accuracy = average(A_WCP, A_WHO, A_TR, A_TF). Measures correctness of discrete multiple-choice outputs against deterministic reference answers derived from scenario generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Empirical correctness (categorical match to reference labels) across different reasoning question types; used to operationalize effectiveness and relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied cognitive-behavioral reasoning in cybersecurity scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Answer classification / predictive correctness for scenario-derived hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported per-model categorical accuracy values across four question types; see OllaBench overall results (e.g., gemini-1.5 overall 0.5102). Category-specific top performers differ by category (detailed in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric computed programmatically from model outputs and programmatic reference answers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Reference-answer logic based on scenario generator (seeded from validated survey instruments and knowledge graph); no explicit external human labeling reported for this metric.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Only measures exact-match correctness on multiple-choice items; may not capture partial correctness or nuanced explanations; depends on the fidelity of the scenario-to-reference mapping; does not directly measure explanatory depth or novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity", 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4424.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4424.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wastefulness (W_x)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token-based wastefulness metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mean number of tokens consumed when producing wrong answers: for model x, average tokens used across its incorrect responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Wastefulness (W_x)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For model x with n_missed wrong answers, and T_i the tokens used to generate wrong answer i, W_x = (sum_{i=0}^{n_missed-1} T_i) / n_missed. Measures token efficiency conditioned on errors, i.e., how 'wasteful' a model is when it answers incorrectly.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Resource use / efficiency (tokens consumed per incorrect response) — operationalizes the Resource Use evaluation domain.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation of LLM outputs in cybersecurity reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Efficiency metric for generative responses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported per-model: lowest wastefulness openchat-7b-v3.5-q4 ≈ 6.427 tokens per wrong answer; other low values e.g., phi-2.7b ≈ 34.53 tokens; highest wastefulness orca2-13b ≈ 402.038 tokens, orca2-7b ≈ 377.1546 tokens, wizardlm2-7b ≈ 305.9823 tokens. Indicates large variation in token consumption on incorrect outputs across models.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated measurement via token counting of model responses.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Internal consistency: token counts are deterministic; interpretation validated by cross-model comparisons. No external validation reported linking wastefulness to developer cost or latency trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Token counts depend on model verbosity and decoding settings (e.g., temperature, max tokens); a model may be long-winded yet accurate — metric only conditions on wrong answers and may not capture usefulness of elaborations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity", 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4424.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4424.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consistency-SEM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Consistency measurement via Structural Equation Modeling (SEM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assesses whether LLM response patterns conform to an a priori cognitive-knowledge model by fitting an SEM that links per-question scores (WHO, WCP, TR, TF) to latent constructs and evaluating model fit and parameter estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Consistency via SEM</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Constructs a proposed directed model (Figure 4) linking Which-Cognitive-Path (WCP), Who-is-Who (WHO), Team-Risk (TR), and Target-Factor (TF) scores; fits SEM to the matrix of LLM scores to estimate path coefficients and test whether LLM behavior is consistent with the theoretical model. Consistency is inferred from parameter estimates and fit statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Coherence between LLM outputs and an explicit theoretical causal/associational model (theory-concordance); assesses internal consistency and plausibility of multi-question response patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cognitive-behavioral theory alignment / assessment of reasoning consistency</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Model-concordance / causal-structure consistency</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>SEM parameter estimates and standard errors reported in Table II for top models; qualitative finding: the most accurate LLMs tended to show higher consistency (parameter estimates coherent with proposed model). Specific path estimates vary by model (Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated statistical evaluation (SEM) using model score vectors as observed variables.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Model grounded in Cybonto-4-IC theoretical graph; SEM fitting provides statistical evidence for or against the proposed relations. No report of cross-validation with human expert judgments of consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>SEM results depend on the correctness of the proposed theoretical model; fitting SEM to LLM scores assumes linear/parametric relations and adequate sample size; SEM does not evaluate semantic quality of explanations beyond score patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity", 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4424.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4424.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Theory-driven evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theory-driven evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation approach that begins with an explicit program/theory specifying how inputs map to outcomes; the evaluation design, metrics, and interpretation are guided by that theory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Theory-driven evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Develop a coherent program theory or model (here: knowledge graph grounded on 24 cognitive theories), prioritize evaluation questions derived from that theory, design metrics and scenarios that test constructs and causal links, and analyze outcomes relative to theoretical expectations (e.g., via SEM).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Coherence with theory, ability to measure theoretical constructs (construct validity), detection of causal links, identification of unintended consequences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation science applied to LLM reasoning/cognitive modeling</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Meta-evaluation / framework to evaluate theory-conformant behavior</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used as the guiding methodology for OllaBench design and knowledge-graph construction; not a numeric result but underpins the validity claims of the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Framework guides automated experiments; human judgment used indirectly in selecting theoretical constructs and sourcing validated survey instruments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Grounding theory in 24 cognitive behavioral theories and 38 empirical papers; evaluation outcomes interpreted against the program theory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on the chosen theory being complete and appropriate; may bias evaluation towards theory-aligned LLMs and miss out-of-theory but valuable reasoning; inter-subjective criteria not addressed in current work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity", 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4424.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4424.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Formative evaluation (chosen)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Formative evaluation (Design Science Research strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation execution strategy focused on generating empirically grounded interpretations to improve the artifact during development; OllaBench explicitly adopts formative evaluation and restricts its evaluative criteria to relevance, alignment, effectiveness, and unintended effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Formative evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Design experiments and diagnostics (scenarios, metrics) intended to inform model selection and iterative improvement for early-stage solution developers rather than produce final summative judgments; emphasis on actionable feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Relevance, alignment, effectiveness, unintended effects (subset of full nine-domain evaluative framework).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation science / design science for LLM applications</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation-execution strategy</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used as the stated approach for OllaBench; results interpreted in the context of guiding LLM selection and tuning for application development.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Operationalized via automated metrics but intended to inform human developers; no formal human-expert iterative loop reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Rationale provided from design science literature; not empirically validated within this paper beyond producing comparative metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Formative focus limits generalizability of claims (not summative across all contexts); excludes some evaluative domains by design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity", 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4424.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4424.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-centered quality objectives</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ISO human-centredness quality objectives</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Four ISO-defined quality objectives for human-centredness: usability, accessibility, user experience, and mitigation of use-related risks; used in the paper to motivate human-centered evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ISO human-centred quality objectives</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use the four ISO objectives to ground selection of evaluation goals and metrics, ensuring evaluation considers user context and risk-related outcomes relevant to human-centered design.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Usability, accessibility, user experience, mitigation of use-related risks (high-level human-centered dimensions).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Human-centered design / HCI as applied to LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>High-level evaluative criteria</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Referenced to justify need for human-centered metrics; not operationalized as numerical results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Conceptual guidance; operational metrics in OllaBench are quantitative-subjective rather than direct ISO metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Standard ISO definitions referenced (no additional validation reported).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper acknowledges difficulty in designing universally comprehensive human-centered metrics and that data availability constrains metric choice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity", 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4424.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4424.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Calibration / ECE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model calibration measured by Expected Calibration Error (ECE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic calibration metric (ECE) that measures alignment between a model's predicted probabilities and empirical outcome frequencies; paper cites calibration as an important LLM property for high-stakes decision contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Expected Calibration Error (ECE)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Bin predictions by predicted probability, compute absolute difference between predicted confidence and empirical accuracy per bin, and take a weighted average across bins to produce ECE; lower ECE indicates better calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Calibration (reliability of predicted probabilities), essential for risk-sensitive deployments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning model evaluation / probabilistic forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Probabilistic calibration</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Mentioned as an important evaluation concept (and referenced to calibration literature) but not directly computed or reported for OllaBench LLMs in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric computed from model probability outputs (requires model to emit probabilities/confidence).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Established metric from literature (Guo et al., Naeini et al. referenced) — no new validation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>ECE requires meaningful probabilistic outputs from LLMs; many LLM setups return only text; not adapted into the multiple-choice token-based scoring pipeline of OllaBench as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity", 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Holistic Evaluation of Language Models <em>(Rating: 2)</em></li>
                <li>On calibration of modern neural networks <em>(Rating: 2)</em></li>
                <li>A systematic review of theory-driven evaluation practice from 1990 to 2009 <em>(Rating: 2)</em></li>
                <li>Evaluative Criteria: An Integrated Model of Domains and Sources <em>(Rating: 2)</em></li>
                <li>Structural equation modeling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4424",
    "paper_id": "paper-270380186",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "OllaBench",
            "name_full": "OllaBench evaluation framework for human-centric interdependent cybersecurity",
            "brief_description": "A scenario-driven, theory-grounded evaluation framework that ranks LLMs by accuracy, token-efficiency (wastefulness), and consistency when reasoning about human cognitive-behavioral compliance/non-compliance in information security scenarios; built on a 24-theory knowledge graph and a 10,000-item synthetic scenario benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "OllaBench",
            "evaluation_method_description": "Generates synthetic two-person cognitive-behavioral profiles using a knowledge graph (Cybonto-4-IC) grounded on 24 cognitive behavioral theories and 38 empirical papers, then asks each LLM a fixed set of four multiple-choice questions per scenario (Which-Cog-Path, Who-is-Who, Team-Risk, Target-Factor). Responses are compared to reference answers derived deterministically from the scenario generator; scores are aggregated into categorical accuracies, token-based wastefulness, and a consistency statistic derived from fitting a Structural Equation Model (SEM) linking question-type scores to latent constructs.",
            "evaluation_criteria": "Relevance, alignment, effectiveness, unintended effects (selected formative-evaluation focuses); operationalized as categorical accuracy on four question types, wastefulness (average tokens per wrong answer), and model–knowledge consistency via SEM.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Human-centered interdependent cybersecurity / cognitive behavioral reasoning",
            "theory_type": "Behavioral-cognitive construct identification and predictive/team-risk reasoning (applied explanatory/predictive reasoning about human behavior)",
            "human_comparison": false,
            "evaluation_results": "Applied to 21 LLMs. Overall accuracy top performers: gemini-1.5-flash-latest (0.5102), gpt-4o (0.4735), claude-3-opus (0.4688), mixtral-8x7b (0.4667), mistral-7b (0.4634). Per-question top scores listed in paper (e.g., Which-Cognitive-Path top: gemini 0.4485, gpt-4o 0.4306). Wastefulness (lower better): best openchat-7b-v3.5-q4 ~6.427 tokens per wrong answer; worst orca2-13b ~402.038 tokens per wrong answer. Consistency: authors report that the most accurate LLMs are the most consistent by SEM; SEM parameter estimates reported in Table II for top models. Detailed full results and benchmark files available on project GitHub.",
            "automated_vs_human_evaluation": "Automated/hybrid: evaluation is automated — deterministic reference answers, numeric scoring, token counting, and SEM fitting; no human raters for answer correctness during main experiments (reference answers are programmatically constructed from scenario generation).",
            "validation_method": "Design validation via theory-driven grounding: knowledge graph nodes/edges rooted in 24 cognitive-behavioral theories and 38 peer-reviewed papers; correctness labels are derived deterministically from generated scenarios (seeded from validated survey instruments). Consistency assessed via SEM fit to the proposed model; no independent human-judged cross-validation reported beyond theoretical grounding.",
            "limitations_challenges": "Scope limited to scenario-based cognitive-behavioral compliance questions (not general scientific-theory generation); assumes LLMs have indexed the domain theories and instruments; inter-subjective transmission criteria not evaluated; chosen metrics are quantitative-subjective only; formative evaluation choices (relevance/alignment/effectiveness/unintended effects) exclude some domains (e.g., equity, sustainability). No explicit human vs LLM theory-generation comparison was performed.",
            "benchmark_dataset": "Standard benchmark: 10,000 synthetic two-person cognitive-behavioral scenarios (each with 4 multiple-choice questions) generated from Cybonto-4-IC knowledge graph and verbal description libraries; dataset and generator code available on GitHub.",
            "uuid": "e4424.0",
            "source_info": {
                "paper_title": "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Categorical Accuracy (A_cat)",
            "name_full": "Categorical accuracy per question category",
            "brief_description": "Accuracy metric computed per question-type category (Which-Cog-Path, Who-is-Who, Team-Risk, Target-Factor) as number of correct responses divided by number of questions in that category; overall accuracy is the mean of category accuracies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Categorical Accuracy",
            "evaluation_method_description": "For each category cat with n_cat questions and r_cat correct responses, A_cat = r_cat / n_cat. Overall Accuracy = average(A_WCP, A_WHO, A_TR, A_TF). Measures correctness of discrete multiple-choice outputs against deterministic reference answers derived from scenario generation.",
            "evaluation_criteria": "Empirical correctness (categorical match to reference labels) across different reasoning question types; used to operationalize effectiveness and relevance.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Applied cognitive-behavioral reasoning in cybersecurity scenarios",
            "theory_type": "Answer classification / predictive correctness for scenario-derived hypotheses",
            "human_comparison": false,
            "evaluation_results": "Reported per-model categorical accuracy values across four question types; see OllaBench overall results (e.g., gemini-1.5 overall 0.5102). Category-specific top performers differ by category (detailed in paper).",
            "automated_vs_human_evaluation": "Automated metric computed programmatically from model outputs and programmatic reference answers.",
            "validation_method": "Reference-answer logic based on scenario generator (seeded from validated survey instruments and knowledge graph); no explicit external human labeling reported for this metric.",
            "limitations_challenges": "Only measures exact-match correctness on multiple-choice items; may not capture partial correctness or nuanced explanations; depends on the fidelity of the scenario-to-reference mapping; does not directly measure explanatory depth or novelty.",
            "uuid": "e4424.1",
            "source_info": {
                "paper_title": "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Wastefulness (W_x)",
            "name_full": "Token-based wastefulness metric",
            "brief_description": "Mean number of tokens consumed when producing wrong answers: for model x, average tokens used across its incorrect responses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Wastefulness (W_x)",
            "evaluation_method_description": "For model x with n_missed wrong answers, and T_i the tokens used to generate wrong answer i, W_x = (sum_{i=0}^{n_missed-1} T_i) / n_missed. Measures token efficiency conditioned on errors, i.e., how 'wasteful' a model is when it answers incorrectly.",
            "evaluation_criteria": "Resource use / efficiency (tokens consumed per incorrect response) — operationalizes the Resource Use evaluation domain.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Evaluation of LLM outputs in cybersecurity reasoning tasks",
            "theory_type": "Efficiency metric for generative responses",
            "human_comparison": false,
            "evaluation_results": "Reported per-model: lowest wastefulness openchat-7b-v3.5-q4 ≈ 6.427 tokens per wrong answer; other low values e.g., phi-2.7b ≈ 34.53 tokens; highest wastefulness orca2-13b ≈ 402.038 tokens, orca2-7b ≈ 377.1546 tokens, wizardlm2-7b ≈ 305.9823 tokens. Indicates large variation in token consumption on incorrect outputs across models.",
            "automated_vs_human_evaluation": "Automated measurement via token counting of model responses.",
            "validation_method": "Internal consistency: token counts are deterministic; interpretation validated by cross-model comparisons. No external validation reported linking wastefulness to developer cost or latency trade-offs.",
            "limitations_challenges": "Token counts depend on model verbosity and decoding settings (e.g., temperature, max tokens); a model may be long-winded yet accurate — metric only conditions on wrong answers and may not capture usefulness of elaborations.",
            "uuid": "e4424.2",
            "source_info": {
                "paper_title": "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Consistency-SEM",
            "name_full": "Consistency measurement via Structural Equation Modeling (SEM)",
            "brief_description": "Assesses whether LLM response patterns conform to an a priori cognitive-knowledge model by fitting an SEM that links per-question scores (WHO, WCP, TR, TF) to latent constructs and evaluating model fit and parameter estimates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Consistency via SEM",
            "evaluation_method_description": "Constructs a proposed directed model (Figure 4) linking Which-Cognitive-Path (WCP), Who-is-Who (WHO), Team-Risk (TR), and Target-Factor (TF) scores; fits SEM to the matrix of LLM scores to estimate path coefficients and test whether LLM behavior is consistent with the theoretical model. Consistency is inferred from parameter estimates and fit statistics.",
            "evaluation_criteria": "Coherence between LLM outputs and an explicit theoretical causal/associational model (theory-concordance); assesses internal consistency and plausibility of multi-question response patterns.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Cognitive-behavioral theory alignment / assessment of reasoning consistency",
            "theory_type": "Model-concordance / causal-structure consistency",
            "human_comparison": false,
            "evaluation_results": "SEM parameter estimates and standard errors reported in Table II for top models; qualitative finding: the most accurate LLMs tended to show higher consistency (parameter estimates coherent with proposed model). Specific path estimates vary by model (Table II).",
            "automated_vs_human_evaluation": "Automated statistical evaluation (SEM) using model score vectors as observed variables.",
            "validation_method": "Model grounded in Cybonto-4-IC theoretical graph; SEM fitting provides statistical evidence for or against the proposed relations. No report of cross-validation with human expert judgments of consistency.",
            "limitations_challenges": "SEM results depend on the correctness of the proposed theoretical model; fitting SEM to LLM scores assumes linear/parametric relations and adequate sample size; SEM does not evaluate semantic quality of explanations beyond score patterns.",
            "uuid": "e4424.3",
            "source_info": {
                "paper_title": "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Theory-driven evaluation",
            "name_full": "Theory-driven evaluation methodology",
            "brief_description": "An evaluation approach that begins with an explicit program/theory specifying how inputs map to outcomes; the evaluation design, metrics, and interpretation are guided by that theory.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Theory-driven evaluation",
            "evaluation_method_description": "Develop a coherent program theory or model (here: knowledge graph grounded on 24 cognitive theories), prioritize evaluation questions derived from that theory, design metrics and scenarios that test constructs and causal links, and analyze outcomes relative to theoretical expectations (e.g., via SEM).",
            "evaluation_criteria": "Coherence with theory, ability to measure theoretical constructs (construct validity), detection of causal links, identification of unintended consequences.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Evaluation science applied to LLM reasoning/cognitive modeling",
            "theory_type": "Meta-evaluation / framework to evaluate theory-conformant behavior",
            "human_comparison": false,
            "evaluation_results": "Used as the guiding methodology for OllaBench design and knowledge-graph construction; not a numeric result but underpins the validity claims of the benchmark.",
            "automated_vs_human_evaluation": "Framework guides automated experiments; human judgment used indirectly in selecting theoretical constructs and sourcing validated survey instruments.",
            "validation_method": "Grounding theory in 24 cognitive behavioral theories and 38 empirical papers; evaluation outcomes interpreted against the program theory.",
            "limitations_challenges": "Relies on the chosen theory being complete and appropriate; may bias evaluation towards theory-aligned LLMs and miss out-of-theory but valuable reasoning; inter-subjective criteria not addressed in current work.",
            "uuid": "e4424.4",
            "source_info": {
                "paper_title": "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Formative evaluation (chosen)",
            "name_full": "Formative evaluation (Design Science Research strategy)",
            "brief_description": "An evaluation execution strategy focused on generating empirically grounded interpretations to improve the artifact during development; OllaBench explicitly adopts formative evaluation and restricts its evaluative criteria to relevance, alignment, effectiveness, and unintended effects.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Formative evaluation",
            "evaluation_method_description": "Design experiments and diagnostics (scenarios, metrics) intended to inform model selection and iterative improvement for early-stage solution developers rather than produce final summative judgments; emphasis on actionable feedback.",
            "evaluation_criteria": "Relevance, alignment, effectiveness, unintended effects (subset of full nine-domain evaluative framework).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Evaluation science / design science for LLM applications",
            "theory_type": "Evaluation-execution strategy",
            "human_comparison": false,
            "evaluation_results": "Used as the stated approach for OllaBench; results interpreted in the context of guiding LLM selection and tuning for application development.",
            "automated_vs_human_evaluation": "Operationalized via automated metrics but intended to inform human developers; no formal human-expert iterative loop reported in this paper.",
            "validation_method": "Rationale provided from design science literature; not empirically validated within this paper beyond producing comparative metrics.",
            "limitations_challenges": "Formative focus limits generalizability of claims (not summative across all contexts); excludes some evaluative domains by design.",
            "uuid": "e4424.5",
            "source_info": {
                "paper_title": "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Human-centered quality objectives",
            "name_full": "ISO human-centredness quality objectives",
            "brief_description": "Four ISO-defined quality objectives for human-centredness: usability, accessibility, user experience, and mitigation of use-related risks; used in the paper to motivate human-centered evaluation metrics.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "ISO human-centred quality objectives",
            "evaluation_method_description": "Use the four ISO objectives to ground selection of evaluation goals and metrics, ensuring evaluation considers user context and risk-related outcomes relevant to human-centered design.",
            "evaluation_criteria": "Usability, accessibility, user experience, mitigation of use-related risks (high-level human-centered dimensions).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Human-centered design / HCI as applied to LLM evaluation",
            "theory_type": "High-level evaluative criteria",
            "human_comparison": false,
            "evaluation_results": "Referenced to justify need for human-centered metrics; not operationalized as numerical results in this paper.",
            "automated_vs_human_evaluation": "Conceptual guidance; operational metrics in OllaBench are quantitative-subjective rather than direct ISO metrics.",
            "validation_method": "Standard ISO definitions referenced (no additional validation reported).",
            "limitations_challenges": "Paper acknowledges difficulty in designing universally comprehensive human-centered metrics and that data availability constrains metric choice.",
            "uuid": "e4424.6",
            "source_info": {
                "paper_title": "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Calibration / ECE",
            "name_full": "Model calibration measured by Expected Calibration Error (ECE)",
            "brief_description": "A probabilistic calibration metric (ECE) that measures alignment between a model's predicted probabilities and empirical outcome frequencies; paper cites calibration as an important LLM property for high-stakes decision contexts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "Expected Calibration Error (ECE)",
            "evaluation_method_description": "Bin predictions by predicted probability, compute absolute difference between predicted confidence and empirical accuracy per bin, and take a weighted average across bins to produce ECE; lower ECE indicates better calibration.",
            "evaluation_criteria": "Calibration (reliability of predicted probabilities), essential for risk-sensitive deployments.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Machine learning model evaluation / probabilistic forecasting",
            "theory_type": "Probabilistic calibration",
            "human_comparison": false,
            "evaluation_results": "Mentioned as an important evaluation concept (and referenced to calibration literature) but not directly computed or reported for OllaBench LLMs in this paper.",
            "automated_vs_human_evaluation": "Automated metric computed from model probability outputs (requires model to emit probabilities/confidence).",
            "validation_method": "Established metric from literature (Guo et al., Naeini et al. referenced) — no new validation in this paper.",
            "limitations_challenges": "ECE requires meaningful probabilistic outputs from LLMs; many LLM setups return only text; not adapted into the multiple-choice token-based scoring pipeline of OllaBench as reported.",
            "uuid": "e4424.7",
            "source_info": {
                "paper_title": "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Holistic Evaluation of Language Models",
            "rating": 2,
            "sanitized_title": "holistic_evaluation_of_language_models"
        },
        {
            "paper_title": "On calibration of modern neural networks",
            "rating": 2,
            "sanitized_title": "on_calibration_of_modern_neural_networks"
        },
        {
            "paper_title": "A systematic review of theory-driven evaluation practice from 1990 to 2009",
            "rating": 2,
            "sanitized_title": "a_systematic_review_of_theorydriven_evaluation_practice_from_1990_to_2009"
        },
        {
            "paper_title": "Evaluative Criteria: An Integrated Model of Domains and Sources",
            "rating": 2,
            "sanitized_title": "evaluative_criteria_an_integrated_model_of_domains_and_sources"
        },
        {
            "paper_title": "Structural equation modeling",
            "rating": 1,
            "sanitized_title": "structural_equation_modeling"
        }
    ],
    "cost": 0.013507,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity* * The final conference/journal version may have significantly more content updates
11 Jun 2024</p>
<p>Tam N Nguyen tom.nguyen@ieee.org 
Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity* * The final conference/journal version may have significantly more content updates
11 Jun 2024EBB3378FEA862E80AD50DED95361D4A1arXiv:2406.06863v1[cs.CR]large language modelcybersecurityartificial intelligenceinterdependent cybersecurityinformation security compliance
Large Language Models (LLMs) have the potential to enhance Agent-Based Modeling by better representing complex interdependent cybersecurity systems, improving cybersecurity threat modeling and risk management.However, evaluating LLMs in this context is crucial for legal compliance and effective application development.Existing LLM evaluation frameworks often overlook the human factor and cognitive computing capabilities essential for interdependent cybersecurity.To address this gap, I propose OllaBench, a novel evaluation framework that assesses LLMs' accuracy, wastefulness, and consistency in answering scenario-based information security compliance and non-compliance questions.OllaBench is built on a foundation of 24 cognitive behavioral theories and empirical evidence from 38 peer-reviewed papers.OllaBench was used to evaluate 21 LLMs, including both open-weight and commercial models from OpenAI, Anthropic, Google, Microsoft, Meta and so on.The results reveal that while commercial LLMs have the highest overall accuracy scores, there is significant room for improvement.Smaller low-resolution open-weight LLMs are not far behind in performance, and there are significant differences in token efficiency and consistency among the evaluated models.OllaBench provides a user-friendly interface and supports a wide range of LLM platforms, making it a valuable tool for researchers and solution developers in the field of human-centric interdependent cybersecurity and beyond.</p>
<p>I. INTRODUCTION</p>
<p>Most CEOs care about maintaining the right level of cybersecurity at an optimal cost.With the cyber threat landscape keeps evolving, organizations cannot manage cybersecurity risks effectively despite increased cybersecurity investments [1].This challenge can be explained via Interdependent Cybersecurity (IC) [2].For the first reason, optimizing cybersecurity investments in existing large interdependent systems is a wellknown non-convex hard problem with no effective solution [3].Second, smaller systems are growing in complexity and interdependence [4].Last, new low frequency, near simultaneous, macro-scale risks such as global pandemics, financial shocks, and geopolitical conflicts have compound effects on IC [5].</p>
<p>One way to increase effectiveness in IC risk management is hardening the key components shared by all large to small systems.With humans account for half of the problems in IC [6], [7], hardening the human factors is an obvious strategy.Figure 1 summarized the US NIST Cybersecurity Framework [8], Kianpour et al. [6], Andrade et al. [9], and Laszka et al. [7] into main application areas of Human-centric IC.These application areas ultimately support the common goal of staying at least one step ahead of cybersecurity threats.</p>
<p>Specifically, the "Risk and Threat Modeling" area plays out the interdependent games which involve normal and adversarial players [7].The total player number is 1 ≤ i ≤ n.Cybersecurity investments vector X = [x 1 , ..., x n ] T where x − i is the cybersecurity investments of all players except player x i .The risk is f i (x) = f i (x i , x −i ), f i ∈ [0, 1].With C i as the unit cost of cybersecurity investment for player i, the loss function is L i and the expected cost is L i f i (x) + C i x i .Being ahead of adversaries lowers expected costs by mechanisms such as insurance, rewards, penalties, regulations, audits, coordination, and sharing information, alongside enhancing cybersecurity technology.Cybersecurity Independence Game models' outputs can be the inputs of Risk Management models, Decision Theory models, and other models.The results of these model will then inform the specific actions which an organization should act on.Finally, the organization must gather the intelligence to measure the effectiveness of respond/protect action plans as well as potential threats.The cycle completes and a new cycle begins.</p>
<p>In contrast to mathematical models used in IC interdependent games, Agent-Based Modeling (ABM) is better in capturing individual characteristics [21].Rooted in methodological individualism, ABM incorporates three principal elements: agents, environment, and rules of interaction [22].ABM agents represent subsystems and their entities, governed by flexible rules that facilitate the simulation of complex relationships and interactions [23], [24].This bottom-up approach allows the micro-level agent behaviors to collectively generate emergent macro-level structures.Therefore, ABM enables the exploration of various complex scenarios at the macro scale, invaluable in assessing the potential development of cybersecurity threats within different large and complex IC contexts.</p>
<p>Large Language Models (LLMs) can be highly effective in developing ABM agents that mimic human networks, human cognitive patterns and articulate decisions in ways comprehensible to humans.As an illustrative example, Park et al. [25] showed a network of 25 LLM-powered agents which can replicate the behavioral economics observed in a human group, complete with discernible cognitive traits.More future LLM-  based simulation can be expected.However, in accordance with President Joe Biden's Executive Order 14110 [26], it is imperative to establish rigorous evaluation mechanisms for the LLMs to ensure they are safe, secure, and trustworthy.The order mandates enhanced scrutiny of AI systems to protect public welfare, prevent discrimination, and ensure privacy and civil liberties are upheld.Table I shows recent works on the evaluation of LLMs that are relevant to IC.We notice that most recent LLM evaluation works for cybersecurity does not involve human factor while LLM reasoning evaluation works do not involve both cybersecurity and human factor.This phenomenon is consistent with the fact that research and applications focusing on human-centered IC remains limited [6] while contemporary LLM evaluation frameworks, such as Stanford's HELM [27], fail to assess the cognitive computing capabilities of LLMs.For example, HELM includes a broad array of metrics for evaluating various aspects of NLP.However, none of these metrics directly evaluates cognitive computing functions.This oversight may stem from the relative novelty of LLMs, skewed public interest, nascent use cases, and a general lack of understanding of how LLMs can be leveraged in IC risk and threat modeling.</p>
<p>Therefore, OllaBench was born to help both researchers and application developers conveniently evaluate their LLM models within the context of cybersecurity compliance or noncompliance behaviors.In other words, OllaBench helps with answering the question</p>
<p>• What is the best LLM for reasoning about human cognitive behavioral observation within the context of cybersecurity compliance or non-compliance?</p>
<p>Figure 2 describes the overall design of OllaBench.Specifically, OllaBench can rank LLMs based on their accuracy, wastefulness, and consistency in answering scenario-based information security compliance/non-compliance questions.Early stage solution developers may leverage OllaBench to identify the best LLM for their human-centric IC application development.Researchers may leverage OllaBench to measure their fine-tuned LLMs' performances.</p>
<p>OllaBench's contributions are many fold.LLMs as well as commercial LLMs from OpenAI, Google, and Anthropic.• A friendly yet professional Graphical User Interface to support a broad base of OllaBench users.The rest of the paper is organized as follows.Section II describes the foundational theories for the paper's evaluation methodology and artifact design.Section III presents major aspects of the novel OllaBench.Section IV describes the results of applying OllaBench in evaluation of 21 LLMs towards information security policy compliant/non-compliance cognitive behavioral reasoning.Section discusses additional important topics before the final conclusion.</p>
<p>II. THEORETICAL BACKGROUND A. On Evaluation Science</p>
<p>According to the classical definition, science represents the intellectual and practical endeavor characterized by the methodical examination of the physical and natural realms, achieved through empirical observation and experimental investigation [28].Evaluation pertains to the process of rendering judgments concerning the merit, value, significance, credibility, and utility of various subjects under scrutiny [28].Notably, evaluation Science is confronted with challenges stemming from its interdisciplinary nature and expansive scope which lead to a lack of uniformity in defining what constitutes evaluation [28].</p>
<p>In such context, evaluative criteria are essential for delineating what constitutes a "high quality" or "successful" evaluand.Domain and source are the main evaluative criteria components.The domain refers to the focus or substance of a criterion, encapsulating the specific areas of assessment relevant to the evaluand [29].Conversely, the source denotes the origin of the criterion, whether it be an individual, a collective body, or a documented standard [29].</p>
<p>There are nine domains: Relevance, which assesses the pertinence of the evaluand to its intended objectives; Design, which evaluates the evaluand based on established principles, best practices, standards, and laws; Alignment, which measures the congruence of the evaluand with overarching initiatives; Effectiveness, which gauges the degree to which the evaluand achieves its intended outcomes; Unintended Effects, which considers both positive and negative consequences not originally anticipated; Consequence, which examines the broader impacts of the evaluand; Equity, which assesses the fairness and inclusivity of the evaluand; Resource Use, which evaluates the efficiency and appropriateness of resource allocation; and Sustainability, which considers the long-term viability and environmental impact of the evaluand [29].</p>
<p>Evaluation methodologies in Design Science Research include formative and summative approaches with two execution strategies of artificial, involving laboratory experiments and simulations, and naturalistic, which examines technology performance in real-world settings [30].Formative evaluations aim to generate empirically grounded interpretations to enhance the evaluand's characteristics or performance, primarily focusing on outcomes to facilitate improvement actions [30].Conversely, summative evaluations seek to establish empirically grounded common understandings of the evaluand across varying contexts, concentrating on meanings to inform influential decisions [30].While formative evaluations guide improvements during the development process, summative evaluations assess the results post-development.The utility of formative evaluations lies in their capacity to refine process outcomes, whereas summative evaluations ascertain how well these outcomes align with preset expectations.</p>
<p>Notably, theory-driven evaluation has been increasingly advocated by scholars, practitioners, and organizations, positioning it as a favored methodology in evaluation practices [31].Such theory-driven evaluations are grounded in a clear theory or model that delineates how the program leads to desired or observed outcomes, with the evaluation itself being informed by this model.The theory-driven evaluation's main principles [31] are:</p>
<p>• A coherent and plausible program theory must be developed.</p>
<p>• Evaluations should center and be prioritized following the framework of the program's theory.• The framework should direct the evaluation's planning, design, and implementation, taking into account pertinent contingencies.</p>
<p>• Framework constructs must be assessed • Any shortcomings or unintended consequences, effectiveness, and causal links between the theoretical constructs should be identified.</p>
<p>B. On Human-centered Metrics</p>
<p>ISO standards' definition of human-centredness is anchored in four quality objectives: usability, accessibility, user experience, and the mitigation of use-related risks [32].It underscores the context of use as a preliminary step to designing human-centeredness metric requirements.Additionally, evaluation metric must be versatile enough to serve both researchers focusing on human-centered design principles and product designers.</p>
<p>Generally, metrics are classified into three categories: quantitative-subjective, qualitative, and integrated, with the choice often dictated by the availability and precision of the underlying data [33].Data accessibility and affordability are crucial; without these, reliance on assumptions or proxy data may compromise the assessment's validity and reliability.The selection of an appropriate measurement metric is contingent upon the user's specific needs and objectives, the field of study, and the data at hand [33].Central to this process is establishing a clear evaluation purpose; absent from this, metrics amount to mere data, lacking actionable values.Additionally, various agencies, companies, and researchers have developed metrics to tackle diverse challenges, often within disparate frameworks [33].This has led to a proliferation of assessment methods characterized by differing rationale, terminology, and methodologies, resulting in contrasting outcomes.The design of universally comprehensive metrics, serving as a panacea for decision-makers, proves to be a formidable task.</p>
<p>C. On Measuring Reasoning Ability</p>
<p>In contemporary psychology, deductive and inductive reasoning are the main forms of reasoning.Deductive reasoning involves deriving a conclusion that is true provided the premises are accurate [34].Conversely, inductive reasoning entails extrapolating beyond the given premises to generate conclusions of new semantic content [34].Notably, people differ in their capacity to reason.While theoretically capable of rational thought, humans exhibit varying degrees of failure in practical reasoning.In principle, individuals accept reasoning as valid when they do not possess a conflicting mental representation [35], [36].</p>
<p>The mental model theory, extensively utilized in understanding both deductive and inductive reasoning [37], conceptualizes thinking as the manipulation of internal models [38].Entities are represented as tokens while relationships connect the tokens.Negations and implicit information are integrated within these models, shaping the reasoning process.</p>
<p>The complexity of reasoning tasks is significantly influenced by the number of mental models congruent with the given premises [34].Unlike the typical psychometric approach, which prioritizes test construction based on statistical criteria, a theory-driven approach would derive test indicators directly from a cognitive model of reasoning processes.</p>
<p>Additionally, Spearman's perspective on intelligence emphasizes the ability to identify relationships and patterns, a skill central to reasoning and reflected in its strong association with general intelligence metrics [34].Also, Carroll's framework [39] identifies three reasoning dimensions: Sequential Reasoning, where conclusions are deduced from given premises or conditions; Induction, involving the detection and application of underlying rules or patterns in presented materials; and Quantitative Reasoning, which entails logical operations with numerical or mathematical concepts.</p>
<p>D. On Evaluation of Large Language Models</p>
<p>Language models, fundamentally designed to process and generate text, become highly versatile when trained on extensive datasets [27].This versatility underscores the necessity for comprehensive benchmarking to ensure their efficacy and transparency across various applications.Evaluation requires defining specific scenarios, a model, and metrics to assess performance.These evaluations span a broad spectrum of user-centric tasks, including question answering [40]- [42], information retrieval [43], summarizing, sentiment analysis, and toxicity detection.</p>
<p>While accuracy is crucial for system reliability [44], it alone does not ensure AI models' utility or desirability.The integration of machine learning models into larger systems demands not only accuracy but also the capability to express uncertainty-vital for error anticipation and management, particularly in high-stakes applications like decision-making processes.Calibration [45], or a model's ability to accurately estimate the probabilities of its predictions, is essential for deploying models in sensitive settings.This is quantitatively assessed through the expected calibration error (ECE), which gauges the alignment between predicted probabilities and actual outcomes [46], [47].</p>
<p>III. OLLABENCH</p>
<p>A. Theoretical Alignment</p>
<p>Evaluation begins with setting the context and the overarching context was presented in figure 1.The paper further narrow this context into the cognitive behavioral compliance and non-compliance of information security policies.With rising geopolitical tensions and state-sponsored mis/dis-information operations, divisive domestic political campaigns, persistent inflation, and growing lay-offs, the paper predicts cognitive behavioral compliance and non-compliance of information security policies is a timely focus for developing LLM-based applications in support of monitoring, modeling, and predicting the risk of insider threats.This initial scoping further involves identifying the main audience who are researchers and Fig. 3.The Knowledge Graph of OllaBench application developers being at the early stage of their LLMbased projects.They mainly need to pick the best LLM for successful project development and most efficient outcomes.Therefore, the chosen methodology is formative evaluation and the evaluative criteria for successful metrics only focus on relevance, alignment, effectiveness, and unintended effects.</p>
<p>Knowledge selection is the next step.Heylighen [48] identified three principal classes of criteria for knowledge selection: objective, subjective, and inter-subjective.Objective criteria evaluate the "objectivity" or "reality" of knowledge.Subjective criteria concern the ease with which an evaluation target can assimilate knowledge.Inter-subjective criteria pertain to the ease of knowledge transmission and assimilation.Identifying the knowledge completeness is always difficult.The paper decided to employ the theory-driven method to formalize the knowledge base in the form of a knowledge graph which is a less formal form of ontology and is the right form for evaluation based on program theory [31].This decision also allows for the highest degree of objectivity to best support the goal of evaluating LLMs' subjective responses.The intersubjective criteria is not currently in focus and will be explored in future projects.</p>
<p>Chosen evaluative metrics are exclusively quantitativesubjective.This decision follows the recommended practices [33] and was dictated by the selected scope and the underlying data.The metrics evaluate both deductive and inductive reasoning capabilities of the targeted LLMs.Notably, the paper assumes the LLMs have indexed the theories and related empirical evidences that belonged to the selected knowledge base.Based on the mental model theory [37], the more difficult evaluative questions require the LLMs to construct more complex internal cognitive models for correct answers.</p>
<p>B. The Mechanisms</p>
<p>This section presents how theoretical alignment principles translate into actual implementation of OllaBench which has four main components of the knowledge graph, the scenario generator, the response generator, and the evaluator (figure 2).Customization of the component parameters are possible via OllaBench's graphic user interface (GUI), changing parameters in json files, or modifying code variable values.The codes are publicly available on Github1 .</p>
<p>1) The Knowledge Graph: The following theories informed the nodes and edges of the knowledge graph (a.k.a Cybonto-4-IC): Deterrence Theory [49], Elaboration Likelihood Model [50], Extended Parallel Processing Model [51], Fear Appeal Theory [52], General Deterrence Theory [53], IT Relatedness Theory [54], Neutralization Theory [55], Organizational Support Theory [56], Protection Motivation Theory [57], Rational Choice Theory [58], Self-Determination Theory [59], Self-Efficacy Theory [60], Situation Action Theory [61], Social Bond Theory [62], Social Cognitive Learning Theory [63], Social Exchange Theory [64], Social Norms Theory [65], Technology Threat Avoidance Theory [66], Theory of Acceptance Model [67], Theory of Computer Crime Opportunity Structure [68], Theory of Emotion Process [69], Theory of Interpersonal Behavior [70], Theory of Planned Behavior [71], and Theory of Reasoned Action [72].</p>
<p>The nodes are the cognitive behavioral constructs while the edges are the relationships among the nodes, all of which were proposed by the theories.The paper further looked up 38 peer-reviewed papers to gather empirical supports for the edges.Empirical evidence is usually gathered using statistical approaches involving factor analysis and multiple regression analysis such as Structural Equation Modeling [73].Further details can be found at the project's online Cybonto-4-InterdendentCybersecurity page [74].</p>
<p>2) The Scenario Generator: Each node has two set of seed description -one for information security policy compliance and another one for non-compliance.A set of seed description for each node (a cognitive behavioral construct) contains at least two sentences describing the cognitive behavioral details specific to the node.These sentences came from the validated and evaluated survey instruments used by the peer-reviewed papers.Each sentence was mutated into a library of similar sentences using an LLM -the paper used GPT-4.These sentences form a verbal description library for each node.Below is one example of how seeds were mutated.Seed and verbal description library datasets are also available on the project's Github repository.</p>
<p>Sample mutation of seeds:</p>
<p>Construct: Belief Compliant seed: The person believes that rules are made to be followed.Compliant mutation: The individual believes in strict adherence to rules and regulations.Non-compliant seed: The person believes that it's ok to get around a policy if the person can get away with it.Non-compliant mutation: The individual sees loopholes in rules as acceptable if they go unnoticed.</p>
<p>A cognitive behavioral profile generation begins with selecting a random path of length n from the knowledge graph.The paper recommends n = 4 which yields a list of 5 nodes.Nodes are then assigned properties of compliant or non-compliant to information security policies.The maximum and minimum numbers of non-compliant nodes in a cognitive behavioral profile are 5 and 0 respectively.From the list of selected nodes, node properties, and corresponding verbal description libraries, the verbalized cognitive behavioral profiles are made.This modular approach allows users to customize scenarios for their evaluation purposes.Each scenario contains two cognitive profiles of two hypothetical employees.The standard benchmark dataset generated by this paper has 10,000 unique scenarios.</p>
<p>3) The Response Generator: For each unique scenario, the response generator asks the targeted LLMs the same set of multiple (4) choice questions as follows:</p>
<p>• Which of the following options best reflects [Person A]'s or [Person B]'s cognitive behavioral constructs?</p>
<p>• Who is [LESS/MORE] compliant with information security policies?• Will information security non-compliance risk level [increase/decrease] if these employees work closely in the same team?• To [increase information security compliance/reduce information security non-compliance], which cognitive behavioral factor should be targeted for strengthening?</p>
<p>Each choice begins with "(option [a/b/c/d])" and LLMs were instructed to begin their answers with "(option [a/b/c/d])".The instruction does not limit LLMs' responses which means LLMs are free to elaborate on their answer choices as they wish.</p>
<p>A sample cognitive behavioral profile:</p>
<p>Here are the intelligence about Olivia Hernandez with comments from trusted experts and/or Olivia Hernandez's recorded statement(s):</p>
<p>• It comes easily to the individual to utilize cybersecurity protection software.• The individual is knowledgeable about the breaches and repercussions of the company's protocols.• The individual is capable of easily utilizing security software for online protection.• The individual views following the institution's policies regarding information security as compulsory.• My disregard for information security policies could benefit me.</p>
<p>4) The Evaluator: For each correct response, the evaluator gives a score of 1 and 0 vice versa.This grading involves comparing LLMs' responses with reference answers which can be reliably constructed as follows.</p>
<p>For question 1 (a.k.a the Which-Cog-Path question), the reference answer is the list of nodes based on the selected path used for the cognitive behavioral profile of either person A or B. The choices includes some permutations of the correct answer.To answer this question correctly, the LLMs must be able to map each description sentence back to the right construct which is possible if the LLMs are aware of the relevant cognitive behavioral instruments used by numerous peer-reviewed papers.</p>
<p>For question 2 (a.k.a the Who-is-Who question), person A is more compliant than person B if A has more nodes of compliant attribute than B. The LLMs must be able to count the numbers of compliant and non-compliant attributes of the nodes (cognitive behavioral constructs) in each profile to compare them.There is appropriate option for the LLMs to choose if A and B share the same numbers of node attributes.</p>
<p>For question 3 (a.k.a the Team-Risk question), there are 3 possibilities.First, if both individuals A and B have no nodes with non-compliant attributes, there will be no change in the team's compliance risk profile.Second, if A and B possess non-overlapping nodes with non-compliant attributes, this results in a higher aggregate number of non-compliant nodes, thereby increasing the non-compliant risk profile should they collaborate in a team.Third, if A and B share the same non-compliant nodes, the impact on the non-compliant risk profile may vary.The main rationale for these dynamics is that team members tend to influence and adopt each other's behaviors, including potentially harmful cognitive behavioral patterns.This assertion is supported by various theoretical frameworks including the Social Cognitive Learning Theory [63], Social Norms Theory [65], Theory of Interpersonal Behavior [70], and Theory of Computer Crime Opportunity Structure [68].</p>
<p>For question 4 (a.k.a the Target-Factor question), if A and B shares one non-compliant node then that cognitive behavioral construct (node) is the correct answer.For other possibilities, the target construct for strengthening should be the node (construct) with the highest page-rank among the combined non-compliant nodes or the combined compliant nodes (if there is no non-compliant node).In terms of difficulty, question 4 is the hardest since it requires the LLMs to think beyond the information presented by the scenario.The LLM that can construct the most robust networks around the involved nodes will perform the best in answering this question.</p>
<p>C. The Metrics</p>
<p>The paper denotes A W CP , A W HO , A T R , A T F as the accuracy values when a model responses to Which-Cog-Path questions, Who-is-Who questions, Team-Risk questions, and Target-Factor questions respectively.Such values can also be called as "Categorical Accuracy" or A cat .In a category cat, the number of questions is n cat and the number of correct responses is r cat .The formula for calculating categorical accuracy is
A cat = r cat n cat(1)
The overall Accuracy score is the average of the categorical accuracy values.The paper denotes W x for LLM x's wastefulness value.n missed is the total number of the model's wrong answer.T i is the tokens used to generate a wrong answer i.The wastefulness of a model x is
W x = n missed −1 i=0 T i n missed(2)
Finally, the consistency metrics measures the possibility of whether the LLMs' responses are consistent with a knowledge model.To achieve this, the paper employs Structural Equation Modeling [73], a comprehensive statistical approach used to assess complex relationships among variables.Initially, the paper propose a model (figure 4) which is based on the paper's subjective knowledge of the domain.</p>
<p>In Figure 4, "WHO," "WCP," "TR," and "TF" represent scores from the LLMs' answers to the Who-is-Who questions, the Which-Cognitive-Path questions, the Team-Risk questions, and the Target-Factor questions, respectively.A directed arrow</p>
<p>IV. EVALUATING LLMS RESULTS</p>
<p>Key findings:</p>
<p>• Commercial LLMs have the highest overall accuracy scores as expected, although the scores should be higher.</p>
<p>• Smaller and low-resolution open-weight LLMs are not far from the leading commercial LLMs.</p>
<p>• There are large margins between the most efficient LLMs and the most wasteful LLMs in terms of tokens spent in wrong answers.</p>
<p>• The most accurate LLMs are most consistent in answering questions.</p>
<p>OllaBench was used to evaluate the following LLM models: 1) aya-8b-23-q4 (released by Cohere) 2) claude-3-opus-20240229 (released by Anthropic) 3) deepseek-llm-7b-chat-q4 (released by Deepseek ) 4) gemini-1.5-flash-latest(released by Google) 5) gpt-4o (released by OpenAI)   6 show the LLMs performance in categorical accuracy.Figure 5 shows the overall LLMs accuracy in answering questions. Figure 7 shows the LLMs wastefulness (the lower the better).Full benchmark results are available at the project's Github page 2   V. CONCLUSION Large Language Models (LLMs) can be highly useful in building realistic agents for Agent-Based Modeling (ABM).ABM agents can represent subsystems and their entities, governed by flexible learnt rules that facilitate the simulation of complex relationships and interactions [23], [24].This bottom-up approach allows the micro-level agent behaviors to collectively generate emergent macro-level structures.This unique capability may help with addressing Interdependent Cybersecurity problems that involve complex networks of humans, infrastructures, technologies, finance, and so on.</p>
<p>Evaluation of LLMs is required by law especially when the LLMs' decisions may affect humans.Evaluation of LLMs is also essential for LLM-based application development and commercial deployment.We notice that most recent LLM evaluation works for cybersecurity does not involve human factor while LLM reasoning evaluation works do not involve both cybersecurity and human factor.This phenomenon is consistent with the fact that research and applications focusing on human-centered IC remains limited [6] while contemporary LLM evaluation frameworks, such as Stanford's HELM [27], fail to assess the cognitive computing capabilities of LLMs.</p>
<p>Therefore, OllaBench was born to help both researchers and application developers conveniently evaluate their LLM models within the context of cybersecurity compliance or noncompliance behaviors.Specifically, OllaBench can rank LLMs based on their accuracy, wastefulness, and consistency in answering scenario-based information security compliance/noncompliance questions.After applying OllaBench in evaluating 21 LLMs including major commercial LLMS, the key findings are:</p>
<p>• Commercial LLMs have the highest overall accuracy scores as expected, although the scores should be higher.</p>
<p>• Smaller and low-resolution open-weight LLMs are not far from the leading commercial LLMs.</p>
<p>• There are large margins between the most efficient LLMs and the most wasteful LLMs in term of tokens spent in wrong answers.</p>
<p>2 https://github.com/Cybonto/OllaBench/tree/main/BenchmarkResults</p>
<p>• The most accurate LLMs are most consistent in answering questions.</p>
<p>OllaBench results are reliable because OllaBench was rigorously designed based on a solid foundation of 24 cognitive behavioral theories and empirical evidence from 38 peerreviewed papers.OllaBench is also robust as it supports evaluation of all open-weight models and models from major platforms such as OpenAI, Anthropic, and Google.OllaBench is user-friendly with supported command-line interface and graphical user interface.Early stage solution developers may leverage OllaBench to identify the best LLM for their humancentric IC application development.Researchers may leverage OllaBench to measure their fine-tuned LLMs' performances.Customizing OllaBench for other human-centric purposes is convenient with carefully documented source codes publicly available on Github.</p>
<p>Fig. 1 .
1
Fig. 1.The Human-centered Application Areas for Interdependent Cybersecurity</p>
<p>Fig. 2 .
2
Fig. 2. The Design of OllaBench</p>
<p>Fig. 4 .
4
Fig. 4. Consistency Model</p>
<p>Fig. 5 .
5
Fig. 5. Overall Accuracy</p>
<p>TABLE I RECENT
I
WORKS ON EVALUATION OF LLMS APPLICABLE TOWARDS INTERDEPENDENT CYBERSECURITY
WorkYear H C RTypeGroundingNovel ContributionOLLABENCH2024•••Scenarios24 Cognitive Behavioral Theories
[11]ledge graph, dataset (10000 items), dataset generator, GUI interface Agrawal et al.[10]2024 • QA AISecKG Data set generator (CyberGen) and benchmark dataset (CyberQ -4000 items) Bhat et al.[11]2024 • • Test case Mitre ATT&amp;CK False refusal rate and propensity of complying with malicious instructions Garza Assessing 2023 • QA Mitre ATT&amp;CK Prompt engineering insights in threat behavior domain Guha et al. [12] 2024 • Scenarios Expert knowledge and experience Benchmark dataset for evaluating LLMs' legal reasoning Hendrycks et al. [13] 2021 • Scenarios Five core ethics theories The ETHICS benchmark dataset (130,000 items) Jin et al. [14] 2024 • Scenarios Existing causal graphs and queries Benchmark dataset for evaluating LLMs' formal causal reasoning Jin et al. [15] 2024 • • Scenarios Mitre ATT&amp;CK Retrieval-Aware Training with Reason method, Abstract Syntax Trees metric Kosinki et al. [16] 2024 • • Scenarios Theory of Mind Benchmark dataset to measure LLMs' performance on false-belief tasks Li et al. [17] 2024 • QA &amp; Scenarios Existing taxonomies and policies Multi-level benchmark dataset with attack/defense enhanced scenarios Scherrer et al. [18] 2024 • Scenarios Five papers on moral psychology A statistical method for eliciting encoded moral beliefs in LLMs Sun et al. [19] 2024 • • Scenarios 75 Smart Contract vulnerabilities 4950 scenarios to measure LLMs vulnerability reasoning Yuan et al. [20] 2024 • Test case Self-curated Risk Taxonomy Expert safety-critique LLM to generate prompt-based evaluation tests Note: H, C, R indicate the papers' focus on Human Factor, Cybersecurity, and Reasoning respectively.</p>
<p>TABLE II STRUCTURAL
II
EQUATION MODELING RESULTS -TOP 5
FigureModelEstimate Std. Errz-value p-valuemixtral-8x7b-0.9520.0006-1,546.090openchat-7b-v3.5-q4-0.95410-19,361.530Team Risk ∼Which Cognitive Pathorca2-13b-0.93980-36,803.240yi-9b-chat-v1.5-q41.34530 1,345,339.470wizardlm2-7b-q41.5240.68442.22670.026gemini-1.5-flash-latest-0.12250.0136-9.00780gpt-4o0.1720.009418.22740Team Risk ∼Who is Whollama3-8b-instruct-q40.08480.008210.30260llama2-7b0.08460.01834.62470tinyllama-1.1b-chat-v0.6-q40.02760.01232.24540.0247mixtral-8x7b0.86620.95740.90470.3656openchat-7b-v3.5-q40.86470.99360.87020.3842Target Factor ∼Which Cognitive Pathopenhermes-7b-mistral-v2.5-q40.86481.06620.81110.4173mistral-7b0.14310.18310.78190.4343orca2-7b0.54361.56210.3480.7278
• A novel knowledge graph (Cybonto-4-IC) grounded on 24 cognitive behavioral theories and 38 peer-reviewed papers. • A scenario-based default benchmark dataset with 10,000 items about cognitive behavioral cybersecurity compliance or non-compliance.
https://github.com/Cybonto/OllaBench</p>
<p>10) mixtral-8x7b (released by Mistral AI) 11) openchat-7b-v3.5-q4 (released by OpenChat) 12) openhermes-7b-mistral-v2.5-q4 (released by Teknium) 13) orca2-13b (released by Microsoft Research) 14) orca2-7b (released by Microsoft Research) 15) phi-2.7b (released by Microsoft Research) 16) qwen-4b-chat-v1.5-q4 (released by Alibaba Cloud) 17) solar-10.7b-instruct-v1-q4 (released by Upstage) 18) tinyllama-1.1b-chat-v0.6-q4 (released by Peiyuan Zhang et al.) 19) vicuna-13b (released by LMSYS org) 20). A I Mistral, mistral-7b. released by Microsoft AI) 21) yi-9b-chat-v1.5-q4 (released by 01.AI</p>
<p>All open-weight LLMs are 4-bit quantized. The top 5 LLMs in answering Which-Cognitive-Path questions are: 1) gemini-1.5-flash-latest (0.4485) 2) gpt-4o (0.4306) 3) claude-3-opus-20240229 (0.4177) 4) mixtral-8x7b (0.3593) 5) mistral-7b (0.3221) The top 5 LLMs in answering Who-is-Who questions are: 1) claude-3-opus-20240229 (0.649) 2). orca2-13b (0.6252) 3) llama2-13b (0.6054) 4) llama2-7b (0.6027</p>
<p>The top 5 LLMs in answering Team-Risk questions are: 1) gemini-1.5-flash-latest (0.5536) 2) mistral-7b (0.5322) 3) llama3-8b. mixtral-8x7b (0.6013instruct-q4 (0.5168</p>
<p>The top 5 LLMs in answering Target-Factor questions are: 1) yi-9b-chat-v1.5-q4 (0.4597) 2) gemini-1.5-flash-latest (0.4467) 3) vicuna-13b (0.4241) 4) claude-3-opus-20240229 (0.4213) 5) mixtral-8x7b (0.4093) The top 5 LLMs in overall accuracy are: 1) gemini-1.5-flash-latest (0.5102) 2) gpt-4o (0.4735) 3) claude-3-opus-20240229 (0.4688) 4) mixtral-8x7b (0.4667) 5) mistral-7b (0.4634) The top 5 LLMs in efficiency. 7b-mistral-v2.5-q4 (0.5083) 5) llama2-13b (0.5047lowest in wastefulness) are: 1) openchat-7b-v3.5-q4 (6.427 tokens) 2) phi-2.7b (34.5295 tokens</p>
<p>tokens) 5) openhermes-7b-mistral-v2.5-q4 (48.2287 tokens) The top 5 LLMs in wastefulness are: 1) orca2-13b (402.038 tokens) 2) orca2-7b (377.1546 tokens) 3) wizardlm2-7b-q4 (305.9823 tokens) 4) solar-10. 4b-chat-v1.5-q4 (36.7592 tokens) 4) gpt-4o (42.21217b-instruct-v1-q4 (184.0692 tokens) 5) llama2-7b (138.885 tokens</p>
<p>The Global Risks Report 2021 16th Edition. M Mclennan, World Economic Forum. 2021Tech. Rep</p>
<p>Interdependent security. H Kunreuther, G Heal, Journal of risk and uncertainty. 2622003</p>
<p>V S Mai, R J La, A Battou, Optimal cybersecurity investments in large networks using SIS model: Algorithm design. 202129</p>
<p>Why today's cybersecurity threats are more dangerous. C Brumfield, 2021</p>
<p>Connecting Risk And Resilience For Sustainable Growth. J Bremen, 2022</p>
<p>Systematically Understanding Cybersecurity Economics: A Survey. M Kianpour, S J Kowalski, H Øverby, Sustainability. 1324136772021</p>
<p>A survey of interdependent information security games. A Laszka, M Felegyhazi, L Buttyan, ACM Computing Surveys (CSUR). 4722014</p>
<p>Framework for improving critical infrastructure cybersecurity version 1.1. M P Barrett, NIST, Tech. Rep. 2018</p>
<p>Cognitive security: A comprehensive study of cognitive science in cybersecurity. R O Andrade, S G Yoo, Journal of Information Security and Applications. 481023522019</p>
<p>CyberQ: Generating Questions and Answers for Cybersecurity Education Using Knowledge Graph-Augmented LLMs. G Agrawal, K Pal, Y Deng, H Liu, Y C Chen, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>CyberSecEval 2: A Wide-Ranging Cybersecurity Evaluation Suite for Large Language Models. M Bhatt, S Chennabasappa, Y Li, C Nikolaidis, D Song, S Wan, F Ahmad, C Aschermann, Y Chen, D Kapil, D Molnar, S Whitman, J Saxe, 2024</p>
<p>Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models. N Guha, J Nyarko, D Ho, C Ré, A Chilton, A Chohlas-Wood, A Peters, B Waldon, D Rockmore, D Zambrano, Advances in Neural Information Processing Systems. 202436</p>
<p>ALIGNING AI WITH SHARED HUMAN VALUES. D Hendrycks, C Burns, S Basart, A Critch, J Li, D Song, J Steinhardt, ICLR 2021 -9th International Conference on Learning Representations. 2021</p>
<p>Cladder: A benchmark to assess causal reasoning capabilities of language models. Z Jin, Y Chen, F Leeb, L Gresele, O Kamal, Z Lyu, K Blin, F Gonzalez Adauto, M Kleiman-Weiner, M Sachan, Advances in Neural Information Processing Systems. 202436</p>
<p>Crimson: Empowering Strategic Reasoning in Cybersecurity through Large Language Models. J Jin, B Tang, M Ma, X Liu, Y Wang, Q Lai, J Yang, C Zhou, arXiv:2403.008782024arXiv preprint</p>
<p>Evaluating Large Language Models in Theory of Mind Tasks. M Kosinski, 2023</p>
<p>L Li, B Dong, R Wang, X Hu, W Zuo, D Lin, Y Qiao, J Shao, arXiv:2402.05044SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models. 2024arXiv preprint</p>
<p>Evaluating the Moral Beliefs Encoded in LLMs. N Scherrer, C Shi, A Feder, D M Blei, 2023</p>
<p>LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning. Y Sun, D Wu, Y Xue, H Liu, W Ma, L Zhang, M Shi, Y Liu, arXiv:2401.161852024arXiv preprint</p>
<p>S-Eval: Automatic and Adaptive Test Generation for Benchmarking Safety Evaluation of Large Language Models. X Yuan, J Li, D Wang, Y Chen, X Mao, L Huang, H Xue, W Wang, K Ren, J Wang, arXiv:2405.141912024arXiv preprint</p>
<p>An agent-based model to evaluate the COVID-19 transmission risks in facilities. E Cuevas, Computers in Biology and Medicine. 121January, p. 293, 2020</p>
<p>Agent-Based Modeling in Economics and Finance: Past, Present, and Future. R L Axtell, J D Farmer, Journal of Economic Literature. 2022</p>
<p>Editorial: Meeting grand challenges in agent-based models. L An, V Grimm, B L Turner, Jasss. 2312020</p>
<p>Simulating Systems-of-Systems with Agent-Based Modeling: A Systematic Literature Review. R D A Silva, R T Braga, IEEE Systems Journal. 1432020</p>
<p>Generative Agents: Interactive Simulacra of Human Behavior. J S Park, J O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, UIST 2023 -Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. 2023</p>
<p>Executive order on the safe, secure, and trustworthy development and use of artificial intelligence. J R Biden, 2023</p>
<p>Holistic Evaluation of Language Models. R Bommasani, P Liang, T Lee, Annals of the New York Academy of Sciences. 152512023</p>
<p>Evaluation Science. M Q Patton, American Journal of Evaluation. 3922018</p>
<p>Evaluative Criteria: An Integrated Model of Domains and Sources. R M Teasdale, American Journal of Evaluation. 4232021</p>
<p>FEDS: A Framework for Evaluation in Design Science Research. J Venable, J Pries-Heje, R Baskerville, European Journal of Information Systems. 2512016</p>
<p>A systematic review of theory-driven evaluation practice from 1990 to 2009. C L Coryn, L A Noakes, C D Westine, D C Schröter, American Journal of Evaluation. 3222011</p>
<p>O Sankowski, D Krause, The Human-Centredness Metric: Early Assessment of the Quality of Human-Centred Design Activities. 20231312090</p>
<p>Metrics for Technology Evaluation. 2013PSU</p>
<p>Handbook of understanding and measuring intelligence. O Wilhelm, 2005Measuring reasoning ability</p>
<p>Models and deductive rationality. P N Johnson-Laird, R M J Byrne, Rationality: Psychological and philosophical perspectives. Taylor &amp; Frances/Routledge1993</p>
<p>Who is rational?: Studies of individual differences in reasoning. K E Stanovich, 1999Psychology Press</p>
<p>Models of deduction. P N Johnson-Laird, Reasoning: Representation and process. Psychology Press2015</p>
<p>The nature of explanation. K J W Craik, CUP Archive. 4451967</p>
<p>Human cognitive abilities: A survey of factor-analytic studies. J B Carroll, 1993Cambridge University Press</p>
<p>Natural questions: a benchmark for question answering research. T Kwiatkowski, J Palomaki, O Redfield, M Collins, A Parikh, C Alberti, D Epstein, I Polosukhin, J Devlin, K Lee, Transactions of the Association for Computational Linguistics. 72019</p>
<p>The narrativeqa reading comprehension challenge. T Kočiský, J Schwarz, P Blunsom, C Dyer, K M Hermann, G Melis, E Grefenstette, Transactions of the Association for Computational Linguistics. 62018</p>
<p>QUAC: Question answering in context. E Choi, H He, M Iyyer, M Yatskar, W T Yih, Y Choi, P Liang, L Zettlemoyer, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018. 2018</p>
<p>An introduction to information retrieval. C D Manning, 2009Cambridge university press</p>
<p>The fallacy of AI functionality. I D Raji, I E Kumar, A Horowitz, A Selbst, Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. the 2022 ACM Conference on Fairness, Accountability, and Transparency2022</p>
<p>The comparison and evaluation of forecasters. M H Degroot, S E Fienberg, Journal of the Royal Statistical Society: Series D (The Statistician). 321-21983</p>
<p>Obtaining well calibrated probabilities using bayesian binning. M P Naeini, G Cooper, M Hauskrecht, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201529</p>
<p>On calibration of modern neural networks. C Guo, G Pleiss, Y Sun, K Q Weinberger, International conference on machine learning. 2017</p>
<p>Objective, subjective and intersubjective selectors of knowledge. F Heylighen, Cognition. 1997</p>
<p>Does deterrence work in reducing information security policy abuse by employees?. Q Hu, Z Xu, T Dinev, H Ling, Communications of the ACM. 5462011</p>
<p>The elaboration likelihood model of persuasion. R E Petty, J T Cacioppo, Advances in Experimental Social Psychology. 19C1986</p>
<p>Putting the fear back into fear appeals: The extended parallel process model. K Witte, Communication Monographs. 5941992</p>
<p>Fear Appeal Theory. K Williams, Research in Business and Economics Journal. 52012</p>
<p>Protection motivation and deterrence: a framework for security policy compliance in organisations. T Herath, H R Rao, European Journal of information systems. 182009</p>
<p>Information technology relatedness, knowledge management capability, and performance of multibusiness firms. H Tanriverdi, MIS quarterly. 2005</p>
<p>Techniques of Neutralization: A Theory of Delinquency. G M Sykes, D Matza, American Sociological Review. 2261957</p>
<p>Social exchange in organizations: Perceived organizational support, leader-member exchange, and employee reciprocity. R P Settoon, N Bennett, R C Liden, Journal of applied psychology. 8132191996</p>
<p>in Handbook of health behavior research 1: Personal and social determinants. R W Rogers, S Prentice-Dunn, 1997Plenum PressNew York, NY, USProtection motivation theory</p>
<p>La philosophie contemporaine/Contemporary philosophy: Chroniques nouvelles/A new survey. J Elster, 1982Rationality</p>
<p>Self-determination theory. E L Deci, R M Ryan, Handbook of theories of social psychology. Thousand Oaks, CASage Publications Ltd20121</p>
<p>Self-efficacy: The exercise of control. A Bandura, 1997W H Freeman/Times Books/ Henry Holt &amp; CoNew York, NY, US</p>
<p>Violence as situational action. P.-O H Wikstrom, K H Treiber, International Journal of Conflict and Violence. 312009</p>
<p>T Hirschi, Causes of delinquency. BerkeleyUniversity of California Press2017</p>
<p>Social Cognitive Theory: An Agentic Perspective. A Bandura, 10.1146/annurev.psych.52.1.1Annual Review of Psychology. 5212 2001</p>
<p>Exchange and power in social life. P M Blau, 2017Routledge</p>
<p>Changing the culture of college drinking: A socially situated health communication campaign. A D Berkowitz, 20051An overview of the social norms approach</p>
<p>Avoidance of information technology threats: A theoretical perspective. H Liang, Y Xue, MIS quarterly. 2009</p>
<p>User Acceptance of Information Technology: Toward a Unified View. V Venkatesh, M G Morris, G B Davis, F D Davis, MIS Quarterly. 2732003</p>
<p>Opportunities for computer crime: considering systems risk from a criminological perspective. R Willison, J Backhouse, European journal of information systems. 1542006</p>
<p>Relations among emotion, appraisal, and emotional action readiness. N H Frijda, P Kuipers, E Ter Schure, Journal of personality and social psychology. 5722121989</p>
<p>Values, attitudes, and interpersonal behavior. H C Triandis, Nebraska symposium on motivation. University of Nebraska Press1979</p>
<p>The theory of planned behavior. I Ajzen, Organizational Behavior and Human Decision Processes. 5021991</p>
<p>A theory of reasoned action: some applications and implications. M Fishbein, Nebraska Symposium on Motivation. Nebraska Symposium on Motivation. 198027</p>
<p>Structural equation modeling. J B Ullman, P M Bentler, 20122Handbook of Psychology. Second Edition</p>
<p>CYBONTO-1.0/Cybonto-4-InterdependentCybersecurity.md at main • Cybonto/CYBONTO-1.0. T Nguyen, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
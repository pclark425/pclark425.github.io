<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-442 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-442</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-442</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-227247527</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2012.01172v1.pdf" target="_blank">ReproducedPapers.org: Openly teaching and structuring machine learning reproducibility</a></p>
                <p><strong>Paper Abstract:</strong> We present ReproducedPapers.org: an open online repository for teaching and structuring machine learning reproducibility. We evaluate doing a reproduction project among students and the added value of an online reproduction repository among AI researchers. We use anonymous self-assessment surveys and obtained 144 responses. Results suggest that students who do a reproduction project place more value on scientific reproductions and become more critical thinkers. Students and AI researchers agree that our online reproduction repository is valuable.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e442.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e442.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>paper-implementation gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discrepancies between natural-language paper descriptions and their code implementations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents general occurrences where implementations (code) and published natural-language descriptions (paper methods, results) differ, leading to partial or failed reproductions; these are observed via student reproduction projects, self-reported blogs, and surveys and are linked to missing/ambiguous details, hidden implementation choices, and incentive/resource issues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reproduction workflow / experimental pipeline (student reproduction projects and repository)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A light-weight, open online repository (ReproducedPapers.org) and associated student reproduction projects used to reproduce figures/tables from ML papers; contributions include blog posts, code evaluations, hyperparameter checks and full re-implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods/results description and experimental protocol</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>existing code evaluation, re-implementation from scratch, and experiment scripts contributed by students</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / ambiguous description / omitted implementation details</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Students reported and experienced concrete differences between published results and implementations. The paper frames these differences as arising when paper text and experimental descriptions omit details (e.g., implicit preprocessing, training tricks, or parameter choices) or when available code diverges from the paper's stated method, making reproductions partial, inconsistent, or failing outright.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>multiple pipeline locations including data preprocessing, training procedure, hyperparameters, and evaluation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual reproduction efforts by students (re-implementations and evaluations of available code), self-reported blog posts uploaded to the repository, and surveys of contributors and AI researchers</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Repository-derived counts and self-reported outcomes: Figure 2 reports success and failure rates (from self-reported blog posts) both to be around 40%; repository statistics (90 registered users, 24 unique papers, 57 reproductions) and survey response summaries (43 student respondents, 101 external AI researchers) were used to quantify prevalence and perceptions.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Reproduction attempts sometimes failed or only partially succeeded; the paper does not report concrete numeric performance deltas but reports that both success and failure rates are ~40% (self-reported). Impacts described include inability to obtain claimed results reliably, increased critical thinking among students, and community-level perceptions that reproductions are valuable but under-rewarded.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed frequently in the repository's contributions: self-reported success and failure rates around 40% each according to Figure 2; repository hosts 57 reproductions across 24 papers, indicating multiple reproductions per paper and recurring issues.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguous or incomplete natural-language descriptions in papers, omitted implementation details, misaligned incentives (novelty vs. reproduction), resource constraints, and high barrier of rigorous peer-reviewed reproductions; the paper explicitly mentions misaligned incentives and omitted details as contributing reasons.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Provide a low-barrier public repository (ReproducedPapers.org) to collect light-weight reproductions (including partial reproductions, hyperparameter checks, code evaluations), integrate reproduction projects in teaching to train students to detect and document gaps, encourage sharing of code and blog-style documentation, and offer badges/aspects to structure and label types of reproduction (e.g., Replicated, Reproduced, Hyperparams check).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Preliminary and qualitative: repository uptake (90 registered users, 57 reproductions) and surveys show positive responses — students reported increased critical thinking and valuing reproductions; AI researchers and students consider the repository valuable. No direct quantitative reduction in gap incidence is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / reproducibility in ML</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReproducedPapers.org: Openly teaching and structuring machine learning reproducibility', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e442.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e442.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>hyperparams-aspect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hyperparameter sensitivity checks (repository aspect)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The repository explicitly supports a 'Hyperparams check' aspect for contributors to evaluate hyperparameter sensitivity and robustness, recognizing hyperparameter mismatches as a source of divergence between natural-language descriptions and code/experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReproducedPapers.org aspect tagging and reproduction experiments</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ReproducedPapers.org allows contributors to tag reproductions with aspects (e.g., Hyperparams check) and to submit work that inspects hyperparameter sensitivity or performs hyperparameter sweeps to investigate reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / reported experimental hyperparameters</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment scripts and code evaluations submitted by repository contributors (student blogs, evaluated existing code, or re-implementations)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hyperparameter mismatch / missing hyperparameter specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The repository includes a dedicated 'Hyperparams check' aspect to capture cases where reproductions investigate sensitivity to hyperparameters; this implies recognition that papers often omit or under-specify hyperparameter settings or that default implementation choices in code differ from those implied in text, potentially causing divergent results.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>hyperparameters and training configuration</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>explicit hyperparameter sensitivity experiments and evaluations performed by contributors (tagged as Hyperparams check) and self-reported outcomes in blogs; usage counts in repository metadata identify such checks.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Usage statistics: 'Hyperparams check' was used 17 times (one of the top-3 most-used aspects). Outcomes of these checks are part of the self-reported success/failure metadata (Figure 2) but the paper does not report specific performance deltas from hyperparameter changes.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Potentially large: hyperparameter mismatches are implied to be a contributor to failed or partial reproductions; the paper does not provide quantified effect sizes for hyperparameter-induced performance changes, only that hyperparameter checks are commonly used and that reproductions sometimes fail (~40% failure reported overall).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>'Hyperparams check' aspect used 17 times across the repository's contributions; among top-3 aspects (Replicated 32, Reproduced 29, Hyperparams check 17).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Under-specification of hyperparameters in papers, implicit defaults in code, and lack of standardized reporting of hyperparameter search/details.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Encourage contributors to perform and report hyperparameter sensitivity analyses via the repository; allow light-weight contributions to document hyperparameter effects; label reproductions with the 'Hyperparams check' aspect to make such investigations discoverable.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantitatively evaluated in the paper; repository usage data shows contributors are performing such checks (17 uses), and survey results indicate participants value reproductions, but no measured reduction in gap incidence or performance differences reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep learning experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReproducedPapers.org: Openly teaching and structuring machine learning reproducibility', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO <em>(Rating: 2)</em></li>
                <li>A step toward quantifying independently reproducible machine learning research <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning that matters <em>(Rating: 2)</em></li>
                <li>Do imagenet classifiers generalize to imagenet? <em>(Rating: 2)</em></li>
                <li>Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches <em>(Rating: 2)</em></li>
                <li>DLPaper2Code: Autogeneration of code from deep learning research papers <em>(Rating: 1)</em></li>
                <li>Are gans created equal? a large-scale study <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-442",
    "paper_id": "paper-227247527",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "paper-implementation gap",
            "name_full": "Discrepancies between natural-language paper descriptions and their code implementations",
            "brief_description": "The paper documents general occurrences where implementations (code) and published natural-language descriptions (paper methods, results) differ, leading to partial or failed reproductions; these are observed via student reproduction projects, self-reported blogs, and surveys and are linked to missing/ambiguous details, hidden implementation choices, and incentive/resource issues.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Reproduction workflow / experimental pipeline (student reproduction projects and repository)",
            "system_description": "A light-weight, open online repository (ReproducedPapers.org) and associated student reproduction projects used to reproduce figures/tables from ML papers; contributions include blog posts, code evaluations, hyperparameter checks and full re-implementations.",
            "nl_description_type": "research paper methods/results description and experimental protocol",
            "code_implementation_type": "existing code evaluation, re-implementation from scratch, and experiment scripts contributed by students",
            "gap_type": "incomplete specification / ambiguous description / omitted implementation details",
            "gap_description": "Students reported and experienced concrete differences between published results and implementations. The paper frames these differences as arising when paper text and experimental descriptions omit details (e.g., implicit preprocessing, training tricks, or parameter choices) or when available code diverges from the paper's stated method, making reproductions partial, inconsistent, or failing outright.",
            "gap_location": "multiple pipeline locations including data preprocessing, training procedure, hyperparameters, and evaluation protocol",
            "detection_method": "manual reproduction efforts by students (re-implementations and evaluations of available code), self-reported blog posts uploaded to the repository, and surveys of contributors and AI researchers",
            "measurement_method": "Repository-derived counts and self-reported outcomes: Figure 2 reports success and failure rates (from self-reported blog posts) both to be around 40%; repository statistics (90 registered users, 24 unique papers, 57 reproductions) and survey response summaries (43 student respondents, 101 external AI researchers) were used to quantify prevalence and perceptions.",
            "impact_on_results": "Reproduction attempts sometimes failed or only partially succeeded; the paper does not report concrete numeric performance deltas but reports that both success and failure rates are ~40% (self-reported). Impacts described include inability to obtain claimed results reliably, increased critical thinking among students, and community-level perceptions that reproductions are valuable but under-rewarded.",
            "frequency_or_prevalence": "Observed frequently in the repository's contributions: self-reported success and failure rates around 40% each according to Figure 2; repository hosts 57 reproductions across 24 papers, indicating multiple reproductions per paper and recurring issues.",
            "root_cause": "Ambiguous or incomplete natural-language descriptions in papers, omitted implementation details, misaligned incentives (novelty vs. reproduction), resource constraints, and high barrier of rigorous peer-reviewed reproductions; the paper explicitly mentions misaligned incentives and omitted details as contributing reasons.",
            "mitigation_approach": "Provide a low-barrier public repository (ReproducedPapers.org) to collect light-weight reproductions (including partial reproductions, hyperparameter checks, code evaluations), integrate reproduction projects in teaching to train students to detect and document gaps, encourage sharing of code and blog-style documentation, and offer badges/aspects to structure and label types of reproduction (e.g., Replicated, Reproduced, Hyperparams check).",
            "mitigation_effectiveness": "Preliminary and qualitative: repository uptake (90 registered users, 57 reproductions) and surveys show positive responses — students reported increased critical thinking and valuing reproductions; AI researchers and students consider the repository valuable. No direct quantitative reduction in gap incidence is reported.",
            "domain_or_field": "machine learning / reproducibility in ML",
            "reproducibility_impact": true,
            "uuid": "e442.0",
            "source_info": {
                "paper_title": "ReproducedPapers.org: Openly teaching and structuring machine learning reproducibility",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "hyperparams-aspect",
            "name_full": "Hyperparameter sensitivity checks (repository aspect)",
            "brief_description": "The repository explicitly supports a 'Hyperparams check' aspect for contributors to evaluate hyperparameter sensitivity and robustness, recognizing hyperparameter mismatches as a source of divergence between natural-language descriptions and code/experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ReproducedPapers.org aspect tagging and reproduction experiments",
            "system_description": "ReproducedPapers.org allows contributors to tag reproductions with aspects (e.g., Hyperparams check) and to submit work that inspects hyperparameter sensitivity or performs hyperparameter sweeps to investigate reproducibility.",
            "nl_description_type": "research paper methods section / reported experimental hyperparameters",
            "code_implementation_type": "experiment scripts and code evaluations submitted by repository contributors (student blogs, evaluated existing code, or re-implementations)",
            "gap_type": "hyperparameter mismatch / missing hyperparameter specification",
            "gap_description": "The repository includes a dedicated 'Hyperparams check' aspect to capture cases where reproductions investigate sensitivity to hyperparameters; this implies recognition that papers often omit or under-specify hyperparameter settings or that default implementation choices in code differ from those implied in text, potentially causing divergent results.",
            "gap_location": "hyperparameters and training configuration",
            "detection_method": "explicit hyperparameter sensitivity experiments and evaluations performed by contributors (tagged as Hyperparams check) and self-reported outcomes in blogs; usage counts in repository metadata identify such checks.",
            "measurement_method": "Usage statistics: 'Hyperparams check' was used 17 times (one of the top-3 most-used aspects). Outcomes of these checks are part of the self-reported success/failure metadata (Figure 2) but the paper does not report specific performance deltas from hyperparameter changes.",
            "impact_on_results": "Potentially large: hyperparameter mismatches are implied to be a contributor to failed or partial reproductions; the paper does not provide quantified effect sizes for hyperparameter-induced performance changes, only that hyperparameter checks are commonly used and that reproductions sometimes fail (~40% failure reported overall).",
            "frequency_or_prevalence": "'Hyperparams check' aspect used 17 times across the repository's contributions; among top-3 aspects (Replicated 32, Reproduced 29, Hyperparams check 17).",
            "root_cause": "Under-specification of hyperparameters in papers, implicit defaults in code, and lack of standardized reporting of hyperparameter search/details.",
            "mitigation_approach": "Encourage contributors to perform and report hyperparameter sensitivity analyses via the repository; allow light-weight contributions to document hyperparameter effects; label reproductions with the 'Hyperparams check' aspect to make such investigations discoverable.",
            "mitigation_effectiveness": "Not quantitatively evaluated in the paper; repository usage data shows contributors are performing such checks (17 uses), and survey results indicate participants value reproductions, but no measured reduction in gap incidence or performance differences reported.",
            "domain_or_field": "machine learning / deep learning experiments",
            "reproducibility_impact": true,
            "uuid": "e442.1",
            "source_info": {
                "paper_title": "ReproducedPapers.org: Openly teaching and structuring machine learning reproducibility",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO",
            "rating": 2,
            "sanitized_title": "implementation_matters_in_deep_policy_gradients_a_case_study_on_ppo_and_trpo"
        },
        {
            "paper_title": "A step toward quantifying independently reproducible machine learning research",
            "rating": 2,
            "sanitized_title": "a_step_toward_quantifying_independently_reproducible_machine_learning_research"
        },
        {
            "paper_title": "Deep reinforcement learning that matters",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_that_matters"
        },
        {
            "paper_title": "Do imagenet classifiers generalize to imagenet?",
            "rating": 2,
            "sanitized_title": "do_imagenet_classifiers_generalize_to_imagenet"
        },
        {
            "paper_title": "Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches",
            "rating": 2,
            "sanitized_title": "are_we_really_making_much_progress_a_worrying_analysis_of_recent_neural_recommendation_approaches"
        },
        {
            "paper_title": "DLPaper2Code: Autogeneration of code from deep learning research papers",
            "rating": 1,
            "sanitized_title": "dlpaper2code_autogeneration_of_code_from_deep_learning_research_papers"
        },
        {
            "paper_title": "Are gans created equal? a large-scale study",
            "rating": 1,
            "sanitized_title": "are_gans_created_equal_a_largescale_study"
        }
    ],
    "cost": 0.00832175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ReproducedPapers.org: Openly teaching and structuring machine learning reproducibility</p>
<p>Burak Yildiz 
Delft University of Technology
Postbus 52600 AADelftThe Netherlands</p>
<p>] 
Delft University of Technology
Postbus 52600 AADelftThe Netherlands</p>
<p>Hayley Hung 
Delft University of Technology
Postbus 52600 AADelftThe Netherlands</p>
<p>Jesse H 
Delft University of Technology
Postbus 52600 AADelftThe Netherlands</p>
<p>ReproducedPapers.org: Openly teaching and structuring machine learning reproducibility
Krijthe [0000−0003−3435−6358]Cynthia C S Liem [0000−0002−5385−7695]Marco Loog [0000−0002−1298−8461]Gosia MigutFrans OliehoekAnnibale Panichella [0000−0002−7395−3588]Przemys law Pawe lczak [0000−0002−1302−1148]Stjepan Picek [0000−0001−7509−4337]Mathijs de Weerdt [0000−0002−0470−6241]and Jan van Gemert [0000−0002−3913−2786] Keywords: Machine Learning · Reproducibility · Online Repository
We present ReproducedPapers.org: an open online repository for teaching and structuring machine learning reproducibility. We evaluate doing a reproduction project among students and the added value of an online reproduction repository among AI researchers. We use anonymous self-assessment surveys and obtained 144 responses. Results suggest that students who do a reproduction project place more value on scientific reproductions and become more critical thinkers. Students and AI researchers agree that our online reproduction repository is valuable.</p>
<p>Introduction</p>
<p>Reproducibility is a cornerstone of science: if an experiment is not reproducible, we should question its conclusions. Yet, machine learning papers are lacking reproductions [7,12]. Possible reasons may include a misaligned incentive between reproducing results and the short-term measures of career success associated with more 'wins' [26] and publishing 'novel' work [15]. Nevertheless, high-impact can be achieved, for instance, when a reproduction fails spectacularly, e.g. [6,8,10,11,14,16,18,19,24]. Yet, these take colossal amounts of manual effort [1,2,9,22] or massive resources [16,23]. There are venues for publishing reproductions [3,4,25], which are typically peer-reviewed and thus uphold various selection standards to guarantee quality. We argue that this emphasis on quality is a hurdle for sharing light-weight reproductions. Important and useful examples of light-weight reproductions include partial results, small variants on the algorithm, hyperparameter sweeps, etc. Low-barrier options are indeed available in workshop challenges [13,21] organized at conferences such as ICPR, NeurIPS, ICLR, or ICML. However, such avenues are hard to maintain on a long-term basis, as a workshop may or may not be organized. We argue that there is a need for a low-barrier and long-term venue for machine learning reproductions.</p>
<p>A complementary angle on low-barrier reproductions is to improve university student training. We should teach the next generation of machine learning practitioners the importance of the reproducibility of research work, as done in other computer science domains such as computer networking, where results reproduction is the means to learn new material [30]. Doing a reproduction project in a course aligns with several important learning objectives for machine learning students. Among others, students (1) should be able to read, critique, and explain a scientific paper; (2) implement a method; (3) run, evaluate, investigate, and extend existing research or code; and (4) write clearly and concisely about code and methods. A reproduction project also lets students experience differences between published results and an implementation, which stimulates a critical attitude and allows reflections on the scientific process. Fig. 1. A screenshot of ReproducedPapers.org. We allow multiple reproductions of the same original paper and investigations of several aspects, such as Reproduced, Replicated, Hyperparameter check, etc. Our online repository is user-centered: its sufficient if a user sees value in uploading some form of reproduction. Having such a repository is well-suited for students and adds structure to reproducibility in machine learning.</p>
<p>In this paper, we align the benefits of an online reproduction repository with those of teaching reproducibility. We introduce ReproducedPapers.org: an open, light-weight repository of reproduced papers which flexibly allows any sort of reproduction work, see Figure 1. This repository benefits the research community while at the same time being well-equipped at accepting contributions from students. Although the standard of student reproductions might be lower than those required for peer reviewed reproductions, they can still give valuable insights such as clarifying which parts are difficult to implement or identifying the reproducibility level of elements. Such online reproductions are a low-threshold portfolio-building opportunity, which in turn may prove a valuable incentive to start doing more reproductions, as well as an opportunity to facilitate sharing reproductions that otherwise would not have been shared.</p>
<p>Our online repository shares traits with other light-weight, bottom-up, grassroots community efforts such as ArXiv [5], Open Review [28], and Papers with Code [29]. Other efforts on facilitating reproducibility include software for reproducible and reusable experiments [20], open specification neural network diagrams [17], and a framework for automatic parsing of deep learning research paper to generate the implementation [27]. Similar to these approaches, in our work, we combine the traits from online repositories with those of tools facilitating reproducibility by providing an online repository that facilitates teaching as well as structuring reproducibility.</p>
<p>We make the following contributions. 1. We propose a new online reproduction repository; 2. We conduct a proof of concept with students from an MSc Deep Learning course to perform a reproduction project and populate the repository; 3. We evaluate the usefulness of the repository among AI researchers and the learning objectives among students by anonymous surveys.</p>
<p>The online reproduction repository</p>
<p>We performed a proof of concept experiment with a reproducibility project for students of the MSc Deep Learning course taught by this paper's last author at Delft University of Technology (TU Delft). We solicited relevant papers among university staff and ensured that (i) data is available, (ii) it is clear which table or figure to reproduce, and (iii) the computational demands are reasonable. Students were also allowed to themselves suggest a paper to reproduce. On their paper of choice, they worked in groups of 2 to 4, for 8 weeks, for approximately onethird of their studying time (i.e., about 13 hours a week). For grading, students submitted a blog in PDF and also the URL of an online version of their blog to ReproducedPapers.org to populate the repository. For students who do not wish to share a blog with the world, we offer a private option, which is only visible to course administrators. The option to publicly blog about reproducing machine learning provides an simple opportunity for students to build an online portfolio while simultaneously incentivizing making reproductions. Additional ablation studies. Table 1. Different aspects of reproduction which are highlighted as badges (see Figure 1).</p>
<p>We explicitly allow for light-weight reproduction efforts such as evaluating existing code, checking only certain parts of the paper, proposing minor variations, doing hyperparameter sweeps, etc. Our current options (aspects) are shown in Table 1, and we will add others as the need arises. Authors label their reproduction with the relevant aspects themselves.</p>
<p>We developed ReproducedPapers.org in-house as a simple web application. It is implemented by this paper's first author, and its source code is available on GitHub 1 . Registering is necessary only when adding reproductions. Currently, the repository has 90 registered users and hosts 24 unique papers and 57 paper reproductions. Most papers have multiple reproductions, and only five reproductions are marked as private. The top-3 most-used aspects are Replicated (32 times); Reproduced (29 times) and Hyperparams check (17 times). Figure 2 whose data is derived from self-reported blog posts by users shows both success and failure rates to be around 40%.  (e) I would like to do this again.</p>
<p>Strongly disagree Disagree Neutral Agree Strongly agree Fig. 3. Responses to survey questions from students who contributed to ReproducedPapers.org. Letting students themselves do a reproduction promotes a critical mindset (a and b), and teaches the value of scientific reproductions (c). In addition, the students considered it a positive experience (d,e). We conclude that these traits align with our learning objectives.</p>
<p>Survey analysis</p>
<p>We evaluate student learning objectives and how AI researchers perceive our online reproduction repository by analyzing the results of small anonymous surveys for two groups: (i) students who recently added their reproduction to our repository and (ii) anybody identifying her/himself working in AI. The second group was invited to the survey through social media and emails. Both groups share the same questions, where the students have five additional questions to evaluate education. The survey data is available at ReproducedPapers.org 2 We received a total of 144 responses: 43 from course students and 101 from third-party AI researchers all over the world. Of the latter, 87 identify themselves as a junior or senior researcher, and 14 as a student.</p>
<p>Evaluating student learning objectives</p>
<p>The survey questions and results can be found in Figure 3. We evaluate the following objectives.</p>
<p>Doing a reproduction project increases critical thinking. Results in Figure 3(a) show that doing a reproduction taught most students something new about the scientific process. Figure 3(b) suggests that students become more critical to published results.</p>
<p>Doing a reproduction project makes students value reproductions more. The results in Figure 3(c) indicate that after doing a reproduction, a great majority of students place more value on scientific reproductions.</p>
<p>Students find a reproduction project a positive experience. The results in Figure 3(d,e) demonstrates that students valued the work and prefer to do a reproduction more often. Results suggest that having a reproducibility project teaches skills considered important by both student and teacher. Figure 4 shows results for the third party AI researchers. We found the following.</p>
<p>Evaluating the AI researcher survey respondents</p>
<p>The AI researcher survey respondents find online reproductions valuable. Results in Figure 4(a,d,g) show that students and, especially, researchers find an online reproduction valuable and useful. According to Figure 4(i), there is no clear preference for doing a reproduction or writing a paper. Figure 4(e) suggests that the perceived value of reproduction by the community is smaller for researchers than for students.</p>
<p>The AI researcher survey respondents find an online reproduction repository valuable. Results in Figure 4(b,c) show that students and researchers appreciate an online reproduction repository. Figure 4(f) shows that researchers are less likely than students to help contribute by doing reproductions.</p>
<p>The AI researcher survey respondents see an educational role for courses where students do a reproduction project. Results in Figure 4(h) show that researchers and students agree that reproduction projects should be used more often in courses. Additionally, we make the following observations from Figure 4:</p>
<p>(i) When compared to students, the researchers think the community values reproductions less (e) and want their own team to work on reproductions less (f ). This may suggest an inverse relationship between perceived value and willingness to contribute. Yet, when comparing researchers against themselves, most think the community values reproductions, and most researchers would like to contribute.</p>
<p>(ii) More researchers want their work reproduced (g) than that they are willing to contribute (f ). Can we place our hope on the students as future researchers, as they are much more willing to contribute?</p>
<p>(iii) There is a clear consensus that reproductions are valuable (a, d, g, i) but some researchers feel that the community does not reward it enough (e). Therefore, an important question is how we can change the perception of a low reward for doing reproductions, beyond repositories as reported on here. (i) Writing a new paper is more valuable than doing a reproduction.</p>
<p>Strongly disagree Disagree Neutral Agree Strongly agree Fig. 4. Responses to survey questions by 57 students and 87 self-identified AI researchers. The survey question is in the sub-caption. Researchers and students agree that: Reproductions are valuable (a, d, g), that an online repository adds value (b, c), and that more courses should use a reproduction project (h). Researchers differ from students in that researchers more strongly find a reproduction valuable (a), and would consult online reproductions more (d). Researchers think a reproduction is valued less by the community (e) and are less likely to contribute with reproductions (f). Students and researchers both do not agree among themselves if a new paper is more valuable then a reproduction (i), suggesting that the answer is 'it depends'. We conclude that the respondents welcome an online repository for teaching and structuring reproducibility.</p>
<p>Discussion and conclusions</p>
<p>It should be clear that our results and corresponding analysis are rather preliminary. We are convinced, however, that they warrant low-barrier and long-term solutions accommodating research reproduction. Our ReproducedPapers.org pro-vides one such outlet. We hope that future analysis of the further accumulated survey data may sketch an even clearer picture. We hope others consider reproducing our effort.</p>
<p>The main conclusions that we draw at present are the following three. 1. Doing a reproduction course project aligns well with learning objectives, and students find it a positive experience. 2. A reproducibility project improves the perceived value of reproductions, and allowing students to blog online about their reproduction project offers an extra incentive to do a reproduction. 3. AI researcher survey respondents are positive about online reproductions and a reproduction repository.</p>
<p>We finally call on the community to add their reproductions to the website ReproducedPapers.org and deploy it in courses: may the next generation of machine learners be reproducers.</p>
<p>Fig. 2 .
2Current ReproducedPapers.org statistics. (a) Reproduction success rates; (b) Number of reproductions per paper ID. Count (a) Doing a reproduction changed how I view the scientific process. (b) Doing a reproduction made me more critical of results in scientific papers.(c) Doing a reproduction made me value reproductions more.(d) Doing a reproduction was a valuable experience for me.</p>
<p>Hyperparams check New evaluation of hyperparameter sensitivity. • New data Evaluating new datasets to obtain similar results. • New algorithm variant Evaluating a different variant. • New code variant Rewrote/ported existing code to be more efficient/readable. • Ablation studyAspect 
Description </p>
<p>• Replicated 
A full implementation from scratch without using any pre-
existing code. 
• Reproduced 
Existing code was evaluated. 
• 
https://reproducedpapers.org/survey-data.zip</p>
<p>Black magic in deep learning: How human skill impacts network training. K Anand, Z Wang, M Loog, J Van Gemert, British Machine Vision Conference (BMVC). Anand, K., Wang, Z., Loog, M., van Gemert, J.: Black magic in deep learning: How human skill impacts network training. In: British Machine Vision Conference (BMVC) (2020)</p>
<p>Code replicability in computer graphics. N Bonneel, D Coeurjolly, J Digne, N Mellado, ACM Transactions on Graphics. 394Bonneel, N., Coeurjolly, D., Digne, J., Mellado, N.: Code replicability in computer graphics. ACM Transactions on Graphics 39(4) (2020)</p>
<p>An overview of platforms for reproducible research and augmented publications. M Colom, B Kerautret, A Krähenbühl, International Workshop on Reproducible Research in Pattern Recognition. SpringerColom, M., Kerautret, B., Krähenbühl, A.: An overview of platforms for reproducible research and augmented publications. In: International Workshop on Reproducible Research in Pattern Recognition. pp. 25-39. Springer (2018)</p>
<p>Ipol: a new journal for fully reproducible research; analysis of four years development. M Colom, B Kerautret, N Limare, P Monasse, J M Morel, 7th International Conference on New Technologies, Mobility and Security (NTMS). IEEEColom, M., Kerautret, B., Limare, N., Monasse, P., Morel, J.M.: Ipol: a new journal for fully reproducible research; analysis of four years development. In: 2015 7th International Conference on New Technologies, Mobility and Security (NTMS). pp. 1-5. IEEE (2015)</p>
<p>Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches. M F Dacrema, P Cremonesi, D Jannach, Proceedings of the 13th ACM Conference on Recommender Systems. the 13th ACM Conference on Recommender SystemsDacrema, M.F., Cremonesi, P., Jannach, D.: Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches. In: Proceed- ings of the 13th ACM Conference on Recommender Systems (2019)</p>
<p>Replicability is not reproducibility: Nor is it good science. C Drummond, Evaluation Methods for Machine Learning Workshop at the 26th ICML. Drummond, C.: Replicability is not reproducibility: Nor is it good science. In: Evaluation Methods for Machine Learning Workshop at the 26th ICML (2009)</p>
<p>L Engstrom, A Ilyas, S Santurkar, D Tsipras, F Janoos, L Rudolph, A Madry, arXiv:2005.12729Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO. arXiv preprintEngstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L., Madry, A.: Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO. arXiv preprint arXiv:2005.12729 (2020)</p>
<p>Asplos 2020 artifact evaluation report. G Fursin, T Moreau, V Reddi, Proc. ASPLOS. pp. vi-vii. ASPLOS. pp. vi-viiACMFursin, G., Moreau, T., Reddi, V.: Asplos 2020 artifact evaluation report. In: Proc. ASPLOS. pp. vi-vii. ACM (2020)</p>
<p>We need to talk about standard splits. K Gorman, S Bedrick, Proceedings of the 57th annual meeting of the association for computational linguistics. the 57th annual meeting of the association for computational linguisticsGorman, K., Bedrick, S.: We need to talk about standard splits. In: Proceedings of the 57th annual meeting of the association for computational linguistics. pp. 2786-2791 (2019)</p>
<p>Deep reinforcement learning that matters. P Henderson, R Islam, P Bachman, J Pineau, D Precup, D Meger, Thirty-Second AAAI Conference on Artificial Intelligence. Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., Meger, D.: Deep re- inforcement learning that matters. In: Thirty-Second AAAI Conference on Artificial Intelligence (2018)</p>
<p>Artificial intelligence faces reproducibility crisis. M Hutson, Science. 3596377Hutson, M.: Artificial intelligence faces reproducibility crisis. Science 359(6377), 725-726 (2018)</p>
<p>B Kerautret, M Colom, D Lopresti, P Monasse, H Talbot, Reproducible Research in Pattern Recognition: Second International Workshop, RRPR 2018. Beijing, ChinaSpringer11455Kerautret, B., Colom, M., Lopresti, D., Monasse, P., Talbot, H.: Reproducible Research in Pattern Recognition: Second International Workshop, RRPR 2018, Beijing, China, August 20, 2018, Revised Selected Papers, vol. 11455. Springer (2019)</p>
<p>The neural hype and comparisons against weak baselines. J Lin, ACM SIGIR Forum. 522Lin, J.: The neural hype and comparisons against weak baselines. ACM SIGIR Forum 52(2), 40-51 (2019)</p>
<p>Research for practice: Troubling trends in machinelearning scholarship. Z C Lipton, J Steinhardt, Commun. ACM. 626Lipton, Z.C., Steinhardt, J.: Research for practice: Troubling trends in machine- learning scholarship. Commun. ACM 62(6), 45-53 (May 2019)</p>
<p>Are gans created equal? a large-scale study. M Lucic, K Kurach, M Michalski, S Gelly, O Bousquet, Advances in neural information processing systems. Lucic, M., Kurach, K., Michalski, M., Gelly, S., Bousquet, O.: Are gans created equal? a large-scale study. In: Advances in neural information processing systems. pp. 700-709 (2018)</p>
<p>G Marshall, A Freitas, arXiv:1812.11142The Diagrammatic AI Language (DIAL): Version 0.1. arXiv preprintMarshall, G., Freitas, A.: The Diagrammatic AI Language (DIAL): Version 0.1. arXiv preprint arXiv:1812.11142 (2018)</p>
<p>On the state of the art of evaluation in neural language models. G Melis, C Dyer, P Blunsom, International Conference on Learning Representations. Melis, G., Dyer, C., Blunsom, P.: On the state of the art of evaluation in neural language models. In: International Conference on Learning Representations (2018), https://openreview.net/forum?id=ByJHuTgA-</p>
<p>K Musgrave, S Belongie, S N Lim, arXiv:2003.08505A metric learning reality check. arXiv preprintMusgrave, K., Belongie, S., Lim, S.N.: A metric learning reality check. arXiv preprint arXiv:2003.08505 (2020)</p>
<p>dagger: A Python Framework for Reproducible Machine Learning Experiment Orchestration. M Paganini, J Z Forde, Paganini, M., Forde, J.Z.: dagger: A Python Framework for Reproducible Machine Learning Experiment Orchestration (2020)</p>
<p>J Pineau, K Sinha, G Fried, R N Ke, H Larochelle, ICLR Reproducibility Challenge. 5Pineau, J., Sinha, K., Fried, G., Ke, R.N., Larochelle, H.: ICLR Reproducibility Challenge 2019. ReScience C 5(2), 5 (may 2019)</p>
<p>A step toward quantifying independently reproducible machine learning research. E Raff, NeurIPS. pp. Raff, E.: A step toward quantifying independently reproducible machine learning research. In: NeurIPS. pp. 5486-5496 (2019)</p>
<p>B Recht, R Roelofs, L Schmidt, V Shankar, Do imagenet classifiers generalize to imagenet? In: ICML. Recht, B., Roelofs, R., Schmidt, L., Shankar, V.: Do imagenet classifiers generalize to imagenet? In: ICML. pp. 5389-5400 (2019)</p>
<p>Deep bayesian bandits showdown: An empirical comparison of bayesian deep networks for thompson sampling. C Riquelme, G Tucker, J Snoek, International Conference on Learning Representations. 6Riquelme, C., Tucker, G., Snoek, J.: Deep bayesian bandits showdown: An empirical comparison of bayesian deep networks for thompson sampling. In: International Conference on Learning Representations (2018), https://openreview.net/forum? id=SyYe6k-CW</p>
<p>Rescience c: a journal for reproducible replications in computational science. N P Rougier, K Hinsen, International Workshop on Reproducible Research in Pattern Recognition. SpringerRougier, N.P., Hinsen, K.: Rescience c: a journal for reproducible replications in computational science. In: International Workshop on Reproducible Research in Pattern Recognition. pp. 150-156. Springer (2018)</p>
<p>Winner's Curse? On Pace, Progress, and Empirical Rigor. D Sculley, J Snoek, A Wiltschko, A Rahimi, ICLR workshop. Sculley, D., Snoek, J., Wiltschko, A., Rahimi, A.: Winner's Curse? On Pace, Progress, and Empirical Rigor. In: ICLR workshop (2018), https://openreview.net/forum? id=rJWF0Fywf</p>
<p>DLPaper2Code: Autogeneration of code from deep learning research papers. A Sethi, A Sankaran, N Panwar, S Khare, S Mani, Thirty-Second AAAI Conference on Artificial Intelligence. Sethi, A., Sankaran, A., Panwar, N., Khare, S., Mani, S.: DLPaper2Code: Auto- generation of code from deep learning research papers. In: Thirty-Second AAAI Conference on Artificial Intelligence (2018)</p>
<p>Open scholarship and peer review: a time for experimentation. D Soergel, A Saunders, A Mccallum, Proc. ICML. ICMLSoergel, D., Saunders, A., McCallum, A.: Open scholarship and peer review: a time for experimentation. In: Proc. ICML (2013)</p>
<p>Papers with code-a facebook AI project. R Stojnic, R Taylor, Stojnic, R., Taylor, R.: Papers with code-a facebook AI project. https:// paperswithcode.com (Jul 2018), last accessed: Jun. 20, 2020</p>
<p>Learning networking by reproducing research results. SIG-COMM Comput. L Yan, N Mckeown, Commun. Rev. 472Yan, L., McKeown, N.: Learning networking by reproducing research results. SIG- COMM Comput. Commun. Rev. 47(2), 19-26 (Apr 2017)</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3066 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3066</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3066</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-270702477</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.16078v2.pdf" target="_blank">First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Explicit multi-step reasoning, such as chain-of-thought, is widely adopted in the community to explore the better performance of language models (LMs). We report on the systematic strategy that LMs use in this process.Our controlled experiments reveal that LMs rely more heavily on heuristics, such as lexical overlap, in the earlier stages of reasoning when more steps are required to reach an answer. Conversely, their reliance on heuristics decreases as LMs progress closer to the final answer. This suggests that LMs track only a limited number of future steps and dynamically combine heuristic strategies with rational ones in solving tasks involving multi-step reasoning.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3066.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3066.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (snapshot gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performing OpenAI large language model used in this paper to evaluate step-by-step arithmetic reasoning; shows the strongest accuracy and a clear shift from heuristic to goal-oriented reasoning as steps approach the answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 snapshot (gpt-4-0613) ‚Äî a large pre-trained transformer-based language model used via API; prompted with few-shot step-by-step instructions to produce minimal solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['step-by-step (chain-of-thought style, few-shot)', 'heuristic-driven selection (OVERLAP, POSITION, NEGATIVE avoidance)', 'goal-oriented minimal-solution search (shortest-path style planning)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Models were prompted with 4-shot examples and instructed to produce a minimal step-by-step solution. At each step the model selects a premise and paraphrases it into a derived fact. The paper distinguishes two internal styles: (1) heuristic selection (e.g., choose premises with lexical/person-name overlap or particular positions) and (2) rational/goal-oriented selection that reduces the remaining distance to the answer (minimal-solution path). The NEGATIVE heuristic was analyzed as avoidance of premises containing negation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar styles within a single prompting paradigm: the model exhibits two distinct internal strategies (heuristic vs rational) during the same step-by-step method rather than being switched between fundamentally different algorithmic methods; the diversity is behavioral (dynamic switching) not architectural.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K (modified) and artificial multi-step arithmetic reasoning datasets</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step arithmetic reasoning problems composed of premises describing numeric relations among named entities and a question asking the final numeric value; experiments used natural GSM8K subsets (with inserted distractors) and artificially controlled 5-step problems to probe step-wise selection strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>GSM8K accuracies (Table 6): Base 85.5%, Overlap 84.2%, Position 82.9%, Negative 92.1%. Artificial dataset accuracies (Table 7): Base 98.7%, Overlap 94.0%, Position 98.0%, Negative 99.7%. Behavioral measure (distractor-selection ratio r, Figure 3): GPT-4 shows a negative slope of r vs remaining distance d (higher r when d large; r decreases as d‚Üí0), indicating more heuristic selections early and more rational selections later. Exact r values not tabulated numerically in the text but reported qualitatively and plotted (typically above chance at larger d).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>The paper compares heuristic-driven selection vs minimal-solution (goal-oriented) selection by (a) controlled artificial distractor experiments across reasoning steps (teacher forcing partial histories and testing next-step choice) and (b) dataset variants that add distractors to induce specific heuristics. GPT-4 exhibits a clear dynamic transition: when the remaining distance to the goal is large, it more frequently chooses heuristic distractors; as the goal nears, it prefers goal-reducing actions. GPT-4's behavior contrasts with weaker models which show different bias patterns (see other entries). Few-shot example manipulations had little effect on GPT-4's heuristic frequency (no large mimicry of few-shot heuristics).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 both uses heuristics (lexical overlap/position biases) and can perform rational minimal-path reasoning; it systematically shifts from heuristic to rational behavior as the remaining number of steps to the answer decreases, suggesting limited lookahead/planning capacity but a dynamic search strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No demonstrated cases where diverse external reasoning methods (e.g., retrieval vs chain-of-thought) were compared for GPT-4 within this paper. The NEGATIVE heuristic was not probed in the stepwise distance experiment (¬ß4.2) because its avoidance nature complicates the design. While GPT-4 shows robust behavior, occasional selection of heuristic distractors at large d remains (i.e., heuristics still mislead early steps).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3066.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3066.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (snapshot gpt-3.5-turbo-0125)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI LLM evaluated in the paper; performs well on many tasks but shows different heuristic tendencies compared to GPT-4 and PaLM2, and its distractor-selection patterns vary by dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0125</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5-turbo snapshot used with temperature=0.0 and few-shot step-by-step prompting to produce minimal solutions; architecture: transformer-based pre-trained LLM (precise parameter count not specified in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['step-by-step (few-shot chain-of-thought style)', 'heuristic-driven selection (OVERLAP, POSITION, NEGATIVE avoidance)', 'goal-oriented minimal-solution selection']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same experimental prompting as other models: few-shot examples + instruction to produce minimal stepwise derivations. The paper measures whether the model picks heuristic distractors (person-name overlap or positional) versus the correct next step at each reasoning step.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>behaviorally similar to other models: uses a mix of heuristic and goal-oriented selection within the same step-by-step prompting; not shown to employ diverse external reasoning algorithms in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K (modified) and artificial multi-step arithmetic reasoning datasets</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above; tasks require selecting relevant premises, resolving references, and performing arithmetic; distractors were inserted to probe heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>GSM8K accuracies (Table 6): Base 81.6%, Overlap 64.5%, Position 81.6%, Negative 82.9%. Artificial dataset accuracies (Table 7): Base 84.6%, Overlap 87.3%, Position 87.6%, Negative 82.9%. Behavioral results: GPT-3.5 shows heuristic selection but patterns differ from GPT-4/PaLM2; in some experiments (e.g., overlap) GPT-3.5's selection rates varied and a few anomalies were noted (e.g., overlap results not following the same trend for the high-distance steps in one ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to GPT-4 and PaLM2, GPT-3.5 sometimes shows less consistent reduction in heuristic use as distance to goal decreases. Few-shot manipulations did not substantially change GPT-3.5's heuristic frequency in most settings (unlike Llama2 which was more sensitive). In the 'many distractors' ablation (1:8 heuristic:non-heuristic distractor ratio), GPT-3.5 and GPT-4 were tested: heuristic distractors were selected more often than expected by chance, indicating reliance on heuristics even with many non-heuristic distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5 exhibits both heuristic and rational behaviors but is less consistent than GPT-4 in transitioning from heuristic to goal-oriented choices across steps; it is still capable of solving many problems but is more vulnerable to certain heuristic distractors (e.g., overlap in GSM8K variants).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Some anomalous results were observed (e.g., overlap ablation in the many-distractors experiment) where GPT-3.5 did not follow the same trends as other models. GPT-3.5 accuracy drops substantially on GSM8K when Overlap distractors are introduced (64.5% vs base 81.6%), indicating that heuristic distractors can materially harm performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3066.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3066.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM2 (textbison-001)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's PaLM2 (textbison-001) variant evaluated in this paper; shows the dynamic heuristic-to-rational transition similar to GPT-4 but with different absolute accuracies on the tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>textbison-001 (PaLM2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google PaLM2 textbison-001 variant (Anil et al., 2023 cited in paper) used with zero sampling (temperature=0) and few-shot/step-by-step prompting to produce minimal solutions on arithmetic reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['step-by-step minimal-solution reasoning (few-shot)', 'heuristic selection (OVERLAP, POSITION, NEGATIVE avoidance)', 'goal-oriented minimal-path selection']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompted to generate minimal reasoning sequences; the paper probes whether PaLM2 picks heuristic distractors (lexical/person overlap or positional biases) early in reasoning and whether it shifts to rational choices as the remaining distance decreases.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>PaLM2 similarly shows two behavioral styles within the step-by-step method: initial reliance on heuristics and later rational planning; diversity is dynamic behavior rather than separate reasoning algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K (modified) and artificial multi-step arithmetic reasoning datasets</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same multi-step arithmetic premise-resolution tasks with inserted distractors for targeted heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>GSM8K accuracies (Table 6): Base 64.5%, Overlap 59.2%, Position 60.5%, Negative 71.1%. Artificial dataset accuracies (Table 7): Base 60.0%, Overlap 58.7%, Position 77.0%, Negative 80.7%. Behavioral: PaLM2 shows the negative slope in distractor-selection ratio r vs remaining distance d (similar to GPT-4), indicating greater heuristic selection when d is larger and more rational selection when d is smaller.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>PaLM2 and GPT-4 both show clear decreases in heuristic selection as the goal approaches (negative slope of r vs d). Compared with Llama2 and GPT-3.5, PaLM2 tends to show less positional bias than Llama2 and less susceptibility to some distractors than GPT-3.5 in certain datasets. Few-shot manipulations did not substantially alter PaLM2's heuristic frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PaLM2 demonstrates the dynamic heuristic-to-rational transition: heuristics more likely in early steps (farther from goal) and rational goal-oriented steps more likely when close to the answer; absolute accuracy is lower than GPT-4 on the tested datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>PaLM2 accuracy is substantially lower than GPT-4 on both GSM8K and artificial tasks; for some dataset variants (e.g., Position) PaLM2's artificial accuracy was higher (77.0%) than its GSM8K base, indicating dataset-specific sensitivities. The study does not demonstrate external mechanisms (e.g., retrieval or explicit planner modules) to remove these heuristic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3066.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3066.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama2-13B (Touvron et al., 2023) evaluated in the paper; lower overall accuracy and a stronger susceptibility to positional and few-shot induced heuristics compared to larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama 2 13B-parameter variant (open-weight transformer LLM) used with temperature=0 and few-shot step-by-step prompting; evaluated for premise-selection behavior in multi-step arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['step-by-step reasoning (few-shot)', 'heuristic selection (notably POSITION and OVERLAP biases)', 'attempted goal-oriented selection (less consistent)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same prompting and experimental procedure as other models. The paper measures which premises the model picks at each step and quantifies bias towards heuristics (positional, lexical overlap, negation avoidance). Llama2-13B often selects positional distractors and is more influenced by few-shot example manipulations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Behaviorally similar (heuristic vs rational) but shows weaker goal-oriented behavior and stronger heuristic biases; not shown to apply diverse external reasoning methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K (modified) and artificial multi-step arithmetic reasoning datasets</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above; tasks require chaining premises to compute final numeric answers, with controlled distractors added to probe heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>GSM8K accuracies (Table 6): Base 30.3%, Overlap 34.2%, Position 30.3%, Negative 27.6%. Artificial dataset accuracies (Table 7): Base 21.3%, Overlap 14.3%, Position 22.3%, Negative 20.0%. Behavioral: Llama2 shows larger positional bias and greater sensitivity to few-shot-example changes (Table 8 indicates Llama2's heuristic frequency changes more with few-shot modifications).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Llama2 contrasts with GPT-4/PaLM2 by exhibiting weaker ability to transition from heuristic to rational choices; it is more strongly influenced by positional distractors and by manipulations to few-shot examples, indicating a higher tendency to mimic input heuristics rather than perform robust goal-directed planning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Llama2-13B is more prone to heuristic biases (especially POSITION) and less capable of consistent minimal-solution planning; few-shot example modifications more strongly affect its heuristic usage than they do for larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Llama2 sometimes showed increased heuristic selection when few-shot examples were changed to induce particular heuristics (unlike GPT-4/PaLM2 which were stable), demonstrating that small/medium models may more directly mimic input examples and are less robust to distractors. Overall accuracy is much lower than other evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PaLM 2 Technical Report <em>(Rating: 2)</em></li>
                <li>Llama 2: Open Foundation and Fine-Tuned Chat Models <em>(Rating: 2)</em></li>
                <li>GPT-4 Technical Report <em>(Rating: 2)</em></li>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>When Do Language Models Use Heuristics? (Du et al., 2022) <em>(Rating: 1)</em></li>
                <li>How Language Models Use Long Contexts (Transactions of the ACL) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3066",
    "paper_id": "paper-270702477",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (snapshot gpt-4-0613)",
            "brief_description": "A high-performing OpenAI large language model used in this paper to evaluate step-by-step arithmetic reasoning; shows the strongest accuracy and a clear shift from heuristic to goal-oriented reasoning as steps approach the answer.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4-0613",
            "model_description": "OpenAI GPT-4 snapshot (gpt-4-0613) ‚Äî a large pre-trained transformer-based language model used via API; prompted with few-shot step-by-step instructions to produce minimal solutions.",
            "model_size": null,
            "reasoning_methods": [
                "step-by-step (chain-of-thought style, few-shot)",
                "heuristic-driven selection (OVERLAP, POSITION, NEGATIVE avoidance)",
                "goal-oriented minimal-solution search (shortest-path style planning)"
            ],
            "reasoning_methods_description": "Models were prompted with 4-shot examples and instructed to produce a minimal step-by-step solution. At each step the model selects a premise and paraphrases it into a derived fact. The paper distinguishes two internal styles: (1) heuristic selection (e.g., choose premises with lexical/person-name overlap or particular positions) and (2) rational/goal-oriented selection that reduces the remaining distance to the answer (minimal-solution path). The NEGATIVE heuristic was analyzed as avoidance of premises containing negation.",
            "diversity_of_methods": "similar styles within a single prompting paradigm: the model exhibits two distinct internal strategies (heuristic vs rational) during the same step-by-step method rather than being switched between fundamentally different algorithmic methods; the diversity is behavioral (dynamic switching) not architectural.",
            "reasoning_task_name": "GSM8K (modified) and artificial multi-step arithmetic reasoning datasets",
            "reasoning_task_description": "Multi-step arithmetic reasoning problems composed of premises describing numeric relations among named entities and a question asking the final numeric value; experiments used natural GSM8K subsets (with inserted distractors) and artificially controlled 5-step problems to probe step-wise selection strategies.",
            "performance_by_method": "GSM8K accuracies (Table 6): Base 85.5%, Overlap 84.2%, Position 82.9%, Negative 92.1%. Artificial dataset accuracies (Table 7): Base 98.7%, Overlap 94.0%, Position 98.0%, Negative 99.7%. Behavioral measure (distractor-selection ratio r, Figure 3): GPT-4 shows a negative slope of r vs remaining distance d (higher r when d large; r decreases as d‚Üí0), indicating more heuristic selections early and more rational selections later. Exact r values not tabulated numerically in the text but reported qualitatively and plotted (typically above chance at larger d).",
            "comparison_of_methods": "The paper compares heuristic-driven selection vs minimal-solution (goal-oriented) selection by (a) controlled artificial distractor experiments across reasoning steps (teacher forcing partial histories and testing next-step choice) and (b) dataset variants that add distractors to induce specific heuristics. GPT-4 exhibits a clear dynamic transition: when the remaining distance to the goal is large, it more frequently chooses heuristic distractors; as the goal nears, it prefers goal-reducing actions. GPT-4's behavior contrasts with weaker models which show different bias patterns (see other entries). Few-shot example manipulations had little effect on GPT-4's heuristic frequency (no large mimicry of few-shot heuristics).",
            "key_findings": "GPT-4 both uses heuristics (lexical overlap/position biases) and can perform rational minimal-path reasoning; it systematically shifts from heuristic to rational behavior as the remaining number of steps to the answer decreases, suggesting limited lookahead/planning capacity but a dynamic search strategy.",
            "counter_examples_or_negative_results": "No demonstrated cases where diverse external reasoning methods (e.g., retrieval vs chain-of-thought) were compared for GPT-4 within this paper. The NEGATIVE heuristic was not probed in the stepwise distance experiment (¬ß4.2) because its avoidance nature complicates the design. While GPT-4 shows robust behavior, occasional selection of heuristic distractors at large d remains (i.e., heuristics still mislead early steps).",
            "uuid": "e3066.0",
            "source_info": {
                "paper_title": "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5-turbo (snapshot gpt-3.5-turbo-0125)",
            "brief_description": "An OpenAI LLM evaluated in the paper; performs well on many tasks but shows different heuristic tendencies compared to GPT-4 and PaLM2, and its distractor-selection patterns vary by dataset.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0125",
            "model_description": "OpenAI GPT-3.5-turbo snapshot used with temperature=0.0 and few-shot step-by-step prompting to produce minimal solutions; architecture: transformer-based pre-trained LLM (precise parameter count not specified in the paper).",
            "model_size": null,
            "reasoning_methods": [
                "step-by-step (few-shot chain-of-thought style)",
                "heuristic-driven selection (OVERLAP, POSITION, NEGATIVE avoidance)",
                "goal-oriented minimal-solution selection"
            ],
            "reasoning_methods_description": "Same experimental prompting as other models: few-shot examples + instruction to produce minimal stepwise derivations. The paper measures whether the model picks heuristic distractors (person-name overlap or positional) versus the correct next step at each reasoning step.",
            "diversity_of_methods": "behaviorally similar to other models: uses a mix of heuristic and goal-oriented selection within the same step-by-step prompting; not shown to employ diverse external reasoning algorithms in these experiments.",
            "reasoning_task_name": "GSM8K (modified) and artificial multi-step arithmetic reasoning datasets",
            "reasoning_task_description": "As above; tasks require selecting relevant premises, resolving references, and performing arithmetic; distractors were inserted to probe heuristics.",
            "performance_by_method": "GSM8K accuracies (Table 6): Base 81.6%, Overlap 64.5%, Position 81.6%, Negative 82.9%. Artificial dataset accuracies (Table 7): Base 84.6%, Overlap 87.3%, Position 87.6%, Negative 82.9%. Behavioral results: GPT-3.5 shows heuristic selection but patterns differ from GPT-4/PaLM2; in some experiments (e.g., overlap) GPT-3.5's selection rates varied and a few anomalies were noted (e.g., overlap results not following the same trend for the high-distance steps in one ablation).",
            "comparison_of_methods": "Compared to GPT-4 and PaLM2, GPT-3.5 sometimes shows less consistent reduction in heuristic use as distance to goal decreases. Few-shot manipulations did not substantially change GPT-3.5's heuristic frequency in most settings (unlike Llama2 which was more sensitive). In the 'many distractors' ablation (1:8 heuristic:non-heuristic distractor ratio), GPT-3.5 and GPT-4 were tested: heuristic distractors were selected more often than expected by chance, indicating reliance on heuristics even with many non-heuristic distractors.",
            "key_findings": "GPT-3.5 exhibits both heuristic and rational behaviors but is less consistent than GPT-4 in transitioning from heuristic to goal-oriented choices across steps; it is still capable of solving many problems but is more vulnerable to certain heuristic distractors (e.g., overlap in GSM8K variants).",
            "counter_examples_or_negative_results": "Some anomalous results were observed (e.g., overlap ablation in the many-distractors experiment) where GPT-3.5 did not follow the same trends as other models. GPT-3.5 accuracy drops substantially on GSM8K when Overlap distractors are introduced (64.5% vs base 81.6%), indicating that heuristic distractors can materially harm performance.",
            "uuid": "e3066.1",
            "source_info": {
                "paper_title": "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "PaLM2",
            "name_full": "PaLM2 (textbison-001)",
            "brief_description": "Google's PaLM2 (textbison-001) variant evaluated in this paper; shows the dynamic heuristic-to-rational transition similar to GPT-4 but with different absolute accuracies on the tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "textbison-001 (PaLM2)",
            "model_description": "Google PaLM2 textbison-001 variant (Anil et al., 2023 cited in paper) used with zero sampling (temperature=0) and few-shot/step-by-step prompting to produce minimal solutions on arithmetic reasoning tasks.",
            "model_size": null,
            "reasoning_methods": [
                "step-by-step minimal-solution reasoning (few-shot)",
                "heuristic selection (OVERLAP, POSITION, NEGATIVE avoidance)",
                "goal-oriented minimal-path selection"
            ],
            "reasoning_methods_description": "Prompted to generate minimal reasoning sequences; the paper probes whether PaLM2 picks heuristic distractors (lexical/person overlap or positional biases) early in reasoning and whether it shifts to rational choices as the remaining distance decreases.",
            "diversity_of_methods": "PaLM2 similarly shows two behavioral styles within the step-by-step method: initial reliance on heuristics and later rational planning; diversity is dynamic behavior rather than separate reasoning algorithms.",
            "reasoning_task_name": "GSM8K (modified) and artificial multi-step arithmetic reasoning datasets",
            "reasoning_task_description": "Same multi-step arithmetic premise-resolution tasks with inserted distractors for targeted heuristics.",
            "performance_by_method": "GSM8K accuracies (Table 6): Base 64.5%, Overlap 59.2%, Position 60.5%, Negative 71.1%. Artificial dataset accuracies (Table 7): Base 60.0%, Overlap 58.7%, Position 77.0%, Negative 80.7%. Behavioral: PaLM2 shows the negative slope in distractor-selection ratio r vs remaining distance d (similar to GPT-4), indicating greater heuristic selection when d is larger and more rational selection when d is smaller.",
            "comparison_of_methods": "PaLM2 and GPT-4 both show clear decreases in heuristic selection as the goal approaches (negative slope of r vs d). Compared with Llama2 and GPT-3.5, PaLM2 tends to show less positional bias than Llama2 and less susceptibility to some distractors than GPT-3.5 in certain datasets. Few-shot manipulations did not substantially alter PaLM2's heuristic frequency.",
            "key_findings": "PaLM2 demonstrates the dynamic heuristic-to-rational transition: heuristics more likely in early steps (farther from goal) and rational goal-oriented steps more likely when close to the answer; absolute accuracy is lower than GPT-4 on the tested datasets.",
            "counter_examples_or_negative_results": "PaLM2 accuracy is substantially lower than GPT-4 on both GSM8K and artificial tasks; for some dataset variants (e.g., Position) PaLM2's artificial accuracy was higher (77.0%) than its GSM8K base, indicating dataset-specific sensitivities. The study does not demonstrate external mechanisms (e.g., retrieval or explicit planner modules) to remove these heuristic errors.",
            "uuid": "e3066.2",
            "source_info": {
                "paper_title": "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama2-13B",
            "name_full": "Llama 2 (13B)",
            "brief_description": "Llama2-13B (Touvron et al., 2023) evaluated in the paper; lower overall accuracy and a stronger susceptibility to positional and few-shot induced heuristics compared to larger models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama2-13B",
            "model_description": "Llama 2 13B-parameter variant (open-weight transformer LLM) used with temperature=0 and few-shot step-by-step prompting; evaluated for premise-selection behavior in multi-step arithmetic reasoning.",
            "model_size": "13B",
            "reasoning_methods": [
                "step-by-step reasoning (few-shot)",
                "heuristic selection (notably POSITION and OVERLAP biases)",
                "attempted goal-oriented selection (less consistent)"
            ],
            "reasoning_methods_description": "Same prompting and experimental procedure as other models. The paper measures which premises the model picks at each step and quantifies bias towards heuristics (positional, lexical overlap, negation avoidance). Llama2-13B often selects positional distractors and is more influenced by few-shot example manipulations.",
            "diversity_of_methods": "Behaviorally similar (heuristic vs rational) but shows weaker goal-oriented behavior and stronger heuristic biases; not shown to apply diverse external reasoning methods.",
            "reasoning_task_name": "GSM8K (modified) and artificial multi-step arithmetic reasoning datasets",
            "reasoning_task_description": "As above; tasks require chaining premises to compute final numeric answers, with controlled distractors added to probe heuristics.",
            "performance_by_method": "GSM8K accuracies (Table 6): Base 30.3%, Overlap 34.2%, Position 30.3%, Negative 27.6%. Artificial dataset accuracies (Table 7): Base 21.3%, Overlap 14.3%, Position 22.3%, Negative 20.0%. Behavioral: Llama2 shows larger positional bias and greater sensitivity to few-shot-example changes (Table 8 indicates Llama2's heuristic frequency changes more with few-shot modifications).",
            "comparison_of_methods": "Llama2 contrasts with GPT-4/PaLM2 by exhibiting weaker ability to transition from heuristic to rational choices; it is more strongly influenced by positional distractors and by manipulations to few-shot examples, indicating a higher tendency to mimic input heuristics rather than perform robust goal-directed planning.",
            "key_findings": "Llama2-13B is more prone to heuristic biases (especially POSITION) and less capable of consistent minimal-solution planning; few-shot example modifications more strongly affect its heuristic usage than they do for larger models.",
            "counter_examples_or_negative_results": "Llama2 sometimes showed increased heuristic selection when few-shot examples were changed to induce particular heuristics (unlike GPT-4/PaLM2 which were stable), demonstrating that small/medium models may more directly mimic input examples and are less robust to distractors. Overall accuracy is much lower than other evaluated models.",
            "uuid": "e3066.3",
            "source_info": {
                "paper_title": "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PaLM 2 Technical Report",
            "rating": 2,
            "sanitized_title": "palm_2_technical_report"
        },
        {
            "paper_title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "rating": 2,
            "sanitized_title": "llama_2_open_foundation_and_finetuned_chat_models"
        },
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "When Do Language Models Use Heuristics? (Du et al., 2022)",
            "rating": 1,
            "sanitized_title": "when_do_language_models_use_heuristics_du_et_al_2022"
        },
        {
            "paper_title": "How Language Models Use Long Contexts (Transactions of the ACL)",
            "rating": 1,
            "sanitized_title": "how_language_models_use_long_contexts_transactions_of_the_acl"
        }
    ],
    "cost": 0.013786499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning
7 Oct 2024</p>
<p>Yoichi Aoki 
Tohoku University</p>
<p>Keito Kudo 
Tohoku University</p>
<p>DubrovnikCroatia</p>
<p>Tatsuki Kuribayashi tatsuki.kuribayashi@mbzuai.ac.ae 
Shusaku Sone 
Tohoku University</p>
<p>Masaya Taniguchi masaya.taniguchi@riken.jp 
DubrovnikCroatia</p>
<p>Keisuke Sakaguchi keisuke.sakaguchi@tohoku.ac.jp 
Tohoku University</p>
<p>DubrovnikCroatia</p>
<p>Kentaro Inui kentaro.inui@mbzuai.ac.ae 
Tohoku University</p>
<p>DubrovnikCroatia</p>
<p>ùë° = 
Tohoku University</p>
<p>ùëù " , ùëù $ ] ‚àà ùëÉ * ùíõ = [ùëß ! , ùëß " , ùëß # ]ùíâ * = [ùëù ! 
Rohan Anil 
Andrew M Dai 
Orhan Firat 
Melvin Johnson 
Dmitry Lepikhin 
Alexandre Passos 
Sia- Mak Shakeri 
Emanuel Taropa 
Paige Bailey 
Zhifeng Chen 
Eric Chu 
Jonathan H Clark 
Lau- Rent El 
Yanping Huang 
Kathy Meier- Hellstern 
DubrovnikCroatia</p>
<p>Gaurav Mishra 
Erica Moreira 
Mark Omernick 
Kevin Robinson 
Sebastian Ruder 
Yi Tay 
Kefan Xiao 
Yuanzhong Xu 
Yujing Zhang 
Gustavo Hern√°ndez √Åbrego 
Junwhan Ahn 
Jacob Austin 
Paul Barham 
Jan A Botha 
James Brad- Bury 
Siddhartha Brahma 
Kevin Brooks 
Michele Catasta 
Yong Cheng 
Colin Cherry 
Christopher A Choquette-Choo 
Aakanksha Chowdhery 
Cl√©ment Crepy 
Shachi Dave 
Mostafa Dehghani 
Sunipa Dev 
Jacob Devlin 
Mark D√≠az 
Nan Du 
Ethan Dyer 
Vladimir Feinberg 
Fangxiaoyu Feng 
Vlad Fienber 
Markus Freitag 
Xavier Garcia 
Sebastian Gehrmann 
Lucas Gonzalez 
Xinyun Chen 
Ryan A Chi 
Xuezhi Wang 
Denny 2024 Zhou 
Peter Clark 
Oyvind Tafjord 
Kyle Richardson 
Karl Cobbe 
Vineet Kosaraju 
Mohammad Bavarian 
Mark Chen 
Heewoo Jun 
Lukasz Kaiser 
Matthias Plappert 
Jerry Tworek 
Jacob Hilton 
Reiichiro Nakano 
Christopher Hesse 
Thomas D Erickson 
Mark E 1981 Mattson 
Kyle Hamilton 
Aparna Nayak 
Bojan Bozic 
Luca 2022 Longo 
Nelson F Liu 
Kevin Lin 
John Hewitt 
Ashwin Paran- Jape 
Michele Bevilacqua 
Fabio Petroni 
Percy 2024 Liang 
Lost 
Hugo Touvron 
Louis Martin 
Kevin Stone 
Peter Al- Bert 
Amjad Almahairi 
Yasmine Babaei 
Niko- Lay Bashlykov 
Soumya Batra 
Prajjwal Bhargava 
Shruti Bhosale 
Dan Bikel 
Lukas Blecher 
Cris- Tian Canton-Ferrer 
Moya Chen 
Guillem Cucurull 
David Esiobu 
Jude Fernandes 
Jeremy Fu 
Wenyin Fu 
Brian Fuller 
Cynthia Gao 
Vedanuj Goswami 
Naman Goyal 
Anthony Hartshorn 
Saghar Hos- Seini 
Rui Hou 
Hakan Inan 
Marcin Kardas 
Viktor Kerkez 
Madian Khabsa 
Isabel Kloumann 
PunitArtem Korenev 
Singh Koura 
Marie-Anne Lachaux 
Thibaut Lavril 
Jenya Lee 
Diana Liskovich 
Yinghai Lu 
Yuning Mao 
Xavier Martinet 
Todor Mihaylov 
Pushkar Mishra 
Igor Molybog 
Yixin Nie 
Andrew Poulton 
Jeremy Reizenstein 
Rashi Rungta 
Kalyan Saladi 
Alan Schelten 
Ruan Silva 
Eric Michael Smith 
XiaoqingRanjan Subramanian 
Ellen Tan 
Binh Tang 
Ross Taylor 
Adina Williams 
Jian Xiang Kuan 
Puxin Xu 
Zheng Yan 
Iliyan Zarov 
Yuchen Zhang 
Angela Fan 
Melanie Kambadur 
Sharan Narang 
Aur√©lien Rodriguez 
SergeyRobert Stojnic 
Shunyu Yao 
Dian Yu 
Jeffrey Zhao 
Izhak Shafran 
Thomas L Griffiths 
Yuan Cao 
First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning
7 Oct 20248E3E9AB08A6ACE3419C43A901F035ECBarXiv:2406.16078v2[cs.CL]How many apples does Judy have? Let's think step-by-step How Language Models Use Long Contexts. Transactions of the Association for Computational Linguistics, 12:157-173
RIKEN, 3 MBZUAI  {youichi.aoki.p2, keito.kudo.q4, sone.shusaku.r8}@dc.tohoku.ac.jp,</p>
<p>Introduction</p>
<p>When facing complex tasks, humans tend to seek shallow, heuristic solutions first (Erickson and Mattson, 1981;Frederick, 2005).Once these attempts are revealed to fail or elicit another reasonable solution, they switch to being more rational (Stanovich and West, 2000).Such systematic behavior helps us to predict how humans will tackle new problems.Given such a view, when it comes to predicting the behavior of language models (LMs) (Madaan and Yazdanbakhsh 2022; Zhang et al. 2024; inter alia), the following question naturally arises-Do LMs also use a similar systematic strategy to solve complex tasks, or is their strategy totally different from humans, or do they have no such strategies?This study explores an answer to this question.Such analyses will shed light on the cognitive plausibility of LMs in problem solving (Opedal et al., 2024;Eisape et al., 2024;Aher et al., 2023) as well as address general concerns of current neural models relying on superficial, heuristic cues overly and ending up with Figure 1: Illustration of the systematic reasoning strategy we discovered in language models .When the goal is distant from the current reasoning step, they tend to rely on heuristics to take the next reasoning step, such as lexical overlap with a question, leading to the wrong direction (red path).In contrast, when the goal is within a limited distance, they are more likely to take rational actions (green path) to reach the goal.</p>
<p>irrational conclusions (Du et al., 2022;Lai et al., 2021;Jia and Liang, 2017;Ye et al., 2023;Chen et al., 2024).</p>
<p>In this paper, we demonstrate that LMs rely on shallow heuristics more frequently in the earlier phase of multi-step reasoning, and then gradually switch their reasoning strategy to be more rational and goal-oriented to make the right choice to reach the goal.From an engineering perspective, this highlights a limitation of modern LMs, including GPT-4 (OpenAI, 2023), in searching for a solution at the initial stage of step-by-step reasoning, particularly when tasks require many-steplong solutions, implying that they can backtrack only a limited number of future steps from the answer to the current progress of reasoning.From a cognitive perspective, their behaviors would be somewhat human-like in the sense that the models try to employ heuristics first in solving a complex problem.Moreover, this paper is the first to show that models are equipped with both heuristic and goal-oriented reasoning and dynamically switch between them as needed.</p>
<p>Task</p>
<p>We adopt an arithmetic reasoning task as a controlled testbed to analyze LM's reasoning ability (Figure 2 left).We will use natural and artificially controlled datasets in the experiments, but let us use the latter, more formal examples to explain the task overview.</p>
<p>Arithmetic reasoning task: The problem consists of a set of premises P = {p 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , p k } and a question q.Each premise describes either type of fact: (i) Person A has n items (A=n), or (ii) Person B has n more/fewer items than A has (B=A+n or B=A-n).The question q asks the exact number of specific items a particular person ultimately has (How many items does B have?).Here, one should consider multiple premises to derive the final answer, e.g., A=3; B=2+A; B=2+3=5.Notably, some premises are irrelevant to the answer; thus, models have to track which premise is necessary to reach the final answer.</p>
<p>Reasoning step: Let f be a model that is instructed to solve the task step-by-step.In each reasoning step t, the model f selects a particular premise p i ‚àà P and paraphrases it into a new fact z t by eagerly resolving reference expressions based on the already stated facts
z &lt;t = [z 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , z t‚àí1 ]
as in equation ( 1):
(p i , z t ) = f (P, q, z &lt;t ) .(1)
For example, in Figure 2, when p 2 , Walter has 2 more apples than Peggy., is selected at a particular reasoning step, the respective z t should be Walter has 2+5=7 apples.if z &lt;t already contains the number of apples Peggy has, i.e., p 1 .2Starting with an empty set of stated facts z = {}, the model recursively performs a reasoning step and can stop when outputting a special symbol EOS or answering the question q.Here, we denote the whole history of selected premises as h = [p i , ‚Ä¢ ‚Ä¢ ‚Ä¢ , p j ] ‚àà P * , where P * is Kleene closure of P .Its t-th element h t is the premise to derive the t-th reasoning step z t .Henceforth, we call h reasoning steps and focus on the ability to search for the right h.</p>
<p>Solutions: Among the possible reasoning steps P * , there is a set of solutions H  10). Notably, we do not care about the ability to correctly introduce a new fact z t (Eq.1), e.g., the accuracy of arithmetic operation (e.g., 5+2=7), but separately focus on their search strategy to select the relevant premise to perform the next reasoning step.</p>
<p>Heuristics</p>
<p>Given existing studies on LMs' use of heuristics ( ¬ß5), we focus on the following types of heuristics:</p>
<p>Lexical overlap between premise and question (OVERLAP): Neural models generally tend to rely on superficial, shallow similarity of texts when considering their associations.We specifically examine whether models select premises with the same person name (PN) as the one in question as a representative of such biases.For example, given a question how many apples Judy has, premises such as Judy's mother got 3 apples might be selected as a relevant fact, regardless of its necessity to reach the answer.</p>
<p>Position of premise (POSITION): It has been reported that models tend to select information, e.g., first and last, in specific positions in the context (Liu et al., 2024).We examine whether models tend to select the premise in the initial position of context.</p>
<p>Grammatical feature of premise (NEGATIVE):</p>
<p>Given that a specific grammatical feature, e.g., negation word, is often a superficial cue (Du et al., 2021;Niven and Kao, 2019), we specifically analyze the bias that models avoid selecting premise with negation word, i.e., not.</p>
<p>Experiments</p>
<p>We first confirm that models indeed rely on particular types of heuristics in our setting ( ¬ß4.1).Then, we investigate when in the step-by-step reasoning, such heuristics are more frequently exploited ( ¬ß4.2).</p>
<p>General settings: We use four representative variants of large language models (LLMs): textbison-001 version of Google'sPaLM2 (Anil et al., 2023), Llama2-13B (Touvron et al., 2023), gpt-3.5-turbo-0125and gpt-4-0613 snapshots of Ope-nAI's GPT-3.5-turbo (OpenAI, 2022) and GPT-4 (OpenAI, 2023).These models are instructed to yield a minimum solution via prompting.</p>
<p>Preliminary experiments</p>
<p>First, we confirm that LLMs exploit specific heuristics in natural and artificially controlled datasets during step-by-step reasoning.</p>
<p>Settings for Negative) than the BASE scores across models and datasets.This indicates that LLMs, on average, tend to rely on our targeted heuristics ( ¬ß3).Interestingly, different models yield different preferences towards distractor types; for example, Llama2 and GPT-3.5 have more biased premise positions than PaLM2 and GPT-4.Note that whether or not a distractor was selected was determined by some rules.Such details are exmplained in the Appendices A.2 and B.2.</p>
<p>Main experiments</p>
<p>Then, we further investigate the LMs' dynamic use of heuristics.We hypothesized that the more distant the current reasoning step is from the answer (higher d in ¬ß2), the more heavily models rely on heuristics.Again, generating the minimal solution requires the model to track/plan the future path (remaining necessary and sufficient information) to reach the final answer, and its remaining length becomes longer at the initial phase of reasoning.If models have a limited capacity to track the future path, they may have to give up rational reasoning and rely on heuristics, particularly at the earlier stage of reasoning, where the volume of the remaining future path is likely to exceed the model's capacity.</p>
<p>Distractor and evaluation: To investigate the relationship between the distance to the answer and the models' reliance on heuristics, we identify at which steps heuristics are more likely to be exploited.Ideally, to facilitate a fair step-wise comparison, one should design a distractor equally attractive to all the reasoning steps in h * and analyze when it is selected during the reasoning; however, such a distractor is inherently difficult</p>
<p>Context: Peggy has 5 apples.Walter has 2 more apples than Peggy.Judy's mother has 3 less apples than Peggy.Judy has 4 more apples than Walter has.Question: How many apples does Judy have?</p>
<p>Table 2: Example of a distractor examined in ¬ß4.2.Suppose that h * 1 is "Peggy has 5 apples."Two candidate premises with "Peggy" seem to be plausible continuations as h *</p>
<p>2 , but only one is relevant to the final answer (green), and the other is a distractor (orange).to implement.Instead, we prepare multiple distractors P to the problem in artificial data; each of them pt ‚àà P correspond to each reasoning step h * t in the sense that both share the same person name that appeared in the previous step h * t‚àí1 (Table 2).3Similar to ¬ß4.1, we further modify each distractor pt ‚àà P to match each heuristic (Overlap with question or Position in ¬ß3). 4 In evaluation, for each t, partial correct reasoning steps h * &lt;t are teacher-forced to a model, and we analyze whether the model selects the right next step h * t or its respective distractor pt .Specifically, we calculate the frequency #(‚Ä¢) of models' selecting pt or h * t ; then, the ratio of exploiting a distractor r = #pt #pt+#h * t is reported.The chance rate should be 0.5.</p>
<p>Note that we could not use the GSM8K dataset in this experiment since designing such controlled distractors for each reasoning step was not feasible, i.e., our creation of artificial, controlled data enables this kind of analysis.In addition, we will not analyze the NEGATIVE heuristic in this experiment because it is a bias in the direction of avoidance, making the experimental design complicated.</p>
<p>Data: We used 5-step artificial reasoning data; that is, the distance to the answer is five at first d = 5 and will monotonically decrease (d = 5 ‚Üí 4 ‚Üí ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Üí 0) through the steps in the minimal solution.The first (d = 5 ‚Üí 4) and the last (d = 1 ‚Üí 0) steps are excluded from our analysis for their special properties; e.g., the right last step can be identified simply by detecting the lexical overlap with the question.</p>
<p>Results:</p>
<p>The results are shown in Figure 3.The x-axis is the change of remaining steps d to the goal in the respective reasoning step, and the yaxis is the ratio r of selecting distractors pt .The more distant the current step is from the answer (larger d), the more frequently the distractor is selected (larger r), which is typically above the chance rate.PaLM2 and GPT-4 exhibited apparent tendencies of the negative slopes between d and r.These results match the hypothesized behavior, the model's less rational behavior in earlier reasoning steps, and imply that they have a limited capacity to track the future reasoning path.</p>
<p>Related work</p>
<p>Multi-step symbolic reasoning: Given the general contrast between the symbolic and neuralbased approaches in the artificial intelligence field (Hamilton et al., 2022), the community has questioned the ability of neural LMs in emulating particular symbolic operations, e.g., graph search algorithm (Aoki et al., 2023;Yao et al., 2023;Fang et al., 2024).In contrast, to identify what kind of symbolic tasks are (im)possible to solve for LMs by varying task complexities (Clark et al., 2020), we investigate the inherent, systematic biases in solving a particular symbolic reasoning task.</p>
<p>Heuristics in LM: Neural models have typically been distracted by superficial biases (Du et al., 2022).For example, they tend to use superficial linguistic artifacts (Lai et al., 2021;Sen and Saffari, 2020;Du et al., 2021;Niven and Kao, 2019), or more simply, positional features (Ko et al., 2020), even with chain-of-thought prompting (Madaan and Yazdanbakhsh, 2022); these motivated our experiments.This paper shows that the LLMs' reliance on such heuristics changes dynamically as the reasoning progresses.</p>
<p>Search algorithm: Finding the shortest path between the start and the goal on a graph is a standard problem in computer science (Russell and Norvig, 1995).Our investigation of LMs on the arithmetic tasks can be seen as characterizing LMs' biases as a search algorithm.The use of heuristics in graph search is, more or less, related to the A<em> search algorithm (Hart et al., 1968), although heuristics in A</em> search is a more narrow concept regarding the distance to the goal than those employed by LMs.</p>
<p>Conclusion</p>
<p>We have found a systematic strategy for the use of heuristics in LMs' multi-step reasoning-a dynamic transition from a heuristic to a rational reasoning strategy during LMs' step-by-step reasoning.These results are hopefully helpful for researchers to understand their underlying mechanism as well as for LM users to consider the inherent biases systems have.</p>
<p>Limitations</p>
<p>This study focused only on four specific language models and two arithmetic tasks.Increasing the coverage of models and tasks is a possible future direction, although we ensured that our finding generalizes at least several models and task settings.In ¬ß4.2, we only used artificial datasets for designing controlled experiments to reduce confounding factors.Constructing a controlled but natural dataset to evaluate the reasoning strategies of LMs should be encouraged.Furthermore, our findings are based solely on the model's outputs, i.e., behaviors.Elucidating the underlying mechanisms inside the model and the source of these biases (e.g., statistical patterns in training data) should be investigated in future work.</p>
<p>Ethics statement</p>
<p>This paper does not involve ethical concerns in the sense that we (i) did not conduct human experiments, (ii) just created artificial data without any potentially harmful contents, and (iii) did not address tasks related to ethically sensitive topics.</p>
<p>Context: Jamesname decides to run 3num sprints 3num times a week.Hepronoun runs 60num meters each sprint.</p>
<p>Question: How many total meters does hepronoun run a week?</p>
<p>Person's NamesÔºö James NumbersÔºö 3,60 A GSM8K experiments in ¬ß4.1</p>
<p>A.1 Dataset construction process</p>
<p>As described in 4.1, we modify the existing multihop numerical reasoning dataset, GSM8K (distributed under the MIT license), to construct the evaluation dataset.The dataset construction process is divided into two steps: 1. Extracting instances from GSM8K, and 2. Inserting distractors according to the heuristic we want to evaluate for each extracted problem statement.</p>
<p>A.1.1 Instance extraction</p>
<p>There is no guarantee that the premises added during data expansion will not affect the solution.Therefore, instances are extracted in which the addition of premises has little effect on the solution.Specifically, we extract instances from the GSM8K evaluation dataset following the process below:</p>
<ol>
<li>
<p>We manually create a list of 50 person names (PNs) from a subset of the GSM8K evaluation dataset.</p>
</li>
<li>
<p>Using regular expressions, we identify PNs from this list.</p>
</li>
<li>
<p>We identify pronouns and numerical expressions present in each instance.</p>
</li>
<li>
<p>We extracted instances that included precisely one from our list in both the context and the question, and where either the PN or a pronoun appears in the question (e.g., Table 3).</p>
</li>
<li>
<p>We replaced all pronouns within the extracted instances with the corresponding person's name.</p>
</li>
</ol>
<p>In the following process, information on persons not appearing in the problem statement is added as distractors.The instance questions extracted above are about persons in the problem statement.Therefore, it can be guaranteed that adding a distractor will not change the answer by such a process.</p>
<p>A.1.2 Distractor insertion</p>
<p>Subsequently, we added distractors to the extracted instances according to each heuristic, thereby constructing 76 instances for the evaluation dataset.Below, we will describe the process of creating the evaluation dataset for each heuristic.</p>
<p>Base We insert a template-based random distractor (i.e., p) into each instance as a baseline.</p>
<p>The distractor was created using the following steps:</p>
<ol>
<li>
<p>We randomly selected one sentence from the instance that included a PN or pronoun and a numerical expression.</p>
</li>
<li>
<p>We replaced the PN or pronoun in the selected sentence with a placeholder, [name].</p>
</li>
<li>
<p>We replaced the numerical expression in the selected sentence with a placeholder, [num].</p>
</li>
<li>
<p>We replaced [name] with a randomly selected name from the list of PNs created in A.1.1,excluding the name already present in the instance.</p>
</li>
<li>
<p>We replaced [num] with another value.5</p>
</li>
</ol>
<p>6.We inserted the created distractor into a random position in the context other than the beginning of the instance.</p>
<p>For example, When the sentence "James decides to run 3 sprints 3 times a week." is selected from the instance in Overlap To evaluate whether the Overlap heuristic influences the model, we insert distractors p into each instance following the steps below:</p>
<ol>
<li>
<p>We substituted the placeholder [name] within the Base distractor template with the person' s name found in the instance, appended by relational phrases such as "'s mother", "'s father", "'s son", or "'s neighborhood" (e.g., in the instance from Table 3, this would become "James's mother").</p>
</li>
<li>
<p>We replaced the number in the sentence with another numerical value.</p>
</li>
<li>
<p>We placed the constructed distractor into context at the location where the Base distractor was positioned in the instance.</p>
</li>
</ol>
<p>Position To evaluate whether the heuristic of Position induces the model, we insert distractors p into each instance.Each distractor is identical to the Base distractor except for its insertion point.Specifically, we relocated the distractor's insertion point to a random position closer to the beginning of the context than the position used for the Base distractor.</p>
<p>Negative To evaluate the model's response to the Negative heuristic, we insert distractors p into each instance created based on the following template:</p>
<p>[name] doesn't have [num] [object].</p>
<p>In this template:</p>
<p>‚Ä¢ [name] is substituted with a random PN included in the instance.</p>
<p>‚Ä¢ [object] is replaced with one of the following items: "apples," "bananas," "grapes," "pencils," or "books."</p>
<p>‚Ä¢ [num] is replaced with a different numerical value, using the same algorithm for creating the Base distractor.</p>
<p>A.2 Evaluation</p>
<p>To determine whether the LMs selected the distractor during reasoning, we check if the numbers in the distractor p are in the facts z.We calculate the frequency of instances where the distractor is selected.Base We construct the artificial data using the method outlined below, based on the template presented in Table 4.</p>
<p>‚Ä¢ Randomly assign one of the following names to the placeholders [nameA] to [nameE]: "Alice," "Bob," "Carol," "Dave," "Eve," "Frank," "Grace," "Heidi," "Ivan," "Judy," "Kevin," "Larry," "Mallory," "Nancy," "Olivia," "Peggy," "Quentin," "Rob," "Sybil," "Trent," "Ursula," "Victor," "Walter," "Xavier," "Yvonne," or "Zoe."</p>
<p>‚Ä¢ Assign a randomly selected value from [nameA] to [nameD] to the placeholder [nameX].</p>
<p>‚Ä¢ Assign a random number from 0 to 100 to the placeholder [num].</p>
<p>‚Ä¢ Assign one of the objects "apples," "bananas," "grapes," "pencils," or "books" to the placeholder [object].</p>
<p>‚Ä¢ Assign either "more" or "fewer" to the placeholder [relation].</p>
<p>‚Ä¢ Randomly shuffle the order of the sentences.</p>
<p>Overlap We constructed a dataset to evaluate whether the Overlap heuristic induced the model by making certain modifications to the Base distractor for each instance.Specifically, we modified the value of [nameD] by appending relational phrases such as " 's mother", " 's father", " 's son", or " 's neighborhood" to the existing value of [nameD].We then assigned this modified value to [nameE].</p>
<p>B.2 Evaluation</p>
<p>To determine whether the LMs selected the distractor during reasoning, we check if the subject of the distractor (i.e., [nameE]) is included in the facts z.We calculate the frequency of instances where the distractor is selected.</p>
<p>C Artificial data in ¬ß4.2</p>
<p>We prepare a template similar to a Table 5 and assign values to the template according to the following steps:</p>
<p>‚Ä¢ We create template as shown in table4.</p>
<p>‚Ä¢ Within the template, placeholders [nameA] to [nameQ] is filled randomly with names such as "Alice", "Bob", "Carol", "Dave", "Eve", "Frank", "Grace", "Heidi", "Ivan", "Judy", "Kevin", "Larry", "Mallory", "Nancy", "Olivia", "Peggy", "Quentin", "Rob", "Sybil", "Trent", "Ursula", "Victor", "Walter", "Xavier", "Yvonne", "Zoe".</p>
<p>‚Ä¢ The placeholder [num] is filled with a random number from 0 to 100.</p>
<p>‚Ä¢ The placeholder [object] is filled randomly with items such as "apples", "bananas", "grapes", "pencils", "books".</p>
<p>‚Ä¢ The placeholder [relation] is assigned either "more" or "fewer".</p>
<p>‚Ä¢ Sentences within the context are shuffled randomly.</p>
<p>‚Ä¢ A distractor is inserted at a random position.</p>
<p>Then, using the following procedures, We create each expanded dataset.Each targeted heuristic strongly influences the heuristic distractors designed in this study.Each dataset consists of 300 problems.</p>
<p>‚Ä¢ For the Overlap dataset, the values "'s mother", "'s father", "'s son", and "'s neighborhood" are appended to [nameE]  ‚Ä¢ For the Negative dataset, the form of the heuristic distractor is changed to a negative form.</p>
<p>In ¬ß4.2, the method to identify which premises are used for reasoning was similar to that in App.B, relying on regular expressions.</p>
<p>D Mearuing accuracy in ¬ß4.1</p>
<p>This paper was mainly concerned with the frequency of distractor selection.To ensure that the model is not producing crappy output in these experiments, we measure the accuracy.GSM8K.Additionally, Table 7 below shows the accuracy rates of each model on artificial data.</p>
<p>From the Table 6, 7, it is shown that GPT-4 had the highest accuracy rates across the datasets, while Llama2 had the lowest.It is expected that these outcomes are due to differences in the number of parameters in the model.</p>
<p>E Generation settings</p>
<p>When using GPT-3.5 and GPT-4, the settings are adjusted to temperature=0.0, fre-quency_penalty=0, and presence_penalty=0.Similarly, for PaLM2 and Llama2, the temperature is set to 0, with no sampling.</p>
<p>We use NVIDIA RTX A6000 (48GB) GPUs for inference with Llama2.</p>
<p>F Few-shot examples</p>
<p>The few-shot examples for models regarding datasets GSM8K and artificial data are shown in the respective Tables 9, 10.</p>
<p>G Effect of few-shot examples</p>
<p>We investigate whether the few-shot examples trigger the model's heuristic.Specifically, we replace the few-shot examples in the following ways to study the relationship between the model's heuristic and its inputs: 1.We change the few-shot examples to induce Overlap (as shown in  an increase in reasoning frequency with the use of distractors in the Position dataset compared to Table 1.</p>
<ol>
<li>We change the few-shot examples to induce Negative (as shown in Table 13), and investigate if there's a decrease in reasoning frequency with the use of distractors in the Negative dataset compared to Table 1.We measure the frequency of selecting p in ¬ß4.1.The results are presented in Table 8.As shown in Tables 1, 8, although the few-shot examples fed into the models such as GPT-3.5,GPT-4, and PaLM2 was changed, there was no significant change in reasoning frequency as described.This suggests that the model's heuristic does not merely mimic the examples provided as input.On the other hand, the Llama2 model was more prone to being misled by changes in input, and smaller models demonstrated a reduced capacity to reach the correct answers.</li>
</ol>
<p>H Effects of increasing the number of distructors</p>
<p>We investigate whether the same results could be obtained when the number of distractors that do not include heuristics increases.Specifically, we prepare the same settings as in ¬ß4.2 and add distractors that do not include heuristics.We define the distractors that include heuristics as pt,heuristic and those that do not as pt,non-heuristic .The ratio of pt,heuristic to pt,non-heuristic in the problem text is 1:8.In this experiment, we use a model that could produce reasonable output even when the context length increases.Specifically, we use only the GPT-3.5-turboand GPT-4.Figures 4 and 5 show how many times out of 100 times the model selected pt,heuristic , pt,non-heuristic , and h * t at each step.Figures 4 and 5 show the experimental results for the cases where the heuristics included in the distructor are overlap and position, respectively.Figures 4 and 5 show that the number of cases in which the shortest path Answer the context question using the following example.</p>
<p>Context: Leo's assignment was divided into three parts.Weng earns $12 an hour for babysitting.It took Leo twice as long to finish the second part.Yesterday, she just did 50 minutes of babysitting.Question: How much did Weng earn?Answer: Weng earns 12/60 = 0.2 per minute.Working 50 minutes, she earned 0.2 x 50 = 10.The final answer is 10.</p>
<p>Context: Betty is saving money for a new wallet, which costs $100.Betty has only half of the money she needs.Alice is saving money for a new wallet, which costs $2000.Betty's parents decided to give Betty $15 for that purpose, and her grandparents twice as much as her parents.Question: How much more money does Betty need to buy the wallet?Question: How much more money does Betty need to buy the wallet?Answer:</p>
<p>In the beginning, Betty has only 100 / 2 = 50.Betty's parents gave her 15.Betty's grandparents gave her 15 * 2 = 30.This means Betty needs 100 -50 -15 -30 = 5 more.The final answer is 5.</p>
<p>Context: Julie is reading a 120-page book.Yesterday, Julie was able to read 12 pages, and today, she read twice as many pages as yesterday.Julie's mother makes $18.00 an hour.Question: If Julie wants to read half of the remaining pages tomorrow, how many are left to read?Answer: Julie read 12 x 2 = ¬´12*2=24¬ª24 pages today So, she was able to read a total of 12 + 24 = 36 pages since yesterday.There are 120 -36 = 84 pages left to be read.Since she wants to read half of the remaining pages tomorrow, she should read 84/2 = 42 pages.The final answer is 42.</p>
<p>Context: James writes a 2-page letter to 4 different friends who live in America twice a week.James writes a 3-page letter to 2 different friends who live in Japan twice a week.Question: How many pages does James write each friend who lived in Japan for a year?Answer: He writes each friend 3<em>2=6 pages a week.So, he writes 6</em>2=12 pages every week.That means he writes 12*52=624 pages a year.The final answer is 624.d).The heuristic to be included in the distructor is overlap.</p>
<p>is selected increases as the goal is reached.The ratio of distructors that include heuristics to those that do not is 1:8, but the ratio of distructors that include heuristics when distructors are selected is higher than 1/(1+8).Therefore, this suggests that premises are being selected using heuristics.Except for the GPT-3.5 (overlap) results, the number  d).The heuristic to be included in the distructor is position.</p>
<p>of cases where a destructor is selected decreases as the goal is approached.These results are as with the results for 4.2.The results of  may indicate that the increase in the number of distructors has reduced the focus on distructors, including heuristics.</p>
<p>I Usage of Writing Assistance</p>
<p>We use publicly available writing assistance tools, including Grammarly, to refine the language for readability.</p>
<p>Answer the context question using the following example.</p>
<p>Context</p>
<p>s o n in g s te p s</p>
<p>Figure 2 :
2
Figure 2: Overview of the task setting.Given premises and a question, a model answers the question step-by-step (left part).Through each reasoning step t of selecting/paraphrasing relevant premise p k ‚àà P , the available facts z are enriched (reasoning state progresses in the right part).If a reasoning step follows the minimal solution (green path in the right part), the distance to the answer d decreases.</p>
<p>reasoning step and goad Probability of distractors selected</p>
<p>Figure 3 :
3
Figure3: The ratio at which a particular distractor is selected (y-axis: r) in each reasoning step (x-axis: d).</p>
<p>Figure 4 :
4
Figure 4: Number of cases where a particular distractor is selected (y-axis: r) in each reasoning step (x-axis:d).The heuristic to be included in the distructor is overlap.</p>
<p>Figure 5 :
5
Figure 5: Number of cases where a particular distractor is selected (y-axis: r) in each reasoning step (x-axis:d).The heuristic to be included in the distructor is position.</p>
<p>In each reasoning step t, one can determine the minimum number of remaining reasoning steps to reach the answer d ‚àà N, given h ‚â§t ‚àà P * and the initially provided premises P .The distance d can be derived from a state transition graph and the minimum number of transitions to the closest final states, as shown in Figure2(right part).Here, we denote the mapping function from h ‚â§t to d as g : P * ‚Üí N.
For ex-ample, g([p 2 , p 1 , p 2 ]) = 1 in Figure 2. A minimalsolution h  *  satisfies ‚àÄt g(h  *  ‚â§t ) &lt; g(h  *  ‚â§t‚àí1 ).Targeted ability of LMs: We evaluate LMs'ability to derive the minimal solution h  *  as in-structed by 4-shot examples (Table
‚Ä¢ ‚äÇ P * , where the final stated fact z ‚àí1 in a solution h ‚àà H ‚Ä¢ yields the right answer to the question q.Figure2illustrates such a set of solutions H ‚Ä¢ as the steps leading to the final states of the state transition graph (right part of Figure2), e.g., [p 1 , p 3 , p 2 , p 4 ] ‚àà H ‚Ä¢ .Minimal solution: Within the set of solutions, there is only one minimal solution h * ‚àà H ‚Ä¢ ‚äÇ P * .Intuitively, h * does not contain any irrelevant step to approach the answer; for example, the minimal solution of the problem in Figure2is [p 1 , p 2 , p 4 ] = h * .To define h * , let us first intro-duce a distance to the answer.</p>
<p>Table 3 :
3
Extraction of names, personal pronouns, and numbers on GSM8K.</p>
<p>Table 3
3, a template "[name] de-cides to run [num] sprints [num] times a week."is crafted. Names and numbers are randomly se-lected from the candidates and placed into theseplaceholders, and the resulting distractor is theninserted into the context.</p>
<p>Table 4 :
4
Template of artificial data in ¬ß4.1.
B Artificial-data experiments in  ¬ß4.1B.1 Dataset construction process</p>
<p>Table 5 :
5
Template of artificial data in ¬ß4.2.</p>
<p>Position We modify the Base distractor to evaluate if the Position heuristic induces the model.Specifically, we altered the insertion point of the Base distractor to a randomly chosen position closer to the context's beginning than the original position used in the Base distractor.Negative To evaluate whether the Neg induces the model heuristic, we construct a dataset by modifying the Base distractor.Specifically, we convert the Base distractor into a negative expression (e.g., [nameE] doesn't have[num] [relation][object] than[nameX]).</p>
<p>Table 6 below shows the accuracy rates of each model on
BaseOver.Pos.Neg.PaLM264.5% 59.2% 60.5% 71.1%Llama230.3% 34.2% 30.3% 27.6%GPT-3.5 81.6% 64.5% 81.6% 82.9%GPT-485.5% 84.2% 82.9% 92.1%</p>
<p>Table 6 :
6
The accuracy while solving GSM8K.
BaseOver.Pos.Neg.PaLM260.0% 58.7% 77.0% 80.7%Llama221.3% 14.3% 22.3% 20.0%GPT-3.5 84.6% 87.3% 87.6% 82.9%GPT-498.7% 94.0% 98.0% 99.7%</p>
<p>Table 7 :
7
The accuracy while solving artificial reasoning tasks.</p>
<p>Table 11) and examine whether this increases the reasoning frequency with the use of distractors in the Overlap dataset compared to what is shown in Table 1. 2. We change the few-shot examples to induce Position (as shown in Table 12) and check if there's
Over.‚ÜëPos.‚ÜëNeg.‚ÜìPaLM241.0% 14.0%5.7%Llama282.0% 93.3% 28.0%GPT-3.5 11.7% 35.0%0.0%GPT-40.0%0.0%0.0%</p>
<p>Table 8 :
8
The frequency at which the model selected a distractor (i,e., p) while solving artificial reasoning tasks after changing few-shot examples.</p>
<p>Table 9 :
9
Examples of input given when solving GSM8K.</p>
<p>:</p>
<p>Walter has -22 apples.Ursula has 3 more apples than Walter.Victor has 3 more apples than Ursula.Quentin has 2 more apples than Ursula.Nancy has 3 more apples than Walter.Zoe has 3 more apples than Nancy.Heidi has 3 more apples than Nancy.Carol's mother has 4 apples.Xavier has 3 more apples than Carol's mother.Peggy has 4 more apples than Xavier.Dave has 13 more apples than Xavier.Bob has 1 more apples than Carol's mother.Alice has 3 more apples than Bob.Sybil has 56 more apples than Bob.Question: How many apples does Dave have?Answer: Carol's mother has 4 apples, and Xavier has 3 more apples than Carol's mother.So, Xavier has 4+3=7 apples.Xavier has 7 apples, and Dave has 13 more apples than Xavier.So, Dave has 7+13=20 apples.The final answer is 20.Context: Alice has 92 more bananas than Mallory.Victor has 10 fewer bananas than Walter.Xavier has 59 more bananas than Sybil.Yvonne has 79 more bananas than Sybil.Judy has 23 more bananas than Alice.Dave has 60 more bananas than Victor.Quentin has 35 fewer bananas than Peggy.Heidi has 95 more bananas than Victor.Ursula doesn't have 32 more bananas than Peggy.Larry has 17 fewer bananas than Alice.Zoe has 58 fewer bananas than Yvonne.Ivan has 43 fewer bananas than Yvonne.Walter has 43 fewer bananas than Mallory.Nancy has 34 bananas.Grace has 41 more bananas than Xavier.Mallory has 55 fewer bananas than Nancy.Sybil has 3 fewer bananas than Nancy.Peggy has 50 more bananas than Walter.Trent has 33 fewer bananas than Xavier.Question: How many bananas does Quentin have?Answer: Nancy has 34 bananas, and Mallory has 55 fewer bananas than Nancy.So, Mallory has 34-55=-21 bananas.Mallory has -21 bananas, and Walter has 43 fewer bananas than Mallory.So, Walter has -21-43=-64 bananas.Walter has -64 bananas, and Peggy has 50 more bananas than Walter.So, Peggy has -64+50=-14 bananas.Peggy has -14 bananas, and Quentin has 35 fewer bananas than Peggy.So, Quentin has -14-35=-49 bananas.The final answer is -49.Zoe has 10 more apples than Yvonne's son.Eve has 2 apples.Yvonne's son has 3 more apples than Eve.Quentin has 3 more apples than Yvonne.Yvonne has 3 fewer apples than Zoe.Alice has 3 more apples than Grace.Trent has 34 more apples than Zoe.Ivan has 3 apples.Ursula has 3 more apples than Zoe.Grace has 3 apples.Xavier doesn't have 3 more apples than Ivan.Kevin's friend has 33 fewer grapes than Rob.Ivan has 43 more grapes than Victor.Victor has 33 fewer grapes than Kevin's friend.Ursula has 75 fewer grapes than Zoe.Alice has 11 more grapes than Eve.Dave has 11 more grapes than Eve.Olivia has 29 more grapes than Kevin's friend.Mallory has 97 more grapes than Olivia.Judy has 78 more grapes than Olivia.Rob has 55 grapes.Frank has 70 fewer grapes than Heidi.Eve has 84 fewer grapes than Sybil.Xavier has 36 more grapes than Heidi.Sybil has 55 fewer grapes than Trent.Kevin has 43 fewer grapes than Zoe.Heidi has 61 fewer grapes than Trent.Zoe has 88 more grapes than Sybil.Trent has 40 more grapes than Rob.Walter has 38 more grapes than Victor.
Question: How many apples does Yvonne have?Answer:Eve has 2 apples, and Yvonne's son has 3 more apples than Eve. So, Yvonne's son has 2+3=5 apples.Yvonne's son has 5 apples, and Zoe has 10 more apples than Yvonne's son. So, Zoe has 5+10=15 apples.Zoe has 15 apples, and Yvonne has 3 fewer apples than Zoe. So, Yvonne has 15-3=12 apples.The final answer is 12.Question: How many grapes does Kevin have?Answer:Rob has 55 grapes, and Trent has 40 more grapes than Rob. So, Trent has 55+40=95 grapes.Trent has 95 grapes, and Sybil has 55 fewer grapes than Trent. So, Sybil has 95-55=40 grapes.Sybil has 40 grapes, and Zoe has 88 more grapes than Sybil. So, Zoe has 40+88=128 grapes.Zoe has 128 grapes, and Kevin has 43 fewer grapes than Zoe. So, Kevin has 128-43=85 grapes.The final answer is 85.
Context:Context:</p>
<p>Table 10 :
10
Examples of input given when solving an artificial dataset.</p>
<p>Table 11 :
11
Examples of input given when solving the Overlap dataset.Carol's mother has 4 apples.Xavier has 3 more apples than Carol's mother.Dave has 13 more apples than Xavier.Walter has -22 apples.Ursula has 3 more apples than Walter.Victor has 3 more apples than Ursula.Quentin has 2 more apples than Ursula.Nancy has 3 more apples than Walter.Zoe has 3 more apples than Nancy.Heidi has 3 more apples than Nancy.Peggy has 4 more apples than Xavier.Bob has 1 more apples than Carol's mother.Alice has 3 more apples than Bob.Sybil has 56 more apples than Bob.Question: How many apples does Dave have?Answer: Carol's mother has 4 apples, and Xavier has 3 more apples than Carol's mother.So, Xavier has 4+3=7 apples.Xavier has 7 apples, and Dave has 13 more apples than Xavier.So, Dave has 7+13=20 apples.The final answer is 20.Context: Nancy has 34 bananas.Mallory has 55 fewer bananas than Nancy.Walter has 43 fewer bananas than Mallory.Peggy has 50 more bananas than Walter.Quentin has 35 fewer bananas than Peggy.Alice has 92 more bananas than Mallory.Victor has 10 fewer bananas than Walter.Xavier has 59 more bananas than Sybil.Yvonne has 79 more bananas than Sybil.Judy has 23 more bananas than Alice.Dave has 60 more bananas than Victor.Heidi has 95 more bananas than Victor.Ursula doesn't have 32 more bananas than Peggy.Larry has 17 fewer bananas than Alice.Zoe has 58 fewer bananas than Yvonne.Ivan has 43 fewer bananas than Yvonne.Grace has 41 more bananas than Xavier.Sybil has 3 fewer bananas than Nancy.Trent has 33 fewer bananas than Xavier.Question: How many bananas does Quentin have?Answer: Nancy has 34 bananas, and Mallory has 55 fewer bananas than Nancy.So, Mallory has 34-55=-21 bananas.Mallory has -21 bananas, and Walter has 43 fewer bananas than Mallory.So, Walter has -21-43=-64 bananas.Walter has -64 bananas, and Peggy has 50 more bananas than Walter.So, Peggy has -64+50=-14 bananas.Peggy has -14 bananas, and Quentin has 35 fewer bananas than Peggy.So, Quentin has -14-35=-49 bananas.The final answer is -49.Context: Eve has 2 apples.Yvonne's son has 3 more apples than Eve.Zoe has 10 more apples than Yvonne's son.Yvonne has 3 fewer apples than Zoe.Alice has 3 more apples than Grace.Quentin has 3 more apples than Yvonne.Trent has 34 more apples than Zoe.Ivan has 3 apples.Ursula has 3 more apples than Zoe.Grace has 3 apples.Xavier has 3 more apples than Ivan.Question: How many apples does Yvonne have?Answer: Eve has 2 apples, and Yvonne's son has 3 more apples than Eve.So, Yvonne's son has 2+3=5 apples.Yvonne's son has 5 apples, and Zoe has 10 more apples than Yvonne's son.So, Zoe has 5+10=15 apples.Zoe has 15 apples, and Yvonne has 3 fewer apples than Zoe.So, Yvonne has 15-3=12 apples.The final answer is 12. Context: Rob has 55 grapes.Trent has 40 more grapes than Rob.Sybil has 55 fewer grapes than Trent.Zoe has 88 more grapes than Sybil.Kevin has 43 fewer grapes than Zoe.Kevin's friend has 33 fewer grapes than Rob.Ivan has 43 more grapes than Victor.Victor has 33 fewer grapes than Kevin's friend.Ursula has 75 fewer grapes than Zoe.Alice has 11 more grapes than Eve.Dave has 11 more grapes than Eve.Olivia has 29 more grapes than Kevin's friend.Mallory has 97 more grapes than Olivia.Judy has 78 more grapes than Olivia.Frank has 70 fewer grapes than Heidi.Eve has 84 fewer grapes than Sybil.Xavier has 36 more grapes than Heidi.Heidi has 61 fewer grapes than Trent.Walter has 38 more grapes than Victor.Question: How many grapes does Kevin have?Answer: Rob has 55 grapes, and Trent has 40 more grapes than Rob.So, Trent has 55+40=95 grapes.Trent has 95 grapes, and Sybil has 55 fewer grapes than Trent.So, Sybil has 95-55=40 grapes.Sybil has 40 grapes, and Zoe has 88 more grapes than Sybil.So, Zoe has 40+88=128 grapes.Zoe has 128 grapes, and Kevin has 43 fewer grapes than Zoe.So, Kevin has 128-43=85 grapes.The final answer is 85.
Answer the context question using the following example.
Context:</p>
<p>Table 12 :
12
Examples of input given when solving the Position dataset.</p>
<p>If the reference can not be resolved with z&lt;t, the model repeats the selected premise pi as zt.
To rule out the shortcut cue regarding the reference frequencies of each person name, we further added distractor premises to make the frequencies uniform.
  4  We excluded the Negative (avoidance) bias because if a model avoids negation in the latter step, we cannot distinguish whether it was due to the heuristic or rational search.
The replacement number was calculated by multiplying each number appearing in the sentence by either 0.5, 0.8, 1.2, 1.5, or 2 and then rounding down to the nearest whole number.
AcknowledgementsWe thank the member of the Tohoku NLP Group for their cooperation in this research.This work was supported by JSPS KAKENHI Grant Numbers 21K21343 and 24K16077, JST CREST Grant Number JPMJCR20D2, JST SPRING Grant Number JPMJSP2114, and JST BOOST Grant Number JPMJBS2421.
Alice has 3 more apples than Bob. Sybil has 56 more apples than Bob. Question: How many apples does Dave have? Answer: Carol's mother has 4 apples, and Xavier has 3 more apples than Carol's mother. So, Xavier has 4+3=7 apples. Xavier has 7 apples, and Dave has 13 more apples than Xavier. So, Dave has 7+13=20 apples. The final answer is 20. Context: Alice has 92 more bananas than Mallory. Victor has 10 fewer bananas than Walter. Xavier has 59 more bananas than Sybil. Yvonne doesn't have 79 more bananas than Sybil. Judy doesn't have 23 more bananas than Alice. Dave has 60 more bananas than Victor. Quentin has 35 fewer bananas than Peggy. Heidi has 95 more bananas than Victor. Ursula doesn't have 32 more bananas than Peggy. Larry doesn't have 17 fewer bananas than Alice. Zoe has 58 fewer bananas than Yvonne. Ivan has 43 fewer bananas than Yvonne. Walter has 43 fewer bananas than Mallory. Nancy has 34 bananas. Grace doesn't have 41 more bananas than Xavier. Mallory has 55 fewer bananas than Nancy. Sybil doesn't have 3 fewer bananas than Nancy. Peggy has 50 more bananas than Walter. Trent doesn't have 33 fewer bananas than Xavier. Question: How many bananas does Quentin have? Answer: Nancy has 34 bananas, and Mallory has 55 fewer bananas than Nancy. So, Mallory has 34-55=-21 bananas. Mallory has -21 bananas, and Walter has 43 fewer bananas than Mallory. So, Walter has -21-43=-64 bananas. Walter has -64 bananas, and Peggy has 50 more bananas than Walter. So, Peggy has -64+50=-14 bananas. Peggy has -14 bananas, and Quentin has 35 fewer bananas than Peggy. So, Quentin has -14-35=-49 bananas. The final answer is -49. Context: Zoe has 10 more apples than Yvonne's son. Eve has 2 apples. Yvonne's son has 3 more apples than Eve. Quentin has 3 more apples than Yvonne. Yvonne has 3 fewer apples than Zoe. References Gati, V Aher, Rosa I Arriaga, Adam Tauman, Kalai , International Conference on Machine Learning. 2023Alice has 3 more apples than Grace. Trent has 34 more apples than Zoe. Ivan has 3 apples. Ursula has 3 more apples than Zoe. Grace has 3 apples. Xavier doesn't have 3 more apples than Ivan. Question: How many apples does Yvonne have? Answer: Eve has 2 apples, and Yvonne's son has 3 more apples than Eve. So, Yvonne's son has 2+3=5 apples. Yvonne's son has 5 apples, and Zoe has 10 more apples than Yvonne's son. So, Zoe has 5+10=15 apples. Zoe has 15 apples, and Yvonne has 3 fewer apples than Zoe. So, Yvonne has 15-3=12 apples. The final answer is 12. Context: Kevin's friend has 33 fewer grapes than Rob. Ivan doesn't have 43 more grapes than Victor. Victor doesn't have 33 fewer grapes than Kevin's friend. Ursula has 75 fewer grapes than Zoe. Alice has 11 more grapes than Eve. Dave has 11 more grapes than Eve. Olivia doesn't have 29 more grapes than Kevin's friend. Mallory has 97 more grapes than Olivia. Judy doesn't have 78 more grapes than Olivia. Rob has 55 grapes. Frank has 70 fewer grapes than Heidi. Eve has 84 fewer grapes than Sybil. Xavier doesn't have 36 more grapes than Heidi. Sybil has 55 fewer grapes than Trent. Kevin has 43 fewer grapes than Zoe. Heidi has 61 fewer grapes than Trent. Zoe has 88 more grapes than Sybil. Trent has 40 more grapes than Rob. Walter has 38 more grapes than Victor. Question: How many grapes does Kevin have? Answer: Rob has 55 grapes, and Trent has 40 more grapes than Rob. So, Trent has 55+40=95 grapes. Trent has 95 grapes, and Sybil has 55 fewer grapes than Trent. So, Sybil has 95-55=40 grapes. Sybil has 40 grapes, and Zoe has 88 more grapes than Sybil. So, Zoe has 40+88=128 grapes. Zoe has 128 grapes, and Kevin has 43 fewer grapes than Zoe. So, Kevin has 128-43=85 grapes. The final answer is 85. Table 13: Examples of input given when solving the Neg dataset</p>            </div>
        </div>

    </div>
</body>
</html>
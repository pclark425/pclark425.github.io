<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2061 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2061</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2061</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-277467991</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.00255v2.pdf" target="_blank">SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers</a></p>
                <p><strong>Paper Abstract:</strong> This study evaluates large language models (LLMs) in generating code from algorithm descriptions in recent NLP papers. The task requires two key competencies: (1) algorithm comprehension: synthesizing information from papers and academic literature to understand implementation logic, and (2) coding expertise: identifying dependencies and correctly implementing necessary APIs. To facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark of 100 tasks from 36 NLP papers published in 2024, featuring detailed annotations and comprehensive test cases. Building on SciReplicate-Bench, we propose Sci-Reproducer, a dual-agent framework consisting of a Paper Agent that interprets algorithmic concepts from literature and a Code Agent that retrieves dependencies from repositories and implements solutions. To assess algorithm understanding, we introduce reasoning graph accuracy, which quantifies similarity between generated and reference reasoning graphs derived from code comments and structure. For evaluating implementation quality, we employ execution accuracy, CodeBLEU, and repository dependency/API recall metrics. In our experiments, we evaluate various powerful non-reasoning and reasoning LLMs as foundational models. The best-performing LLM using \ModelName~achieves only 39% execution accuracy, highlighting the benchmark's difficulty. Our analysis identifies missing or inconsistent algorithm descriptions as key barriers to successful reproduction. We make available our benchmark and code at https://github.com/xyzCS/SciReplicate-Bench and project homepage at https://xyzcs.github.io/scireplicate.github.io/.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2061.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2061.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sci-Reproducer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sci-Reproducer dual-agent framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-agent system (Paper Agent + Code Agent) that coordinates literature retrieval and repository-aware code synthesis to reproduce algorithms described in research papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sci-Reproducer</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>agentic system composed of large language model agents (dual-agent architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computational reproducibility / NLP algorithm implementation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>executable code implementing algorithms described in papers (function/method implementations)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution to moderately novel (reproducing recent 2024 papers; aims to synthesize implementations from descriptions rather than invent new algorithms)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-based reasoning + retrieval-augmented generation via Paper Agent (selective literature extraction) and Code Agent (repository search, API/dependency retrieval, iterative compile-debug loop)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>execution-based validation (integrate generated code into repository and run verification suite/test cases), CodeBLEU similarity to reference, reasoning graph similarity (reasoning graph accuracy), and recall metrics for dependency/API identification</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Generation (algorithm comprehension) measured by reasoning graph accuracy: average ~0.716 (without agents); small +0.037 gain with both agents. Generated code quality (synthesis) measured by CodeBLEU: average 0.320.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Execution accuracy (strict test-suite pass): average 0.235 across models with Sci-Reproducer; best model (Claude-Sonnet-3.7 with Sci-Reproducer) achieved 0.390 execution accuracy. Dependency/API recall improved markedly with Code Agent (avg increases: intra-file +0.441, cross-file +0.239, API +0.100).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Paper shows a clear drop in validation (execution accuracy) relative to generation-level understanding as tasks become less concretely specified; missing/mismatched paper details (novel or underspecified cases) reduced execution accuracy substantially. Quantitative per-novelty curves are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Yes — explicit asymmetry: high algorithmic-comprehension scores (reasoning graph accuracy ≈0.716) but much lower successful execution (execution accuracy average 0.235, best 0.390), demonstrating generation (understanding/specification) outpacing validation (executable correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported as part of generation outputs. Deterministic settings (temperature=0, top-p=1) were used for GPT-4o in node-matching evaluation to reduce evaluator randomness, but the generation agents themselves are not reported to produce calibrated uncertainty estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported / no calibration metrics provided for agents' confidence in their generated code or reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not directly reported. Dataset selection intentionally focused on 2024 papers to avoid data leakage; no explicit OOD benchmark results are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — proxy metrics include CodeBLEU (syntactic/semantic similarity) and dependency/API recall; reasoning graph accuracy is used as a proxy for correct algorithmic understanding rather than direct functional correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Used for dataset annotation and for validating the LLM-based reasoning-graph evaluator on a subset (20 tasks) — not applied across the full benchmark; authors recommend further comprehensive human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (applied NLP / software code reproduction); not highly formal like mathematics — domain empirical nature contributes to generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Dual-agent decomposition (Paper Agent + Code Agent), repository dependency/API recall, iterative compile-debug via Compiler tool, supplying missing/mismatched information as extra prompt context; these strategies yielded measurable improvements (e.g., Code Agent reduced syntax error rate from ~80% to ~29%, improved recalls and execution accuracy increments).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Quantitative: reasoning graph accuracy averages ≈0.716 while execution accuracy averages 0.235; best execution accuracy 0.390. Syntax error rates fall from ~80% without Code Agent to ~24.9-29.4% with Code Agent, indicating implementation (validation) is the bottleneck. Authors explicitly state LLMs 'can comprehend algorithm logic' but 'performance in code generation remains suboptimal.'</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None reported in the paper; authors report some modest improvements from agents but no evidence that validation capability exceeds generation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported — paper gives no quantitative comparison of compute/time cost of validation (test execution) vs generation; iterative compilation/debugging counts are reported qualitatively (e.g., reasoning models call Compiler more frequently) but not converted to cost ratios.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2061.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2061.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciReplicate-Bench benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated benchmark of 100 repository-level algorithm reproduction tasks from 36 NLP papers (2024), with reference implementations, reasoning-graph annotations, dependency annotations, and verification suites for execution-based evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmark / evaluation dataset</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computational reproducibility / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>reference implementations and verification suites; used to validate generated code outputs</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution relative to target papers (tasks drawn from contemporary 2024 literature); some tasks include missing/mismatched descriptions to simulate realistic challenges</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not a generator — provides inputs (LaTeX algorithm descriptions, function signatures, repository context) for LLMs/agents to generate code</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Execution-based verification suites (≈10 test cases per task where possible), comparison scripts, CodeBLEU, reasoning graph accuracy, and dependency/API recall metrics</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A (benchmark does not generate outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Enables measurement of execution accuracy (strict pass if all test cases match reference), CodeBLEU, and reasoning graph accuracy; used to report average execution accuracy (0.235) and CodeBLEU (0.320) across models.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Benchmark includes annotations for missing/mismatched information and reports that supplying missing information improves execution accuracy (examples: GPT-4o-mini +0.05, Deepseek-V3 +0.25, O3-mini-low +0.04).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Benchmark explicitly designed to expose a generation-validation gap by separating algorithm comprehension (reasoning graph) from executable correctness (execution accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not applicable for the benchmark itself (it provides deterministic tests and disables randomness in annotated repositories).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported; benchmark curated from recent in-domain papers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — CodeBLEU and reasoning graph accuracy are used as proxies for code quality and algorithm understanding in addition to direct execution tests.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human annotators created dataset, refactored code, verified test suites, and performed some human evaluation of reasoning-graph scoring on a subset.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (software / NLP reproducibility).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Dataset annotations include dependency/API documentation and reasoning graph nodes to help agents; authors also supply missing info as an experimental condition to show impact on execution accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Benchmark measurements show high reasoning graph accuracy but low execution accuracy across models, indicating that comprehension (generation of algorithmic reasoning) does not reliably translate into executable correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None in-benchmark; supplying missing information reduces the gap for some models (e.g., large improvement for Deepseek-V3 +0.25), showing that when underspecification is removed validation can improve.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2061.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2061.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluated large language models (GPT-4o-mini, GPT-4o, Claude-Sonnet-3.7, Gemini-2.0-Flash, Deepseek-V3, O3-mini variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of both non-reasoning and reasoning LLMs used as base models for the Paper Agent and Code Agent; compared on algorithm comprehension, code generation, and tool usage behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Various LLMs (GPT-4o-mini, GPT-4o, Claude-Sonnet-3.7, Gemini-2.0-Flash, Deepseek-V3, O3-mini family)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language models (some configured as reasoning models)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / code generation / scientific reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>natural language reasoning steps (comments), code snippets (function implementations), literature reports, repository queries/actions</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>primarily recombinative within training distribution; tasked to synthesize code for recent papers (attempting near-novel recombination of learned code + retrieved repo content)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Transformer-based language modelling with retrieval/tool-augmented actions (SearchPaper, SearchFile, SearchCode, SearchWeb, Compiler) orchestrated by agents; reasoning variants produce internal chain-of-thought-like outputs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Generated outputs validated by insertion into repositories and execution against test suites (execution accuracy), CodeBLEU similarity to references, reasoning graph accuracy via GPT-4o matching and human validation on subset, dependency/API recall.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Algorithm comprehension (reasoning graph accuracy) average ~0.716 without agents; with agents modest improvement (+0.037). Non-reasoning models exhibited larger gains from Sci-Reproducer than reasoning models. Generation/tool invocation patterns differ: non-reasoning LLMs used repository/paper tools much more (e.g., SearchFile avg ~210.4) vs reasoning models (avg ~25.0).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Execution accuracy averaged 0.235 across models with Sci-Reproducer; best model Claude-Sonnet-3.7 achieved 0.390 exec acc. CodeBLEU average 0.320. Recall metrics with Sci-Reproducer (Claude-Sonnet-3.7): intra-file 0.776, cross-file 0.636, API 0.626.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not explicitly stratified by novelty, but authors show that missing or mismatched paper details (higher implicit novelty/underspecification) reduce execution accuracy; supplying missing info improves performance for some models (e.g., Deepseek-V3 +0.25).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper reports that LLMs' internal algorithmic reasoning (high RG Acc) exceeds their practical implementation success (low exec acc), particularly when papers omit implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported for model-generated outputs; node-matching evaluation used deterministic LLM settings to reduce evaluator randomness, but models did not provide calibrated uncertainty for generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported explicitly; benchmark chosen to minimize leakage (in-distribution-ish).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — CodeBLEU and recall are used as proxies to assess implementation faithfulness beyond execution tests.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human annotators were used for dataset creation and for a human evaluation subset (20 tasks) of reasoning graph accuracy; broader human validation recommended but not applied across all tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (applied NLP/software engineering).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Agentic decomposition, Code Agent dependency extraction, iterative compile-debug loop, and providing missing/mismatched information in prompts; non-reasoning LLMs benefited more from tool use than reasoning LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Reported metrics: RG Acc ~0.716 vs Exec Acc avg 0.235; reasoning models invoked external tools far less and relied on internal deliberation ('overthinking'), leading to fewer improvements from agentic tooling compared to non-reasoning models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Supplying missing implementation details closes the gap somewhat for some models (e.g., Deepseek-V3 exec acc increase of +0.25), indicating the gap is partly due to underspecification rather than pure incapacity.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported; paper does note reasoning LLMs invoke Compiler more frequently (implying more compile-debug cycles) but no quantitative cost ratio provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2061.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2061.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Paper Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paper Agent (Sci-Reproducer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent that selectively retrieves and summarizes relevant content from the LaTeX source and literature context to fill missing algorithmic details for code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Paper Agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval-augmented LLM agent</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>literature understanding / information extraction for code synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>literature reports, extracted section content, missing-detail summaries</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>not a generator of novel scientific claims; extracts and synthesizes existing literature content (in-distribution extraction and synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Retrieval-augmented selective parsing of LaTeX and cited literature using SearchPaper, SearchSection, SearchLiterature tools following a RAG-like strategy and ReAct agent approach</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Outputs feed into Code Agent and are indirectly validated by downstream execution accuracy and reasoning graph matching; Paper Agent's contribution to understanding measured by change in reasoning graph accuracy (+0.009 average when used alone).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Small positive impact on reasoning graph accuracy (+0.009 avg) and modest impact on execution accuracy when combined with Code Agent. Alone yields high syntactic-content extraction but cannot fix implementation issues.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Indirect — improves reasoning graph accuracy slightly; direct functional validation depends on downstream Code Agent and execution tests.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Paper Agent helps fill missing hyperparameters and literature-linked details; improvement is larger when missing/mismatched information resides in literature. Exact stratified numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper Agent increases algorithm comprehension but has limited direct effect on executable success without Code Agent and repository context.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Its effectiveness is proxied by delta in reasoning graph accuracy and downstream execution accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Used in dataset annotation and authors recommend human oversight for comprehensive evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical/document retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Provides missing algorithmic details from literature to reduce underspecification.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Paper Agent improves reasoning graph accuracy only modestly (+0.009) and cannot by itself close the generation-validation gap, indicating literature extraction alone is insufficient for executable correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>When missing information is present in literature, Paper Agent can help (but quantitative evidence limited).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2061.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2061.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Code Agent (Sci-Reproducer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Repository-aware agent that searches code items/files, retrieves dependencies/APIs, inserts generated functions into codebase, and invokes a compiler to test and iteratively debug generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Code Agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>tool-augmented LLM agent with repository inspection and compiler feedback</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>software engineering / code generation for research reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>implemented function/method code integrated into target repository</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>implementation synthesis (not proposing new science) — recombines repository knowledge and algorithm descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>SearchCode/SearchFile for dependencies, SearchWeb if needed, then program synthesis by LLM; iterative compile-run-debug cycle using Compiler tool to fix syntax/runtime issues</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Uses Compiler feedback to run tests locally and iteratively debug; final validation via benchmark execution suites (execution accuracy) and CodeBLEU; also contributes to dependency/API recall metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Substantial practical effect: insertion of Code Agent reduced syntax error rates from ~80% (NoAgent) to ~29.4% (Code Agent) and to ~24.9% when combined (Sci-Reproducer). Led to recall gains (avg increases: intra-file +0.441, cross-file +0.239, API +0.100).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Improves execution accuracy when present; combined agents yield average execution accuracy increase of 0.181 over NoAgent baseline, and CodeBLEU increase of 0.057 on average.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Most beneficial when missing/mismatched repository dependency information is a primary cause of failure; precise stratified numbers not given but recall improvements indicate better handling of repository-specific (low-novelty) implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Code Agent closes some of the gap by improving syntactic correctness and dependency resolution, but execution accuracy still remains substantially lower than reasoning comprehension metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Directly leverages compiler feedback and execution tests rather than only proxy metrics; also improves recall metrics used as proxies for implementation fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human annotators reviewed generated annotations and final dataset; Code Agent reduces but does not eliminate need for human debugging in complex/underspecified tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / software engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Repository-aware search for dependencies, iterative compilation and debugging, and supplying implementation patterns from repository context; empirically reduced syntax errors and improved recall/execution metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Even with Code Agent, average execution accuracy is only 0.235, and the best model reaches 0.390, showing implementation/validation remains an open challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Code Agent produced large reductions in syntax errors and large recall improvements, demonstrating that tool augmentation can substantially reduce the gap in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported; authors note Code Agent increases compiler invocations (and debugging iterations) especially for reasoning models, but no numeric cost ratio is given.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Paper2CodeBench <em>(Rating: 2)</em></li>
                <li>PaperBench: Evaluating AI's Ability to Replicate AI Research <em>(Rating: 2)</em></li>
                <li>The AI Scientist <em>(Rating: 2)</em></li>
                <li>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 1)</em></li>
                <li>Agent laboratory: Using llm agents as research assistants <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2061",
    "paper_id": "paper-277467991",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "Sci-Reproducer",
            "name_full": "Sci-Reproducer dual-agent framework",
            "brief_description": "A dual-agent system (Paper Agent + Code Agent) that coordinates literature retrieval and repository-aware code synthesis to reproduce algorithms described in research papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Sci-Reproducer",
            "system_type": "agentic system composed of large language model agents (dual-agent architecture)",
            "scientific_domain": "computational reproducibility / NLP algorithm implementation",
            "output_type": "executable code implementing algorithms described in papers (function/method implementations)",
            "novelty_level": "in-distribution to moderately novel (reproducing recent 2024 papers; aims to synthesize implementations from descriptions rather than invent new algorithms)",
            "generation_method": "LLM-based reasoning + retrieval-augmented generation via Paper Agent (selective literature extraction) and Code Agent (repository search, API/dependency retrieval, iterative compile-debug loop)",
            "validation_method": "execution-based validation (integrate generated code into repository and run verification suite/test cases), CodeBLEU similarity to reference, reasoning graph similarity (reasoning graph accuracy), and recall metrics for dependency/API identification",
            "generation_performance": "Generation (algorithm comprehension) measured by reasoning graph accuracy: average ~0.716 (without agents); small +0.037 gain with both agents. Generated code quality (synthesis) measured by CodeBLEU: average 0.320.",
            "validation_performance": "Execution accuracy (strict test-suite pass): average 0.235 across models with Sci-Reproducer; best model (Claude-Sonnet-3.7 with Sci-Reproducer) achieved 0.390 execution accuracy. Dependency/API recall improved markedly with Code Agent (avg increases: intra-file +0.441, cross-file +0.239, API +0.100).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Paper shows a clear drop in validation (execution accuracy) relative to generation-level understanding as tasks become less concretely specified; missing/mismatched paper details (novel or underspecified cases) reduced execution accuracy substantially. Quantitative per-novelty curves are not provided.",
            "generation_validation_comparison": "Yes — explicit asymmetry: high algorithmic-comprehension scores (reasoning graph accuracy ≈0.716) but much lower successful execution (execution accuracy average 0.235, best 0.390), demonstrating generation (understanding/specification) outpacing validation (executable correctness).",
            "uncertainty_quantification": "Not reported as part of generation outputs. Deterministic settings (temperature=0, top-p=1) were used for GPT-4o in node-matching evaluation to reduce evaluator randomness, but the generation agents themselves are not reported to produce calibrated uncertainty estimates.",
            "calibration_quality": "Not reported / no calibration metrics provided for agents' confidence in their generated code or reasoning.",
            "out_of_distribution_performance": "Not directly reported. Dataset selection intentionally focused on 2024 papers to avoid data leakage; no explicit OOD benchmark results are provided.",
            "validation_proxy_metrics": "Yes — proxy metrics include CodeBLEU (syntactic/semantic similarity) and dependency/API recall; reasoning graph accuracy is used as a proxy for correct algorithmic understanding rather than direct functional correctness.",
            "human_validation_required": true,
            "human_validation_frequency": "Used for dataset annotation and for validating the LLM-based reasoning-graph evaluator on a subset (20 tasks) — not applied across the full benchmark; authors recommend further comprehensive human evaluation.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (applied NLP / software code reproduction); not highly formal like mathematics — domain empirical nature contributes to generation-validation gap.",
            "gap_mitigation_strategies": "Dual-agent decomposition (Paper Agent + Code Agent), repository dependency/API recall, iterative compile-debug via Compiler tool, supplying missing/mismatched information as extra prompt context; these strategies yielded measurable improvements (e.g., Code Agent reduced syntax error rate from ~80% to ~29%, improved recalls and execution accuracy increments).",
            "evidence_supporting_gap": "Quantitative: reasoning graph accuracy averages ≈0.716 while execution accuracy averages 0.235; best execution accuracy 0.390. Syntax error rates fall from ~80% without Code Agent to ~24.9-29.4% with Code Agent, indicating implementation (validation) is the bottleneck. Authors explicitly state LLMs 'can comprehend algorithm logic' but 'performance in code generation remains suboptimal.'",
            "evidence_contradicting_gap": "None reported in the paper; authors report some modest improvements from agents but no evidence that validation capability exceeds generation capability.",
            "computational_cost_ratio": "Not reported — paper gives no quantitative comparison of compute/time cost of validation (test execution) vs generation; iterative compilation/debugging counts are reported qualitatively (e.g., reasoning models call Compiler more frequently) but not converted to cost ratios.",
            "uuid": "e2061.0"
        },
        {
            "name_short": "SciReplicate-Bench",
            "name_full": "SciReplicate-Bench benchmark",
            "brief_description": "A curated benchmark of 100 repository-level algorithm reproduction tasks from 36 NLP papers (2024), with reference implementations, reasoning-graph annotations, dependency annotations, and verification suites for execution-based evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SciReplicate-Bench",
            "system_type": "benchmark / evaluation dataset",
            "scientific_domain": "computational reproducibility / NLP",
            "output_type": "reference implementations and verification suites; used to validate generated code outputs",
            "novelty_level": "in-distribution relative to target papers (tasks drawn from contemporary 2024 literature); some tasks include missing/mismatched descriptions to simulate realistic challenges",
            "generation_method": "Not a generator — provides inputs (LaTeX algorithm descriptions, function signatures, repository context) for LLMs/agents to generate code",
            "validation_method": "Execution-based verification suites (≈10 test cases per task where possible), comparison scripts, CodeBLEU, reasoning graph accuracy, and dependency/API recall metrics",
            "generation_performance": "N/A (benchmark does not generate outputs).",
            "validation_performance": "Enables measurement of execution accuracy (strict pass if all test cases match reference), CodeBLEU, and reasoning graph accuracy; used to report average execution accuracy (0.235) and CodeBLEU (0.320) across models.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Benchmark includes annotations for missing/mismatched information and reports that supplying missing information improves execution accuracy (examples: GPT-4o-mini +0.05, Deepseek-V3 +0.25, O3-mini-low +0.04).",
            "generation_validation_comparison": "Benchmark explicitly designed to expose a generation-validation gap by separating algorithm comprehension (reasoning graph) from executable correctness (execution accuracy).",
            "uncertainty_quantification": "Not applicable for the benchmark itself (it provides deterministic tests and disables randomness in annotated repositories).",
            "calibration_quality": "Not applicable.",
            "out_of_distribution_performance": "Not reported; benchmark curated from recent in-domain papers.",
            "validation_proxy_metrics": "Yes — CodeBLEU and reasoning graph accuracy are used as proxies for code quality and algorithm understanding in addition to direct execution tests.",
            "human_validation_required": true,
            "human_validation_frequency": "Human annotators created dataset, refactored code, verified test suites, and performed some human evaluation of reasoning-graph scoring on a subset.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (software / NLP reproducibility).",
            "gap_mitigation_strategies": "Dataset annotations include dependency/API documentation and reasoning graph nodes to help agents; authors also supply missing info as an experimental condition to show impact on execution accuracy.",
            "evidence_supporting_gap": "Benchmark measurements show high reasoning graph accuracy but low execution accuracy across models, indicating that comprehension (generation of algorithmic reasoning) does not reliably translate into executable correctness.",
            "evidence_contradicting_gap": "None in-benchmark; supplying missing information reduces the gap for some models (e.g., large improvement for Deepseek-V3 +0.25), showing that when underspecification is removed validation can improve.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2061.1"
        },
        {
            "name_short": "LLMs (evaluated)",
            "name_full": "Evaluated large language models (GPT-4o-mini, GPT-4o, Claude-Sonnet-3.7, Gemini-2.0-Flash, Deepseek-V3, O3-mini variants)",
            "brief_description": "A set of both non-reasoning and reasoning LLMs used as base models for the Paper Agent and Code Agent; compared on algorithm comprehension, code generation, and tool usage behaviors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Various LLMs (GPT-4o-mini, GPT-4o, Claude-Sonnet-3.7, Gemini-2.0-Flash, Deepseek-V3, O3-mini family)",
            "system_type": "large language models (some configured as reasoning models)",
            "scientific_domain": "NLP / code generation / scientific reproducibility",
            "output_type": "natural language reasoning steps (comments), code snippets (function implementations), literature reports, repository queries/actions",
            "novelty_level": "primarily recombinative within training distribution; tasked to synthesize code for recent papers (attempting near-novel recombination of learned code + retrieved repo content)",
            "generation_method": "Transformer-based language modelling with retrieval/tool-augmented actions (SearchPaper, SearchFile, SearchCode, SearchWeb, Compiler) orchestrated by agents; reasoning variants produce internal chain-of-thought-like outputs",
            "validation_method": "Generated outputs validated by insertion into repositories and execution against test suites (execution accuracy), CodeBLEU similarity to references, reasoning graph accuracy via GPT-4o matching and human validation on subset, dependency/API recall.",
            "generation_performance": "Algorithm comprehension (reasoning graph accuracy) average ~0.716 without agents; with agents modest improvement (+0.037). Non-reasoning models exhibited larger gains from Sci-Reproducer than reasoning models. Generation/tool invocation patterns differ: non-reasoning LLMs used repository/paper tools much more (e.g., SearchFile avg ~210.4) vs reasoning models (avg ~25.0).",
            "validation_performance": "Execution accuracy averaged 0.235 across models with Sci-Reproducer; best model Claude-Sonnet-3.7 achieved 0.390 exec acc. CodeBLEU average 0.320. Recall metrics with Sci-Reproducer (Claude-Sonnet-3.7): intra-file 0.776, cross-file 0.636, API 0.626.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not explicitly stratified by novelty, but authors show that missing or mismatched paper details (higher implicit novelty/underspecification) reduce execution accuracy; supplying missing info improves performance for some models (e.g., Deepseek-V3 +0.25).",
            "generation_validation_comparison": "Paper reports that LLMs' internal algorithmic reasoning (high RG Acc) exceeds their practical implementation success (low exec acc), particularly when papers omit implementation details.",
            "uncertainty_quantification": "Not reported for model-generated outputs; node-matching evaluation used deterministic LLM settings to reduce evaluator randomness, but models did not provide calibrated uncertainty for generated code.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not reported explicitly; benchmark chosen to minimize leakage (in-distribution-ish).",
            "validation_proxy_metrics": "Yes — CodeBLEU and recall are used as proxies to assess implementation faithfulness beyond execution tests.",
            "human_validation_required": true,
            "human_validation_frequency": "Human annotators were used for dataset creation and for a human evaluation subset (20 tasks) of reasoning graph accuracy; broader human validation recommended but not applied across all tasks.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (applied NLP/software engineering).",
            "gap_mitigation_strategies": "Agentic decomposition, Code Agent dependency extraction, iterative compile-debug loop, and providing missing/mismatched information in prompts; non-reasoning LLMs benefited more from tool use than reasoning LLMs.",
            "evidence_supporting_gap": "Reported metrics: RG Acc ~0.716 vs Exec Acc avg 0.235; reasoning models invoked external tools far less and relied on internal deliberation ('overthinking'), leading to fewer improvements from agentic tooling compared to non-reasoning models.",
            "evidence_contradicting_gap": "Supplying missing implementation details closes the gap somewhat for some models (e.g., Deepseek-V3 exec acc increase of +0.25), indicating the gap is partly due to underspecification rather than pure incapacity.",
            "computational_cost_ratio": "Not reported; paper does note reasoning LLMs invoke Compiler more frequently (implying more compile-debug cycles) but no quantitative cost ratio provided.",
            "uuid": "e2061.2"
        },
        {
            "name_short": "Paper Agent",
            "name_full": "Paper Agent (Sci-Reproducer)",
            "brief_description": "Agent that selectively retrieves and summarizes relevant content from the LaTeX source and literature context to fill missing algorithmic details for code generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Paper Agent",
            "system_type": "retrieval-augmented LLM agent",
            "scientific_domain": "literature understanding / information extraction for code synthesis",
            "output_type": "literature reports, extracted section content, missing-detail summaries",
            "novelty_level": "not a generator of novel scientific claims; extracts and synthesizes existing literature content (in-distribution extraction and synthesis)",
            "generation_method": "Retrieval-augmented selective parsing of LaTeX and cited literature using SearchPaper, SearchSection, SearchLiterature tools following a RAG-like strategy and ReAct agent approach",
            "validation_method": "Outputs feed into Code Agent and are indirectly validated by downstream execution accuracy and reasoning graph matching; Paper Agent's contribution to understanding measured by change in reasoning graph accuracy (+0.009 average when used alone).",
            "generation_performance": "Small positive impact on reasoning graph accuracy (+0.009 avg) and modest impact on execution accuracy when combined with Code Agent. Alone yields high syntactic-content extraction but cannot fix implementation issues.",
            "validation_performance": "Indirect — improves reasoning graph accuracy slightly; direct functional validation depends on downstream Code Agent and execution tests.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Paper Agent helps fill missing hyperparameters and literature-linked details; improvement is larger when missing/mismatched information resides in literature. Exact stratified numbers not provided.",
            "generation_validation_comparison": "Paper Agent increases algorithm comprehension but has limited direct effect on executable success without Code Agent and repository context.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not reported.",
            "validation_proxy_metrics": "Its effectiveness is proxied by delta in reasoning graph accuracy and downstream execution accuracy.",
            "human_validation_required": true,
            "human_validation_frequency": "Used in dataset annotation and authors recommend human oversight for comprehensive evaluation.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical/document retrieval.",
            "gap_mitigation_strategies": "Provides missing algorithmic details from literature to reduce underspecification.",
            "evidence_supporting_gap": "Paper Agent improves reasoning graph accuracy only modestly (+0.009) and cannot by itself close the generation-validation gap, indicating literature extraction alone is insufficient for executable correctness.",
            "evidence_contradicting_gap": "When missing information is present in literature, Paper Agent can help (but quantitative evidence limited).",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2061.3"
        },
        {
            "name_short": "Code Agent",
            "name_full": "Code Agent (Sci-Reproducer)",
            "brief_description": "Repository-aware agent that searches code items/files, retrieves dependencies/APIs, inserts generated functions into codebase, and invokes a compiler to test and iteratively debug generated code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Code Agent",
            "system_type": "tool-augmented LLM agent with repository inspection and compiler feedback",
            "scientific_domain": "software engineering / code generation for research reproducibility",
            "output_type": "implemented function/method code integrated into target repository",
            "novelty_level": "implementation synthesis (not proposing new science) — recombines repository knowledge and algorithm descriptions",
            "generation_method": "SearchCode/SearchFile for dependencies, SearchWeb if needed, then program synthesis by LLM; iterative compile-run-debug cycle using Compiler tool to fix syntax/runtime issues",
            "validation_method": "Uses Compiler feedback to run tests locally and iteratively debug; final validation via benchmark execution suites (execution accuracy) and CodeBLEU; also contributes to dependency/API recall metrics.",
            "generation_performance": "Substantial practical effect: insertion of Code Agent reduced syntax error rates from ~80% (NoAgent) to ~29.4% (Code Agent) and to ~24.9% when combined (Sci-Reproducer). Led to recall gains (avg increases: intra-file +0.441, cross-file +0.239, API +0.100).",
            "validation_performance": "Improves execution accuracy when present; combined agents yield average execution accuracy increase of 0.181 over NoAgent baseline, and CodeBLEU increase of 0.057 on average.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Most beneficial when missing/mismatched repository dependency information is a primary cause of failure; precise stratified numbers not given but recall improvements indicate better handling of repository-specific (low-novelty) implementation details.",
            "generation_validation_comparison": "Code Agent closes some of the gap by improving syntactic correctness and dependency resolution, but execution accuracy still remains substantially lower than reasoning comprehension metrics.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not reported.",
            "validation_proxy_metrics": "Directly leverages compiler feedback and execution tests rather than only proxy metrics; also improves recall metrics used as proxies for implementation fidelity.",
            "human_validation_required": true,
            "human_validation_frequency": "Human annotators reviewed generated annotations and final dataset; Code Agent reduces but does not eliminate need for human debugging in complex/underspecified tasks.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / software engineering.",
            "gap_mitigation_strategies": "Repository-aware search for dependencies, iterative compilation and debugging, and supplying implementation patterns from repository context; empirically reduced syntax errors and improved recall/execution metrics.",
            "evidence_supporting_gap": "Even with Code Agent, average execution accuracy is only 0.235, and the best model reaches 0.390, showing implementation/validation remains an open challenge.",
            "evidence_contradicting_gap": "Code Agent produced large reductions in syntax errors and large recall improvements, demonstrating that tool augmentation can substantially reduce the gap in many cases.",
            "computational_cost_ratio": "Not reported; authors note Code Agent increases compiler invocations (and debugging iterations) especially for reasoning models, but no numeric cost ratio is given.",
            "uuid": "e2061.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Paper2CodeBench",
            "rating": 2
        },
        {
            "paper_title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
            "rating": 2
        },
        {
            "paper_title": "The AI Scientist",
            "rating": 2
        },
        {
            "paper_title": "Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 1
        },
        {
            "paper_title": "Agent laboratory: Using llm agents as research assistants",
            "rating": 1
        }
    ],
    "cost": 0.016332,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers
7 Aug 2025</p>
<p>Yanzheng Xiang yanzheng.xiang@kcl.ac.uk 
King's College London</p>
<p>Hanqi Yan hanqi.yan@kcl.ac.uk 
King's College London</p>
<p>Shuyin Ouyang shuyin.ouyang@kcl.ac.uk 
King's College London</p>
<p>Lin Gui lin.1.gui@kcl.ac.uk 
King's College London</p>
<p>Yulan He yulan.he@kcl.ac.uk 
King's College London</p>
<p>The Alan Turing Institute</p>
<p>SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers
7 Aug 202531A2EB758D35837D1F67695B1219E75CarXiv:2504.00255v2[cs.CL]
This study evaluates large language models (LLMs) in generating code from algorithm descriptions in recent NLP papers.The task requires two key competencies: (1) algorithm comprehension: synthesizing information from papers and academic literature to understand implementation logic, and (2) coding expertise: identifying dependencies and correctly implementing necessary APIs.To facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark of 100 tasks from 36 NLP papers published in 2024, featuring detailed annotations and comprehensive test cases.Building on SciReplicate-Bench, we propose Sci-Reproducer, a dualagent framework consisting of a Paper Agent that interprets algorithmic concepts from literature and a Code Agent that retrieves dependencies from repositories and implements solutions.To assess algorithm understanding, we introduce reasoning graph accuracy, which quantifies similarity between generated and reference reasoning graphs derived from code comments and structure.For evaluating implementation quality, we employ execution accuracy, CodeBLEU, and repository dependency/API recall metrics.In our experiments, we evaluate various powerful non-reasoning and reasoning LLMs as foundational models.The best-performing LLM using Sci-Reproducer achieves only 39% execution accuracy, highlighting the benchmark's difficulty.Our analysis identifies missing or inconsistent algorithm descriptions as key barriers to successful reproduction.We make available our benchmark and code at GitHub and project homepage at Homepage.</p>
<p>Introduction</p>
<p>The evolution of Large Language Models (LLMs) has ushered in a transformative era in scientific discovery, positioning them as powerful tools for streamlining research (Gridach et al., 2025;Buehler, 2024;Lu et al., 2024), from idea generation to verification and publication writing.For instance, Si et al. (2025); Gu &amp; Krenn (2025) demonstrated how LLMs can be prompted to generate novel research ideas, while Yuan et al. (2022); Du et al. (2024) explored their use in producing literature reviews for idea evaluation.Additionally, LLMs are increasingly integrated into tools like Semantic Scholar1 , Research Rabbit2 , and Undermind Research Assistant3 , enhancing literature discovery, citation analysis, and knowledge synthesis.These advancements, both in research methodologies and practical applications, suggest that LLMs have the potential to assist across multiple stages of scientific discovery.</p>
<p>Among the aforementioned advancements in research acceleration, the ability of LLMs to correctly generate code for validating real-world scientific ideas is particularly noteworthy.Computational validation is crucial across many fields, yet researchers often face barriers due to limited coding expertise or inaccessible implementations.By converting scientific Table 1: Comparisons of different machine learning software engineering benchmarks.</p>
<p>To address this gap, we developed SciReplicate-Bench, the first benchmark specifically designed to evaluate LLMs' capabilities in code generation for reproducing research findings from academic papers.It consists of 100 code reproduction tasks derived from 36 papers published in leading conferences in 2024.This recent publication window was deliberately chosen to minimize the risk of data leakage.An overview of the task is illustrated in Figure 1, with a concrete example provided in Figure A2 in Appendix E. The task consists of two main steps: 1. Algorithm understanding.LLMs must extract essential information from the paper, such as workflow details, algorithm descriptions, and hyperparameter values.2. Code implementation.LLMs then implement a function or method within a provided repository, using both the extracted information and the LaTeX representation of the algorithm from the paper.We introduce Sci-Reproducer, a dual-agent system that combines a Paper Agent and a Code Agent to handle these two steps collaboratively and implement code for the target algorithm.</p>
<p>To rigorously assess LLM performance on this benchmark, we evaluate two dimensions corresponding to the aforementioned two steps: algorithm comprehension correctness and code correctness.To evaluate algorithm comprehension, we introduce a reasoning graph to represent the reasoning logic behind algorithm reproduction.Each node in the graph represents a code comment, which reflects a single reasoning step and is aligned with a specific segment of code.Edges between nodes are defined based on data flow relationships across different code segments.We compute the similarity between the generated reasoning graph and a reference graph to derive the reasoning graph accuracy.To evaluate code correctness, we employ established metrics including execution accuracy (Rajkumar et al., 2022;Xiang et al., 2023), CodeBLEU (Ren et al., 2020), and recall of intra/cross-file dependencies and APIs.</p>
<p>Our work makes the following contributions:</p>
<p>Benchmarks: SciReplicate-Bench, a benchmark of 100 algorithm reproduction tasks from recent NLP publications.</p>
<p>Metric:</p>
<p>We propose a novel reasoning graph accuracy metric for evaluating algorithmic comprehension.Approach: Sci-Reproducer, a dual-agent framework combining paper understanding and code implementation.Insights: Comprehensive evaluation across state-of-the-art LLMs reveals four key findings: (i) the task remains highly challenging, with execution accuracy below 40% for all models; (ii) reasoning models exhibit "overthinking" behavior (Cuadron et al., 2025;Sui et al., 2025), over-relying on internal reasoning rather than utilizing available tools for information extraction; (iii) while LLMs demonstrate strong algorithmic comprehension, they struggle with practical implementation; and (iv) algorithm reproduction failures often stem from incomplete paper descriptions, which our Sci-Reproducer effectively addresses.</p>
<p>Related Work</p>
<p>Our work lies at the intersection of AI for automating scientific discovery and LLM-based code generation.</p>
<p>AI for Automating Scientific Discovery</p>
<p>The application of LLMs to accelerate scientific research has emerged as a rapidly growing field with diverse approaches.Several studies have demonstrated the potential for comprehensive research automation through end-to-end AI systems.Schmidgall et al. (2025); Lu et al. (2024) developed frameworks that integrate idea generation, experimental validation, and manuscript composition, with some AI-authored papers successfully passing workshop review processes (Yamada et al., 2025).Complementary research has focused on the creative aspects of scientific inquiry, with Wang et al. (2023); Ghafarollahi &amp; Buehler (2024); O'Neill et al. (2025) investigating LLMs' capacity for generating novel research hypotheses.Notably, recent evaluations by Gu et al. (2024); Kumar et al. (2024); Liu et al. (2025); Si et al. (2024) suggest that AI-generated research concepts may occasionally exceed human-generated ideas in terms of novelty and originality.Within computational disciplines where implementation validation is essential, LLMs have shown promise in algorithm design and code development tasks.Our proposed SciReplicate-Bench addresses a previously underexplored area: the automated reproduction of algorithms directly from academic publications.This represents a unique challenge at the intersection of scientific literature comprehension and executable code synthesis.While recent parallel efforts such as PaperBench (Starace et al., 2025) and Paper2CodeBench (Seo et al., 2025) have tackled related problems by exploring full codebase reconstruction, these evaluation approaches rely substantially on manual assessment criteria and LLM-based correctness judgments, introducing potential inconsistencies and reliability concerns.Our approach prioritizes objective evaluation through execution accuracy, providing more rigorous validation than non-executable assessment methodologies (Wang et al., 2022;Chen et al., 2021).</p>
<p>The broader ecosystem of computational reproducibility research includes specialized frameworks such as MLGym (Nathani et al., 2025) for baseline improvement, and evaluation benchmarks developed by Siegel et al. (2024);Ren et al. (2023) that assess LLMs' ability to reproduce published experimental results using existing codebases.</p>
<p>LLMs for Code Generation</p>
<p>Code generation has emerged as a prominent application of LLMs, with benchmarks ranging from basic programming tasks (Chen et al., 2021;Jain et al., 2024;Austin et al., 2021;Hendrycks et al., 2021;Liu et al., 2022) to realistic software engineering challenges like SWE-bench (Jimenez et al., 2023), which uses actual repository pull requests.However, these benchmarks primarily target general software engineering rather than scientific algorithm reproduction.</p>
<p>Recent efforts have developed machine learning-specific benchmarks (Liu et al., 2023;Huang et al., 2023;Chan et al., 2024), but these typically involve implementing algorithms proposed by the models themselves or solving relatively straightforward tasks.They lack the depth of algorithmic understanding and rigorous paper analysis required for reproducing algorithms from peer-reviewed publications.</p>
<p>Despite advances in tool-augmented code generation (Schick et al., 2023;Zhang et al., 2024a;a;2023b), no existing system specifically addresses the unique challenge of translating academic papers into executable code.Our Sci-Reproducer framework demonstrates the ability to comprehend academic publications and convert abstract algorithm descriptions into functional implementations.</p>
<p>SciReplicate-Bench</p>
<p>Overview SciReplicate-Bench is designed to evaluate LLMs' ability to reproduce algorithms from academic papers, consisting of 100 tasks curated from 36 recent NLP publications with their corresponding open-source implementations.The task categories are detailed in Figure A1 in Appendix B. The benchmark focuses on repository-level code generation, where each task is centered around implementing a specific function or class method.As illustrated in Figure A3 in Appendix E, each task comprises nine components, which can be categorized into three groups corresponding to code generation, evaluation, and analysis, respectively.</p>
<p>For code generation, the following components are provided as inputs to LLMs:</p>
<p>Function signature: the definition of the target function, including detailed descriptions of its input and output variables.</p>
<p>Algorithm Description: The LaTeX code description of the target algorithm, typically located within a subsection or paragraph of the target paper.</p>
<p>Literature context: the original paper along with its cited references, providing broader conceptual context.</p>
<p>Repository context: all source files and code in the repository that inform or support the target implementation.</p>
<p>For evaluation, the following components are provided for code execution and metrics calculation:</p>
<p>Reference implementation: ground-truth code serving as the reference for CodeBLEU evaluation.</p>
<p>Reasoning graph annotations: structured representations of the algorithmic logic and implementation flow, enabling assessment of reasoning graph accuracy.Dependency annotations: comprehensive documentation of internal dependencies, crossfile relationships, and external API usage for computing recall metrics across all dependency categories.</p>
<p>Test environment: isolated Python execution environment containing validation cases and automated verification scripts for assessing implementation correctness.</p>
<p>To enable further analysis of the underlying causes of LLM failures, the benchmark includes: Missing/Mismatch Information: the LaTeX description of the algorithm may omit certain implementation details, which could either appear elsewhere in the paper or be entirely absent.We also annotate mismatches between the paper description and the reference implementation.</p>
<p>Task Definition Based on SciReplicate-Bench, an LLM is given the algorithm description, function signature, literature context, and repository context as input.The LLM is asked to output a function that implements the target algorithm.</p>
<p>Benchmark Construction</p>
<p>The benchmark construction process comprises four key steps: paper selection, Python environment setup, documentation, and verification suite preparation.To mitigate the risk of data leakage, we selected papers published in 2024 that provide publicly available code repositories.During the annotation process, each repository was refactored to isolate the core algorithm into a standalone function, and all sources of randomness were removed to ensure reproducibility and prevent leakage.On average, annotating each paper requires approximately 12 hours.Details of the annotation process are provided in Appendix A.</p>
<p>Evaluation Metrics</p>
<p>Evaluating Algorithm Comprehension</p>
<p>We propose the reasoning graph accuracy metric to evaluate how well LLMs understand the logic and implementation of algorithms.During code generation, LLMs are prompted to insert specially formatted, non-overlapping, non-nested comments that mark reasoning steps derived from the algorithm's LaTeX code (The prompt can be found in Figure A5).</p>
<p>We then construct a reasoning graph G = {N, E} (illustrated in Figure A3), modeled as a Directed Acyclic Graph (DAG).Each node n i = ⟨w i , c i ⟩, n i ∈ N represents a reasoning step with a comment w i and corresponding code snippet c i .An edge e i = ⟨n i , n j ⟩, e i ∈ E is added if a variable used in c j is defined or last modified in c i .To compute the reasoning graph accuracy, we compare the generated graph G g with the reference graph G r via node and edge matching:</p>
<p>Node matching: comments from G r and G g are passed to GPT-4o, which maps each reference node to one or more nodes in the generated graph.A node in G r is considered matched if it has at least one corresponding node in G g .The prompt template used for this process is available in Figure A4.</p>
<p>Edge matching: for each reference edge e r = ⟨n i r , n j r ⟩, if both endpoint nodes have corresponding matches in G g , we apply Breadth-First Search(BFS) to verify whether a corresponding edge exists in G g .</p>
<p>The reasoning graph accuracy S r is computed as:
S r = n i ∈N m ∑ n i s n i + e j ∈E m ∑ e j s e j . (1)
where N m and E m denote the sets of matched nodes and edges, respectively, and s n i and s e j represent their corresponding significance scores.Node significance is determined by the complexity of its corresponding code segment, measured by the number of variable definitions and usages, function calls, arithmetic operations, and lines of code, then normalized across the reference graph.Edge significance is calculated as the product of the significance scores of its connected nodes, followed by normalization.</p>
<p>Evaluating Code Generation</p>
<p>For assessing coding ability, we use the following evaluation metrics:</p>
<p>• Execution accuracy (Xiang et al., 2023;Zhang et al., 2024b;Long et al., 2022): we integrate the generated code into the repository and execute it to obtain results.If all test cases match the reference results, we consider the code correct.</p>
<p>• CodeBLEU (Ren et al., 2020): this metric evaluates how similar the generated code is to reference code by using the traditional BLEU metric (Papineni et al., 2002) while incorporating syntactic information through abstract syntax trees (AST) and semantic understanding via data-flow graphs (DFG).</p>
<p>• Recall (Li et al., 2024): we calculate recall scores specifically for intra-file dependencies, cross-file dependencies, and external APIs.</p>
<p>SearchSection Section ID</p>
<p>The entire content of a section based on the section label.</p>
<p>SearchLiterature Paper ID, query The answer to the query searched from the literature (identified by Paper ID).</p>
<p>Code Agent SearchCode Name</p>
<p>The definition of a specific code element in repository.</p>
<p>SearchFile Name</p>
<p>The content of a certain file in repository.</p>
<p>SearchWeb Query</p>
<p>The information obtained from the website.</p>
<p>Compiler code</p>
<p>The feedback from the compiler after executing the code.</p>
<p>Sci-Reproducer</p>
<p>To address this task, we introduce Sci-Reproducer4 , a dual-agent framework designed for scientific paper algorithm replication.As illustrated in Figure 1, Sci-Reproducer comprises a Paper Agent and a Code Agent that collaboratively work to replicate algorithms described in a given paper.The predefined actions employed by the agents are summarized in Table 2, with implementation details provided in Appendix C.</p>
<p>Paper Agent</p>
<p>Based on the provided algorithm description, the Paper Agent systematically retrieves contextual information from the literature context to support algorithmic understanding.Due to the input length limitations of LLMs, it is infeasible to input entire paper along with their associated literature.Consequently, the Paper Agent must selectively extract pertinent information, following a strategy akin to Retrieval Augmented Generation (RAG) (Wang et al., 2024b;Sarthi et al., 2024).The Paper Agent incrementally builds an understanding of the target algorithm by executing predefined actions to query the literature context.To facilitate this process, we adopt ReAct (Yao et al., 2022) as the agent strategy, which enables seamless integration of action execution with intermediate reasoning steps.</p>
<p>After the Paper Agent concludes that all necessary information has been collected, it generates a comprehensive literature report comprising key findings that fill in the missing  components of the target algorithm's LaTeX source.An example of the literature report is shown in Figure A8.This report subsequently serves as a crucial input for the Code Agent.</p>
<p>The prompt used to guide the Paper Agent is provided in Figure A6.</p>
<p>Code Agent</p>
<p>Informed by the algorithm description, literature report, and code context, the Code Agent searches the repository to locate essential dependencies required for implementation.It can also browse websites for additional information and use a compiler to test and iteratively debug the code, ensuring proper execution by identifying and fixing syntax errors.The prompt for the Code Agent is provided in Figure A7.</p>
<p>Experiments</p>
<p>We evaluate Sci-Reproducer on the SciReplicate-Bench benchmark using 7 advanced LLMs, including five non-reasoning LLMs: GPT-4o-mini (4o mini, 2024), GPT-4o (GPT-4o, 2024), Claude-Sonnet-3.7 (Claude-Sonnet-3.7,2025), Gemini-2.0-Flash(Gemini-2.0-Flash,2024), and Deepseek-V3 (DeepSeek-AI et al., 2024), and different versions of the reasoning models O3-mini (o3 mini, 2024), i.e., three different levels of reasoning intensity.For the reasoning graph accuracy metric, node matching is performed using GPT-4o, which may introduce some randomness.To reduce this variability, we set the temperature to 0 and top-p to 1, ensuring more deterministic generation.The calculation is repeated three times, and we report the average score as the final result.</p>
<p>Results on SciReplicate-Bench</p>
<p>LLMs face challenges with actual implementation</p>
<p>Although LLMs are capable of understanding algorithms, their performance in code generation remains suboptimal.Despite using Sci-Reproducer, the average execution accuracy remains low at 0.235, with a CodeBLEU score of 0.320.</p>
<p>Accurate dependency and API identification is crucial for code implementation</p>
<p>Effectively recognizing and leveraging dependencies from the source repository and external APIs is essential for accurate code implementation.The integration of Code Agent led to substantial gains in recall with average increases of 0.441, 0.239, and 0.100, respectively, compared to cases without the agent.With Sci-Reproducer, Claude-Sonnet-3.7 attains the highest execution accuracy of 0.390, with the highest recall for intra/cross file dependency and API usage, at 0.776, 0.636, and 0.626 respectively.</p>
<p>Overthinking leads to limited improvement in reasoning LLMs</p>
<p>Reasoning LLMs exhibit more modest performance gains when using Sci-Reproducer.While they achieve an average execution accuracy improvement of 0.13, non-reasoning models demonstrate substantially larger gains of 0.212.This pattern extends to recall metrics, where reasoning LLMs show improvements of 0.243, 0.061, and 0.041 respectively, compared to non-reasoning LLMs' more pronounced gains of 0.560, 0.345, and 0.135.We attribute this performance gap to the "overthinking" phenomenon (Cuadron et al., 2025;Sui et al., 2025), where excessive internal reasoning impedes effective action execution, a limitation that we examine in detail in the following subsection.</p>
<p>Tool Usage Analysis</p>
<p>Figure 2 presents the number of times each LLM invokes actions on the full dataset with Sci-Reproducer.We observe the following: • For code-related actions, reasoning LLMs demonstrate limited tool usage, employing "SearchFile", "SearchCodeItem", and "SearchWeb" only 25.0, 3.3, and 0.0 times on average, respectively.Non-reasoning LLMs use these same actions far more extensively, with averages of 210.4,68.2, and 16.8 times respectively.This disparity reveals a fundamental behavioral difference: reasoning models favor internal deliberation over external information gathering.Conversely, reasoning LLMs invoke "Compiler" more frequently, indicating they require more debugging iterations due to inadequate contextual information gathering.This over-reliance on internal reasoning undermines performance: advanced models like o3-mini-high and o3-mini-low achieve execution accuracy comparable to GPT-4o-mini, negating their theoretical computational advantages.• Paper-related actions exhibit a similar pattern.Reasoning LLMs use "SearchPaper", "SearchSection", and "SearchLiterature" an average of 56.3, 70.0, and 20.0 times respectively, while non-reasoning LLMs demonstrate substantially higher usage at 244.8, 188.4,and 58.0 times respectively.Additionally, we observe a clear preference for target paper extraction over external literature consultation.Actions targeting the primary paper ("SearchPaper" and "SearchSection") are invoked 174.1 and 144 times on average, significantly more than "SearchLiterature" which accesses related works only 43.8 times.</p>
<p>Error Analysis</p>
<p>Syntax Errors</p>
<p>Table A4 shows the syntax error rates for each model across different configurations.Without the Code Agent, syntax errors occurred at rates of 80.3% ("NoAgent") and 83.3% ("Paper Agent").After implementing the Code Agent, these error rates dropped significantly to 29.4% ("Code Agent") and 24.9% ("Sci-Reproducer").The remaining syntax errors mainly result from incorrectly using repository dependencies.This occurs because our approach, unlike human developers, cannot dynamically access runtime information through a compiler during the code generation process.</p>
<p>Logic Errors</p>
<p>Another issue stems from differences in implementation logic, which can be broadly categorized into: (1) discrepancy in algorithm implementation that result in differing outputs, and</p>
<p>(2) missing or mismatch information in the algorithm descriptions in the paper compared to the actual code.</p>
<p>Implementation discrepancy An algorithm may have multiple valid implementation approaches.For example, the cross-entropy loss function can be implemented by directly  invoking the PyTorch API "torch.nn.CrossEntropy" or by manually coding it from scratch.Such implementation choices may introduce subtle differences that lead to variations in the final output of the function.</p>
<p>Missing/Mismatched information in algorithm description Algorithmic descriptions in research papers often lack concrete implementation details, and in certain cases, the provided code may exhibit minor discrepancies compared to the descriptions in the paper.</p>
<p>We manually compared the implementation code of all tasks in the dataset with their descriptions in the papers to identify missing or mismatch information.We then provided this information as additional input and apply Sci-Reproducer framework on three LLMs.The Results is shown in Table 4, regarding to Execution Acc, the performance for GPT-4o-mini, Deepseek-V3 and O3-mini-low improved 0.050, 0.250 and 0.040 respectively.The missing information can be divided into four categories:</p>
<p>• Hyperparameters and configurations: descriptions of target algorithms in papers often omit specific hyperparameter settings, such as the batch size.• Numerical stability techniques: standard techniques for ensuring numerical stability, such as handling division by zero.• Implementation logic: common implementation practices and model design choices, such as data splitting protocols.• Coding strategy: practical programming techniques that enhance implementation efficiency and reliability, such as early stopping criteria.</p>
<p>More examples for each category can be found in Table A5 in Appendix E. As for mismatched information, it occurs far less frequently compared to missing information, and its categories largely overlap with those mentioned above.</p>
<p>To mitigate the widespread issues of missing and mismatched information, the first category can generally be addressed by referencing the original research paper and related literature, or by inspecting the code repository for explicit configurations.However, addressing the other three categories requires familiarity with general machine learning coding conventions, thus necessitating that the LLMs identify and utilize implementation patterns from comparable algorithms to enhance code quality.Future research may improve performance by incorporating implementation insights from similar algorithms through techniques such as in-context learning (Zhou et al., 2023;Xiang et al., 2024), and by leveraging real-time compiler feedback to infer precise variable values.</p>
<p>Conclusion</p>
<p>We evaluate LLMs' ability to replicate algorithms described in recent NLP papers.To support this, we introduce SciReplicate-Bench, a benchmark with rich annotations, and Sci-Reproducer, a dual-agent framework for bridging algorithm understanding and code generation.We assess performance using reasoning graph accuracy and standard implementation metrics.Results show the task is highly challenging, with failures largely caused by missing or inconsistent algorithm descriptions.</p>
<ol>
<li>Detailed annotation: for each aligned function, annotators documented input/output variables, intra-and cross-file dependencies, and external API usage.Additionally, they inserted explanatory comments mapping code segments to algorithm components.Based on these annotations and variable dependencies, we can construct a reasoning graph representing the implementation logic.During the annotation process, LLMs were employed to assist with algorithm-function alignment and the generation of variable descriptions and code comments.All outputs were subsequently reviewed and corrected by human annotators to ensure accuracy.</li>
</ol>
<p>The final selected papers are listed in Table A1.</p>
<p>Step 4: verification suite preparation Finally, annotators created verification suites with 10 test cases per task, drawn from the original datasets used in each repository for the majority of papers.For a small number of repositories, fewer than 10 test cases could be constructed.For instance, algorithms that analyze LLM parameters may have only a single test case.Given the inherent randomness in many NLP implementations and potential machine-related variability, we addressed reproducibility from two angles:</p>
<p>• Eliminating code randomness: annotators fixed random seeds and replaced nondeterministic operations (e.g., unordered sets) with deterministic equivalents to ensure consistent outputs across runs.• Controlling hardware variability: users were instructed to run both reference and generated code locally to eliminate discrepancies caused by hardware differences.</p>
<p>Lastly, annotators implemented task-specific comparison scripts to evaluate output correctness, accounting for variations in return types across tasks.</p>
<p>B Details of the Task Categories</p>
<p>C Details of the Actions</p>
<p>In this section, we provide implement details for all actions defined in the Sci-Reproducer.</p>
<p>SearchPaper We obtain the LaTeX source code of the target academic paper from arXiv 7and apply regular expression-based parsing to extract the content corresponding to each section.Subsequently, we iteratively feed the content of each subsection, along with the query generated by the large language model, into GPT-4o-mini.The model extracts relevant information and returns it as an observation to the paper agent.</p>
<p>SearchSection Following the same approach as SearchPaper, the tool begins by parsing the LaTeX source code of the target algorithm.Upon receiving a section ID from the Paper Agent, it retrieves and returns the content of the corresponding section.</p>
<p>SearchLiterature Given a paper ID and a query, the tool attempts to download the corresponding LaTeX source code from arXiv.If the LaTeX source code is unavailable, it returns no information.Otherwise, it extracts content relevant to the query from the paper, following the same procedure as the SearchPaper action.</p>
<p>SearchCode For each Python file in the code repository, we utilize the Python AST8 package to parse the file and extract all defined classes, functions, and global variables.Unlike embedding-based code search methods (Zhang et al., 2024c;2023c), the Code Agent in our framework directly provides the name of a code item.The tool then returns the corresponding definition if it exists; otherwise, it returns an empty response.</p>
<p>SearchFile When the Code Agent provides a file name, the tool returns the full content of the corresponding file.</p>
<p>SearchWeb When the Code Agent issues a query, we use the Google Search API 9 to retrieve relevant information from websites.These results are then processed by GPT-4omini, which filters the content and extracts the information most relevant to the query for return.</p>
<p>Compiler Once the Code Agent completes code generation, it invokes the compiler to execute the code.The generated function or method is inserted into the original Python file, and the corresponding Python environment is used to run the code.The output from the compiler is then returned as the feedback.</p>
<p>D Human Evaluation of Reasoning Graph Accuracy</p>
<p>To validate the reliability of our LLM-based evaluation metrics, we conducted a human evaluation study on a subset of our benchmark.Three PhD students in computer science independently assessed the reasoning graph accuracy for 20 tasks generated by GPT-4o-mini and O3-mini-medium using Sci-Reproducer.</p>
<p>As shown in Table A2, human evaluations demonstrate strong alignment with our automated LLM-based assessments.The mean scores show consistent agreement between human annotators and GPT-4o evaluations, with standard deviations indicating reasonable inter-annotator consistency.</p>
<p>Furthermore, we computed the Pearson correlation coefficient between human and LLMbased evaluations across all assessed instances.As presented in Table A3, the correlation (r = 0.7518, p ¡ 0.005) indicates a strong positive relationship between human judgments and automated assessments, supporting the validity of our LLM-based evaluation approach.The code snippets corresponding to each comment must not overlap, and nesting is not allowed.5. Single Function: Implement the code within a single function (or method), without breaking it into multiple functions.6. Import Package: Import all packages within the function (or method) to ensure the code is self -contained.7. Format, the comment for each snippet should be in the following format: # ---------------------------------------------------------------------------# Snippet x: Comment here # ---------------------------------------------------------------------------# # -------------------------------------------------------------------# Snippet 3: The transformation is a simple element-wise operation combined with # the scaling_factor, illustrating a simplified version of the # alignment concept from the LaTeX, which might involve more complex # contrastive or attentional calculations.# -------------------------------------------------------------------# [Begin Snippet 3] updated_segment = torch.add(segment_main,torch.mul(segment_aux,scaling_factor)) transformed_tokens[i, start_idx:end_idx, :] = updated_segment # [End Snippet 3] # ---------------------------------------------------------------------------# Snippet 4: The final transformed_tokens are now partially aligned with the auxiliary # reference, reflecting the notion of augmenting token-level outputs # (Equation references in the LaTeX snippet would correspond to eq. (2 -3) or # similar definitions of reference alignment).# ---------------------------------------------------------------------------# [Begin Snippet 4] return transformed_tokens # [End Snippet 4] ``Ỳ our answer:</p>
<p>Figure 1 :
1
Figure 1: Overview of the task and the proposed Sci-Reproducer framework.The task involves algorithm understanding and code implementation, handled by a Paper Agent and a Code Agent operating in separate contexts with specialized actions.</p>
<p>Figure 2 :
2
Figure 2: A grouped bar chart illustrating the frequency of tool usage by different models.The x-axis represents various actions, while the y-axis indicates the total number of times each tool was used on this dataset.</p>
<p>Model</p>
<p>(Sci-Reproducer) Exe Acc(↑) CodeBLEU(↑) RG Acc(↑) Recall Intra-File(↑) Cross-File(↑) API(↑)</p>
<p>Figure A1 :
A1
Figure A1: The categories of the tasks within SciReplicate-Bench.The benchmark encompasses five main task categories in the NLP domain: representation and embedding methods, loss functions and optimization objectives, information extraction and aggregation, model architecture components, and inference and search algorithms.The distribution of each task category is illustrated in Figure A1.</p>
<p>start_idx, end_idx) in enumerate(transformation_indices): # -----------------------------------------------------------------------# Snippet 1: We first verify if the current batch item is valid by checking the # batch_mask, analogous to referencing an in-context example (\mathbf{S}) # that has a corresponding reference (\mathbf{H_r}) in the LaTeX snippet.# -----------------------------------------------------------------------#[Begin Snippet 1] if batch_mask[i]: # [End Snippet 1]# -------------------------------------------------------------------# Snippet 2: Here, we apply a basic shift to the token_representation by # incorporating a slice from the auxiliary_representation, akin to # combining (\mathbf{H}) with a portion of (\mathbf{H_r}) for # enhanced alignment.# -------------------------------------------------------------------# [Begin Snippet 2] segment_main = transformed_tokens[i, start_idx:end_idx, :] segment_aux = auxiliary_representation[i, start_idx:end_idx, :] # [End Snippet 2]</p>
<p>Figure A5 :
A5
Figure A5: The prompt for code generation.</p>
<p>Table 2 :
2
The pre-defined actions for the Paper Agent and the Code Agent.</p>
<p>Table 3 :
3
Performance evaluation on the SciReplicate-Bench benchmark.Models with notation indicate reasoning LLMs."Exe Acc" represents execution accuracy while "RG Acc" indicates reasoning graph accuracy.</p>
<p>Table 3 displays
3
Sci-Reproducer's evaluation results and contributions of Code/Paper Agent.The "No Agent"directly prompts the LLM to generate code based solely on the algorithm description and function signature."NoPaper Agent" allows the LLM to use Code Agent actions, but restricts access to Paper Agent actions."NoCode Agent" grants access to Paper Agent actions but blocks Code Agent capabilities.The results offer key insights, discussed in the following.executionaccuracy without using the agent to examine literature and repository contexts.With enhancement of Sci-Reproducer, these LLMs show notable improvements, with an average increase of 0.181 in execution ACC and 0.057 in CodeBLEU, although even the best-performing model, Claude-Sonnet-3.7,only achieved 0.390 execution accuracy.This highlights the exceptional challenge presented by our SciReplicate-Bench.
LLMs struggles on SciReplicate-Bench Most LLMs perform poorly, achieving less than0.1
LLMs can comprehend algorithm logicLLMs demonstrate strong algorithmic comprehension capabilities, as evidenced by reasoning graph accuracy scores averaging 0.716 even without agent assistance.The addition of individual agents provides modest but consistent improvements: the Paper Agent increases understanding by 0.009 on average, while the Code Agent contributes a larger gain of 0.036.Combined agent deployment yields a cumulative improvement of 0.037.These enhancements stem from complementary mechanisms: the Paper Agent strengthens theoretical comprehension by gathering relevant contextual information from academic literature, while the Code Agent facilitates practical understanding through extraction of pertinent code patterns and dependency structures from repositories.</p>
<p>Table 4 :
4
Experimental Results when missing/mismatched information is regard as external input in the prompt.
GPT-4o-mini0.2200.3160.8090.5880.4850.409Deepseek-V30.4700.3780.8340.6820.4240.609o3-mini-low0.2200.2920.8500.2590.0910.460</p>
<p>Please complete the target function (or method) and provide the pure code output without any additional text.Include comments in the code following these specific guidelines:Code Comments: 1. Focus on Reasoning: Comments should explain the reasoning behind the code generation process, as derived from the LaTeX description.2. No Implementation Details: Avoid including any code-specific implementation details in the comments.3. Mapping to LaTeX: Each comment must indicate which functionality described in the LaTeX code is implemented in the subsequent Python code snippet.4. No overlap:
TitleConference1. From Zero to Hero: Cold-Start Anomaly Detection (Reiss et al., 2024)Findings of ACL 20242. Addressing Order Sensitivity of In-Context Demonstration Examples in Causal LanguageFindings of ACL 2024Models (Xiang et al., 2024)3. Breaking the Ceiling of the LLM Community by Treating Token Generation as a ClassificationFindings of EMNLP 2024for Ensembling (Yu et al., 2024)4. Simple but Effective Compound Geometric Operations for Temporal Knowledge GraphCompletion (Ying et al., 2024)
https://www.semanticscholar.org
https://www.researchrabbit.ai/
https://www.undermind.ai/
A video demonstration showcasing Sci-Reproducer's capabilities is provided at https://youtu. be/qcSIMgyehjE
https://paperswithcode.com/api/v1/docs/
https://docs.github.com/en/rest?apiVersion=2022-11-28
https://arxiv.org/
https://docs.python.org/3/library/ast.html
https://developers.google.com/custom-search/v1/
AcknowledgementsThis work was supported in part by the UK Engineering and Physical Sciences Research Council (EPSRC) through a Turing AI Fellowship (grant no.EP/V020579/1, EP/V020579/2).We thank the authors of the selected papers for making their code openly available.Code Repository Code Context Website Actions Search web Code Interpreter Search Code Literature ReportAppendixA Details of the Annotation ProcessStep 1: paper selection We curated NLP papers from leading conferences in 2024, including ACL, EMNLP, ICLR, Neurips, and COLING.Using a web crawler, we collected accepted paper titles and employed the PapersWithCode API 5 to identify those with open-source implementations.For each identified paper, we retrieved corresponding GitHub repository links and metadata (e.g., stars, issues, release dates) via the GitHub REST API 6 .To filter candidates, we applied the following criteria:• Removed survey/exploratory papers while retaining method-focused research.• Applied a cutoff date of January 1, 2024 to avoid data leakage.• Excluded repositories with fewer than 5 stars to ensure basic quality assurance.Subsequently, researchers manually reviewed each candidate paper and its repository.We discarded papers with excessive computational demands, poorly structured code, ambiguous documentation, missing preprocessing steps, or reported reproduction issues.Step2: python environment setup For papers passing the initial screening, annotators followed the README to set up the environment and replicate experiments.Common issues included dependency conflicts, data loading failures, and incomplete or buggy code.Annotators attempted to resolve these problems; repositories with irrecoverable errors were excluded.Step3: annotation Annotation consists of two steps:1. Algorithm-Function alignment: most papers contain multiple algorithmic components, often organized as subsections.Annotators segmented these into distinct units and mapped each to its corresponding implementation.Code was refactored to encapsulate each algorithm in a standalone function or method.Papers with implementations too fragmented for restructuring were excluded.Published as a conference paper at COLM 2025 While this preliminary validation demonstrates the effectiveness of our automated metrics, comprehensive human evaluation across the full benchmark remains an important direction for future work.E Figures and TablesTarget Algorithm 4.1 Sentence ExtractionWe propose to model sentence extraction as extractive summarization.For this purpose, we concatenate all the sentences in the document into an input sequence, which is then fed to BertSum(Liu and Lapata, 2019)to obtain the score for each sentence.The details ofthe process can be found in Appendix a.  Step2: Calculate positive similarity score: cos(query, key) measures how close the model token is to the reference token.Step3: Calculate negative similarity score: cos(query, query.clone().detach())measures how close the token is to itself (detached) which serves as the negative reference.Step4: Final contrastive loss for these tokens: -log(positive / (positive + negative)) This aligns with the InfoNCE or contrastive objective described in the paper's Eq. (Information Augmentation).Step5: Accumulate the loss and update count and average the total contrastive loss by the number of token-level comparisons.The loss is averaged across tokens.Mismatch details: None⑨ Verification Suite 1. Test Cases: 10 test cases chosen from benchmark adopted in the original code repository.Output Compare Script:A Python program designed to verify the accuracy of the generated code's output.Implementation logic Data splitting, application of dropout, formatting of input sequences, and handling special or edge cases in the input data.Code EnvironmentCoding strategyCaching for performance enhancement, retry mechanisms to handle failures, early stopping criteria, and strategies for memory optimization.TableA5: Some examples for different missing information categories.[Task Overview] Reproduce Python code corresponding to a LaTeX-based methodology from a scientific paper.However, due to the paper's length, it cannot be fully ingested by a large language model at once.Therefore, the solution requires two main steps: 1. Information Retrieval (Your Current Task): Extract relevant details, insights, and supporting information from the academic paper's LaTeX description and related literature.2. Code Reproduction (Subsequent Task): Implement the Python code based on the information gathered and the provided LaTeX.[Your Specific Focus] You are tasked exclusively with Step 1: Information Retrieval.You must gather and organize all necessary details that will later be used to implement the Python code.[Input] 1. List of sections: The paper includes the following sections (titles are provided for reference): -w'x' (string): The title of the referenced section.Example:-Latex: "The full derivation of our loss function can be found in method Section .",Action: SearchSection["method"] * SearchLiterature[key, query] Description: If the target section cites another paper (\cite{label}) and you determine that some information needs to be retrieved from that paper, return SearchLiterature[label, query], where query is the specific information you need to look for in the referenced paper.Parameters:-'key' (string): The citation key of the referenced paper.In LaTeX, when citing a paper, we use \cite{x}, where x represents the citation key.-'query' (string): The specific information to search for in the referenced paper.Example: I -Latex: "We adopt the metric proposed in \cite{wang2025}".Action: SearchLiterature["wang2025", "The proposed metric in the paper"] -Latex: "The algorithm is based on the work of \cite{smith2018}".Action: SearchLiterature["smith2018", "The algorithm details in the paper"] -Latex: "The dataset is based on the study by \cite{jones2020}".Action: SearchLiterature["jones2020", "The dataset details in the paper"] [Instruction] In order to complete code reproduction, it is first necessary to understand the algorithm described in the LaTeX description.The tools "SearchPaper", "SearchSection" and "SearchLiterature" should be used to retrieve relevant information from the paper to help you understand the methodology proposed in the latex description.For example: 1.If the LaTeX Description lacks the definition of a variable, use "SearchPaper" tool to find its definition.2. If the LaTeX Description references other sections of the paper, use "SearchSection" tool to retrieve those sections and supplement the missing details. 3.If the LaTeX Description cites methods from other papers, use "SearchLiterature" tool to extract relevant information from the referenced papers.[Action] 1. Apply a tool defined above to gather external information.2. If you have gathered all the necessary information, fully understood the LaTeX code, and are prepared to proceed to the Code Reproduction stage, the appropriate action is "Finish"[Observation] 1.If the action is apply predefined tool, then the observation should be the return response of the tool.You are a code assistant tasked with reproducing a Python function corresponding to a algorithm in the methods part of a scientific paper.The local coding environment includes a GPU and supports CUDA.I will provide the following information:1. Repository structure: The organization of files within the code repository.This is a repository-level code generation task, so you should explore the repo thoroughly to extract useful code.2. Target function: The definition of the python function you need to implement.3. LaTeX description: The LaTeX code for the corresponding algorithm in the paper, describing the algorithm implemented by the target function.4. The extracted information: The information extracted from the target paper, and relevant literature that can provide you more details when implement the target function.5. Tools: Tools that can be adopted to gather external information during the generation process.The information is extracted from the paper and relevant literature by a paper search agent, which consists of a series of information points.When you implement the target function, you should refer to the extracted information to understand the target algorithm.When information from "Relevant Literature" conflicts with the target paper, always prioritize the information from the target paper.The extracted information is as follows: In order to complete this task, it is necessary to use tools to search the code repository for context that can help implement the target function.For example: 1. Use "SearchFile" to retrieve the content of a Python file from the repository.2. Use "SearchCodeItem" to find details about a specific code item within the repository.3. Use "SearchWeb" to retrieve information from the website.To effectively tackle the code reproduction task, follow a structured process that alternates between Thought, Action, and Observation steps:[Thought] 1. Analyze the current situation.2. Identify missing information from code.As it is a repo-level code generation task, you need to explore the relvant functions, classes, in the code repository.3. Plan the next steps to gather the required information.[Action] 1. Apply a tool defined above to gather external information.2. If you are ready to generate the code, then the action should be "GenerateCode".[Observation] 1.If the action is apply predefined tool, then the observation should be the return response of the tool.2. If the action is "GenerateCode", then the observation is the result returned by the interpreter after executing the generated code.
GPT 4o mini. </p>
<p>Adaptive contrastive search: Uncertainty-guided decoding for open-ended text generation. Esteban Garces Arias, Julian Rodemann, Meimingwei Li, Christian Heumann, M Aßenmacher, Conference on Empirical Methods in Natural Language Processing. 2024</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J Cai, Michael Terry, Quoc V Le, Charles Sutton, ArXiv, abs/2108.077322021</p>
<p>Accelerating scientific discovery with generative knowledge extraction, graph-based representation, and multimodal intelligent graph reasoning. Markus J Buehler, 10.1088/2632-2153/ad7228Machine Learning: Science and Technology. 2024</p>
<p>Mle-bench: Evaluating machine learning agents on machine learning engineering. Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal A Maksin, Lilian Patwardhan, Aleksander Weng, Mkadry, ArXiv, abs/2410.070952024</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo Bavarian, Clemens Winter, Phil Tillet, Felipe Petroski Such, David W Cummings, Matthias Plappert, Fotios Chantzis, ArXiv, abs/2107.03374Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaJan Leike. 2021Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew CarrEvaluating large language models trained on code</p>
<p>Lifelong knowledge editing for llms with retrieval-augmented continuous prompt learning. Qizhou Chen, Taolin Zhang, Dongyang Li, Longtao Huang, Hui Xue, Chengyu Wang, Xiaofeng He, Conference on Empirical Methods in Natural Language Processing. 2024a</p>
<p>Routerdc: Querybased router by dual contrastive learning for assembling large language models. Shuhao Chen, Weisen Jiang, Baijiong Lin, James T Kwok, Yu Zhang, ArXiv, abs/240919886. 2024b</p>
<p>Learning to maximize mutual information for chain-of-thought distillation. Xin Chen, Hanxian Huang, Yanjun Gao, Yi Wang, Jishen Zhao, Ke Ding, Annual Meeting of the Association for Computational Linguistics. 2024c</p>
<p>When is tree search useful for llm planning? it depends on the discriminator. Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu Su, Huan Sun, Annual Meeting of the Association for Computational Linguistics. 2024d</p>
<p>Reasoning paths optimization: Learning to reason and explore from diverse paths. Ken Yew, Guizhen Chia, Weiwen Chen, Anh Tuan Xu, Soujanya Luu, Li Poria, Bing, ArXiv, abs/2410.108582024</p>
<p>Nearest neighbor normalization improves multimodal retrieval. Neil Chowdhury, Franklin Wang, Sumedh Shenoy, Douwe Kiela, Sarah Schwettmann, Tristan Thrush, Conference on Empirical Methods in Natural Language Processing. 2024</p>
<p>The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks. Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis , Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, Joseph E Gonzalez, ArXiv, abs/2502.082352025</p>
<p>Deepseek-Ai , Aixin Liu, Bei Feng, Bing Xue, Bing-Li Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dong-Li Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J L Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Jun-Mei Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R J Chen, R L Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S S Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shao-Ping Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, T Shuting Pan, Tao Wang, Tian Yun, Tianyu Pei, W L Sun, Wangding Xiao, Wanjia Zeng, Wei Zhao, Wen An, Wenfeng Liu, Wenjun Liang, Wen-Xuan Gao, Wentao Yu, X Q Zhang, Xiangyu Li, Xianzu Jin, Xiaoling Wang, Xiaodong Bi, Xiaohan Liu, Xi-Cheng Wang, Xiaokang Shen, Xiaokang Chen, Xiaosha Zhang, Xiaotao Chen, Xiaowen Nie, Xiaoxiang Sun, Xin Wang, Xin Cheng, Xin Liu, Xingchao Xie, Xingkai Liu, Xinnan Yu, Xinxia Song, Xinyi Shan, Xinyu Zhou, Xinyuan Yang, Xuecheng Li, Xuheng Su, Y K Lin, Y Q Li, Y X Wang, Y X Wei, Yang Zhu, Yanhong Zhang, Yanping Xu, Yao Huang, Yao Li, Yaofeng Zhao, Yao Sun, Yaohui Li, Yi Wang, Yi Yu, Yichao Zheng, Yifan Zhang, Yi Shi, Ying Xiong, Ying He, Yishi Tang, Yisong Piao, Yixuan Wang, Yi-Bing Tan, Yiyuan Ma, Yongqiang Liu, Yu Guo, Yuan Wu, Yuchen Ou, Yuduan Zhu, Yue Wang, Yuheng Gong, Yujia Zou, Yukun He, Yunfan Zha, Yunxiang Xiong, Yuting Ma, Yu-Wei Yan, Yu Luo, Yuxuan Mei You, Yuyang Liu, Z F Zhou, Zehui Wu, Zehui Ren, Zhangli Ren, Zhe Sha, Zhean Fu, Zhen Xu, Zhen Huang, Zhenda Zhang, Xie, Zhewen Zhen Guo Zhang, Zhibin Hao, Zhicheng Gou, Zhigang Ma, Zhihong Yan, Zhipeng Shao, Zhiyu Xu, Zhongyu Wu, Zhuoshu Zhang, Zihui Li, Zijia Gu, Zijun Zhu, Zi-An Liu, Ziwei Li, Xie, ArXiv, abs/2412.19437Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report. Ziyang Song2024</p>
<p>Document-level claim extraction and decontextualisation for fact-checking. Zhenyun Deng, M Schlichtkrull, Andreas Vlachos, Annual Meeting of the Association for Computational Linguistics. 2024</p>
<p>LLMs assist NLP researchers: Critique paper (meta-)reviewing. Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, Ranran Haoran, Vipul Zhang, Yinghui Gupta, Tao Li, Fei Li, Qin Wang, Tianlin Liu, Pengzhi Liu, Congying Gao, Chen Xia, Cheng Xing, Zhaowei Jiayang, Ying Wang, Raj Sanjay Su, Ruohao Shah, Jing Guo, Haoran Gu, Kangda Li, Zihao Wei, Lu Wang, Surangika Cheng, Meng Ranathunga, Jie Fang, Fei Fu, Ruihong Liu, Eduardo Huang, Yixin Blanco, Rui Cao, Philip S Zhang, Wenpeng Yu, Yin, 10.18653/v1/2024.emnlp-main.292Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning. Alireza Ghafarollahi, Markus J Buehler, ArXiv, abs/2409.055562024</p>
<p>GPT-4o. </p>
<p>Agentic ai for scientific discovery: A survey of progress, challenges, and future directions. Mourad Gridach, Jay Nanavati, Khaldoun Zine El Abidine, Lenon Mendes, Christina Mack, 2025</p>
<p>Xuemei Gu and Mario Krenn. Interesting scientific idea generation using knowledge graphs and llms: Evaluations with 100 research group leaders. Tianyang Gu, Jingjin Wang, Zhihao Zhang, Haohong Li, ArXiv, abs/2412.141412024. 2025Llms can realize combinatorial creativity: generating creative ideas via llms for scientific research</p>
<p>Measuring coding challenge competence with apps. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Xiaodong Song, Jacob Steinhardt, ArXiv, abs/2105.099382021</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, International Conference on Machine Learning. 2023</p>
<p>Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, ArXiv, abs/2310.067702024. 2023Ofir Press, and Karthik NarasimhanSwe-bench: Can language models resolve real-world github issues?</p>
<p>Exploring concept depth: How large language models acquire knowledge at different layers?. Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, Fan Yang, Mengnan Du, Yongfeng Zhang, ArXiv, abs/2404.070662024</p>
<p>Masklid: Code-switching language identification through iterative masking. Franccois Amir Hossein Kargaran, Hinrich Yvon, Schutze, ArXiv, abs/2406.062632024</p>
<p>Towards robust and generalized parameterefficient fine-tuning for noisy label learning. Yeachan Kim, Junho Kim, Sangkeun Lee, ArXiv, abs/2411.008732024</p>
<p>Can large language models unlock novel scientific research ideas?. Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal, ArXiv, abs/2409.061852024</p>
<p>Style-specific neurons for steering llms in text style transfer. Wen Lai, Viktor Hangya, Alexander Fraser, ArXiv, abs/2410.005932024</p>
<p>Deveval: A manually-annotated code generation benchmark aligned with real-world code repositories. Jia Li, Ge Li, Yunfei Zhao, Yongming Li, Huanyu Liu, Hao Zhu, Lecheng Wang, Kaibo Liu, Zheng Fang, Lanshen Wang, Jiazheng Ding, Xuanming Zhang, Yuqi Zhu, Yihong Dong, Zhi Jin, Binhua Li, Fei Huang, Yongbin Li, ArXiv, abs/2405Conference on Empirical Methods in Natural Language Processing. Hongfu Liu, Yuxi Xie, Ye Wang, Michael Shieh, 19856. 2024. 2024aAdvancing adversarial suffix transfer learning on aligned large language models</p>
<p>Beyond single-event extraction: Towards efficient document-level multi-event argument extraction. Wanlong Liu, Li Zhou, Dingyi Zeng, Yichen Xiao, Shaohuan Cheng, Chen Zhang, Grandee Lee, Malu Zhang, Wenyu Chen, ArXiv, abs/2405.018842024b</p>
<p>Haotian Ye, and Hinrich Sch ütze. Transmi: A framework to create strong baselines from multilingual pretrained language models for transliterated data. Yihong Liu, Chunlan Ma, ArXiv, abs/2405.099132024c</p>
<p>Researchbench: Benchmarking llms in scientific discovery via inspiration-based task decomposition. Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou ; Yuliang, Xiangru Liu, Zefan Tang, Junjie Cai, Yichi Lu, Yanjun Zhang, Zexuan Shao, Helan Deng, Zengxian Hu, Kaikai Yang, Ruijun An, Shuzheng Huang, Sheng Si, Haozhe Chen, Zheng Zhao, Liang Li, Yiming Chen, Yan Zong, Tianyu Wang, Zhiwei Liu, Baobao Jiang, Yujia Chang, Wangchunshu Qin, Yilun Zhou, Zhao, ArXiv, abs/2503.21248Arman Cohan, and Mark B. Gerstein. Ml-bench: Evaluating large language models and agents for machine learning tasks on repository-level code. 2025. 2023</p>
<p>Code generation from flowcharts with texts: A benchmark dataset and an approach. Zejie Liu, Xiaoyu Hu, Deyu Zhou, Lin Li, Xu Zhang, Yanzheng Xiang, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>Nl2sql generation with noise labels based on multi-task learning. Lingli Long, Yongjin Zhu, Jun Shao, Zheng Kong, Jian Li, Yanzheng Xiang, Xu Zhang, Journal of Physics: Conference Series. 22942022</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob N Foerster, Jeff Clune, David Ha, ArXiv, abs/2408.062922024</p>
<p>Less is ken: a universal and simple non-parametric pruning algorithm for large language models. Michele Mastromattei, Fabio Massimo Zanzotto ; Deepak, Lovish Nathani, Nicholas Madaan, Niko Roberts, Ajay Lay Bashlykov, Vincent Menon, Amar Moens, Despoina Budhiraja, Vladislav Magka, Gaurav Vorotilov, Dieuwke Chaurasia, Ricardo Silveira Hupkes, Tatiana Cabral, Jakob Shavrina, Yoram Foerster, William Bachrach, Wang Yang, Roberta Raileanu, ArXiv, abs/2502.144992024. 2025Mlgym: A new framework and benchmark for advancing ai research agents. o3 mini</p>
<p>Sparks of science: Hypothesis generation using structured paper data. O' Charles, Tirthankar Neill, Roberta Ghosal, Mike Raileanu, Thang Walmsley, Kevin Bui, Ioana Schawinski, Ciuca, ArXiv, abs/2504.129762025</p>
<p>Sophie Ostmeier, Justin Xu, Zhihong Chen, Maya Varma, Louis Blankemeier, Christian Bluethgen, Arne Edward Michalson, Michael E Moseley, Curtis P Langlotz, Akshay S Chaudhari, Jean-Benoit Delbrouck, ArXiv, abs/2405.03595Generative radiology report evaluation and error notation. Green2024</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Annual Meeting of the Association for Computational Linguistics. 2002</p>
<p>Enhancing knowledge distillation of large language models through efficient multi-modal distribution alignment. Tianyu Peng, Jiajun Zhang, International Conference on Computational Linguistics. 2024</p>
<p>Neuromax: Enhancing neural topic modeling via maximizing mutual information and group topic regularization. Duy-Tung Pham, Thien , Trang Nguyen Vu, Tung Nguyen, Linh Ngo Van, Duc , Anh Nguyen, Thien Huu Nguyen, ArXiv, abs/2409.197492024</p>
<p>Evaluating the text-to-sql capabilities of large language models. Nitarshan Rajkumar, Raymond Li, Dzmitry Bahdanau, ArXiv, abs/2204.004982022</p>
<p>From zero to hero: Cold-start anomaly detection. Tal Reiss, George Kour, Naama Zwerdling, Ateret Anaby-Tavor, Yedid Hoshen, ArXiv, abs/2405.203412024</p>
<p>Superbench: A super-resolution benchmark dataset for scientific machine learning. N Pu Ren, Shashank Benjamin Erichson, Omer Subramanian, Zarija San, Michael W Lukic, Mahoney, ArXiv, abs/2306.140702023</p>
<p>Codebleu: a method for automatic evaluation of code synthesis. Daya Shuo Ren, Shuai Guo, Long Lu, Shujie Zhou, Duyu Liu, M Tang, Ambrosio Zhou, Shuai Blanco, Ma, ArXiv, abs/2009.102972020</p>
<p>Raptor: Recursive abstractive processing for tree-organized retrieval. Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D Manning, ArXiv, abs/2401.180592024</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, ArXiv, abs/2302.047612023</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, ArXiv, abs/2501.04227Agent laboratory: Using llm agents as research assistants. 2025</p>
<p>Paper2code: Automating code generation from scientific papers in machine learning. Minju Seo, Jinheon Baek, Seongyun Lee, Sung Ju Hwang, ArXiv, abs/2504.171922025</p>
<p>Ircan: Mitigating knowledge conflicts in llm generation via identifying and reweighting contextaware neurons. Dan Shi, Renren Jin, Tianhao Shen, Weilong Dong, Xinwei Wu, Deyi Xiong, ArXiv, abs/2406.184062024</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, ArXiv, abs/2409.041092024</p>
<p>Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark. Zachary S Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, Arvind Narayanan, Trans. Mach. Learn. Res. 20242024</p>
<p>Unsupervised homography estimation on multimodal image pair via alternating optimization. Sanghyeob Song, Jaihyun Lew, Hyemi Jang, Sungroh Yoon ; Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, ArXiv, abs/2504.01848Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan2024. 2025Paperbench: Evaluating ai's ability to replicate ai research</p>
<p>Stop overthinking: A survey on efficient reasoning for large language models. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, Xia Hu, 2025</p>
<p>Language-specific neurons: The key to multilingual capabilities in large language models. Liyan Tang, Philippe Laban, Greg Durrett, ; Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei, Ji-Rong Wen, ArXiv, abs/2402.16438Conference on Empirical Methods in Natural Language Processing. 2024a. 2024bMinicheck: Efficient fact-checking of llms on grounding documents</p>
<p>Unifying dualspace embedding for entity alignment via contrastive learning. Hung Quoc To, Minh Huynh Nguyen, D Q Nghi, Bui ; Cunda, Weihua Wang, Qiuyu Wang, Feilong Liang, Guanglai Bao, Gao, ArXiv, abs/2412.05028Annual Meeting of the Association for Computational Linguistics. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 2023. 2024a. 2023Annual Meeting of the Association for Computational Linguistics</p>
<p>Garlic: Llm-guided dynamic progress control with hierarchical weighted graph for long document qa. Xinyu Wang, Yanzheng Xiang, Lin Gui, Yulan He ; Yaoke, Yun Wang, Wenqiao Zhu, Yueting Zhang, Yunfei Zhuang, Siliang Li, Tang, ArXiv, abs/2410.04790Conference on Empirical Methods in Natural Language Processing. 2024b. 2024cBridging local details and global context in text-attributed graphs</p>
<p>Execution-based evaluation for open-domain code generation. Zhiruo Wang, Shuyan Zhou, Daniel Fried, Graham Neubig, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>G³r: A graph-guided generate-and-rerank framework for complex and cross-domain text-to-sql generation. Yanzheng Xiang, Qian-Wen Zhang, Xu Zhang, Zejie Liu, Yunbo Cao, Deyu Zhou, Annual Meeting of the Association for Computational Linguistics. 2023</p>
<p>Bill Yuchen Lin, and Radha Poovendran. Safedecoding: Defending against jailbreak attacks via safety-aware decoding. Yanzheng Xiang, Hanqi Yan, Lin Gui, Yulan He ; Zhangchen, Fengqing Xu, Luyao Jiang, Jinyuan Niu, Jia, ArXiv, abs/2402.089832024. 2024Addressing order sensitivity of incontext demonstration examples in causal language models</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Nicolaus Foerster, Jeff Clune, David Ha, ArXiv, abs/2504.080662025</p>
<p>Simple but effective compound geometric operations for temporal knowledge graph completion. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao ; Rui Ying, Mengting Hu, Jianfeng Wu, Yalan Xie, Xiaoyi Liu, Zhunheng Wang, Ming Jiang, Hang Gao, Linlin Zhang, Renhong Cheng, ArXiv, abs/2210.03629Annual Meeting of the Association for Computational Linguistics. 2022. 2024React: Synergizing reasoning and acting in language models</p>
<p>Breaking the ceiling of the llm community by treating token generation as a classification for ensembling. Yao-Ching Yu, Chun-Chih Kuo, Ziqi Ye, Yu-Cheng Chang, Yueh-Se Li, Conference on Empirical Methods in Natural Language Processing. 2024</p>
<p>Neuron-level knowledge attribution in large language models. Zeping Yu, Sophia Ananiadou, Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Can we automate scientific reviewing?. Weizhe Yuan, Pengfei Liu, Graham Neubig, 10.1613/jair.1.12862J. Artif. Int. Res. 1076-975775December 2022</p>
<p>Gliner: Generalist model for named entity recognition using bidirectional transformer. Urchade Zaratiana, Nadi Tomeh, Pierre Holat, Thierry Charnois, ArXiv, abs/2311.085262023</p>
<p>End-to-end beam retrieval for multi-hop question answering. Jiahao Zhang, H Zhang, Dongmei Zhang, Yong Liu, Sheng Huang, North American Chapter. the Association for Computational Linguistics2023a</p>
<p>Toolcoder: Teach code generation models to use api search tools. Kechi Zhang, Ge Li, Jia Li, Zhuo Li, Zhi Jin, ArXiv, abs/2305.040322023b</p>
<p>Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, Zhi Jin, Annual Meeting of the Association for Computational Linguistics. 2024a</p>
<p>I2r: Intra and intermodal representation learning for code search. Xu Zhang, Yanzheng Xiang, Zejie Liu, Xiaoyu Hu, Deyu Zhou, Intell. Data Anal. 282023c</p>
<p>Hfd: Hierarchical feature decoupling for sql generation from text. Xu Zhang, Xiaoyu Hu, Zejie Liu, Yanzheng Xiang, Deyu Zhou, Intell. Data Anal. 282024b</p>
<p>Secon: Maintaining semantic consistency in data augmentation for code search. Xu Zhang, Zexu Lin, Xiaoyu Hu, Jianlei Wang, Wenpeng Lu, Deyu Zhou, ACM Transactions on Information Systems. 2024c</p>
<p>Ratescore: A metric for radiology report generation. W Zhao, C Wu, X Zhang, Y Zhang, Y Wang, W Xie, Conference on Empirical Methods in Natural Language Processing. 2024</p>
<p>The mystery of in-context learning: A comprehensive survey on interpretation and analysis. Yuxiang Zhou, Jiazheng Li, Yanzheng Xiang, Hanqi Yan, Lin Gui, Yulan He, Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>When is Tree Search Useful for LLM Planning?. Chen , 2024d2024It Depends on the Discriminator</p>
<p>Functional Overlap Reranking for Neural Code Generation. To, Findings of ACL 2024. 2023</p>
<p>Beyond Single-Event Extraction: Towards Efficient Document-Level Multi-Event Argument Extraction. Liu, Findings of ACL 2024. 2024b</p>
<p>Unifying Dual-Space Embedding for Entity Alignment via Contrastive Learning. Wang , 2024a2025</p>
<p>Exploring Concept Depth: How Large Language Models Acquire Knowledge and Concepts at Different Layers?. Jin , 20242025</p>
<p>TRANSMI: A Framework to Create Strong Baselines from Multilingual Pretrained Language Models for Transliterated Data. Liu, 2024c2025</p>
<p>Enhancing Knowledge Distillation of Large Language Models through Efficient Multi-Modal Distribution Alignment. 2024Peng &amp; Zhang2025</p>
<p>Document-level Claim Extraction and Decontextualisation for Fact-Checking. Deng, 20242024</p>
<p>IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying and Reweighting Context-Aware Neurons. Shi, 20242024</p>
<p>RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models. Chen , 2024b2024</p>
<p>Unsupervised Homography Estimation on Multimodal Image Pair via Alternating Optimization. Song, 20242024</p>
<p>Sarthi, ICLR 2024RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval. 2024</p>
<p>Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models. Findings of ACL 2024. Mastromattei &amp; Zanzotto2024</p>
<p>Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation. Arias, Findings of EMNLP 2024. 2024</p>
<p>Efficient Fact-Checking of LLMs on Grounding Documents. ; Minicheck, Tang, 2024a2024</p>
<p>Nearest Neighbor Normalization Improves Multimodal Retrieval. Chowdhury, 20242024</p>
<p>Neuron-Level Knowledge Attribution in Large Language Models. Yu &amp; Ananiadou20232024</p>
<p>Zhao, Metric for Radiology Report Generation. 20242024</p>
<p>Ostmeier, GREEN: Generative Radiology Report Evaluation and Error Notation. 2024Findings of EMNLP 2024</p>
<p>Lai, Specific Neurons for Steering LLMs in Text Style Transfer. 20242024</p>
<p>Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models. Tang, 2024b2024</p>
<p>Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths. Chia , Findings of EMNLP 2024. 2024</p>
<p>Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning. Chen , 2024a2024</p>
<p>Enhancing Neural Topic Modeling via Maximizing Mutual Information and Group Topic Regularization. ; Neuromax, Pham, Findings of EMNLP 2024. 2024</p>
<p>Bridging Local Details and Global Context in Text-Attributed Graphs. Wang , 2024c2024</p>
<p>Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models. Liu, 2024a2024</p>
<p>SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding. Xu, 20242024</p>
<p>MaskLID: Code-Switching Language Identification through Iterative Masking. Kargaran, 20242024</p>
<p>Towards Robust and Generalized Parameter-Efficient Fine-Tuning for Noisy Label Learning. Kim, 20242024</p>
<p>Learning to Maximize Mutual Information for Chain-of-Thought Distillation. Chen , Findings of ACL 2024. 2024c</p>
<p>Zaratiana Gliner, Generalist Model for Named Entity Recognition using Bidirectional Transformer. 20232024</p>
<p>End-to-End Beam Retrieval for Multi-Hop Question Answering. Zhang, 2023a2024</p>
<p>Table A1: List of papers used in our Sci-Replicate benchmark. </p>            </div>
        </div>

    </div>
</body>
</html>
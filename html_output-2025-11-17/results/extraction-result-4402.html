<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4402 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4402</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4402</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-265034148</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.03056v4.pdf" target="_blank">LitSumm: large language models for literature summarization of noncoding RNAs</a></p>
                <p><strong>Paper Abstract:</strong> Abstract Curation of literature in life sciences is a growing challenge. The continued increase in the rate of publication, coupled with the relatively fixed number of curators worldwide, presents a major challenge to developers of biomedical knowledgebases. Very few knowledgebases have resources to scale to the whole relevant literature and all have to prioritize their efforts. In this work, we take a first step to alleviating the lack of curator time in RNA science by generating summaries of literature for noncoding RNAs using large language models (LLMs). We demonstrate that high-quality, factually accurate summaries with accurate references can be automatically generated from the literature using a commercial LLM and a chain of prompts and checks. Manual assessment was carried out for a subset of summaries, with the majority being rated extremely high quality. We apply our tool to a selection of >4600 ncRNAs and make the generated summaries available via the RNAcentral resource. We conclude that automated literature summarization is feasible with the current generation of LLMs, provided that careful prompting and automated checking are applied. Database URL: https://rnacentral.org/</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4402.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4402.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitSumm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitSumm: Large language models for literature summarisation of non-coding RNAs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end pipeline that uses a commercial LLM (GPT-4-turbo) together with sentence extraction, topic-based sentence selection, prompt chaining, automated reference checks and self-consistency checking to generate provenance-tracked natural-language summaries of literature for individual ncRNAs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LitSumm</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline that (1) finds open-access full-text articles mentioning a target RNA via EuropePMC (LitScan), (2) extracts sentences that mention the RNA and stores them as a context, (3) selects representative sentences using embedding-based topic modelling (SentenceTransformers -> UMAP -> HDBSCAN) and greedy exemplar selection to fit a token budget, (4) generates a first-pass summary with a GPT-4-turbo model using carefully engineered prompts that require in-context citations, (5) applies automated checks on references (adequacy, formatting, presence in context, citation location and citation density) with specific rescue prompts for failures, (6) runs a self-consistency/veracity stage where the LLM classifies each summary bullet as true/false with supporting context and then asks the LLM to revise problematic statements (chain-of-thought style), and (7) final reference re-checks and output storage; the pipeline is parameterised to control temperature, penalties and attempt limits.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4-turbo (gpt-4-1106-preview) via OpenAI API</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Sentence-level extraction from full text via EuropePMC (LitScan) followed by embedding-based selection using SentenceTransformers ('all-MiniLM-L6-v2') + UMAP dimensionality reduction + HDBSCAN clustering; greedy exemplar selection when necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Prompt-chained single-document/ multi-statement summarisation with iterative refinement: initial instructive summarization prompt, automated reference checks with rescue prompts, and a self-consistency veracity-check stage that elicits justification and directs revision (combines self-fact-checking and chain-of-thought prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Contexts assembled from ~177,500 papers mentioning the selected RNAs; final outputs produced for 4,618 RNA identifiers representing ~28,700 transcripts and ~4,605 unique RNA sequences</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biomedical literature — non-coding RNA (ncRNA) literature</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Provenance-tracked natural-language literature summaries per RNA (with PMC citation tokens [PMCXXXXXX])</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Automated reference checks (multiple pass criteria), automated self-consistency/veracity checks, human expert ratings on a 1-5 rubric (subset of 50 summaries rated by multiple experts), pass-rate statistics, runtime and monetary cost per summary</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Automated reference-first-pass pass rate 97.9% (4,519/4,618); after automated revision 99.5% (4,594/4,618). Self-consistency stage: 82.7% had no problems on first self-check (3,820/4,618); after revision overall self-consistency pass 91.5% (4,226/4,618). Human evaluation (50-sample subset): 94% of summaries rated good or excellent; average generation time ~29 seconds and cost ~$0.05 per summary.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparisons discussed qualitatively against prior summarization efforts (e.g., T5-style summarizers) and an internal A/B preference test between GPT-3.5 and GPT-4; human curation used as implicit gold standard for evaluation (human expert ratings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Paper reports qualitative and limited A/B improvement of GPT-4 over GPT-3.5 (details in Appendix D) and overall high human ratings versus expectation from prior automated methods; no broad numeric baseline comparison to prior automated pipelines provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Careful prompt engineering combined with automated provenance checks and a self-consistency/fact-check loop can produce high-quality, provenance-linked literature summaries at scale for biomedical entities; most common failures arise when the LLM synthesises across diverse sources, misattributes references, or over-infers conclusions. Self-checking reduces but does not eliminate multi-source synthesis errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LLM hallucination and over-inference, difficulty reliably synthesising information across multiple documents (multi-document synthesis), reference misattribution, sensitivity to context construction (topic-model selection can introduce errors), inability to use closed-access literature (pipeline limited to open-access EU PMC subset), cost and token-window trade-offs, and residual need for human curation for difficult cases.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Pipeline constrained by API token costs and context limits (used 4,096 token model window with 2,560-token contexts for practical cost reasons); authors report improved performance moving from GPT-3.5 to GPT-4, but note increasing model/context size alone may not solve multi-document synthesis problems; topic-modelling selection does not scale cleanly and can introduce errors — authors recommend RAG/sectional decomposition for better scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitSumm: large language models for literature summarization of noncoding RNAs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4402.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4402.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPINDOCTOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPINDOCTOR (Joachimiak et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven tool that generates summaries from gene descriptions to support downstream analyses (e.g., gene set enrichment interpretation); reported to be comparable to standard tools but can miss some important terms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gene Set Summarization using Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SPINDOCTOR</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Tool that takes a set of genes (typically those enriched in an experiment) and uses a large language model to generate a natural-language summary of commonalities across the gene set; designed to aid interpretation of enrichment results rather than provide provenance-tracked literature summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Operates on supplied gene lists/descriptions rather than retrieving sentences from full-text literature (human-derived input); uses prompt-based summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Aggregate summarization of features common across an input gene set (multi-input synthesis into a single summary).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biomedical — gene set interpretation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Natural-language summaries describing shared characteristics of a gene set for enrichment analysis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Comparison against standard enrichment interpretation tools; qualitative assessment of missing terms</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported comparable performance to standard tools but misses some important terms (no detailed numeric metrics provided in this paper's citation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard gene enrichment interpretation tools / existing enrichment analysis summarization methods</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Comparable overall, but with omissions of some important terms (qualitative statement from cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can produce useful summaries for gene sets but may omit salient terms; provenance tracing and consistency checks were not required in that use-case because input was human-derived.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Missed important terms in summaries; input dependence on human-provided gene lists reduces need for provenance but limits automated literature grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitSumm: large language models for literature summarization of noncoding RNAs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4402.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4402.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shaib2023-GPT3-med</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-3 applied to summarisation and synthesis of randomized controlled trial reports, finding coherent summaries but frequent failures in multi-source synthesis and overconfident conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-3 medical evidence summarization (Shaib et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Application of GPT-3 to produce summaries and syntheses of many randomized controlled trial reports; assesses whether GPT-3 can accurately combine evidence across multiple RCTs and produce usable medical evidence summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Multi-document summarization of RCT reports (exact retrieval approach not detailed in this paper's citation summary).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-document evidence synthesis across RCTs into readable summaries and simplified explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Many randomized controlled trial reports (exact number not specified in this paper's citation summary)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Medical literature — randomized controlled trials</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Coherent summaries and simplified syntheses of medical evidence</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Qualitative assessment of synthesis quality; identification of failures to synthesize and over-confidence in conclusions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Found coherent but imperfect summaries; often failed to synthesise information from multiple sources adequately and could be over-confident in conclusions (no precise numeric metrics provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can produce readable summaries of medical trials but struggle to reliably synthesize multi-study evidence and may overstate conclusions, mirroring failure modes seen in LitSumm.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Multi-document synthesis failures, over-confidence in conclusions, need for careful validation and human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitSumm: large language models for literature summarization of noncoding RNAs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4402.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4402.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Visconde</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visconde: Multi-document QA with GPT-3 and Neural Reranking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system combining GPT-3 with neural reranking to perform question-answering across multiple documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visconde: Multi-document QA with GPT-3 and Neural Reranking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Visconde</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multi-document question-answering pipeline that uses neural reranking of candidate passages followed by GPT-3 to generate answers from top-ranked evidence passages.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Neural reranking of retrieved passages (retrieval + rerank) to select evidence for the LLM to condition on.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Answer synthesis from multiple top-ranked document passages (multi-document QA)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Multi-document question answering (general/document QA)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Answer texts to user queries grounded in retrieved passages</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining retrieval + neural reranking with an LLM improves the evidence grounding for multi-document QA tasks (implied from citation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitSumm: large language models for literature summarization of noncoding RNAs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4402.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4402.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CheckYourFacts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments LLM outputs with automated fact-checking against external knowledge sources and uses feedback-driven regeneration to correct errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Check Your Facts and Try Again</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Approach that performs automated fact-checks of LLM outputs against external data/knowledge and uses automated feedback loops to ask the LLM to re-generate corrected outputs, reducing hallucinations and factual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Automated verification against external knowledge sources (fact-checking); uses LLM to propose corrections based on failure signals.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative regeneration with external-knowledge-guided corrections</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General NLP / LLM fact-checking and correction</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Corrected LLM outputs with improved factuality</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automated fact-checking plus feedback-driven regeneration can improve LLM factuality; cited as motivation for LitSumm's self-consistency and rescue-prompt stages.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Potential for feedback hallucination (hallucinations introduced into the feedback prompts), dependency on quality and coverage of external knowledge sources.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitSumm: large language models for literature summarization of noncoding RNAs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4402.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4402.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate reasoning steps from an LLM (chain-of-thought) to improve complex reasoning tasks and support guided revision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompt engineering method that instructs or elicits the model to produce intermediate reasoning or justification steps which can be used to improve correctness, support verifiability, and guide subsequent automated checks or edits.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>General technique applied to instruction-tuned LLMs (used with GPT-4 in LitSumm's self-consistency stage)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Elicits internal reasoning traces to support multi-step revision and factuality checks across statements drawn from multiple documents.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General LLM reasoning and explanation; used in document-grounded tasks</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Intermediate reasoning chains and justifications used to inform final outputs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chain-of-thought style prompts can improve the model's ability to identify and correct inaccurate statements when combined with self-checking; used as part of LitSumm's revision loop.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Risk of producing plausible-sounding but incorrect reasoning; may enable feedback-hallucination if used unguarded in iterative loops.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitSumm: large language models for literature summarization of noncoding RNAs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4402.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4402.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid approach that augments LLM generation with retrieved documents or passages selected by semantic search, recommended as an alternative to large topic-model selection alone for multi-document summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Method that first performs retrieval/semantic search over a document corpus to select relevant passages and then conditions an LLM on those passages when generating answers or summaries; can be applied per-section (e.g., expression, localisation) to reduce multi-document synthesis error.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based semantic retrieval (vector store) to select supporting passages for generation</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>RAG conditions LLM generation on retrieved passages; can be used in a decomposed, hierarchical or section-wise manner to synthesize across documents more robustly than flat topic-model sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General knowledge-intensive NLP tasks and multi-document summarization</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Grounded summaries / answers conditioned on retrieved evidence</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Authors recommend RAG over topic-modelling-only context construction because topic selection can introduce misleading evidence; RAG allows more targeted retrieval by subtopic reducing unsupported synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires reliable semantic retrieval and good passage-level indexing; retrieval quality limits downstream factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitSumm: large language models for literature summarization of noncoding RNAs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4402.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4402.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prometheus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prometheus: Inducing Fine-grained Evaluation Capability in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that supplies a gold-standard answer and a marking rubric to an LLM to induce fine-grained evaluation scoring of other LLM outputs; an example of LLM-as-evaluator methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prometheus: Inducing Fine-grained Evaluation Capability in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Prometheus (LLM-based evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Structured LLM-based evaluation method that provides a rubric and gold standard answers to an LLM so it can produce fine-grained evaluations of candidate outputs; positioned as an automated alternative to human grading for LLM-generated summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Evaluation-by-comparison using rubric and gold answer; not a synthesizer per se but a structured assessment approach for LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Automated evaluation of LLM outputs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automated structured evaluations/grades of LLM-generated content</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Uses provided rubric and gold-standard comparisons to score outputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human expert grading</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-based grading with a rubric can be valuable to scale evaluation of generated summaries without extensive human curation, but developing appropriate rubrics is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Potential unreliability if rubric/gold data are poor; LLM-as-judge limitations and potential biases.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitSumm: large language models for literature summarization of noncoding RNAs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4402.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4402.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Xiao2023-GPT4-synthbio</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative artificial intelligence GPT-4 accelerates knowledge mining and machine learning for synthetic biology (Xiao et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of GPT-4 to knowledge mining and downstream ML tasks in synthetic biology, cited as an example of LLMs being used to mine scientific literature for domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative artificial intelligence GPT-4 accelerates knowledge mining and machine learning for synthetic biology</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 knowledge mining (Xiao et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Work applying GPT-4 to accelerate knowledge mining from synthetic biology literature and to assist machine learning pipeline development; cited as evidence that GPT-4 can be applied to literature mining tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Synthetic biology literature and knowledge mining</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Knowledge-mining outputs to support ML in synthetic biology (summaries/insights)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Example of GPT-4 successfully applied to domain knowledge mining; supports the broader claim that modern LLMs enable literature-level tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitSumm: large language models for literature summarization of noncoding RNAs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gene Set Summarization using Large Language Models <em>(Rating: 2)</em></li>
                <li>Generative artificial intelligence GPT-4 accelerates knowledge mining and machine learning for synthetic biology <em>(Rating: 2)</em></li>
                <li>Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success) <em>(Rating: 2)</em></li>
                <li>Visconde: Multi-document QA with GPT-3 and Neural Reranking <em>(Rating: 2)</em></li>
                <li>Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks <em>(Rating: 2)</em></li>
                <li>Prometheus: Inducing Fine-grained Evaluation Capability in Language Models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4402",
    "paper_id": "paper-265034148",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "LitSumm",
            "name_full": "LitSumm: Large language models for literature summarisation of non-coding RNAs",
            "brief_description": "An end-to-end pipeline that uses a commercial LLM (GPT-4-turbo) together with sentence extraction, topic-based sentence selection, prompt chaining, automated reference checks and self-consistency checking to generate provenance-tracked natural-language summaries of literature for individual ncRNAs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LitSumm",
            "system_description": "Pipeline that (1) finds open-access full-text articles mentioning a target RNA via EuropePMC (LitScan), (2) extracts sentences that mention the RNA and stores them as a context, (3) selects representative sentences using embedding-based topic modelling (SentenceTransformers -&gt; UMAP -&gt; HDBSCAN) and greedy exemplar selection to fit a token budget, (4) generates a first-pass summary with a GPT-4-turbo model using carefully engineered prompts that require in-context citations, (5) applies automated checks on references (adequacy, formatting, presence in context, citation location and citation density) with specific rescue prompts for failures, (6) runs a self-consistency/veracity stage where the LLM classifies each summary bullet as true/false with supporting context and then asks the LLM to revise problematic statements (chain-of-thought style), and (7) final reference re-checks and output storage; the pipeline is parameterised to control temperature, penalties and attempt limits.",
            "llm_model_used": "GPT-4-turbo (gpt-4-1106-preview) via OpenAI API",
            "extraction_technique": "Sentence-level extraction from full text via EuropePMC (LitScan) followed by embedding-based selection using SentenceTransformers ('all-MiniLM-L6-v2') + UMAP dimensionality reduction + HDBSCAN clustering; greedy exemplar selection when necessary.",
            "synthesis_technique": "Prompt-chained single-document/ multi-statement summarisation with iterative refinement: initial instructive summarization prompt, automated reference checks with rescue prompts, and a self-consistency veracity-check stage that elicits justification and directs revision (combines self-fact-checking and chain-of-thought prompting).",
            "number_of_papers": "Contexts assembled from ~177,500 papers mentioning the selected RNAs; final outputs produced for 4,618 RNA identifiers representing ~28,700 transcripts and ~4,605 unique RNA sequences",
            "domain_or_topic": "Biomedical literature — non-coding RNA (ncRNA) literature",
            "output_type": "Provenance-tracked natural-language literature summaries per RNA (with PMC citation tokens [PMCXXXXXX])",
            "evaluation_metrics": "Automated reference checks (multiple pass criteria), automated self-consistency/veracity checks, human expert ratings on a 1-5 rubric (subset of 50 summaries rated by multiple experts), pass-rate statistics, runtime and monetary cost per summary",
            "performance_results": "Automated reference-first-pass pass rate 97.9% (4,519/4,618); after automated revision 99.5% (4,594/4,618). Self-consistency stage: 82.7% had no problems on first self-check (3,820/4,618); after revision overall self-consistency pass 91.5% (4,226/4,618). Human evaluation (50-sample subset): 94% of summaries rated good or excellent; average generation time ~29 seconds and cost ~$0.05 per summary.",
            "comparison_baseline": "Comparisons discussed qualitatively against prior summarization efforts (e.g., T5-style summarizers) and an internal A/B preference test between GPT-3.5 and GPT-4; human curation used as implicit gold standard for evaluation (human expert ratings).",
            "performance_vs_baseline": "Paper reports qualitative and limited A/B improvement of GPT-4 over GPT-3.5 (details in Appendix D) and overall high human ratings versus expectation from prior automated methods; no broad numeric baseline comparison to prior automated pipelines provided.",
            "key_findings": "Careful prompt engineering combined with automated provenance checks and a self-consistency/fact-check loop can produce high-quality, provenance-linked literature summaries at scale for biomedical entities; most common failures arise when the LLM synthesises across diverse sources, misattributes references, or over-infers conclusions. Self-checking reduces but does not eliminate multi-source synthesis errors.",
            "limitations_challenges": "LLM hallucination and over-inference, difficulty reliably synthesising information across multiple documents (multi-document synthesis), reference misattribution, sensitivity to context construction (topic-model selection can introduce errors), inability to use closed-access literature (pipeline limited to open-access EU PMC subset), cost and token-window trade-offs, and residual need for human curation for difficult cases.",
            "scaling_behavior": "Pipeline constrained by API token costs and context limits (used 4,096 token model window with 2,560-token contexts for practical cost reasons); authors report improved performance moving from GPT-3.5 to GPT-4, but note increasing model/context size alone may not solve multi-document synthesis problems; topic-modelling selection does not scale cleanly and can introduce errors — authors recommend RAG/sectional decomposition for better scaling.",
            "uuid": "e4402.0",
            "source_info": {
                "paper_title": "LitSumm: large language models for literature summarization of noncoding RNAs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "SPINDOCTOR",
            "name_full": "SPINDOCTOR (Joachimiak et al.)",
            "brief_description": "An LLM-driven tool that generates summaries from gene descriptions to support downstream analyses (e.g., gene set enrichment interpretation); reported to be comparable to standard tools but can miss some important terms.",
            "citation_title": "Gene Set Summarization using Large Language Models",
            "mention_or_use": "mention",
            "system_name": "SPINDOCTOR",
            "system_description": "Tool that takes a set of genes (typically those enriched in an experiment) and uses a large language model to generate a natural-language summary of commonalities across the gene set; designed to aid interpretation of enrichment results rather than provide provenance-tracked literature summaries.",
            "llm_model_used": null,
            "extraction_technique": "Operates on supplied gene lists/descriptions rather than retrieving sentences from full-text literature (human-derived input); uses prompt-based summarization.",
            "synthesis_technique": "Aggregate summarization of features common across an input gene set (multi-input synthesis into a single summary).",
            "number_of_papers": null,
            "domain_or_topic": "Biomedical — gene set interpretation",
            "output_type": "Natural-language summaries describing shared characteristics of a gene set for enrichment analysis",
            "evaluation_metrics": "Comparison against standard enrichment interpretation tools; qualitative assessment of missing terms",
            "performance_results": "Reported comparable performance to standard tools but misses some important terms (no detailed numeric metrics provided in this paper's citation).",
            "comparison_baseline": "Standard gene enrichment interpretation tools / existing enrichment analysis summarization methods",
            "performance_vs_baseline": "Comparable overall, but with omissions of some important terms (qualitative statement from cited work).",
            "key_findings": "LLMs can produce useful summaries for gene sets but may omit salient terms; provenance tracing and consistency checks were not required in that use-case because input was human-derived.",
            "limitations_challenges": "Missed important terms in summaries; input dependence on human-provided gene lists reduces need for provenance but limits automated literature grounding.",
            "scaling_behavior": null,
            "uuid": "e4402.1",
            "source_info": {
                "paper_title": "LitSumm: large language models for literature summarization of noncoding RNAs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Shaib2023-GPT3-med",
            "name_full": "Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)",
            "brief_description": "Evaluation of GPT-3 applied to summarisation and synthesis of randomized controlled trial reports, finding coherent summaries but frequent failures in multi-source synthesis and overconfident conclusions.",
            "citation_title": "Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)",
            "mention_or_use": "mention",
            "system_name": "GPT-3 medical evidence summarization (Shaib et al.)",
            "system_description": "Application of GPT-3 to produce summaries and syntheses of many randomized controlled trial reports; assesses whether GPT-3 can accurately combine evidence across multiple RCTs and produce usable medical evidence summaries.",
            "llm_model_used": "GPT-3",
            "extraction_technique": "Multi-document summarization of RCT reports (exact retrieval approach not detailed in this paper's citation summary).",
            "synthesis_technique": "Multi-document evidence synthesis across RCTs into readable summaries and simplified explanations.",
            "number_of_papers": "Many randomized controlled trial reports (exact number not specified in this paper's citation summary)",
            "domain_or_topic": "Medical literature — randomized controlled trials",
            "output_type": "Coherent summaries and simplified syntheses of medical evidence",
            "evaluation_metrics": "Qualitative assessment of synthesis quality; identification of failures to synthesize and over-confidence in conclusions",
            "performance_results": "Found coherent but imperfect summaries; often failed to synthesise information from multiple sources adequately and could be over-confident in conclusions (no precise numeric metrics provided here).",
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "LLMs can produce readable summaries of medical trials but struggle to reliably synthesize multi-study evidence and may overstate conclusions, mirroring failure modes seen in LitSumm.",
            "limitations_challenges": "Multi-document synthesis failures, over-confidence in conclusions, need for careful validation and human oversight.",
            "scaling_behavior": null,
            "uuid": "e4402.2",
            "source_info": {
                "paper_title": "LitSumm: large language models for literature summarization of noncoding RNAs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Visconde",
            "name_full": "Visconde: Multi-document QA with GPT-3 and Neural Reranking",
            "brief_description": "A system combining GPT-3 with neural reranking to perform question-answering across multiple documents.",
            "citation_title": "Visconde: Multi-document QA with GPT-3 and Neural Reranking",
            "mention_or_use": "mention",
            "system_name": "Visconde",
            "system_description": "Multi-document question-answering pipeline that uses neural reranking of candidate passages followed by GPT-3 to generate answers from top-ranked evidence passages.",
            "llm_model_used": "GPT-3",
            "extraction_technique": "Neural reranking of retrieved passages (retrieval + rerank) to select evidence for the LLM to condition on.",
            "synthesis_technique": "Answer synthesis from multiple top-ranked document passages (multi-document QA)",
            "number_of_papers": null,
            "domain_or_topic": "Multi-document question answering (general/document QA)",
            "output_type": "Answer texts to user queries grounded in retrieved passages",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Combining retrieval + neural reranking with an LLM improves the evidence grounding for multi-document QA tasks (implied from citation).",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4402.3",
            "source_info": {
                "paper_title": "LitSumm: large language models for literature summarization of noncoding RNAs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "CheckYourFacts",
            "name_full": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
            "brief_description": "A method that augments LLM outputs with automated fact-checking against external knowledge sources and uses feedback-driven regeneration to correct errors.",
            "citation_title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
            "mention_or_use": "mention",
            "system_name": "Check Your Facts and Try Again",
            "system_description": "Approach that performs automated fact-checks of LLM outputs against external data/knowledge and uses automated feedback loops to ask the LLM to re-generate corrected outputs, reducing hallucinations and factual errors.",
            "llm_model_used": null,
            "extraction_technique": "Automated verification against external knowledge sources (fact-checking); uses LLM to propose corrections based on failure signals.",
            "synthesis_technique": "Iterative regeneration with external-knowledge-guided corrections",
            "number_of_papers": null,
            "domain_or_topic": "General NLP / LLM fact-checking and correction",
            "output_type": "Corrected LLM outputs with improved factuality",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Automated fact-checking plus feedback-driven regeneration can improve LLM factuality; cited as motivation for LitSumm's self-consistency and rescue-prompt stages.",
            "limitations_challenges": "Potential for feedback hallucination (hallucinations introduced into the feedback prompts), dependency on quality and coverage of external knowledge sources.",
            "scaling_behavior": null,
            "uuid": "e4402.4",
            "source_info": {
                "paper_title": "LitSumm: large language models for literature summarization of noncoding RNAs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "A prompting technique that elicits intermediate reasoning steps from an LLM (chain-of-thought) to improve complex reasoning tasks and support guided revision.",
            "citation_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "mention_or_use": "mention",
            "system_name": "Chain-of-Thought prompting",
            "system_description": "Prompt engineering method that instructs or elicits the model to produce intermediate reasoning or justification steps which can be used to improve correctness, support verifiability, and guide subsequent automated checks or edits.",
            "llm_model_used": "General technique applied to instruction-tuned LLMs (used with GPT-4 in LitSumm's self-consistency stage)",
            "extraction_technique": null,
            "synthesis_technique": "Elicits internal reasoning traces to support multi-step revision and factuality checks across statements drawn from multiple documents.",
            "number_of_papers": null,
            "domain_or_topic": "General LLM reasoning and explanation; used in document-grounded tasks",
            "output_type": "Intermediate reasoning chains and justifications used to inform final outputs",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Chain-of-thought style prompts can improve the model's ability to identify and correct inaccurate statements when combined with self-checking; used as part of LitSumm's revision loop.",
            "limitations_challenges": "Risk of producing plausible-sounding but incorrect reasoning; may enable feedback-hallucination if used unguarded in iterative loops.",
            "scaling_behavior": null,
            "uuid": "e4402.5",
            "source_info": {
                "paper_title": "LitSumm: large language models for literature summarization of noncoding RNAs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A hybrid approach that augments LLM generation with retrieved documents or passages selected by semantic search, recommended as an alternative to large topic-model selection alone for multi-document summarization.",
            "citation_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "mention_or_use": "mention",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "Method that first performs retrieval/semantic search over a document corpus to select relevant passages and then conditions an LLM on those passages when generating answers or summaries; can be applied per-section (e.g., expression, localisation) to reduce multi-document synthesis error.",
            "llm_model_used": null,
            "extraction_technique": "Embedding-based semantic retrieval (vector store) to select supporting passages for generation",
            "synthesis_technique": "RAG conditions LLM generation on retrieved passages; can be used in a decomposed, hierarchical or section-wise manner to synthesize across documents more robustly than flat topic-model sampling.",
            "number_of_papers": null,
            "domain_or_topic": "General knowledge-intensive NLP tasks and multi-document summarization",
            "output_type": "Grounded summaries / answers conditioned on retrieved evidence",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Authors recommend RAG over topic-modelling-only context construction because topic selection can introduce misleading evidence; RAG allows more targeted retrieval by subtopic reducing unsupported synthesis.",
            "limitations_challenges": "Requires reliable semantic retrieval and good passage-level indexing; retrieval quality limits downstream factuality.",
            "scaling_behavior": null,
            "uuid": "e4402.6",
            "source_info": {
                "paper_title": "LitSumm: large language models for literature summarization of noncoding RNAs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Prometheus",
            "name_full": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models",
            "brief_description": "An approach that supplies a gold-standard answer and a marking rubric to an LLM to induce fine-grained evaluation scoring of other LLM outputs; an example of LLM-as-evaluator methods.",
            "citation_title": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models",
            "mention_or_use": "mention",
            "system_name": "Prometheus (LLM-based evaluation)",
            "system_description": "Structured LLM-based evaluation method that provides a rubric and gold standard answers to an LLM so it can produce fine-grained evaluations of candidate outputs; positioned as an automated alternative to human grading for LLM-generated summaries.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": "Evaluation-by-comparison using rubric and gold answer; not a synthesizer per se but a structured assessment approach for LLM outputs.",
            "number_of_papers": null,
            "domain_or_topic": "Automated evaluation of LLM outputs (general)",
            "output_type": "Automated structured evaluations/grades of LLM-generated content",
            "evaluation_metrics": "Uses provided rubric and gold-standard comparisons to score outputs",
            "performance_results": null,
            "comparison_baseline": "Human expert grading",
            "performance_vs_baseline": null,
            "key_findings": "LLM-based grading with a rubric can be valuable to scale evaluation of generated summaries without extensive human curation, but developing appropriate rubrics is challenging.",
            "limitations_challenges": "Potential unreliability if rubric/gold data are poor; LLM-as-judge limitations and potential biases.",
            "scaling_behavior": null,
            "uuid": "e4402.7",
            "source_info": {
                "paper_title": "LitSumm: large language models for literature summarization of noncoding RNAs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Xiao2023-GPT4-synthbio",
            "name_full": "Generative artificial intelligence GPT-4 accelerates knowledge mining and machine learning for synthetic biology (Xiao et al.)",
            "brief_description": "Application of GPT-4 to knowledge mining and downstream ML tasks in synthetic biology, cited as an example of LLMs being used to mine scientific literature for domain knowledge.",
            "citation_title": "Generative artificial intelligence GPT-4 accelerates knowledge mining and machine learning for synthetic biology",
            "mention_or_use": "mention",
            "system_name": "GPT-4 knowledge mining (Xiao et al.)",
            "system_description": "Work applying GPT-4 to accelerate knowledge mining from synthetic biology literature and to assist machine learning pipeline development; cited as evidence that GPT-4 can be applied to literature mining tasks.",
            "llm_model_used": "GPT-4",
            "extraction_technique": null,
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "Synthetic biology literature and knowledge mining",
            "output_type": "Knowledge-mining outputs to support ML in synthetic biology (summaries/insights)",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Example of GPT-4 successfully applied to domain knowledge mining; supports the broader claim that modern LLMs enable literature-level tasks.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4402.8",
            "source_info": {
                "paper_title": "LitSumm: large language models for literature summarization of noncoding RNAs",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gene Set Summarization using Large Language Models",
            "rating": 2,
            "sanitized_title": "gene_set_summarization_using_large_language_models"
        },
        {
            "paper_title": "Generative artificial intelligence GPT-4 accelerates knowledge mining and machine learning for synthetic biology",
            "rating": 2,
            "sanitized_title": "generative_artificial_intelligence_gpt4_accelerates_knowledge_mining_and_machine_learning_for_synthetic_biology"
        },
        {
            "paper_title": "Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)",
            "rating": 2,
            "sanitized_title": "summarizing_simplifying_and_synthesizing_medical_evidence_using_gpt3_with_varying_success"
        },
        {
            "paper_title": "Visconde: Multi-document QA with GPT-3 and Neural Reranking",
            "rating": 2,
            "sanitized_title": "visconde_multidocument_qa_with_gpt3_and_neural_reranking"
        },
        {
            "paper_title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
            "rating": 2,
            "sanitized_title": "check_your_facts_and_try_again_improving_large_language_models_with_external_knowledge_and_automated_feedback"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models",
            "rating": 2,
            "sanitized_title": "prometheus_inducing_finegrained_evaluation_capability_in_language_models"
        }
    ],
    "cost": 0.0179505,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LitSumm: Large language models for literature summarisation of non-coding RNAs</p>
<p>Andrew Green 
European Molecular Biology Laboratory
European Bioinformatics Institute (EMBL-EBI)
Wellcome Genome Campus
CB10 1SDHinxtonUK</p>
<p>Carlos Ribas 
European Molecular Biology Laboratory
European Bioinformatics Institute (EMBL-EBI)
Wellcome Genome Campus
CB10 1SDHinxtonUK</p>
<p>Nancy Ontiveros-Palacios 
European Molecular Biology Laboratory
European Bioinformatics Institute (EMBL-EBI)
Wellcome Genome Campus
CB10 1SDHinxtonUK</p>
<p>Sam Griffiths-Jones 
Faculty of Biology, Medicine and Health
The University of Manchester
M13 9PTManchesterUK</p>
<p>Anton I Petrov 
Riboscope Ltd
23 King StCB1 1AHCambridgeUK</p>
<p>Alex Bateman 
European Molecular Biology Laboratory
European Bioinformatics Institute (EMBL-EBI)
Wellcome Genome Campus
CB10 1SDHinxtonUK</p>
<p>Blake Sweeney 
European Molecular Biology Laboratory
European Bioinformatics Institute (EMBL-EBI)
Wellcome Genome Campus
CB10 1SDHinxtonUK</p>
<p>LitSumm: Large language models for literature summarisation of non-coding RNAs
D9A80951770DCB2C22092E6B5DBA7618
Curation of literature in life sciences is a growing challenge.The continued increase in the rate of publication, coupled with the relatively fixed number of curators worldwide presents a major challenge to developers of biomedical knowledgebases.Very few knowledgebases have resources to scale to the whole relevant literature and all have to prioritise their efforts.In this work, we take a first step to alleviating the lack of curator time in RNA science by generating summaries of literature for non-coding RNAs using large language models (LLMs).We demonstrate that high-quality, factually accurate summaries with accurate references can be automatically generated from the literature using a commercial LLM and a chain of prompts and checks.Manual assessment was carried out for a subset of summaries, with the majority being rated extremely high quality.We apply our tool to a selection of over 4,600 ncRNAs and make the generated summaries available via the RNAcentral resource.We conclude that automated literature summarization is feasible with the current generation of LLMs, provided careful prompting and automated checking are applied.</p>
<p>Introduction</p>
<p>Curation in life sciences is the process by which facts about a biological entity or process are extracted from the scientific literature, collated and organised into a structured form for storage in a database.This knowledge can then be more easily understood, compared and computed upon.The curation task is a time consuming and often challenging one in which subject matter experts triage literature, select curateable papers and review them for the rich information they provide about a given biological entity (1).Researchers search curated databases (knowledgebases) for information about the entities they are studying and incorporate curated facts into the design of their next study, which may in turn be curated.This virtuous circle is fundamental to the functioning of research in life sciences.</p>
<p>One of the most basic requirements for a researcher is a broad understanding of their molecule of interest.A broad overview is most easily gained from a short summary of the literature.Such summaries are often produced as part of the curation process, for example UniProt (2) gives an overview of a protein's function on its protein entry pages.Similarly, some Model Organism Databases have curator-written descriptions of the genes they contain (e.g.Saccharomyces Genome Database (3) and FlyBase (4)).Summaries are time consuming to produce because there may be a large amount of disparate information to synthesise; because of the difficulty, many databases still do not yet have summaries for all the entities they contain, e.g. the RNAcentral database does not contain summaries for non-coding RNAs (ncRNAs).In addition, human written summaries are prone to become out-dated due to the lack of available curator time.</p>
<p>There are a limited number of curators in the world and the rate of publication and the complexity of the research papers continues to increase.The mismatch between the effort that is required and that which can be applied has led many to use computational techniques at all stages of curation.Natural Language Processing (NLP) has been applied for many years, with cutting edge techniques being used as they become available.However to date these approaches have had limited success.Recently, language models, and in particular Large Language Models (LLMs) have attained sufficient quality to be applicable to curation.Recent efforts have used LLMs to summarise gene sets (5), mine knowledge from synthetic biology literature (6), and other tasks previously done by NLP methods (7).In most cases, LLMs are able to perform remarkably well with little or no fine-tuning training data, opening the potential for their application in resource limited fields.</p>
<p>One field in which the lack of curation effort is particularly acute is ncRNA science.ncRNAs are any RNA transcribed in the cell which does not encode a protein.ncRNAs are critical to the functioning of the cell by forming the core of the ribosome, splicing pre-mRNAs in the spliceosome, and regulating gene expression through microRNAs (miRNAs), long non-coding RNAs (lncRNAs), small nucleolar RNAs (snoRNAs) and many other RNA types.However, as a field, ncRNA has very little curation resource compared to the field of proteins.Rfam (8) and RNAcentral (9) are two of the primary databases in RNA science.Rfam is a database containing over 4,100 RNA families, while RNAcentral is the ncRNA equivalent of UniProt containing over 30 million sequences at the time of writing.Rfam includes curated descriptions of each RNA family.These descriptions are quite general as they describe the function across all organisms in which the family is found.RNAcentral imports data from other resources and as of release 22 contains data from 52 other resources of which 12 provide curated data.RNAcentral has previously made efforts to connect users with the relevant literature with the development of the LitScan tool (described in detail in the Sentence Acquisition section below) to explore the EuropePMC API and extract citations and relevant sentences from the literature.However LitScan still lacks a way to provide a coherent and comprehensive overview of an RNA.As a comprehensive source of information on ncRNA, the RNAcentral database is a natural location for the development of tools to more easily connect users with the ncRNA literature.</p>
<p>In this work, we apply a tool based on GPT4, developed by OpenAI, to produce automated summaries for a large number of ncRNA genes.Summaries are generated from sentences mentioning ncRNAs extracted from the literature, and displayed on the RNAcentral website.We detail our approach to sentence acquisition by exploring the EuropePMC API to allow the extraction of relevant passages.These snippets are then passed through a pipeline of selection, summarisation, automated checking, and automated refinement when necessary, which we named LitSumm.The output of this is 4,618 summaries detailing the literature relating to approximately 28,700 transcripts.A randomly selected subset of 50 summaries representative of RNA type and context size are manually evaluated by four expert raters.</p>
<p>Methods and Materials</p>
<p>RNA selection</p>
<p>To keep costs and computation size within reasonable limits we focus on a subset of RNAs of broad interest to the community.We include RNAs contributed by HGNC (10), miRBase (11), mirGeneDB (12) and snoDB (13).Within these, we identify primary identifiers and aliases as supplied by the source database.</p>
<p>A large fraction of the RNAs we consider are microRNAs (miRNAs) that are associated with a large corpus of scientific literature.Many of these are referred to by identifiers that are not organism specific such as 'mir-21'.Having non-specific identifiers leads to a very large number of papers that must be summarised, across a diverse range of organisms; this can lead to confusing or inaccurate statements about the function on a miRNA in a given organism when the function was actually observed elsewhere.More recently, identifiers including an indication of the species have become more common, in this case for example 'hsa-mir-21' for the human specific miRNA.The difference in the number of papers discussing these identifiers is enormous.To ensure the specificity of summaries, we restrict the IDs used to generate summaries of miRNAs to only those specific to a species.The exception to this rule is for human miRNAs coming from HGNC, which often have the identifiers like 'MIR944', and are included in the set of ncRNAs we summarise.</p>
<p>Large Language Models</p>
<p>Large Language Models are a class of machine learning models that have very large numbers of parameters, hundreds of billions is common, and are adept at predicting the most probable next token given an input sequence.LLMs are built on the transformer architecture (14).This architecture imposes several limits, first it is expensive in memory and compute to operate on large amounts of text.This imposes a limit on the amount of text, called a context length.Secondly, LLMs do not operate on words, but instead tokens.This means, context lengths are always given as the number of tokens that can be fed into a model; a helpful rule of thumb is that a token is approximately 0.75 words, so a 4,096 token context would be approximately 3,000 words (15).</p>
<p>In this work, GPT4-turbo (https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo),an LLM provided by OpenAI is used.Specifically, we use the gpt-4-1106-preview model, through the OpenAI API.The primary parameter controlling the text generation is temperature, T, which alters the sampling distribution of the next token; T=0 would make the model only choose the most likely next token, while higher values allow the model to explore the distribution of next tokens.We use a relatively low T=0.1 (default T=1), a balance between determinism and flexibility to rewrite parts of the context into a coherent summary.Low T also reduces the likelihood of model 'hallucinations', a common problem where the LLM will invent facts (16).</p>
<p>Two other parameters used to control the generation of the model are the presence and frequency penalties.These alter the sampling distribution by adding a penalty to tokens already present in the text, to reduce repetition.They can also be used to encourage reuse by giving negative values.We use a presence penalty of -2 in the initial summary generation call, to ensure the model re-states tokens from the context in the summary, but with a frequency penalty of 1 to avoid repetition.All operations involving the LLM are abstracted using the langchain python library (https://github.com/hwchase17/langchain).</p>
<p>Sentence Acquisition</p>
<p>To gather what is being said about an RNA in the literature, we explore the EuropePMC API using a query designed to find articles discussing non-coding RNA while minimising false positives.The query used is 'query=("<RNA ID>" AND ("rna" OR "mrna" OR "ncrna" OR "lncrna" OR "rrna" OR "sncrna") AND IN_EPMC:Y AND OPEN_ACCESS:Y AND NOT SRC:PPR)', with the collection of terms in parentheses aiming to filter out false positives that mention the ID but not a type of ncRNA; the query also explicitly requires open access and excludes preprints.We restrict this search to the open access subset at EuropePMC such that we can access and re-use the full text, aside from this no other restrictions are placed on the articles we retrieve or use.RNAcentral's comprehensive and regularly updated collection of cross-references between RNA resources enables us to identify papers that refer to the same RNA using different names or identifiers.</p>
<p>Once articles about an RNA have been identified, the full text is retrieved and searched to 1) validate that the ID is mentioned in the article and 2) extract sentences that mention the ID.The identified article PMCIDs and contained sentences are stored in a database at RNAcentral.The results of this can be seen on RNAcentral, where the tool is referred to as LitScan (https://rnacentral.org/help/litscan) and has an interface allowing users to explore the results (for example: https://rnacentral.org/rna/URS000075D66B/9606?tab=pub).In this work, we use LitScan as a source of statements about RNAs which can be used to provide an overview of the literature about them.</p>
<p>Sentence Selection</p>
<p>Not all ncRNAs are studied equally.For many, we know about their existence only because they have been sequenced and deposited in sequence archives such as ENA (17); for these RNAs, we have no papers to summarise.A significant subset of RNAs appear in only a few articles where their existence is established, and occasionally some aspect of function, localisation or other information is determined.To ensure a reasonable amount of information for the LLM to summarise, we restrict the lower bound of sentence count to 5.These 5 sentences could come from a single paper, which allows summarisation of single papers that present the only source of information about an RNA.</p>
<p>Above this threshold, there are two factors driving the selection of sentences from which to summarise: context length and information coverage.For LitSumm, we restrict ourselves to a 4,096 token context, and impose a limit of 2,560 tokens (approximately 1920 words) in the context to allow for prompting and revisions.We limited ourselves to a 4,096 token context for two reasons: feasibility of downstream finetuning of an open LLM; and cost -using the full 128k token window of GPT4 would be prohibitively expensive, since API calls are charged per token.The limit of 2,560 is arrived at by considering the length of output summaries (~255 tokens), prompts (60-140 tokens), and the number of tokens required to send a summary for revision (~1,150) for a random subset of RNAs.For the majority of ncRNAs, the total available sentences fall within this context limit, so no selection is applied beyond the 5 sentence lower limit.</p>
<p>For some ncRNAs such as well studied miRNAs (e.g.hsa-mir-191) and snoRNAs (e.g.SNORD35A) among others, totalling 1704 RNAs, we find too many sentences to use them all, meaning a selection step is necessary.To select sentences, we apply a topic modelling approach (18).We used the SentenceTransformers package (19), with the pretrained 'all-MiniLM-L6-v2' model which embeds each sentence into a 384 dimensional vector, the dimensionality of its output layer.Then, the UMAP dimensionality reduction technique ( 20) is applied to reduce the vector dimension to 20, and the HDBSCAN clustering algorithm (21) produces clusters of similar sentences.While 384 is not a particularly high dimensionality, it has been shown that reducing dimensionality significantly improves clustering performance across a variety of tasks (22); we choose a dimensionality of 20 such that we can use the fast_hdbscan python library (https://github.com/TutteInstitute/fast_hdbscan),for which 20D is the maximum recommended dimensionality.Cluster exemplars were sampled in a round-robin fashion until the context was filled to ensure a broad coverage of topics.In the case where all exemplars did not fill the context, sentences were sampled from the clusters themselves in the same round-robin way.</p>
<p>An important minority of ncRNAs are very heavily studied.These include ncRNAs like XIST, MALAT1 and NEAT1, each of which appear in thousands of articles.In these cases, our selection technique still results in too many tokens, so we apply a greedy selection algorithm to the cluster exemplars.An exemplar is selected in the largest cluster, then the vector embedding is used to calculate the similarity to all exemplars in other clusters.The exemplar least similar to the selected exemplar is selected, and the process continues by evaluating the distance from all selected exemplars.The process repeats until the context is filled.</p>
<p>For some RNAs, there were too many sentences to use all, but not enough to apply the topic modelling approach.In this case, the sentences were sorted in descending order of tokenised length, and the first k-sentences were taken such that the context was filled.</p>
<p>In all cases, no criteria are applied to the selection of sentences to use in building the summary beyond them having a direct mention of the RNA being summarised.</p>
<p>Prompts</p>
<p>One of the most critical criteria for a scientific summary is that it contains only factual information.Additionally, tracing the provenance of statements in the summary is important for verifiability.We have designed a chain of prompts through iterative refinement on a subset of examples with these objectives in mind.The first prompt generates the summary, and if there are problems, subsequent prompts attempt to guide the LLM into rectifying them.</p>
<p>The first prompt is shown in Figure 1, panel A. Here, the model is instructed several times to use references, and the style of reference desired, with an example.The LLM is further instructed not to use 'external sources'; this aims to stop the LLM inserting any facts that are not present in the context.While these facts could be accurate, there is no way of finding out where they come from, and they risk being inaccurate, which we try to avoid at all costs.These instructions, combined with the sampling parameters reduce the likelihood of the model inserting facts not present in the context.</p>
<p>After the summary is returned from the model, references are evaluated.This consists of five checks, any one of which can trigger a re-generation of the summary.The five checks are:</p>
<ol>
<li>Adequacy of references -are there enough references for the number of sentences in the summary?We require at least 0.5 references per sentence.2. Formatting of references -We require the model to cite sentences by using PubMed</li>
</ol>
<p>Central identifiers [PMCXXXXXX].3. Realness of references -Are all the references in the summary present in the context?This should catch cases where the model has invented a PMCID.4. Location of references -references should be at the end of sentences usually.This is intended to stop the model from putting all references at the end of the summary and not indicating which statement comes from which reference.</p>
<p>Number of references per instance -this check catches the model putting many</p>
<p>PMCIDs into a single pair of brackets, which is undesirable for the purposes of provenance checking.We require no more than 50% of the total number of references in any given citation.</p>
<p>Each check has a specific 'rescue' prompt which is applied when the summary makes the particular mistake.There are four of these, shown in Figure A1 in the supplementary material.To keep costs and computation time within four hundred dollars and a total run-time of 1 day, a maximum of 4 attempts are given to produce a summary.If the summary is still not produced after these attempts, it is flagged as potentially problematic.Provided the summary passes the check within the limit of 4 attempts, it continues to the next stage.</p>
<p>Once all reference based checks have passed, the accuracy of the summary is evaluated.</p>
<p>To do this, the summary is broken into a bulleted list, and provided alongside the original context.The model is instructed to state whether each bullet is true or false based on the context, and to find the support in the context.Importantly we not only ask for a true/false, but also ask for an explanation of why.When a summary contains a misleading or false statement, the output of this step, along with the summary, is fed back to the model which is instructed to amend the summary accordingly.These two steps combine approaches to LLM self-fact checking (23) and chain-of-thought prompting (24).In combination, these improve summaries.The prompts used in these stages are shown in Figure 1, panel B. Once this stage is complete, the summary is given a final reference check, and if successful the summary is considered finished.An overview figure of the whole LitSumm tool is shown in Figure 2.</p>
<p>Human and Automated Assessment</p>
<p>To evaluate the quality of the output, a subset of the summaries was assessed in parallel by four reviewers.These reviewers were chosen from the coauthors and were specifically selected to represent diverse academic backgrounds, including expertise in data curation, RNA biology, and machine learning.A subset of 50 summaries was randomly selected, stratified by context length, and loaded into a web platform to provide feedback.Figure B1 in supplementary materials shows a screenshot of the web platform used.The distribution of summaries over RNA types is shown in Table 1.Summaries were presented alongside the context from which they were generated to allow the raters to evaluate the claims made in the summary.The ratings were given on a 1-5 scale based on the rubric shown in Table B1 in supplementary materials.Briefly: a rating of 1 would indicate multiple serious problems with a summary (e.g.fake references, inaccurate statements, etc); 2 indicates at most two misleading/incorrect statements or one serious error; 3 indicates an acceptable summary with at most one minor misleading/incorrect statement; 4 indicates a summary with no incorrect/misleading statements but with other problems such as poor flow; and 5 would indicate an excellent summary (all statements referenced and true, good flow, etc).All summaries rated 3 and above must have correct, adequate references.Raters were asked to score a summary based only on the information in the context, not using any extra information from the linked articles, or their own knowledge.We also provided a series of tickboxes designed to identify particular failure modes, these are also shown in Table B2 of the supplementary materials.</p>
<p>Raters 0-2 were involved in the design of the rubric, and had the same training with the tool.Experience in RNA science differed between the raters, with Raters 0 and 1 being more experienced than Rater 2, and Rater 0 being a professional curator.Rater 3 was not involved in the development of the summarisation or assessment tools, and received written training to use the rubric before completing their rating session.</p>
<p>Results</p>
<p>We focused on RNAs from authoritative databases including HGNC, miRBase, mirGeneDB, and snoDB, with particular attention to unambiguous identifiers to ensure accurate functional attribution.From these databases, an initial set of 4,618 RNA identifiers were selected for summarisation.Our pipeline extracted relevant sentences from open access papers in EuropePMC using RNA-specific search queries.Representative sentences were selected using topic modelling and sampling to maintain information diversity.These selected sentences were then processed through GPT-4-turbo using a prompt chain that enforces factual accuracy and proper citation.Each generated summary underwent multiple rounds of self-consistency checking and refinement, validating both reference formatting and factual accuracy against the source material.A subset of the final summaries were evaluated by a panel of experts across multiple domains including RNA biology, data curation, and machine learning.</p>
<p>The 4,618 RNA identifiers selected for summarisation represent a coverage of approximately 28,700 transcripts and 4,605 unique RNA sequences in RNAcentral, and approximately 177,500 papers containing the identifiers.The distribution of RNA types is shown in Figure 3.The majority of RNAs come from the RNA type specific databases miRBase, mirGeneDB and snoDB, who provide miRNAs and snoRNAs; all lncRNAs come from HGNC and are therefore only those found in humans.The small number of 'other' type RNAs are from HGNC including for example rRNAs, RNAses and some RNAs with imprecise type labels such as the generic ncRNA.As expected from the chosen databases, the majority of the RNAs selected are human, with non-human RNAs coming primarily from miRBase and mirGeneDB.</p>
<p>The full generation process for each summary, including the automated checking, consistency checking and all revisions took on average 29 seconds and cost $0.05.These values are estimated from the total time to generate all 4618 summaries, and the total cost across the generation period as recorded by OpenAI for billing.An example summary is shown in Figure 4, and all summaries can be browsed by going to the RNAcentral website and searching 'has_litsumm:"True"' (https://rnacentral.org/search?q=has_litsumm:%22True%22).We also make the entire dataset of contexts and their summaries available online at https://huggingface.co/datasets/RNAcentral/litsumm-v1.5.</p>
<p>Figure 4: Example summary generated by the tool. This example is a lncRNA, examples for other RNA types can be found in supplementary materials, appendix C.</p>
<p>Automated checking, primarily of reference adequacy and accuracy identified problems in 2.1% of summaries which were adequately rectified within the four allowed revisions in 76% of cases, meaning overall 99.5% of summaries passed our automated checks.The self-consistency check identified problems in 17.3% of summaries, which were rectified in 51% of cases giving an overall pass rate of 91.5%.The pass rates at each stage are shown in Table 2.An example of the type of error identified and rectified by the consistency check is shown in Figure 5. Human evaluation was carried out for a subset of randomly sampled RNAs.These RNAs cover the full range of context sizes and ncRNA types.The subset consists of 50 RNAs in total, for which three raters assessed quality.21 of these 50, covering only miRNA, were also scored by an expert in miRNA (Rater 3).The human rating, on a scale of 1-5 with 5 being excellent and 1 indicating the presence of some serious failure is shown for all raters in Figure 6.From the human ratings, 94% of summaries were rated good or excellent.In the majority of cases where a summary was rated inadequate (score of 2 or less) the problem identified by the raters had to do with poor synthesis of facts from multiple sources not caught by the automated consistency check, or reference misattribution, where a reference for a given sentence does not match the information content, or is irrelevant.Reference errors are penalised strongly in the marking rubric, as are misleading statements.LLMs are known to struggle to accurately combine facts across different documents (25).This has been observed in previous studies of multi-document summarizations and may be connected to input construction.A summary of the failures identified is shown in Our analysis of the literature summaries revealed several notable issues with the performance of the LLM.Firstly, the model struggled with synthesising information from diverse sources, leading to unwarranted inferences and a lack of support for its claims.</p>
<p>Additionally, we observed instances of reference misattribution, where unrelated or incorrect references were added to sentences, undermining the ease of verification of the summaries.Furthermore, the model exhibited a tendency to over-infer and make unwarranted connections, often overstating the significance of findings, such as prematurely identifying biomarkers.We also noted cases of unsupported expansions, where the model introduced information that was not supported by the original text; for example expanding DFU as 'Diabetic Foot Ulcer' when the provided context made no mention of diabetes, feet or ulcers.Finally, issues with flow and coherence were evident, including the inclusion of internal instructions and inappropriate recommendations.These findings highlight the need for continued refinement and evaluation of LLMs in generating accurate and reliable summaries of scientific literature.</p>
<p>Summaries are available on RNAcentral by exploring results of the search 'has_litsumm:"True"' (https://rnacentral.org/search?q=has_litsumm:%22True%22), or by exploring the "Literature Integration" facet and selecting "AI generated summaries".We also make the entire dataset of contexts and their summaries available online at https://huggingface.co/datasets/RNAcentral/litsumm-v1.5.Summaries will be updated with each RNAcentral release by re-running the LitSumm pipeline, including re-running LitScan to identify new literature; future releases may move to the use of a local model.Previous versions of the summaries will be versioned and made available, including metadata on the generation, such as the LLM model and sentence selection techniques used.Future work will include moving to a fine-tuned local model, which will allow expansion to more ncRNAs, and a more even taxonomic coverage without significant additional cost.</p>
<p>Discussion</p>
<p>In this work we present an application of LLMs to perform literature curation for ncRNA.We show that a pipeline with a series of automated checks and carefully designed prompts can produce high-quality literature summaries.We also demonstrate techniques to minimise untrue information, and ensure high quality referencing in the summary.</p>
<p>Human ratings of a representative subset of the summaries generated have been collected, and show that the majority of summaries are of high or very high quality, with a small number of common failure modes.The identified failure modes primarily fall into two categoriesrelating to referencing, and relating to information synthesis/inference from multiple sources.</p>
<p>The size of the fully annotated set is small, at only 50 summaries, but thorough, with three of the raters rating all 50 summaries.This is in line with the sizes of other multi-rated summarization datasets ( 26), but with a greater overlap (100% vs 20% in Wang et al.).</p>
<p>Collecting multiple ratings for each summary improves our sensitivity to nuanced errors at the cost of coverage.The failure modes we identified were consistent across instances, and picked up by all raters; however, more nuanced errors were identified in context-specific cases and not picked up by all raters.We are confident that our ratings identified the nuanced errors made by the LLM, but given the small coverage, identifying a clear pattern to these errors is difficult.It may be preferable to allocate rating time differently to maximise coverage with some minimal overlap to assess consistency.</p>
<p>In this work, the human ratings have been performed by four of the co-authors of the paper for all summaries.Having authors rate summaries introduces a conflict of interest in that the authors have a vested interest in the ratings being 'good'.To mitigate the potential for bias, we employed a grading rubric and multiple rating for all summaries, and further asked an external expert to rate summaries in their own field of expertise (also a co-author).We believe the bias is minimal, evidenced by the low ratings for poor quality summaries given by all raters.In future it would be preferable to have additional independent ratings.</p>
<p>Summarisation is a task which has been approached by language models previously, such as the T5 architecture (27).While these models do perform well on summarisation and other tasks, they are not as general purpose as a modern, instruction tuned LLMs which are often equally adept at summarisation.As such, basing the LitSumm architecture on a single driving model simplifies the tool.LLM driven summarisation has been done in several other fields.For example, Joachimiak et al. developed a similar tool, SPINDOCTOR, which is used to generate a summary from gene descriptions; the summary is then used in a gene enrichment analysis (5).Joachimiak et al. evaluate the results of their gene enrichment against standard tools and find their method is comparable, though it misses some important terms.SPINDOCTOR differs from LitSumm in that the input is a set of genes known to be enriched in an experiment, and the output is a summary of their commonalities, whereas LitSumm produces a broad overview summary about a single RNA from many literature-derived statements.SPINDOCTOR also does not need to assess the consistency of their summary with the context from which it is generated, and do not give the provenance of statements, since their input is human-derived.</p>
<p>One field in which similar considerations have to be made is medicine, where the accuracy and provenance of statements is paramount.Shaib et al. evaluate GPT 3 for the summarization and synthesis of many randomised controlled trial reports.They find that while the LLM produces coherent summaries, it often fails to synthesise information from multiple sources adequately, and may be over-confident in its conclusions (28).In our evaluation we find similar failure modes, where the model misunderstands statements where it tries to synthesise information from more than one source.</p>
<p>A key aspect of our pipeline is the use of self consistency checking and revision using chain-of-thought prompting.These two concepts have been applied in other contexts, such as question answering over documents (29), but have yet to be applied to literature curation.Despite our best efforts to reduce hallucinations and ensure wholly factual summaries, around 17% of cases still have some problems, indicating the need for consistency checking.Feeding the output of the self checking back into the model reduces this to 8.5%, which is encouraging, but also indicates the need for human intervention in this complex field where LLMs still struggle to fully comprehend scientific literature.In particular, the consistency check developed here is not effective at identifying inferences made by the LLM that are incorrect, because there is 'indirect' support in the context.There is also the possibility of 'feedback hallucination' in which the generated instructions to rescue a summary contain a hallucination, which if not checked will be inserted by the LLM as it follows its own instructions.As LLMs become stronger, and have more advanced reasoning capabilities, this will become an increasingly problematic failure mode; the detection of errors of this sort is an area of active research in the NLP field generally.</p>
<p>We investigated the observed errors in the produced summaries, finding that the majority of poorly rated summaries share several common problems.Namely, we observe incorrect reference attribution, the inclusion of irrelevant details while missing important information, and statements unsupported by the context as the most common problems.More details of this error analysis can be found in Supplementary materials Table B2 and Appendix C.</p>
<p>Another limitation relates to the literature itself, and here is primarily seen with miRNAs.Many gene names or identifiers are ambiguous in that they can be used to refer to multiple organisms, or may conflate the mature product and precursor hairpin.We have restricted ourselves to species specific IDs (e.g.hsa-mir-126), meaning the generated summaries should be consistent and limited to a single organism, but a significant fraction of the literature does not use these IDs.Thus, we are missing information.We could use a broader set of identifiers, but then we must be able to distinguish which species is being discussed in each paper.There are ways this could be addressed -for example using the ORGANISMS database (30) to identify which organism a given article is about and then use this information to produce organism specific summaries, despite the usage of non-specific terms.However, the accuracy of such resources is questionable, meaning we do not know which organism a paper discusses at present.We leave this problem as future work.</p>
<p>Other automated assessment methods have been developed that use another LLM to give a rating to the output of an LLM.This can be done with very little guidance as in single answer grading, described by (31), where GPT4 is simply asked to grade output, or in a much more guided and structured way as in Prometheus (32) where a gold standard answer and marking rubric are provided to the model.While LLM rating approaches have not been applied here, such a tool would be valuable to ensure the quality of summaries without extensive human curation.However, the development of a suitable rubric is not straightforward; we plan to approach this problem in a future work.</p>
<p>One limitation that will be difficult to address is the openness of literature.Our sentences come from the open-access subset of EuropePMC.While this data source is growing as more authors publish open-access, it still does not allow access to the majority of knowledge, particularly that from earlier decades.Many knowledgebases make extensive use of closed-access literature in their curation; their primary concern is the quality of the information being curated, not the availability of the information, therefore the open access status of a paper is not an impediment to its being curated.However, the inability of this tool, and those which will doubtless come after it, to use closed-access literature does highlight the need for authors, institutions and funders to push for open access publication with a permissive licence for reuse.</p>
<p>Often there is too much literature available to feed all of it into an LLM to generate a summary.Recently, LLMs have been getting considerably larger context sizes, for example GPT4 can now accept up to 128k tokens.However, this is unlikely to be a solution in itself; LLMs do not attend to their entire context equally (33), and having a larger context and expecting the LLM to use it all is unlikely to work, though some recent work has shown that this may be soluble (34).In this work, topic modelling is used to reduce the amount of text to be summarised.This introduces problems related to the context construction that lead to inaccurate sentences being generated by the LLM.Worse, the automated fact checking is blind to this type of failure, due to there being 'evidence' in the context which supports the inaccurate sentence.Therefore, we would recommend against the use of topic modelling alone to generate input context for an LLM, since it likely introduces more problems than alternative approaches such as vector store based retrieval augmented generation (RAG).A better approach may be to decompose the summary into sections and apply a RAG (35) approach to each in turn by applying semantic search for only passages about, for example, expression.</p>
<p>The field of LLM research is moving extremely rapidly and we expect that significant improvements will be possible in our pipeline simply by adopting newer LLM technology.Our current work is based on GPT4, having been originally developed with GPT3.5; this allows us to see the improvement in LLM technology, which we show in Appendix D with a brief A/B preference test between GPT3.5 and 4, and examination of pass rates through our pipeline.</p>
<p>Moving to openly available models could enable future work on fine-tuning the LLM for the biological summarisation task.</p>
<p>Conclusion</p>
<p>In conclusion, we have demonstrated that LLMs are a powerful tool for the summarisation of scientific literature and, with appropriate prompting and self checking, can produce summaries of high quality with adequate references.Using the tool developed here, 4,618 high quality summaries have been provided for RNAcentral, providing natural language summaries for these RNAs where none previously existed.This is the first step to automating the summarisation of literature in ncRNAs, and providing helpful overviews to researchers.</p>
<p>Figure 1 :
1
Figure 1: A: The initial prompt used to generate a first pass summary from the generated context.Variables are enclosed in {} and are replaced with their values before sending the prompt to the LLM.B: Prompts used for the self-consistency checking stage including inaccurate statement detection and revision.All prompts are reproduced as plain text in Supplementary Materials A.</p>
<p>Figure 2 :
2
Figure 2: A flow diagram of the whole LitSumm tool.Information from the EuropePMC API flows from the left to the right, through a sentence selection step before several rounds of self checking and refinement.Finished summaries are written to disk before being uploaded to the RNAcentral database en-masse.</p>
<p>Figure 3 :
3
Figure 3: The distribution of RNA types selected for summarization.</p>
<p>Figure 5 :
5
Figure 5: Example output of the veracity checker.In this case, CTBP1-DT presents two sentences validated as TRUE and two FALSE sentences.The offending sentences have been removed by the model in the final summary.</p>
<p>Figure 6 :
6
Figure 6: The average rating per summary across all raters.Note that Rater 3 gave scores only for a subset of 21 miRNAs.</p>
<p>Table 1 :
1
Distribution of RNA types for the 50 summaries reviewed.Since we primarily sampled according to the total number of tokens available for each RNA, without considering the type, this distribution broadly follows the distribution of RNA types in the whole dataset.
RNA typeCountlncRNA23pre_miRNA11miRNA10snoRNA4other2</p>
<p>Table 2 :
2
Pass rates of the automated checking and self-consistency checking stages in the LitSumm pipeline.The table shows the percentage and number of summaries that pass each stage, including revision stages.
Failure modePass rateNumberofpassingsummariesReferences -first pass97.9%4,519References -after revision99.5%4,594Self-consistency-no82.7%3,820problems foundSelf-consistency-no91.5%4,226problems after revision</p>
<p>Table 3
3
, and the best and worst rated summaries per RNA type are shown in Supplementary Materials Appendix C. but not displayed.This means that from a total of 4,605 unique RNA sequences, 4,602 have a displayed summary on the RNAcentral website</p>
<p>Summaries rated inadequate by human inspection are retained, but not displayed on RNAcentral.Similarly, where problems were identified by automated checks, e.g.failing to insert correct references, or automatically detected self consistency errors, the summaries are retained</p>
<p>Table 3 :
3
Reasons given for poor rating in those cases where a rating less than 3 was given.
Total (out of 50ReferenceHallucination/falseRatersummaries)FormattinginfoOtherRater 0 9491Rater 1 5032Rater 2 7333Rater 3 3/2101/214/21
to produce these summaries is available here: https://github.com/RNAcentral/litscan-summarizationand the dataset of contexts and summaries is available here: https://huggingface.co/datasets/RNAcentral/litsumm-v1.5.Summaries are also available on the RNA report pages in RNAcentral (https://rnacentral.org/) Database URL: https://rnacentral.org/AvailabilityCode usedAvailabilityAll code used to produce these summaries can be found here: https://github.com/RNAcentral/litscan-summarizationand the dataset of contexts and summaries can be found here: https://huggingface.co/datasets/RNAcentral/litsumm-v1.5.Summaries are also displayed on the RNA report pages in RNAcentral (https://rnacentral.org/) and can be explored by searching with the query 'has_litsumm:"True"' (https://rnacentral.org/search?q=has_litsumm:%22True%22).Funding This project has received funding from the European Union's Horizon 2020 Marie Skłodowska-Curie Actions under grant agreement No 945405; and from Wellcome Trust [218302/Z/19/Z].For the purpose of open access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.AB and BS are funded by core EMBL funds.Conflict of Interest: A.B. is an Editor at DATABASE, but was not involved in the editorial process of this manuscript.
Biocuration: Distilling data into knowledge. PLoS Biol. 16e20028462018International Society for Biocuration</p>
<p>UniProt: the Universal Protein Knowledgebase in 2023. Nucleic Acids Res. 512023</p>
<p>Saccharomyces genome database update: server architecture, pan-genome nomenclature, and external resources. E D Wong, S R Miyasato, S Aleksander, Genetics. 2242023</p>
<p>FlyBase: updates to the Drosophila melanogaster knowledge base. A Larkin, S J Marygold, G Antonazzo, Nucleic Acids Res. 492021</p>
<p>Gene Set Summarization using Large Language Models. M P Joachimiak, J H Caufield, N L Harris, 2023ArXiv</p>
<p>Generative artificial intelligence GPT-4 accelerates knowledge mining and machine learning for synthetic biology. Generative artificial intelligence GPT-4 accelerates knowledge mining and machine learning for synthetic biology. Z Xiao, W Li, H Moon, 2023.06.14.544984bioRxiv. 2023. 2023</p>
<p>A Comprehensive Benchmark Study on Biomedical Text Generation and Mining with ChatGPT. A Comprehensive Benchmark Study on Biomedical Text Generation and Mining with ChatGPT. Q Chen, H Sun, H Liu, 2023.04.19.537463bioRxiv. 2023. 2023</p>
<p>Rfam 14: expanded coverage of metagenomic, viral and microRNA families. I Kalvari, E P Nawrocki, N Ontiveros-Palacios, Nucleic Acids Res. 492021</p>
<p>RNAcentral 2021: secondary structure integration, improved sequence search and new member databases. Nucleic Acids Res. 492021</p>
<p>R L Seal, B Braschi, K Gray, Genenames.org: the HGNC resources in 2023. 202351</p>
<p>miRBase: from microRNA sequences to function. A Kozomara, M Birgaoanu, S Griffiths-Jones, Nucleic Acids Res. 472019</p>
<p>MirGeneDB 2.1: toward a complete sampling of all major animal phyla. B Fromm, E Høye, D Domanska, Nucleic Acids Res. 502022</p>
<p>) snoDB 2.0: an enhanced interactive database, specializing in human snoRNAs. D Bergeron, H Paraqindes, É Fafard-Couture, Nucleic Acids Res. 512023</p>
<p>Attention Is All You Need. Attention Is All You Need. A Vaswani, N Shazeer, N Parmar, arXiv [cs.CL2017. 2017</p>
<p>Language Models are Few-Shot Learners. T B Brown, B Mann, N Ryder, arXiv [cs.CL]Language Models are Few-Shot Learners. 2020. 2020</p>
<p>Survey of Hallucination in Natural Language Generation. Survey of Hallucination in Natural Language Generation. Z Ji, N Lee, R Frieske, arXiv [cs.CL]2022. 2022</p>
<p>The European Nucleotide Archive in 2022. J Burgin, A Ahamed, C Cummins, Nucleic Acids Res. 512023</p>
<p>BERTopic: Neural topic modeling with a class-based TF-IDF procedure. BERTopic: Neural topic modeling with a class-based TF-IDF procedure. M Grootendorst, arXiv [cs.CL]2022. 2022</p>
<p>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. N Reimers, I Gurevych, arXiv [cs.CL]Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. 2019. 2019</p>
<p>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. L Mcinnes, J Healy, J Melville, arXiv [stat.ML]2018. 2018</p>
<p>Accelerated Hierarchical Density Based Clustering. L Mcinnes, J Healy, IEEE International Conference on Data Mining Workshops (ICDMW). 2017. 2017</p>
<p>Considerably improving clustering algorithms using UMAP dimensionality reduction technique: A comparative study. M Allaoui, M L Kherfi, A Cheriet, Lecture Notes in Computer Science. 2020Springer International PublishingLecture notes in computer science</p>
<p>Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback. Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback. B Peng, M Galley, P He, arXiv [cs.CL]2023. 2023</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, D Schuurmans, arXiv [cs.CL]2022. 2022</p>
<p>Do Multi-Document Summarization Models Synthesize? Do Multi-Document Summarization Models Synthesize?. J Deyoung, S C Martinez, I J Marshall, arXiv [cs.CL]2023. 2023</p>
<p>Automated metrics for medical multi-document summarization disagree with human evaluations. Automated metrics for medical multi-document summarization disagree with human evaluations. L L Wang, Y Otmakhova, J Deyoung, arXiv [cs.CL]2023. 2023</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. C Raffel, N Shazeer, A Roberts, arXiv [cs.LG]2019. 2019</p>
<p>Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success). Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success. C Shaib, M L Li, S Joseph, arXiv [cs.CL]2023. 2023</p>
<p>Visconde: Multi-document QA with GPT-3 and Neural Reranking. Visconde: Multi-document QA with GPT-3 and Neural Reranking. J Pereira, R Fidalgo, R Lotufo, arXiv [cs.CL]2022. 2022</p>
<p>The SPECIES and ORGANISMS Resources for Fast and Accurate Identification of Taxonomic Names in Text. E Pafilis, S P Frankild, L Fanini, PLoS One. 8e653902013</p>
<p>L Zheng, W.-L Chiang, Y Sheng, arXiv [cs.CL]Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. 2023. 2023</p>
<p>Prometheus: Inducing Fine-grained Evaluation Capability in Language Models. Prometheus: Inducing Fine-grained Evaluation Capability in Language Models. S Kim, J Shin, Y Cho, arXiv [cs.CL]2023. 2023</p>
<p>Lost in the Middle: How Language Models Use Long Contexts. N F Liu, K Lin, J Hewitt, arXiv [cs.CL]Lost in the Middle: How Language Models Use Long Contexts. 2023. 2023</p>
<p>Effective Long-Context Scaling of Foundation Models. Effective Long-Context Scaling of Foundation Models. W Xiong, J Liu, I Molybog, arXiv [cs.CL]2023. 2023</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. P Lewis, E Perez, A Piktus, arXiv [cs.CL]2020. 2020</p>            </div>
        </div>

    </div>
</body>
</html>
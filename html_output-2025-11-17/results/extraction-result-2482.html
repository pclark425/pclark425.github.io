<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2482 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2482</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2482</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-245124375</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2112.06649v2.pdf" target="_blank">Hypothesis Learning in Automated Experiment: Application to Combinatorial Materials Libraries</a></p>
                <p><strong>Paper Abstract:</strong> Machine learning is rapidly becoming an integral part of experimental physical discovery via automated and high-throughput synthesis, and active experiments in scattering and electron/probe microscopy. This, in turn, necessitates the development of active learning methods capable of exploring relevant parameter spaces with the smallest number of steps. Here we introduce an active learning approach based on co-navigation of the hypothesis and experimental spaces. This is realized by combining the structured Gaussian Processes containing probabilistic models of the possible system's behaviors (hypotheses) with reinforcement learning policy refinement (discovery). This approach closely resembles classical human-driven physical discovery, when several alternative hypotheses realized via models with adjustable parameters are tested during an experiment. We demonstrate this approach for exploring concentration-induced phase transitions in combinatorial libraries of Sm-doped BiFeO3 using Piezoresponse Force Microscopy, but it is straightforward to extend it to higher-dimensional parameter spaces and more complex physical problems once the experimental workflow and hypothesis-generation are available.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2482.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2482.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis Learning (co-navigation of hypothesis and experimental spaces)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active learning framework that jointly navigates experimental parameter space and a discrete set of probabilistic hypotheses (models) by combining Bayesian inference (MCMC/NUTS), structured Gaussian Processes, and an RL-style policy for model selection to minimize overall Bayesian predictive uncertainty with a small experimental budget.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hypothesis Learning (HL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>HL maintains a list of candidate probabilistic models (standalone parametric models or wrapped into structured Gaussian Processes). It begins with a small set of seed measurements (warm-up), performs Bayesian inference (using MCMC/NUTS) for each model to obtain posterior samples and posterior-predictive mean/variance over unmeasured points, computes a total (or median) predictive uncertainty for each model, rewards models that reduce uncertainty, and then iteratively selects one model per exploration step using an ε-greedy policy to propose the next experimental coordinate (typically the argmax of the posterior predictive variance of the sampled model). The algorithm records sample-averaged rewards for models, updates data D with each measurement, and uses MCMC diagnostics to flag potential model mismatch. Warm-up iterations and limited BI runs are used to mitigate computational cost and avoid incorrect lock-in.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science (combinatorial libraries, Piezoresponse Force Microscopy) but described as general for sequential experimental design in low-to-moderate dimensional parameter spaces</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocation is driven by model-specific posterior predictive uncertainty: after warm-up BI, HL selects the model with lowest total uncertainty during warm-up (rewarding it) and thereafter uses an ε-greedy policy over model rewards to pick a model at each step; within a sampled model the next experiment is chosen at the parameter point with maximal posterior predictive variance (uncertainty sampling). Rewards are binary (+1 if total uncertainty decreased since last step, −1 otherwise) averaged into sample-averaged rewards Ra used by ε-greedy. Warm-up steps (1–5) and limits on BI runs are used to trade computation vs experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Qualitative/time and MCMC workload: cost discussed in terms of Bayesian inference runtime and increasing time as new points are added; MCMC convergence diagnostics (effective sample size n_eff, Gelman-Rubin r_hat) are used to monitor sampler quality. No explicit FLOP or dollar metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Total (or median) posterior predictive uncertainty (aggregate predictive variance over unmeasured coordinates); reward is based on decrease of this total uncertainty between steps (binary +1/−1). Posterior predictive mean/variance from BI is used as the acquisition information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>ε-greedy policy over models (ε=0.4 in experiments) balances exploration of alternative models vs exploitation of high-reward models. Within a selected model, exploitation is realized by selecting the argmax predictive variance to reduce uncertainty; stochastic ε fraction forces exploration of other models/points. Authors note Thompson sampling or policy embeddings as alternative strategies but did not implement them in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity across hypotheses is promoted via maintaining multiple candidate models and using ε-greedy sampling to occasionally explore less-rewarded models; initial warm-up across all models (BI per model) also provides diversification. Additionally, the paper advises downselecting hypothesis space size according to experimental budget to control diversity vs tractability.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experimental budget in number of sequential measurements (Nsteps), and practical computational budget for performing BI (limits on number of warm-up BI runs); also matches hypothesis-space size to experimental throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Controls include limiting number of warm-up BI iterations (~1–5), constraining number of models considered (downselection before experiment), setting total Nsteps for exploration, and using ε-greedy to trade exploration with exploitation under a small number of permitted experiments; authors explicitly say hypothesis-space compression should match expected experimental budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Observed model-data mismatches and locally increased posterior predictive uncertainty (or MCMC convergence problems such as low n_eff and high r_hat) indicate potential novel behavior; discontinuities or high-uncertainty regions highlight areas for follow-up and model revision (i.e., discovery signaled by elevated uncertainty/poor fit).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics include total/median posterior predictive uncertainty (plotted vs steps), sample-averaged rewards Ra per model (dimensionless), and MCMC diagnostics (n_eff, r_hat). Example numbers: in synthetic 1D case sGP with model 3 received Ra ≈ 0.82 after 15 steps; in real PFM experiment two sGP models received cumulative Ra of 0.13 and 0.56 respectively after 7 exploration steps with 5 warm-ups. Experiments used seeds of 2–4 points and total steps like 15 (3 warm-up + 12 exploration) or 20–25 in supplements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Vanilla Gaussian Process active learning / Bayesian optimization (GP-AL / GP-BO), standalone probabilistic models without sGP augmentation, random/uniform sampling (implicitly), and classical GP with zero mean prior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>HL with structured priors (sGP) outperformed vanilla GP around discontinuities: vanilla GP 'completely failed around the discontinuity point' while sGP (and HL) recovered the data distribution and identified the correct model with far fewer measurements. Exact numerical head-to-head efficiency gains vs GP-AL/GP-BO are qualitative rather than percent-based in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative acceleration: authors report 'significant acceleration' and successful model identification with a small number (tens) of measurements versus the larger sampling requirements of vanilla GP and classical RL; no general % reduction metric given. Example: correct model learned in synthetic test after 15 steps (3 warm-up + 12 exploration) starting from 4 seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses tradeoffs explicitly: (1) BI per model is computationally costly (time rises with data), so warm-up BI runs are limited; (2) sGP mean functions accelerate learning by encoding priors but can cause lock-in on wrong models because GP kernel can compensate for model mismatch — mitigated via multiple warm-up steps and comparing absolute uncertainties across models; (3) hypothesis-space size should be matched to experimental budget (wider hypothesis space allowed for high-throughput experiments, tighter preselection for expensive experiments); (4) MCMC convergence diagnostics can indicate when model assumptions should be revised — i.e., trade computational effort in BI vs risk of poor model selection.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key insights: (a) Co-navigation of hypothesis and experiment spaces reduces overall Bayesian uncertainty and accelerates discovery compared with purely data-driven GP; (b) structured priors (sGP mean functions) improve reconstruction from sparse data but require warm-up BI to avoid incorrect lock-in; (c) limit BI runs (warm-ups) to a small number for computational tractability (authors used ~1–5), and use ε-greedy policy for model exploration/exploitation; (d) match hypothesis-space complexity to available experimental budget (downselect with domain knowledge or literature mining); (e) use MCMC diagnostics to detect model mismatch and trigger hypothesis revision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Learning in Automated Experiment: Application to Combinatorial Materials Libraries', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2482.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2482.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>sGP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured Gaussian Process (sGP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Gaussian Process whose prior mean function is replaced by a parametric probabilistic model (or family of models) whose parameters are inferred jointly with GP kernel hyperparameters using Bayesian inference, so that physical priors are incorporated while retaining GP flexibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Structured Gaussian Process (sGP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>sGP augments a standard GP by setting the GP prior mean to a structured probabilistic model (e.g., piecewise power law/linear models). Parameters of the mean model and GP kernel (Matern kernel used here) are inferred simultaneously via MCMC/NUTS, yielding posterior predictive mean and variance that accounts for both model-parameter uncertainty and GP residual uncertainty. sGPs are used either as standalone surrogates or as wrapped components inside Hypothesis Learning.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active experimental design and model-based discovery in materials characterization (but generalizable to other physical sciences problems with strong priors)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Within HL, sGPs provide per-model posterior predictive uncertainty over unmeasured points; the allocation uses argmax predictive variance from the sampled sGP to select next measurement. Model selection among sGPs is based on comparing aggregated predictive uncertainties over the domain (lowest uncertainty preferred).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Qualitative MCMC runtime and sampler diagnostics; inference performed with NUTS in NumPyro; cost grows with number of data points and number of models since BI is run per model during warm-up. No explicit time or FLOP numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Posterior predictive variance (including uncertainty from mean-model parameters) aggregated across unmeasured coordinates; used to rank models and choose next acquisition locations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>As a component, sGP provides uncertainty-driven acquisition (variance maximization); the higher-level HL ε-greedy policy decides which sGP (model) to sample/exploit.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity of model hypotheses is achieved by instantiating multiple sGPs with different structured mean functions; sGP flexibility reduces the penalty of an imperfect mean but may necessitate warm-up comparisons to avoid lock-in.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Computational budget for per-model BI (MCMC) and experimental budget (number of measurements).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Authors limit the number of warm-up BI runs (~1–5) because BI is costly, and they jointly infer GP kernel and mean parameters to reduce number of experiments needed for accurate reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Regions where sGP posterior predictive uncertainty remains high or where MCMC diagnostics show poor convergence are treated as signals of novel behavior requiring model revision and targeted exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Comparative plots of uncertainty evolution and sample-averaged rewards; example: sGP with correct structured mean produced Ra=0.82 in synthetic test. sGPs recovered discontinuous features that vanilla GP missed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Vanilla GP (zero mean prior), standalone probabilistic models without GP residuals, classical GP-AL/GP-BO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>sGPs outperform vanilla GP near discontinuities by encoding physical structure in the mean; vanilla GP failed to capture discontinuity in reported experiments, while sGP succeeded with fewer points.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative improvement in sample efficiency and reconstruction fidelity in presence of structured priors; no general quantitative speedup provided beyond example rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Using structured priors reduces sample complexity but introduces potential false confidence if mean is wrong; GP kernel can mask model error (decreasing uncertainty even for wrong mean), so multiple warm-up in HL and uncertainty comparisons across models are necessary to avoid incorrect commitments.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Structured priors should be used when reliable physical hypotheses exist; when hypotheses are imperfect, wrap them into sGP to allow GP residuals to compensate, but use warm-up BI and cross-model uncertainty comparison to avoid lock-in; limit BI frequency for computational tractability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Learning in Automated Experiment: Application to Combinatorial Materials Libraries', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2482.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2482.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP-AL/GP-BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Process Active Learning / Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classical nonparametric active learning and Bayesian optimization methods that use Gaussian Process regression to predict a property and its uncertainty over parameter space and choose experiments via acquisition functions (e.g., uncertainty sampling or expected improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GP-based Active Learning / Bayesian Optimization (GP-AL / GP-BO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GP-AL chooses next points to minimize predictive uncertainty (exploratory). GP-BO uses predictive mean and variance combined in an acquisition function (e.g., expected improvement) to trade exploration and exploitation for optimizing a target property. Both rely on GP kernel hyperparameters fitted from data and typically assume zero prior mean if no physical priors are available.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General experimental design and optimization across sciences; in this paper used as related work and baseline for materials discovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Acquisition functions derived from GP predictive mean and variance guide allocation — e.g., select highest predictive variance (exploration) or maximize acquisition (expected improvement) — with sequential updates after each measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Implicit: number of experiments required and GP fitting costs; paper notes classical GP methods can require a fairly large number of steps and are limited to low-dimensional parameter spaces. No explicit numeric cost metric in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Predictive uncertainty and acquisition functions such as expected improvement; the paper frames GP-AL as minimizing uncertainty directly and GP-BO as using acquisition functions combining mean and uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition functions (e.g., uncertainty sampling for AL or EI/PI/LCB for BO) explicitly balance exploration and exploitation. The paper contrasts these methods with HL and notes their limitations for discontinuities and when physics priors exist.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via acquisition (uncertainty-driven sampling); no explicit diversity across hypotheses beyond the GP posterior distribution over functions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Number of experiments (sequential queries) and practical limitations on dimension; typical BO settings assume a fixed sequential budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Acquisition functions implicitly perform budget-aware greedy selection per step; the paper criticizes GP methods for needing many samples for complex/discontinuous targets.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Typically, objective improvement (e.g., finding higher-performing compositions); the paper argues GP struggles to identify discontinuities (phase transitions) efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported quantitatively here; used qualitatively as baseline and described as requiring larger numbers of steps in difficult problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>HL/sGP and vanilla GP comparisons are made in the paper (vanilla GP is shown to fail near discontinuity).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Vanilla GP-based acquisitions failed near discontinuities in the paper's examples; sGP/Hypothesis Learning performed better in reconstruction and model identification with far fewer steps in those examples.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>GP-AL/BO are known to be sample-efficient for smooth low-dimensional problems but less effective for discontinuities; no direct numeric gains provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper contrasts GP methods (data-driven kernel-centric) with physics-informed models (HL/sGP), noting GP's strengths in interpolation but weaknesses near discontinuities and for high-dimensional or structured-physics problems.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>When physical models or structured priors are available, augmenting GP with physical priors (sGP) or conducting hypothesis-driven allocation is preferable to pure GP-AL/BO; for purely data-driven, GP-BO remains standard but may need many samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Learning in Automated Experiment: Application to Combinatorial Materials Libraries', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2482.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2482.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ε-greedy policy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Epsilon-greedy model-selection policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple reinforcement-learning inspired strategy used within HL to pick which hypothesis/model to sample: with probability ε select a random model (explore), otherwise select the model with highest cumulative reward (exploit).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ε-greedy policy for model sampling</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used at each exploration step of HL to choose one model from the candidate set based on their sample-averaged rewards Ra: with probability ε (set to 0.4 in experiments) choose a model uniformly at random; otherwise choose the model with highest Ra. This balances exploration of alternative hypotheses with exploitation of currently favoured models.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Model-selection in active experimental design within materials discovery experiments in the paper, but generically applicable to multi-model active learning.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Decides allocation across hypotheses (which model to use to propose next experiment) rather than between experimental parameter points directly; exploration fraction ε forces sampling of diverse hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not applicable directly; indirectly reduces BI runs by focusing BI on sampled model per exploration step, while BI for all models occurs only during warm-up.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Indirect — model rewards are derived from change in total predictive uncertainty (information) and thus guide ε-greedy selection.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit ε parameter controls exploration probability; authors used ε=0.4, mention possible annealing of ε or alternative sampling via Thompson sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Encourages hypothesis diversity via the ε-driven random sampling; no explicit diversity regularizer beyond this.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Experimental budget (number of experiments) and computational budget for BI; ε chosen with small-sample regime in mind.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Larger ε encourages exploration (useful when few experiments) while warm-up steps are used to seed model rewards — parameter choices reflect constraints on total allowed experiments and computational BI cost.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not directly applicable; promotes trying alternative hypotheses which can lead to discovering unexpected behavior when models produce high uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Policy parameter used (ε=0.4) and outcomes reported in experiments (reward trajectories and correct model identification statistics across multiple initializations).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned that alternatives like Thompson sampling or policy embeddings could be used but were not explored.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>No numerical comparison in paper; chosen for simplicity and small-number-of-experiments regime.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Practical efficiency in experiments: allowed successful model identification across multiple random initializations; authors note annealing or smaller ε can be implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Authors recommend tuning ε (and warm-up count) to balance exploration/exploitation under small experimental budgets; too little exploration risks lock-in on suboptimal model.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use of moderate ε (0.4) was effective in their low-N experiments; warm-up steps and multiple initializations improve robustness against poor initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Learning in Automated Experiment: Application to Combinatorial Materials Libraries', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2482.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2482.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoHypothesisGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated hypothesis generation (symbolic regression and related frameworks: PySINDy, PySR, SISSO, AI Feynman, Eureka)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of symbolic regression and model-discovery frameworks that generate mathematical hypotheses (functional forms, descriptors) from data; proposed in the paper as an upstream step to produce candidate hypotheses for HL, with hypothesis-space downselection based on prior knowledge and experimental budget.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Automated hypothesis generation / symbolic regression frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The paper mentions several frameworks (Eureka, PySINDy, PySR, SISSO, AI Feynman, Feynman AI) that can generate candidate symbolic expressions or low-dimensional descriptors from static data. Authors propose interfacing such hypothesis generation with HL: generate a (potentially large) hypothesis space, apply hard physical constraints and literature/prior-based soft priors to downselect plausible hypotheses, and feed the resulting list into the HL active-testing pipeline. The paper emphasizes matching hypothesis-space size to experimental throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Model discovery and hypothesis generation for experimental testing in materials science and broader physical sciences.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Downselection of the generated hypothesis space using physical constraints (dimensionality, symmetry, conservation), prior probabilities from domain knowledge or literature mining, to keep the number of candidate models compatible with experimental budget. The remaining hypotheses are then tested via HL with uncertainty-driven allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not specified in paper; symbolic regression workloads typically measured in CPU/GPU time, search iterations, or expression complexity — authors do not quantify.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not explicitly defined for generator itself; the generated hypotheses are evaluated within HL using posterior predictive uncertainty and reward signals.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Hypothesis-space compression before experiments acts as exploitation of domain knowledge; within HL the ε-greedy policy controls testing/exploration across generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity of hypotheses controlled by generation algorithms but then intentionally pruned using hard/soft priors to match budget — explicit tradeoff between diversity and tractability is discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Experimental budget and computational budget for hypothesis generation/search (implicitly).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Authors recommend expanding hypothesis space for high-throughput/fast experiments and aggressive downselection for expensive/slow experiments; use literature mining and domain constraints to prune candidate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not specified for generation step; within HL, hypotheses that lead to persistent high uncertainty or unexpected predictive patterns would flag the need for new hypothesis generation and model revision.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No experimental metrics provided in this paper for the generation frameworks; they are discussed as complementary tools.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not compared quantitatively in this paper; referenced literature includes PySINDy, PySR, SISSO, AI Feynman, Eureka.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not applicable in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Paper argues conceptual gains: automated generation + downselection allows matching hypothesis complexity to experimental budget, potentially saving experimental costs, but provides no quantitative numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Explicitly discussed: hypothesis-space size vs experimental budget (more hypotheses allowed with high-throughput experiments; for expensive experiments, careful preselection is justified). Also tradeoff between risk level and assumptions in hypothesis selection.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation-level conclusions: integrate automated hypothesis generation with HL and use domain constraints/literature priors to compress hypothesis space so that testing fits the available experimental budget and computational resources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Learning in Automated Experiment: Application to Combinatorial Materials Libraries', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2482.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2482.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Thompson</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Thompson sampling (mentioned alternative)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian sampling-based policy for model or action selection where actions (models) are sampled according to their posterior probability of being optimal; mentioned as an alternative to ε-greedy for model sampling in HL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Thompson sampling for model selection</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Paper briefly notes Thompson sampling of posteriors or policy embeddings could replace ε-greedy for selecting which model to sample during exploration, offering principled posterior-probability-driven exploration/exploitation balance. Not implemented in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Model selection within active experimental design; general RL/BO applications.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Would allocate experiments by sampling models proportionally to posterior belief in their optimality; next measurement would then follow the sampled-model acquisition (e.g., argmax predictive variance or other acquisition).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not specified; would require posterior sampling but can be cheaper than exhaustive BI per model if using existing posterior samples.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicit via posterior beliefs; would favor models that have higher posterior probability of reducing uncertainty or improving objective.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration stems naturally from posterior sampling; exploitation via sampling frequently the models with higher posterior probability.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Provides diversity by sampling models according to posterior distribution rather than deterministic greedy picks; can preserve exploration proportional to uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Experimental budget; not further specified.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Not implemented — paper only mentions as a feasible alternative to ε-greedy that might better balance exploration/exploitation under uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported (not implemented).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Offered as alternative to ε-greedy in HL; no empirical comparison in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not available.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Not analyzed in detail; authors note Thompson sampling is an available strategy and policy embeddings could be used but were not explored due to small number of theoretical models.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>No findings — suggested as promising alternative to improve sampling of hypothesis space.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Learning in Automated Experiment: Application to Combinatorial Materials Libraries', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>On-the-fly closed-loop materials discovery via Bayesian active learning <em>(Rating: 2)</em></li>
                <li>Autonomy in materials research: a case study in carbon nanotube growth <em>(Rating: 2)</em></li>
                <li>Taking the Human Out of the Loop: A Review of Bayesian Optimization <em>(Rating: 1)</em></li>
                <li>Physics makes the difference: Bayesian optimization and active learning via augmented Gaussian process <em>(Rating: 2)</em></li>
                <li>Self-driving laboratory for accelerated discovery of thin-film materials <em>(Rating: 2)</em></li>
                <li>PySINDy: A comprehensive Python package for robust sparse system identification <em>(Rating: 1)</em></li>
                <li>Distilling Free-Form Natural Laws from Experimental Data <em>(Rating: 1)</em></li>
                <li>A physics-inspired method for symbolic regression <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2482",
    "paper_id": "paper-245124375",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "HL",
            "name_full": "Hypothesis Learning (co-navigation of hypothesis and experimental spaces)",
            "brief_description": "An active learning framework that jointly navigates experimental parameter space and a discrete set of probabilistic hypotheses (models) by combining Bayesian inference (MCMC/NUTS), structured Gaussian Processes, and an RL-style policy for model selection to minimize overall Bayesian predictive uncertainty with a small experimental budget.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Hypothesis Learning (HL)",
            "system_description": "HL maintains a list of candidate probabilistic models (standalone parametric models or wrapped into structured Gaussian Processes). It begins with a small set of seed measurements (warm-up), performs Bayesian inference (using MCMC/NUTS) for each model to obtain posterior samples and posterior-predictive mean/variance over unmeasured points, computes a total (or median) predictive uncertainty for each model, rewards models that reduce uncertainty, and then iteratively selects one model per exploration step using an ε-greedy policy to propose the next experimental coordinate (typically the argmax of the posterior predictive variance of the sampled model). The algorithm records sample-averaged rewards for models, updates data D with each measurement, and uses MCMC diagnostics to flag potential model mismatch. Warm-up iterations and limited BI runs are used to mitigate computational cost and avoid incorrect lock-in.",
            "application_domain": "Materials science (combinatorial libraries, Piezoresponse Force Microscopy) but described as general for sequential experimental design in low-to-moderate dimensional parameter spaces",
            "resource_allocation_strategy": "Allocation is driven by model-specific posterior predictive uncertainty: after warm-up BI, HL selects the model with lowest total uncertainty during warm-up (rewarding it) and thereafter uses an ε-greedy policy over model rewards to pick a model at each step; within a sampled model the next experiment is chosen at the parameter point with maximal posterior predictive variance (uncertainty sampling). Rewards are binary (+1 if total uncertainty decreased since last step, −1 otherwise) averaged into sample-averaged rewards Ra used by ε-greedy. Warm-up steps (1–5) and limits on BI runs are used to trade computation vs experiment.",
            "computational_cost_metric": "Qualitative/time and MCMC workload: cost discussed in terms of Bayesian inference runtime and increasing time as new points are added; MCMC convergence diagnostics (effective sample size n_eff, Gelman-Rubin r_hat) are used to monitor sampler quality. No explicit FLOP or dollar metric provided.",
            "information_gain_metric": "Total (or median) posterior predictive uncertainty (aggregate predictive variance over unmeasured coordinates); reward is based on decrease of this total uncertainty between steps (binary +1/−1). Posterior predictive mean/variance from BI is used as the acquisition information.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "ε-greedy policy over models (ε=0.4 in experiments) balances exploration of alternative models vs exploitation of high-reward models. Within a selected model, exploitation is realized by selecting the argmax predictive variance to reduce uncertainty; stochastic ε fraction forces exploration of other models/points. Authors note Thompson sampling or policy embeddings as alternative strategies but did not implement them in reported experiments.",
            "diversity_mechanism": "Diversity across hypotheses is promoted via maintaining multiple candidate models and using ε-greedy sampling to occasionally explore less-rewarded models; initial warm-up across all models (BI per model) also provides diversification. Additionally, the paper advises downselecting hypothesis space size according to experimental budget to control diversity vs tractability.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed experimental budget in number of sequential measurements (Nsteps), and practical computational budget for performing BI (limits on number of warm-up BI runs); also matches hypothesis-space size to experimental throughput.",
            "budget_constraint_handling": "Controls include limiting number of warm-up BI iterations (~1–5), constraining number of models considered (downselection before experiment), setting total Nsteps for exploration, and using ε-greedy to trade exploration with exploitation under a small number of permitted experiments; authors explicitly say hypothesis-space compression should match expected experimental budget.",
            "breakthrough_discovery_metric": "Observed model-data mismatches and locally increased posterior predictive uncertainty (or MCMC convergence problems such as low n_eff and high r_hat) indicate potential novel behavior; discontinuities or high-uncertainty regions highlight areas for follow-up and model revision (i.e., discovery signaled by elevated uncertainty/poor fit).",
            "performance_metrics": "Reported metrics include total/median posterior predictive uncertainty (plotted vs steps), sample-averaged rewards Ra per model (dimensionless), and MCMC diagnostics (n_eff, r_hat). Example numbers: in synthetic 1D case sGP with model 3 received Ra ≈ 0.82 after 15 steps; in real PFM experiment two sGP models received cumulative Ra of 0.13 and 0.56 respectively after 7 exploration steps with 5 warm-ups. Experiments used seeds of 2–4 points and total steps like 15 (3 warm-up + 12 exploration) or 20–25 in supplements.",
            "comparison_baseline": "Vanilla Gaussian Process active learning / Bayesian optimization (GP-AL / GP-BO), standalone probabilistic models without sGP augmentation, random/uniform sampling (implicitly), and classical GP with zero mean prior.",
            "performance_vs_baseline": "HL with structured priors (sGP) outperformed vanilla GP around discontinuities: vanilla GP 'completely failed around the discontinuity point' while sGP (and HL) recovered the data distribution and identified the correct model with far fewer measurements. Exact numerical head-to-head efficiency gains vs GP-AL/GP-BO are qualitative rather than percent-based in the paper.",
            "efficiency_gain": "Qualitative acceleration: authors report 'significant acceleration' and successful model identification with a small number (tens) of measurements versus the larger sampling requirements of vanilla GP and classical RL; no general % reduction metric given. Example: correct model learned in synthetic test after 15 steps (3 warm-up + 12 exploration) starting from 4 seeds.",
            "tradeoff_analysis": "Paper discusses tradeoffs explicitly: (1) BI per model is computationally costly (time rises with data), so warm-up BI runs are limited; (2) sGP mean functions accelerate learning by encoding priors but can cause lock-in on wrong models because GP kernel can compensate for model mismatch — mitigated via multiple warm-up steps and comparing absolute uncertainties across models; (3) hypothesis-space size should be matched to experimental budget (wider hypothesis space allowed for high-throughput experiments, tighter preselection for expensive experiments); (4) MCMC convergence diagnostics can indicate when model assumptions should be revised — i.e., trade computational effort in BI vs risk of poor model selection.",
            "optimal_allocation_findings": "Key insights: (a) Co-navigation of hypothesis and experiment spaces reduces overall Bayesian uncertainty and accelerates discovery compared with purely data-driven GP; (b) structured priors (sGP mean functions) improve reconstruction from sparse data but require warm-up BI to avoid incorrect lock-in; (c) limit BI runs (warm-ups) to a small number for computational tractability (authors used ~1–5), and use ε-greedy policy for model exploration/exploitation; (d) match hypothesis-space complexity to available experimental budget (downselect with domain knowledge or literature mining); (e) use MCMC diagnostics to detect model mismatch and trigger hypothesis revision.",
            "uuid": "e2482.0",
            "source_info": {
                "paper_title": "Hypothesis Learning in Automated Experiment: Application to Combinatorial Materials Libraries",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "sGP",
            "name_full": "Structured Gaussian Process (sGP)",
            "brief_description": "A Gaussian Process whose prior mean function is replaced by a parametric probabilistic model (or family of models) whose parameters are inferred jointly with GP kernel hyperparameters using Bayesian inference, so that physical priors are incorporated while retaining GP flexibility.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Structured Gaussian Process (sGP)",
            "system_description": "sGP augments a standard GP by setting the GP prior mean to a structured probabilistic model (e.g., piecewise power law/linear models). Parameters of the mean model and GP kernel (Matern kernel used here) are inferred simultaneously via MCMC/NUTS, yielding posterior predictive mean and variance that accounts for both model-parameter uncertainty and GP residual uncertainty. sGPs are used either as standalone surrogates or as wrapped components inside Hypothesis Learning.",
            "application_domain": "Active experimental design and model-based discovery in materials characterization (but generalizable to other physical sciences problems with strong priors)",
            "resource_allocation_strategy": "Within HL, sGPs provide per-model posterior predictive uncertainty over unmeasured points; the allocation uses argmax predictive variance from the sampled sGP to select next measurement. Model selection among sGPs is based on comparing aggregated predictive uncertainties over the domain (lowest uncertainty preferred).",
            "computational_cost_metric": "Qualitative MCMC runtime and sampler diagnostics; inference performed with NUTS in NumPyro; cost grows with number of data points and number of models since BI is run per model during warm-up. No explicit time or FLOP numbers provided.",
            "information_gain_metric": "Posterior predictive variance (including uncertainty from mean-model parameters) aggregated across unmeasured coordinates; used to rank models and choose next acquisition locations.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "As a component, sGP provides uncertainty-driven acquisition (variance maximization); the higher-level HL ε-greedy policy decides which sGP (model) to sample/exploit.",
            "diversity_mechanism": "Diversity of model hypotheses is achieved by instantiating multiple sGPs with different structured mean functions; sGP flexibility reduces the penalty of an imperfect mean but may necessitate warm-up comparisons to avoid lock-in.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Computational budget for per-model BI (MCMC) and experimental budget (number of measurements).",
            "budget_constraint_handling": "Authors limit the number of warm-up BI runs (~1–5) because BI is costly, and they jointly infer GP kernel and mean parameters to reduce number of experiments needed for accurate reconstruction.",
            "breakthrough_discovery_metric": "Regions where sGP posterior predictive uncertainty remains high or where MCMC diagnostics show poor convergence are treated as signals of novel behavior requiring model revision and targeted exploration.",
            "performance_metrics": "Comparative plots of uncertainty evolution and sample-averaged rewards; example: sGP with correct structured mean produced Ra=0.82 in synthetic test. sGPs recovered discontinuous features that vanilla GP missed.",
            "comparison_baseline": "Vanilla GP (zero mean prior), standalone probabilistic models without GP residuals, classical GP-AL/GP-BO",
            "performance_vs_baseline": "sGPs outperform vanilla GP near discontinuities by encoding physical structure in the mean; vanilla GP failed to capture discontinuity in reported experiments, while sGP succeeded with fewer points.",
            "efficiency_gain": "Qualitative improvement in sample efficiency and reconstruction fidelity in presence of structured priors; no general quantitative speedup provided beyond example rewards.",
            "tradeoff_analysis": "Using structured priors reduces sample complexity but introduces potential false confidence if mean is wrong; GP kernel can mask model error (decreasing uncertainty even for wrong mean), so multiple warm-up in HL and uncertainty comparisons across models are necessary to avoid incorrect commitments.",
            "optimal_allocation_findings": "Structured priors should be used when reliable physical hypotheses exist; when hypotheses are imperfect, wrap them into sGP to allow GP residuals to compensate, but use warm-up BI and cross-model uncertainty comparison to avoid lock-in; limit BI frequency for computational tractability.",
            "uuid": "e2482.1",
            "source_info": {
                "paper_title": "Hypothesis Learning in Automated Experiment: Application to Combinatorial Materials Libraries",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "GP-AL/GP-BO",
            "name_full": "Gaussian Process Active Learning / Bayesian Optimization",
            "brief_description": "Classical nonparametric active learning and Bayesian optimization methods that use Gaussian Process regression to predict a property and its uncertainty over parameter space and choose experiments via acquisition functions (e.g., uncertainty sampling or expected improvement).",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "GP-based Active Learning / Bayesian Optimization (GP-AL / GP-BO)",
            "system_description": "GP-AL chooses next points to minimize predictive uncertainty (exploratory). GP-BO uses predictive mean and variance combined in an acquisition function (e.g., expected improvement) to trade exploration and exploitation for optimizing a target property. Both rely on GP kernel hyperparameters fitted from data and typically assume zero prior mean if no physical priors are available.",
            "application_domain": "General experimental design and optimization across sciences; in this paper used as related work and baseline for materials discovery tasks.",
            "resource_allocation_strategy": "Acquisition functions derived from GP predictive mean and variance guide allocation — e.g., select highest predictive variance (exploration) or maximize acquisition (expected improvement) — with sequential updates after each measurement.",
            "computational_cost_metric": "Implicit: number of experiments required and GP fitting costs; paper notes classical GP methods can require a fairly large number of steps and are limited to low-dimensional parameter spaces. No explicit numeric cost metric in this paper.",
            "information_gain_metric": "Predictive uncertainty and acquisition functions such as expected improvement; the paper frames GP-AL as minimizing uncertainty directly and GP-BO as using acquisition functions combining mean and uncertainty.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition functions (e.g., uncertainty sampling for AL or EI/PI/LCB for BO) explicitly balance exploration and exploitation. The paper contrasts these methods with HL and notes their limitations for discontinuities and when physics priors exist.",
            "diversity_mechanism": "Implicit via acquisition (uncertainty-driven sampling); no explicit diversity across hypotheses beyond the GP posterior distribution over functions.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Number of experiments (sequential queries) and practical limitations on dimension; typical BO settings assume a fixed sequential budget.",
            "budget_constraint_handling": "Acquisition functions implicitly perform budget-aware greedy selection per step; the paper criticizes GP methods for needing many samples for complex/discontinuous targets.",
            "breakthrough_discovery_metric": "Typically, objective improvement (e.g., finding higher-performing compositions); the paper argues GP struggles to identify discontinuities (phase transitions) efficiently.",
            "performance_metrics": "Not reported quantitatively here; used qualitatively as baseline and described as requiring larger numbers of steps in difficult problems.",
            "comparison_baseline": "HL/sGP and vanilla GP comparisons are made in the paper (vanilla GP is shown to fail near discontinuity).",
            "performance_vs_baseline": "Vanilla GP-based acquisitions failed near discontinuities in the paper's examples; sGP/Hypothesis Learning performed better in reconstruction and model identification with far fewer steps in those examples.",
            "efficiency_gain": "GP-AL/BO are known to be sample-efficient for smooth low-dimensional problems but less effective for discontinuities; no direct numeric gains provided in the paper.",
            "tradeoff_analysis": "Paper contrasts GP methods (data-driven kernel-centric) with physics-informed models (HL/sGP), noting GP's strengths in interpolation but weaknesses near discontinuities and for high-dimensional or structured-physics problems.",
            "optimal_allocation_findings": "When physical models or structured priors are available, augmenting GP with physical priors (sGP) or conducting hypothesis-driven allocation is preferable to pure GP-AL/BO; for purely data-driven, GP-BO remains standard but may need many samples.",
            "uuid": "e2482.2",
            "source_info": {
                "paper_title": "Hypothesis Learning in Automated Experiment: Application to Combinatorial Materials Libraries",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "ε-greedy policy",
            "name_full": "Epsilon-greedy model-selection policy",
            "brief_description": "A simple reinforcement-learning inspired strategy used within HL to pick which hypothesis/model to sample: with probability ε select a random model (explore), otherwise select the model with highest cumulative reward (exploit).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ε-greedy policy for model sampling",
            "system_description": "Used at each exploration step of HL to choose one model from the candidate set based on their sample-averaged rewards Ra: with probability ε (set to 0.4 in experiments) choose a model uniformly at random; otherwise choose the model with highest Ra. This balances exploration of alternative hypotheses with exploitation of currently favoured models.",
            "application_domain": "Model-selection in active experimental design within materials discovery experiments in the paper, but generically applicable to multi-model active learning.",
            "resource_allocation_strategy": "Decides allocation across hypotheses (which model to use to propose next experiment) rather than between experimental parameter points directly; exploration fraction ε forces sampling of diverse hypotheses.",
            "computational_cost_metric": "Not applicable directly; indirectly reduces BI runs by focusing BI on sampled model per exploration step, while BI for all models occurs only during warm-up.",
            "information_gain_metric": "Indirect — model rewards are derived from change in total predictive uncertainty (information) and thus guide ε-greedy selection.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicit ε parameter controls exploration probability; authors used ε=0.4, mention possible annealing of ε or alternative sampling via Thompson sampling.",
            "diversity_mechanism": "Encourages hypothesis diversity via the ε-driven random sampling; no explicit diversity regularizer beyond this.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Experimental budget (number of experiments) and computational budget for BI; ε chosen with small-sample regime in mind.",
            "budget_constraint_handling": "Larger ε encourages exploration (useful when few experiments) while warm-up steps are used to seed model rewards — parameter choices reflect constraints on total allowed experiments and computational BI cost.",
            "breakthrough_discovery_metric": "Not directly applicable; promotes trying alternative hypotheses which can lead to discovering unexpected behavior when models produce high uncertainty.",
            "performance_metrics": "Policy parameter used (ε=0.4) and outcomes reported in experiments (reward trajectories and correct model identification statistics across multiple initializations).",
            "comparison_baseline": "Mentioned that alternatives like Thompson sampling or policy embeddings could be used but were not explored.",
            "performance_vs_baseline": "No numerical comparison in paper; chosen for simplicity and small-number-of-experiments regime.",
            "efficiency_gain": "Practical efficiency in experiments: allowed successful model identification across multiple random initializations; authors note annealing or smaller ε can be implemented.",
            "tradeoff_analysis": "Authors recommend tuning ε (and warm-up count) to balance exploration/exploitation under small experimental budgets; too little exploration risks lock-in on suboptimal model.",
            "optimal_allocation_findings": "Use of moderate ε (0.4) was effective in their low-N experiments; warm-up steps and multiple initializations improve robustness against poor initialization.",
            "uuid": "e2482.3",
            "source_info": {
                "paper_title": "Hypothesis Learning in Automated Experiment: Application to Combinatorial Materials Libraries",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "AutoHypothesisGen",
            "name_full": "Automated hypothesis generation (symbolic regression and related frameworks: PySINDy, PySR, SISSO, AI Feynman, Eureka)",
            "brief_description": "A class of symbolic regression and model-discovery frameworks that generate mathematical hypotheses (functional forms, descriptors) from data; proposed in the paper as an upstream step to produce candidate hypotheses for HL, with hypothesis-space downselection based on prior knowledge and experimental budget.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Automated hypothesis generation / symbolic regression frameworks",
            "system_description": "The paper mentions several frameworks (Eureka, PySINDy, PySR, SISSO, AI Feynman, Feynman AI) that can generate candidate symbolic expressions or low-dimensional descriptors from static data. Authors propose interfacing such hypothesis generation with HL: generate a (potentially large) hypothesis space, apply hard physical constraints and literature/prior-based soft priors to downselect plausible hypotheses, and feed the resulting list into the HL active-testing pipeline. The paper emphasizes matching hypothesis-space size to experimental throughput.",
            "application_domain": "Model discovery and hypothesis generation for experimental testing in materials science and broader physical sciences.",
            "resource_allocation_strategy": "Downselection of the generated hypothesis space using physical constraints (dimensionality, symmetry, conservation), prior probabilities from domain knowledge or literature mining, to keep the number of candidate models compatible with experimental budget. The remaining hypotheses are then tested via HL with uncertainty-driven allocation.",
            "computational_cost_metric": "Not specified in paper; symbolic regression workloads typically measured in CPU/GPU time, search iterations, or expression complexity — authors do not quantify.",
            "information_gain_metric": "Not explicitly defined for generator itself; the generated hypotheses are evaluated within HL using posterior predictive uncertainty and reward signals.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Hypothesis-space compression before experiments acts as exploitation of domain knowledge; within HL the ε-greedy policy controls testing/exploration across generated hypotheses.",
            "diversity_mechanism": "Diversity of hypotheses controlled by generation algorithms but then intentionally pruned using hard/soft priors to match budget — explicit tradeoff between diversity and tractability is discussed.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Experimental budget and computational budget for hypothesis generation/search (implicitly).",
            "budget_constraint_handling": "Authors recommend expanding hypothesis space for high-throughput/fast experiments and aggressive downselection for expensive/slow experiments; use literature mining and domain constraints to prune candidate hypotheses.",
            "breakthrough_discovery_metric": "Not specified for generation step; within HL, hypotheses that lead to persistent high uncertainty or unexpected predictive patterns would flag the need for new hypothesis generation and model revision.",
            "performance_metrics": "No experimental metrics provided in this paper for the generation frameworks; they are discussed as complementary tools.",
            "comparison_baseline": "Not compared quantitatively in this paper; referenced literature includes PySINDy, PySR, SISSO, AI Feynman, Eureka.",
            "performance_vs_baseline": "Not applicable in this paper.",
            "efficiency_gain": "Paper argues conceptual gains: automated generation + downselection allows matching hypothesis complexity to experimental budget, potentially saving experimental costs, but provides no quantitative numbers.",
            "tradeoff_analysis": "Explicitly discussed: hypothesis-space size vs experimental budget (more hypotheses allowed with high-throughput experiments; for expensive experiments, careful preselection is justified). Also tradeoff between risk level and assumptions in hypothesis selection.",
            "optimal_allocation_findings": "Recommendation-level conclusions: integrate automated hypothesis generation with HL and use domain constraints/literature priors to compress hypothesis space so that testing fits the available experimental budget and computational resources.",
            "uuid": "e2482.4",
            "source_info": {
                "paper_title": "Hypothesis Learning in Automated Experiment: Application to Combinatorial Materials Libraries",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Thompson",
            "name_full": "Thompson sampling (mentioned alternative)",
            "brief_description": "A Bayesian sampling-based policy for model or action selection where actions (models) are sampled according to their posterior probability of being optimal; mentioned as an alternative to ε-greedy for model sampling in HL.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Thompson sampling for model selection",
            "system_description": "Paper briefly notes Thompson sampling of posteriors or policy embeddings could replace ε-greedy for selecting which model to sample during exploration, offering principled posterior-probability-driven exploration/exploitation balance. Not implemented in the experiments.",
            "application_domain": "Model selection within active experimental design; general RL/BO applications.",
            "resource_allocation_strategy": "Would allocate experiments by sampling models proportionally to posterior belief in their optimality; next measurement would then follow the sampled-model acquisition (e.g., argmax predictive variance or other acquisition).",
            "computational_cost_metric": "Not specified; would require posterior sampling but can be cheaper than exhaustive BI per model if using existing posterior samples.",
            "information_gain_metric": "Implicit via posterior beliefs; would favor models that have higher posterior probability of reducing uncertainty or improving objective.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Exploration stems naturally from posterior sampling; exploitation via sampling frequently the models with higher posterior probability.",
            "diversity_mechanism": "Provides diversity by sampling models according to posterior distribution rather than deterministic greedy picks; can preserve exploration proportional to uncertainty.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Experimental budget; not further specified.",
            "budget_constraint_handling": "Not implemented — paper only mentions as a feasible alternative to ε-greedy that might better balance exploration/exploitation under uncertainty.",
            "breakthrough_discovery_metric": "Not specified here.",
            "performance_metrics": "Not reported (not implemented).",
            "comparison_baseline": "Offered as alternative to ε-greedy in HL; no empirical comparison in paper.",
            "performance_vs_baseline": "Not available.",
            "efficiency_gain": "Not quantified in this paper.",
            "tradeoff_analysis": "Not analyzed in detail; authors note Thompson sampling is an available strategy and policy embeddings could be used but were not explored due to small number of theoretical models.",
            "optimal_allocation_findings": "No findings — suggested as promising alternative to improve sampling of hypothesis space.",
            "uuid": "e2482.5",
            "source_info": {
                "paper_title": "Hypothesis Learning in Automated Experiment: Application to Combinatorial Materials Libraries",
                "publication_date_yy_mm": "2021-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "On-the-fly closed-loop materials discovery via Bayesian active learning",
            "rating": 2,
            "sanitized_title": "onthefly_closedloop_materials_discovery_via_bayesian_active_learning"
        },
        {
            "paper_title": "Autonomy in materials research: a case study in carbon nanotube growth",
            "rating": 2,
            "sanitized_title": "autonomy_in_materials_research_a_case_study_in_carbon_nanotube_growth"
        },
        {
            "paper_title": "Taking the Human Out of the Loop: A Review of Bayesian Optimization",
            "rating": 1,
            "sanitized_title": "taking_the_human_out_of_the_loop_a_review_of_bayesian_optimization"
        },
        {
            "paper_title": "Physics makes the difference: Bayesian optimization and active learning via augmented Gaussian process",
            "rating": 2,
            "sanitized_title": "physics_makes_the_difference_bayesian_optimization_and_active_learning_via_augmented_gaussian_process"
        },
        {
            "paper_title": "Self-driving laboratory for accelerated discovery of thin-film materials",
            "rating": 2,
            "sanitized_title": "selfdriving_laboratory_for_accelerated_discovery_of_thinfilm_materials"
        },
        {
            "paper_title": "PySINDy: A comprehensive Python package for robust sparse system identification",
            "rating": 1,
            "sanitized_title": "pysindy_a_comprehensive_python_package_for_robust_sparse_system_identification"
        },
        {
            "paper_title": "Distilling Free-Form Natural Laws from Experimental Data",
            "rating": 1,
            "sanitized_title": "distilling_freeform_natural_laws_from_experimental_data"
        },
        {
            "paper_title": "A physics-inspired method for symbolic regression",
            "rating": 1,
            "sanitized_title": "a_physicsinspired_method_for_symbolic_regression"
        }
    ],
    "cost": 0.0210025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Hypothesis learning in automated experiment: application to combinatorial materials libraries</p>
<p>Maxim Ziatdinov 
Computational Sciences and Engineering Division
Oak Ridge National Laboratory
37831Oak RidgeTN</p>
<p>Center for Nanophase Materials Sciences
Oak Ridge National Laboratory
37831Oak RidgeTN</p>
<p>Yongtao Liu 
Center for Nanophase Materials Sciences
Oak Ridge National Laboratory
37831Oak RidgeTN</p>
<p>Anna N Morozovska 
Institute of Physics
National Academy of Sciences of Ukraine
46, pr. Nauky03028KyivUkraine</p>
<p>Eugene A Eliseev 
Institute for Problems of Materials Science
National Academy of Sciences of Ukraine
Krjijanovskogo 303142KyivUkraine</p>
<p>Xiaohang Zhang 
Department of Materials Science and Engineering
Ridge National Laboratory's Center for Nanophase Materials Sciences (CNMS), a U.S. Department of Energy, Office
University of Maryland
20742College ParkMD</p>
<p>Ichiro Takeuchi 
Department of Materials Science and Engineering
Ridge National Laboratory's Center for Nanophase Materials Sciences (CNMS), a U.S. Department of Energy, Office
University of Maryland
20742College ParkMD</p>
<p>Sergei V Kalinin 
Center for Nanophase Materials Sciences
Oak Ridge National Laboratory
37831Oak RidgeTN</p>
<p>Hypothesis learning in automated experiment: application to combinatorial materials libraries
1 Notice: This manuscript has been authored by UT-Battelle, LLC, under Contract No. DE-AC0500OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for the United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan 2
Machine learning is rapidly becoming an integral part of experimental physical discovery via automated and high-throughput synthesis, and active experiments in scattering and electron/probe microscopy. This, in turn, necessitates the development of active learning methods capable of exploring relevant parameter spaces with the smallest number of steps. Here we introduce an active learning approach based on co-navigation of the hypothesis and experimental spaces. This is realized by combining the structured Gaussian Processes containing probabilistic models of the possible system's behaviors (hypotheses) with reinforcement learning policy refinement (discovery). This approach closely resembles classical human-driven physical discovery, when several alternative hypotheses realized via models with adjustable parameters are tested during an experiment. We demonstrate this approach for exploring concentration-induced phase transitions in combinatorial libraries of Sm-doped BiFeO3 using Piezoresponse Force Microscopy, but it is straightforward to extend it to higher-dimensional parameter spaces and more complex physical problems once the experimental workflow and hypothesis-generation are available. a ziatdinovma@ornl.gov b sergei2@ornl.gov Recent advances in accelerated and combinatorial synthesis have allowed rapid fabrication of immense number of materials' compositions in the form of combinatorial spread libraries in pulsed laser deposition, 1-7 high-throughput microfluidic 8-12 and pipetting robot sets.[13][14][15][16][17][18][19]In parallel, conventional sample fabrication has been drastically accelerated via laboratory robotization and autonomous and combined human-automation workflows.[20][21][22][23][24]Complementary to the largethroughput synthesis are rapid characterization methods that allow establishing structure-property relations in the specific systems and in certain cases target functionalities of interest.However, even with this outstanding progress, simple grid-based exploration of the immense compositional spaces is impractical. For example, for a quarternary alloy with the properties of interest peaking within 1% from the morphotropic phase boundary or quantum critical point, a standard grid search would require sampling 10 6 compositions for discovery. Correspondingly, numerous strategies for active learning in such settings were proposed, using statistical learning methods to navigate compositional space. These were based on direct Bayesian optimization (BO) of the functionality of interest [25][26][27]or relied on the presence of cheap proxy signals to select candidates for expensive target measurements. In several cases, theoretical calculations were used to augment the BO-based strategies.[28][29][30]However, fundamentally, this discovery process has been preponderantly based on the Gaussian process (GP) regression, 31, 32 a non-parametric method that learns the behavior of function of interest over relatively low-dimensional parameter space. The GP is an example of a purely data-driven method 33 aiming to interpolate the behavior of interest and its uncertainty over the specified domain based on the measurements in discrete locations. The characteristic aspect of classical GP is that the expectation value of the parameter of interest is assumed to be zero, and variability across the parameter space is captured via correlations only. The kernel function defining the degree of correlation within this parameter space is typically defined within a certain functional form and the parameters of this function are learned from the available data at each optimization/exploration step. However, the simple GP-based methods do not incorporate any physical model of behavior within the system. Consequently, they can be prone to trivial solutions, often require a fairly large number of steps to reconstruct the complicated data distributions, and are limited to low-dimensional parameter spaces.An alternative approach for the active learning is based on the reinforcement learning (RL) methods.[34][35][36][37]The RL field has experienced rapid growth over last several years and comprises multiple subfields. However, common to simple RL algorithms is extreme data requirements, stemming from the necessity to build either a low-dimensional model of the process, or discover low-dimensional representations of the state and policies of the system. Parenthetically, it is important to note that many classical RL problems become trivial if the generative physical model of the system is known explicitly.Recently, we have introduced the structured GP (sGP) approach combining the flexibility of GP models with the expressive power of the physical priors, and demonstrated significant acceleration of the active learning/optimization based on the sGP. 38 Here, we demonstrate that this approach can be further extended towards physics discovery via active learning of competing hypotheses, effectively combining the sGP and reinforcement learning. We refer to this approach as hypothesis learning. This approach is illustrated for exploring concentration-induced phase transition in Sm-doped BiFeO3 using a Piezoresponse force microscopy (PFM) but can be expected to be applicable universally.
 Figure 1
. Hypothesis learning. The workflow of the scientific discovery process based on conavigation of the experimental and hypothesis space.</p>
<p>The general concept of the proposed hypothesis learning approach is illustrated in Figure  1. Here, we assume a classical experimental set-up in the context of sequential materials synthesis or sequential characterization of compositional-or parameter spread systems. The examples of the former are classical wet chemical solid-state synthesis/ceramics processing, or film growth via techniques such as pulsed laser deposition, atomic layer deposition, or chemical deposition. Common to all these techniques is that the material is fabricated for a fixed set of parameters such as component ratios, stoichiometries, and deposition conditions. Jointly, these form a (relatively) low-dimensional parameter space spanning chemical and physical control variables. Once the film is grown (or material is synthesized), its properties are explored and based on the results, a new exploration point (e.g., different composition or growth condition) is chosen. Notably, a similar approach can be applied for composition spread libraries (where the composition changes across the sample plane), thickness or growth temperature gradients, 4,39 or samples under heat, electric, or magnetic field gradients, if the local measurements are performed sequentially. These in turn can be based on the modalities of scanning probe microscopies (piezoresponse, conductive, microwave, acoustic), electron microscopy, micro-Raman imaging, or on the classical semiconductor characterization performed via microfabricated electrode arrays.</p>
<p>Traditionally, these synthesis-measurement cycles are performed using human intuition and past experience for selecting initial parameter settings, and often proceed via dense grid sampling of the compositional or parameter spaces. This can include characterization of the 1D compositional library, or synthesis of the epitaxial solid solution films. However, such a strategy is not efficient, since functionalities of interest are often concentrated in specific parts of the parameter space, necessitating the development of active learning methods for experimental planning, namely selection of the next experimental point based on the results of previous experiments.</p>
<p>In the last several years, the emergence of the automated synthesis and control systems have stimulated the implementation of the Gaussian Process-based active learning (GP-AL) and Bayesian Optimization (GP-BO) methods. By definition, GP seeks to reconstruct a scalar function ( ) over N-dimensional parameter space x ∈ R N from the sparse observations (yi, xi), i = 1, … n, where the observations are the noisy values of the function. From the perspective of the physical sciences, the GP seeks to reconstruct a physical property of interest over the entire parameter space based on the limited number of noisy measurements of this property. To do this, the classical GP method introduces all possible functions with the zero mean over the parameter space and assumes that their probability is updated based on measurement results. The mean and dispersion of the resulting functions at each point then provide the interpolated behavior of the physical property of interest and its uncertainty over the full parameter space.</p>
<p>Both the predicted target property values and the corresponding uncertainty estimates can be directly used as a basis for the AL or BO, which both represent a sequential measurement protocol where the subsequent point for measuring is selected based on the previous ones. In the GP-AL, we operate in the pure exploratory mode seeking to minimize the uncertainty by selecting at each step a point associated with the largest uncertainty. In the GP-BO, the predicted values and uncertainty are combined in the acquisition function and used to guide the optimization of the (unknown) underlying function describing the property of interest. The principles and applications of the GP-BO are described in details in Ref [ 40 ].</p>
<p>The fundamental limitation of the GP-based AL and BO for physical sciences is that the state of the system is learned in the form of the kernel function describing the correlations between functional behavior. The latter can be chosen from a given functional form, most typically radial basis function or Matern. 41 In special cases, more complex kernel functions such as asymmetric Gaussian or spectral kernels are chosen. However, while this approach allows building a nonparametric model of system behavior, the kernel function itself is not directly linked to the physics of the system. Curiously, in most cases the mean function of GP is taken to be zero. In this manner, variability of system behavior is effectively reduced to correlation structure.</p>
<p>Note that the alternative approach for the analysis of physical behaviors can be found in the field of classical Bayesian inference, 42,43 where a probabilistic model is constructed using the known functional form(s) reflecting a physical model, and some prior knowledge on model parameters. [44][45][46] Given the experimental observations, the posteriors of the parameter values are inferred, representing an increase of knowledge based on observations.</p>
<p>Here, we introduce a combined approach, referred to as hypothesis learning, integrating active discovery and Bayesian Inference, and further integrate this approach with a structured GP. Our method is summarized in Algorithm 1 and is based on the idea that during active learning a correct model of the system's behavior decreases the overall Bayesian uncertainty about the system under study. The method consists of the (very) short warm-up phase and exploration phase and assumes the existence of several measurements at randomly (or uniformly) initialized coordinates of the parameter space (we call them 'seed' measurements). The role of the warm-up phase is to produce a 'momentum' for the exploration phase. This is necessary since in the active learning setup we aim at discovering the overall data distribution with a minimal number of steps which is usually much smaller than the number of exploration steps in standard RL problems. We also must supply to the algorithm a list of probabilistic models that, based on our understanding of the system, can describe the system's behavior. These models can come as standalone parametric models with appropriate priors over their parameters or they can be wrapped into sGPs. The practical differences between the two approaches will be explained further in the text. It is assumed that there is one 'correct' model in the list, but we do not know which one. We also note that if the 'correct' model is absent, the flexibility of the sGP approach allows for convergence comparable to vanilla GP. Once we have our 'seed' measurements and a list of models, we proceed to the warm-up phase where we conduct Bayesian inference (BI) for each model. In the BI, the prior, ( ), represents our knowledge about the system before the measurement. The measurement produces the data, D, based on which the posterior distribution, ( | ), is computed via Bayes formula as
( | ) = ( | ) ( ) ( )(1)
where ( | ) represents the likelihood that this data can be generated by the model with parameters and the denominator ( ) = ∫ ( | ) ( ) defines the space of possible outcomes. This step yields "degree of trust" in the model via the derived posterior distributions.</p>
<p>In practice, the BI is performed using sampling algorithms based on the Markov Chain Monte Carlo (MCMC) techniques. [47][48][49] Here, specifically, we are using the No-U-Turn Sampler (NUTS) algorithm as implemented in the NumPyro probabilistic programming library. 50 The posterior predictive mean and variance for a new point, * , given the measured data, , are then computed as
( * | ) = ∫ ( * | ) ( | ) ≈ ∑ ( * | , ) = * ,(2a)
[ * ] = ∑ ( * − * ) , (2b) where ~ ( | ) are samples drawn from the posterior. We then select a model with the lowest total (or median) uncertainty over all the unmeasured coordinates, * , and reward it according to a pre-defined reward function. The uncertainty map produced by the rewarded model is then used to select the coordinate(s) of the next measurement point the same way as in the standard AL setup. This "warm-up" procedure can be repeated multiple times (Nwarmup in Algorithm 1) to ensure that the model selection was not affected by bad initialization of the seed points. We note that conducting BI for each model is computationally costly and time-consuming (the cost and time quickly rise as new points are added) and in practice, depending on the number of models, we limit the number of warm-up steps to ~1-5.</p>
<p>After the warm-up phase is completed, we use a standard -greedy policy to sample only one model at each exploration step. The total uncertainty computed from the posterior of a sampled model is compared to the total uncertainty recorded in the previous step resulting in either positive (the uncertainty decreased) or negative (the uncertainty increased) reward. The next measurement point is then derived from the uncertainty map produced by the sampled model. Figure 2. (a) Synthetic data describing a 1D system with a discontinuous "phase transition". The full data ('noisy observations') is shown but is never seen by our models. (b-d) The results of active learning after 15 steps (3 warm-up + 12 exploration) with standalone probabilistic models: model 1 (b), model 2 (e), model 3 (d), with sample-averaged rewards denoted as Ra.</p>
<p>As a practical example chosen here, we are interested in the active learning of phase diagram that has a transition between different phases. The phase transition manifests in a discontinuity of a measurable system's property, such as heat capacity. An example using synthetic data is shown in Figure 2(a) where x-axis is a tunable parameter (such as temperature) and y-axis is a system's property that is measured experimentally. However, we usually do not know where a phase transition occurs precisely, nor are we aware of the exact behavior of the property of interest in different phases. We note that using a standard GP-AL is not an optimal choice in these cases as simple GP struggles around the discontinuity point. Instead, we are going to utilize structured probabilistic models of expected system's behavior.</p>
<p>We hypothesize that the system of interest can be described by a piecewise function as
( ) = ( ), &lt; ( ), ≥(3)
where ( ) are the functions describing the system's behavior before and after the transition point . We consider three different models based on Eq (3), namely i) a power-law behavior before transition point and linear behavior after transition point (model 1), ii) a linear behavior before and after transition point (model 2), iii) a power law behavior before and after transition point (model 3). We write them down as three standalone probabilistic models and put Bayesian priors over their parameters. Specifically, the parameter is modeled using a uniform prior while the rest of the parameters (slopes and power-law coefficients) are described by normal priors (for more implementation details, see the accompanying source code). We apply the hypothesis learning (HL) algorithm to the system with a single-phase transition shown in Figure 2(a). The algorithm is initialized with four seed datapoints and our measurements at the coordinates suggested by the algorithm uncover new data points from the existing ground truth. We defined a simple reward function,
, = +1, &lt; −1, ≥(4)
where is model's total uncertainty for the prediction over the unmeasured part of the parameter space at exploration step i. Here, the  parameter in the -greedy policy was set to 0.4 in all experiments, reflecting the need for exploration over small (tens of data points) number of attempts. However, similar to classical RL methods, smaller  values and annealing of  can be trivially implemented. Furthermore, the exploration policy can be based on the Thompson sampling of the posteriors or use policy embeddings. Here, given the small number of theoretical models, we did not explore these strategies. Figure 2(b-d) shows the result of the Bayesian fit over the entire parameter space for the three models after running HL for 15 steps. The model that received the highest reward (i.e., is favored by our algorithm) clearly provided the best fit. Hence, we were able both to learn a correct data distribution with a small number of sparse measurements while also identifying a correct model that describes the system's behavior.</p>
<p>The evolution of the total uncertainty during the exploration for a single warm-up step and for three warm-up steps are shown in Figure 3 (a) and 3 (b), respectively, for the larger number of exploration steps. Again, in both cases, the algorithm favored the third model where the system's behavior before and after the transition point is described by the power-law functions. To confirm the reproducibility and robustness of our algorithm, we ran it for 30 different initializations. The histograms with averaged rewards received by three models are shown in Figure 3 (c) and 3 (d) for a single warm-up step and for three warm-up steps, respectively. The algorithm showed a good performance in both cases although the delineation of the correct model (model 3) was better when we ran the warm-up procedure three times. One disadvantage of the current approach is that it fails when the list of probabilistic models does not contain the 'correct' model, as one can quickly conclude from Fig. 2(bc, ). At the same time, most theoretical models describe the experimental reality only to a certain approximation, and the complete/correct model may simply be not available.</p>
<p>To address this issue, we utilize a structured Gaussian process (sGP) method where a fully Bayesian GP is augmented by a structured probabilistic model of expected system's behavior. 38 Recall that in standard GP, we place a multivariate normal (MVN) prior over a target function such that ~( ( ), ( , )) and = ( ) + , where is a normally distributed noise. The prior mean function is usually set to 0 and the GP is completely defined by its kernel function . In the sGP, we replace the GP's constant prior mean function with one of the probabilistic models defined earlier. The parameters of the GP's new probabilistic mean function and those of the GP's kernel (here chosen to be Matern 41 ) are inferred simultaneously via NUTS. Hence, the posterior predictive uncertainty associated with the parameters of sGP's mean function is automatically taken into account in our active learning setup and provides additional control via the choice of the model's priors. The sGP's probabilistic mean function is expected to capture general or even partial trends in the data, but it does not have to be precise. Note that we replace standalone models M in the Algorithm 1 with sGPs containing the same models without changing any other part of the algorithm.  Fig. 2b-d), with sampleaveraged rewards denoted as Ra. Results of the active learning with vanilla GP (i.e., with prior mean function set to 0) is shown in (d) for comparison. Figures 4 and 5 show the sGP results for the same set of conditions as used for the standalone probabilistic models in Fig. 2 and 3. In the sGP, the flexibility of the GP kernel can partially correct for the wrong model choice. This offers an advantage for the reconstruction from sparse data but also introduces a potential hurdle for learning the correct model in our setup, since even with a wrong model the total uncertainty will likely keep decreasing. Therefore, a bad initialization may result in being "locked-up" with an incorrect model (Fig. 5a).</p>
<p>Fortunately, this can be easily addressed by performing several warm-up steps (Fig. 5b) where we compare the absolute values of the total (or median) uncertainties between different sGP models. Indeed, as one can see from the histograms of sample-averaged rewards for 30 different initializations in Fig. 5c and 5d, the increased number of warmup steps allows for a more robust identification of the correct model. The Bayesian fits over the entire parameter space for the three models after running the HL for 15 steps demonstrate that even with 'wrong' models, the sGP can still recover the data distribution reasonably well (Fig. 4a, b). At the same time, the best reconstruction was produced by sGP with the third model as its prior mean function (Fig. 4c) that received the largest reward (0.82), in agreement with the results of the standalone probabilistic models and the known ground truth. For comparison, we also showed the results of the active learning with a vanilla GP (i.e., GP with a prior mean function set to 0), which completely failed around the discontinuity point (Fig. 4d). In the Supplementary Note I, we describe the application of sGP to several more systems with discontinuous behavior, including a scenario where none of the proposed models correctly describes a behavior of the full system. Figure 5. Hypothesis-driven active learning with structured Gaussian processes (sGPs). (a, b) Evolution of the total uncertainty during the active learning with sGPs augmented by three different structured probabilistic models (the models are the same as in Fig. 3) with one 'warmup' step (a) and with three 'warm-up' steps (b). See Algorithm 1 for the definition of the 'warmup steps.' At each exploration step, the model is selected according to ε-greedy policy with ε=0.4. For the warm-up steps, only the model that produced the lowest uncertainty is shown. (c, d) Statistics of the sample-averaged rewards for 30 different random initializations of the four seed measurements for one 'warm-up' step (c) and for three 'warm-up' steps (d)</p>
<p>Having confirmed the performance of our approach on synthetic data, we move to the realtime PFM experiments on the combinatorial library of Sm-doped BiFeO3, as shown in Figure 6.</p>
<p>This material system has been extensively studied in the context of the evolution of ferroelectric properties, with the pure BiFeO3 being rhombohedral ferroelectric materials and 20% Sm-doped BiFeO3 being the orthorhombic non-ferroelectric. 51 The intermediate phases at the boundary between crystallographically-incompatible phases are known to exhibit complex nanoscale ordering patterns and can be associated with enhanced electromechanical responses. 52 However, these behaviors are strongly dependent on the mechanical and electrical boundary conditions, [53][54][55] resulting in strong variability of behaviors between bulk ceramics and thin films. Generally, the mechanisms describing the emergence of these properties are poorly understood and are of continuous interest for physical community. [56][57][58][59][60][61][62] The compositional spread library hence encodes the full constant-temperature cross-section of the phase diagram in a given concentration range. Previously, we have extensively explored the property of this combinatorial library using highresolution electron microscopy. 46,63 Here, we have explored the mesoscale electromechanical responses in this material system using PFM experiments. [64][65][66][67] This technique is based on the detection of the local electromechanical response induced by application of small amplitude (~1 Vac) periodic bias (100-300 kHz) to the scanning probe. The detection of the response as a function of slowly (~Hz) varying large amplitude (3-10 V) waveforms allows measuring the local hysteresis loops. [68][69][70][71][72] The shape of the hysteresis loop is closely linked to tip-induced domain dynamics, 73,74 and thus represents local polarization switching mechanisms. 75,76 Here, we use the switching spectroscopy PFM approach, where hysteresis loops are measured over the rectangular grid of points, offering better statistics and information on the spatial variability of switching behavior. 77,78 Important for subsequent discussion is that in linear approximation, the PFM signal is independent on tip-surface contact radius and hence surface topography, making PFM signal quantitative. 79  Topography and corresponding band excitation PFM amplitude images from three representative locations: 0% Sm side, center, and 20% Sm side, which correspond to the three optical images in (b), respectively. (b) Picture of the Sm-BFO sample, which has a gradually increased Sm ratio from the left side to the right side. The optical images under a Cypher microscopy optical camera show different color due to Sm ratio variation, this color gradient can also be observed by naked eyes; however, note that the color shown by the microscope camera deviates due to optical path effects and illumination conditions. (c). Hysteresis loops from six selected locations obtained in Band excitation piezoresponse spectroscopy measurements; note that 56 locations representing gradual Sm concentration change were uniformly marked along the sample.</p>
<p>To determine the possible models of the hysteresis loop behavior across the compositional phase transition, we develop a simple Ginburg-Landau based model. Here, we assume that piezoresponse (PR) loops are determined by the ferroelectric polarization reversal via piezoelectric coupling as:
(V) ≅ + (V),(5a)+ ( , ) + + + = ( ),(5b)
where is an effective piezocoefficient related to a pure BiFeO3. Applied voltage is = ( ) + ( ), where ( ) is a slow-varying amplitude of the voltage applied to the tip.</p>
<p>The acting field ( ) = − ( ) , where ℎ is the BiFeO3 thickness, and we assume that an ultrathin gap of width  separates the BiFeO3 surface from the tip. Constant is a relative background permittivity of BFO, 10, and ~(1 − 10) is a relative permittivity in the gap. 83 The coefficient ( , ) linearly depends on the Sm concentration: 84,85 ( , ) = − ( ) ,
( ) = 1 − ,(6)
where is the temperature, is a Curie temperature of a pure BiFeO3, and is the relative content of rare-earth Sm atoms. The critical concentration is a fitting parameter. Note that Sm doping also affects the parameters , , and , and the effect can be nonlinear and complex. Only the shift ( ) contains a linear component.</p>
<p>Several cases of possible materials behaviors are analyzed in the Supplementary Note II. Here, we considered the two possible models that relate the measured loop area S to the dopant concentration x. The first model is defined as
= 1 − + , ≤ , , &gt;(7)
where xc is the transition point. This model does not have a discontinuous jump at the transition point. The second model is defined as
= 1 − + , ≤ , , &gt;(8)
and is characterized by a discontinuity at xc. Note that this analysis is thermodynamic in nature and, in particular assumes structurally homogeneous phases. As such, it does not account for potential formation of morphotropic phases with enhanced electromechanical responses. We note that, unlike in the examples on synthetic data, we do not know the true function in the actual experiment. We, therefore, cannot exclude the possibility that none of these models is correct. We place a uniform prior over the transition point location and log-normal priors on all the remaining parameters of the models as well as GP kernel hyperparameters and do not change them during the experiment. The experiment starts by measuring two extreme points on the concentration axis. These seed points are used to initialize Algorithm 1 with two sGP models. We used five warmup states and seven exploration steps. The median uncertainty as a function of the active learning steps is shown in Fig. 7a. At the end of the exploration, the first model received a cumulative sampleaveraged reward of 0.13. The second model received a reward of 0.56. This suggests that the second model is better at describing the current system. However, the second model experienced significant difficulties in convergence at each exploration step, as judged by the effective number of MCMC samples and the Gelman-Rubin convergence statistics (see Supplementary Note III). The convergence problems indicate that, even though the second model received a larger reward, it may not be the correct model for describing the system's behavior (see also a discussion in the Supplementary Note I for synthetic data). To get a better understanding, we examined the sGP predictions sampled from the posterior distribution at the two last steps of the exploration. Shown in Fig. 7b and 7c are the sampled sGP predictions together with the experimental values measured in the points suggested by the algorithm. We can see there's a discontinuity-like behavior at x~17%, which explains why the algorithm favored the second model, preceded by a prominent peak-like feature. The latter was not a part of any prior model, which explains the problems with convergence. The inconsistency between the prior model and observations resulted in larger uncertainty around the transition region, allowing us to quickly discover a potentially new behavior. This discovery, in turn, will require the development of an improved model of the system's behavior to account for the peak before the transition, i.e., the behavior associated with the morphotropic behavior. Figure 8. The "automated scientist" concept. Here, the initial hypothesis space is formed as a random symbolic sequence up to a given length. The hard physical constraints (dimensionality, conservation laws, symmetry) are used to select the physical hypothesis and provide hard priors (e.g., non-negativity). The prior "soft" knowledge is used to ascribe relevant probability and form parameter priors. These models form inputs to the (active) hypothesis learning algorithm. Here, the part in red dashed rectangular corresponds to Figure 1.</p>
<p>We further consider the opportunity to combine the hypothesis learning algorithm with the automated hypothesis generation, as illustrated in Figure 8. Over the last decade, several notable frameworks have been proposed to discover of the laws and functional expressions from the experimental data. They include Eureka originally developed for the dynamic data 86 and more recent frameworks such as PySindy, 87 PySR, 88 SISSO 89 , and many others. The overview of symbolic regression methods in materials science is given in Ref [ 90 ]. These methods operate with the static data, but generally allow the generation and simplification of the symbolic functional form consistent with the observations. Recent work illustrates how this approach can be generalized to incorporate physical laws. 91 Other developments in this area includes frameworks such as Feynman AI. 92 We argue that the same approach can be extended for experimental hypothesis learning, interfacing hypothesis generation with experiment. Clearly, the hypothesis spaces formed by all possible symbolic expressions are intractable for experimental verification, as are all physically possible models. Hence, an additional stage will be down the selection of the hypotheses using the prior knowledge derived from domain expertise or literature mining. [93][94][95][96] In the context of HL, this includes assigning probabilities to the possible theories and identifying the priors on the relevant parameters. The formed list of the hypotheses and corresponding priors can be used as an input for the HL, substituting the model list in Fig. 1. It is important to note that this approach dynamically allows further matching the degree of compression of the hypothesis space and expected experimental budget. For example, when the experiments are high throughput, and fast, hypothesis space can be expanded, whereas for expensive experiments, the effort in down selecting hypotheses is justified. Similarly, hypothesis selection can be based on different selection strategies, varying degrees of risk, level of assumption, i.e., within its own optimization strategies. Overall, this approach combines hypotheses generation and testing and allows constraining the experimental budget.</p>
<p>Finally, it is important to note that the hypotheses generation is not necessarily limited to symbolic models and can also include, for example, the lattice Hamiltonians and force field models. However, in this case, the inverse problems are considerably more complex and are addressed only for special cases. 97,98 Similarly, this will necessitate transition from discrete models with Bernoulli distribution to more complex representations such as continuous embeddings, etc.</p>
<p>To summarize, we introduced an active learning approach based on co-navigation of hypothesis and experimental spaces via a combination of fully Bayesian structured Gaussian Processes with reinforcement learning policy refinement. This approach closely resembles classical human-driven physical discovery, when multiple alternative hypotheses realized via models with adjustable parameters are tested during the experiment. We demonstrated this method for exploring concentration-induced phase transitions in the combinatorial libraries of Sm-doped BiFeO3 using PFM measurements.</p>
<p>Looking forward, we believe that the proposed approach makes a strong case for the synergistic combination between scanning probe and other imaging methods with combinatorial material science. While traditionally combinatorial studies have been limited by the need to quantify materials structure and functionality across the libraries, t recent advances in the focused X-Ray methods and SPM offer a solution. At the same time, the proposed hypothesis-driven Gaussian Processes framework developed here further allows incorporation and selection between physical models. Note that while the approach is implemented here for the 1D case, it is straightforward to extend it to higher-dimensional parameter spaces and more complex physical problems.</p>
<p>Target Program of Basic Research of the National Academy of Sciences of Ukraine "Prospective basic research and innovative development of nanomaterials and nanotechnologies for 2020 -2024", Project № 1/20-Н, state registration number: 0120U102306) and received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 778070. The work at the University of Maryland was supported in part by the National Institute of Standards and Technology Cooperative Agreement 70NANB17H301. The authors gratefully acknowledge Dr. Bobby Sumpter for careful reading and editing of the manuscript.</p>
<p>Experimental details</p>
<p>A Sm-doped BiFeO3 (Sm-BFO) thin film, grown on SrTiO3 substrate, with Sm concentration gradually increases from 0% to 20% along the sample was used as a model system. The detailed growth conditions and characterization are described in previous publications. 46,63 Band excitation piezoresponse spectroscopy (BEPS) measurements were performed on an Oxford Instrument Asylum Research MFP3D atomic force microscopy system. A National Instruments DAQ card and a LabView framework are equipped for band excitation measurement. All measurements were carried out using Budget Sensor Multi75E-G Cr/Pt coated AFM probes (∼3 N/m). 56 locations were marked on the side of sample, which correspond to the Sm concentration spanning from 0% to 20%. Band excitation piezoresponse spectroscopy measurements were performed on the locations determined by the hypothesis learning algorithm.</p>
<p>Code availability</p>
<p>Code is available without restrictions at https://github.com/ziatdinovmax/hypoAL</p>
<p>Algorithm 1 (,
1Hypothesis Learning) Inputs: Initial measurements D, List of unmeasured points * , List of models M (standalone or wrapped into sGP) and corresponding sample-averaged rewards Ra (initialized at zeros), Exploration steps Nsteps, Warm-up steps Nwarmup, Reward function R,  in the -greedy policy. for i=1, …, Nwarmup do for each model in M do Run BI to obtain posterior samples for model parameters given D Compute posterior predictive uncertainty, [ * ], over * Store the total uncertainty, of the predicted function values end Reward the model that produced the lowest uncertainty in prediction Use the rewarded model to derive the next measurement point, xnext = argmax( [ * ]) Perform measurement in xnext; update D and * end Average model rewards over the warm-up steps, update Ra for i=1, …, Nsteps do Use -greedy policy to sample model from M based on Ra[i] or at random Run BI to obtain posterior samples for parameters of selected model given D Compute posterior predictive uncertainty, [ * ], over * Reward/penalize according to R ( , ), update Ra Derive the next measurement point, xnext = argmax( [ * ]) Perform measurement in xnext; update D and * end</p>
<p>Figure 3 .
3Hypothesis-driven active learning with standalone structured probabilistic models. (a, b) Evolution of the total uncertainty during the active learning with standalone probabilistic models with one 'warm-up' step (a) and with three 'warm-up' steps (b). See Algorithm 1 for the definition of the 'warm-up' steps.' At each exploration step, the model is selected according to ε-greedy policy with ε=0.4. For the warm-up steps, only the model that produced the lowest uncertainty is shown. (c, d) Statistics of the sample-averaged rewards for 30 different random initializations of the four seed measurements for one 'warm-up' step (c) and for three 'warm-up' steps (d).</p>
<p>Figure 4 .
4Hypothesis-driven active learning with structured Gaussian processes (sGPs). (a-c) The results of active learning after 15 steps (3 warm-up + 12 exploration) with sGPs augmented by three different structured probabilistic models (same settings as in</p>
<p>-82 Shown in Figure 6a are band excitation PFM results showing the sample morphology and domain structures from three representative locations with varying Sm concentration, along with the picture of the Sm concentration gradient sample in Figure 6b. Figure 6c shows hysteresis loops from six locations corresponding to various Sm concentration, indicating the dependence of switching behavior on Sm concentration.</p>
<p>Figure 6 .
6Piezoresponse force microscopy (PFM) results of Sm doped BFO sample. (a)</p>
<p>Figure 7 .
7Application of Algorithm 1 with two different sGP models to real experiment. (a) Evolution of the uncertainty during the active learning with sGPs. (b, c) sGP predictions sampled from the posterior distribution at step 11 (b) and 12 (c), before performing a new measurement.
Supplementary MaterialsI. Application of hypoAL for active learning of different discontinuous functions.Here we illustrate application of hypoAL (Algorithm 1) to two different discontinuous functions. In the first case, there is one correct model in the input list M. In the second case, none of the models in the input list is correct.I.1. In the SupplementaryFigure 1, we show data generated by the discontinuous function of the form:The noisy observations generated by this function together with the 'true function' are shown inSupplementary Figure 1.SupplementaryFigure 1. The noisy observations of discontinuous function in Eq. S1Same as in the main text example, the full data ("noisy observations") inSupplementary  Figure 1is never seen by the algorithm. Instead, we start with four 'seed' measurement to initialize the algorithm. The algorithm is provided with the two possible models of system's behavior, wrapped into structured Gaussian processes. The first model is a Lorentzian peak function of the form + ( − ) + ⁄ , with a Uniform(0, 1) prior on and Normal (0, 1) priors on A, B, and C parameters. The second model is the one from Eq. S1, with the same priors on its parameters.The hypoAL results after 20 steps (3 warm-up + 17 exploration) with the same settings as used in the main text are shown in Supplementary figure 1, together with the reward values for each model.Figure 2. Results of the hypotheses-driven active learning after 20 steps (3 warm-up + 17 exploration) with sGPs augmented by two different probabilistic models, with sample-averaged rewards denoted as Ra.Figure 3, we show noisy observations of a discontinuous function which, unlike the one in the main text, has two transition points. We are going to use the same HypoAL settigns with the same models, wrapped into structured Gaussian process, as in the main text. This means that none of the models is correct (as they all assume only a single transition).SupplementaryI.2. In SupplementarySupplementaryFigure 3. The noisy observations of discontinuous function with two "phase transitions".The hypoAL results are shown inSupplementary Figure 4. The first two models that assume a linear behavior after the transition point received high positive rewards whereas the third model that assumes a power law behavior before and after the transition point received a negative reward. Although none of the models is correct, the preference for the first two models can be easily understood since it is easier to approximate the behavior after the first transition point with a linear behavior than a power law one. In both cases, the flexibility of GP kernel partially compensates for the incorrect model choice. We note that the models with high rewards described correctly one of the transitions and provided a hint for the existence of the second transition.SupplementaryFigure 4. Results of the hypotheses-driven active learning after 25 steps (3 warmup + 12 exploration) with sGPs augmented by the same probabilistic models as in the main text, with sample-averaged rewards denoted as Ra.In the example above, the correct piece of information in all the models was the presence of a transition point. The obvious question is: how the hypoAL algorithm distinguishes between a model that is only partially correct and a model that is completely wrong? To test this, we run hypoAL using two models: the third model in the example above and a wrong model of the form ax 2 + b with Normal(0, 1) priors over its parameters.SupplementaryFigure 5. Results of the hypotheses-driven active learning after 25 steps (3 warmup + 12 exploration) with sGPs augmented by a partially correct model (a) and by a wrong model (b), with sample-averaged rewards denoted as Ra.The results shown in SupplementaryFigure 5clearly demonstrate that when presented with a wrong model(Fig. S5b)and a partially correct model(Fig. S5a), the algorithm chooses the partially correct model. At the same time, a model that is only partially correct typically results in a slow MCMC convergence at each step. In this case, the MCMC diagnostics may show a low effective number of samples (n_eff) and high values (&gt;1.05) of Gelman-Rubin criteria (r_hat), as demonstrated below for the final AL step that performed Bayesian inference on the first model parameters using NUTS with 5000 samples: Such diagnostics can serve as an indicator that the model needs to be updated.II. Phenomenological models of the local piezoresponse formation in Bi1-xSmxFeO3One of the most widely used model for the formation of vertical PR loops is the semiempirical equation of Landau type, which reflects the fact that PR loops are determined by the ferroelectric polarization reversal via piezoelectric coupling, rather than by electrostrictive coupling as it follows from the classical Landau theory. The mathematical formulation of the above statement is:where is an effective piezocoefficient related to a pure BiFeO3. Applied voltage is = ( ) + ( ), where ( ) is a slow-varying amplitude of the voltage applied to the tip.The acting field ( ) = − ( ) , where ℎ is the BiFeO3 thickness, and we assume that an ultrathin gap of width  separates the BiFeO3 surface from the tip. Constant is a relative background permittivity of BFO, 10, and ~(1 − 10) is a relative permittivity in the gap.82The coefficient ( , ) linearly depends on the Sm concentration:83,84( , ) = − ( ) ,where is the temperature, is a Curie temperature of a pure BiFeO3, and is the relative content of rare-earth Sm atoms. The critical concentration is a fitting parameter. Note that Sm doping affect also on the parameters , , and , but the effect can be nonlinear and complex. Only the shift ( ) contains a linear component. has a quasi-rectangular shape in a quasi-static case, is roughly proportional to their product, but has sense only when &lt; 0. So:Here and are fitting parameters, and is tabulated.For the purposes of the implementation in the code, we can introduce as the concentration for which phase transition happens at room temperature, = 1 − . With this re-designation Eq.(3a) can be rewritten as:There is no jump at = and zero afterwards. To account for the possible electrostatic effects,98we additionally add the constant offset that we assume to be independent of concentration.Case II. The coefficients &lt; 0, &gt; 0 and = 0 for the first order ferroelectrics of displacement type. For the case, the width of the PR loop is proportional to the spontaneous polarization = , the loop height is proportional to the coercive field = 2 + 9 − 20 ⁄ .99The PR loop area , which has a quasi-rectangular shape in a quasi-static case, is roughly proportional to their product, and has sense only in the polar phase:(3b) where ( , ) is a linear function of x according to Eq.(2). Equation (3b) is too cumbersome for the machine learning. Allowing for = at the transition temperature, = 1 − + , it can be simplified further as:Here and are fitting parameters, = and are tabulated. Note the expression (4a) is valid near the transition point only. Introducing the concentration for which phase transition happens at room temperature, = 1 − , Eq.(4a) can be rewritten as:There is a jump of the height at = and zero afterwards. Similar to case I, we add a constant concentration-independent offset to account for the possible electrostatic effectsCase III. For the order-disorder type ferroelectrics we can put &lt; 0, = = 0 and &gt; 0. Important than within the model is x-dependent but should be temperature independent.Moreover, we can assume that the unknown parametercan be x-dependent. Eq.(1b) can be rewritten asUsing a series expansion ℎ( ) ≈ + + in Eq.(4a) we can reduce it either to the Case I or to the Case II in dependence on the cutting term in the expansion. In the simplest case: (5a)Here , and are fitting parameters, by the x-dependence can be assumed:Note that this functional form is equivalent to case I, and hence these forms cannot be separated in this approximation. 7.53, =1100 K, = 8.37 10 5 C -2 ·mJ/K (10 8 C -4 ·m 5 J)11=III. The MCMC diagnostics for hypoAL steps inFigure 7. The effective number of samples (n_eff) and the Gelman-Rubin convergence criteria (r_hat) for the two hypoAL steps shown inFigure 7aand 7b are shown below. In each case the Bayesian inference was performed using NUTS algorithm with 5000 warmup steps (not to be confused with the warm-up phase in the hypoAL) and 5000 samples.Similar to the example in Supplementary Note I, the problems with MCMC convergence may indicate that a model is only partially correct (for the completely wrong model, the GP kernel will take over resulting in a good convergence but a trivial solution).
The highthroughput highway to computational materials design. S Curtarolo, G L W Hart, M B Nardelli, N Mingo, S Sanvito, O Levy, Nat. Mater. 123Curtarolo, S.; Hart, G. L. W.; Nardelli, M. B.; Mingo, N.; Sanvito, S.; Levy, O., The high- throughput highway to computational materials design. Nat. Mater. 2013, 12 (3), 191-201.</p>
<p>Identification of novel compositions of ferromagnetic shape-memory alloys using composition spreads. I Takeuchi, O O Famodu, J C Read, M A Aronova, K S Chang, C Craciunescu, S E Lofland, M Wuttig, F C Wellstood, L Knauss, A Orozco, Nat. Mater. 23Takeuchi, I.; Famodu, O. O.; Read, J. C.; Aronova, M. A.; Chang, K. S.; Craciunescu, C.; Lofland, S. E.; Wuttig, M.; Wellstood, F. C.; Knauss, L.; Orozco, A., Identification of novel compositions of ferromagnetic shape-memory alloys using composition spreads. Nat. Mater. 2003, 2 (3), 180-184.</p>
<p>Highthroughput growth temperature optimization of ferroelectric SrxBa1−xNb2O6 epitaxial thin films using a temperature gradient method. I Ohkubo, H M Christen, S V KalininJr, G E J Rouleau, C M Lowndes, D H , Applied Physics Letters. 848Ohkubo, I.; Christen, H. M.; Kalinin, S. V.; Jr., G. E. J.; Rouleau, C. M.; Lowndes, D. H., High- throughput growth temperature optimization of ferroelectric SrxBa1−xNb2O6 epitaxial thin films using a temperature gradient method. Applied Physics Letters 2004, 84 (8), 1350-1352.</p>
<p>An improved continuous compositional-spread technique based on pulsed-laser deposition and applicable to large substrate areas. H M Christen, C M Rouleau, I Ohkubo, H Y Zhai, H N Lee, S Sathyamurthy, D H Lowndes, Review of Scientific Instruments. 749Christen, H. M.; Rouleau, C. M.; Ohkubo, I.; Zhai, H. Y.; Lee, H. N.; Sathyamurthy, S.; Lowndes, D. H., An improved continuous compositional-spread technique based on pulsed-laser deposition and applicable to large substrate areas. Review of Scientific Instruments 2003, 74 (9), 4058-4062.</p>
<p>Identification of a Blue Photoluminescent Composite Material from a Combinatorial Library. J Wang, Y Yoo, C Gao, I Takeuchi, X Sun, H Chang, X.-D Xiang, P G Schultz, Science. 2795357Wang, J.; Yoo, Y.; Gao, C.; Takeuchi, I.; Sun, X.; Chang, H.; Xiang, X.-D.; Schultz, P. G., Identification of a Blue Photoluminescent Composite Material from a Combinatorial Library. Science 1998, 279 (5357), 1712-1714.</p>
<p>Discovery of Wall-Selective Carbon Nanotube Growth Conditions via Automated Experimentation. P Nikolaev, D Hooper, N Perea-López, M Terrones, B Maruyama, ACS Nano. 810Nikolaev, P.; Hooper, D.; Perea-López, N.; Terrones, M.; Maruyama, B., Discovery of Wall- Selective Carbon Nanotube Growth Conditions via Automated Experimentation. ACS Nano 2014, 8 (10), 10214-10222.</p>
<p>Autonomy in materials research: a case study in carbon nanotube growth. P Nikolaev, D Hooper, F Webber, R Rao, K Decker, M Krein, J Poleski, R Barto, B Maruyama, Computational Materials. 2016116031Nikolaev, P.; Hooper, D.; Webber, F.; Rao, R.; Decker, K.; Krein, M.; Poleski, J.; Barto, R.; Maruyama, B., Autonomy in materials research: a case study in carbon nanotube growth. npj Computational Materials 2016, 2 (1), 16031.</p>
<p>Organic synthesis in a modular robotic system driven by a chemical programming language. S Steiner, J Wolf, S Glatzel, A Andreou, J M Granda, G Keenan, T Hinkley, G Aragon-Camarasa, P J Kitson, D Angelone, L Cronin, Science. 201964232211Steiner, S.; Wolf, J.; Glatzel, S.; Andreou, A.; Granda, J. M.; Keenan, G.; Hinkley, T.; Aragon- Camarasa, G.; Kitson, P. J.; Angelone, D.; Cronin, L., Organic synthesis in a modular robotic system driven by a chemical programming language. Science 2019, 363 (6423), eaav2211.</p>
<p>Convergence of multiple synthetic paradigms in a universally programmable chemical synthesis machine. D Angelone, A J S Hammer, S Rohrbach, S Krambeck, J M Granda, J Wolf, S Zalesskiy, G Chisholm, L Cronin, Nat. Chem. 20211Angelone, D.; Hammer, A. J. S.; Rohrbach, S.; Krambeck, S.; Granda, J. M.; Wolf, J.; Zalesskiy, S.; Chisholm, G.; Cronin, L., Convergence of multiple synthetic paradigms in a universally programmable chemical synthesis machine. Nat. Chem. 2021, 13 (1), 63-69.</p>
<p>Self-Driven Multistep Quantum Dot Synthesis Enabled by Autonomous Robotic Experimentation in Flow. K Abdel-Latif, R W Epps, F Bateni, S Han, K G Reyes, M Abolhasani, Advanced Intelligent Systems. 20212Abdel-Latif, K.; Epps, R. W.; Bateni, F.; Han, S.; Reyes, K. G.; Abolhasani, M., Self-Driven Multistep Quantum Dot Synthesis Enabled by Autonomous Robotic Experimentation in Flow. Advanced Intelligent Systems 2021, 3 (2), 2000245.</p>
<p>Artificial Chemist: An Autonomous Quantum Dot Synthesis Bot. R W Epps, M S Bowen, A A Volk, K Abdel-Latif, S Han, K G Reyes, A Amassian, M Abolhasani, Adv. Mater. 202030Epps, R. W.; Bowen, M. S.; Volk, A. A.; Abdel-Latif, K.; Han, S.; Reyes, K. G.; Amassian, A.; Abolhasani, M., Artificial Chemist: An Autonomous Quantum Dot Synthesis Bot. Adv. Mater. 2020, 32 (30), 2001626.</p>
<p>Automated microfluidic platform for systematic studies of colloidal perovskite nanocrystals: towards continuous nano-manufacturing. R W Epps, K C Felton, C W Coley, M Abolhasani, 17Epps, R. W.; Felton, K. C.; Coley, C. W.; Abolhasani, M., Automated microfluidic platform for systematic studies of colloidal perovskite nanocrystals: towards continuous nano-manufacturing. Lab on a Chip 2017, 17 (23), 4040-4047.</p>
<p>Chemical Robotics Enabled Exploration of Stability in Multicomponent Lead Halide Perovskites via Machine Learning. K Higgins, S M Valleti, M Ziatdinov, S V Kalinin, M Ahmadi, ACS Energy Lett. 202011Higgins, K.; Valleti, S. M.; Ziatdinov, M.; Kalinin, S. V.; Ahmadi, M., Chemical Robotics Enabled Exploration of Stability in Multicomponent Lead Halide Perovskites via Machine Learning. ACS Energy Lett. 2020, 5 (11), 3426-3436.</p>
<p>Chemical Robotics Enabled Exploration of Stability in Multicomponent Lead Halide Perovskites via Machine Learning. K Higgins, S M Valleti, M Ziatdinov, S V Kalinin, M Ahmadi, ACS Energy Letters. 2020Higgins, K.; Valleti, S. M.; Ziatdinov, M.; Kalinin, S. V.; Ahmadi, M., Chemical Robotics Enabled Exploration of Stability in Multicomponent Lead Halide Perovskites via Machine Learning. ACS Energy Letters 2020, 3426-3436.</p>
<p>Exploring the physics of cesium lead halide perovskite quantum dots via Bayesian inference of the photoluminescence spectra in automated experiment Nanophotonics. A Heimbrook, K Higgins, S V Kalinin, M Ahmadi, Heimbrook, A.; Higgins, K.; Kalinin, S. V.; Ahmadi, M., Exploring the physics of cesium lead halide perovskite quantum dots via Bayesian inference of the photoluminescence spectra in automated experiment Nanophotonics 2021.</p>
<p>Robot-Based High-Throughput Screening of Antisolvents for Lead Halide Perovskites. E Gu, X Tang, S Langner, P Duchstein, Y Zhao, I Levchuk, V Kalancha, T Stubhan, J Hauch, H J Egelhaaf, D Zahn, A Osvet, C J Brabec, Joule. 20208Gu, E.; Tang, X.; Langner, S.; Duchstein, P.; Zhao, Y.; Levchuk, I.; Kalancha, V.; Stubhan, T.; Hauch, J.; Egelhaaf, H. J.; Zahn, D.; Osvet, A.; Brabec, C. J., Robot-Based High-Throughput Screening of Antisolvents for Lead Halide Perovskites. Joule 2020, 4 (8), 1806-1822.</p>
<p>Crystal Site Feature Embedding Enables Exploration of Large Chemical Spaces. H Choubisa, M Askerka, K Ryczko, O Voznyy, K Mills, I Tamblyn, E H Sargent, Matter. 20202Choubisa, H.; Askerka, M.; Ryczko, K.; Voznyy, O.; Mills, K.; Tamblyn, I.; Sargent, E. H., Crystal Site Feature Embedding Enables Exploration of Large Chemical Spaces. Matter 2020, 3 (2), 433-448.</p>
<p>Discovery of temperature-induced stability reversal in perovskites using high-throughput robotic learning. Y Zhao, J Zhang, Z Xu, S Sun, S Langner, N T P Hartono, T Heumueller, Y Hou, J Elia, N Li, G J Matt, X Du, W Meng, A Osvet, K Zhang, T Stubhan, Y Feng, J Hauch, E H Sargent, T Buonassisi, C J Brabec, Nat. Commun. 202112191Zhao, Y.; Zhang, J.; Xu, Z.; Sun, S.; Langner, S.; Hartono, N. T. P.; Heumueller, T.; Hou, Y.; Elia, J.; Li, N.; Matt, G. J.; Du, X.; Meng, W.; Osvet, A.; Zhang, K.; Stubhan, T.; Feng, Y.; Hauch, J.; Sargent, E. H.; Buonassisi, T.; Brabec, C. J., Discovery of temperature-induced stability reversal in perovskites using high-throughput robotic learning. Nat. Commun. 2021, 12 (1), 2191.</p>
<p>Z Li, M A Najeeb, L Alves, A Z Sherman, V Shekar, P Cruz Parrilla, I M Pendleton, W Wang, P W Nega, M Zeller, J Schrier, A J Norquist, E M Chan, Robot-Accelerated Perovskite Investigation and Discovery. 2020Li, Z.; Najeeb, M. A.; Alves, L.; Sherman, A. Z.; Shekar, V.; Cruz Parrilla, P.; Pendleton, I. M.; Wang, W.; Nega, P. W.; Zeller, M.; Schrier, J.; Norquist, A. J.; Chan, E. M., Robot-Accelerated Perovskite Investigation and Discovery. Chemistry of Materials 2020, 32 (13), 5650-5663.</p>
<p>Self-driving laboratory for accelerated discovery of thin-film materials. B P Macleod, F G L Parlane, T D Morrissey, F Hase, L M Roch, K E Dettelbach, R Moreira, L P E Yunker, M B Rooney, J R Deeth, V Lai, G J Ng, H Situ, R H Zhang, M S Elliott, T H Haley, D J Dvorak, A Aspuru-Guzik, J E Hein, C P Berlinguette, Science Advances. 202020MacLeod, B. P.; Parlane, F. G. L.; Morrissey, T. D.; Hase, F.; Roch, L. M.; Dettelbach, K. E.; Moreira, R.; Yunker, L. P. E.; Rooney, M. B.; Deeth, J. R.; Lai, V.; Ng, G. J.; Situ, H.; Zhang, R. H.; Elliott, M. S.; Haley, T. H.; Dvorak, D. J.; Aspuru-Guzik, A.; Hein, J. E.; Berlinguette, C. P., Self-driving laboratory for accelerated discovery of thin-film materials. Science Advances 2020, 6 (20), 8.</p>
<p>Self-driving laboratory for accelerated discovery of thin-film materials. B P Macleod, F G L Parlane, T D Morrissey, F Häse, L M Roch, K E Dettelbach, R Moreira, L P E Yunker, M B Rooney, J R Deeth, V Lai, G J Ng, H Situ, R H Zhang, M S Elliott, T H Haley, D J Dvorak, A Aspuru-Guzik, J E Hein, C P Berlinguette, Science Advances. 2020208867MacLeod, B. P.; Parlane, F. G. L.; Morrissey, T. D.; Häse, F.; Roch, L. M.; Dettelbach, K. E.; Moreira, R.; Yunker, L. P. E.; Rooney, M. B.; Deeth, J. R.; Lai, V.; Ng, G. J.; Situ, H.; Zhang, R. H.; Elliott, M. S.; Haley, T. H.; Dvorak, D. J.; Aspuru-Guzik, A.; Hein, J. E.; Berlinguette, C. P., Self-driving laboratory for accelerated discovery of thin-film materials. Science Advances 2020, 6 (20), eaaz8867.</p>
<p>A robotic platform for flow synthesis of organic compounds informed by AI planning. C W Coley, D A Thomas, J A M Lummiss, J N Jaworski, C P Breen, V Schultz, T Hart, J S Fishman, L Rogers, H Gao, R W Hicklin, P P Plehiers, J Byington, J S Piotti, W H Green, A J Hart, T F Jamison, K F Jensen, Science. 201964531566Coley, C. W.; Thomas, D. A.; Lummiss, J. A. M.; Jaworski, J. N.; Breen, C. P.; Schultz, V.; Hart, T.; Fishman, J. S.; Rogers, L.; Gao, H.; Hicklin, R. W.; Plehiers, P. P.; Byington, J.; Piotti, J. S.; Green, W. H.; Hart, A. J.; Jamison, T. F.; Jensen, K. F., A robotic platform for flow synthesis of organic compounds informed by AI planning. Science 2019, 365 (6453), eaax1566.</p>
<p>A mobile robotic chemist. B Burger, P M Maffettone, V V Gusev, C M Aitchison, Y Bai, X Wang, X Li, B M Alston, B Li, R Clowes, N Rankin, B Harris, R S Sprick, A I Cooper, Nature. 20207815Burger, B.; Maffettone, P. M.; Gusev, V. V.; Aitchison, C. M.; Bai, Y.; Wang, X.; Li, X.; Alston, B. M.; Li, B.; Clowes, R.; Rankin, N.; Harris, B.; Sprick, R. S.; Cooper, A. I., A mobile robotic chemist. Nature 2020, 583 (7815), 237-241.</p>
<p>Beyond Ternary OPV: High-Throughput Experimentation and Self-Driving Laboratories Optimize Multicomponent Systems. S Langner, F Häse, J D Perea, T Stubhan, J Hauch, L M Roch, T Heumueller, A Aspuru-Guzik, C J Brabec, P Nikolaev, D Hooper, N Perea-Lopez, M Terrones, B Maruyama, 1907801. 25Adv. Mater. 202014Acs NanoLangner, S.; Häse, F.; Perea, J. D.; Stubhan, T.; Hauch, J.; Roch, L. M.; Heumueller, T.; Aspuru- Guzik, A.; Brabec, C. J., Beyond Ternary OPV: High-Throughput Experimentation and Self-Driving Laboratories Optimize Multicomponent Systems. Adv. Mater. 2020, 32 (14), 1907801. 25. Nikolaev, P.; Hooper, D.; Perea-Lopez, N.; Terrones, M.; Maruyama, B., Discovery of Wall- Selective Carbon Nanotube Growth Conditions via Automated Experimentation. Acs Nano 2014, 8 (10), 10214-10222.</p>
<p>Autonomy in materials research: a case study in carbon nanotube growth. P Nikolaev, D Hooper, F Webber, R Rao, K Decker, M Krein, J Poleski, R Barto, B Maruyama, npj Comput. Mater. 2016, 2, 6Nikolaev, P.; Hooper, D.; Webber, F.; Rao, R.; Decker, K.; Krein, M.; Poleski, J.; Barto, R.; Maruyama, B., Autonomy in materials research: a case study in carbon nanotube growth. npj Comput. Mater. 2016, 2, 6.</p>
<p>On-the-fly closed-loop materials discovery via Bayesian active learning. A G Kusne, H Yu, C Wu, H Zhang, J Hattrick-Simpers, B Decost, S Sarker, C Oses, C Toher, S Curtarolo, A V Davydov, R Agarwal, L A Bendersky, M Li, A Mehta, I Takeuchi, Nat. Commun. 202015966Kusne, A. G.; Yu, H.; Wu, C.; Zhang, H.; Hattrick-Simpers, J.; DeCost, B.; Sarker, S.; Oses, C.; Toher, C.; Curtarolo, S.; Davydov, A. V.; Agarwal, R.; Bendersky, L. A.; Li, M.; Mehta, A.; Takeuchi, I., On-the-fly closed-loop materials discovery via Bayesian active learning. Nat. Commun. 2020, 11 (1), 5966.</p>
<p>Autonomous experimentation systems for materials development: A community perspective. E Stach, B Decost, A G Kusne, J Hattrick-Simpers, K A Brown, K G Reyes, J Schrier, S Billinge, T Buonassisi, I Foster, C P Gomes, J M Gregoire, A Mehta, J Montoya, E Olivetti, C Park, E Rotenberg, S K Saikin, S Smullin, V Stanev, B Maruyama, Matter. 20219Stach, E.; DeCost, B.; Kusne, A. G.; Hattrick-Simpers, J.; Brown, K. A.; Reyes, K. G.; Schrier, J.; Billinge, S.; Buonassisi, T.; Foster, I.; Gomes, C. P.; Gregoire, J. M.; Mehta, A.; Montoya, J.; Olivetti, E.; Park, C.; Rotenberg, E.; Saikin, S. K.; Smullin, S.; Stanev, V.; Maruyama, B., Autonomous experimentation systems for materials development: A community perspective. Matter 2021, 4 (9), 2702-2726.</p>
<p>. R Pollice, G D Gomes, M Aldeghi, R J Hickman, M Krenn, C Lavigne, Lindner-Pollice, R.; Gomes, G. D.; Aldeghi, M.; Hickman, R. J.; Krenn, M.; Lavigne, C.; Lindner-</p>
<p>Data-Driven Strategies for Accelerated Materials Design. M D&apos;addario, A Nigam, C T Ser, Z P Yao, A Aspuru-Guzik, Accounts of Chemical Research. 20214D'Addario, M.; Nigam, A.; Ser, C. T.; Yao, Z. P.; Aspuru-Guzik, A., Data-Driven Strategies for Accelerated Materials Design. Accounts of Chemical Research 2021, 54 (4), 849-860.</p>
<p>A data fusion approach to optimize compositional stability of halide perovskites. S J Sun, A Tiihonen, F Oviedo, Z Liu, J Thapa, Y C Zhao, N T P Hartono, A Goyal, T Heumueller, C Batali, A Encinas, J J Yoo, R P Li, Z Ren, I M Peters, C J Brabec, M G Bawendi, V Stevanovic, J Fisher, T Buonassisi, Matter. 20214Sun, S. J.; Tiihonen, A.; Oviedo, F.; Liu, Z.; Thapa, J.; Zhao, Y. C.; Hartono, N. T. P.; Goyal, A.; Heumueller, T.; Batali, C.; Encinas, A.; Yoo, J. J.; Li, R. P.; Ren, Z.; Peters, I. M.; Brabec, C. J.; Bawendi, M. G.; Stevanovic, V.; Fisher, J.; Buonassisi, T., A data fusion approach to optimize compositional stability of halide perovskites. Matter 2021, 4 (4), 1305-1322.</p>
<p>Efficient Phase Diagram Sampling by Active Learning. C Y Dai, I R Bruss, S C Glotzer, J. Phys. Chem. B. 20207Dai, C. Y.; Bruss, I. R.; Glotzer, S. C., Efficient Phase Diagram Sampling by Active Learning. J. Phys. Chem. B 2020, 124 (7), 1275-1284.</p>
<p>Advances in Kriging-Based Autonomous X-Ray Scattering Experiments. M M Noack, G S Doerk, R P Li, M Fukuto, K G Yager, Scientific Reports. 2020117Noack, M. M.; Doerk, G. S.; Li, R. P.; Fukuto, M.; Yager, K. G., Advances in Kriging-Based Autonomous X-Ray Scattering Experiments. Scientific Reports 2020, 10 (1), 17.</p>
<p>. R Garnett, Bayesian Optimization, Cambridge University Press2022Garnett, R., Bayesian Optimization, https://bayesoptbook.com/. Cambridge University Press: 2022.</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, S Dieleman, D Grewe, J Nham, N Kalchbrenner, I Sutskever, T Lillicrap, M Leach, K Kavukcuoglu, T Graepel, D Hassabis, Nature. 20167587484Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.; van den Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam, V.; Lanctot, M.; Dieleman, S.; Grewe, D.; Nham, J.; Kalchbrenner, N.; Sutskever, I.; Lillicrap, T.; Leach, M.; Kavukcuoglu, K.; Graepel, T.; Hassabis, D., Mastering the game of Go with deep neural networks and tree search. Nature 2016, 529 (7587), 484-+.</p>
<p>Deep learning in neural networks: An overview. J Schmidhuber, Neural Netw. 61Schmidhuber, J., Deep learning in neural networks: An overview. Neural Netw. 2015, 61, 85-117.</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran, D Wierstra, S Legg, D Hassabis, Nature. 20157540Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.; Antonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg, S.; Hassabis, D., Human-level control through deep reinforcement learning. Nature 2015, 518 (7540), 529-533.</p>
<p>Transfer Learning for Reinforcement Learning Domains: A Survey. M E Taylor, P Stone, J. Mach. Learn. Res. 10Taylor, M. E.; Stone, P., Transfer Learning for Reinforcement Learning Domains: A Survey. J. Mach. Learn. Res. 2009, 10, 1633-1685.</p>
<p>Physics makes the difference: Bayesian optimization and active learning via augmented Gaussian process. M A Ziatdinov, A Ghosh, S V Kalinin, arXiv:2108.10280Ziatdinov, M. A.; Ghosh, A.; Kalinin, S. V., Physics makes the difference: Bayesian optimization and active learning via augmented Gaussian process. arXiv:2108.10280 2021.</p>
<p>Highthroughput growth temperature optimization of ferroelectric SrxBa1-xNb2O6 epitaxial thin films using a temperature gradient method. I Ohkubo, H M Christen, S V Kalinin, G E Jellison, C M Rouleau, D H Lowndes, Applied Physics Letters. 848Ohkubo, I.; Christen, H. M.; Kalinin, S. V.; Jellison, G. E.; Rouleau, C. M.; Lowndes, D. H., High- throughput growth temperature optimization of ferroelectric SrxBa1-xNb2O6 epitaxial thin films using a temperature gradient method. Applied Physics Letters 2004, 84 (8), 1350-1352.</p>
<p>Taking the Human Out of the Loop: A Review of Bayesian Optimization. B Shahriari, K Swersky, Z Wang, R P Adams, N D Freitas, Proceedings of the IEEE 2016. the IEEE 2016104Shahriari, B.; Swersky, K.; Wang, Z.; Adams, R. P.; Freitas, N. d., Taking the Human Out of the Loop: A Review of Bayesian Optimization. Proceedings of the IEEE 2016, 104 (1), 148-175.</p>
<p>Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). C E Rasmussen, C K I Williams, The MIT PressRasmussen, C. E.; Williams, C. K. I., Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press: 2005.</p>
<p>Bayesian Analysis with Python: Introduction to statistical modeling and probabilistic programming using PyMC3 and ArviZ. O Martin, Packt Publishing2nd EditionMartin, O., Bayesian Analysis with Python: Introduction to statistical modeling and probabilistic programming using PyMC3 and ArviZ, 2nd Edition. Packt Publishing: 2018.</p>
<p>A Student's Guide to Bayesian Statistics. B Lambert, SAGE Publications Ltd1 editionLambert, B., A Student's Guide to Bayesian Statistics. SAGE Publications Ltd; 1 edition: 2018.</p>
<p>Ultrafast current imaging by Bayesian inversion. S Somnath, K J H Law, A N Morozovska, P Maksymovych, Y Kim, X Lu, M Alexe, R Archibald, S V Kalinin, S Jesse, R K Vasudevan, Nat. Commun. 9Somnath, S.; Law, K. J. H.; Morozovska, A. N.; Maksymovych, P.; Kim, Y.; Lu, X.; Alexe, M.; Archibald, R.; Kalinin, S. V.; Jesse, S.; Vasudevan, R. K., Ultrafast current imaging by Bayesian inversion. Nat. Commun. 2018, 9.</p>
<p>Bayesian inference in band excitation scanning probe microscopy for optimal dynamic model selection in imaging. R K Vasudevan, K P Kelley, E Eliseev, S Jesse, H Funakubo, A Morozovska, S V Kalinin, J. Appl. Phys. 2020510Vasudevan, R. K.; Kelley, K. P.; Eliseev, E.; Jesse, S.; Funakubo, H.; Morozovska, A.; Kalinin, S. V., Bayesian inference in band excitation scanning probe microscopy for optimal dynamic model selection in imaging. J. Appl. Phys 2020, 128 (5), 10.</p>
<p>Exploring physics of ferroelectric domain walls via Bayesian analysis of atomically resolved STEM data. C T Nelson, R K Vasudevan, X H Zhang, M Ziatdinov, E A Eliseev, I Takeuchi, A N Morozovska, S V Kalinin, Nat. Commun. 2020112Nelson, C. T.; Vasudevan, R. K.; Zhang, X. H.; Ziatdinov, M.; Eliseev, E. A.; Takeuchi, I.; Morozovska, A. N.; Kalinin, S. V., Exploring physics of ferroelectric domain walls via Bayesian analysis of atomically resolved STEM data. Nat. Commun. 2020, 11 (1), 12.</p>
<p>An Introduction to MCMC for Machine Learning. C Andrieu, N De Freitas, A Doucet, M I Jordan, Machine Learning. 501Andrieu, C.; de Freitas, N.; Doucet, A.; Jordan, M. I., An Introduction to MCMC for Machine Learning. Machine Learning 2003, 50 (1), 5-43.</p>
<p>Handbook of markov chain monte carlo. S Brooks, A Gelman, G Jones, X.-L Meng, CRC pressBrooks, S.; Gelman, A.; Jones, G.; Meng, X.-L., Handbook of markov chain monte carlo. CRC press: 2011.</p>
<p>. A Mcdannald, M Frontzek, A T Savici, M Doucet, E E Rodriguez, K Meuse, Opsahl-McDannald, A.; Frontzek, M.; Savici, A. T.; Doucet, M.; Rodriguez, E. E.; Meuse, K.; Opsahl-</p>
<p>J Ong, D Samarov, I Takeuchi, A G Kusne, arXiv:2108.08918On-the-fly Autonomous Control of Neutron Diffraction via Physics-Informed Bayesian Active Learning. arXiv preprintOng, J.; Samarov, D.; Takeuchi, I.; Kusne, A. G., On-the-fly Autonomous Control of Neutron Diffraction via Physics-Informed Bayesian Active Learning. arXiv preprint arXiv:2108.08918 2021.</p>
<p>Composable Effects for Flexible and Accelerated Probabilistic Programming in NumPyro. arxiv preprint: ArXiv:1912. D Phan, N Pradhan, M Jankowiak, Phan, D.; Pradhan, N.; Jankowiak, M., Composable Effects for Flexible and Accelerated Probabilistic Programming in NumPyro. arxiv preprint: ArXiv:1912.11554 2019.</p>
<p>Universal Behavior and Electric-Field-Induced Structural Transition in Rare-Earth-Substituted BiFeO(3). D Kan, L Palova, V Anbusathaiah, C J Cheng, S Fujino, V Nagarajan, K M Rabe, I Takeuchi, Advanced Functional Materials. 207Kan, D.; Palova, L.; Anbusathaiah, V.; Cheng, C. J.; Fujino, S.; Nagarajan, V.; Rabe, K. M.; Takeuchi, I., Universal Behavior and Electric-Field-Induced Structural Transition in Rare-Earth- Substituted BiFeO(3). Advanced Functional Materials 2010, 20 (7), 1108-1115.</p>
<p>Phase Transitions, Magnetic and Piezoelectric Properties of Rare-Earth-Substituted BiFeO3 Ceramics. I O Troyanchuk, D V Karpinsky, M V Bushinsky, O S Mantytskaya, N V Tereshko, V N Shut, Journal of the American Ceramic Society. 9412Troyanchuk, I. O.; Karpinsky, D. V.; Bushinsky, M. V.; Mantytskaya, O. S.; Tereshko, N. V.; Shut, V. N., Phase Transitions, Magnetic and Piezoelectric Properties of Rare-Earth-Substituted BiFeO3 Ceramics. Journal of the American Ceramic Society 2011, 94 (12), 4502-4506.</p>
<p>. R J Zeches, M D Rossell, J X Zhang, A J Hatt, Q He, C H Yang, A Kumar, C H Wang, A Melville, C Adamo, G Sheng, Y H Chu, J F Ihlefeld, R Erni, C Ederer, V Gopalan, L Q Chen, D G Schlom, N A Spaldin, L W Martin, R Ramesh, A Strain-Driven Morphotropic Phase Boundary in BiFeO. 3263Zeches, R. J.; Rossell, M. D.; Zhang, J. X.; Hatt, A. J.; He, Q.; Yang, C. H.; Kumar, A.; Wang, C. H.; Melville, A.; Adamo, C.; Sheng, G.; Chu, Y. H.; Ihlefeld, J. F.; Erni, R.; Ederer, C.; Gopalan, V.; Chen, L. Q.; Schlom, D. G.; Spaldin, N. A.; Martin, L. W.; Ramesh, R., A Strain-Driven Morphotropic Phase Boundary in BiFeO(3). Science 2009, 326 (5955), 977-980.</p>
<p>Atomic-scale evolution of modulated phases at the ferroelectric-antiferroelectric morphotropic phase boundary controlled by flexoelectric interaction. A Y Borisevich, E A Eliseev, A N Morozovska, C J Cheng, J Y Lin, Y H Chu, D Kan, I Takeuchi, V Nagarajan, S V Kalinin, Nat. Commun. 3Borisevich, A. Y.; Eliseev, E. A.; Morozovska, A. N.; Cheng, C. J.; Lin, J. Y.; Chu, Y. H.; Kan, D.; Takeuchi, I.; Nagarajan, V.; Kalinin, S. V., Atomic-scale evolution of modulated phases at the ferroelectric-antiferroelectric morphotropic phase boundary controlled by flexoelectric interaction. Nat. Commun. 2012, 3.</p>
<p>Predicting morphotropic phase boundary locations and transition temperatures in Pb-and Bi-based perovskite solid solutions from crystal chemical data and first-principles calculations. I Grinberg, M R Suchomel, P K Davies, A M Rappe, J. Appl. Phys. 998Grinberg, I.; Suchomel, M. R.; Davies, P. K.; Rappe, A. M., Predicting morphotropic phase boundary locations and transition temperatures in Pb-and Bi-based perovskite solid solutions from crystal chemical data and first-principles calculations. J. Appl. Phys 2005, 98 (9).</p>
<p>Review of crystal and domain structures in the PbZrxTi1-xO3 solid solution. D I Woodward, J Knudsen, I M Reaney, Phys. Rev. B. 1072Woodward, D. I.; Knudsen, J.; Reaney, I. M., Review of crystal and domain structures in the PbZrxTi1-xO3 solid solution. Phys. Rev. B 2005, 72 (10).</p>
<p>Ferroelectric, dielectric and piezoelectric properties of ferroelectric thin films and ceramics. D Damjanovic, Rep. Prog. Phys. 9Damjanovic, D., Ferroelectric, dielectric and piezoelectric properties of ferroelectric thin films and ceramics. Rep. Prog. Phys. 1998, 61 (9), 1267-1324.</p>
<p>Complexity in strongly correlated electronic systems. E Dagotto, Science. 3095732Dagotto, E., Complexity in strongly correlated electronic systems. Science 2005, 309 (5732), 257- 262.</p>
<p>Colossal magnetoresistant materials: The key role of phase separation. E Dagotto, T Hotta, A Moreo, Phys. Rep.-Rev. Sec. Phys. Lett. 3441-3Dagotto, E.; Hotta, T.; Moreo, A., Colossal magnetoresistant materials: The key role of phase separation. Phys. Rep.-Rev. Sec. Phys. Lett. 2001, 344 (1-3), 1-153.</p>
<p>Metal-insulator transitions. M Imada, A Fujimori, Y Tokura, Reviews of Modern Physics. 704Imada, M.; Fujimori, A.; Tokura, Y., Metal-insulator transitions. Reviews of Modern Physics 1998, 70 (4), 1039-1263.</p>
<p>Magnetic control of ferroelectric polarization. T Kimura, T Goto, H Shintani, K Ishizaka, T Arima, Y Tokura, Nature. 4266962Kimura, T.; Goto, T.; Shintani, H.; Ishizaka, K.; Arima, T.; Tokura, Y., Magnetic control of ferroelectric polarization. Nature 2003, 426 (6962), 55-58.</p>
<p>Critical features of colossal magnetoresistive manganites. Y Tokura, Rep. Prog. Phys. 693Tokura, Y., Critical features of colossal magnetoresistive manganites. Rep. Prog. Phys. 2006, 69 (3), 797-851.</p>
<p>Causal analysis of competing atomistic mechanisms in ferroelectric materials from high-resolution scanning transmission electron microscopy data. M Ziatdinov, C T Nelson, X H Zhang, R K Vasudevan, E Eliseev, A N Morozovska, I Takeuchi, S V Kalinin, Npj Computational Materials. 202019Ziatdinov, M.; Nelson, C. T.; Zhang, X. H.; Vasudevan, R. K.; Eliseev, E.; Morozovska, A. N.; Takeuchi, I.; Kalinin, S. V., Causal analysis of competing atomistic mechanisms in ferroelectric materials from high-resolution scanning transmission electron microscopy data. Npj Computational Materials 2020, 6 (1), 9.</p>
<p>. O Kolosov, A Gruverman, J Hatano, K Takahashi, H Tokumoto, Nanoscale, Control, Ferroelectric, By Atomic-Force, Microscopy, Phys. Rev. Lett. 21Kolosov, O.; Gruverman, A.; Hatano, J.; Takahashi, K.; Tokumoto, H., NANOSCALE VISUALIZATION AND CONTROL OF FERROELECTRIC DOMAINS BY ATOMIC-FORCE MICROSCOPY. Phys. Rev. Lett. 1995, 74 (21), 4309-4312.</p>
<p>Ferroelectric domain characterisation and manipulation: a challenge for scanning probe microscopy. L M Eng, M Bammerlin, C Loppacher, M Guggisberg, R Bennewitz, R Luthi, E Meyer, T Huser, H Heinzelmann, H J Guntherodt, Ferroelectrics. 2221-4Eng, L. M.; Bammerlin, M.; Loppacher, C.; Guggisberg, M.; Bennewitz, R.; Luthi, R.; Meyer, E.; Huser, T.; Heinzelmann, H.; Guntherodt, H. J., Ferroelectric domain characterisation and manipulation: a challenge for scanning probe microscopy. Ferroelectrics 1999, 222 (1-4), 153-162.</p>
<p>Nanoscale ferroelectrics: processing, characterization and future trends. A Gruverman, A Kholkin, Rep. Prog. Phys. 8Gruverman, A.; Kholkin, A., Nanoscale ferroelectrics: processing, characterization and future trends. Rep. Prog. Phys. 2006, 69 (8), 2443-2474.</p>
<p>Electromechanical Imaging and Spectroscopy of Ferroelectric and Piezoelectric Materials: State of the Art and Prospects for the Future. N Balke, I Bdikin, S V Kalinin, A L Kholkin, Journal of the American Ceramic Society. 928Balke, N.; Bdikin, I.; Kalinin, S. V.; Kholkin, A. L., Electromechanical Imaging and Spectroscopy of Ferroelectric and Piezoelectric Materials: State of the Art and Prospects for the Future. Journal of the American Ceramic Society 2009, 92 (8), 1629-1647.</p>
<p>Three-dimensional electric field probing of ferroelectrics on the nanometer scale using scanning force microscopy. L M Eng, S Grafstrom, C Loppacher, F Schlaphof, S Trogisch, A Roelofs, R Waser, Advances in Solid State Physics 41. Kramer, B., Ed; Berlin; BerlinSpringer-VerlagEng, L. M.; Grafstrom, S.; Loppacher, C.; Schlaphof, F.; Trogisch, S.; Roelofs, A.; Waser, R., Three-dimensional electric field probing of ferroelectrics on the nanometer scale using scanning force microscopy. In Advances in Solid State Physics 41, Kramer, B., Ed. Springer-Verlag Berlin: Berlin, 2001;</p>
<p>Piezoresponse force microscopy and nanoferroic phenomena. A Gruverman, M Alexe, D Meier, Nat. Commun. 20191Gruverman, A.; Alexe, M.; Meier, D., Piezoresponse force microscopy and nanoferroic phenomena. Nat. Commun. 2019, 10 (1), 1-9.</p>
<p>Effect of surface charges on the polarization of BaTiO3 thin films investigated by UHV-SPM. K Suzuki, T Hosokura, T Okamoto, J Steffes, K Murayama, N Tanaka, B D Huey, Journal of the American Ceramic Society. 10110Suzuki, K.; Hosokura, T.; Okamoto, T.; Steffes, J.; Murayama, K.; Tanaka, N.; Huey, B. D., Effect of surface charges on the polarization of BaTiO3 thin films investigated by UHV-SPM. Journal of the American Ceramic Society 2018, 101 (10), 4677-4688.</p>
<p>Applications of piezoresponse force microscopy in materials research: from inorganic ferroelectrics to biopiezoelectrics and beyond. D Denning, J Guyonnet, B J Rodriguez, International Materials Reviews. 611Denning, D.; Guyonnet, J.; Rodriguez, B. J., Applications of piezoresponse force microscopy in materials research: from inorganic ferroelectrics to biopiezoelectrics and beyond. International Materials Reviews 2016, 61 (1), 46-70.</p>
<p>Switchable induced polarization in LaAlO3/SrTiO3 heterostructures. C Bark, P Sharma, Y Wang, S H Baek, S Lee, S Ryu, C Folkman, T R Paudel, A Kumar, S V Kalinin, Nano Letters. 124Bark, C.; Sharma, P.; Wang, Y.; Baek, S. H.; Lee, S.; Ryu, S.; Folkman, C.; Paudel, T. R.; Kumar, A.; Kalinin, S. V., Switchable induced polarization in LaAlO3/SrTiO3 heterostructures. Nano Letters 2012, 12 (4), 1765-1771.</p>
<p>Nanoscale domains and local piezoelectric hysteresis in Pb(Zn1/3Nb2/3)O-3-4.5%PbTIO3 single crystals. I K Bdikin, V V Shvartsman, A L Kholkin, Appl. Phys. Lett. 8320Bdikin, I. K.; Shvartsman, V. V.; Kholkin, A. L., Nanoscale domains and local piezoelectric hysteresis in Pb(Zn1/3Nb2/3)O-3-4.5%PbTIO3 single crystals. Appl. Phys. Lett. 2003, 83 (20), 4232-4234.</p>
<p>Domain dynamics in piezoresponse force spectroscopy: Quantitative deconvolution and hysteresis loop fine structure. I K Bdikin, A L Kholkin, A N Morozovska, S V Svechnikov, S H Kim, S V Kalinin, Applied Physics Letters. 1892Bdikin, I. K.; Kholkin, A. L.; Morozovska, A. N.; Svechnikov, S. V.; Kim, S. H.; Kalinin, S. V., Domain dynamics in piezoresponse force spectroscopy: Quantitative deconvolution and hysteresis loop fine structure. Applied Physics Letters 2008, 92 (18).</p>
<p>Domain nucleation and hysteresis loop shape in piezoresponse force spectroscopy. A N Morozovska, E A Eliseev, S V Kalinin, Applied Physics Letters. 89Morozovska, A. N.; Eliseev, E. A.; Kalinin, S. V., Domain nucleation and hysteresis loop shape in piezoresponse force spectroscopy. Applied Physics Letters 2006, 89 (19).</p>
<p>Local polarization switching in piezoresponse force microscopy. A N Morozovska, S V Kalinin, E A Eliseev, S V Svechnikov, Ferroelectrics. 354Morozovska, A. N.; Kalinin, S. V.; Eliseev, E. A.; Svechnikov, S. V., Local polarization switching in piezoresponse force microscopy. Ferroelectrics 2007, 354, 198-207.</p>
<p>Quantitative mapping of switching behavior in piezoresponse force microscopy. S Jesse, H N Lee, S V Kalinin, Review of Scientific Instruments. 77773702Jesse, S.; Lee, H. N.; Kalinin, S. V., Quantitative mapping of switching behavior in piezoresponse force microscopy. Review of Scientific Instruments 2006, 77 (7), 073702.</p>
<p>Switching spectroscopy piezoresponse force microscopy of ferroelectric materials. S Jesse, A P Baddorf, S V Kalinin, Applied Physics Letters. 88662908Jesse, S.; Baddorf, A. P.; Kalinin, S. V., Switching spectroscopy piezoresponse force microscopy of ferroelectric materials. Applied Physics Letters 2006, 88 (6), 062908.</p>
<p>Nanoelectromechanics of piezoresponse force microscopy. S V Kalinin, E Karapetian, M Kachanov, Phys. Rev. B. 18184101Kalinin, S. V.; Karapetian, E.; Kachanov, M., Nanoelectromechanics of piezoresponse force microscopy. Phys. Rev. B 2004, 70 (18), 184101.</p>
<p>Stiffness relations for piezoelectric indentation of flat and non-flat punches of arbitrary planform: Applications to probing nanoelectromechanical properties of materials. E Karapetian, M Kachanov, S V Kalinin, Journal of the Mechanics and Physics of Solids. 574Karapetian, E.; Kachanov, M.; Kalinin, S. V., Stiffness relations for piezoelectric indentation of flat and non-flat punches of arbitrary planform: Applications to probing nanoelectromechanical properties of materials. Journal of the Mechanics and Physics of Solids 2009, 57 (4), 673-688.</p>
<p>Materials contrast in piezoresponse force microscopy. S V Kalinin, E A Eliseev, A N Morozovska, Applied Physics Letters. 2388Kalinin, S. V.; Eliseev, E. A.; Morozovska, A. N., Materials contrast in piezoresponse force microscopy. Applied Physics Letters 2006, 88 (23).</p>
<p>Electromechanical detection in scanning probe microscopy: Tip models and materials contrast. E A Eliseev, S V Kalinin, S Jesse, S L Bravina, A N Morozovska, J. Appl. Phys. 1102Eliseev, E. A.; Kalinin, S. V.; Jesse, S.; Bravina, S. L.; Morozovska, A. N., Electromechanical detection in scanning probe microscopy: Tip models and materials contrast. J. Appl. Phys 2007, 102 (1).</p>
<p>Depolarization in modeling nano-scale ferroelectrics using the Landau free energy functional. C H Woo, Y Zheng, Applied Physics a-Materials Science &amp; Processing. 911Woo, C. H.; Zheng, Y., Depolarization in modeling nano-scale ferroelectrics using the Landau free energy functional. Applied Physics a-Materials Science &amp; Processing 2008, 91 (1), 59-63.</p>
<p>Melting of spatially modulated phases at domain wall/surface junctions in antiferrodistortive multiferroics. A N Morozovska, E A Eliseev, D Y Chen, V Shvetz, C T Nelson, S V Kalinin, Phys. Rev. B. 2020714Morozovska, A. N.; Eliseev, E. A.; Chen, D. Y.; Shvetz, V.; Nelson, C. T.; Kalinin, S. V., Melting of spatially modulated phases at domain wall/surface junctions in antiferrodistortive multiferroics. Phys. Rev. B 2020, 102 (7), 14.</p>
<p>A combined theoretical and experimental study of the phase coexistence and morphotropic boundaries in ferroelectric-antiferroelectric-antiferrodistortive multiferroics. A N Morozovska, D V Karpinsky, D O Alikin, A Abramov, E A Eliseev, M D Glinchuk, A D Yaremkevich, O M Fesenko, T V Tsebrienko, A Pakalniskis, A Kareiva, M V Silibin, V V Sidski, S V Kalinin, A L Kholkin, Acta Materialia. 21314Morozovska, A. N.; Karpinsky, D. V.; Alikin, D. O.; Abramov, A.; Eliseev, E. A.; Glinchuk, M. D.; Yaremkevich, A. D.; Fesenko, O. M.; Tsebrienko, T. V.; Pakalniskis, A.; Kareiva, A.; Silibin, M. V.; Sidski, V. V.; Kalinin, S. V.; Kholkin, A. L., A combined theoretical and experimental study of the phase coexistence and morphotropic boundaries in ferroelectric-antiferroelectric-antiferrodistortive multiferroics. Acta Materialia 2021, 213, 14.</p>
<p>Distilling Free-Form Natural Laws from Experimental Data. M Schmidt, H Lipson, Science. 3245923Schmidt, M.; Lipson, H., Distilling Free-Form Natural Laws from Experimental Data. Science 2009, 324 (5923), 81-85.</p>
<p>PySINDy: A comprehensive Python package for robust sparse system identification. A A Kaptanoglu, B M De Silva, U Fasel, K Kaheman, J L Callaham, C B Delahunt, K Champion, J.-C Loiseau, J N Kutz, S L Brunton, arXiv:2111.08481arXiv preprintKaptanoglu, A. A.; de Silva, B. M.; Fasel, U.; Kaheman, K.; Callaham, J. L.; Delahunt, C. B.; Champion, K.; Loiseau, J.-C.; Kutz, J. N.; Brunton, S. L., PySINDy: A comprehensive Python package for robust sparse system identification. arXiv preprint arXiv:2111.08481 2021.</p>
<p>M Cranmer, A Sanchez-Gonzalez, P Battaglia, R Xu, K Cranmer, D Spergel, S J Ho, Discovering symbolic models from deep learning with inductive biases. Cranmer, M.; Sanchez-Gonzalez, A.; Battaglia, P.; Xu, R.; Cranmer, K.; Spergel, D.; Ho, S. J. a. p. a., Discovering symbolic models from deep learning with inductive biases. 2020.</p>
<p>SISSO: A compressedsensing method for identifying the best low-dimensional descriptor in an immensity of offered candidates. R Ouyang, S Curtarolo, E Ahmetcik, M Scheffler, L M Ghiringhelli, Physical Review Materials. 2018883802Ouyang, R.; Curtarolo, S.; Ahmetcik, E.; Scheffler, M.; Ghiringhelli, L. M., SISSO: A compressed- sensing method for identifying the best low-dimensional descriptor in an immensity of offered candidates. Physical Review Materials 2018, 2 (8), 083802.</p>
<p>Symbolic regression in materials science. Y Wang, N Wagner, J M J M C Rondinelli, 2019Wang, Y.; Wagner, N.; Rondinelli, J. M. J. M. C., Symbolic regression in materials science. 2019, 9 (3), 793-805.</p>
<p>Logic Guided Genetic Algorithms. D Ashok, J Scott, S J Wetzel, M Panju, V Ganesh, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2021Ashok, D.; Scott, J.; Wetzel, S. J.; Panju, M.; Ganesh, V., Logic Guided Genetic Algorithms. Proceedings of the AAAI Conference on Artificial Intelligence 2021, 35 (18), 15753-15754.</p>
<p>A physics-inspired method for symbolic regression. S.-M Udrescu, M Tegmark, Ai Feynman, Science Advances. 6162631Udrescu, S.-M.; Tegmark, M., AI Feynman: A physics-inspired method for symbolic regression. Science Advances 6 (16), eaay2631.</p>
<p>Materials Synthesis Insights from Scientific Literature via Text Extraction and Machine Learning. E Kim, K Huang, A Saunders, A Mccallum, G Ceder, E Olivetti, Chemistry of Materials. 2921Kim, E.; Huang, K.; Saunders, A.; McCallum, A.; Ceder, G.; Olivetti, E., Materials Synthesis Insights from Scientific Literature via Text Extraction and Machine Learning. Chemistry of Materials 2017, 29 (21), 9436-9444.</p>
<p>Analysis of citation networks as a new tool for scientific research. R K Vasudevan, M Ziatdinov, C Chen, S V Kalinin, MRS Bull. 12Vasudevan, R. K.; Ziatdinov, M.; Chen, C.; Kalinin, S. V., Analysis of citation networks as a new tool for scientific research. MRS Bull. 2016, 41 (12), 1009-1015.</p>
<p>Data mining for better material synthesis: The case of pulsed laser deposition of complex oxides. S R Young, A Maksov, M Ziatdinov, Y Cao, M Burch, J Balachandran, L L Li, S Somnath, R M Patton, S V Kalinin, R K Vasudevan, J. Appl. Phys. 1231111Young, S. R.; Maksov, A.; Ziatdinov, M.; Cao, Y.; Burch, M.; Balachandran, J.; Li, L. L.; Somnath, S.; Patton, R. M.; Kalinin, S. V.; Vasudevan, R. K., Data mining for better material synthesis: The case of pulsed laser deposition of complex oxides. J. Appl. Phys 2018, 123 (11), 11.</p>
<p>Unsupervised word embeddings capture latent knowledge from materials science literature. V Tshitoyan, J Dagdelen, L Weston, A Dunn, Z Q Rong, O Kononova, K A Persson, G Ceder, A Jain, Nature. 2019776395Tshitoyan, V.; Dagdelen, J.; Weston, L.; Dunn, A.; Rong, Z. Q.; Kononova, O.; Persson, K. A.; Ceder, G.; Jain, A., Unsupervised word embeddings capture latent knowledge from materials science literature. Nature 2019, 571 (7763), 95-+.</p>
<p>Learning from Imperfections: Predicting Structure and Thermodynamics from Atomic Imaging of Fluctuations. L Vlcek, M Ziatdinov, A Maksov, A Tselev, A P Baddorf, S V Kalinin, R K Vasudevan, ACS Nano. 131Vlcek, L.; Ziatdinov, M.; Maksov, A.; Tselev, A.; Baddorf, A. P.; Kalinin, S. V.; Vasudevan, R. K., Learning from Imperfections: Predicting Structure and Thermodynamics from Atomic Imaging of Fluctuations. ACS Nano 2019, 13 (1), 718-727.</p>
<p>Knowledge Extraction from Atomically Resolved Images. L Vlcek, A Maksov, M H Pan, R K Vasudevan, S V Kahnin, Acs Nano. 1110Vlcek, L.; Maksov, A.; Pan, M. H.; Vasudevan, R. K.; Kahnin, S. V., Knowledge Extraction from Atomically Resolved Images. Acs Nano 2017, 11 (10), 10313-10320.</p>            </div>
        </div>

    </div>
</body>
</html>
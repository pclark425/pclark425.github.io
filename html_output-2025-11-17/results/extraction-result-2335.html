<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2335 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2335</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2335</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-270560485</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.10557v5.pdf" target="_blank">Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence</a></p>
                <p><strong>Paper Abstract:</strong> The scientific method is the cornerstone of human progress across all branches of the natural and applied sciences, from understanding the human body to explaining how the universe works. The scientific method is based on identifying systematic rules or principles that describe the phenomenon of interest in a reproducible way that can be validated through experimental evidence. In the era of generative artificial intelligence, there are discussions on how AI systems may discover new knowledge. We argue that human complex reasoning for scientific discovery remains of vital importance, at least before the advent of artificial general intelligence. Yet, AI can be leveraged for scientific discovery via explainable AI. More specifically, knowing the `principles' the AI systems used to make decisions can be a point of contact with domain experts and scientists, that can lead to divergent or convergent views on a given scientific problem. Divergent views may spark further scientific investigations leading to interpretability-guided explanations (IGEs), and possibly to new scientific knowledge. We define this field as Explainable AI for Science, where domain experts -- potentially assisted by generative AI -- formulate scientific hypotheses and explanations based on the interpretability of a predictive AI system.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2335.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2335.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer (ECG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer neural network for ECG time-series classification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised Transformer model applied to multilead ECG time-series to classify cardiac pathologies (example: RBBB) and paired with post-hoc interpretability (SHAP) to produce relevance maps that expose the model's 'machine view'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Medicine — cardiac diagnostics (ECG classification)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Binary/multi-class classification of patient ECG recordings to identify underlying pathology (example task: classify pathology A vs B; demonstrated on right bundle branch block (RBBB) detection).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Implied labeled clinical ECG datasets are available and sufficient for supervised training in the reported experiments, but the paper does not quantify dataset size, provenance, or public accessibility.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Multichannel time series (multilead ECG signals); structured sequential data with lead/channel dimensions and temporal dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-to-high temporal complexity: non-linear, multivariate time-series with clinically-important local waveform features (e.g., R-wave morphology), inter-patient variability and potential noise/artifacts; requires capturing both local waveform and longer-range temporal context.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>High — cardiology and ECG interpretation are mature fields with extensive prior knowledge and well-defined diagnostic markers; strong domain expertise available to validate model explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — clinical deployment requires interpretable/causal understanding for trust, regulatory compliance, and actionable medical decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Transformer neural networks (time-series adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Supervised Transformer architecture adapted to time-series ECG classification, trained on labeled ECG recordings to predict pathology labels; post-hoc interpretability applied (SHAP) to generate relevance heatmaps highlighting which leads/time segments contributed to predictions (example: model focused on delayed/broadened R-wave in V1 for RBBB).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning (sequence models / attention-based models)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and appropriate for modeling temporal dependencies and multichannel ECG data; the paper demonstrates the approach as competitive and usable for generating machine views, but emphasizes the need for ARU (accuracy, reproducibility, understandability) and causality checks before scientific/clinical conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively effective in the example: the model correctly predicted RBBB and the SHAP relevance map highlighted a clinically-recognized feature (delayed/broadened R-wave in V1), indicating alignment between model focus and domain knowledge; authors warn of potential spurious correlations if interpretability is inaccurate.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — if validated, interpretable Transformer models can aid diagnosis, increase clinician trust, produce interpretability-guided explanations (IGEs) that map to known physiology, and potentially suggest new hypotheses about ECG-pathology relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively to human expertise (domain experts) rather than detailed numeric baselines; ante-hoc interpretable models are discussed as more faithful but possibly underperforming on complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of labeled ECG time-series, model capacity to capture temporal patterns (Transformer attention), use of post-hoc interpretability (SHAP) that produced clinically meaningful relevance maps, and involvement of domain experts to assess machine view (ARU criteria).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Applying attention-based sequence models to ECGs plus post-hoc interpretability can surface clinically meaningful features and produce machine views that align with domain knowledge, but rigorous ARU validation and causal assessment are required to avoid misleading conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2335.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2335.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer (Weather)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer neural network for spatio-temporal weather forecasting (heatwave prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised Transformer applied to spatio-temporal weather data to forecast extreme events (example: next-day heatwave), with SHAP used to produce relevance maps indicating variable and temporal importance (e.g., T2M, SLP, TCWV).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Climate science / meteorology — medium-range extreme event forecasting (heatwave detection)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Binary classification forecasting whether an extreme event (heatwave) will occur the following day using multivariate spatio-temporal atmospheric variables.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Described as abundant in the domain (large historical reanalysis and observational datasets are implied), though the paper provides no quantitative dataset description for the exemplar experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional spatio-temporal data (gridded atmospheric fields over time), multivariate continuous variables (e.g., 2-m temperature T2M, sea level pressure SLP, total column water vapour TCWV, total precipitation TP), with temporal sequences and spatial structure.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — large state space, non-linear dynamics, multiscale temporal and spatial dependencies, high dimensionality and chaotic features of atmospheric systems.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>High — meteorology is a well-established field with mature physics-based models and rich observational datasets; both classical forecasting and ML approaches are active research areas.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — for scientific insight and operational trust (e.g., hazard forecasting), interpretability and causal understanding are important to validate model predictions and understand precursors.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Transformer neural networks (spatio-temporal adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Supervised Transformer trained on multivariate spatio-temporal weather fields to predict next-day extreme event occurrence; post-hoc interpretability (SHAP) produced relevance maps showing reliance on recent T2M increases as primary indicator, with SLP and TCWV as supporting features and minimal influence from TP; the model prioritized recent days (Days -2 to -1).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning (sequence / spatio-temporal attention models)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate given abundant data and the need to capture temporal/spatial dependencies; demonstrated applicability in the thought experiment, but authors stress interpretability validation and causal checks before translating findings into scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively effective: relevance maps align with known heatwave precursors (recent warming and high-pressure anomalies), indicating the model learned plausible predictors; authors note potential for AI to outperform traditional methods in some cases but caution about spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Significant — could improve forecasting skill, provide interpretable precursors for domain experts, and generate interpretability-guided hypotheses to refine physical understanding and models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Paper cites broader evidence that ML/AI has outperformed traditional methods in some weather forecasting cases, but in this specific experiment comparisons are qualitative rather than numeric.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Abundant historical data, model architecture capable of spatio-temporal pattern learning (Transformer), and interpretability method (SHAP) producing domain-plausible importance maps; domain expert review to assess ARU and causality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>In data-rich, physically-grounded domains like weather forecasting, attention-based models with post-hoc interpretability can identify known precursors and offer machine views that both validate and potentially extend domain understanding, but rigorous ARU and causal analysis are essential.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2335.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2335.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SHAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SHapley Additive exPlanations (SHAP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-agnostic post-hoc interpretability method that assigns feature importance values (saliency/relevance) for individual predictions based on Shapley value concepts, used here to produce machine-view relevance maps for time-series and spatio-temporal models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A unified approach to interpreting model predictions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>General interpretability for scientific ML applications (applied to medicine and climate examples in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Recover the data/features the model deemed important for specific predictions to create a machine view that domain experts can inspect.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Method must operate over high-dimensional inputs and potentially structured data (time-series, spatio-temporal fields); computational cost scales with feature dimension and sampling strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature as an interpretability technique in ML literature; widely used across domains though caveats exist.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium-to-high — SHAP provides attribution maps but does not by itself deliver causality; domain experts require mechanistic interpretation and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>SHAP (post-hoc feature attribution)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Computes additive feature attributions approximating Shapley values to explain individual model predictions; produces saliency/relevance maps that highlight which input features or time/space regions contributed positively/negatively to a prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Post-hoc interpretability / explainable AI</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and convenient for generating machine views across model types; however, the paper highlights issues: explanations may be unreliable or non-unique and require accuracy and reproducibility validation (ARU).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Useful in examples (provided clinically- and physically-plausible relevance maps) but authors caution that post-hoc explanations can be inaccurate and need validation to avoid misleading scientists.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High when combined with ARU validation and domain expert analysis — can bridge machine outputs and human hypotheses, enabling interpretability-guided explanations (IGEs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted with other post-hoc methods (DeepLIFT, Grad-CAM) and ante-hoc/self-explaining approaches; trade-off noted between model-agnostic convenience and potential inaccuracy vs. intrinsic interpretability faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Careful validation of explanation accuracy, reproducibility checks, and ensuring explanations are understandable to domain experts (ARU); pairing with domain review reduces risk of scientific hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>SHAP can expose the model's feature-level 'machine view' across domains, but its interpretability value for science hinges on ARU validation and careful expert-driven explanation to avoid spurious inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2335.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2335.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Post-hoc interpretability (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Post-hoc interpretability methods (e.g., DeepLIFT, Grad-CAM, SHAP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Techniques applied after model training to produce saliency maps or feature attributions showing which inputs influenced predictions; model-agnostic and widely used to form a machine view for expert inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>General — applies across scientific ML problems (medicine, climate, materials, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provide explanations for black-box model outputs by attributing importance to input features, times, or spatial regions to inform scientists about model reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Operates over potentially high-dimensional and structured inputs; issues include non-uniqueness and sensitivity to model and input perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established set of methods with active research on limitations and validation techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — provides input importance but does not guarantee mechanistic or causal explanations; requires expert interpretation and further causal analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Post-hoc saliency/feature-attribution methods</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Algorithms (gradient-based, perturbation, Shapley-value approximations) that compute per-input or per-feature relevance scores for already-trained models; outputs are typically heatmaps or importance lists used as the machine view.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Post-hoc explainable AI</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Widely applicable to many model classes and tasks; caution advised because explanations may be unreliable or non-unique and should be validated (sanity checks, uncertainty modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Effective at producing interpretable artifacts for expert review in many cases, but susceptible to generating misleading or non-reproducible explanations without validation; the paper emphasizes accuracy, reproducibility, understandability (ARU) as necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Substantial if explanations are validated and integrated into scientific workflows — can spark hypotheses and guide further experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to ante-hoc/intrinsically interpretable models: post-hoc is more flexible and model-agnostic but often less faithful; ante-hoc models may be more faithful but can underperform on complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Robustness of explanation method, reproducibility across runs/samples, and interpretability to domain experts (ARU); combining with causal methods increases scientific value.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Post-hoc interpretability enables generation of machine views that can drive scientific inquiry, but their utility depends critically on accuracy, reproducibility, and understandability validated against domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2335.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2335.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ante-hoc / Intrinsic interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ante-hoc / intrinsic interpretable models (e.g., decision trees, generalized additive models, prototype-based methods, neural additive models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Model classes designed to be interpretable by construction, offering more faithful explanations of decision logic at the potential cost of reduced performance on highly complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>General — proposed as safer alternatives for high-stakes scientific applications (medicine, climate, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provide faithful, built-in explanations of model decisions to facilitate mechanistic interpretation and scientific insight.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Tend to be better suited to low-to-moderate complexity problems; may struggle with very high-dimensional or highly non-linear tasks where black-box models excel.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Well-established modeling families with growing research into scalable, expressive intrinsically interpretable architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — these methods are recommended when mechanistic/causal understanding is critical, such as high-stakes scientific or medical decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Ante-hoc / intrinsically interpretable models</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Models whose structure enables direct human-understandable reasoning (rules in decision trees, additive components in GAMs, prototype-based explanations); they map inputs to outputs with transparent internal representations that can be inspected without post-hoc methods.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Interpretable/symbolic/hybrid ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate when interpretability and fidelity are prioritized over raw predictive performance; may be insufficient for the most complex scientific prediction tasks without careful design or hybridization.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>More faithful explanation of decision logic compared to many post-hoc methods, but can underperform on complex tasks such as large-scale weather or protein structure prediction; trade-off between interpretability and performance emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High in safety-critical scientific contexts where model transparency is required; can prevent misleading scientific inferences and enable direct hypothesis extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted with post-hoc methods: ante-hoc offers higher fidelity of explanation but may be outperformed by black-box deep models on complex, data-rich tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Problem suitability (complexity vs interpretability needs), careful feature engineering, and alignment of model inductive biases with domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Intrinsic interpretability improves faithfulness of machine views for science, but the interpretability-vs-performance trade-off means hybrid approaches or improved intrinsic methods are needed for very complex domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2335.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2335.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic regression for equation discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that searches symbolic mathematical expressions to fit data, proposed as a pathway to convert interpretability-guided explanations (IGEs) into actionable mathematical models for scientific knowledge discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>General scientific model discovery (physics, biology, engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Derive compact mathematical expressions or laws from data or from hypotheses derived via model interpretability to produce human-understandable models.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Search over a combinatorial space of symbolic expressions; complexity grows with allowed operator set, expression depth, and number of variables.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Active research area with growing applications in scientific discovery; not a mature turnkey solution for all domains.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — objective is to produce mechanistic, interpretable mathematical relationships that can be validated experimentally or theoretically.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Symbolic regression (e.g., genetic programming, sparse regression approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Search-based or optimization-based methods that propose symbolic formulas to explain relationships observed in data or highlighted by model interpretability; can incorporate constraints or priors to favor physically-plausible expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Symbolic / equation-discovery / scientific ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Potentially applicable as a bridge from IGEs to explicit mathematical models, but the paper notes a significant gap remains and more development is required to make this pathway reliable in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Promising for converting interpretability-derived hypotheses into mathematical laws, but challenges remain in search tractability, robustness to noise, and ensuring physical plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — successful symbolic regression can translate machine-discovered patterns into human-readable scientific laws, enabling theory formation and deeper mechanistic insight.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Positioned as complementary to interpretability outputs and as an alternative to purely black-box models for producing explicit scientific knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Good candidate features/hypotheses from interpretability, constraints informed by domain knowledge, and robust optimization/search strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Symbolic regression is a promising route to turn interpretability-guided explanations into explicit mathematical models, but practical gaps and robustness challenges must be overcome for reliable scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2335.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2335.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Physics-informed NN (PINNs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-Informed Neural Networks (PINNs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural network frameworks that embed differential-equation-based physical laws into the loss function to solve forward and inverse PDE problems, cited as an example of domain-informed ML.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Computational physics / computational engineering — solving PDEs and inverse problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Solve forward and inverse problems governed by nonlinear partial differential equations while enforcing known physical constraints during training.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High computational complexity from solving PDEs in high-dimensional spatio-temporal domains; requires capturing nonlinearity and boundary/initial conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging-to-mature within computational science; active research area with many successful demonstrations but practical limitations for very large-scale problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — explicit incorporation of mechanistic (physical) constraints is central to PINN methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Physics-informed neural networks (PINNs)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Neural networks trained with loss terms that penalize deviation from governing PDEs and boundary/initial conditions, enabling data-efficient learning and embedding of mechanistic constraints into models.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Physics-informed / hybrid ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited where governing equations are known and can be encoded; particularly useful in data-sparse regimes or when combining observational data with theory.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Powerful for integrating physics and data; the paper cites PINNs as an example of scientific ML that embeds domain-specific knowledge and produces domain-constrained results.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Significant for scientific domains with known governing equations — can improve data efficiency, enforce physically-plausible solutions, and facilitate interpretable models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted with purely data-driven black-box models; PINNs offer stronger physical consistency but may be harder to scale for complex, large-scale systems.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of trustworthy governing equations, careful loss weighting, and numerical stability considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Embedding mechanistic knowledge into ML (PINNs) yields models that are more physically consistent and interpretable for scientific problems, especially when data is limited or physics is well-understood.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2335.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2335.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold (deep learning for protein structure prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep learning system (AlphaFold) that predicts 3D protein structures from amino-acid sequences with high accuracy; cited as a prominent example where AI learned scientific-relevant 'principles'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Highly accurate protein structure prediction with AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Structural biology — protein folding / structure prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict the three-dimensional folded structure of proteins from primary amino-acid sequence information.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Large public structural databases (e.g., PDB) and evolutionary sequence databases exist and were used to train AlphaFold; data availability is abundant relative to many scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Sequence data (protein amino-acid sequences), multiple sequence alignments, and 3D coordinate data (structures); multimodal between 1D sequence and 3D geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very high — high-dimensional conformational space, complex physics, many-body interactions; challenge to capture folding energetics and long-range contacts.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature experimental field with decades of structural biology and established databases; theoretical understanding incomplete in some aspects but rich empirical resources exist.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High for scientific insight; while black-box predictions can be useful, mechanistic/structural explanations remain important for biological interpretation and downstream applications.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep learning (AlphaFold's attention-based architectures / specialized model)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Complex deep network architecture that leverages sequence information, evolutionary couplings, attention mechanisms and structural modules to predict inter-residue distances and ultimately 3D coordinates; trained supervised on experimental structures and multiple-sequence-alignment features.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning / domain-informed foundation model</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable — AlphaFold demonstrated major advances in predicting protein structure and has broad applicability in biology and chemistry, though interpretability and mechanistic explanation are active research areas.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported as highly accurate in original literature; in the context of this paper AlphaFold serves as an example where ML outperformed many traditional approaches and motivated interest in extracting the learned 'principles'.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Very high — enables rapid structural insight, accelerates research in biology, medicine, and bioengineering, and motivates efforts to extract interpretable rules from high-performing models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared generally to traditional physics-based and homology-based structure prediction methods; ML achieved superior predictive performance in many cases as cited by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large labeled structural databases, clever architecture design capturing evolutionary and geometric constraints, and extensive engineering for training and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>High-performing deep models in mature, data-rich scientific domains (like AlphaFold in structural biology) can discover patterns that outperform traditional models, motivating the need for interpretability to translate model success into scientific understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning skillful medium-range global weather forecasting <em>(Rating: 2)</em></li>
                <li>Highly accurate protein structure prediction with AlphaFold <em>(Rating: 2)</em></li>
                <li>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations <em>(Rating: 2)</em></li>
                <li>Evaluation of posthoc interpretability methods in time-series classification <em>(Rating: 2)</em></li>
                <li>Reliable post hoc explanations: Modeling uncertainty in explainability <em>(Rating: 2)</em></li>
                <li>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead <em>(Rating: 2)</em></li>
                <li>Large language models for automated open-domain scientific hypotheses discovery <em>(Rating: 1)</em></li>
                <li>A unified approach to interpreting model predictions <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2335",
    "paper_id": "paper-270560485",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "Transformer (ECG)",
            "name_full": "Transformer neural network for ECG time-series classification",
            "brief_description": "A supervised Transformer model applied to multilead ECG time-series to classify cardiac pathologies (example: RBBB) and paired with post-hoc interpretability (SHAP) to produce relevance maps that expose the model's 'machine view'.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Medicine — cardiac diagnostics (ECG classification)",
            "problem_description": "Binary/multi-class classification of patient ECG recordings to identify underlying pathology (example task: classify pathology A vs B; demonstrated on right bundle branch block (RBBB) detection).",
            "data_availability": "Implied labeled clinical ECG datasets are available and sufficient for supervised training in the reported experiments, but the paper does not quantify dataset size, provenance, or public accessibility.",
            "data_structure": "Multichannel time series (multilead ECG signals); structured sequential data with lead/channel dimensions and temporal dependencies.",
            "problem_complexity": "Moderate-to-high temporal complexity: non-linear, multivariate time-series with clinically-important local waveform features (e.g., R-wave morphology), inter-patient variability and potential noise/artifacts; requires capturing both local waveform and longer-range temporal context.",
            "domain_maturity": "High — cardiology and ECG interpretation are mature fields with extensive prior knowledge and well-defined diagnostic markers; strong domain expertise available to validate model explanations.",
            "mechanistic_understanding_requirements": "High — clinical deployment requires interpretable/causal understanding for trust, regulatory compliance, and actionable medical decisions.",
            "ai_methodology_name": "Transformer neural networks (time-series adaptation)",
            "ai_methodology_description": "Supervised Transformer architecture adapted to time-series ECG classification, trained on labeled ECG recordings to predict pathology labels; post-hoc interpretability applied (SHAP) to generate relevance heatmaps highlighting which leads/time segments contributed to predictions (example: model focused on delayed/broadened R-wave in V1 for RBBB).",
            "ai_methodology_category": "Supervised deep learning (sequence models / attention-based models)",
            "applicability": "Applicable and appropriate for modeling temporal dependencies and multichannel ECG data; the paper demonstrates the approach as competitive and usable for generating machine views, but emphasizes the need for ARU (accuracy, reproducibility, understandability) and causality checks before scientific/clinical conclusions.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively effective in the example: the model correctly predicted RBBB and the SHAP relevance map highlighted a clinically-recognized feature (delayed/broadened R-wave in V1), indicating alignment between model focus and domain knowledge; authors warn of potential spurious correlations if interpretability is inaccurate.",
            "impact_potential": "High — if validated, interpretable Transformer models can aid diagnosis, increase clinician trust, produce interpretability-guided explanations (IGEs) that map to known physiology, and potentially suggest new hypotheses about ECG-pathology relationships.",
            "comparison_to_alternatives": "Compared qualitatively to human expertise (domain experts) rather than detailed numeric baselines; ante-hoc interpretable models are discussed as more faithful but possibly underperforming on complex tasks.",
            "success_factors": "Availability of labeled ECG time-series, model capacity to capture temporal patterns (Transformer attention), use of post-hoc interpretability (SHAP) that produced clinically meaningful relevance maps, and involvement of domain experts to assess machine view (ARU criteria).",
            "key_insight": "Applying attention-based sequence models to ECGs plus post-hoc interpretability can surface clinically meaningful features and produce machine views that align with domain knowledge, but rigorous ARU validation and causal assessment are required to avoid misleading conclusions.",
            "uuid": "e2335.0",
            "source_info": {
                "paper_title": "Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Transformer (Weather)",
            "name_full": "Transformer neural network for spatio-temporal weather forecasting (heatwave prediction)",
            "brief_description": "A supervised Transformer applied to spatio-temporal weather data to forecast extreme events (example: next-day heatwave), with SHAP used to produce relevance maps indicating variable and temporal importance (e.g., T2M, SLP, TCWV).",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Climate science / meteorology — medium-range extreme event forecasting (heatwave detection)",
            "problem_description": "Binary classification forecasting whether an extreme event (heatwave) will occur the following day using multivariate spatio-temporal atmospheric variables.",
            "data_availability": "Described as abundant in the domain (large historical reanalysis and observational datasets are implied), though the paper provides no quantitative dataset description for the exemplar experiment.",
            "data_structure": "High-dimensional spatio-temporal data (gridded atmospheric fields over time), multivariate continuous variables (e.g., 2-m temperature T2M, sea level pressure SLP, total column water vapour TCWV, total precipitation TP), with temporal sequences and spatial structure.",
            "problem_complexity": "High — large state space, non-linear dynamics, multiscale temporal and spatial dependencies, high dimensionality and chaotic features of atmospheric systems.",
            "domain_maturity": "High — meteorology is a well-established field with mature physics-based models and rich observational datasets; both classical forecasting and ML approaches are active research areas.",
            "mechanistic_understanding_requirements": "High — for scientific insight and operational trust (e.g., hazard forecasting), interpretability and causal understanding are important to validate model predictions and understand precursors.",
            "ai_methodology_name": "Transformer neural networks (spatio-temporal adaptation)",
            "ai_methodology_description": "Supervised Transformer trained on multivariate spatio-temporal weather fields to predict next-day extreme event occurrence; post-hoc interpretability (SHAP) produced relevance maps showing reliance on recent T2M increases as primary indicator, with SLP and TCWV as supporting features and minimal influence from TP; the model prioritized recent days (Days -2 to -1).",
            "ai_methodology_category": "Supervised deep learning (sequence / spatio-temporal attention models)",
            "applicability": "Appropriate given abundant data and the need to capture temporal/spatial dependencies; demonstrated applicability in the thought experiment, but authors stress interpretability validation and causal checks before translating findings into scientific claims.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively effective: relevance maps align with known heatwave precursors (recent warming and high-pressure anomalies), indicating the model learned plausible predictors; authors note potential for AI to outperform traditional methods in some cases but caution about spurious correlations.",
            "impact_potential": "Significant — could improve forecasting skill, provide interpretable precursors for domain experts, and generate interpretability-guided hypotheses to refine physical understanding and models.",
            "comparison_to_alternatives": "Paper cites broader evidence that ML/AI has outperformed traditional methods in some weather forecasting cases, but in this specific experiment comparisons are qualitative rather than numeric.",
            "success_factors": "Abundant historical data, model architecture capable of spatio-temporal pattern learning (Transformer), and interpretability method (SHAP) producing domain-plausible importance maps; domain expert review to assess ARU and causality.",
            "key_insight": "In data-rich, physically-grounded domains like weather forecasting, attention-based models with post-hoc interpretability can identify known precursors and offer machine views that both validate and potentially extend domain understanding, but rigorous ARU and causal analysis are essential.",
            "uuid": "e2335.1",
            "source_info": {
                "paper_title": "Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SHAP",
            "name_full": "SHapley Additive exPlanations (SHAP)",
            "brief_description": "A model-agnostic post-hoc interpretability method that assigns feature importance values (saliency/relevance) for individual predictions based on Shapley value concepts, used here to produce machine-view relevance maps for time-series and spatio-temporal models.",
            "citation_title": "A unified approach to interpreting model predictions",
            "mention_or_use": "use",
            "scientific_problem_domain": "General interpretability for scientific ML applications (applied to medicine and climate examples in this paper)",
            "problem_description": "Recover the data/features the model deemed important for specific predictions to create a machine view that domain experts can inspect.",
            "data_availability": null,
            "data_structure": null,
            "problem_complexity": "Method must operate over high-dimensional inputs and potentially structured data (time-series, spatio-temporal fields); computational cost scales with feature dimension and sampling strategy.",
            "domain_maturity": "Mature as an interpretability technique in ML literature; widely used across domains though caveats exist.",
            "mechanistic_understanding_requirements": "Medium-to-high — SHAP provides attribution maps but does not by itself deliver causality; domain experts require mechanistic interpretation and validation.",
            "ai_methodology_name": "SHAP (post-hoc feature attribution)",
            "ai_methodology_description": "Computes additive feature attributions approximating Shapley values to explain individual model predictions; produces saliency/relevance maps that highlight which input features or time/space regions contributed positively/negatively to a prediction.",
            "ai_methodology_category": "Post-hoc interpretability / explainable AI",
            "applicability": "Applicable and convenient for generating machine views across model types; however, the paper highlights issues: explanations may be unreliable or non-unique and require accuracy and reproducibility validation (ARU).",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Useful in examples (provided clinically- and physically-plausible relevance maps) but authors caution that post-hoc explanations can be inaccurate and need validation to avoid misleading scientists.",
            "impact_potential": "High when combined with ARU validation and domain expert analysis — can bridge machine outputs and human hypotheses, enabling interpretability-guided explanations (IGEs).",
            "comparison_to_alternatives": "Contrasted with other post-hoc methods (DeepLIFT, Grad-CAM) and ante-hoc/self-explaining approaches; trade-off noted between model-agnostic convenience and potential inaccuracy vs. intrinsic interpretability faithfulness.",
            "success_factors": "Careful validation of explanation accuracy, reproducibility checks, and ensuring explanations are understandable to domain experts (ARU); pairing with domain review reduces risk of scientific hallucinations.",
            "key_insight": "SHAP can expose the model's feature-level 'machine view' across domains, but its interpretability value for science hinges on ARU validation and careful expert-driven explanation to avoid spurious inferences.",
            "uuid": "e2335.2",
            "source_info": {
                "paper_title": "Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Post-hoc interpretability (general)",
            "name_full": "Post-hoc interpretability methods (e.g., DeepLIFT, Grad-CAM, SHAP)",
            "brief_description": "Techniques applied after model training to produce saliency maps or feature attributions showing which inputs influenced predictions; model-agnostic and widely used to form a machine view for expert inspection.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "General — applies across scientific ML problems (medicine, climate, materials, etc.)",
            "problem_description": "Provide explanations for black-box model outputs by attributing importance to input features, times, or spatial regions to inform scientists about model reasoning.",
            "data_availability": null,
            "data_structure": null,
            "problem_complexity": "Operates over potentially high-dimensional and structured inputs; issues include non-uniqueness and sensitivity to model and input perturbations.",
            "domain_maturity": "Established set of methods with active research on limitations and validation techniques.",
            "mechanistic_understanding_requirements": "Medium — provides input importance but does not guarantee mechanistic or causal explanations; requires expert interpretation and further causal analysis.",
            "ai_methodology_name": "Post-hoc saliency/feature-attribution methods",
            "ai_methodology_description": "Algorithms (gradient-based, perturbation, Shapley-value approximations) that compute per-input or per-feature relevance scores for already-trained models; outputs are typically heatmaps or importance lists used as the machine view.",
            "ai_methodology_category": "Post-hoc explainable AI",
            "applicability": "Widely applicable to many model classes and tasks; caution advised because explanations may be unreliable or non-unique and should be validated (sanity checks, uncertainty modeling).",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Effective at producing interpretable artifacts for expert review in many cases, but susceptible to generating misleading or non-reproducible explanations without validation; the paper emphasizes accuracy, reproducibility, understandability (ARU) as necessary.",
            "impact_potential": "Substantial if explanations are validated and integrated into scientific workflows — can spark hypotheses and guide further experiments.",
            "comparison_to_alternatives": "Compared to ante-hoc/intrinsically interpretable models: post-hoc is more flexible and model-agnostic but often less faithful; ante-hoc models may be more faithful but can underperform on complex tasks.",
            "success_factors": "Robustness of explanation method, reproducibility across runs/samples, and interpretability to domain experts (ARU); combining with causal methods increases scientific value.",
            "key_insight": "Post-hoc interpretability enables generation of machine views that can drive scientific inquiry, but their utility depends critically on accuracy, reproducibility, and understandability validated against domain knowledge.",
            "uuid": "e2335.3",
            "source_info": {
                "paper_title": "Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Ante-hoc / Intrinsic interpretability",
            "name_full": "Ante-hoc / intrinsic interpretable models (e.g., decision trees, generalized additive models, prototype-based methods, neural additive models)",
            "brief_description": "Model classes designed to be interpretable by construction, offering more faithful explanations of decision logic at the potential cost of reduced performance on highly complex tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "General — proposed as safer alternatives for high-stakes scientific applications (medicine, climate, etc.)",
            "problem_description": "Provide faithful, built-in explanations of model decisions to facilitate mechanistic interpretation and scientific insight.",
            "data_availability": null,
            "data_structure": null,
            "problem_complexity": "Tend to be better suited to low-to-moderate complexity problems; may struggle with very high-dimensional or highly non-linear tasks where black-box models excel.",
            "domain_maturity": "Well-established modeling families with growing research into scalable, expressive intrinsically interpretable architectures.",
            "mechanistic_understanding_requirements": "High — these methods are recommended when mechanistic/causal understanding is critical, such as high-stakes scientific or medical decision-making.",
            "ai_methodology_name": "Ante-hoc / intrinsically interpretable models",
            "ai_methodology_description": "Models whose structure enables direct human-understandable reasoning (rules in decision trees, additive components in GAMs, prototype-based explanations); they map inputs to outputs with transparent internal representations that can be inspected without post-hoc methods.",
            "ai_methodology_category": "Interpretable/symbolic/hybrid ML",
            "applicability": "Appropriate when interpretability and fidelity are prioritized over raw predictive performance; may be insufficient for the most complex scientific prediction tasks without careful design or hybridization.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "More faithful explanation of decision logic compared to many post-hoc methods, but can underperform on complex tasks such as large-scale weather or protein structure prediction; trade-off between interpretability and performance emphasized.",
            "impact_potential": "High in safety-critical scientific contexts where model transparency is required; can prevent misleading scientific inferences and enable direct hypothesis extraction.",
            "comparison_to_alternatives": "Contrasted with post-hoc methods: ante-hoc offers higher fidelity of explanation but may be outperformed by black-box deep models on complex, data-rich tasks.",
            "success_factors": "Problem suitability (complexity vs interpretability needs), careful feature engineering, and alignment of model inductive biases with domain knowledge.",
            "key_insight": "Intrinsic interpretability improves faithfulness of machine views for science, but the interpretability-vs-performance trade-off means hybrid approaches or improved intrinsic methods are needed for very complex domains.",
            "uuid": "e2335.4",
            "source_info": {
                "paper_title": "Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Symbolic regression",
            "name_full": "Symbolic regression for equation discovery",
            "brief_description": "A technique that searches symbolic mathematical expressions to fit data, proposed as a pathway to convert interpretability-guided explanations (IGEs) into actionable mathematical models for scientific knowledge discovery.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "General scientific model discovery (physics, biology, engineering)",
            "problem_description": "Derive compact mathematical expressions or laws from data or from hypotheses derived via model interpretability to produce human-understandable models.",
            "data_availability": null,
            "data_structure": null,
            "problem_complexity": "Search over a combinatorial space of symbolic expressions; complexity grows with allowed operator set, expression depth, and number of variables.",
            "domain_maturity": "Active research area with growing applications in scientific discovery; not a mature turnkey solution for all domains.",
            "mechanistic_understanding_requirements": "High — objective is to produce mechanistic, interpretable mathematical relationships that can be validated experimentally or theoretically.",
            "ai_methodology_name": "Symbolic regression (e.g., genetic programming, sparse regression approaches)",
            "ai_methodology_description": "Search-based or optimization-based methods that propose symbolic formulas to explain relationships observed in data or highlighted by model interpretability; can incorporate constraints or priors to favor physically-plausible expressions.",
            "ai_methodology_category": "Symbolic / equation-discovery / scientific ML",
            "applicability": "Potentially applicable as a bridge from IGEs to explicit mathematical models, but the paper notes a significant gap remains and more development is required to make this pathway reliable in practice.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Promising for converting interpretability-derived hypotheses into mathematical laws, but challenges remain in search tractability, robustness to noise, and ensuring physical plausibility.",
            "impact_potential": "High — successful symbolic regression can translate machine-discovered patterns into human-readable scientific laws, enabling theory formation and deeper mechanistic insight.",
            "comparison_to_alternatives": "Positioned as complementary to interpretability outputs and as an alternative to purely black-box models for producing explicit scientific knowledge.",
            "success_factors": "Good candidate features/hypotheses from interpretability, constraints informed by domain knowledge, and robust optimization/search strategies.",
            "key_insight": "Symbolic regression is a promising route to turn interpretability-guided explanations into explicit mathematical models, but practical gaps and robustness challenges must be overcome for reliable scientific discovery.",
            "uuid": "e2335.5",
            "source_info": {
                "paper_title": "Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Physics-informed NN (PINNs)",
            "name_full": "Physics-Informed Neural Networks (PINNs)",
            "brief_description": "Neural network frameworks that embed differential-equation-based physical laws into the loss function to solve forward and inverse PDE problems, cited as an example of domain-informed ML.",
            "citation_title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Computational physics / computational engineering — solving PDEs and inverse problems",
            "problem_description": "Solve forward and inverse problems governed by nonlinear partial differential equations while enforcing known physical constraints during training.",
            "data_availability": null,
            "data_structure": null,
            "problem_complexity": "High computational complexity from solving PDEs in high-dimensional spatio-temporal domains; requires capturing nonlinearity and boundary/initial conditions.",
            "domain_maturity": "Emerging-to-mature within computational science; active research area with many successful demonstrations but practical limitations for very large-scale problems.",
            "mechanistic_understanding_requirements": "High — explicit incorporation of mechanistic (physical) constraints is central to PINN methodology.",
            "ai_methodology_name": "Physics-informed neural networks (PINNs)",
            "ai_methodology_description": "Neural networks trained with loss terms that penalize deviation from governing PDEs and boundary/initial conditions, enabling data-efficient learning and embedding of mechanistic constraints into models.",
            "ai_methodology_category": "Physics-informed / hybrid ML",
            "applicability": "Well-suited where governing equations are known and can be encoded; particularly useful in data-sparse regimes or when combining observational data with theory.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Powerful for integrating physics and data; the paper cites PINNs as an example of scientific ML that embeds domain-specific knowledge and produces domain-constrained results.",
            "impact_potential": "Significant for scientific domains with known governing equations — can improve data efficiency, enforce physically-plausible solutions, and facilitate interpretable models.",
            "comparison_to_alternatives": "Contrasted with purely data-driven black-box models; PINNs offer stronger physical consistency but may be harder to scale for complex, large-scale systems.",
            "success_factors": "Availability of trustworthy governing equations, careful loss weighting, and numerical stability considerations.",
            "key_insight": "Embedding mechanistic knowledge into ML (PINNs) yields models that are more physically consistent and interpretable for scientific problems, especially when data is limited or physics is well-understood.",
            "uuid": "e2335.6",
            "source_info": {
                "paper_title": "Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "AlphaFold",
            "name_full": "AlphaFold (deep learning for protein structure prediction)",
            "brief_description": "A deep learning system (AlphaFold) that predicts 3D protein structures from amino-acid sequences with high accuracy; cited as a prominent example where AI learned scientific-relevant 'principles'.",
            "citation_title": "Highly accurate protein structure prediction with AlphaFold",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Structural biology — protein folding / structure prediction",
            "problem_description": "Predict the three-dimensional folded structure of proteins from primary amino-acid sequence information.",
            "data_availability": "Large public structural databases (e.g., PDB) and evolutionary sequence databases exist and were used to train AlphaFold; data availability is abundant relative to many scientific tasks.",
            "data_structure": "Sequence data (protein amino-acid sequences), multiple sequence alignments, and 3D coordinate data (structures); multimodal between 1D sequence and 3D geometry.",
            "problem_complexity": "Very high — high-dimensional conformational space, complex physics, many-body interactions; challenge to capture folding energetics and long-range contacts.",
            "domain_maturity": "Mature experimental field with decades of structural biology and established databases; theoretical understanding incomplete in some aspects but rich empirical resources exist.",
            "mechanistic_understanding_requirements": "High for scientific insight; while black-box predictions can be useful, mechanistic/structural explanations remain important for biological interpretation and downstream applications.",
            "ai_methodology_name": "Deep learning (AlphaFold's attention-based architectures / specialized model)",
            "ai_methodology_description": "Complex deep network architecture that leverages sequence information, evolutionary couplings, attention mechanisms and structural modules to predict inter-residue distances and ultimately 3D coordinates; trained supervised on experimental structures and multiple-sequence-alignment features.",
            "ai_methodology_category": "Supervised deep learning / domain-informed foundation model",
            "applicability": "Highly applicable — AlphaFold demonstrated major advances in predicting protein structure and has broad applicability in biology and chemistry, though interpretability and mechanistic explanation are active research areas.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported as highly accurate in original literature; in the context of this paper AlphaFold serves as an example where ML outperformed many traditional approaches and motivated interest in extracting the learned 'principles'.",
            "impact_potential": "Very high — enables rapid structural insight, accelerates research in biology, medicine, and bioengineering, and motivates efforts to extract interpretable rules from high-performing models.",
            "comparison_to_alternatives": "Compared generally to traditional physics-based and homology-based structure prediction methods; ML achieved superior predictive performance in many cases as cited by the authors.",
            "success_factors": "Large labeled structural databases, clever architecture design capturing evolutionary and geometric constraints, and extensive engineering for training and inference.",
            "key_insight": "High-performing deep models in mature, data-rich scientific domains (like AlphaFold in structural biology) can discover patterns that outperform traditional models, motivating the need for interpretability to translate model success into scientific understanding.",
            "uuid": "e2335.7",
            "source_info": {
                "paper_title": "Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning skillful medium-range global weather forecasting",
            "rating": 2,
            "sanitized_title": "learning_skillful_mediumrange_global_weather_forecasting"
        },
        {
            "paper_title": "Highly accurate protein structure prediction with AlphaFold",
            "rating": 2,
            "sanitized_title": "highly_accurate_protein_structure_prediction_with_alphafold"
        },
        {
            "paper_title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "rating": 2,
            "sanitized_title": "physicsinformed_neural_networks_a_deep_learning_framework_for_solving_forward_and_inverse_problems_involving_nonlinear_partial_differential_equations"
        },
        {
            "paper_title": "Evaluation of posthoc interpretability methods in time-series classification",
            "rating": 2,
            "sanitized_title": "evaluation_of_posthoc_interpretability_methods_in_timeseries_classification"
        },
        {
            "paper_title": "Reliable post hoc explanations: Modeling uncertainty in explainability",
            "rating": 2,
            "sanitized_title": "reliable_post_hoc_explanations_modeling_uncertainty_in_explainability"
        },
        {
            "paper_title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
            "rating": 2,
            "sanitized_title": "stop_explaining_black_box_machine_learning_models_for_high_stakes_decisions_and_use_interpretable_models_instead"
        },
        {
            "paper_title": "Large language models for automated open-domain scientific hypotheses discovery",
            "rating": 1,
            "sanitized_title": "large_language_models_for_automated_opendomain_scientific_hypotheses_discovery"
        },
        {
            "paper_title": "A unified approach to interpreting model predictions",
            "rating": 2,
            "sanitized_title": "a_unified_approach_to_interpreting_model_predictions"
        }
    ],
    "cost": 0.018345,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence
27 Feb 2025</p>
<p>Gianmarco Mengaldo 
Department of Mechanical Engineering
National University of Singapore
9 Engineering Drive 1117575Singapore</p>
<p>Department of Mathematics (courtesy)
National University of Singapore
10 Lower Kent Ridge Road119076Singapore</p>
<p>Department of Aeronautics (honorary research fellow)
Imperial College London</p>
<p>South Kensington Campus
SW7 2AZLondon</p>
<p>Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence
27 Feb 20254A61C85D8384CF1C1164DB126118A3FFarXiv:2406.10557v5[cs.AI]Explainable artificial intelligenceinterpretabilityscientific methodknowledge discovery
The scientific method is the cornerstone of human progress across all branches of the natural and applied sciences, from understanding the human body to explaining how the universe works.The scientific method is based on identifying systematic rules or principles that describe the phenomenon of interest in a reproducible way that can be validated through experimental evidence.In the era of generative artificial intelligence, there are discussions on how AI systems may discover new knowledge.We argue that human complex reasoning for scientific discovery remains of vital importance, at least before the advent of artificial general intelligence.Yet, AI can be leveraged for scientific discovery via explainable AI.More specifically, knowing the 'principles' the AI systems used to make decisions can be a point of contact with domain experts and scientists, that can lead to divergent or convergent views on a given scientific problem.Divergent views may spark further scientific investigations leading to interpretability-guided explanations (IGEs), and possibly to new scientific knowledge.We define this field as Explainable AI for Science, where domain experts -potentially assisted by generative AI -formulate scientific hypotheses and explanations based on the interpretability of a predictive AI system.</p>
<p>Human progress is inherently related to our ability to observe the world around us, and make sense of it.This frequently means identifying a set of rules or principles that describe systematically the phenomenon we are interested in.The word 'systematic' is crucial here: it means that the rules identified generalize to observations of the phenomenon that were unseen when deriving those rules.Indeed, this is the cornerstone of the modern scientific method [1], that has allowed us to discover new knowledge in various fields, from the natural sciences, including physics and biology, to the applied sciences, including medicine and engineering.</p>
<p>The scientific method is inherently connected to humans, as we have been the key players in discovering new principles (or theories) that explain the real world.However, with the advent of and recent advances in artificial intelligence (AI), machines are also learning 'principles' (intended in a colloquial meaning) on scientific problems that underlie natural phenomena -see e.g., [2][3][4][5][6][7][8][9].Today, for a machine to learn 'principles' means to identify patterns in data and learn relationships that may generalize to the problem the machine is set to solve.The identification of these principles (i.e., patterns and relationships) is usually achieved via defining a learning task for an AI model, training the model on that task, and verifying that the trained model generalizes to unseen observations.Once this step is achieved, we are commonly interested in the results that the AI model gives us on the provided learning task, rather than on the 'principles' it used to reach those results.This is frequently satisfactory, and may be indeed our final goal.Yet, driven by curiosity, regulatory requirements, or else, we may want to know what 'principles' the machine used to obtain a certain result.To this end, the questions 'what' and 'why' are central to our discussion: (i) 'what' did the machine use to reach a certain decision, and (ii) 'why' did the machine use the 'what' ?The 'what' can be the input-output relationships identified by the machine, or just the data deemed important by the machine to reach a certain conclusion (when those explicit relationships are not available).In both cases, data is central.The 'why' is left to human complex reasoning to figure out, trying to interpret or explain the 'what'.We argue that, for answering the 'why' question, and uncover the principles learned by the machine, data is the key, and it represents the point of contact between machines and humans, at least for the natural and applied sciences.We name the data the machine deemed important to reach a certain result, the machine view, as depicted in Fig. 1.The scrutiny of the machine view by human scientists and their complex reasoning can be the key to discovering new scientific knowledge.We shall make an important observation here: 'how' the machine used the information/data it is given is also an crucially important.However, that information is frequently not available to us, and deriving it may be quite challenging.Therefore, for this manuscript, we focus on the 'what' and 'why' questions, noting that the 'how' is equally important, when available.</p>
<p>To support our argument, we will dive into the scientific method that constitute the foundation of scientific discovery, and show how this can be revisited in the era of AI through explainability.We will further present a thought experiment (yet practically reproducible) that shows how this could be achieved, while highlighting some potential limitations.</p>
<p>We should also add that we share the concerns regarding the risks of adopting AI for scientific research (e.g., [10,11]) recently brought forward by Messeri and Crockett [12], where they state that: "adopting AI in scientific research can bind to our cognitive limitations and impede scientific understanding despite promising to improve it".We believe that explainable AI (XAI) is a way of bridging the gap between human knowledge and machine's usage of the data, and can guide human experts to enrich human knowledge rather than being detrimental to it.</p>
<p>The scientific method in the era of AI AI is commonly seen as a black-box tool, whereby humans struggle to understand why the machine took a certain decision or provided a certain forecast.This prevents us to understand possible 'principles' AI may have learnt, that could be useful to humans to enrich their knowledge or assess whether to trust the AI results.A way of addressing this issue, at least partly, is via XAI, where the latter is composed of two elements: (i) interpretability, and (ii) explanation of interpretability results in human-understandable terms; the two together provide a human-feasible pathway to 'explaining' AI, and yield interpretabilityguided explanations (IEGs).The first element, interpretability, aims to answer the question 'what' (and possibly 'how ', when available [13]) data the machine deemed important to reach a certain result.The second element, explanation of interpretability results is left to domain experts and scientists that can generate hypotheses, if they have a divergent understanding of the data space compared to the machine.Answering the 'what' and 'why' questions can unveil those sought-for 'principles' the predictive AI model might have learnt.</p>
<p>The rationale for this argument is as follows.The scientific method, that underlies both the natural and applied sciences, is based on inductive reasoning, i.e., the formulation of hypothetical rules, and empirical evidence, i.e., observations and experiments, that validate the hypothetical rules.The hypothetical rules may be formulated before or after empirical evidence, and they can in fact be amended and improved after new empirical evidence comes to light.This approach to the scientific method is closely related to the prevailing philosophical view of science provided by Popper in his work "The Logic of Scientific Discovery" [14], although alternative views exist -see e.g., Feyereband [15], among others.To understand this approach in practice, consider Newton's law of universal gravitation, which defines the force between two masses as F = Gm 1 m 2 /r 2 .Newton derived this law inductively from empirical observations [16], where the gravitational constant G was only determined later through Cavendish's experiment.</p>
<p>Similarly, Einstein's theories of special [17] and general relativity [18], which extended Newtonian mechanics, were developed using both theoretical insights The scientific method in the era of AI.Comparative view of the approach used by humans (left, green) and AI (right, grey) to identify scientific principles that can generalize to unseen observations/data.Comparing the human view (existing knowledge) with the machine view (AI-used data) can drive new investigations from divergent perspectives or validate results for critical applications like medicine.The key to this process is interpretability (red box, bottom right), that can give us the machine view.Through this, we can anchor human explanations to the machine view, leading to interpretabilityguided explanations (IGEs), that may lead to knowledge discovery.and empirical evidence.These examples highlight how scientific discovery often combines hypothesis formation, prior knowledge, and empirical validation.This pattern appears across disciplines.In biology, Mendel deduced inheritance laws through pea plant experiments [19].In medicine, Fleming discovered penicillin by observing bacterial inhibition due to fungal contamination [20].In engineering, Volta's battery was inspired by Galvani's findings on electrical stimulation in frogs.</p>
<p>These examples illustrate how scientific knowledge emerges from real-world observations and prior knowledge.Hypothetical rules are formulated, then validated or refuted through further evidence [14] -a process that underpins the scientific method and drives discovery in the form of principles or theories (Fig. 1, left green box).</p>
<p>Observations, for what AI is concerned, are data and, in some cases, labels or known outputs, that the machine can use to learn a certain task, and become better at it by experience (i.e., training) [21].By training on some data (and labels), the machine learns some rules that are hopefully applicable to unseen data; in other words, the machine may have learnt some 'principles' in the form of patterns in data and relationships that generalize to unseen observations.The 'principles' AI learns may or may not align with empirically validated scientific knowledge (Fig. 1, right red box).Often, we are satisfied with AI results without investigating why they were obtained -i.e., what principles the machine relied on.</p>
<p>Two notable AI applications in science -weather forecasting [2] and protein folding [4] -showcase this.Both fields have vast data and theoretical understanding.Despite this, AI has in some cases outperformed traditional methods based on human knowledge, suggesting it has identified underlying patterns that improve performance.Hence, we might want to understand what those 'principles' are, and possibly close existing human-knowledge gaps.However, with the traditional workflow shown in Fig. 1 (right red block only) we are unable to achieve this step.</p>
<p>We argue that the key to bridge the gap between human and machine understanding is through interpretability methods applied to the AI system, that in turn can provide what we name here the machine view on a certain scientific problem (yellow box on the bottom right of Fig. 1).The machine view corresponds to 'what' data the machine deemed important to obtain a certain result, and it gives us a glimpse of those principles that the machine has learnt1 .We can then compare the machine view with the existing body of knowledge available (what we name human view ), and try to respond the 'why' question, that is: why the machine deemed those data important.This comparison may lead to convergent views or divergent views on the problem being solved.Divergent views can spark further scientific investigation.More specifically, scientists and domain experts may look at the machine view, and take a divergent (yet machine viable) understanding on what is commonly accepted in a certain field, thus providing possible pathways to fill knowledge gaps, and discover new knowledge.</p>
<p>The workflow presented in Fig. 1 can open the path towards scientific discovery via XAI, also referred to as XAI for Science.The latter naming convention is in analogy with AI for science, and scientific machine learning, that usually embeds domain-specific knowledge into AI models to produce plausible domain-constrained results [22].XAI for Science (also referred to as scientific XAI) may use both constrained and unconstrained AI models, where the data the machine used to take a certain decision is available to the users.</p>
<p>However, in order for scientific XAI to have a chance of being successful, it needs to satisfy certain key pillars or requirements, that are necessary but not sufficient to achieve the task.</p>
<p>Explainable AI for Science: what (and how ) + why</p>
<p>The starting point of XAI for Science is related to what we named the machine view, that answers the 'what' question: 'what' is the data the machine deemed important to obtain a certain result?However, before diving into explainability and the machine view, we shall clarify a crucial assumption.</p>
<p>Assumption.In this work, we assume that the predictive AI model has learnt something useful (i.e., some rules or 'principles' in the form of patterns and relationships) about the real-world phenomenon of interest.We further assume that those principles are encoded within the machine view.This translates into having a causal AI model that can accurately accomplish a scientific task (e.g., predict the future weather evolution or a certain protein structure) similarly or better than state-of-the-art human-knowledge-driven models.</p>
<p>The assumption just made implies that there is a chance of learning new rules in the form of patterns and relationships, using an AI model.In the assumption, we refer to a causal model.We note that the concept of causality, in machine learning, and more generally in science, remains widely debated [23], and may take different connotations and viewpoints [14,18,24,25].Therefore, causality should be carefully assessed, by e.g., AI experts, as well as by scientists and domain experts, as part of their answer to the 'why' question.</p>
<p>Having framed our problem with the above assumption, we can now return to the machine view, and on how this can be retrieved.The relevant area of AI research for this task is called outcome interpretability, which explains how AI models use input features to generate outputs [26].The two main approaches within this context are: (i) post-hoc interpretability and (ii) antehoc interpretability.</p>
<p>Post-hoc interpretability (e.g., DeepLIFT [27], SHAP [28], GradCAM [29]) generate saliency maps (i.e., machine views) to highlight important input features [30].They are model-agnostic and do not alter AI models but may produce unreliable and non-unique explanations [13,31,32], requiring careful validation [33].</p>
<p>Ante-hoc interpretability (e.g., decision trees [34], generalized additive models [35], prototype-based methods [36]) may provide a more faithful machine view but may underperform in complex tasks [2,4].This trade-off remains a topic of debate [13].For further reading on these methods, see [37].</p>
<p>The issues just outlined pose challenges for our overall objective.Relying on inaccurate explanations can lead to scientific hallucinations [12], while nonreproducible or unclear insights may provide little value to scientists.</p>
<p>To prevent such pitfalls, we define foundational pillars for scientific XAI, applicable to both post-hoc interpretability and self-explainable models.While various authors have proposed XAI requirements for different applications [37][38][39][40], we focus on three key pillars that the machine view should have for XAI for Science to succeed, accuracy, reproducibility, and understandability, or in brief ARU:</p>
<ol>
<li>Accuracy.The machine view should be accurate.In both post-hoc and ante-hoc interpretability, practitioners tend to assume that the machine view is accurate; assumption that is frequently wrong.</li>
</ol>
<p>2.</p>
<p>Reproducibility.The machine view should be reproducible, at least in a statistical sense, which means that if a certain outcome explanation is given for a certain learning task, this can be systematically reproduced.</p>
<p>3.</p>
<p>Understandability.The machine view must be understandable to scientists and domain experts, meaning it should present viable features that connect to existing knowledge.This aligns with the common XAI desideratum of transparency, but here, we emphasize its relevance to scientific understanding.</p>
<p>The first requirement is not new, and several frameworks have been deployed to evaluate interpretability results -see e.g., [33] among others.The other two requirements are instead more tailored towards XAI for science, and specify how to interface the machine view to domain experts and scientists.We note that the important assumption on the model being causal made at the beginning of this section must hold [41,42].By having these pillars in place, scientists may grasp divergent views, increase their understanding of a given problem, and possibly fill knowledge gaps.Let us now take two practical examples to outline the workflow, as depicted in Figure 2: (i) ECG time series signals for identifying a possible underling pathology of a given patient, and (ii) spatio-temporal weather data to forecast if an extreme event is or not occurring tomorrow.For both datasets, we use Transformer neural networks, that we deem casual for the sake of showcasing the possible workflow, and they we setup (predictive) classification tasks: classify pathology A or B for a given patient, and classify if an extreme event is happening tomorrow or not.The first step we perform is to obtain classification scores, and evaluate whether these score are competitive, and the predictive AI outperforms human knowledge and predictive abilities.We then apply post-hoc interpretability, and assess that the relevance maps produced satisfy the ARU (accuracy, reproducibility, and understandability) requirements we just set.We then pass these interpretability results to human domain experts (cardiologists and meteorologists, respectively), and/or generative AI 'scientists' (large language models), and derive interpretability-guided explanations (also referred to as IGEs).These interpretability-guided explanations or IGEs can then be compared to existing human knowledge on the subject and they may generate nothing new, or could yield knowledge discovery.</p>
<p>From the two examples just introduced, we shall note some limitations and potential issues of the proposed methodology, that require further AI developments -these are discussed next.</p>
<p>MACHINE VIEW (ARU PILLARS)</p>
<p>ECG time series</p>
<p>Spatio-temporal weather data</p>
<p>The neural network predicts heatwaves by relying on rising temperatures (T2M) as the primary indicator, with high-pressure anomalies (SLP) and water vapor (TCWV) playing supporting roles, while low precipitation (TP) has minimal impact.It prioritizes recent atmospheric conditions (Days -2 to -1), aligning with known heatwave precursors in Indochina.</p>
<p>The figure is an ECG of a patient diagnosed with RBBB, where the heatmap corresponds to the relevance map obtained with SHAP.The neural network correctly predicts RBBB primarily focusing on the delayed and broadened R -wave in the V1 lead, which is a hallmark feature of this heart conduction abnormality.Limitations and future outlook XAI for Science faces some obvious limitations, related to both the causality assumption, as well as to the ARU requirements we set.In addition, translating the explanations provided by domain experts, and potentially by large language models, into actionable findings is still a nascent field that demands significant developments over the next few years.Let us start from the assumption of causality.The patterns and relationships identified by the machine may be just spurious correlations, without any causality, and may fool scientists towards chasing wrong answers to the 'why' question.Understanding whether causality is appropriate to the problem being investigated, and assessing whether the machine view is causal is a key aspect that scientists should take into account in answering 'why' the machine deemed important certain data.Indeed, several works are now exploring the intersection between XAI and causality [42], two fields that in the computer science community have drifted apart over the years [41], but that should be kept together, especially in the natural and applied sciences.To this end, some promising works are emerging -see e.g., [43][44][45].In addition, Peters et al. book [46] provides an introduction to causality for learning algorithms, while Pearl's book offers an accessible roadmap and perspective on causal inference from both a statistical and a philosophical perspective [23].</p>
<p>In terms of the accuracy requirement, to obtain what we called the machine view there exist several methods, grouped into two different categories, namely post-hoc and ante-hoc interpretability -see e.g., [47] for a comprehensive review and taxonomy of interpretable AI.Post-hoc interpretability suffers from inaccuracy, aspect that may render futile the effort by domain experts to explain why certain data was used, if the data pinpointed is wrong.To this end, accuracy evaluation methods are as important, see e.g., [33].Intrinsic interpretability may address the accuracy issues that plague post-hoc interpretability, yet they may be model-specific, human-constrained, and unable to provide machine views for complex deployed models that may not be easily accessible.Yet, several promising works are being carried out in the context of intrinsic interpretability -see e.g., [48][49][50].Another recent promising area is (intrinsic) compositional interpretability [51], that may help tie together different intrinsic interpretability viewpoints under a unified framework.</p>
<p>In terms of reproducibility, we should be able to retrieve the same machine view repeatedly, for a given task and set of samples.In this context, some issues derive from the possible non-uniqueness of the machine view that interpretability methods may provide (especially post-hoc interpretability methods).This can hinder our understanding of why those machine views were used.To this end, answering the question 'how ' was the machine view (i.e., the data deemed important) used by the machine can provide a more useful insight, as well pointed out by Rudin [13].</p>
<p>In terms of understandability, the machine view should allow scientists and domain expert to tap into their body of knowledge, to allow for comparing machine and human views on a certain phenomenon of interest.This is partly connected to what discussed by Freiesleben et al. [52].More specifically, the model should be able to address a question understandable to scientists, with a machine view that can also be understood by scientists within their framework of domain-specific knowledge.The choice of data and data configuration is therefore critical, and frequently arbitrary, leaving a degree of subjectivity and arbitrariness to the problem.For instance, if we consider the growing field of domain adaptability (i.e., using an AI model for a different domain than the one it was trained on) [53], the machine view may not be understandable to scientists.These issues are less pronounced in the context of fine-tuning a certain model, as the pre-training is usually accomplished within the same domain as the fine-tuning, hence understandable to domain experts working in the specific field.</p>
<p>Finally, how do we make potentially useful interpretability-guided explanations (IGEs) provided by domain experts actionable -in other words, how do we use them in practice for knowledge discovery?A key field that has consistently driven human innovations is our ability to synthesize mathematical models of the real world [1].However, from IGEs to mathematical models there is still a significant gap, with symbolic regression (e.g., [54]) possibly constituting feasible pathway.</p>
<p>Without a mathematical model, we can still grasp useful and actionable insights.For instance, Strogatz, in its inspiring New York Times editorial covering the winning of AlphaZero against Stockfish [55] states: "What is frustrating about machine learning, however, is that the algorithms can't articulate what they're thinking.We don't know why they work, so we don't know if they can be trusted.AlphaZero gives every appearance of having discovered some important principles about chess, but it can't share that understanding with us.".He additionally cites Garry Kasparov (the former world chess champion) that stated: "we would say that its [AlphaZero] style reflects the truth.This superior understanding allowed it to outclass the world's top traditional program despite calculating far fewer positions per second."[56].A few years later from that editorial, researchers showed that Go players are improving their game looking at how AlphaGo, DeepMind's Go machine is playing, and providing them with a divergent view [57].</p>
<p>While science is a rather different application than Go, we believe that leveraging explanation for scientific inquiry that may ultimately lead to knowledge discovery may not be a foolish path.XAI is a possible path forward to keep complex human reasoning in the loop, possibly complemented by AI reasoning in large language models, while bridging current AI power in discovering patterns and relationships in data.XAI may also alleviate some of the risks that we may face when using AI for scientific discovery, that we share with Messeri and Crockett [12].</p>
<p>Figure 1 :
1
Figure1: The scientific method in the era of AI.Comparative view of the approach used by humans (left, green) and AI (right, grey) to identify scientific principles that can generalize to unseen observations/data.Comparing the human view (existing knowledge) with the machine view (AI-used data) can drive new investigations from divergent perspectives or validate results for critical applications like medicine.The key to this process is interpretability (red box, bottom right), that can give us the machine view.Through this, we can anchor human explanations to the machine view, leading to interpretabilityguided explanations (IGEs), that may lead to knowledge discovery.</p>
<p>'Figure 2 :
2
Figure 2: XAI for Science workflow.A predictive AI outperforms human expertise (grey box).Interpretability is then applied to generate the machine view (red boxes) that meet accuracy, reproducibility, and understandability (ARU) criteria.These interpretability results are then analyzed by human domain experts or generative AI models (green box) to derive interpretabilityguided explanations (IGEs), which can either align with existing knowledge or lead to new scientific discoveries (in yellow).We depict two examples, one in the context of medicine (ECG data) and one in the context of climate science (weather data).</p>
<p>A glimpse and not the full picture, because to really retrieve those principles, we should respond to the 'how ' the machine used those data deemed important. To achieve this, we would need to have full access to the inner workings of the AI system, along with all the relationships among variables and parameters that constitute the model; route that is practically not viable today.
AcknowledgmentsWe want to thank my two PhD students, Jiawen Wei and Chenyu Dong for many of the results presented in this work.We thank all members of my group at NUS, MathEXLab, for their dedication to research that made this work possible.We also thank all the colleagues that provided constructive criticism, from several different research fields and perspectives, that helped improve this manuscript significantly.We acknowledge funding from MOE Tier 2 grant 22-5191-A0001-0 'Prediction-to-Mitigation with Digital Twins of the Earth's Weather' and MOE Tier 1 grant 22-4900-A0001-0 'Discipline-Informed Neural Networks for Interpretable Time-Series Discovery'.
Discourses and mathematical demonstrations relating to two new sciences. G Galilei, 1638</p>
<p>Learning skillful medium-range global weather forecasting. R Lam, Science. 38266772023</p>
<p>Machine learning in the search for new fundamental physics. G Karagiorgi, G Kasieczka, S Kravitz, B Nachman, D Shih, Nature Reviews Physics. 462022</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, Nature. 59678732021</p>
<p>Unifying machine learning and quantum chemistry with a deep neural network for molecular wavefunctions. K T Schütt, M Gastegger, A Tkatchenko, K.-R Müller, R J Maurer, Nature communications. 10150242019</p>
<p>A guide to deep learning in healthcare. A Esteva, Nature medicine. 2512019</p>
<p>Obtaining genetics insights from deep learning via explainable artificial intelligence. G Novakovsky, N Dexter, M W Libbrecht, W W Wasserman, S Mostafavi, Nature Reviews Genetics. 2422023</p>
<p>Scaling deep learning for materials discovery. A Merchant, Nature. 62479902023</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Z Yang, arXiv:2309.027262023arXiv preprint</p>
<p>On scientific understanding with artificial intelligence. M Krenn, Nature Reviews Physics. 4122022</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, Nature. 62079722023</p>
<p>Artificial intelligence and illusions of understanding in scientific research. L Messeri, M Crockett, Nature. 6272024</p>
<p>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. C Rudin, Nature machine intelligence. 152019</p>
<p>The logic of scientific discovery. K Popper, 2005Routledge</p>
<p>Against method: Outline of an anarchistic theory of knowledge. P Feyerabend, 2020Verso Books</p>
<p>. I Newton, Philosophiae Naturalis Principia Mathematica. 1687</p>
<p>On the electrodynamics of moving bodies. A Einstein, 1905</p>
<p>The foundation of the general theory of relativity. A Einstein, Annalen Phys. 4971916</p>
<p>G Mendel, G Mendel, Versuche über pflanzenhybriden. Springer1970</p>
<p>On the antibacterial action of cultures of a penicillium, with special reference to their use in the isolation of b. influenzae. A Fleming, British journal of experimental pathology. 1032261929</p>
<p>. T Mitchell, Machine learning. Annual review of computer science. 411990</p>
<p>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. M Raissi, P Perdikaris, G E Karniadakis, Journal of Computational physics. 3782019</p>
<p>The book of why: the new science of cause and effect. J Pearl, D Mackenzie, 2018Basic books</p>
<p>Causality and complementarity. N Bohr, Philosophy of Science. 431937</p>
<p>A treatise of human nature. D Hume, 2000Oxford University Press</p>
<p>Recommendation of the council on artificial intelligence (OECD). K Yeung, 10.1017/ilm.2020.5International Legal Materials. 5912020</p>
<p>Learning important features through propagating activation differences. A Shrikumar, P Greenside, A Kundaje, 2017</p>
<p>A unified approach to interpreting model predictions. S M Lundberg, S.-I Lee, 201730Advances in neural information processing systems</p>
<p>Grad-cam: Visual explanations from deep networks via gradient-based localization. R R Selvaraju, 2017</p>
<p>Explaining deep neural networks and beyond: A review of methods and applications. W Samek, G Montavon, S Lapuschkin, C J Anders, K.-R Müller, Proceedings of the IEEE. 10932021</p>
<p>Sanity checks for saliency maps. J Adebayo, 201831</p>
<p>Reliable post hoc explanations: Modeling uncertainty in explainability. D Slack, A Hilgard, S Singh, H Lakkaraju, 202134</p>
<p>Evaluation of posthoc interpretability methods in time-series classification. H Turbé, M Bjelogrlic, C Lovis, G Mengaldo, Nature Machine Intelligence. 532023</p>
<p>B Letham, C Rudin, T H Madigan, D , Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model. 2015</p>
<p>Neural additive models: Interpretable machine learning with neural nets. R Agarwal, Advances in neural information processing systems. 342021</p>
<p>Pip-net: Patchbased intuitive prototypes for interpretable image classification. M Nauta, J Schlötterer, M Van Keulen, C Seifert, 2023</p>
<p>Interpretable machine learning: Fundamental principles and 10 grand challenges. C Rudin, Statistic Surveys. 162022</p>
<p>Four principles of explainable artificial intelligence. P J Phillips, 2021</p>
<p>Explainable AI (XAI): Core ideas, techniques, and solutions. R Dwivedi, ACM Computing Surveys. 5592023</p>
<p>Seven pillars for the future of artificial intelligence. E Cambria, R Mao, M Chen, Z Wang, S.-B Ho, IEEE Intelligent Systems. 3862023</p>
<p>Causal explanations and XAI. S Beckers, 2022</p>
<p>G Carloni, A Berti, S Colantonio, arXiv:2309.09901The role of causality in explainable artificial intelligence. 2023arXiv preprint</p>
<p>Causality for machine learning. B Schölkopf, 2022</p>
<p>Feature relevance quantification in explainable AI: A causal problem. D Janzing, L Minorics, P Blöbaum, 2020</p>
<p>G Xu, T D Duong, Q Li, S Liu, X Wang, arXiv:2006.16789Causality learning: A new perspective for interpretable machine learning. 2020arXiv preprint</p>
<p>Elements of causal inference: foundations and learning algorithms. J Peters, D Janzing, B Schölkopf, 2017The MIT Press</p>
<p>A global taxonomy of interpretable AI: unifying the terminology for the technical and social sciences. M Graziani, Artificial intelligence review. 5642023</p>
<p>Towards robust interpretability with selfexplaining neural networks. D Alvarez Melis, T Jaakkola, Advances in neural information processing systems. 201831</p>
<p>Protgnn: Towards selfexplaining graph neural networks. Z Zhang, Q Liu, H Wang, C Lu, C Lee, 202236</p>
<p>ProtoS-ViT: Visual foundation models for sparse self-explainable classifications. H Turbé, M Bjelogrlic, G Mengaldo, C Lovis, arXiv:2406.100252024arXiv preprint</p>
<p>S Tull, R Lorenz, S Clark, I Khan, B Coecke, arXiv:2406.17583Towards compositional interpretability for XAI. 2024arXiv preprint</p>
<p>Scientific inference with interpretable machine learning: Analyzing models to learn about real-world phenomena. T Freiesleben, G König, C Molnar, A Tejero-Cantero, arXiv:2206.054872022arXiv preprint</p>
<p>Learning transferable features with deep adaptation networks. M Long, Y Cao, J Wang, M Jordan, 2015</p>
<p>Artificial intelligence in physical sciences: Symbolic regression trends and perspectives. D Angelis, F Sofos, T E Karakasidis, Archives of Computational Methods in Engineering. 3062023</p>
<p>One giant step for a chess-playing machine. S Strogatz, New York Times. 262018</p>
<p>Chess, a drosophila of reasoning. G Kasparov, 2018</p>
<p>Superhuman artificial intelligence can improve human decision-making by increasing novelty. M Shin, J Kim, B Van Opheusden, T L Griffiths, Proceedings of the National Academy of Sciences. 12012e22148401202023</p>            </div>
        </div>

    </div>
</body>
</html>
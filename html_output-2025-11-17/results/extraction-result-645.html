<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-645 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-645</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-645</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-56eb3ef3656454c317ff7a1bd40662d920e6efa3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/56eb3ef3656454c317ff7a1bd40662d920e6efa3" target="_blank">Logical Neural Networks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A novel framework seamlessly providing key properties of both neural nets (learning) and symbolic logic (knowledge and reasoning), which enables the open-world assumption by maintaining bounds on truth values which can have probabilistic semantics, yielding resilience to incomplete knowledge.</p>
                <p><strong>Paper Abstract:</strong> We propose a novel framework seamlessly providing key properties of both neural nets (learning) and symbolic logic (knowledge and reasoning). Every neuron has a meaning as a component of a formula in a weighted real-valued logic, yielding a highly intepretable disentangled representation. Inference is omnidirectional rather than focused on predefined target variables, and corresponds to logical reasoning, including classical first-order logic theorem proving as a special case. The model is end-to-end differentiable, and learning minimizes a novel loss function capturing logical contradiction, yielding resilience to inconsistent knowledge. It also enables the open-world assumption by maintaining bounds on truth values which can have probabilistic semantics, yielding resilience to incomplete knowledge.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e645.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e645.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logical Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic framework that gives a 1-to-1 mapping between neurons and elements of weighted real-valued logic, enabling bidirectional logical inference, end-to-end differentiable learning, and per-neuron interpretable truth bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Logical Neural Networks (LNN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LNN is a recurrent/neural graph whose nodes correspond exactly to logical formula elements (propositions, connectives, quantifiers). Each neuron stores lower and upper truth-value bounds in [0,1], uses activation functions chosen from families of real-valued logics (t-norms, weighted nonlinear logic), and runs alternating upward (evaluate formula from leaves to root) and downward (inverse/bidirectional inference from formulas to operands) passes to tighten bounds until convergence. The model is end-to-end differentiable (weights, biases, and some bounds are learnable), supports first-order logic by grounding, and uses tailored activation functions and a contradiction loss to maintain classical behavior when desired.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Explicit symbolic logic representation: propositional and function-free first-order logic (FOL) formulae represented as syntax trees; logical connectives (AND, OR, NOT, IMPLIES, FORALL, EXISTS) and predicate atoms are explicit neurons; weighted real-valued logics (weighted Łukasiewicz, Gödel, product, and generalized weighted nonlinear logic) provide semantics. Declarative data are provided as initial truth-value bounds on atom neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Differentiable neural computation and gradient-based learning: recurrent neural graph implementing activation functions corresponding to chosen real-valued logic; learnable operand importance weights, biases, and optionally initial bounds; gradient descent (backpropagation) optimization, tailored activation functions for improved gradients; iterative recurrent inference (alternating upward/downward passes) as a procedural algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Tight integration via 1-to-1 correspondence between symbolic elements and neurons: connectives are implemented by constrained activation functions that exactly implement (or approximate) the semantics of chosen real-valued logic; proof aggregation and bidirectional inverse activation functions implement procedural inference rules (modus ponens, modus tollens, etc.); end-to-end differentiable optimization of weights and bounds (including contradiction-penalty loss) allows training to adjust importance of symbolic rules and facts; FOL handled by grounding tuples and tensorized truth-value tables, so symbolic quantifiers and joins are executed via tensor operations within the neural machinery.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Provides interpretable, disentangled neuron meanings (each neuron maps to a logical subformula or predicate); omnidirectional (bidirectional) inference enabling theorem-prover-like deduction in multiple directions; resilience to incomplete knowledge via maintained lower/upper bounds (open-world assumption) and probabilistic semantics for those bounds; resilience to inconsistent knowledge via contradiction loss and learned relaxation (weights/biases) to down-weight conflicting rules; ability to learn weights for rules enabling soft-rule induction/ILP-like behavior; provable monotonic bound tightening and convergence for propositional fragments; can perform classical theorem proving as a special case (when inputs are classical).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Smokers-and-friends (LTN experiment K_exp2), LUBM (OWL / ontology reasoning benchmark), subset of TPTP (first-order theorem proving problems), synthetic and noise/contradiction handling experiments</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>LUBM (OWL reasoning, 14 queries on 1-university data): precision 100%, recall 100% (LNN); Virtuoso recall ~72%, Blazegraph recall ~78% (from paper). TPTP subset: from a filtered set of 25 FOF problems LNN proved 25/25 problems, each within seconds. Smokers-and-friends experiments: LNN removed remaining contradictions (reported 'Contradiction (remaining)' = 0 in training variants), and learned bounds/weights that reduced contradiction while maintaining axiom satisfaction (detailed per-axiom bounds reported in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td>Symbolic reasoners reported for same ontology (LUBM): Stardog: precision 100%, recall 100%; Virtuoso: average recall 72%; Blazegraph: average recall 78% (as reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Many-task/general-purpose inference via omnidirectional/bidirectional inference (can answer different queries without retraining); supports open-world reasoning and probabilistic interpretation of bounds, enabling handling of incomplete and partially observed data; supports compositional and modular extension of networks (subclauses as modules) which aids reuse; FOL support through grounding lets it handle first-order queries but faces the usual grounding explosion and undecidability issues (so theoretical guarantees limited to propositional fragment). No explicit out-of-distribution numeric generalization benchmarks reported beyond the demonstrated theorem-proving and ontology reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability due to 1-to-1 mapping of neurons to logical atoms and connectives; each neuron's bounds and weight values are meaningful (importance of operands); chain-of-inference/proof traces can be inspected (upward/downward passes provide explicit proofs used to tighten bounds); bounds have probabilistic semantics enabling interpretable confidence intervals for inferred truths.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Current implementation does not support function symbols or equality (so some FOL problems excluded); FOL inference may be undecidable and may not converge due to unbounded groundings; grounding management can cause computational and memory blowup for large domains (trade-offs required between goal-driven and data-driven grounding propagation); inverse handling of existential quantifiers is approximate and can be complex; constraints to guarantee classical behavior complicate learning (requires tailored activations or constrained optimization); scalability to very large KBs or very deep/large grounding sets is a practical concern.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Weighted real-valued logics (weighted nonlinear logic), t-norm logics (Gödel, product, Łukasiewicz) mapped to neural activation functions; proofs include convergence theorem for propositional upward-downward algorithm (Theorem 1) and a result linking LNN bounds to probability bounds over classical interpretations (Theorem 2). Contradiction-penalty loss and tailored activation functions provide a principled route to preserve classical semantics while enabling gradient-based learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical Neural Networks', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e645.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e645.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Markov Logic Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A statistical relational approach that attaches weights to first-order logic clauses and interprets the result as a Markov random field; clause weights govern the relative probability of worlds satisfying the clause set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Markov logic networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Markov Logic Networks (MLN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A hybrid statistical-relational system where first-order clauses are given weights and combined into a Markov random field; probabilistic inference typically requires approximate methods (e.g., MCMC). In this paper MLN is used to induce additional axioms (with associated log-probability weights) that are then included in LNN experiments for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Weighted first-order logic clauses (each clause is symbolic and atomic in many MRF-based approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Probabilistic inference algorithms over MRFs (e.g., MCMC) and weight learning procedures; not typically end-to-end differentiable with symbolic structure.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Statistical weighting of symbolic clauses to yield a probabilistic graphical model (MRF); integration of symbolic structure and probabilistic inference but internal clause structure often not used for logical inversion or theorem proving.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines soft (probabilistic) reasoning with logical structure to handle noisy and uncertain data; provides a probability distribution over possible worlds given weighted rules.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Used to induce axioms for the Smokers-and-friends experiment; specific MLN metric quoted: log-probability weight 6.88 for ∃y F(x,y) in that induced-axiom set.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Enables probabilistic tolerance to noisy/uncertain rules; the paper reports MLN assigned high log-probability to axioms that produced contradictions, showing MLN may prefer high-weight rules despite inconsistency.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Clause weights are interpretable as log-probability contributions, but internal clause structure is typically not used for theorem-prover style deduction in MLN implementations; inference traces are probabilistic rather than symbolic proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires expensive probabilistic inference (e.g., MCMC) to obtain marginal probabilities; clauses are atomic in many implementations (internal logical structure not leveraged), making certain logical deductions and theorem-proving tasks onerous; in the paper, MLN-produced axioms led to contradictions that LNN handled by relaxing bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Probabilistic graphical models (Markov random fields) combined with first-order logic clauses; probabilistic semantics over possible worlds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical Neural Networks', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e645.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e645.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PSL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic Soft Logic (Hinge-Loss MRFs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework using hinge-loss Markov random fields to perform soft, continuous relaxation of logical constraints for probabilistic reasoning over relational data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hinge-loss markov random fields and probabilistic soft logic</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Probabilistic Soft Logic (PSL / Hinge-loss MRFs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An MRF-based approach where logical formulas are relaxed to continuous values and combined in hinge-loss potentials to form a convex optimization for probabilistic inference. Mentioned in the paper as part of the class of MRF/neuro-symbolic approaches contrasted with LNN.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Logical formulas relaxed to continuous-valued potentials representing soft constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Convex optimization / inference over hinge-loss potentials (procedural numerical solvers).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Continuous relaxation of logical clauses into a probabilistic graphical model with hinge-loss terms; inference performed via optimization rather than neural gradient-based end-to-end learning.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Scales convex probabilistic inference to relational domains; tolerates noisy/soft constraints with convex optimizers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Handles uncertainty and noisy rules via convex relaxation; the paper contrasts closed-world assumptions of MRF approaches with LNN's open-world bounds, but does not provide PSL-specific generalization metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Weights and hinge-loss terms are interpretable as penalties for violating soft constraints; however, internal clause structure is not used for bidirectional theorem-proving as in LNN.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Makes closed-world assumptions in many setups (contrasted with LNN's open-world bounds); solving global MRFs can be computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Hinge-loss MRFs; convex optimization for probabilistic inference with relaxed logical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical Neural Networks', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e645.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e645.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LTN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic Tensor Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable framework combining logic and continuous embeddings to learn from knowledge and data, using tensor representations of predicates and fuzzy logic semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logic tensor networks: Deep learning and logical reasoning from data and knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Logic Tensor Networks (LTN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A neuro-symbolic approach that represents logical predicates and constants as continuous tensors and applies fuzzy-logic inspired operators to combine logical constraints with learned embeddings; used in the paper as a comparative baseline (LTN experiment K_exp2 Smokers-and-friends).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>First-order logic axioms represented in fuzzy/logical form (used to provide constraints and axioms).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Tensor-based continuous embeddings and neural-style learning to optimize predicate embeddings and satisfy fuzzy constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Embedding-based relaxation of logic: logical formulas converted to continuous differentiable loss terms over tensors, and optimized with gradient descent to learn embeddings that satisfy formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables learning from both data and logical knowledge; provides soft satisfiability degrees for axioms and can be trained end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Smokers-and-friends (K_exp2) experiment used in the paper for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Can integrate prior logical knowledge into learned continuous representations; in the paper LTN assigned high satisfiability to some axioms (e.g., 96% for an axiom) even when evidence suggested otherwise, indicating potential limitations in logical deduction fidelity (e.g., lacked friendship symmetry that LNN inferred).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Gives degrees of satisfiability for axioms (interpretable fuzzy scores), but internal embeddings are less directly interpretable than per-neuron logical units in LNN.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>May fail to produce precise symbolic deductions (e.g., unable to infer friendship symmetry in the smokers experiment) and may produce high satisfiability for axioms despite contradictory evidence; fewer explicit bidirectional logical inference capabilities compared to LNN.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Fuzzy/fuzzy-like logic combined with tensor embeddings and differentiable loss formulations to integrate logic and learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical Neural Networks', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e645.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e645.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NTP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Theorem Provers / Differentiable Proving</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural models that learn to perform logical proving using differentiable modules, typically limited to Horn clauses and embedding-based unification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural Theorem Provers / End-to-end differentiable proving (NTPs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Family of neural models that implement differentiable versions of symbolic proving (e.g., differentiable backward chaining) often limited to Horn clauses and using neural unification/embedding mechanisms; referenced as recent neural theorem provers that have not demonstrated success on classical ATP benchmarks used by symbolic provers.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Clauses/rules (often Horn clauses) used as templates for proving; symbolic structure is represented but often mediated via embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural modules for unification, rule application, and differentiable search; end-to-end gradient learning to score proof paths.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Differentiable relaxation of discrete proof search (embedding-based unification, soft matching), enabling gradient-based learning over proof traces but typically limited to restricted clause forms (Horn).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Learns soft, approximate proving capabilities and rule representations; can learn from noisy data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Mentioned with respect to classical Automated Theorem Proving (TPTP) — paper notes NTPs have not demonstrated success on classical ATP benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Good at learning explanatory rules from noisy data in limited Horn-clause domains, but not demonstrated for general first-order ATP; limited expressivity relative to general FOL.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Can provide learned rule-like representations and soft proof traces, but these are typically in embedding space and less directly interpretable than explicit symbolic proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Limited to Horn-clause families and embedding-based approximate unification; the paper highlights that NTPs have not solved classical theorem-proving benchmarks (TPTP) that LNN can handle for a filtered subset.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Differentiable relaxation of symbolic proof search, embedding-based unification and soft matching of predicates/rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical Neural Networks', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e645.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e645.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TensorLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TensorLog</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable deductive database that maps logic program inference to sparse matrix/tensor operations for end-to-end differentiable learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tensorlog: A differentiable deductive database</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TensorLog</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A system that compiles logic programs into differentiable sparse-matrix operations so that logical deduction can be embedded into differentiable computation and learned via gradient methods; cited as an approach that differentiates logical deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Logic programs / deductive database rules (definite clauses).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Sparse-matrix/tensor algebra and differentiable computation enabling gradient-based parameter learning.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Compilation of symbolic deduction rules into sparse linear algebra operations (matrix multiplications and sums) that are differentiable and can be optimized end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables scaling logic-based inference with differentiable parameter learning and efficient tensorized implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Preserves logical structure while enabling gradient learning; can exploit tensor optimizations for scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Logical rules remain explicit in the compilation, though learned parameters live in matrices and may be less directly interpretable than symbolic constants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Primarily designed for definite-clause logic and may not directly express general FOL with full quantifier handling as LNN does; scaling requires sparse representations and careful grounding management.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Differentiable compilation of logical deduction to sparse linear algebra operations; bridges deductive databases and differentiable learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical Neural Networks', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e645.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e645.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KBANN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Based Artificial Neural Networks (KBANN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Early approach mapping symbolic rules into neural network architectures to combine symbolic knowledge with connectionist learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge-based artificial neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge-Based Artificial Neural Networks (KBANN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A historical neuro-symbolic scheme that transforms propositional rules into neural network structures whose initial weights encode the rules; subsequent training fine-tunes the network on data.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Propositional rules / logic program encoded into initial network structure and weights.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Standard neural networks (feedforward) and gradient-based learning fine-tuning weights.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Direct mapping of symbolic rules into initial network connectivity and weights, followed by gradient descent to adapt to data.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Allows prior knowledge to guide learning initialization and helps to learn from small data, while permitting adaptation to exceptions via training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Can leverage encoded symbolic knowledge to improve data efficiency and generalization in domains where rules are correct; limited by propositional expressivity.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Initial mapping preserves some interpretability (rules correspond to subnetworks), but training can distort these correspondences unless constrained.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Early methods often lost symbolic fidelity after training; limited logical expressivity (propositional rules) compared to approaches like LNN; no bidirectional logical inference mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Connectionist representation of symbolic rules and supervised fine-tuning; historical bridge between symbolic AI and neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical Neural Networks', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Markov logic networks <em>(Rating: 2)</em></li>
                <li>Hinge-loss markov random fields and probabilistic soft logic <em>(Rating: 2)</em></li>
                <li>Logic tensor networks: Deep learning and logical reasoning from data and knowledge <em>(Rating: 2)</em></li>
                <li>End-to-end differentiable proving <em>(Rating: 2)</em></li>
                <li>Tensorlog: A differentiable deductive database <em>(Rating: 2)</em></li>
                <li>Learning explanatory rules from noisy data <em>(Rating: 1)</em></li>
                <li>Knowledge-based artificial neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-645",
    "paper_id": "paper-56eb3ef3656454c317ff7a1bd40662d920e6efa3",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "LNN",
            "name_full": "Logical Neural Networks",
            "brief_description": "A neuro-symbolic framework that gives a 1-to-1 mapping between neurons and elements of weighted real-valued logic, enabling bidirectional logical inference, end-to-end differentiable learning, and per-neuron interpretable truth bounds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Logical Neural Networks (LNN)",
            "system_description": "LNN is a recurrent/neural graph whose nodes correspond exactly to logical formula elements (propositions, connectives, quantifiers). Each neuron stores lower and upper truth-value bounds in [0,1], uses activation functions chosen from families of real-valued logics (t-norms, weighted nonlinear logic), and runs alternating upward (evaluate formula from leaves to root) and downward (inverse/bidirectional inference from formulas to operands) passes to tighten bounds until convergence. The model is end-to-end differentiable (weights, biases, and some bounds are learnable), supports first-order logic by grounding, and uses tailored activation functions and a contradiction loss to maintain classical behavior when desired.",
            "declarative_component": "Explicit symbolic logic representation: propositional and function-free first-order logic (FOL) formulae represented as syntax trees; logical connectives (AND, OR, NOT, IMPLIES, FORALL, EXISTS) and predicate atoms are explicit neurons; weighted real-valued logics (weighted Łukasiewicz, Gödel, product, and generalized weighted nonlinear logic) provide semantics. Declarative data are provided as initial truth-value bounds on atom neurons.",
            "imperative_component": "Differentiable neural computation and gradient-based learning: recurrent neural graph implementing activation functions corresponding to chosen real-valued logic; learnable operand importance weights, biases, and optionally initial bounds; gradient descent (backpropagation) optimization, tailored activation functions for improved gradients; iterative recurrent inference (alternating upward/downward passes) as a procedural algorithm.",
            "integration_method": "Tight integration via 1-to-1 correspondence between symbolic elements and neurons: connectives are implemented by constrained activation functions that exactly implement (or approximate) the semantics of chosen real-valued logic; proof aggregation and bidirectional inverse activation functions implement procedural inference rules (modus ponens, modus tollens, etc.); end-to-end differentiable optimization of weights and bounds (including contradiction-penalty loss) allows training to adjust importance of symbolic rules and facts; FOL handled by grounding tuples and tensorized truth-value tables, so symbolic quantifiers and joins are executed via tensor operations within the neural machinery.",
            "emergent_properties": "Provides interpretable, disentangled neuron meanings (each neuron maps to a logical subformula or predicate); omnidirectional (bidirectional) inference enabling theorem-prover-like deduction in multiple directions; resilience to incomplete knowledge via maintained lower/upper bounds (open-world assumption) and probabilistic semantics for those bounds; resilience to inconsistent knowledge via contradiction loss and learned relaxation (weights/biases) to down-weight conflicting rules; ability to learn weights for rules enabling soft-rule induction/ILP-like behavior; provable monotonic bound tightening and convergence for propositional fragments; can perform classical theorem proving as a special case (when inputs are classical).",
            "task_or_benchmark": "Smokers-and-friends (LTN experiment K_exp2), LUBM (OWL / ontology reasoning benchmark), subset of TPTP (first-order theorem proving problems), synthetic and noise/contradiction handling experiments",
            "hybrid_performance": "LUBM (OWL reasoning, 14 queries on 1-university data): precision 100%, recall 100% (LNN); Virtuoso recall ~72%, Blazegraph recall ~78% (from paper). TPTP subset: from a filtered set of 25 FOF problems LNN proved 25/25 problems, each within seconds. Smokers-and-friends experiments: LNN removed remaining contradictions (reported 'Contradiction (remaining)' = 0 in training variants), and learned bounds/weights that reduced contradiction while maintaining axiom satisfaction (detailed per-axiom bounds reported in Table 2).",
            "declarative_only_performance": "Symbolic reasoners reported for same ontology (LUBM): Stardog: precision 100%, recall 100%; Virtuoso: average recall 72%; Blazegraph: average recall 78% (as reported in the paper).",
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Many-task/general-purpose inference via omnidirectional/bidirectional inference (can answer different queries without retraining); supports open-world reasoning and probabilistic interpretation of bounds, enabling handling of incomplete and partially observed data; supports compositional and modular extension of networks (subclauses as modules) which aids reuse; FOL support through grounding lets it handle first-order queries but faces the usual grounding explosion and undecidability issues (so theoretical guarantees limited to propositional fragment). No explicit out-of-distribution numeric generalization benchmarks reported beyond the demonstrated theorem-proving and ontology reasoning tasks.",
            "interpretability_properties": "High interpretability due to 1-to-1 mapping of neurons to logical atoms and connectives; each neuron's bounds and weight values are meaningful (importance of operands); chain-of-inference/proof traces can be inspected (upward/downward passes provide explicit proofs used to tighten bounds); bounds have probabilistic semantics enabling interpretable confidence intervals for inferred truths.",
            "limitations_or_failures": "Current implementation does not support function symbols or equality (so some FOL problems excluded); FOL inference may be undecidable and may not converge due to unbounded groundings; grounding management can cause computational and memory blowup for large domains (trade-offs required between goal-driven and data-driven grounding propagation); inverse handling of existential quantifiers is approximate and can be complex; constraints to guarantee classical behavior complicate learning (requires tailored activations or constrained optimization); scalability to very large KBs or very deep/large grounding sets is a practical concern.",
            "theoretical_framework": "Weighted real-valued logics (weighted nonlinear logic), t-norm logics (Gödel, product, Łukasiewicz) mapped to neural activation functions; proofs include convergence theorem for propositional upward-downward algorithm (Theorem 1) and a result linking LNN bounds to probability bounds over classical interpretations (Theorem 2). Contradiction-penalty loss and tailored activation functions provide a principled route to preserve classical semantics while enabling gradient-based learning.",
            "uuid": "e645.0",
            "source_info": {
                "paper_title": "Logical Neural Networks",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "MLN",
            "name_full": "Markov Logic Networks",
            "brief_description": "A statistical relational approach that attaches weights to first-order logic clauses and interprets the result as a Markov random field; clause weights govern the relative probability of worlds satisfying the clause set.",
            "citation_title": "Markov logic networks",
            "mention_or_use": "use",
            "system_name": "Markov Logic Networks (MLN)",
            "system_description": "A hybrid statistical-relational system where first-order clauses are given weights and combined into a Markov random field; probabilistic inference typically requires approximate methods (e.g., MCMC). In this paper MLN is used to induce additional axioms (with associated log-probability weights) that are then included in LNN experiments for comparison.",
            "declarative_component": "Weighted first-order logic clauses (each clause is symbolic and atomic in many MRF-based approaches).",
            "imperative_component": "Probabilistic inference algorithms over MRFs (e.g., MCMC) and weight learning procedures; not typically end-to-end differentiable with symbolic structure.",
            "integration_method": "Statistical weighting of symbolic clauses to yield a probabilistic graphical model (MRF); integration of symbolic structure and probabilistic inference but internal clause structure often not used for logical inversion or theorem proving.",
            "emergent_properties": "Combines soft (probabilistic) reasoning with logical structure to handle noisy and uncertain data; provides a probability distribution over possible worlds given weighted rules.",
            "task_or_benchmark": "Used to induce axioms for the Smokers-and-friends experiment; specific MLN metric quoted: log-probability weight 6.88 for ∃y F(x,y) in that induced-axiom set.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Enables probabilistic tolerance to noisy/uncertain rules; the paper reports MLN assigned high log-probability to axioms that produced contradictions, showing MLN may prefer high-weight rules despite inconsistency.",
            "interpretability_properties": "Clause weights are interpretable as log-probability contributions, but internal clause structure is typically not used for theorem-prover style deduction in MLN implementations; inference traces are probabilistic rather than symbolic proofs.",
            "limitations_or_failures": "Requires expensive probabilistic inference (e.g., MCMC) to obtain marginal probabilities; clauses are atomic in many implementations (internal logical structure not leveraged), making certain logical deductions and theorem-proving tasks onerous; in the paper, MLN-produced axioms led to contradictions that LNN handled by relaxing bounds.",
            "theoretical_framework": "Probabilistic graphical models (Markov random fields) combined with first-order logic clauses; probabilistic semantics over possible worlds.",
            "uuid": "e645.1",
            "source_info": {
                "paper_title": "Logical Neural Networks",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "PSL",
            "name_full": "Probabilistic Soft Logic (Hinge-Loss MRFs)",
            "brief_description": "A framework using hinge-loss Markov random fields to perform soft, continuous relaxation of logical constraints for probabilistic reasoning over relational data.",
            "citation_title": "Hinge-loss markov random fields and probabilistic soft logic",
            "mention_or_use": "mention",
            "system_name": "Probabilistic Soft Logic (PSL / Hinge-loss MRFs)",
            "system_description": "An MRF-based approach where logical formulas are relaxed to continuous values and combined in hinge-loss potentials to form a convex optimization for probabilistic inference. Mentioned in the paper as part of the class of MRF/neuro-symbolic approaches contrasted with LNN.",
            "declarative_component": "Logical formulas relaxed to continuous-valued potentials representing soft constraints.",
            "imperative_component": "Convex optimization / inference over hinge-loss potentials (procedural numerical solvers).",
            "integration_method": "Continuous relaxation of logical clauses into a probabilistic graphical model with hinge-loss terms; inference performed via optimization rather than neural gradient-based end-to-end learning.",
            "emergent_properties": "Scales convex probabilistic inference to relational domains; tolerates noisy/soft constraints with convex optimizers.",
            "task_or_benchmark": null,
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Handles uncertainty and noisy rules via convex relaxation; the paper contrasts closed-world assumptions of MRF approaches with LNN's open-world bounds, but does not provide PSL-specific generalization metrics.",
            "interpretability_properties": "Weights and hinge-loss terms are interpretable as penalties for violating soft constraints; however, internal clause structure is not used for bidirectional theorem-proving as in LNN.",
            "limitations_or_failures": "Makes closed-world assumptions in many setups (contrasted with LNN's open-world bounds); solving global MRFs can be computationally expensive.",
            "theoretical_framework": "Hinge-loss MRFs; convex optimization for probabilistic inference with relaxed logical constraints.",
            "uuid": "e645.2",
            "source_info": {
                "paper_title": "Logical Neural Networks",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "LTN",
            "name_full": "Logic Tensor Networks",
            "brief_description": "A differentiable framework combining logic and continuous embeddings to learn from knowledge and data, using tensor representations of predicates and fuzzy logic semantics.",
            "citation_title": "Logic tensor networks: Deep learning and logical reasoning from data and knowledge",
            "mention_or_use": "use",
            "system_name": "Logic Tensor Networks (LTN)",
            "system_description": "A neuro-symbolic approach that represents logical predicates and constants as continuous tensors and applies fuzzy-logic inspired operators to combine logical constraints with learned embeddings; used in the paper as a comparative baseline (LTN experiment K_exp2 Smokers-and-friends).",
            "declarative_component": "First-order logic axioms represented in fuzzy/logical form (used to provide constraints and axioms).",
            "imperative_component": "Tensor-based continuous embeddings and neural-style learning to optimize predicate embeddings and satisfy fuzzy constraints.",
            "integration_method": "Embedding-based relaxation of logic: logical formulas converted to continuous differentiable loss terms over tensors, and optimized with gradient descent to learn embeddings that satisfy formulas.",
            "emergent_properties": "Enables learning from both data and logical knowledge; provides soft satisfiability degrees for axioms and can be trained end-to-end.",
            "task_or_benchmark": "Smokers-and-friends (K_exp2) experiment used in the paper for comparison.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Can integrate prior logical knowledge into learned continuous representations; in the paper LTN assigned high satisfiability to some axioms (e.g., 96% for an axiom) even when evidence suggested otherwise, indicating potential limitations in logical deduction fidelity (e.g., lacked friendship symmetry that LNN inferred).",
            "interpretability_properties": "Gives degrees of satisfiability for axioms (interpretable fuzzy scores), but internal embeddings are less directly interpretable than per-neuron logical units in LNN.",
            "limitations_or_failures": "May fail to produce precise symbolic deductions (e.g., unable to infer friendship symmetry in the smokers experiment) and may produce high satisfiability for axioms despite contradictory evidence; fewer explicit bidirectional logical inference capabilities compared to LNN.",
            "theoretical_framework": "Fuzzy/fuzzy-like logic combined with tensor embeddings and differentiable loss formulations to integrate logic and learning.",
            "uuid": "e645.3",
            "source_info": {
                "paper_title": "Logical Neural Networks",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "NTP",
            "name_full": "Neural Theorem Provers / Differentiable Proving",
            "brief_description": "Neural models that learn to perform logical proving using differentiable modules, typically limited to Horn clauses and embedding-based unification.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Neural Theorem Provers / End-to-end differentiable proving (NTPs)",
            "system_description": "Family of neural models that implement differentiable versions of symbolic proving (e.g., differentiable backward chaining) often limited to Horn clauses and using neural unification/embedding mechanisms; referenced as recent neural theorem provers that have not demonstrated success on classical ATP benchmarks used by symbolic provers.",
            "declarative_component": "Clauses/rules (often Horn clauses) used as templates for proving; symbolic structure is represented but often mediated via embeddings.",
            "imperative_component": "Neural modules for unification, rule application, and differentiable search; end-to-end gradient learning to score proof paths.",
            "integration_method": "Differentiable relaxation of discrete proof search (embedding-based unification, soft matching), enabling gradient-based learning over proof traces but typically limited to restricted clause forms (Horn).",
            "emergent_properties": "Learns soft, approximate proving capabilities and rule representations; can learn from noisy data.",
            "task_or_benchmark": "Mentioned with respect to classical Automated Theorem Proving (TPTP) — paper notes NTPs have not demonstrated success on classical ATP benchmarks.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Good at learning explanatory rules from noisy data in limited Horn-clause domains, but not demonstrated for general first-order ATP; limited expressivity relative to general FOL.",
            "interpretability_properties": "Can provide learned rule-like representations and soft proof traces, but these are typically in embedding space and less directly interpretable than explicit symbolic proofs.",
            "limitations_or_failures": "Limited to Horn-clause families and embedding-based approximate unification; the paper highlights that NTPs have not solved classical theorem-proving benchmarks (TPTP) that LNN can handle for a filtered subset.",
            "theoretical_framework": "Differentiable relaxation of symbolic proof search, embedding-based unification and soft matching of predicates/rules.",
            "uuid": "e645.4",
            "source_info": {
                "paper_title": "Logical Neural Networks",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "TensorLog",
            "name_full": "TensorLog",
            "brief_description": "A differentiable deductive database that maps logic program inference to sparse matrix/tensor operations for end-to-end differentiable learning.",
            "citation_title": "Tensorlog: A differentiable deductive database",
            "mention_or_use": "mention",
            "system_name": "TensorLog",
            "system_description": "A system that compiles logic programs into differentiable sparse-matrix operations so that logical deduction can be embedded into differentiable computation and learned via gradient methods; cited as an approach that differentiates logical deduction.",
            "declarative_component": "Logic programs / deductive database rules (definite clauses).",
            "imperative_component": "Sparse-matrix/tensor algebra and differentiable computation enabling gradient-based parameter learning.",
            "integration_method": "Compilation of symbolic deduction rules into sparse linear algebra operations (matrix multiplications and sums) that are differentiable and can be optimized end-to-end.",
            "emergent_properties": "Enables scaling logic-based inference with differentiable parameter learning and efficient tensorized implementation.",
            "task_or_benchmark": null,
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Preserves logical structure while enabling gradient learning; can exploit tensor optimizations for scalability.",
            "interpretability_properties": "Logical rules remain explicit in the compilation, though learned parameters live in matrices and may be less directly interpretable than symbolic constants.",
            "limitations_or_failures": "Primarily designed for definite-clause logic and may not directly express general FOL with full quantifier handling as LNN does; scaling requires sparse representations and careful grounding management.",
            "theoretical_framework": "Differentiable compilation of logical deduction to sparse linear algebra operations; bridges deductive databases and differentiable learning.",
            "uuid": "e645.5",
            "source_info": {
                "paper_title": "Logical Neural Networks",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "KBANN",
            "name_full": "Knowledge-Based Artificial Neural Networks (KBANN)",
            "brief_description": "Early approach mapping symbolic rules into neural network architectures to combine symbolic knowledge with connectionist learning.",
            "citation_title": "Knowledge-based artificial neural networks",
            "mention_or_use": "mention",
            "system_name": "Knowledge-Based Artificial Neural Networks (KBANN)",
            "system_description": "A historical neuro-symbolic scheme that transforms propositional rules into neural network structures whose initial weights encode the rules; subsequent training fine-tunes the network on data.",
            "declarative_component": "Propositional rules / logic program encoded into initial network structure and weights.",
            "imperative_component": "Standard neural networks (feedforward) and gradient-based learning fine-tuning weights.",
            "integration_method": "Direct mapping of symbolic rules into initial network connectivity and weights, followed by gradient descent to adapt to data.",
            "emergent_properties": "Allows prior knowledge to guide learning initialization and helps to learn from small data, while permitting adaptation to exceptions via training.",
            "task_or_benchmark": null,
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Can leverage encoded symbolic knowledge to improve data efficiency and generalization in domains where rules are correct; limited by propositional expressivity.",
            "interpretability_properties": "Initial mapping preserves some interpretability (rules correspond to subnetworks), but training can distort these correspondences unless constrained.",
            "limitations_or_failures": "Early methods often lost symbolic fidelity after training; limited logical expressivity (propositional rules) compared to approaches like LNN; no bidirectional logical inference mechanism.",
            "theoretical_framework": "Connectionist representation of symbolic rules and supervised fine-tuning; historical bridge between symbolic AI and neural networks.",
            "uuid": "e645.6",
            "source_info": {
                "paper_title": "Logical Neural Networks",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Markov logic networks",
            "rating": 2
        },
        {
            "paper_title": "Hinge-loss markov random fields and probabilistic soft logic",
            "rating": 2
        },
        {
            "paper_title": "Logic tensor networks: Deep learning and logical reasoning from data and knowledge",
            "rating": 2
        },
        {
            "paper_title": "End-to-end differentiable proving",
            "rating": 2
        },
        {
            "paper_title": "Tensorlog: A differentiable deductive database",
            "rating": 2
        },
        {
            "paper_title": "Learning explanatory rules from noisy data",
            "rating": 1
        },
        {
            "paper_title": "Knowledge-based artificial neural networks",
            "rating": 1
        }
    ],
    "cost": 0.021824,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Logical Neural Networks</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Ryan Riegel</th>
<th style="text-align: center;">Alexander Gray</th>
<th style="text-align: center;">Francois Luus</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">IBM Research - Watson</td>
<td style="text-align: center;">IBM Research - Watson</td>
<td style="text-align: center;">IBM Research - Africa</td>
</tr>
<tr>
<td style="text-align: center;">Naweed Khan</td>
<td style="text-align: center;">Ndivhuwo Makondo</td>
<td style="text-align: center;">Ismail Yunus Akhalwaya</td>
</tr>
<tr>
<td style="text-align: center;">IBM Research - Africa</td>
<td style="text-align: center;">IBM Research - Africa</td>
<td style="text-align: center;">IBM Research - Africa</td>
</tr>
<tr>
<td style="text-align: center;">Haifeng Qian</td>
<td style="text-align: center;">Ronald Fagin</td>
<td style="text-align: center;">Francisco Barahona</td>
</tr>
<tr>
<td style="text-align: center;">IBM Research - Watson</td>
<td style="text-align: center;">IBM Research - Almaden</td>
<td style="text-align: center;">IBM Research - Watson</td>
</tr>
<tr>
<td style="text-align: center;">Udit Sharma</td>
<td style="text-align: center;">Shajith Ikbal</td>
<td style="text-align: center;">Hima Karanam</td>
</tr>
<tr>
<td style="text-align: center;">IBM Research - India</td>
<td style="text-align: center;">IBM Research - India</td>
<td style="text-align: center;">IBM Research - India</td>
</tr>
<tr>
<td style="text-align: center;">Sumit Neelam</td>
<td style="text-align: center;">Ankita Likhyani</td>
<td style="text-align: center;">Santosh Srivastava</td>
</tr>
<tr>
<td style="text-align: center;">IBM Research - India</td>
<td style="text-align: center;">IBM Research - India</td>
<td style="text-align: center;">IBM Research - India</td>
</tr>
</tbody>
</table>
<h2>Abstract</h2>
<p>We propose a novel framework seamlessly providing key properties of both neural nets (learning) and symbolic logic (knowledge and reasoning). Every neuron has a meaning as a component of a formula in a weighted real-valued logic, yielding a highly intepretable disentangled representation. Inference is omnidirectional rather than focused on predefined target variables, and corresponds to logical reasoning, including classical first-order logic theorem proving as a special case. The model is end-to-end differentiable, and learning minimizes a novel loss function capturing logical contradiction, yielding resilience to inconsistent knowledge. It also enables the open-world assumption by maintaining bounds on truth values which can have probabilistic semantics, yielding resilience to incomplete knowledge.</p>
<h2>1 Introduction and related work</h2>
<p>We present Logical Neural Networks (LNNs), a neuro-symbolic framework designed to simultaneously provide key properties of both neural nets (NNs) (learning) and symbolic logic (knowledge and reasoning) - toward direct interpretability, utilization of rich domain knowledge realistically, and the general problem-solving ability of a full theorem prover. The central idea is to create a 1-to-1 correspondence between neurons and the elements of logical formulae, using the observation that the weights of neurons can be constrained to act as, e.g. AND or OR gates. While the view of neurons as logical gates was inherent in the seminal [13], it appears to have remained relatively unexploited since; an exception is the idea of converting logical statements into NN forms [14, 9]; the best known of these is [22], where the final model's neurons do not necessarily retain logical gate behaviors. In LNNs, no conversions are needed because they are identical. Inputs include a propositional or first-order logic (FOL) knowledge base (KB), including the usual training data (feature-value pairs) as a special case, and which variables should be predicted from which.
Per-neuron interoperability, via full logical expressivity. Many approaches are based on Markov random fields (MRFs), such as Markov logic networks [15], probabilistic soft logic [1], and those based on ILP/SLPs e.g. [3], where each logical clause has a weight; the clauses are atomic i.e. their</p>
<p>internal logical structure is not represented. Obtaining probabilities from them requires an unwieldy satisfiability problem to be solved, e.g. via MCMC in [15]. LNN inference is deterministic/repeatable and provably convergent in finite steps. In LNNs, every neuron represents an element in a clause, and is either a concept (e.g. "cat") or a logical connective (e.g. AND, OR), with weights on the connecting edges. Thus each neuron represents 1) a meaning, raising the level of interpretability versus previous approaches, 2) a way to identify the importance of relationships between variables and 3) more parameters, defining a richer model space for potentially more accurate prediction. The network structure is thus compositional and modular, e.g. able to represent that one clause may be a sub-clause of another. The representation is disentangled, versus approaches such as [19, 17] that use a vector representation, sacrificing interpretability of the network. Where many approaches only allow the representational power of propositional logic or Horn clauses, LNNs allow full function-free first-order logic with real values $0 \leq x \leq 1$, and classical $0 / 1$ logic as a special case.</p>
<p>Tolerance to incomplete knowledge, via truth bounds. The line of approach embodied by MRFs make a closed-world assumption, i.e. that if a statement doesn't appear in the KB, it is false. LNN does not require complete specification of all variables' exact degree of truth, more generally maintaining upper and lower bounds for each variable - allowing the open-world assumption that complete knowledge of the world is not realistic in general. Bounds also contain more interpretable information than single values, and we show that they can represent probabilistic semantics.</p>
<p>Many-task generality, via omnidirectional inference. LNN neurons express bidirectional relationships with each neighbor, allowing inference in any direction. This allows task generality versus typical single-task NNs, and allows full-fledged theorem proving. MRF approaches that hide the internal logical structure of clauses cannot draw the same conclusions that a theorem prover can. Many/most neuro-symbolic approaches, e.g. those based on embeddings, are arguably only "logic-like" and typically do not demonstrate reliably precise deduction. We show evidence of such capability on sample tasks used to test state-of-the-art theorem provers.</p>
<h1>2 Overview</h1>
<p>A logical neural network (LNN) is a form of recurrent neural network with a 1-to-1 correspondence to a set of logical formulae in any of various systems of weighted, real-valued logic, in which evaluation performs logical inference. Key innovations that set LNNs aside from other neural networks are 1) neural activation functions constrained to implement the truth functions of the logical operations they represent, i.e. $\wedge, \vee, \neg, \rightarrow$, and, in FOL, $\forall$ and $\exists$, 2) results expressed in terms of bounds on truth values so as to distinguish known, approximately known, unknown, and contradictory states, and 3) bidirectional inference permitting, e.g., $x \rightarrow y$ to be evaluated as usual in addition to being able to prove $y$ given $x$ or, just as well, $\neg x$ given $\neg y$. The nature of the modeled system of logic depends on the family of activation functions chosen for the network's neurons, which implement the logic's various atoms and operations. In particular, it is possible to constrain the network to behave exactly classically when provided classical input. Computation is characterized by tightening bounds on truth values at neurons pertaining to subformulae in upward and downward passes over the represented formulae's syntax trees. Bounds tightening is monotonic; accordingly, computation cannot oscillate and necessarily converges for propositional logic. Because of the network's modular construction, it is possible to partition and/or compose networks, inject formulae serving as logical constraints or queries, and control which parts of the network (or individual neurons) are trained or evaluated.</p>
<p>Inputs are initial truth value bounds for each of the neurons in the network; in particular, neurons pertaining to predicate atoms may be populated with truth values taken from KB data. Additional inputs may take the form of injected formulae representing a query or specific inference problem. Outputs are typically the final computed truth value bounds at one or more neurons pertaining to specific atoms or formulae of interest. In other problem contexts, the outputs of interest may instead be the neural parameters themselves - serving as a form of inductive logic programming (ILP) after learning with a given loss function and input training data set.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Neurons (1a) with alternative activation functions (1b, 1c) configured to match the truth functions of their corresponding operations, with established regions of unambiguously True, unambiguously False, and intermediate truth.</p>
<h1>3 Model structure</h1>
<p>In general, LNNs are described in terms of FOL, but it is useful to discuss LNNs restricted to the scope of propositional logic. ${ }^{1}$ Structurally, an LNN is a graph made up of the syntax trees of all represented formulae connected to each other via neurons added for each proposition. Specifically, as shown in Figure 1a, there exists one neuron for each logical operation occurring in each formula and, in addition, one neuron for each unique proposition occurring in any formula. All neurons return pairs of values in the range $[0,1]$ representing lower and upper bounds on the truth values of their corresponding subformulae and propositions. To aid interpretability of bounds, we define a threshold of truth $\frac{1}{2}&lt;\alpha \leq 1$ such that a continuous truth value is considered True if it is greater than $\alpha$ and False if it is less than $1-\alpha$. Bound values identify one of four primary states that a neuron can be in, whereas secondary states offer a more-true-than-not or more-false-than-not interpretation.</p>
<p>Table 1: Primary truth value bound states</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Bounds</th>
<th style="text-align: center;">Unknown</th>
<th style="text-align: center;">True</th>
<th style="text-align: center;">False</th>
<th style="text-align: center;">Contradiction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Upper</td>
<td style="text-align: center;">$[\alpha, 1]$</td>
<td style="text-align: center;">$[\alpha, 1]$</td>
<td style="text-align: center;">$[0,1-\alpha]$</td>
<td style="text-align: center;">Lower &gt; Upper</td>
</tr>
<tr>
<td style="text-align: center;">Lower</td>
<td style="text-align: center;">$[0,1-\alpha]$</td>
<td style="text-align: center;">$[\alpha, 1]$</td>
<td style="text-align: center;">$[0,1-\alpha]$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Neurons corresponding to logical connectives accept as input the output of neurons corresponding to their operands and have activation functions configured to match the connectives' truth functions. Neurons corresponding to propositions accept as input the output of neurons established as proofs of bounds on the propositions' truth values and have activation functions configured to aggregate the tightest such bounds. Proofs for propositions may be established explicitly, e.g. as the heads of Horn clauses, though Section 4 shows how bidirectional inference permits every occurrence of each proposition in each formula to be used as a potential proof. Negation is modeled as a pass-through node with no parameters, canonically performing $\neg x=1-x$.</p>
<h3>3.1 Activation functions for connectives</h3>
<p>Many candidate neural activation functions can accommodate the classical truth functions of logical connectives, each varying in how it handles inputs strictly between 0 and 1 . For instance, $\min {x, y}$ is a suitable activation function for real-valued conjunction $x \otimes y$, but so are $x \cdot y$ and $\max {0, x+y-1}$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The choice of activation function corresponds to the implemented real-valued logic; Gödel, product, and Łukasiewicz logic are common examples.</p>
<p>In addition to matching their corresponding connectives' truth functions, as described in Section 6, LNN requires monotonicity. Specifically, the activation functions for conjunction and disjunction must increase monotonically with respect to each operand, and the activation function for implication must decrease monotonically with respect to the antecedent and increase monotonically with respect to the consequent. In addition, it is useful though not required for $\otimes$ and real-valued disjunction $x \oplus y$ to be related via the De Morgan laws, for both operations to be commutative and associative, and for realvalued implication $x \rightarrow y$ to be the residuum of $\otimes$, or specifically $(x \rightarrow y)=\max {z \mid y \geq(x \otimes z)}$. These properties do not guarantee that $(x \rightarrow y)=(\neg x \oplus y)$, that $((x \rightarrow 0) \rightarrow 0)=x$, or that $(x \otimes x)=(x \oplus x)=x$, though they may individually hold for certain activation functions.
Gradient-based neural learning also requires differentiable parameters that can be tuned to improve model performance. We introduce the concept of importance weighting, whereby neural inputs with larger weight have more influence on neural output. This can take many forms, as thoroughly explored in [7], though this paper focuses on an easily computable weighting scheme based on nonlinear functions applied to dot products.</p>
<h1>3.2 Weighted nonlinear logic</h1>
<p>We introduce weighted generalizations of the standard real-valued logics in section B of the supplementary material. This completes the mapping of NNs to weighted real-valued logics. Here is shown a weighted generalization of Łukasiewicz-like logics. The $n$-ary weighted nonlinear conjunctions, used for logical AND, are given</p>
<p>$$
\beta\left(\bigotimes_{i \in I} x_{i}^{\otimes w_{i}}\right)=f\left(\beta-\sum_{i \in I} w_{i}\left(1-x_{i}\right)\right)
$$</p>
<p>for $f: \mathbb{R} \rightarrow[0,1]$ with $f(1-x)=1-f(x)$, input set $I$, bias term $\beta \geq 0$, weights $w_{i} \geq 0$, and inputs $x_{i} \in[0,1]$. The $n$-ary weighted nonlinear disjunctions, used for logical OR, are then</p>
<p>$$
\beta\left(\bigoplus_{i \in I} x_{i}^{\oplus w_{i}}\right)=f\left(1-\beta+\sum_{i \in I} w_{i} x_{i}\right)
$$</p>
<p>Observe that $\beta$ and the various $w_{i}$ establish a hyperplane with respect to the inputs $x_{i}$, though clamped to the $[0,1]$ range. For $f(x)=\max {0, \min {1, x}}$, the resulting activation functions are similar to the rectified linear unit (ReLU) from neural network literature and to the Łukasiewicz t- and s-norms. Alternate choices of $f$ include the logistic function as shown in Figure 1b and a linearly interpolated tailored activation function as shown in Figure 1c, as further explored in Section 6.
Bias term $\beta$ permits classically equivalent formulae $x \rightarrow y, \neg y \rightarrow \neg x$, and $\neg x \oplus y$ to be made equivalent in weighted nonlinear logic by adjusting $\beta$. The weighted nonlinear residuum, used for logical implication, is given by the residuum ${ }^{2}$ of $\otimes$,</p>
<p>$$
\beta\left(x^{\otimes w_{x}} \rightarrow y^{\oplus w_{y}}\right)=f\left(1-\beta+w_{x}(1-x)+w_{y} y\right)
$$</p>
<p>Note the use of $\otimes$ in the antecedent weight but $\oplus$ in the consequent weight, which is meant to indicate the antecedent has AND-like weighting (scaling its distance from 1) while the consequent has OR-like weighting (scaling its distance from 0 ). This residuum is most disjunction-like when $\beta=1$, most $(x \rightarrow y)$-like when $\beta=w_{y}$, and most $(\neg y \rightarrow \neg x)$-like when $\beta=w_{x}$.</p>
<h3>3.3 Activation functions for atoms</h3>
<p>Neurons pertaining to atoms require activation functions that aggregate truth values found for the various computations identified as proofs of the atom. For example, each of $\left(x_{1} \otimes x_{2} \otimes x_{3}\right) \rightarrow y$, $\left(x_{1} \otimes x_{4}\right) \rightarrow y$, and $\left(x_{2} \otimes x_{4}\right) \rightarrow \neg y$ may constitute proofs (or disproofs) of $y$. A typical means of aggregating proven truth values is to return the maximum proven lower bound and minimum proven upper bound. On the other hand, it may be desirable to employ importance weighting via weighted nonlinear logic in aggregation as well, substituting OR for max in the lower bound computation and AND for min in the upper bound. A natural choice of weights for such aggregations is the weights of the atoms as they occur in the formulae serving as their proofs. For example, if $x_{1}^{\otimes 3} \rightarrow y^{\oplus 2}$ and $x_{2}^{\otimes 1} \rightarrow y^{\oplus .5}$ are proofs of $y$, then aggregation would use weights 2 and .5 , respectively.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4 Inference</h1>
<p>Inference refers to the entire process of computing truth value bounds for (sub)formulae and atoms based on initial knowledge, ultimately resulting in predictions made at neurons pertaining to queried formulae or other results of interest. LNN achieves this with multiple passes over the represented formulae, propagating tightened truth value bounds from neuron to neuron until computation necessarily converges. The previous section already suggests the upward pass of inference, whereby formulae compute their truth value bounds based on bounds available for their subformulae. This section further describes the downward pass, which permits prior belief in the truth or falsity of formulae to inform truth value bound for propositions or predicates used in said formulae.</p>
<h3>4.1 Bidirectional inference</h3>
<p>In addition to the usual evaluation of connectives, LNN infers truth values bounds for each of a connective's inputs based on bounds on its output and other inputs. Depending on the type of connective involved, such computations correspond to the familiar inference rules of classical logic:</p>
<p>$$
\begin{aligned}
&amp; x, x \rightarrow y \vdash y \quad \text { (modus ponens) } \quad x, \neg(x \wedge y) \vdash \neg y \quad \text { (conjunctive syllogism) } \
&amp; \neg y, x \rightarrow y \vdash \neg x \quad \text { (modus tollens) } \neg x, \quad x \vee y \vdash y \quad \text { (disjunctive syllogism) }
\end{aligned}
$$</p>
<p>The precise nature of these computations depends on the selected family of activation functions. For example, as noted in Section 3.1, if implication is defined as the residuum, then modus ponens is performed via the logic's t-norm, i.e. AND. The remaining inference rules follow a similar pattern as prescribed by the functional or logical inverses of their upward computations.
The application of these inference rules immediately suggests a means of generating proofs for atoms based on the formulae they occur in. As further discussed in Section 4.3, given truth value bounds for a formula, it is possible to apply inference rules to obtain truth value bounds for each of its leaves.</p>
<h3>4.2 Inference rules in weighted nonlinear logic</h3>
<p>In the following, $L$ and $U$ denote lower and upper bounds found for neurons corresponding to the formulae indicated in their subscripts, e.g. $L_{x \rightarrow y}$ is the lower-bound truth value for the formula $x \rightarrow y$ as a whole while $U_{x}$ is the upper bound for just $x$. The bounds computations for $\neg$ are trivial:</p>
<p>$$
\begin{array}{ll}
L_{\neg x} \geq \neg U_{x}=1-U_{x}, &amp; L_{x} \geq \neg U_{\neg x}=1-U_{\neg x} \
U_{\neg x} \leq \neg L_{x}=1-L_{x}, &amp; U_{x} \leq \neg L_{\neg x}=1-L_{\neg x}
\end{array}
$$</p>
<p>The use of inequalities here acknowledges that tighter bounds for each value may be available from other sources. For instance, both $y$ and $x \rightarrow \neg y$ can yield $L_{\neg y}$; the tighter of the two would apply.
Observing that, in weighted nonlinear logic, ${ }^{\beta}\left(x^{\oplus w_{x}} \rightarrow y^{\oplus w_{y}}\right)={ }^{\beta}\left((1-x)^{\oplus w_{x}} \oplus y^{\oplus w_{y}}\right)$ and ${ }^{\beta}\left(\bigotimes_{i \in I} x_{i}^{\oplus w_{i}}\right)=1-{ }^{\beta}\left(\bigoplus_{i \in I}\left(1-x_{i}\right)^{\oplus w_{i}}\right)$, it is only necessary to define one set of inference rule computations to cover all connectives. The upward bounds computations for ${ }^{\beta}\left(\bigoplus_{i \in I} x_{i}^{\oplus w_{i}}\right)$ are</p>
<p>$$
L_{\bigoplus_{i} x_{i}} \geq{ }^{\beta}\left(\bigoplus_{i \in I} L_{x_{i}}^{\oplus w_{i}}\right), \quad U_{\bigoplus_{i} x_{i}} \leq{ }^{\beta}\left(\bigoplus_{i \in I} U_{x_{i}}^{\oplus w_{i}}\right)
$$</p>
<p>while the downward bounds computations for disjunction are</p>
<p>$$
\begin{array}{lll}
L_{x_{i}} \geq{ }^{\beta / w_{i}}\left(\left(\bigotimes_{j \neq i}\left(1-U_{x_{j}}\right)^{\otimes w_{j} / w_{i}}\right) \otimes L_{\bigoplus_{i} x_{i}}^{\otimes 1 / w_{i}}\right) &amp; \text { if } L_{\bigoplus_{i} x_{i}}&gt;1-\alpha, &amp; \text { else } 0 \
U_{x_{i}} \leq{ }^{\beta / w_{i}}\left(\left(\bigotimes_{j \neq i}\left(1-L_{x_{j}}\right)^{\otimes w_{j} / w_{i}}\right) \otimes U_{\bigoplus_{i} x_{i}}^{\otimes 1 / w_{i}}\right) &amp; \text { if } U_{\bigoplus_{i} x_{i}}&lt;\alpha, &amp; \text { else } 1
\end{array}
$$</p>
<p>where $\alpha$ is threshold determined by $f$ to address potential divergent behavior at $L_{\bigoplus_{i} x_{i}} \leq 1-\alpha$ and $U_{\bigoplus_{i} x_{i}} \geq \alpha$. To understand why this occurs, observe that, for $f(x)=\max {0, \min {1, x}}$, i.e. the ReLU case, $x \oplus y$ can return 1 for many different values of $x$ and $y$; specifically, whenever $w_{x} x+w_{y} y \geq \beta$. Accordingly, if $U_{\bigoplus_{i} x_{i}}=1$, we cannot infer an upper bound for any $x_{i}$.</p>
<h3>4.3 The Upward-Downward algorithm</h3>
<p>Inference propagates truth value bounds from neuron to neuron along in alternating upwards and downwards passes over the syntax trees of the represented formulae. The upward pass, shown in</p>
<div class="codehilite"><pre><span></span><code><span class="k">function</span><span class="w"> </span><span class="nf">upwardPass(formula \</span><span class="p">(</span>z\) <span class="p">):</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">operand</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="nb">i</span><span class="p">}</span><span class="o">\</span><span class="p">)</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="nb">i</span><span class="w"> </span><span class="o">\</span><span class="n">in</span><span class="w"> </span><span class="n">I</span><span class="o">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="o">\</span>#<span class="w"> </span><span class="n">propagate</span><span class="w"> </span><span class="nb">bounds</span><span class="w"> </span><span class="n">upward</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">leaves</span>
<span class="w">        </span><span class="n">upwardPass</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="nb">i</span><span class="p">}</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="n">z</span><span class="p">=</span><span class="o">\</span><span class="n">neg</span><span class="w"> </span><span class="n">x</span><span class="o">\</span><span class="p">)</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="o">\</span>#<span class="w"> </span><span class="n">negation</span>
<span class="w">        </span><span class="n">aggregate</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">U_</span><span class="p">{</span><span class="n">x</span><span class="p">},</span><span class="w"> </span><span class="mi">1</span><span class="o">-</span><span class="n">L_</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="n">z</span><span class="p">={</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">\</span><span class="nb">beta</span><span class="p">}</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="o">\</span><span class="n">bigoplus_</span><span class="p">{</span><span class="nb">i</span><span class="w"> </span><span class="o">\</span><span class="n">in</span><span class="w"> </span><span class="n">I</span><span class="p">}</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="nb">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">\</span><span class="n">otimes</span><span class="w"> </span><span class="n">w_</span><span class="p">{</span><span class="nb">i</span><span class="p">}}</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="p">)</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="o">\</span>#<span class="w"> </span><span class="n">multi</span><span class="o">-</span><span class="nb">input</span><span class="w"> </span><span class="n">disjunction</span>
<span class="w">        </span><span class="n">aggregate</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="o">\</span><span class="n">left</span><span class="p">({</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">\</span><span class="nb">beta</span><span class="p">}</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="o">\</span><span class="n">bigoplus_</span><span class="p">{</span><span class="nb">i</span><span class="w"> </span><span class="o">\</span><span class="n">in</span><span class="w"> </span><span class="n">I</span><span class="p">}</span><span class="w"> </span><span class="n">L_</span><span class="p">{</span><span class="n">x_</span><span class="p">{</span><span class="nb">i</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="o">\</span><span class="n">oplus</span><span class="w"> </span><span class="n">w_</span><span class="p">{</span><span class="nb">i</span><span class="p">}}</span><span class="o">\</span><span class="n">right</span><span class="p">),{</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">\</span><span class="nb">beta</span><span class="p">}</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="o">\</span><span class="n">bigoplus_</span><span class="p">{</span><span class="nb">i</span><span class="w"> </span><span class="o">\</span><span class="n">in</span><span class="w"> </span><span class="n">I</span><span class="p">}</span><span class="w"> </span><span class="n">U_</span><span class="p">{</span><span class="n">x_</span><span class="p">{</span><span class="nb">i</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="o">\</span><span class="n">oplus</span><span class="w"> </span><span class="n">w_</span><span class="p">{</span><span class="nb">i</span><span class="p">}}</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="n">right</span><span class="o">.\</span><span class="p">)</span>
<span class="w">    </span><span class="o">\</span>#<span class="w"> </span><span class="n">Other</span><span class="w"> </span><span class="n">operations</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">handled</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">combinations</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">above</span>
<span class="k">function</span><span class="w"> </span><span class="nf">aggregate(formula \(z,\left</span><span class="p">(</span>L_{z}^{\prime}, U_{z}^{\prime}\right)\) <span class="p">):</span>
<span class="w">    </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="n">L_</span><span class="p">{</span><span class="n">z</span><span class="p">},</span><span class="w"> </span><span class="n">U_</span><span class="p">{</span><span class="n">z</span><span class="p">}</span><span class="o">\</span><span class="n">right</span><span class="p">):=</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="o">\</span><span class="nb">max</span><span class="w"> </span><span class="o">\</span><span class="n">left</span><span class="o">\</span><span class="p">{</span><span class="n">L_</span><span class="p">{</span><span class="n">z</span><span class="p">},</span><span class="w"> </span><span class="n">L_</span><span class="p">{</span><span class="n">z</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">\</span><span class="n">prime</span><span class="p">}</span><span class="o">\</span><span class="n">right</span><span class="o">\</span><span class="p">},</span><span class="w"> </span><span class="o">\</span><span class="nb">min</span><span class="w"> </span><span class="o">\</span><span class="n">left</span><span class="o">\</span><span class="p">{</span><span class="n">U_</span><span class="p">{</span><span class="n">z</span><span class="p">},</span><span class="w"> </span><span class="n">U_</span><span class="p">{</span><span class="n">z</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">\</span><span class="n">prime</span><span class="p">}</span><span class="o">\</span><span class="n">right</span><span class="o">\</span><span class="p">}</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="w"> </span><span class="o">\</span><span class="n">quad</span><span class="o">\</span><span class="p">)</span><span class="w"> </span><span class="o">\</span>#<span class="w"> </span><span class="n">tighten</span><span class="w"> </span><span class="n">existing</span><span class="w"> </span><span class="nb">bounds</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="n">Downward</span><span class="w"> </span><span class="n">pass</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">infer</span><span class="w"> </span><span class="n">subformula</span><span class="w"> </span><span class="n">truth</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">bounds</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">formula</span><span class="w"> </span><span class="n">bounds</span>
<span class="n">function</span><span class="w"> </span><span class="n">downwardPass</span><span class="w"> </span><span class="p">(</span><span class="n">formula</span><span class="w"> </span><span class="p">\(</span><span class="n">z</span><span class="p">\)</span><span class="w"> </span><span class="p">)</span><span class="o">:</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">operand</span><span class="w"> </span><span class="p">\(</span><span class="n">x_</span><span class="p">{</span><span class="n">j</span><span class="p">}\)</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="p">\(</span><span class="n">z</span><span class="o">,</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="p">\</span><span class="k">in</span><span class="w"> </span><span class="n">I</span><span class="p">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">\(</span><span class="n">z</span><span class="o">=</span><span class="p">\</span><span class="n">neg</span><span class="w"> </span><span class="n">x</span><span class="p">\)</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="p">\#</span><span class="w"> </span><span class="n">negation</span>
<span class="w">            </span><span class="n">aggregate</span><span class="w"> </span><span class="p">\(\</span><span class="n">left</span><span class="p">(</span><span class="n">x</span><span class="o">,</span><span class="p">\</span><span class="n">left</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">U_</span><span class="p">{</span><span class="n">z</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="mi">1</span><span class="o">-</span><span class="n">L_</span><span class="p">{</span><span class="n">z</span><span class="p">}\</span><span class="n">right</span><span class="p">)\</span><span class="n">right</span><span class="p">)\)</span>
<span class="w">        </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">\(</span><span class="n">z</span><span class="o">=</span><span class="p">{</span><span class="w"> </span><span class="p">}^{\</span><span class="n">beta</span><span class="p">}\</span><span class="n">left</span><span class="p">(\</span><span class="n">bigoplus_</span><span class="p">{</span><span class="n">i</span><span class="w"> </span><span class="p">\</span><span class="k">in</span><span class="w"> </span><span class="n">I</span><span class="p">}</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}^{\</span><span class="n">otimes</span><span class="w"> </span><span class="n">w_</span><span class="p">{</span><span class="n">i</span><span class="p">}}\</span><span class="n">right</span><span class="p">)\)</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="p">\#</span><span class="w"> </span><span class="n">multi</span><span class="o">-</span><span class="n">input</span><span class="w"> </span><span class="n">disjunction</span>
<span class="w">            </span><span class="p">\(</span><span class="n">L_</span><span class="p">{</span><span class="n">x_</span><span class="p">{</span><span class="n">j</span><span class="p">}}^{\</span><span class="n">prime</span><span class="p">}</span><span class="o">:=</span><span class="p">{</span><span class="w"> </span><span class="p">}^{\</span><span class="n">beta</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">w_</span><span class="p">{</span><span class="n">j</span><span class="p">}}\</span><span class="n">left</span><span class="p">(\</span><span class="n">left</span><span class="p">(\</span><span class="n">bigotimes_</span><span class="p">{</span><span class="n">i</span><span class="w"> </span><span class="p">\</span><span class="n">neq</span><span class="w"> </span><span class="n">j</span><span class="p">}\</span><span class="n">left</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">U_</span><span class="p">{</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}}\</span><span class="n">right</span><span class="p">)^{\</span><span class="n">otimes</span><span class="w"> </span><span class="n">w_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">w_</span><span class="p">{</span><span class="n">j</span><span class="p">}}\</span><span class="n">right</span><span class="p">)</span><span class="w"> </span><span class="p">\</span><span class="n">otimes</span><span class="w"> </span><span class="n">L_</span><span class="p">{\</span><span class="n">bigoplus_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}}^{\</span><span class="n">otimes</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">w_</span><span class="p">{</span><span class="n">j</span><span class="p">}}\</span><span class="n">right</span><span class="p">)</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="p">\)</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">\(</span><span class="n">L_</span><span class="p">{\</span><span class="n">bigoplus_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}}</span><span class="o">&gt;</span><span class="mi">1</span><span class="o">-</span><span class="p">\</span><span class="n">alpha</span><span class="o">,</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="p">\)</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="mi">0</span>
<span class="w">            </span><span class="p">\(</span><span class="n">U_</span><span class="p">{</span><span class="n">x_</span><span class="p">{</span><span class="n">j</span><span class="p">}}^{\</span><span class="n">prime</span><span class="p">}</span><span class="o">:=</span><span class="p">{</span><span class="w"> </span><span class="p">}^{\</span><span class="n">beta</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">w_</span><span class="p">{</span><span class="n">j</span><span class="p">}}\</span><span class="n">left</span><span class="p">(\</span><span class="n">left</span><span class="p">(\</span><span class="n">bigotimes_</span><span class="p">{</span><span class="n">i</span><span class="w"> </span><span class="p">\</span><span class="n">neq</span><span class="w"> </span><span class="n">j</span><span class="p">}\</span><span class="n">left</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">L_</span><span class="p">{</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}}\</span><span class="n">right</span><span class="p">)^{\</span><span class="n">otimes</span><span class="w"> </span><span class="n">w_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">w_</span><span class="p">{</span><span class="n">j</span><span class="p">}}\</span><span class="n">right</span><span class="p">)</span><span class="w"> </span><span class="p">\</span><span class="n">otimes</span><span class="w"> </span><span class="n">U_</span><span class="p">{\</span><span class="n">bigoplus_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}}^{\</span><span class="n">otimes</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">w_</span><span class="p">{</span><span class="n">j</span><span class="p">}}\</span><span class="n">right</span><span class="p">)</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="p">\)</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">\(</span><span class="n">U_</span><span class="p">{\</span><span class="n">bigoplus_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}}</span><span class="o">&lt;</span><span class="p">\</span><span class="n">alpha</span><span class="o">,</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="p">\)</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="mi">1</span>
<span class="w">            </span><span class="p">\(\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">aggregate</span><span class="p">}\</span><span class="n">left</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">j</span><span class="p">}</span><span class="o">,</span><span class="p">\</span><span class="n">left</span><span class="p">(</span><span class="n">L_</span><span class="p">{</span><span class="n">x_</span><span class="p">{</span><span class="n">j</span><span class="p">}}^{\</span><span class="n">prime</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="n">U_</span><span class="p">{</span><span class="n">x_</span><span class="p">{</span><span class="n">j</span><span class="p">}}^{\</span><span class="n">prime</span><span class="p">}\</span><span class="n">right</span><span class="p">)\</span><span class="n">right</span><span class="p">)\)</span>
<span class="w">        </span><span class="n">downwardPass</span><span class="w"> </span><span class="p">\(\</span><span class="n">left</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">j</span><span class="p">}\</span><span class="n">right</span><span class="p">)</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="p">\)</span><span class="w"> </span><span class="p">\#</span><span class="w"> </span><span class="n">propagate</span><span class="w"> </span><span class="n">bounds</span><span class="w"> </span><span class="n">downward</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">leaves</span>
</code></pre></div>

<p>Algorithm 1, uses truth value bounds available at atoms to compute bounds at each subformula according to the normal evaluation of connectives based on their operands. The downward pass, shown in Algorithm 2, uses truth value bounds known for formulae and previously computed at other subformulae to tighten bounds at each subformula (and ultimately each atom) according to the inference rules given above. This process repeats, as shown in Algorithm 3, until convergence, as proved in section C of the supplementary material:
Theorem 1. Given monotonic $\neg, \oplus$, and $f$, Algorithm 3 converges to within $\epsilon$ in finite time.</p>
<h1>5 LNN bounds as probability bounds</h1>
<p>This section presents a variant of LNN where lower and upper bound truth values at each subformula serve as bounds on the probability that the subformula is True in classical logic. This is achieved by using different activation functions for lower and upper bound computations. Again observing that implication and conjunction may be defined in terms of negation and disjunction, these are</p>
<p>$$
\begin{array}{ll}
L_{\bigoplus_{i} x_{i}} \geq \max <em x__i="x_{i">{i \in I} L</em>\right} \
U_{\bigoplus_{i} x_{i}} \leq \min \left{1, \sum_{i \in I} U_{x_{i}}\right} &amp; U_{x_{i}} \leq U_{\bigoplus_{i} x_{i}}
\end{array}
$$}} &amp; L_{x_{i}} \geq \max \left{0, L_{\bigoplus_{i} x_{i}}-\sum_{j \neq i} U_{x_{j}</p>
<p>with negation unchanged from above. Let $A$ be the set of atomic formulae and let $g: A \rightarrow{\mathrm{~T}, \mathrm{~F}}$ be an interpretation. Further define $g(\sigma)$ for any formula $\sigma$ on $A$ to be the truth value of $\sigma$ under the truth-value assignments by $g$ to the atomic formulae. Let $\Lambda$ be the set of all interpretations.
A sentence is an expression of the form $(\sigma, l, u)$ where $\sigma$ is a formula and $l, u \in[0,1]$. A theory is a set of sentences $\Gamma=\left{\left(\sigma_{1}, l_{1}, u_{1}\right), \cdots,\left(\sigma_{k}, l_{k}, u_{k}\right)\right}$. Define $S_{\sigma} \triangleq{g \mid g(\sigma)=\mathrm{T}}$ for any formula</p>
<div class="codehilite"><pre><span></span><code>Algorithm 3: Recurrent inference procedure with recursive directional graph traversal
function inference(formulae x):
    while \(\sum\left(\left|\delta L_{z}\right|+\left|\delta U_{z}\right|\right)&gt;\epsilon\) do \# loop until convergence
        for \(r \in \operatorname{roots}(\mathbf{z})\) do \# visit all formula roots in sequence
            upwardPass \((r) \quad\) \# leaves-to-root traversal
                downwardPass \((r) \quad\) \# root-to-leaves traversal
</code></pre></div>

<p>$\sigma$. A model is a probability function $p(\cdot)$ over $\Lambda$. We say that $p(\cdot)$ is a model of $\Gamma$ and write $p(\cdot) \models \Gamma$ if and only if $l_{i} \leq p\left(S_{\sigma_{i}}\right) \leq u_{i}$ for $i=1, \cdots, k$. Let $P_{\Gamma}$ denote the set of all models of $\Gamma$.
Initial knowledge is specified by a set of formulas $V_{0}$ and two functions $L_{0}: V_{0} \rightarrow[0,1]$ and $U_{0}: V_{0} \rightarrow[0,1]$. We may then state the following theorem, proved in section D of the supplementary material:
Theorem 2. Let $L_{\sigma}$ and $U_{\sigma}$ denote the lower and upper bounds computed by LNN for formula $\sigma$. Define $\Gamma_{0}=\left{\left(v, L_{0}(v), U_{0}(v)\right) \mid v \in V_{0}\right}$. If $P_{\Gamma_{0}} \neq \emptyset$, the following inequalities hold:</p>
<p>$$
L_{\sigma} \leq \inf <em _Gamma__0="\Gamma_{0">{p \in P</em> \geq \sup }}} p\left(S_{\sigma}\right) \quad U_{\sigma<em _Gamma__0="\Gamma_{0">{p \in P</em>\right)
$$}}} p\left(S_{\sigma</p>
<h1>6 Learning</h1>
<p>A core strength of the LNN model is its differentiability, permitting the optimization via backpropagation of parameters including operand importance weights, formula truth value bounds, and/or the truth value bounds of atoms. Loss functions for LNN may exploit its logical interpretability, in particular by penalizing contradiction, which can then be used to enforce even complicated logical requirements. An important consideration, however, is whether it is desired to preserve neurons' fidelity to their corresponding logical connectives, especially when presented with classical inputs.
Weighted nonlinear logic behaves classically for classical inputs when optimized as</p>
<p>$$
\begin{array}{ll}
\min <em N="N" _in="\in" k="k">{B, W} &amp; E(B, W)+\sum</em>\right} \
\text { s.t. } &amp; \forall k \in N, i \in I_{k}, \quad \alpha \cdot w_{i k}-\beta_{k}+1 \geq \alpha, \quad w_{i k} \geq 0 \
&amp; \forall k \in N, \quad \sum_{i \in I_{k}}(1-\alpha) \cdot w_{i k}-\beta_{k}+1 \leq 1-\alpha, \quad \beta_{k} \geq 0
\end{array}
$$} \max \left{0, L_{B, W, k}-U_{B, W, k</p>
<p>for loss function $E$, bias vector $B$, weight matrix $W$, (disjunction) neuron index set $N$, and inferred lower and upper bounds $L_{B, W, k}$ and $U_{B, W, k}$ at each neuron. Intuitively, (6) requires disjunctions to return True if any of their inputs are true, even if their other inputs are 0 , i.e. maximally false, while (7) requires them to return False if all of their inputs are false. Loss function $E$ often embodies typical NN learning objectives such as mean-square error; in addition, contradiction loss $\sum_{k \in N} \max \left{0, L_{B, W, k}-U_{B, W, k}\right}$ penalizes the sum total contradiction observed in the system.
Given the above linear constraints, methods such as Frank-Wolfe [8, 12] may be used to optimize $B$ and $W$. It is easy to see, however, that weights $w_{i k}$ cannot be made equal to 0 , nor can constraints be relaxed to permit nonclassical behavior. This may be corrected via the introduction of slack variables, though the following presents a means of sidestepping this issue while also improving gradients.</p>
<h3>6.1 Tailored activation functions</h3>
<p>For disjunction with $\beta=1$, the tailored activation function $f_{\mathbf{w}}$, shown in Figure 1c, is a linear interpolation between four critical points $-(0,0),\left(x_{\mathrm{F}}, 1-\alpha\right),\left(x_{\mathrm{T}}, \alpha\right)$, and $\left(x_{\max }, 1\right)$, establishing regions of unambiguous True, intermediate, and False truth values, respectively - given</p>
<p>$$
\begin{aligned}
&amp; f_{\mathbf{w}}(x)= \begin{cases}x \cdot(1-\alpha) / x_{\mathrm{F}} &amp; \text { if } 0 \leq x \leq x_{\mathrm{F}} \
\left(x-x_{\mathrm{F}}\right) \cdot(2 \alpha-1) /\left(x_{\mathrm{T}}-x_{\mathrm{F}}\right)+1-\alpha &amp; \text { if } x_{\mathrm{F}}&lt;x&lt;x_{\mathrm{T}} \
\left(x-x_{\mathrm{T}}\right) \cdot(1-\alpha) /\left(x_{\max }-x_{\mathrm{T}}\right)+\alpha &amp; \text { if } x_{\mathrm{T}} \leq x \leq x_{\max }\end{cases} \
&amp; x_{\mathrm{F}}=\sum_{i \in I} w_{i} \cdot(1-\alpha), \quad x_{\mathrm{T}}=w_{\max } \cdot \alpha, \quad x_{\max }=\sum_{i \in I} w_{i}
\end{aligned}
$$</p>
<p>By construction, this guarantees classical inputs produce classical results without the need for constraints. In addition, because $x_{\mathrm{T}}$ is defined in terms of $w_{\max }$, weights may drop to 0 without significantly impacting $f_{\mathbf{w}}$. By the nature of monotonic linear interpolation, gradients are large and reliable everywhere. Lastly, the tailored activation function establishes $\alpha$ as a means of controlling the system's classicality, with smaller values being more classical.</p>
<h2>7 Empirical evaluation</h2>
<p>Smokers and friends. LTN experiment $\mathcal{K}_{\text {exp2 }}$ [19] has plausible universally quantified axioms for a small universe (a-h) with initial facts for smokes $S(x)$, cancer $C(x)$, and friends $F(x, y)$ (openworld). We repeat the experiment including axioms induced by MLN [15] (total 8 axioms) on this</p>
<p>Table 2: Learnt LNN neuron weights (from $P_{1}^{8}$ ) and axiom lower bounds (as %) for LTN experiment $K_{\text{exp2}}$ (universe: a-h) [19], compares LTN degree of satisfiability (as % for 5 axioms) to LNN $P_{1}^{5},P_{2}^{5}$ and repeats in $P_{1}^{8},P_{2}^{8}$ for 8 axioms including 3 induced by MLN [15], with corresponding MLN log-probability weights, followed by axiom-wise contradiction counts. Loss function (1 + contradiction)/(1 + factalign + tightbounds) (normalized) component values after training show complete removal of contradictions by relaxing facts and inferences. Gradient descent (α = 1, β = 1, wmax = 1 with weight normalization and gradient-transparent clamping) adjusts operand weights (P1), initial axiom/fact bounds (P1, P2). Every training epoch performs inference initialized with updated bounds until convergence after the parameter update.</p>
<table>
<thead>
<tr>
<th>Smokers and friends [Kexp2 (a-h)]</th>
<th>LTN</th>
<th>LNN [L, U]</th>
<th>100 epochs, lr: 0.1→0</th>
<th></th>
<th></th>
<th></th>
<th>MLN L&gt;U</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>$P_{1}^{5}$</td>
<td>$P_{2}^{5}$</td>
<td>$P_{1}^{8}$</td>
<td>$P_{2}^{8}$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>∃yF(x, y)</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>6.88</td>
<td>0</td>
</tr>
<tr>
<td>¬F(x, x)</td>
<td>98</td>
<td>[83, 98]</td>
<td>[83, 98]</td>
<td>[56, 98]</td>
<td>[80, 98]</td>
<td>0.26</td>
<td>0</td>
</tr>
<tr>
<td>¬F(x, y)0.96∖F(y, x)1</td>
<td>90</td>
<td>[96, 97]</td>
<td>[97, 100]</td>
<td>[51, 95]</td>
<td>[82, 97]</td>
<td>-</td>
<td>0</td>
</tr>
<tr>
<td>¬S(x)0.98∖¬F(x, y)1∖S(y)0.97</td>
<td>96</td>
<td>[65, 100]</td>
<td>[65, 100]</td>
<td>[65, 100]</td>
<td>[66, 100]</td>
<td>3.53</td>
<td>2</td>
</tr>
<tr>
<td>¬S(x)1∖C(x)0.98</td>
<td>77</td>
<td>[57, 100]</td>
<td>[58, 100]</td>
<td>[50, 100]</td>
<td>[60, 100]</td>
<td>-1.35</td>
<td>2</td>
</tr>
<tr>
<td>¬F(x, y)0.97∖¬S(y)1∖F(y, x)0.96∖S(x)1</td>
<td></td>
<td></td>
<td></td>
<td>100</td>
<td>100</td>
<td>6.87</td>
<td>0</td>
</tr>
<tr>
<td>¬F(w, x)1∖¬F(w, y)1∖¬F(z, x)1∖C(z)0.97</td>
<td></td>
<td></td>
<td></td>
<td>[73, 100]</td>
<td>[70, 100]</td>
<td>4.33</td>
<td>51</td>
</tr>
<tr>
<td>¬F(w, x)1∖¬F(y, w)1∖¬F(z, y)1∖¬S(y)0.99</td>
<td></td>
<td></td>
<td></td>
<td>[80, 100]</td>
<td>[77, 100]</td>
<td>9.68</td>
<td>66</td>
</tr>
<tr>
<td>Contradiction (remaining)</td>
<td></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td></td>
<td>121</td>
</tr>
<tr>
<td>Factual Ei[</td>
<td>Ei - Ei</td>
<td>+</td>
<td>Ui - Ui</td>
<td>(start: 0.64)</td>
<td>0.42</td>
<td>0.43</td>
<td>0.27</td>
</tr>
<tr>
<td>Bound tightness Ei[exp(Ei - Ui)] (start: 1)</td>
<td></td>
<td>0.88</td>
<td>0.89</td>
<td>0.9</td>
<td>0.97</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>data, and measure total network contradiction and LNN truth bounds for axioms in Table 2. MLN axiom weight 6.88 for ∃yF(x, y) states that a world with n friendless people is e6.88n times less probable than a world where all have friends, other things being equal [15], but LNN sets full truth as it does not conflict. MLN assigns high log-probability to the last axiom even though it produces contradictions, while LNN sufficiently relaxes the axiom to remove conflicts. LTN assigns high satisfiability of 96 to ¬S(x)∖¬F(x, y)∖S(y), despite evidence against the axiom whereas LNN correctly adjusts bounds to remove contradiction. LNN also infers all logical consequences, in particular friendship symmetry that LTN is unable to produce. Learning of neuron weights can possibly reduce bounds relaxation in P18 when comparing to P28 for the last two high-conflict axioms.</p>
<p>LUBM benchmark. To verify the soundness and completeness of the reasoning performed by LNN, we used Lehigh University Benchmark (LUBM) [10], a synthetic OWL reasoning dataset in university domain with 14 benchmark queries. We generated data for 1 university (102707 triples), parsed the OWL axioms into equivalent LNN constructs resulting in a graph with 257 nodes. After three passes of bidirectional inferences for the network to converge, 14 queries were added to the network to compute respective answers. We compared LNN results with few symbolic reasoners namely Stardog [20], Virtuoso [23] and Blazegraph [2]. Although, all the systems are sound, achieving 100% precision, only Stardog and LNN answers are complete with 100% recall, compared to average recall of 72%(Virtuoso) and 78%(BlazeGraph). In another experiment, we evaluated LNN's ability to handle ontological inconsistencies, specifically those arising from incorrect axioms by inserting them into network. During inference, those inconsistencies emerge as bound value contradictions, thus allowing LNN to accurately locate and down-weight corresponding nodes from taking part in further inference (section H.2 of the supplementary material expands on noise handling).</p>
<p>TPTP benchmark. We also used a subset of the TPTP benchmark [21, 4] to evaluate LNN in generic classical theorem proving. The TPTP (Thousands of Problems for Theorem Provers) is a comprehensive library for Automated Theorem Proving (ATP) systems, with over 22K problems. We evaluated on a subset of the Common Sense Reasoning domain (937 problems) represented in first-order form (FOF), also filtering out functions and equality — currently not supported by LNN. From the remaining subset of 25 problems, LNN was able to prove all problems within seconds. Despite this small set of TPTP problems, it is noted that none of the recent neural theorem provers (NTP) [6, 17, 5] have demonstrated success on classical ATP. Moreover, NTP inference is limited to</p>
<p>Horn clauses while LNN can support general first-order logical expressions. These promising results show LNN's potential for generic ATP integrated into an end-to-end differentiable learning system.</p>
<h1>8 Conclusions</h1>
<p>We have 1) introduced a new conceptual neuro-symbolic framework, including introducing a new class of weighted real-valued logics, and ways of providing truth bounds which we show can have probabilistic semantics, 2) demonstrated approaches for learning in the framework including novel loss function minimizing logical contradiction, and a way to provably bypass the need to perform constrained optimization, and 3) demonstrated approaches for inference/reasoning in the framework, including the upward-downward algorithm which is provably convergent in finite steps, via preliminary experiments confirming the efficacy of the approach compared to others. Planned future work includes approaches for rule induction and mixed symbolic/sub-symbolic sub-networks.</p>
<h2>Broader Impact</h2>
<p>As a step in the direction of explainable AI, logical neural networks provide a flexible and wellperforming framework for neuro-symbolic learning that can nonetheless be 1) interpreted on account of their 1-to-1 correspondence to systems of logical formulae, 2) audited by examining the chain of inferences computed for a given query, and 3) controlled by human users through the specification of logical constraints. As a result, LNNs stand to improve the transparency and fairness of modeled tasks, and may serve as a better performing alternative to older methods (e.g. decision trees and logistic regression) when explainability is a design requirement.</p>
<h2>References</h2>
<p>[1] S. H. Bach, M. Broecheler, B. Huang, and L. Getoor. Hinge-loss markov random fields and probabilistic soft logic. The Journal of Machine Learning Research, 18(1):3846-3912, 2017.
[2] Blazegraph. Available at https://blazegraph.com/.
[3] W. W. Cohen. Tensorlog: A differentiable deductive database, 2016.
[4] M. Crouse, I. Abdelaziz, B. Makni, S. Whitehead, C. Cornelio, P. Kapanipathi, E. Pell, K. Srinivas, V. Thost, M. Witbrock, and A. Fokoue. A deep reinforcement learning based approach to learning transferable proof guidance strategies, 2019.
[5] H. Dong, J. Mao, T. Lin, C. Wang, L. Li, and D. Zhou. Neural logic machines. arXiv preprint arXiv:1904.11694, 2019.
[6] R. Evans and E. Grefenstette. Learning explanatory rules from noisy data. Journal of Artificial Intelligence Research, 61:1-64, 2018.
[7] R. Fagin and E. L. Wimmers. A formula for incorporating weights into scoring rules. Theoretical Computer Science, 239(2):309-338, 2000.
[8] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval research logistics quarterly, 3(1-2):95-110, 1956.
[9] A. S. d. Garcez and G. Zaverucha. The connectionist inductive learning and logic programming system. Applied Intelligence, 11(1):59-77, 1999.
[10] Y. Guo, Z. Pan, and J. Heflin. Lubm: A benchmark for owl knowledge base systems. Web Semantics: Science, Services and Agents on the World Wide Web, 3(2-3):158-182, 2005.
[11] P. Hájek. Metamathematics of fuzzy logic, volume 4. Springer Science \&amp; Business Media, 2013.
[12] M. Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In Proceedings of the 30th international conference on machine learning, pages 427-435, 2013.
[13] W. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biophysics, 5:115-133, 1943.
[14] G. Pinkas. Reasoning, connectionist nonmonotonicity and learning in networks that capture propositional knowledge. Artificial Intelligence, 1994.</p>
<p>[15] M. Richardson and P. Domingos. Markov logic networks. Machine learning, 62(1-2):107-136, 2006.
[16] S. Richardson, P. Domingos, and M. S. H. Poon. The alchemy system for statistical relational ai: User manual, 2007.
[17] T. Rocktäschel and S. Riedel. End-to-end differentiable proving. In Advances in Neural Information Processing Systems, pages 3788-3800, 2017.
[18] S. Russell and P. Norvig. Artificial Intelligence: A Modern Approach, chapter 9, pages 322-325. Prentice Hall Press, USA, 3 edition, 2009.
[19] L. Serafini and A. d. Garcez. Logic tensor networks: Deep learning and logical reasoning from data and knowledge. arXiv preprint arXiv:1606.04422, 2016.
[20] Stardog. Available at https://www.stardog.com/.
[21] G. Sutcliffe. The tptp problem library and associated infrastructure. Journal of Automated Reasoning, 43(4):337, 2009.
[22] G. G. Towell and J. W. Shavlik. Knowledge-based artificial neural networks. Artificial intelligence, 70(1-2):119-165, 1994.
[23] Virtuoso. Available at https://virtuoso.openlinksw.com/.</p>
<h1>Supplementary material for Logical Neural Networks</h1>
<h2>A First-order logical neural networks</h2>
<h2>A. 1 Overview</h2>
<p>This section introduces an extension to the representation of the LNN to handle formulae expressed in the first-order logic (FOL) language, and supplements footnote 1 (page 1) in the main paper. The FOL language enables representing a domain in terms of objects that exist in that domain and relations that hold between the objects in that domain. Its vocabulary includes constant symbols, predicate symbols, functional symbols, where constants refer to objects and predicates and functions refer to relations. In addition to the logical connectives in propositional logic, it introduces two quantifiers, the universal and existential quantifiers. Furthermore, some FOL representations introduce an equality symbol, which is treated as either a predicate or a logical connective. This section discusses the treatment of First-order LNN without function and equality symbols and treatment of these will be provided in future work. Furthermore, since equality is not handled, we necessarily make the unique-names assumption, insisting that every constant symbol refers to a distinct object in the domain.</p>
<p>In First-order LNN, there exist a neuron for each logical connective as before, a connective neuron, and also for each predicate symbol, an atomic neuron. In addition, each neuron keeps a table whose columns are keyed by unique variables appearing in the represented (sub)formulae, and rows keyed by a set of $n$-element tuples of groundings, where the tuple size $n$ corresponds to the arity of the neuron. The content of the tables are the bounds of each grounding when substituted for the variables. Similar to Negation, quantifiers are modeled as pass-through nodes with no parameters, with special operations for each quantifier type. The bounds of a universal quantifier node are set to the bounds of the grounding with the lowest upper bound, so that if all of its groundings are True then the universal statement is also True. And the bounds of the existential quantifier are set to the bounds of the grounding with highest lower bound, so that if this lower bound is True the existential statement will be True when at least one of its groundings is True.</p>
<p>Computation of bounds throughout the network proceeds as before, with each grounding treated separately. This method of extending to FOL is similar to approaches that reduce inference in classical first-order logic to propositional logic. Each grounding in each formulae is treated as a proposition.</p>
<h2>A. 2 Inference in first-order logical neural networks</h2>
<p>Section A. 1 introduced the representation of First-order logical neural networks, where constants, predicates and quantifiers were introduced. Instead of proposition neurons there are predicate neurons, or atomic neurons, and in addition to connective neurons there are also neurons for quantifiers. In inference for First-order LNN all neurons return tables of groundings and their corresponding bounds. Neural activation functions are then modified to perform joins over columns pertaining to shared variables while computing truth value bounds for the associated grounding. Inverse activation functions are modified similarly, but must also reduce results over any columns pertaining to variables absent from the target input's corresponding subformula so as to aggregate the tightest bounds. In the special case that tables are keyed by consecutive integers, these computations are equivalent to elementwise broadcast operations on sparse tensors, where each tensor dimension pertains to a different variable.</p>
<p>Inverse computation for quantifiers eliminates a given key column(s) corresponding to quantified variable(s) by reducing with min or max as appropriate. However, a proper treatment of inverse computation for the existential quantifier is more complicated as it would introduce Skolem functions. With functions currently not handled, inverse computation for the existential quantifier only broadcasts its known upper bounds to all key values associated with its column (i.e. variable) and broadcasts</p>
<p>its known lower bounds to a group of new key values identified by each combination of key values associated with any of its columns and vice versa for universal quantifiers.</p>
<h1>A. 3 Grounding management</h1>
<p>Neurons in First-order LNN each have a defined set of variable(s) according to their arity, specifying the number of constants in a grounding tuple. The arity of predicate neurons is usually set beforehand (e.g., informed by a knowledge base ingested by the LNN), and can typically include a variety of nullary (propositional), unary, binary and higher-arity predicates. Connective neurons and quantifiers collect variables from their set of input (or operands) in order of appearance during initialization, where these operands can include atomic neurons, other connective neurons and quantifiers. Variables are collected only once from operands that define repeat occurrences of a specific variable in more than one variable position, unless otherwise specified. Logical formulae can also be defined with arbitrary variable placement across its constituent nodes. A variable mapping operation transforms groundings for enabling truth value lookup in neighboring nodes.</p>
<p>Usually, some formulae initially contain only variables (e.g., axioms asserted corresponding to general rules), leading to neurons with no groundings initially. These neurons must obtain their groundings from their ground operands at initialization or during inference. During inference, each ground neuron propagates its groundings to other neurons with shared variables participating in the same parent neuron, and all operands collectively propagate their groundings to the parent neuron. Similarly, a parent neuron propagates groundings acquired elsewhere to its operands. This grounding management process ensures that constants are propagated throughout the LNN graph in order to compute bounds for relevant queries. Note, however, that this naive grounding management process will propagate all constants, including those not relevant for the query, unnecessarily increasing computation. More efficient methods are discussed in Section A. 5 below.</p>
<p>Quantifiers can also have variables and groundings if partial quantification is required for only a subset of variables from the underlying operand, although existential quantification is typically performed on a single variable to produce a propositional truth value associated with the quantifier output. For partial quantification the maximum lower bound of groundings from the quantified variable subset is chosen for existential quantification and assigned to a unique grounding consisting of the remainder of the variables, whereas the minimum upper bound is used for universal quantification. For existential partial quantification True groundings for the quantified variable subset form arguments stored under the grounding of the remaining variable subset, so that satisfying groundings can be recalled.</p>
<h2>A. 4 Variable binding</h2>
<p>Variable binding assigns specific constant(s) to variables of neurons, typically as part of an inference task, such as answering a query. A variable could be bound in only a subset of occurrences within a logical formulae, although the procedure for producing groundings for inference would typically propagate the binding to all occurrences. It is thus necessary to retain the variable even if bound, in order to interact with other occurrences of the variable in the logical formula to perform join operations.</p>
<h2>A. 5 First-order logical inference</h2>
<p>Inference at a connective neuron involves upward and downward pass computations of the associated logical connective for a given set of groundings, whereas inference at a quantifier neuron involves a reduction operation and creation of new groundings in the case of partial quantification. A provided grounding may not be available in all participating operands of an inference operation, where a retrieval attempt would then add the previously unavailable grounding to the operand with Unknown truth value under the open-world assumption. If a proof is offered to a neuron for an unavailable grounding, the proof aggregation would also assume maximally loose starting bounds.</p>
<p>The computational and memory considerations for large knowledge bases with many constants should be taken under consideration, where action may be taken to avoid storing of groundings with unknown bounds. However, inference is a principal means by which groundings are propagated through a logical formula to enable theorem proving, although there are cases where storage can be avoided. In particular, negation can be viewed as a pass-through operation where inference is performed</p>
<p>instead on the underlying operand or descendent that is not also a negation. Otherwise, if naively approached, negation may have to populate a grounding list of all False or missing groundings from the underlying operand and store these as True under the closed-world assumption.
An inference context involves input operands and an output operation, where input operands are used in the upward inference pass to calculate a proof for the output, or where all but one input operand and the output are used in the downward inference pass to calculate a proof for the remaining input. If any participant in the inference context has a grounding that is not Unknown, then in real-valued logic it is possible in an inference context to derive a truth value that is also not Unknown. Each participant in the proof generation can thus add its groundings to the set of inference groundings. However, for complex reasoning problems that require long chains of reasoning it may be useful to also add groundings with Unknown states and propagate them in hopes that they fetch proofs from other neurons in the graph. These proofs can then be passed to other neurons in the graph to facilitate theorem proving. This introduces a trade-off between efficient computation, through avoiding propagating many constants, and handling complex reasoning problems, by allowing more constants propagation.
A given inference grounding is used as is for other participant operands with the same variable configuration as the originating operand. In case of disjoint variable(s) not present in the inference grounding, the overlapping variables are first searched for a match with all the disjoint variable values used in conjunction to create an expanded set of inference groundings. If no overlapping variables are present or no match is found, then the overlapping variables could be assigned according to the inference grounding, with the disjoint variable(s) covering its set of all observed combinations.
The set of relevant groundings from a real-valued inference context could become a significant expanded set, especially in the presence of disjoint variables. However, guided inference could be used to expand a minimal inference grounding set that only involves groundings relevant to a target proof, and manage the trade-off between computation and reasoning capacity. LNN can use a combination of goal-driven reasoning and data-driven reasoning ${ }^{3}$ to obtain a target proof. A backward-chaining-like algorithm is used here as a means of propagating groundings in search of known truth values that can then be used in a forward-chaining-like computation to infer the goal. If-then conditional rules typically require backward inference in the form of modus tollens to propagate groundings to the antecedent and modus ponens in the forward direction to help calculate the target proof at the consequent. This bidirectional chaining process continues until the target grounding at the consequent is not unknown or until inference does not produce proofs that are any tighter.
In summary, overall computation is characterized similar to the propositional case, with a few minor modifications:</p>
<ol>
<li>Initialize neurons corresponding to predicates and formula roots with starting truth value bounds for given groundings. Usually, all formulae are initialized as True for all associated groundings. Ground atomic neurons with starting bounds represent input data.</li>
<li>For each formula, evaluate neurons in the forward direction in a pass from leaves to root, storing computed bounds at each node for all groundings, and propagating constants to other nodes in the same formula. Then, backtrack to each leaf using inverse computations to update subformula bounds based on stored bounds (and formula initial bounds), and propagate constants potentially fetched from other neurons in the graph.</li>
<li>Aggregate the tightest bounds computed at leaves for each proposition.</li>
<li>Return to step 2 until bounds for all groundings converge. Oscillation cannot occur because bounds tightening is monotonic.</li>
<li>Inspect computed bounds at specific predicates or formulae, i.e. those representing the predictions of the model.
<sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<h1>A. 6 Acceleration</h1>
<p>As bounds tightening is monotonic, the order of evaluation does not change the final result. As a result, and in line with traditional theorem provers, computation may be subject to significant acceleration depending on the order that bounds are updated.
In order for such aggregate operations to be tractable, it is necessary to limit the number of key values that participate in computation, leaving other key value combinations in a sparse state, i.e. with default bounds. We achieve this by filtering predicates whenever possible to include only content pertaining to specific key values referenced in queries or involved in joins with other tables, prioritizing computation towards smaller such content. Because many truth values remain uncomputed in this model, the results of quantifiers and other reductions may not be tight, but they are nonetheless sound. In cases where predicates have known truth values for all key values (i.e. because they make the closed world assumption), we use different bounds for their sparse value and for the sparse values of connectives involving them, such that a connective's sparse value is its result for its inputs sparse values.</p>
<p>Even minimizing the number of key values participating in computation, it is necessary to guide neural evaluation towards rules that are more likely to produce useful results. A first opportunity to this effect is to shortcut computation if it fails to yield tighter bounds than were previously stored at a given neuron. In addition, we exploit the neural graph structure to prioritize evaluation in rules with shorter paths to the query and to visited rules with recently updated bounds.</p>
<p>Tensorization offers another route to accelerate computation, by formulating LNN in terms of weighted adjacency matrices and keeping truth values in multi-dimensional arrays indexed by node and grounding identifiers. The resulting truth value tensor can be sparse so that only entries of existing groundings are stored, and a further batch dimension corresponding to different universes and truth value initializations is also possible. A set of weighted adjacency matrices can represent the neural graph structure, with one adjacency matrix for each of the different operators, including conjunction, disjunction, and implication. The neuron weighting scheme can also extend to admit negative weights used such that corresponding inputs are logically negated.</p>
<h2>B Examples of weighted real-valued logics</h2>
<p>Weighted nonlinear logic is in fact only one family of possible logics implemented in LNN. Notably, the logic introduced in Section 5 for Theorem 2 is a generalization of weighted nonlinear logic in that its lower and upper bounds are computed by different functions. Another important family of logics satisfying the requirements for LNN are the t-norm logics, which we define presently.
Triangular norms, or $t$-norms, and their related t-conorms and residua are natural choices for LNN activation functions as they already behave correctly for classical inputs and have well known inference properties. Logics defined in terms of such function are denoted $t$-norm logics. Common examples of these include</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Gödel</th>
<th style="text-align: left;">Product</th>
<th style="text-align: left;">Łukasiewicz</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">T-norm</td>
<td style="text-align: left;">$\min {x, y}$</td>
<td style="text-align: left;">$x \cdot y$</td>
<td style="text-align: left;">$\max {0, x+y-1}$</td>
</tr>
<tr>
<td style="text-align: left;">T-conorm</td>
<td style="text-align: left;">$\max {x, y}$</td>
<td style="text-align: left;">$x+y-x \cdot y$</td>
<td style="text-align: left;">$\min {1, x+y}$</td>
</tr>
<tr>
<td style="text-align: left;">Residuum</td>
<td style="text-align: left;">$y$ if $x&gt;y$, else 1</td>
<td style="text-align: left;">$\frac{y}{x}$ if $x&gt;y$, else 1</td>
<td style="text-align: left;">$\min {1,1-x+y}$</td>
</tr>
</tbody>
</table>
<p>Of these, only Łukasiewicz logic offers the familiar $(x \rightarrow y)=(\neg x \oplus y)$ identity, while only Gödel logic offers the $(x \otimes x)=(x \oplus x)=x$ identities.</p>
<h2>B. 1 Weighted Łukasiewicz logic</h2>
<p>Weighted Łukasiewicz logic is exactly weighted nonlinear logic with $f(x)=\max {0, \min {1, x}}$. The binary and $n$-ary weighted Łukasiewicz $t$-norms, used for logical AND, are given</p>
<p>$$
\begin{aligned}
\beta\left(x_{1}^{\otimes w_{1}} \otimes x_{2}^{\otimes w_{2}}\right) &amp; =\max \left{0, \min \left{1, \beta-w_{1}\left(1-x_{1}\right)+w_{2}\left(1-x_{2}\right)\right}\right} \
\beta\left(\bigotimes_{i \in I} x_{i}^{\otimes w_{i}}\right) &amp; =\max \left{0, \min \left{1, \beta-\sum_{i \in I} w_{i}\left(1-x_{i}\right)\right}\right}
\end{aligned}
$$</p>
<p>for input set $I$, nonnegative bias term $\beta$, nonnegative weights $w_{i}$, and inputs $x_{i}$ in the $[0,1]$ range. By the De Morgan laws, the binary and $n$-ary weighted Łukasiewicz t-conorms, used for logical OR, are then</p>
<p>$$
\begin{aligned}
^{\beta}\left(x_{1}^{\oplus w_{1}} \oplus x_{2}^{\oplus w_{2}}\right) &amp; =\max \left{0, \min \left{1,1-\beta+w_{1} x_{1}+w_{2} x_{2}\right}\right} \
{ }^{\beta}\left(\bigoplus_{i \in I} x_{i}^{\oplus w_{i}}\right) &amp; =\max \left{0, \min \left{1,1-\beta+\sum_{i \in I} w_{i} x_{i}\right}\right}
\end{aligned}
$$</p>
<p>In either case, the unweighted Łukasiewicz norms are obtained when all $w_{i}=\beta=1$; if any of these parameters are omitted, their presumed value is 1 . The exponent notation is chosen because, for integer weights $k$, this form of weighting is equivalent to repeating the associated term $k$ times using the respective unweighted norm, e.g. $x^{\oplus 3}=(x \oplus x \oplus x)$. Bias term $\beta$ is written as a leading exponent to permit inline ternary and higher arity-norms, for example ${ }^{\beta}\left(x_{1}^{\oplus w_{1}} \oplus x_{2}^{\oplus w_{2}} \oplus x_{3}^{\oplus w_{3}}\right)$, which require only a single bias term to be fully parameterized.
The weighted Łukasiewicz residuum, used for logical implication, solves</p>
<p>$$
\max \left{z: y \geq{ }^{\beta / w_{y}}\left(x^{\otimes w_{x} / w_{y}} \otimes z^{\otimes 1 / w_{y}}\right)\right}
$$</p>
<p>and is given</p>
<p>$$
\begin{aligned}
{ }^{\beta}\left(x^{\otimes w_{x}} \rightarrow y^{\oplus w_{y}}\right) &amp; =\max \left{0, \min \left{1,1-\beta+w_{x}(1-x)+w_{y} y\right}\right} \
&amp; =^{\beta}\left((1-x)^{\oplus w_{x}} \oplus y^{\oplus w_{y}}\right)
\end{aligned}
$$</p>
<p>As for weighted nonlinear logic, note the use of $\otimes$ in the antecedent weight but $\oplus$ in the consequent weight, meant to indicate the antecedent has AND-like weighting (scaling its distance from 1) while the consequent has OR-like weighting (scaling its distance from 0 ). Similarly, this residuum is most disjunction-like when $\beta=1$, most $(x \rightarrow y)$-like when $\beta=w_{y}$, and most $(\neg y \rightarrow \neg x)$-like when $\beta=w_{x}$; that is to say, $\beta=w_{y}$ yields exactly the residuum of $x^{\otimes w_{x} / w_{y}} \otimes z^{\otimes 1 / w_{y}}$ (with no specified bias term of its own), while $\beta=w_{x}$ yields exactly the residuum of $(\neg y)^{\otimes w_{y} / w_{x}} \otimes z^{\otimes 1 / w_{x}}$.
The Łukasiewicz norms are commutative if one permutes weights $w_{i}$ along with inputs $x_{i}$, and are associative if bias term $\beta \leq \min \left{1, w_{i}: i \in I\right}$. Further, they return classical results, i.e. results in the set ${0,1}$, for classical inputs under the condition that $1 \leq \beta \leq \min \left{w_{i}: i \in I\right}$. This clearly requires $\beta=1$ to obtain both associative and classical behavior, though neither is a requirement for LNN. Indeed, constraining $\beta \leq w_{i}$ is problematic if we would like $w_{i}$ to be able to goes to 0 , effectively removing $i$ from input set $I$, whereupon the constraint should no longer apply. Section 6 presents a means of relaxing such constraints so as to facilitate learning.</p>
<h1>B. 2 Weighted Gödel logic</h1>
<p>Gödel logic uses min and max for its t-norm and t-conorm, respectively. It is difficult in general to define weighted versions of these functions-for example, forms like $x^{\oplus 3}=(x \oplus x \oplus x)$ are not meaningful due to min and max's idempotence-though several well-studied options exist, notably Fagin's method [7]. This document, however, uses a technique borrowed from [11] to derive a weighted min from another t-norm, specifically the weighted Łukasiewicz t-norm. Hájek shows that, for any continuous t-norm, one may define weak conjunction</p>
<p>$$
(x \wedge y)=(x \otimes(x \rightarrow y))=\min {x, y}
$$</p>
<p>Using weighted Łukasiewicz logic and a specifically crafted configuration of weights ${ }^{4}$</p>
<p>$$
\left({ }^{\beta_{\kappa}}\left(x^{\otimes w_{x}}\right) \wedge{ }^{\beta_{y}}\left(y^{\otimes w_{y}}\right)\right)={ }^{\beta_{x}}\left(x^{\otimes w_{x}} \otimes\left(x^{\otimes w_{x} / \kappa} \rightarrow y^{\oplus w_{y} / \kappa}\right)^{\otimes \kappa}\right)
$$</p>
<p>for $\kappa=\beta_{x}-\beta_{y}+w_{y}$, one obtains binary and n-ary weighted Gödel t-norms, defined</p>
<p>$$
\begin{aligned}
\left({ }^{\beta_{1}}\left(x_{1}^{\otimes w_{1}}\right) \wedge{ }^{\beta_{2}}\left(x_{2}^{\otimes w_{2}}\right)\right) &amp; =\max \left{0, \min \left{1, \beta_{1}-w_{1}\left(1-x_{1}\right), \beta_{2}-w_{2}\left(1-x_{2}\right)\right}\right} \
\left(\bigwedge_{i \in I}{ }^{\beta_{i}}\left(x_{i}^{\otimes w_{i}}\right)\right) &amp; =\max \left{0, \min \left{1, \beta_{i}-w_{i}\left(1-x_{i}\right): i \in I\right}\right}
\end{aligned}
$$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>which is just the min of the unary weighted Łukasiewicz t-norm applied to each argument, and which, if all $\beta_{i}=\beta$, happens to be very similar to ((Sung 1998)). Again using the De Morgan laws, the related binary and n-ary weighted Gödel t-conorms are then</p>
<p>$$
\begin{aligned}
\left(\beta_{1}\left(x_{1}^{\otimes w_{1}}\right) \vee \beta_{2}\left(x_{2}^{\otimes w_{2}}\right)\right) &amp; =\min \left{1, \max \left{0,1-\beta_{1}+w_{1} x_{1}, 1-\beta_{2}+w_{2} x_{2}\right}\right} \
\left(\bigvee_{i \in I} \beta_{i}\left(x_{i}^{\otimes w_{i}}\right)\right) &amp; =\min \left{1, \max \left{0,1-\beta_{i}+w_{i} x_{i}: i \in I\right}\right}
\end{aligned}
$$</p>
<p>The weighted Gödel residuum now solves</p>
<p>$$
\max \left{z: y \geq \beta_{x y} / w_{y}\left(x^{\otimes w_{x} / w_{y}}\right) \wedge \beta_{y} / w_{y}\left(z^{\otimes 1 / w_{y}}\right)\right}
$$</p>
<p>for $\beta_{x y}=\max \left{0, \beta_{x}+\beta_{y}-1\right}$ and is given</p>
<p>$$
\left(\beta_{x}\left(x^{\otimes w_{x}}\right) \Rightarrow \beta_{y}\left(y^{\oplus w_{y}}\right)\right)=\beta_{y}\left(y^{\oplus w_{y}}\right) \text { if } \beta_{x}\left(x^{\otimes w_{x}}\right)&gt;\beta_{y}\left(y^{\oplus w_{y}}\right) \text {, else } 1
$$</p>
<p>where operands are again unary weighted Łukasiewicz norms, or specifically</p>
<p>$$
\begin{aligned}
&amp; \beta_{x}\left(x^{\otimes w_{x}}\right)=\max \left{0, \min \left{1, \beta_{x}-w_{x}(1-x)\right}\right} \
&amp; \beta_{y}\left(y^{\oplus w_{y}}\right)=\max \left{0, \min \left{1,1-\beta_{y}+w_{y} y\right}\right}
\end{aligned}
$$</p>
<p>The weighted Gödel norms are commutative if one permutes both weights $w_{i}$ and biases $\beta_{i}$ along with inputs $x_{i}$ and, similar to the weighted Łukasiewicz norms, are associative if $\beta_{i} \leq \min \left{1, w_{i}\right}$ for each $i \in I$. Likewise, they behave classically for classical input if $1 \leq \beta_{i} \leq w_{i}$ for each $i \in I$.</p>
<h1>B. 3 Parameter semantics</h1>
<p>Weights $w_{i}$ need not sum to 1 ; accordingly, they are best interpreted as absolute importance as opposed to relative importance. As mentioned above, for conjunctions, increased weight amplifies the respective input's distance from 1, while for disjunctions, increased weight amplifies the respective input's distance from 0 . Decreased weight has the opposite effect, to the point that inputs with zero weight have no effect on the result at all.</p>
<p>Bias term $\beta$ is best interpreted as continuously varying the "difficulty" of satisfying the operation. In weighted Łukasiewicz logic, this can so much as translate from one logical connective to another, e.g. from logical AND to logical OR. Constraints imposed on $\beta$ and $w_{i}$ can guarantee that the operation performed at each neuron matches the corresponding connective in the represented formula, e.g., when inputs are assumed to be within a given distance of 1 or 0 , as further discussed in Section 6.</p>
<h2>C The Upward-Downward algorithm (continued)</h2>
<p>This section supplements its counterpart in the main paper on page 5, and provides more complete algorithm descriptions. Inference tasks in LNN involve generating proofs of truth values for specified output nodes, given truth values at specified input nodes. Note that a node could both be an input and an output node so that initial proofs can be provided and updated through inference to obtain an aggregate proof as output for the same node. Proof generation operates over a system of formulae that is represented as a mapping of the corresponding syntax tree to a directed graph. An upward pass described in Algorithm 1 calculates a proof at a formula using values from its input terms and subformulae, which corresponds to normal forward computation for neurons. A downward pass detailed in Algorithm 2 proves truth values for each input operand based on the other operand truth values and the enclosing formula's truth value.
Inference propagates information through edges of the formula syntax tree until convergence where no new proofs are generated, as shown in Algorithm 3. Information flow in connected components can be optimized through sequential traversal over adjacent nodes such that inference has access to the newest proofs recently calculated for neighboring formulae. Preorder and postorder traversals avoid cycles within an inference epoch so that only a single upward or downward calculation is performed for a node within an epoch. Sequential traversal with upward passes starts from propositions, known truth values, or syntax tree leaves, while downward passes move information from outer formulae to inner terms.</p>
<p>LNN converts to a directed acyclic graph through cycle-avoidance, and over alternating upward and downward passes the graph is unrolled according to the traversal sequence to resemble a finite impulse</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">4</span><span class="p">:</span><span class="w"> </span><span class="nx">Upward</span><span class="w"> </span><span class="nx">pass</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">infer</span><span class="w"> </span><span class="nx">formula</span><span class="w"> </span><span class="nx">truth</span><span class="w"> </span><span class="nx">value</span><span class="w"> </span><span class="nx">bounds</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">subformulae</span><span class="w"> </span><span class="nx">bounds</span><span class="p">.</span>
<span class="nx">function</span><span class="w"> </span><span class="nx">upwardPass</span><span class="p">(</span><span class="nx">formula</span><span class="w"> </span><span class="nx">z</span><span class="p">):</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">recursive</span><span class="w"> </span><span class="nx">upward</span><span class="w"> </span><span class="nx">pass</span><span class="w"> </span><span class="nx">inference</span><span class="w"> </span><span class="nx">function</span>
<span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">z</span><span class="p">=</span><span class="nx">P</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">v_</span><span class="p">{</span><span class="mi">0</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="nx">v_</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\#</span><span class="w"> </span><span class="nx">z</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">atom</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">n</span><span class="err">\</span><span class="p">)</span><span class="o">-</span><span class="nx">ary</span><span class="w"> </span><span class="nx">predicate</span>
<span class="k">if</span><span class="w"> </span><span class="nx">uninitialized</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="nx">z</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">uninitialized</span><span class="w"> </span><span class="nx">truth</span><span class="w"> </span><span class="nx">value</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">L_</span><span class="p">{</span><span class="nx">z</span><span class="p">},</span><span class="w"> </span><span class="nx">U_</span><span class="p">{</span><span class="nx">z</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">):=(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">L_</span><span class="p">{</span><span class="nx">z</span><span class="p">},</span><span class="w"> </span><span class="nx">U_</span><span class="p">{</span><span class="nx">z</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">unknown</span><span class="w"> </span><span class="nx">under</span><span class="w"> </span><span class="nx">open</span><span class="o">-</span><span class="nx">world</span><span class="w"> </span><span class="nx">assumption</span>
<span class="w">    </span><span class="err">\#</span><span class="w"> </span><span class="nx">existing</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="nx">truth</span><span class="w"> </span><span class="nx">value</span><span class="w"> </span><span class="nx">bounds</span>
<span class="k">for</span><span class="w"> </span><span class="nx">subformula</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">I</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">every</span><span class="w"> </span><span class="nx">input</span><span class="w"> </span><span class="nx">operand</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">z</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">L_</span><span class="p">{</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">},</span><span class="w"> </span><span class="nx">U_</span><span class="p">{</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">):=</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">upwardPass</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">obtain</span><span class="w"> </span><span class="nx">operand</span><span class="w"> </span><span class="nx">bounds</span><span class="p">,</span><span class="w"> </span><span class="nx">recurses</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">leaves</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">L_</span><span class="p">{</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}},</span><span class="w"> </span><span class="nx">U_</span><span class="p">{</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">):=</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">aggregate</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">L_</span><span class="p">{</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">},</span><span class="w"> </span><span class="nx">U_</span><span class="p">{</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">aggregate</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="nx">proof</span>
<span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">z</span><span class="p">=</span><span class="err">\</span><span class="nx">neg</span><span class="w"> </span><span class="nx">x</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">z</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">performs</span><span class="w"> </span><span class="nx">negation</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="nx">U_</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="w"> </span><span class="mi">1</span><span class="o">-</span><span class="nx">L_</span><span class="p">{</span><span class="nx">x</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">single</span><span class="w"> </span><span class="nx">input</span><span class="w"> </span><span class="nx">operand</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">x</span><span class="err">\</span><span class="p">)</span>
<span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">z</span><span class="p">={</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">beta</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">otimes</span><span class="w"> </span><span class="nx">w_</span><span class="p">{</span><span class="nx">x</span><span class="p">}}</span><span class="w"> </span><span class="err">\</span><span class="nx">rightarrow</span><span class="w"> </span><span class="nx">y</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">otimes</span><span class="w"> </span><span class="nx">w_</span><span class="p">{</span><span class="nx">y</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">z</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">performs</span><span class="w"> </span><span class="nx">implication</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">({</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">beta</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">U_</span><span class="p">{</span><span class="nx">x</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">otimes</span><span class="w"> </span><span class="nx">w_</span><span class="p">{</span><span class="nx">x</span><span class="p">}}</span><span class="w"> </span><span class="err">\</span><span class="nx">rightarrow</span><span class="w"> </span><span class="nx">L_</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">otimes</span><span class="w"> </span><span class="nx">w_</span><span class="p">{</span><span class="nx">y</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">),{</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">beta</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">L_</span><span class="p">{</span><span class="nx">x</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">otimes</span><span class="w"> </span><span class="nx">w_</span><span class="p">{</span><span class="nx">x</span><span class="p">}}</span><span class="w"> </span><span class="err">\</span><span class="nx">rightarrow</span><span class="w"> </span><span class="nx">U_</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">otimes</span><span class="w"> </span><span class="nx">w_</span><span class="p">{</span><span class="nx">y</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">two</span><span class="w"> </span><span class="nx">input</span><span class="w"> </span><span class="nx">operands</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">y</span><span class="err">\</span><span class="p">)</span>
<span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">z</span><span class="p">={</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">beta</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">bigotimes_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">I</span><span class="p">}</span><span class="w"> </span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">otimes</span><span class="w"> </span><span class="nx">w_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">z</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">performs</span><span class="w"> </span><span class="nx">conjunction</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">({</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">beta</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">bigotimes_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">I</span><span class="p">}</span><span class="w"> </span><span class="nx">L_</span><span class="p">{</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">otimes</span><span class="w"> </span><span class="nx">w_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">),{</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">beta</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">bigotimes_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">I</span><span class="p">}</span><span class="w"> </span><span class="nx">U_</span><span class="p">{</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">otimes</span><span class="w"> </span><span class="nx">w_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">multi</span><span class="o">-</span><span class="nx">input</span><span class="w"> </span><span class="nx">conjunction</span>
<span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">z</span><span class="p">={</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">beta</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">bigoplus_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">I</span><span class="p">}</span><span class="w"> </span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">otimes</span><span class="w"> </span><span class="nx">w_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">z</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">performs</span><span class="w"> </span><span class="nx">disjunction</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">({</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">beta</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">bigoplus_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">I</span><span class="p">}</span><span class="w"> </span><span class="nx">L_</span><span class="p">{</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">otimes</span><span class="w"> </span><span class="nx">w_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">),{</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">beta</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">bigoplus_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">I</span><span class="p">}</span><span class="w"> </span><span class="nx">U_</span><span class="p">{</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">otimes</span><span class="w"> </span><span class="nx">w_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">multi</span><span class="o">-</span><span class="nx">input</span><span class="w"> </span><span class="nx">disjunction</span>
<span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">z</span><span class="p">=</span><span class="err">\</span><span class="nx">forall_</span><span class="p">{</span><span class="nx">g</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">G</span><span class="p">}</span><span class="w"> </span><span class="nx">x</span><span class="p">(</span><span class="nx">g</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">z</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">universal</span><span class="w"> </span><span class="nx">quantifier</span><span class="w"> </span><span class="nx">over</span><span class="w"> </span><span class="nx">groundings</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">G</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">min</span><span class="w"> </span><span class="nx">_</span><span class="p">{</span><span class="nx">g</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">G</span><span class="p">}</span><span class="w"> </span><span class="nx">L_</span><span class="p">{</span><span class="nx">x</span><span class="p">(</span><span class="nx">g</span><span class="p">)},</span><span class="w"> </span><span class="err">\</span><span class="nx">min</span><span class="w"> </span><span class="nx">_</span><span class="p">{</span><span class="nx">g</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">G</span><span class="p">}</span><span class="w"> </span><span class="nx">U_</span><span class="p">{</span><span class="nx">x</span><span class="p">(</span><span class="nx">g</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">universal</span><span class="w"> </span><span class="nx">quantification</span><span class="w"> </span><span class="nx">over</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">G</span><span class="err">\</span><span class="p">)</span>
<span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">z</span><span class="p">=</span><span class="err">\</span><span class="nx">exists_</span><span class="p">{</span><span class="nx">g</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">G</span><span class="p">}</span><span class="w"> </span><span class="nx">x</span><span class="p">(</span><span class="nx">g</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">z</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">existential</span><span class="w"> </span><span class="nx">quantifier</span><span class="w"> </span><span class="nx">over</span><span class="w"> </span><span class="nx">groundings</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">G</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">max</span><span class="w"> </span><span class="nx">_</span><span class="p">{</span><span class="nx">g</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">G</span><span class="p">}</span><span class="w"> </span><span class="nx">L_</span><span class="p">{</span><span class="nx">x</span><span class="p">(</span><span class="nx">g</span><span class="p">)},</span><span class="w"> </span><span class="err">\</span><span class="nx">max</span><span class="w"> </span><span class="nx">_</span><span class="p">{</span><span class="nx">g</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">G</span><span class="p">}</span><span class="w"> </span><span class="nx">U_</span><span class="p">{</span><span class="nx">x</span><span class="p">(</span><span class="nx">g</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">existential</span><span class="w"> </span><span class="nx">quantification</span><span class="w"> </span><span class="nx">over</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">G</span><span class="err">\</span><span class="p">)</span>
<span class="nx">function</span><span class="w"> </span><span class="nx">aggregate</span><span class="p">(</span><span class="nx">formula</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">z</span><span class="p">,</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">L_</span><span class="p">{</span><span class="nx">z</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">},</span><span class="w"> </span><span class="nx">U_</span><span class="p">{</span><span class="nx">z</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">):</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">aggregate</span><span class="w"> </span><span class="nx">offered</span><span class="w"> </span><span class="nx">proof</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">L_</span><span class="p">{</span><span class="nx">z</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">},</span><span class="w"> </span><span class="nx">U_</span><span class="p">{</span><span class="nx">z</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">z</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">max</span><span class="w"> </span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">L_</span><span class="p">{</span><span class="nx">z</span><span class="p">},</span><span class="w"> </span><span class="nx">L_</span><span class="p">{</span><span class="nx">z</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="nx">min</span><span class="w"> </span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">U_</span><span class="p">{</span><span class="nx">z</span><span class="p">},</span><span class="w"> </span><span class="nx">U_</span><span class="p">{</span><span class="nx">z</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">monotonically</span><span class="w"> </span><span class="nx">tighten</span><span class="w"> </span><span class="nx">existing</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">L_</span><span class="p">{</span><span class="nx">z</span><span class="p">},</span><span class="w"> </span><span class="nx">U_</span><span class="p">{</span><span class="nx">z</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">bounds</span>
</code></pre></div>

<p>recurrent network. However, given the monotonic tightening of proof aggregation, the network is more constrained than the dynamic behavior exhibited by recurrent neural networks. Downward traversal can mirror the upward pass, although transient directional edges are effectively introduced by downward inference so separate proofs are offered to each neuron input according to the functional inverse calculation with analytically transformed weights.
Given both upward and downward computations for each connective, overall computation is characterized as follows:</p>
<ol>
<li>Initialize neurons corresponding to propositions and formula roots with starting truth value bounds. Usually, all formulae are initialized as true. Propositions with starting bounds represent input data.</li>
<li>For each formula, evaluate neurons in the forward direction in a pass from leaves to root, storing computed bounds at each node. Then, backtrack to each leaf using inverse computations to update subformula bounds based on stored bounds (and formula initial bounds).</li>
<li>Aggregate the tightest bounds computed at leaves for each proposition.</li>
<li>Return to step 2 until bounds converge. Oscillation cannot occur because bounds tightening is monotonic.</li>
<li>Inspect computed bounds at specific propositions or formulae, i.e. those representing the predictions of the model.</li>
</ol>
<p>As suggested in step 3, one may simply use min and max to aggregate upper and lower bounds proved for each proposition, though smoothed versions of these may be preferred to spread gradient information over multiple proofs. Alternately, when targeting classical logic, one can use conjunction and disjunction (themselves possibly smoothed) to aggregate proposition bounds. When doing so, there is an opportunity to reuse propositions' weights from their respective proofs, so as to limit the effect of proofs in which the proposition only plays a minor role.
As suggested in step 5, prediction results are obtained by inspecting the outputs of one or more neurons, similar to what would be done for a conventional neural network. Different, however, is the fact that different neurons may serve as inputs and results for different queries, indeed with a</p>
<p>```
function downwardPass(formula z): # recursive downward pass inference function
for subformula (x_{j \in I}) do # every input operand (x_{j}) of (z)
    if (z=\neg x) then # (z) performs negation
        (\left(L_{x}^{\prime}, U_{x}^{\prime}\right):=\left(1-U_{z}, 1-L_{z}\right) \quad) # proof for single input operand (x)
    else if (z={ }^{\beta}\left(x^{\otimes w_{x}} \rightarrow y^{\otimes w_{y}}\right)) then # (z) performs implication
        (L_{x}^{\prime}:= \begin{cases}\left.\quad{ }^{\beta / w_{x}}\left(L_{z}^{\otimes 1 / w_{x}} \rightarrow L_{y}^{\otimes w_{y} / w_{x}}\right)\right) &amp; \text { if } U_{z}&lt;1, \ 0 &amp; \text { otherwise. } \ U_{x}^{\prime}:= \begin{cases}\left.\quad{ }^{\beta / w_{x}}\left(L_{z}^{\otimes 1 / w_{x}} \rightarrow U_{y}^{\otimes w_{y} / w_{x}}\right)\right) &amp; \text { if } L_{z}&gt;0, \ 1 &amp; \text { otherwise } \ L_{y}^{\prime}:= \begin{cases}\left.\quad{ }^{\beta / w_{y}}\left(L_{x}^{\otimes w_{x} / w_{y}} \otimes L_{z}^{\otimes 1 / w_{y}}\right)\right) &amp; \text { if } L_{z}&gt;0, \ 0 &amp; \text { otherwise } \ U_{y}^{\prime}:= \begin{cases}\left.\quad{ }^{\beta / w_{y}}\left(U_{x}^{\otimes w_{x} / w_{y}} \otimes U_{z}^{\otimes 1 / w_{y}}\right)\right) &amp; \text { if } U_{z}&lt;1, \ 1 &amp; \text { otherwise }\end{cases} \ #) right input (y) lower bound
    else if (z={ }^{\beta}\left(\bigotimes_{i \in I} x_{i}^{\otimes w_{i}}\right)) then # right input (y) upper bound
    (L_{x_{j}}^{\prime}:= \begin{cases}\left.\quad{ }^{\beta / w_{j}}\left(\left(\bigotimes_{i \neq j} U_{x_{i}}^{\otimes w_{i} / w_{j}}\right) \rightarrow L_{z}^{\otimes 1 / w_{j}}\right) &amp; \text { if } L_{z}&gt;0, \ 0 &amp; \text { otherwise, } \ U_{x_{j}}^{\prime}:= \begin{cases}\left.\quad{ }^{\beta / w_{j}}\left(\left(\bigotimes_{i \neq j} L_{x_{i}}^{\otimes w_{i} / w_{j}}\right) \rightarrow U_{z}^{\otimes 1 / w_{j}}\right) &amp; \text { if } U_{z}&lt;1, \ 1 &amp; \text { otherwise, }\end{cases} \ #) upper bound proof
    else if (z={ }^{\beta}\left(\bigotimes_{i \in I} x_{i}^{\otimes w_{i}}\right)) then # (z) performs disjunction
        (L_{x_{j}}^{\prime}:= \begin{cases}\left.\quad{ }^{\beta / w_{j}}\left(\left(\bigotimes_{i \neq j}\left(\neg U_{x_{i}}\right)^{\otimes w_{i} / w_{j}}\right) \otimes L_{z}^{\otimes 1 / w_{j}}\right) &amp; \text { if } L_{z}&gt;0, \ 0 &amp; \text { otherwise, } \ U_{x_{j}}^{\prime}:= \begin{cases}\left.\quad{ }^{\beta / w_{j}}\left(\left(\bigotimes_{i \neq j}\left(\neg L_{x_{i}}\right)^{\otimes w_{i} / w_{j}}\right) \otimes U_{z}^{\otimes 1 / w_{j}}\right) &amp; \text { if } U_{z}&lt;1, \ 1 &amp; \text { otherwise. }\end{cases} \ #) upper bound
        else if (z=\forall_{g \in G} x(g)) then # (z) is universal quantifier over groundings (G)
            for (g \in G) do # every (n)-ary grounding tuple (g)
            (\left(L_{x(g)}^{\prime}, U_{x(g)}^{\prime}\right):=\left(L_{z}, 1\right) \quad) # proof of minimum lower bound
    else if (z=\exists_{g \in G} x(g)) then # (z) is existential quantifier over groundings (G)
            for (g \in G) do # every (n)-ary grounding tuple (g)
            (\left(L_{x}^{\prime}(g), U_{x}^{\prime}(g)\right):=\left(0, U_{z}\right) \quad) # proof of maximum upper bound
            (\left(L_{x_{i}}, U_{x_{i}}\right):=\operatorname{aggregate}\left(x_{i},\left(L_{x_{i}}^{\prime}, U_{x_{i}}^{\prime}\right)\right) \quad) # aggregate new proof
            downwardPass (\left(x_{i}\right) \quad) # recurses to leaves, stop before cyclic loop
Algorithm 6: Recurrent inference procedure with recursive directional graph traversal.
function inference(formulae z):
while (\sum\left(\left|\delta L_{z}\right|+\left|\delta U_{z}\right|\right)&gt;\epsilon) do
for (r \in \operatorname{roots}(\mathbf{z})) do
    (\left(L_{r}^{\prime}, U_{r}^{\prime}\right):=\operatorname{upwardPass}(r))
    (\left(L_{r}, U_{r}\right):=\operatorname{aggregate}\left(r,\left(L_{r}^{\prime}, U_{r}^{\prime}\right)\right))
    downwardPass ((r))
# iterative function with input formulae
# new proofs being generated
# root nodes of system of formulae
# leaves-to-root traversal
# root aggregates new proof
# root-to-leaf traversal</p>
<p>result for one query possibly used as an input for another. In addition, one may arbitrarily extend an existing LNN model with neurons representing new formulae to serve as a novel query.</p>
<h1>C. 1 Proof of Theorem 1</h1>
<p>We shall now proof Theorem 1, or specifically that, given weighted nonlinear logic with monotonic $\neg, \oplus$, and $f$, Algorithm 3 converges to within $\epsilon$ in finite time for the propositional case.</p>
<p>Proof. All operations in weighted nonlinear logic are implemented in terms of $\neg, \oplus$, and $f$ and are thus also monotonic functions of their inputs. Truth value bounds aggregation is likewise monotonic, always taking the tightest available bounds, as per Algorithms 1, 2, and 3. Because lower bounds can only increase, but have a maximum value of 1 , and upper bounds can only decrease, but have a minimum value of 0 , each sequence of updates for a given bound must be constitute a Cauchy sequence; otherwise, there would have to exist some step size $\delta&gt;0$ by which some truth value bound updates an infinite number of times, but this would clearly push the bound past its limit of 1 or 0 . Accordingly, after some finite number of iterations of Algorithm 3, all truth value bounds will be within $\frac{1}{n}$ of the end point of their Cauchy sequence. For $n$ total lower and upper bounds, the sum of all such deviations will be at most $\epsilon$.</p>
<p>This proof does not apply to FOL because, unlike the propositional case, FOL can introduce lower and upper bounds at new predicate groundings throughout evaluation. For example, a successor function can produce an infinite number of predicate groundings; while each of these constitutes its own Cauchy sequence and thus necessarily converges independently of the others, the entire system may never converge. This result is expected given the well known undecideable nature of FOL.</p>
<h2>D Proof of Theorem 2</h2>
<p>To prove Theorem 2, let us first give an expanded description of the LNN variant from the first paragraph of Section 5. It is mathematically equivalent to that paragraph, and the expanded boundupdate equations for various logical connectives will facilitate the proof.
Consider a bipartite graph $G=\left\langle V_{1} \cup V_{2}, E\right\rangle$, where each node in $V_{1}$ represents a formula and each node in $V_{2}$ represents a connective. Nodes in $V_{2}$ have the following types: NOT, IMPLIES, AND-2, OR-2, AND-3, OR-3, $\cdots$. A NOT node always has a degree of 2. An IMPLIES node always has a degree of 3. An AND- $k$ or OR- $k$ node always has a degree of $k+1$. For a node $v \in V_{1} \cup V_{2}$, let $d_{v}$ denote its degree and let $n_{v, 1}, \cdots, n_{v, d_{v}}$ denote its neighbors.
Define two functions $L: V_{1} \rightarrow[0,1]$ and $U: V_{1} \rightarrow[0,1]$. Each node $v \in V_{2}$ is annotated with $2 d_{v}$ functions:</p>
<p>$$
\begin{aligned}
\tilde{L}<em 1="1" v_="v,">{L, U, v, 1} &amp; =f</em>\right)\right) \
\tilde{U}}\left(L\left(n_{v, 2}\right), \cdots, L\left(n_{v, d_{v}}\right), U\left(n_{v, 2}\right), \cdots, U\left(n_{v, d_{v}<em 1="1" v_="v,">{L, U, v, 1} &amp; =g</em>\right)\right) \
\tilde{L}}\left(L\left(n_{v, 2}\right), \cdots, L\left(n_{v, d_{v}}\right), U\left(n_{v, 2}\right), \cdots, U\left(n_{v, d_{v}<em 2="2" v_="v,">{L, U, v, 2} &amp; =f</em>\right)\right) \
\tilde{U}}\left(L\left(n_{v, 1}\right), L\left(n_{v, 3}\right), \cdots, L\left(n_{v, d_{v}}\right), U\left(n_{v, 1}\right), U\left(n_{v, 3}\right), \cdots, U\left(n_{v, d_{v}<em 2="2" v_="v,">{L, U, v, 2} &amp; =g</em>\right)\right) \
\cdots &amp; \
\tilde{L}}\left(L\left(n_{v, 1}\right), L\left(n_{v, 3}\right), \cdots, L\left(n_{v, d_{v}}\right), U\left(n_{v, 1}\right), U\left(n_{v, 3}\right), \cdots, U\left(n_{v, d_{v}<em v="v">{L, U, v, d</em>\right)\right) \
\tilde{U}}} &amp; =f_{v, d_{v}}\left(L\left(n_{v, 1}\right), \cdots, L\left(n_{v, d_{v}-1}\right), U\left(n_{v, 1}\right), \cdots, U\left(n_{v, d_{v}-1<em v="v">{L, U, v, d</em>\right)\right)
\end{aligned}
$$}} &amp; =g_{v, d_{v}}\left(L\left(n_{v, 1}\right), \cdots, L\left(n_{v, d_{v}-1}\right), U\left(n_{v, 1}\right), \cdots, U\left(n_{v, d_{v}-1</p>
<p>Without loss of generality, let's assume an indexing scheme for neighbors of $v \in V_{2}$ : if $v$ has type AND- $k, n_{v, k+1}=n_{v, 1} \wedge \cdots \wedge n_{v, d_{k}}$; if $v$ has type OR- $k, n_{v, k+1}=n_{v, 1} \vee \cdots \vee n_{v, k}$; if $v$ has type IMPLIES, $n_{v, 3}=n_{v, 1} \rightarrow n_{v, 2}$.
With different choices of $\tilde{L}$ 's and $\tilde{U}$ 's, LNN can implement different flavors of real-valued logic. The variant in Section 5 uses the following choice of $\tilde{L}$ 's and $\tilde{U}$ 's, grouped by types of node $v \in V_{2}$.</p>
<ul>
<li>NOT</li>
</ul>
<p>$$
\begin{aligned}
&amp; \tilde{L}<em 2="2" v_="v,">{L, U, v, 1}=1-U\left(n</em>\right) \
&amp; \tilde{U}<em 2="2" v_="v,">{L, U, v, 1}=1-L\left(n</em>\right) \
&amp; \tilde{L}<em 1="1" v_="v,">{L, U, v, 2}=1-U\left(n</em>\right) \
&amp; \tilde{U}<em 1="1" v_="v,">{L, U, v, 2}=1-L\left(n</em>\right)
\end{aligned}
$$</p>
<ul>
<li>AND- $k$</li>
</ul>
<p>For $i=1, \cdots, k$ :</p>
<p>$$
\begin{aligned}
\tilde{L}<em k_1="k+1" v_="v,">{L, U, v, i} &amp; =L\left(n</em>\right) \
\tilde{U}<em k_1="k+1" v_="v,">{L, U, v, i} &amp; =\min \left(1, U\left(n</em>\right)\right)\right) \
\tilde{L}}\right)+\sum_{1 \leq j \leq k, j \neq i}\left(1-L\left(n_{v, j<em j="1">{L, U, v, k+1} &amp; =\max \left(0,1-\sum</em>\right)\right)\right) \
\tilde{U}}^{k}\left(1-L\left(n_{v, j<em j="1">{L, U, v, k+1} &amp; =\min </em>\right)
\end{aligned}
$$}^{k} U\left(n_{v, j</p>
<ul>
<li>OR- $k$</li>
</ul>
<p>For $i=1, \cdots, k$ :</p>
<p>$$
\begin{aligned}
\tilde{L}<em k_1="k+1" v_="v,">{L, U, v, i} &amp; =\max \left(0, L\left(n</em>\right)\right) \
\tilde{U}}\right)-\sum_{1 \leq j \leq k, j \neq i} U\left(n_{v, j<em k_1="k+1" v_="v,">{L, U, v, i} &amp; =U\left(n</em>\right) \
\tilde{L}<em j="1">{L, U, v, k+1} &amp; =\max </em>\right) \
\tilde{U}}^{k} L\left(n_{v, j<em j="1">{L, U, v, k+1} &amp; =\min \left(1, \sum</em>\right)\right)
\end{aligned}
$$}^{k} U\left(n_{v, j</p>
<ul>
<li>IMPLIES</li>
</ul>
<p>$$
\begin{aligned}
&amp; \tilde{L}<em 3="3" v_="v,">{L, U, v, 1}=1-U\left(n</em>\right) \
&amp; \tilde{U}<em 2="2" v_="v,">{L, U, v, 1}=\min \left(1,1+U\left(n</em>\right)\right) \
&amp; \tilde{L}}\right)-L\left(n_{v, 3<em 1="1" v_="v,">{L, U, v, 2}=\max \left(0, L\left(n</em>\right)-1\right) \
&amp; \tilde{U}}\right)+L\left(n_{v, 3<em 3="3" v_="v,">{L, U, v, 2}=U\left(n</em>\right) \
&amp; \tilde{L}<em 1="1" v_="v,">{L, U, v, 3}=\max \left(1-U\left(n</em>\right)\right) \
&amp; \tilde{U}}\right), L\left(n_{v, 2<em 1="1" v_="v,">{L, U, v, 3}=\min \left(1,1-L\left(n</em>\right)\right)
\end{aligned}
$$}\right)+U\left(n_{v, 2</p>
<p>With the same notations as in Section 5, initial knowledge of LNN inference is specified by a set of formulas $V_{0} \subseteq V_{1}$ and two functions $L_{0}: V_{0} \rightarrow[0,1]$ and $U_{0}: V_{0} \rightarrow[0,1]$. The query formula $\sigma$ is either an existing node in $V_{1}$ or a formula that is composed of existing nodes in $V_{1}$. Without loss of generality, let's assume $\sigma \in V_{1}$, because otherwise we can first expand $G$ by adding connectives until $\sigma$ is included in $V_{1}$</p>
<p>Consider node $v$ and one of its neighbors $n_{v, i}$, let $j_{v, i}$ denote $v$ 's index among $n_{v, i}$ 's neighbors. In other words, $n_{n_{v, i}, j_{v, i}} \equiv v$.
LNN inference, i.e., Algorithm 3 in the main paper, can be written as the following pseudo code, where the selection of $v$ and $i$ in the loop is determined by the upward and downward passes of Algorithms 1 and 2 in the main paper.</p>
<p>$$
\begin{aligned}
&amp; L(v) \leftarrow L_{0}(v), \forall v \in V_{0} \
&amp; U(v) \leftarrow U_{0}(v), \forall v \in V_{0} \
&amp; L(v) \leftarrow 0, \forall v \in V_{1} \backslash V_{0} \
&amp; U(v) \leftarrow 1, \forall v \in V_{1} \backslash V_{0} \
&amp; \text { do until convergence: } \
&amp; \text { select } v \in V_{1} \text { and index } i \in\left{1, \cdots, d_{v}\right} \
&amp; L(v) \leftarrow \max \left(L(v), \tilde{L}<em i="i" v_="v,">{L, U, n</em> \
&amp; U(v) \leftarrow \min \left(U(v), \tilde{U}}, j_{v, i}}\right), \forall v \in V_{1<em i="i" v_="v,">{L, U, n</em> \
&amp; \text { return } L(\sigma) \text { and } U(\sigma)
\end{aligned}
$$}, j_{v, i}}\right), \forall v \in V_{1</p>
<p>To prove Theorem 2, we will start with following lemma which serves as a steppingstone.
Lemma 1. For any functions $L: V_{1} \rightarrow[0,1]$ and $U: V_{1} \rightarrow[0,1]$, and for any $\phi \in V_{1}$ and any $i \in$ $\left{1, \cdots, d_{\phi}\right}$, define $\Gamma=\left{(v, L(v), U(v)) \mid v \in V_{1}\right}$ and define $\Gamma_{\phi, i}^{\prime}=\left(\Gamma \backslash{(\phi, L(\phi), U(\phi))}\right) \cup$ $\left{\left(\phi, \max \left(L(\phi), \tilde{L}<em _phi_="\phi," i="i">{L, U, n</em>}, j_{\phi, i}}\right), \min \left(U(\phi), \tilde{U<em _phi_="\phi," i="i">{L, U, n</em>\right)\right)\right}$. The following equality holds:}, j_{\phi, i}</p>
<p>$$
P_{\Gamma}=P_{\Gamma_{\phi, i}^{\prime}}
$$</p>
<p>Proof of Lemma 1. $\Gamma$ and $\Gamma_{\phi, i}^{\prime}$ differ by exactly one sentence: the former contains $(\phi, L(\phi), U(\phi))$ while the latter contains $\left(\phi, \max \left(L(\phi), \tilde{L}<em _phi_="\phi," i="i">{L, U, n</em>}, j_{\phi, i}}\right), \min \left(U(\phi), \tilde{U<em _phi_="\phi," i="i">{L, U, n</em>}, j_{\phi, i}}\right)\right)$. Since $L(\phi) \leq \max \left(L(\phi), \tilde{L<em _phi_="\phi," i="i">{L, U, n</em>}, j_{\phi, i}}\right)$ and $U(\phi) \geq \min \left(U(\phi), \tilde{U<em _phi_="\phi," i="i">{L, U, n</em>$ :}, j_{\phi, i}}\right)$, by definition any model of $\Gamma_{\phi, i}^{\prime}$ must be a model of $\Gamma$. In other words, we have $P_{\Gamma_{\phi, i}^{\prime}} \subseteq P_{\Gamma}$. Therefore, in order to prove Lemma 1, we only need to show $P_{\Gamma} \subseteq P_{\Gamma_{\phi, i}^{\prime}}$. It is trivially true if $P_{\Gamma}=\emptyset$, and therefore it suffices to show that $p \in P_{\Gamma_{\phi, i}^{\prime}}$ for any $p \in P_{\Gamma}$. By definition, it is equivalent to show the following two inequalities for any $p \in P_{\Gamma</p>
<p>$$
\begin{aligned}
&amp; p\left(S_{\phi}\right) \leq \tilde{U}<em _phi_="\phi," i="i">{L, U, n</em> \
&amp; p\left(S_{\phi}\right) \geq \tilde{L}}, j_{\phi, i}<em _phi_="\phi," i="i">{L, U, n</em>
\end{aligned}
$$}, j_{\phi, i}</p>
<p>Because the right-hand side of (44) and (45) may come from any of (25)-(42), we'll now prove (44) or (45) for all eighteen possibilities:</p>
<ul>
<li>The right-hand side of (45) comes from (25) or (27). By definition, $n_{\phi, i}$ is a node of type NOT, and let $\gamma \triangleq n_{n_{\phi, i}, 3-j_{\phi, i}}$ denote the other neighbor of $n_{\phi, i}$. By definition, we have $\gamma=\neg \phi, \tilde{L}<em _phi_="\phi," i="i">{L, U, n</em>\right) \leq U(\gamma)$. Therefore,}, j_{\phi, i}}=1-U(\gamma)$, and $p\left(S_{\gamma</li>
</ul>
<p>$$
p\left(S_{\phi}\right)=p\left(S_{\neg \gamma}\right)=1-p\left(S_{\gamma}\right) \geq 1-U(\gamma)=\tilde{L}<em _phi_="\phi," i="i">{L, U, n</em>
$$}, j_{\phi, i}</p>
<p>Hence (45) is true in this scenario.</p>
<ul>
<li>The right-hand side of (44) comes from (26) or (28). By definition, $n_{\phi, i}$ is a node of type NOT, and let $\gamma \triangleq n_{n_{\phi, i}, 3-j_{\phi, i}}$ denote the other neighbor of $n_{\phi, i}$. By definition, we have $\gamma=\neg \phi, \tilde{U}<em _phi_="\phi," i="i">{L, U, n</em>\right) \geq L(\gamma)$. Therefore,}, j_{\phi, i}}=1-L(\gamma)$, and $p\left(S_{\gamma</li>
</ul>
<p>$$
p\left(S_{\phi}\right)=p\left(S_{\neg \gamma}\right)=1-p\left(S_{\gamma}\right) \leq 1-L(\gamma)=\tilde{U}<em _phi_="\phi," i="i">{L, U, n</em>
$$}, j_{\phi, i}</p>
<p>Hence (44) is true in this scenario.</p>
<ul>
<li>The right-hand side of (45) comes from (29). By definition, $n_{\phi, i}$ is a node of type AND- $k$, and let $\gamma \triangleq n_{n_{\phi, i}, k+1}$ and $\xi_{1}, \cdots, \xi_{k-1}$ denote the other neighbors of $n_{\phi, i}$. By definition, we have $\gamma=\phi \wedge \xi_{1} \wedge \cdots \wedge \xi_{k-1}, \tilde{L}<em _phi_="\phi," i="i">{L, U, n</em>$. Therefore,}, j_{\phi, i}}=L(\gamma)$, and $p\left(S_{\gamma}\right) \geq L(\gamma)$. It is straightforward to verify that $S_{\gamma} \subseteq S_{\phi</li>
</ul>
<p>$$
p\left(S_{\phi}\right) \geq p\left(S_{\gamma}\right) \geq L(\gamma)=\tilde{L}<em _phi_="\phi," i="i">{L, U, n</em>
$$}, j_{\phi, i}</p>
<p>Hence (45) is true in this scenario.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ This configuration of weights is the only one in which $x$ may be swapped with $y, w_{x}$ with $w_{y}$, and $\beta_{x}$ with $\beta_{y}$ without affecting the result; accordingly, it is the most reasonable adaptation of the above formulae to define weak conjunction for the weighted Łukasiewicz t-norm.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
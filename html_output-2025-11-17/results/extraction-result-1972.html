<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1972 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1972</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1972</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-281681241</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.25794v1.pdf" target="_blank">Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding</a></p>
                <p><strong>Paper Abstract:</strong> Vision-Language Models (VLMs) have demonstrated impressive world knowledge across a wide range of tasks, making them promising candidates for embodied reasoning applications. However, existing benchmarks primarily evaluate the embodied reasoning ability of VLMs through multiple-choice questions based on image annotations -- for example, selecting which trajectory better describes an event in the image. In this work, we introduce the Point-It-Out (PIO) benchmark, a novel benchmark designed to systematically assess the embodied reasoning abilities of VLMs through precise visual grounding. We propose a hierarchical evaluation protocol spanning three stages (S1: referred-object localization, S2: task-driven pointing, and S3: visual trace prediction), with data collected from critical domains for embodied intelligence, including indoor, kitchen, driving, and robotic manipulation scenarios. Extensive experiments with over ten state-of-the-art VLMs reveal several interesting findings. For example, strong general-purpose models such as GPT-4o, while excelling on many benchmarks (e.g., language, perception, and reasoning), underperform compared to some open-source models in precise visual grounding; models such as MoLMO perform well in S1 and S2 but struggle in S3, where requires grounding combined with visual trace planning.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1972.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1972.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PIO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Point-It-Out benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical benchmark introduced in this paper that evaluates VLMs on precise pixel-level visual grounding for embodied reasoning via three stages: S1 (referred-object localization), S2 (task-driven pointing/affordance grounding), and S3 (visual trace / trajectory prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Point-It-Out (PIO) benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A dataset + evaluation protocol (hierarchical) that requires models to output pixel-space groundings (points, polygon masks/bounding boxes, or 2D trajectories) and compares them to human polygon masks and human/GPT-based trajectory ratings; includes prompts for models that output points, boxes, or trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Evaluation-focused mechanism: require explicit pixel-grounded outputs (normalized 2D points, bounding boxes, polygon masks, or 2D point-sequence trajectories) rather than multiple-choice or language-only outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level (2D points and polygon masks), bounding-box-level, and sequence-of-points (2D trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D normalized pixel coordinates, polygon segmentation masks, single bounding boxes, and ordered 2D trajectories (start end-effector pos + sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation, affordance grounding, driving/scene understanding, robotic manipulation trajectory prediction</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Point-It-Out (PIO)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>real-world images and frames drawn from driving (BDD100K), egocentric kitchens (EPIC-Kitchens), indoor placement (Where2Place), and robot manipulation datasets (RT-1, DROID, AgiBot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>S1/S2: normalized IoU between predicted box and polygon mask (and point/point-in-mask comparisons for point-output models); S3: human rating (1-5) and GPT-o4-mini automated assessment (criteria-based)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Benchmark size: ~501 QA pairs for S1+S2 across 5 datasets and ~100 S3 tasks; no single absolute performance target — used to compare models (see paper figures).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Not presented as single numeric ablation; paper reports that models trained/fine-tuned with explicit grounding supervision outperform generalist VLMs on S1/S2 (see model comparison findings).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper identifies precise visual grounding (object-part localization, affordance/contact points) as a bottleneck; models drop from S1→S2 and many fail to extend S1/S2 skills to S3 planning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Observed failure modes: missing correct object part (major in S1), poor affordance and contact prediction (major in S2), inability to produce temporally coherent trajectories even when single-step grounding succeeds (S3); point-vs-box evaluation biases also noted.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Benchmark includes multiple domains to reveal shifts; paper recommends fine-tuning on grounding/embodied data to reduce domain gaps but provides no specific domain-adaptation module.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Not quantified per novel-object experiments in the paper; general observation: grounding-trained models better on in-domain grounding tasks but no explicit novel-object numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Paper contrasts generalist (often frozen) models vs. grounding-finetuned models and finds fine-tuned grounding models perform better on S1/S2; no explicit frozen-vs-finetuned encoder numeric comparison reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Paper notes generalist, strongly pre-trained models (large scale) are better at multi-step planning (S3) despite weaker spatial precision, but does not provide a quantitative study of pretraining scale vs grounding quality.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Benchmark does not mandate a fusion mechanism; supports models that output points/boxes/trajectories via prompts. The paper discusses that candidate models use a variety of fusion strategies (not standardized in the benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>No concrete sample-efficiency numbers; recommendation: adding grounding data and fine-tuning improves grounding performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Precise pixel-level visual grounding is necessary to evaluate embodied reasoning; models explicitly trained/fine-tuned with grounding supervision outperform generalists on single-target grounding (S1/S2), but many such models fail to combine grounding with temporal planning (S3); evaluation metrics must account for point-vs-box biases (normalized IoU introduced).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1972.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1972.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Point-based grounding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Point-based visual grounding (point supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Grounding approach that trains models to predict one or a few normalized 2D point coordinates corresponding to the language-specified interaction/target location; used because point labels scale more cheaply than dense masks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Point-based grounding models (examples used: MoLMO-7B-D, RoboPoint)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models are fine-tuned on point supervision and prompted to output normalized (0-1) 2D coordinate tuples for the task; they are optimized for predicting contact/affordance points and often use instruction-following multimodal backbones fine-tuned on point labels.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Direct regression/selection of normalized 2D points corresponding to target/pick/contact location (point supervision); training uses point-based loss and robotics-specific grounding data in some models (RoboPoint).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level (single/multiple 2D points)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Normalized 2D pixel coordinates (0-1), possibly conditioned on image crops/patch features</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation, affordance prediction, contact localization</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>S1/S2 tasks in PIO (referred-object localization and task-driven pointing)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>robot egocentric/manipulation frames and third-person household/driving frames (datasets used in PIO)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Primarily point-in-mask metrics historically; in this paper compared using normalized IoU (for box outputs) and point-in-mask / coverage for points.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported strong performance on S1 and S2 relative to generalist VLMs; specific notes: MoLMO and RoboPoint excel at point prediction and are among top performers on S1/S2 (figures in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Qualitative: point-supervised models significantly outperform generalist VLMs on S1/S2 tasks requiring precise pointing; no single aggregated % improvement number provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Point-based models perform poorly when asked to output bounding boxes (format mismatch) and struggle to extend to temporal trajectory planning (S3); evaluation bias favors points unless normalized metrics are used.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Failure modes include format mismatch (point-specialists fail on bbox tasks), inability to produce temporally coherent multi-step trajectories (S3), and low affordance/contact accuracy in some affordance subclasses (affordance scores still <0.4 for many models).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Some point-based models (e.g., RoboPoint) are fine-tuned on robotic data to reduce domain gap; no explicit domain adaptation protocol reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Not explicitly quantified for novel objects; point-based fine-tuning appears to improve in-domain performance but no novel-object numbers given.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Point-based approaches reported are fine-tuned on point labels (improves S1/S2); no frozen-vs-finetuned numeric comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Not studied explicitly for point approaches in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Not specified in detail in the paper for point models; typical approach: conditional decoding to output (x,y) from multimodal features.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Paper notes point supervision is scalable (cheaper than dense masks) but does not provide numeric sample-efficiency comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Point-based grounding is effective and scalable for precise single-target localization (S1/S2) but evaluation must account for biases; point-specialist models struggle to generalize to bounding-box outputs and to sequence/trajectory planning (S3).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1972.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1972.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Box-based grounding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bounding-box visual grounding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Grounding approach where models output a bounding box that encloses the referred mask or region; common for referring-expression comprehension and many VLMs trained with box annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bounding-box output VLMs (examples used: GPT-4o, GPT-o3, Gemini variants, Qwen-2.5-VL when operating in bbox-output mode)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models are prompted to output a single bounding box (normalized coordinates) corresponding to the language referent; many generalist VLMs are trained with bounding-box supervision and more readily produce box outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Predict bounding-box coordinates (x,y,w,h) or polygon approximations; grounding arises from region-level matching between language and image-region features within the multimodal backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>region-level / bounding-box-level (coarse spatial localization)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Bounding boxes (normalized) and implicit region-level visual features</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>referred-object localization (S1) and task-driven localization (S2) when expressed as a region</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>S1/S2 tasks in PIO</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>same PIO domains (household, kitchen, driving, manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Normalized IoU (introduced in paper) to correct bias arising from irregular masks; historically point-in-mask used but is biased toward point methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Generalist bounding-box-capable models (e.g., GPT-4o) often underperform against grounding-finetuned models on S1/S2 in PIO; precise numeric differences not provided as single aggregate values but shown in paper figures.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Bounding-box supervision helps region-level grounding tasks, but bounding boxes are disadvantaged when ground-truth is irregular (hence the normalized IoU metric).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Bounding boxes struggle to tightly cover irregular polygon masks, causing lower raw IoU and evaluation bias unless normalized.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Fails on fine-grained part localization and small/irregularly-shaped targets; boxes lose precision on object parts and affordance contact points.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Not specifically addressed; fine-tuning on grounding data recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Not specifically reported.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Generalists that were not finetuned on grounding underperform compared to grounding-finetuned models on S1/S2.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Paper notes large pre-trained generalists do better at planning (S3) but not at fine-grained spatial precision; no numeric pretraining-scale study for bbox grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Region-level cross-modal matching in multimodal backbones (not detailed in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not quantified for bounding-box training in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Bounding-box outputs are typical for generalist VLMs but are biased by mask shape; explicit grounding supervision and/or normalized evaluation improves fairness; boxes perform worse than point-specialists on fine-grained part/affordance tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1972.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1972.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoLMO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MoLMO-7B (point-specialized variant referred in paper as MoLMO-7B-D)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source VLM that is fine-tuned for point-based grounding and achieves strong referred-object and task-driven pointing performance (S1/S2) but underperforms in multi-step visual-trace planning (S3).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MoLMO-7B / MoLMO-7B-D</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A general-purpose VLM family fine-tuned with explicit grounding supervision (point-based in MoLMO-7B-D) to output normalized points; excels at single-target localization and affordance identification but lacks trajectory/temporal planning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Fine-tuned on point supervision to output normalized pixel coordinates; leverages multimodal instruction-following backbone with point-decoding heads (as described by paper references).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level points</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Normalized 2D coordinates (points); trained with robotic/grounding data</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>referred-object localization (S1) and task-driven pointing/affordance (S2); evaluated on S3 but performs poorly</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>PIO S1/S2/S3</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>household, kitchen, driving, robot-manipulation frames used in PIO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Normalized IoU for box outputs (when applicable) and point-in-mask or coverage for points; S3 human/GPT ratings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Top-performing in S1/S2 among many tested models (figures); affordance subclass score < 0.4 in S2 for many models but MoLMO is the best among them though still <0.4; poor S3 trajectory performance (fails to produce coherent multi-step traces).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Fine-tuning with point grounding yields marked improvement on S1/S2 relative to generalist models (qualitative and relative rankings shown), but no single aggregate % improvement reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>MoLMO struggles to extend single-step grounding to temporal/trajectory planning (S3), indicating a bottleneck when grounding must be combined with planning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Strong single-point localization but fails to generate coherent trajectories (S3); scores low in some affordance/contact subclasses despite best-of-class in S1/S2.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Trained/fine-tuned on grounding datasets; improves in-domain performance but still insufficient for S3 generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Not explicitly reported.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Paper describes MoLMO as fine-tuned for pointing; no explicit frozen-vs-finetuned numeric comparison presented.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Not analyzed specifically for MoLMO in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Not described in detail here beyond being a grounding-finetuned multimodal model (point decoder).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>MoLMO (point-finetuned) is highly effective at S1/S2 precise pointing but fails at S3 trajectory planning, demonstrating that single-shot grounding training does not suffice for spatiotemporal/planning grounding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1972.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1972.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5-VL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-2.5-VL (vision-language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capacity vision-language model trained with bounding-box supervision and instruction tuning that attains strong S1/S2 grounding performance but struggles on S3 visual-trace planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Qwen2.5-vl technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-VL (32B variant referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-following VLM trained with bounding-box annotations and multimodal instruction data; produces region-level bounding-box outputs and is competitive on referred-object and task-driven localization.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Region-level grounding via learned alignment between language tokens and image region features; trained with bounding-box supervision in its upstream training.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>region-level / bounding-box</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Bounding boxes (normalized coordinates)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>referred-object localization (S1) and task-driven localization (S2); evaluated on S3</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>PIO S1/S2/S3</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>same PIO domains</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Normalized IoU for boxes vs masks; S3 human/GPT rating</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Consistently among top performers in S1 and S2 (figures), but underperforms on S3 trajectory prediction (struggle to generate temporally coherent traces compared to Gemini-2.5-Pro and GPT-o3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Models trained with explicit grounding (like Qwen) outperform generalists on S1/S2; no single numeric ablation provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Fails to combine single-target grounding with visual-trace planning; S2→S3 transition is a primary bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Strong region grounding but poor multi-step/trajectory planning; struggles on object-part localization and affordance/contact subclasses.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Trained with grounding data which improves in-domain grounding; no explicit domain-adaptation modules described.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Qwen variants used are grounding-trained; no frozen-vs-finetuned comparison provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Not directly analyzed beyond commentary that large generalists differ in S3 behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Region-language alignment via multimodal model (details not provided in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Qwen-2.5-VL (grounding-trained) is strong at S1/S2 but does not reliably produce coherent S3 trajectories, illustrating that grounding supervision alone does not guarantee spatiotemporal/planning grounding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1972.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1972.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboRefer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboRefer (grounding fine-tuned model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model explicitly fine-tuned on grounding/robotic data that achieves high performance on pixel-level localization tasks (S1/S2) in PIO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoboRefer (RoboRefer-SFT-8B referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A vision-language model fine-tuned with grounding supervision using robot-centric datasets to improve precise localization and affordance pointing.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Supervised grounding via box/point annotations from robotic datasets; trained to output localized points/boxes matching polygon masks.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level points and region-level boxes</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Normalized 2D coordinates and bounding boxes; polygon masks used for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>robotic object localization and affordance pointing (S1/S2)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>PIO S1/S2</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>robot-manipulation frames and indoor scenes in PIO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Normalized IoU for boxes, point-in-mask / coverage for points</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>One of the top performers on S1 and S2 (paper ranks RoboRefer-SFT-8B high in figures), but not highlighted for S3 success.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Fine-tuning on grounding data yields notable gains on S1/S2 compared to generalists (relative ranking presented), but no single numeric delta provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>While effective at single-target grounding, RoboRefer-like models still struggle on part-localization and affordance/contact prediction in some cases; also limited in S3.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Main failures: fine-grained object parts and affordance contact localization; inability to generate temporally coherent trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Fine-tuned on robotic grounding data to mitigate domain shift; no explicit numeric analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>RoboRefer is presented as a fine-tuned grounding model; no frozen comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Not detailed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>RoboRefer-style fine-tuning on grounding data substantially improves S1/S2 pixel-level localization compared to generalist VLMs, but alone does not solve S3 trajectory planning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1972.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1972.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboPoint</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboPoint (point-specialized VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLM fine-tuned on robotics point data that excels at spatial affordance prediction and point-based grounding, used in PIO evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robopoint: A vision-language model for spatial affordance prediction for robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoboPoint</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A model trained specifically for spatial affordance prediction/outputting points for robotic interaction; prompted to return normalized coordinate tuples corresponding to interaction points.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Point supervision targeted to robotic affordances and contact points; model outputs list of (x,y) normalized coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level points</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Normalized 2D pixel coordinates</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>robotic affordance localization and manipulation-point prediction (S2 primarily, S1 as well)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>PIO S1/S2</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>robot-manipulation and indoor scenes in PIO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Point-in-mask / normalized IoU comparisons when converted to boxes for cross-format comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Strong at point prediction tasks (one of the better point-specialists) but struggles when evaluated in bounding-box formats or in S3 trajectory tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Point-finetuning improves point prediction performance on S1/S2; specific numeric deltas not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>RoboPoint is limited when crossing to box outputs and in generating temporally coherent trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Fails in irregular mask coverage when evaluations are box-centric; S3 trajectory generation failures.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Trained on robotics data to reduce domain mismatch; no numeric assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Presented as fine-tuned on point supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>RoboPoint demonstrates the utility of point supervision for affordance and contact prediction, but point-specialists have limitations when tasks require region outputs or temporal planning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1972.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1972.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-2.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 2.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strong generalist vision-language model variant that shows relatively balanced performance, including promising S3 (trajectory) results, attributed to broad pretraining including embodied/grounding data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-2.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large pre-trained VLM (Gemini family) that, according to the paper, benefits from inclusion of embodied and grounding data in pretraining and produces reasonable bounding-box outputs and coherent visual traces for S3.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Described qualitatively as pretrained on large multimodal and embodied datasets (paper references Gemini documentation), but exact datasets not specified within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Learned multimodal alignment from large-scale pretraining including embodied grounding data; outputs bounding boxes and trajectories when prompted.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>multi-level (region-level bounding boxes and trajectory sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Bounding boxes, 2D trajectory sequences (normalized coordinates)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>referred-object localization (S1), task-driven grounding (S2), and visual-trace prediction (S3)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>PIO S1/S2/S3</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>PIO domains (household, kitchen, driving, robot manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Normalized IoU (S1/S2) and human/GPT ratings for S3</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Per paper: Gemini-2.5-Pro is among the best models for S3, achieving close to ~4/5 average human rating on S3 trajectories and strong performance on S1/S2 (figures show it near top of rankings).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Paper attributes Gemini-2.5-Pro's S3 strengths to inclusion of embodied and grounding data in pretraining; comparative improvement not provided as a single number.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Although strong in S3, generalists still lack perfect spatial precision—for some fine-grained S1/S2 tasks grounding-finetuned models outperform Gemini in localization precision.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Gemini-2.5-Pro performs better at temporal planning but still shows degradation on fine-grained object-part and tight affordance localization compared to point-specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Pretraining that includes embodied grounding data helps reduce domain shift and improve S3 capability (qualitative claim).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Not quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Not compared explicitly in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Paper suggests strong pretraining (including embodied data) leads to better S3 generalization, but no numerical pretraining-scale study is shown.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Not explicitly detailed in paper; implied multimodal pretraining with cross-modal alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Gemini-2.5-Pro shows that large generalist models with embodied/grounding pretraining can better integrate grounding with planning (S3), even if they are less spatially precise than grounding-finetuned specialists on S1/S2.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1972.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1972.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-family (GPT-4o / GPT-o3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o and GPT-o3 (Vision-capable GPT family members)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General-purpose multimodal VLMs from the GPT family; GPT-4o shows weaker precise grounding relative to some open-source grounding-specialized models, while GPT-o3 achieves better results in grounding+planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o / GPT-o3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-following multimodal models that can output bounding boxes more readily than points (per the paper) and display strong reasoning/planning capabilities; GPT-o3 performs relatively well across S1–S3 compared to other generalists.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>General multimodal alignment learned in pretraining; outputs region-level boxes and trajectory sequences when prompted; not explicitly fine-tuned on pixel-level grounding in the experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>global/region-level with ability to output bounding boxes and trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Bounding boxes and 2D trajectory sequences; may use implicit spatial embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>S1 referred-object localization, S2 task-driven pointing, S3 visual-trace prediction</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>PIO</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>PIO domains</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Normalized IoU (S1/S2) and human/GPT rating for S3</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-4o and Claude-3.7 underperform on precise S1/S2 grounding compared to grounding-finetuned models; GPT-o3 achieves better results (noted to slightly outperform GPT-4o in S3); exact numeric performance shown in paper figures but not reported as single summary numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Paper shows generalists lag behind grounding-trained models on S1/S2 but may be sharper at multi-step planning (S3), indicating differing strengths rather than a single numeric delta.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Generalist GPT models tend to produce coarser spatial outputs and thus perform worse on fine-grained part/affordance localization tasks (S1/S2); however they can produce better multi-step trajectories (S3).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>GPT-4o series often produces coarser bounding boxes or incorrect points for object-part and affordance tasks; smaller variants (e.g., GPT-4o-mini) perform worse.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>No explicit adaptation described in this paper; performance appears to hinge on inclusion of embodied data during pretraining/fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Not directly compared in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Paper implies scale and pretraining data composition (embodied data) affect S3 competence but does not quantify the effect.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Not specified in detail within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Large generalist GPT-family VLMs have strong reasoning/planning ability (S3) but are weaker at fine-grained pixel-level grounding (S1/S2) compared to explicit grounding-finetuned models.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1972.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1972.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Normalized IoU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Normalized Intersection-over-Union evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation metric introduced in this paper that normalizes IoU of a predicted bounding box with a ground-truth polygon mask by the IoU between the tightest bounding box enclosing the mask and the mask itself, reducing bias against boxes on irregular masks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Normalized IoU metric</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Computes s = IoU(bbox_pred, mask) / IoU(bbox_tight, mask) (described qualitatively); this normalization accounts for difficulty of covering irregular-shaped polygonal masks with rectangular boxes and yields fairer comparisons between box and point approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>N/A (evaluation normalization method rather than a model grounding mechanism).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>evaluation-level normalization for bounding-box vs polygon mask comparison</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Uses bounding boxes and polygon masks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>S1 and S2 evaluation in PIO</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>PIO evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Applies to any PIO domains where masks are irregular (household/kitchen/robot/driving)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Normalized IoU itself; reported to improve fairness vs point-in-mask bias.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Addresses evaluation failure mode (bias) where point-in-mask favors point-specialized models and boxes are unfairly penalized on irregular masks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Normalized IoU reduces evaluation bias between box and point outputs, enabling fairer S1/S2 comparisons across models with different output formats.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1972.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1972.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>S1/S2/S3 Hierarchy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Three-stage hierarchical grounding framework (S1, S2, S3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured decomposition of embodied visual grounding into S1: referred-object localization, S2: task-driven grounding/affordance/contact localization, and S3: visual trace/trajectory prediction for spatiotemporal planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>S1/S2/S3 hierarchical evaluation stages</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Defines progressively harder grounding tasks: S1 maps language references to visual entities; S2 identifies where to act based on affordance/context; S3 requires producing temporally coherent 2D trajectories starting from an end-effector position.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Conceptual decomposition used to design prompts, annotations, and evaluations; not a single model architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>S1: object/part-level; S2: affordance/contact-level; S3: spatiotemporal trajectory-level (sequence of 2D points)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>S1: polygon masks and boxes; S2: points/boxes/polygons representing contact/affordance; S3: ordered 2D trajectories starting at annotated end-effector position</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>All: object localization, affordance grounding, and trajectory planning in embodied contexts</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>PIO S1/S2/S3</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>household, kitchen, driving, robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>S1/S2: normalized IoU / point coverage; S3: human ratings and GPT-based scoring</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper demonstrates that success in S1 and S2 is necessary but not sufficient for S3; many models fail at integrating single-target grounding into coherent visual traces.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Explicit: drop in performance from S1→S2 across models; S3 demands additional planning capabilities beyond per-frame grounding and reveals large gaps in VLM capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Hierarchy used to surface domain-specific weaknesses; no explicit domain adaptation strategy supplied.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Decomposing embodied grounding into S1/S2/S3 reveals distinct failure modes: part-localization and affordance/contact are weak in S1/S2; integrating grounding into temporally coherent plans (S3) is a qualitatively different and harder challenge.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Robopoint: A vision-language model for spatial affordance prediction for robotics. <em>(Rating: 2)</em></li>
                <li>Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models. <em>(Rating: 2)</em></li>
                <li>RT-1: Robotics transformer for real-world control at scale. <em>(Rating: 2)</em></li>
                <li>Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. <em>(Rating: 2)</em></li>
                <li>Affordancenet: An end-to-end deep learning approach for object affordance detection. <em>(Rating: 2)</em></li>
                <li>Gemini robotics: Bringing ai into the physical world. <em>(Rating: 1)</em></li>
                <li>Qwen2.5-vl technical report. <em>(Rating: 1)</em></li>
                <li>Towards testing and evaluating vision-language-action models for robotic manipulation: An empirical study. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1972",
    "paper_id": "paper-281681241",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "PIO",
            "name_full": "Point-It-Out benchmark",
            "brief_description": "A hierarchical benchmark introduced in this paper that evaluates VLMs on precise pixel-level visual grounding for embodied reasoning via three stages: S1 (referred-object localization), S2 (task-driven pointing/affordance grounding), and S3 (visual trace / trajectory prediction).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Point-It-Out (PIO) benchmark",
            "model_description": "A dataset + evaluation protocol (hierarchical) that requires models to output pixel-space groundings (points, polygon masks/bounding boxes, or 2D trajectories) and compares them to human polygon masks and human/GPT-based trajectory ratings; includes prompts for models that output points, boxes, or trajectories.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Evaluation-focused mechanism: require explicit pixel-grounded outputs (normalized 2D points, bounding boxes, polygon masks, or 2D point-sequence trajectories) rather than multiple-choice or language-only outputs.",
            "representation_level": "pixel-level (2D points and polygon masks), bounding-box-level, and sequence-of-points (2D trajectories)",
            "spatial_representation": "2D normalized pixel coordinates, polygon segmentation masks, single bounding boxes, and ordered 2D trajectories (start end-effector pos + sequence)",
            "embodied_task_type": "object manipulation, affordance grounding, driving/scene understanding, robotic manipulation trajectory prediction",
            "embodied_task_name": "Point-It-Out (PIO)",
            "visual_domain": "real-world images and frames drawn from driving (BDD100K), egocentric kitchens (EPIC-Kitchens), indoor placement (Where2Place), and robot manipulation datasets (RT-1, DROID, AgiBot)",
            "performance_metric": "S1/S2: normalized IoU between predicted box and polygon mask (and point/point-in-mask comparisons for point-output models); S3: human rating (1-5) and GPT-o4-mini automated assessment (criteria-based)",
            "performance_value": "Benchmark size: ~501 QA pairs for S1+S2 across 5 datasets and ~100 S3 tasks; no single absolute performance target — used to compare models (see paper figures).",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Not presented as single numeric ablation; paper reports that models trained/fine-tuned with explicit grounding supervision outperform generalist VLMs on S1/S2 (see model comparison findings).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Paper identifies precise visual grounding (object-part localization, affordance/contact points) as a bottleneck; models drop from S1→S2 and many fail to extend S1/S2 skills to S3 planning.",
            "failure_mode_analysis": "Observed failure modes: missing correct object part (major in S1), poor affordance and contact prediction (major in S2), inability to produce temporally coherent trajectories even when single-step grounding succeeds (S3); point-vs-box evaluation biases also noted.",
            "domain_shift_handling": "Benchmark includes multiple domains to reveal shifts; paper recommends fine-tuning on grounding/embodied data to reduce domain gaps but provides no specific domain-adaptation module.",
            "novel_object_performance": "Not quantified per novel-object experiments in the paper; general observation: grounding-trained models better on in-domain grounding tasks but no explicit novel-object numbers provided.",
            "frozen_vs_finetuned": "Paper contrasts generalist (often frozen) models vs. grounding-finetuned models and finds fine-tuned grounding models perform better on S1/S2; no explicit frozen-vs-finetuned encoder numeric comparison reported.",
            "pretraining_scale_effect": "Paper notes generalist, strongly pre-trained models (large scale) are better at multi-step planning (S3) despite weaker spatial precision, but does not provide a quantitative study of pretraining scale vs grounding quality.",
            "fusion_mechanism": "Benchmark does not mandate a fusion mechanism; supports models that output points/boxes/trajectories via prompts. The paper discusses that candidate models use a variety of fusion strategies (not standardized in the benchmark).",
            "sample_efficiency": "No concrete sample-efficiency numbers; recommendation: adding grounding data and fine-tuning improves grounding performance.",
            "key_findings_grounding": "Precise pixel-level visual grounding is necessary to evaluate embodied reasoning; models explicitly trained/fine-tuned with grounding supervision outperform generalists on single-target grounding (S1/S2), but many such models fail to combine grounding with temporal planning (S3); evaluation metrics must account for point-vs-box biases (normalized IoU introduced).",
            "uuid": "e1972.0"
        },
        {
            "name_short": "Point-based grounding",
            "name_full": "Point-based visual grounding (point supervision)",
            "brief_description": "Grounding approach that trains models to predict one or a few normalized 2D point coordinates corresponding to the language-specified interaction/target location; used because point labels scale more cheaply than dense masks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Point-based grounding models (examples used: MoLMO-7B-D, RoboPoint)",
            "model_description": "Models are fine-tuned on point supervision and prompted to output normalized (0-1) 2D coordinate tuples for the task; they are optimized for predicting contact/affordance points and often use instruction-following multimodal backbones fine-tuned on point labels.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Direct regression/selection of normalized 2D points corresponding to target/pick/contact location (point supervision); training uses point-based loss and robotics-specific grounding data in some models (RoboPoint).",
            "representation_level": "pixel-level (single/multiple 2D points)",
            "spatial_representation": "Normalized 2D pixel coordinates (0-1), possibly conditioned on image crops/patch features",
            "embodied_task_type": "object manipulation, affordance prediction, contact localization",
            "embodied_task_name": "S1/S2 tasks in PIO (referred-object localization and task-driven pointing)",
            "visual_domain": "robot egocentric/manipulation frames and third-person household/driving frames (datasets used in PIO)",
            "performance_metric": "Primarily point-in-mask metrics historically; in this paper compared using normalized IoU (for box outputs) and point-in-mask / coverage for points.",
            "performance_value": "Reported strong performance on S1 and S2 relative to generalist VLMs; specific notes: MoLMO and RoboPoint excel at point prediction and are among top performers on S1/S2 (figures in paper).",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Qualitative: point-supervised models significantly outperform generalist VLMs on S1/S2 tasks requiring precise pointing; no single aggregated % improvement number provided.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Point-based models perform poorly when asked to output bounding boxes (format mismatch) and struggle to extend to temporal trajectory planning (S3); evaluation bias favors points unless normalized metrics are used.",
            "failure_mode_analysis": "Failure modes include format mismatch (point-specialists fail on bbox tasks), inability to produce temporally coherent multi-step trajectories (S3), and low affordance/contact accuracy in some affordance subclasses (affordance scores still &lt;0.4 for many models).",
            "domain_shift_handling": "Some point-based models (e.g., RoboPoint) are fine-tuned on robotic data to reduce domain gap; no explicit domain adaptation protocol reported in paper.",
            "novel_object_performance": "Not explicitly quantified for novel objects; point-based fine-tuning appears to improve in-domain performance but no novel-object numbers given.",
            "frozen_vs_finetuned": "Point-based approaches reported are fine-tuned on point labels (improves S1/S2); no frozen-vs-finetuned numeric comparison provided.",
            "pretraining_scale_effect": "Not studied explicitly for point approaches in this paper.",
            "fusion_mechanism": "Not specified in detail in the paper for point models; typical approach: conditional decoding to output (x,y) from multimodal features.",
            "sample_efficiency": "Paper notes point supervision is scalable (cheaper than dense masks) but does not provide numeric sample-efficiency comparisons.",
            "key_findings_grounding": "Point-based grounding is effective and scalable for precise single-target localization (S1/S2) but evaluation must account for biases; point-specialist models struggle to generalize to bounding-box outputs and to sequence/trajectory planning (S3).",
            "uuid": "e1972.1"
        },
        {
            "name_short": "Box-based grounding",
            "name_full": "Bounding-box visual grounding",
            "brief_description": "Grounding approach where models output a bounding box that encloses the referred mask or region; common for referring-expression comprehension and many VLMs trained with box annotations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Bounding-box output VLMs (examples used: GPT-4o, GPT-o3, Gemini variants, Qwen-2.5-VL when operating in bbox-output mode)",
            "model_description": "Models are prompted to output a single bounding box (normalized coordinates) corresponding to the language referent; many generalist VLMs are trained with bounding-box supervision and more readily produce box outputs.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Predict bounding-box coordinates (x,y,w,h) or polygon approximations; grounding arises from region-level matching between language and image-region features within the multimodal backbone.",
            "representation_level": "region-level / bounding-box-level (coarse spatial localization)",
            "spatial_representation": "Bounding boxes (normalized) and implicit region-level visual features",
            "embodied_task_type": "referred-object localization (S1) and task-driven localization (S2) when expressed as a region",
            "embodied_task_name": "S1/S2 tasks in PIO",
            "visual_domain": "same PIO domains (household, kitchen, driving, manipulation)",
            "performance_metric": "Normalized IoU (introduced in paper) to correct bias arising from irregular masks; historically point-in-mask used but is biased toward point methods.",
            "performance_value": "Generalist bounding-box-capable models (e.g., GPT-4o) often underperform against grounding-finetuned models on S1/S2 in PIO; precise numeric differences not provided as single aggregate values but shown in paper figures.",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Bounding-box supervision helps region-level grounding tasks, but bounding boxes are disadvantaged when ground-truth is irregular (hence the normalized IoU metric).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Bounding boxes struggle to tightly cover irregular polygon masks, causing lower raw IoU and evaluation bias unless normalized.",
            "failure_mode_analysis": "Fails on fine-grained part localization and small/irregularly-shaped targets; boxes lose precision on object parts and affordance contact points.",
            "domain_shift_handling": "Not specifically addressed; fine-tuning on grounding data recommended.",
            "novel_object_performance": "Not specifically reported.",
            "frozen_vs_finetuned": "Generalists that were not finetuned on grounding underperform compared to grounding-finetuned models on S1/S2.",
            "pretraining_scale_effect": "Paper notes large pre-trained generalists do better at planning (S3) but not at fine-grained spatial precision; no numeric pretraining-scale study for bbox grounding.",
            "fusion_mechanism": "Region-level cross-modal matching in multimodal backbones (not detailed in paper).",
            "sample_efficiency": "Not quantified for bounding-box training in this paper.",
            "key_findings_grounding": "Bounding-box outputs are typical for generalist VLMs but are biased by mask shape; explicit grounding supervision and/or normalized evaluation improves fairness; boxes perform worse than point-specialists on fine-grained part/affordance tasks.",
            "uuid": "e1972.2"
        },
        {
            "name_short": "MoLMO",
            "name_full": "MoLMO-7B (point-specialized variant referred in paper as MoLMO-7B-D)",
            "brief_description": "An open-source VLM that is fine-tuned for point-based grounding and achieves strong referred-object and task-driven pointing performance (S1/S2) but underperforms in multi-step visual-trace planning (S3).",
            "citation_title": "Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models.",
            "mention_or_use": "use",
            "model_name": "MoLMO-7B / MoLMO-7B-D",
            "model_description": "A general-purpose VLM family fine-tuned with explicit grounding supervision (point-based in MoLMO-7B-D) to output normalized points; excels at single-target localization and affordance identification but lacks trajectory/temporal planning capabilities.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Fine-tuned on point supervision to output normalized pixel coordinates; leverages multimodal instruction-following backbone with point-decoding heads (as described by paper references).",
            "representation_level": "pixel-level points",
            "spatial_representation": "Normalized 2D coordinates (points); trained with robotic/grounding data",
            "embodied_task_type": "referred-object localization (S1) and task-driven pointing/affordance (S2); evaluated on S3 but performs poorly",
            "embodied_task_name": "PIO S1/S2/S3",
            "visual_domain": "household, kitchen, driving, robot-manipulation frames used in PIO",
            "performance_metric": "Normalized IoU for box outputs (when applicable) and point-in-mask or coverage for points; S3 human/GPT ratings",
            "performance_value": "Top-performing in S1/S2 among many tested models (figures); affordance subclass score &lt; 0.4 in S2 for many models but MoLMO is the best among them though still &lt;0.4; poor S3 trajectory performance (fails to produce coherent multi-step traces).",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Fine-tuning with point grounding yields marked improvement on S1/S2 relative to generalist models (qualitative and relative rankings shown), but no single aggregate % improvement reported.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "MoLMO struggles to extend single-step grounding to temporal/trajectory planning (S3), indicating a bottleneck when grounding must be combined with planning.",
            "failure_mode_analysis": "Strong single-point localization but fails to generate coherent trajectories (S3); scores low in some affordance/contact subclasses despite best-of-class in S1/S2.",
            "domain_shift_handling": "Trained/fine-tuned on grounding datasets; improves in-domain performance but still insufficient for S3 generalization.",
            "novel_object_performance": "Not explicitly reported.",
            "frozen_vs_finetuned": "Paper describes MoLMO as fine-tuned for pointing; no explicit frozen-vs-finetuned numeric comparison presented.",
            "pretraining_scale_effect": "Not analyzed specifically for MoLMO in this paper.",
            "fusion_mechanism": "Not described in detail here beyond being a grounding-finetuned multimodal model (point decoder).",
            "sample_efficiency": "Not quantified.",
            "key_findings_grounding": "MoLMO (point-finetuned) is highly effective at S1/S2 precise pointing but fails at S3 trajectory planning, demonstrating that single-shot grounding training does not suffice for spatiotemporal/planning grounding.",
            "uuid": "e1972.3"
        },
        {
            "name_short": "Qwen-2.5-VL",
            "name_full": "Qwen-2.5-VL (vision-language model)",
            "brief_description": "A high-capacity vision-language model trained with bounding-box supervision and instruction tuning that attains strong S1/S2 grounding performance but struggles on S3 visual-trace planning.",
            "citation_title": "Qwen2.5-vl technical report.",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5-VL (32B variant referenced)",
            "model_description": "Large instruction-following VLM trained with bounding-box annotations and multimodal instruction data; produces region-level bounding-box outputs and is competitive on referred-object and task-driven localization.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Region-level grounding via learned alignment between language tokens and image region features; trained with bounding-box supervision in its upstream training.",
            "representation_level": "region-level / bounding-box",
            "spatial_representation": "Bounding boxes (normalized coordinates)",
            "embodied_task_type": "referred-object localization (S1) and task-driven localization (S2); evaluated on S3",
            "embodied_task_name": "PIO S1/S2/S3",
            "visual_domain": "same PIO domains",
            "performance_metric": "Normalized IoU for boxes vs masks; S3 human/GPT rating",
            "performance_value": "Consistently among top performers in S1 and S2 (figures), but underperforms on S3 trajectory prediction (struggle to generate temporally coherent traces compared to Gemini-2.5-Pro and GPT-o3).",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Models trained with explicit grounding (like Qwen) outperform generalists on S1/S2; no single numeric ablation provided.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Fails to combine single-target grounding with visual-trace planning; S2→S3 transition is a primary bottleneck.",
            "failure_mode_analysis": "Strong region grounding but poor multi-step/trajectory planning; struggles on object-part localization and affordance/contact subclasses.",
            "domain_shift_handling": "Trained with grounding data which improves in-domain grounding; no explicit domain-adaptation modules described.",
            "novel_object_performance": "Not reported.",
            "frozen_vs_finetuned": "Qwen variants used are grounding-trained; no frozen-vs-finetuned comparison provided in this paper.",
            "pretraining_scale_effect": "Not directly analyzed beyond commentary that large generalists differ in S3 behaviour.",
            "fusion_mechanism": "Region-language alignment via multimodal model (details not provided in paper).",
            "sample_efficiency": "Not quantified.",
            "key_findings_grounding": "Qwen-2.5-VL (grounding-trained) is strong at S1/S2 but does not reliably produce coherent S3 trajectories, illustrating that grounding supervision alone does not guarantee spatiotemporal/planning grounding.",
            "uuid": "e1972.4"
        },
        {
            "name_short": "RoboRefer",
            "name_full": "RoboRefer (grounding fine-tuned model)",
            "brief_description": "A model explicitly fine-tuned on grounding/robotic data that achieves high performance on pixel-level localization tasks (S1/S2) in PIO.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoboRefer (RoboRefer-SFT-8B referenced)",
            "model_description": "A vision-language model fine-tuned with grounding supervision using robot-centric datasets to improve precise localization and affordance pointing.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Supervised grounding via box/point annotations from robotic datasets; trained to output localized points/boxes matching polygon masks.",
            "representation_level": "pixel-level points and region-level boxes",
            "spatial_representation": "Normalized 2D coordinates and bounding boxes; polygon masks used for evaluation",
            "embodied_task_type": "robotic object localization and affordance pointing (S1/S2)",
            "embodied_task_name": "PIO S1/S2",
            "visual_domain": "robot-manipulation frames and indoor scenes in PIO",
            "performance_metric": "Normalized IoU for boxes, point-in-mask / coverage for points",
            "performance_value": "One of the top performers on S1 and S2 (paper ranks RoboRefer-SFT-8B high in figures), but not highlighted for S3 success.",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Fine-tuning on grounding data yields notable gains on S1/S2 compared to generalists (relative ranking presented), but no single numeric delta provided.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "While effective at single-target grounding, RoboRefer-like models still struggle on part-localization and affordance/contact prediction in some cases; also limited in S3.",
            "failure_mode_analysis": "Main failures: fine-grained object parts and affordance contact localization; inability to generate temporally coherent trajectories.",
            "domain_shift_handling": "Fine-tuned on robotic grounding data to mitigate domain shift; no explicit numeric analysis.",
            "novel_object_performance": "Not reported.",
            "frozen_vs_finetuned": "RoboRefer is presented as a fine-tuned grounding model; no frozen comparison provided.",
            "pretraining_scale_effect": "Not discussed.",
            "fusion_mechanism": "Not detailed in the paper.",
            "sample_efficiency": "Not quantified.",
            "key_findings_grounding": "RoboRefer-style fine-tuning on grounding data substantially improves S1/S2 pixel-level localization compared to generalist VLMs, but alone does not solve S3 trajectory planning.",
            "uuid": "e1972.5"
        },
        {
            "name_short": "RoboPoint",
            "name_full": "RoboPoint (point-specialized VLM)",
            "brief_description": "A VLM fine-tuned on robotics point data that excels at spatial affordance prediction and point-based grounding, used in PIO evaluations.",
            "citation_title": "Robopoint: A vision-language model for spatial affordance prediction for robotics.",
            "mention_or_use": "use",
            "model_name": "RoboPoint",
            "model_description": "A model trained specifically for spatial affordance prediction/outputting points for robotic interaction; prompted to return normalized coordinate tuples corresponding to interaction points.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Point supervision targeted to robotic affordances and contact points; model outputs list of (x,y) normalized coordinates.",
            "representation_level": "pixel-level points",
            "spatial_representation": "Normalized 2D pixel coordinates",
            "embodied_task_type": "robotic affordance localization and manipulation-point prediction (S2 primarily, S1 as well)",
            "embodied_task_name": "PIO S1/S2",
            "visual_domain": "robot-manipulation and indoor scenes in PIO",
            "performance_metric": "Point-in-mask / normalized IoU comparisons when converted to boxes for cross-format comparisons",
            "performance_value": "Strong at point prediction tasks (one of the better point-specialists) but struggles when evaluated in bounding-box formats or in S3 trajectory tasks.",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Point-finetuning improves point prediction performance on S1/S2; specific numeric deltas not reported.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "RoboPoint is limited when crossing to box outputs and in generating temporally coherent trajectories.",
            "failure_mode_analysis": "Fails in irregular mask coverage when evaluations are box-centric; S3 trajectory generation failures.",
            "domain_shift_handling": "Trained on robotics data to reduce domain mismatch; no numeric assessment.",
            "novel_object_performance": "Not provided.",
            "frozen_vs_finetuned": "Presented as fine-tuned on point supervision.",
            "pretraining_scale_effect": "Not analyzed.",
            "fusion_mechanism": "Not detailed.",
            "sample_efficiency": "Not quantified.",
            "key_findings_grounding": "RoboPoint demonstrates the utility of point supervision for affordance and contact prediction, but point-specialists have limitations when tasks require region outputs or temporal planning.",
            "uuid": "e1972.6"
        },
        {
            "name_short": "Gemini-2.5-Pro",
            "name_full": "Gemini 2.5-Pro",
            "brief_description": "A strong generalist vision-language model variant that shows relatively balanced performance, including promising S3 (trajectory) results, attributed to broad pretraining including embodied/grounding data.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-2.5-Pro",
            "model_description": "A large pre-trained VLM (Gemini family) that, according to the paper, benefits from inclusion of embodied and grounding data in pretraining and produces reasonable bounding-box outputs and coherent visual traces for S3.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": "Described qualitatively as pretrained on large multimodal and embodied datasets (paper references Gemini documentation), but exact datasets not specified within this paper.",
            "grounding_mechanism": "Learned multimodal alignment from large-scale pretraining including embodied grounding data; outputs bounding boxes and trajectories when prompted.",
            "representation_level": "multi-level (region-level bounding boxes and trajectory sequences)",
            "spatial_representation": "Bounding boxes, 2D trajectory sequences (normalized coordinates)",
            "embodied_task_type": "referred-object localization (S1), task-driven grounding (S2), and visual-trace prediction (S3)",
            "embodied_task_name": "PIO S1/S2/S3",
            "visual_domain": "PIO domains (household, kitchen, driving, robot manipulation)",
            "performance_metric": "Normalized IoU (S1/S2) and human/GPT ratings for S3",
            "performance_value": "Per paper: Gemini-2.5-Pro is among the best models for S3, achieving close to ~4/5 average human rating on S3 trajectories and strong performance on S1/S2 (figures show it near top of rankings).",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Paper attributes Gemini-2.5-Pro's S3 strengths to inclusion of embodied and grounding data in pretraining; comparative improvement not provided as a single number.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Although strong in S3, generalists still lack perfect spatial precision—for some fine-grained S1/S2 tasks grounding-finetuned models outperform Gemini in localization precision.",
            "failure_mode_analysis": "Gemini-2.5-Pro performs better at temporal planning but still shows degradation on fine-grained object-part and tight affordance localization compared to point-specialized models.",
            "domain_shift_handling": "Pretraining that includes embodied grounding data helps reduce domain shift and improve S3 capability (qualitative claim).",
            "novel_object_performance": "Not quantified in the paper.",
            "frozen_vs_finetuned": "Not compared explicitly in the paper.",
            "pretraining_scale_effect": "Paper suggests strong pretraining (including embodied data) leads to better S3 generalization, but no numerical pretraining-scale study is shown.",
            "fusion_mechanism": "Not explicitly detailed in paper; implied multimodal pretraining with cross-modal alignment.",
            "sample_efficiency": "Not provided.",
            "key_findings_grounding": "Gemini-2.5-Pro shows that large generalist models with embodied/grounding pretraining can better integrate grounding with planning (S3), even if they are less spatially precise than grounding-finetuned specialists on S1/S2.",
            "uuid": "e1972.7"
        },
        {
            "name_short": "GPT-family (GPT-4o / GPT-o3)",
            "name_full": "GPT-4o and GPT-o3 (Vision-capable GPT family members)",
            "brief_description": "General-purpose multimodal VLMs from the GPT family; GPT-4o shows weaker precise grounding relative to some open-source grounding-specialized models, while GPT-o3 achieves better results in grounding+planning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o / GPT-o3",
            "model_description": "Large instruction-following multimodal models that can output bounding boxes more readily than points (per the paper) and display strong reasoning/planning capabilities; GPT-o3 performs relatively well across S1–S3 compared to other generalists.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "General multimodal alignment learned in pretraining; outputs region-level boxes and trajectory sequences when prompted; not explicitly fine-tuned on pixel-level grounding in the experiments reported.",
            "representation_level": "global/region-level with ability to output bounding boxes and trajectories",
            "spatial_representation": "Bounding boxes and 2D trajectory sequences; may use implicit spatial embeddings",
            "embodied_task_type": "S1 referred-object localization, S2 task-driven pointing, S3 visual-trace prediction",
            "embodied_task_name": "PIO",
            "visual_domain": "PIO domains",
            "performance_metric": "Normalized IoU (S1/S2) and human/GPT rating for S3",
            "performance_value": "GPT-4o and Claude-3.7 underperform on precise S1/S2 grounding compared to grounding-finetuned models; GPT-o3 achieves better results (noted to slightly outperform GPT-4o in S3); exact numeric performance shown in paper figures but not reported as single summary numbers.",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Paper shows generalists lag behind grounding-trained models on S1/S2 but may be sharper at multi-step planning (S3), indicating differing strengths rather than a single numeric delta.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Generalist GPT models tend to produce coarser spatial outputs and thus perform worse on fine-grained part/affordance localization tasks (S1/S2); however they can produce better multi-step trajectories (S3).",
            "failure_mode_analysis": "GPT-4o series often produces coarser bounding boxes or incorrect points for object-part and affordance tasks; smaller variants (e.g., GPT-4o-mini) perform worse.",
            "domain_shift_handling": "No explicit adaptation described in this paper; performance appears to hinge on inclusion of embodied data during pretraining/fine-tuning.",
            "novel_object_performance": "Not reported.",
            "frozen_vs_finetuned": "Not directly compared in the paper.",
            "pretraining_scale_effect": "Paper implies scale and pretraining data composition (embodied data) affect S3 competence but does not quantify the effect.",
            "fusion_mechanism": "Not specified in detail within this paper.",
            "sample_efficiency": "Not provided.",
            "key_findings_grounding": "Large generalist GPT-family VLMs have strong reasoning/planning ability (S3) but are weaker at fine-grained pixel-level grounding (S1/S2) compared to explicit grounding-finetuned models.",
            "uuid": "e1972.8"
        },
        {
            "name_short": "Normalized IoU",
            "name_full": "Normalized Intersection-over-Union evaluation",
            "brief_description": "An evaluation metric introduced in this paper that normalizes IoU of a predicted bounding box with a ground-truth polygon mask by the IoU between the tightest bounding box enclosing the mask and the mask itself, reducing bias against boxes on irregular masks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Normalized IoU metric",
            "model_description": "Computes s = IoU(bbox_pred, mask) / IoU(bbox_tight, mask) (described qualitatively); this normalization accounts for difficulty of covering irregular-shaped polygonal masks with rectangular boxes and yields fairer comparisons between box and point approaches.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "N/A (evaluation normalization method rather than a model grounding mechanism).",
            "representation_level": "evaluation-level normalization for bounding-box vs polygon mask comparison",
            "spatial_representation": "Uses bounding boxes and polygon masks",
            "embodied_task_type": "S1 and S2 evaluation in PIO",
            "embodied_task_name": "PIO evaluation",
            "visual_domain": "Applies to any PIO domains where masks are irregular (household/kitchen/robot/driving)",
            "performance_metric": "Normalized IoU itself; reported to improve fairness vs point-in-mask bias.",
            "performance_value": null,
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": "Addresses evaluation failure mode (bias) where point-in-mask favors point-specialized models and boxes are unfairly penalized on irregular masks.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Normalized IoU reduces evaluation bias between box and point outputs, enabling fairer S1/S2 comparisons across models with different output formats.",
            "uuid": "e1972.9"
        },
        {
            "name_short": "S1/S2/S3 Hierarchy",
            "name_full": "Three-stage hierarchical grounding framework (S1, S2, S3)",
            "brief_description": "A structured decomposition of embodied visual grounding into S1: referred-object localization, S2: task-driven grounding/affordance/contact localization, and S3: visual trace/trajectory prediction for spatiotemporal planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "S1/S2/S3 hierarchical evaluation stages",
            "model_description": "Defines progressively harder grounding tasks: S1 maps language references to visual entities; S2 identifies where to act based on affordance/context; S3 requires producing temporally coherent 2D trajectories starting from an end-effector position.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Conceptual decomposition used to design prompts, annotations, and evaluations; not a single model architecture.",
            "representation_level": "S1: object/part-level; S2: affordance/contact-level; S3: spatiotemporal trajectory-level (sequence of 2D points)",
            "spatial_representation": "S1: polygon masks and boxes; S2: points/boxes/polygons representing contact/affordance; S3: ordered 2D trajectories starting at annotated end-effector position",
            "embodied_task_type": "All: object localization, affordance grounding, and trajectory planning in embodied contexts",
            "embodied_task_name": "PIO S1/S2/S3",
            "visual_domain": "household, kitchen, driving, robotic manipulation",
            "performance_metric": "S1/S2: normalized IoU / point coverage; S3: human ratings and GPT-based scoring",
            "performance_value": null,
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Paper demonstrates that success in S1 and S2 is necessary but not sufficient for S3; many models fail at integrating single-target grounding into coherent visual traces.",
            "failure_mode_analysis": "Explicit: drop in performance from S1→S2 across models; S3 demands additional planning capabilities beyond per-frame grounding and reveals large gaps in VLM capabilities.",
            "domain_shift_handling": "Hierarchy used to surface domain-specific weaknesses; no explicit domain adaptation strategy supplied.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Decomposing embodied grounding into S1/S2/S3 reveals distinct failure modes: part-localization and affordance/contact are weak in S1/S2; integrating grounding into temporally coherent plans (S3) is a qualitatively different and harder challenge.",
            "uuid": "e1972.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Robopoint: A vision-language model for spatial affordance prediction for robotics.",
            "rating": 2
        },
        {
            "paper_title": "Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models.",
            "rating": 2
        },
        {
            "paper_title": "RT-1: Robotics transformer for real-world control at scale.",
            "rating": 2
        },
        {
            "paper_title": "Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies.",
            "rating": 2
        },
        {
            "paper_title": "Affordancenet: An end-to-end deep learning approach for object affordance detection.",
            "rating": 2
        },
        {
            "paper_title": "Gemini robotics: Bringing ai into the physical world.",
            "rating": 1
        },
        {
            "paper_title": "Qwen2.5-vl technical report.",
            "rating": 1
        },
        {
            "paper_title": "Towards testing and evaluating vision-language-action models for robotic manipulation: An empirical study.",
            "rating": 1
        }
    ],
    "cost": 0.02507475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>30 Sep 2025
30 Sep 20252E1CDF475FE1E48F40CF95EE6280B2A4arXiv:2509.25794v1[cs.CV]
Vision-Language Models (VLMs) have demonstrated impressive world knowledge across a wide range of tasks, making them promising candidates for embodied reasoning applications.However, existing benchmarks primarily evaluate the embodied reasoning ability of VLMs through multiple-choice questions based on image annotations -for example, selecting which trajectory better describes an event in the image.In this work, we introduce the Point-It-Out (PIO) benchmark, a novel benchmark designed to systematically assess the embodied reasoning abilities of VLMs through precise visual grounding.We propose a hierarchical evaluation protocol spanning three stages (S1: referred-object localization, S2: task-driven pointing, and S3: visual trace prediction), with data collected from critical domains for embodied intelligence, including indoor, kitchen, driving, and robotic manipulation scenarios.Extensive experiments with over ten state-of-theart VLMs reveal several interesting findings.For example, strong general-purpose models such as GPT-4o, while excelling on many benchmarks (e.g., language, perception, and reasoning), underperform compared to some open-source models in precise visual grounding; models such as MoLMO perform well in S1 and S2 but struggle in S3, where requires grounding combined with visual trace planning.</p>
<p>Figure 1: Unlike prior benchmarks that rely on indirect evaluation (a), Point-It-Out (PIO) directly assesses embodied reasoning (ER) by prompting VLMs to generate precise visual groundings-such as points, bounding boxes, or trajectories-in a hierarchical manner as shown in (b).To our knowledge, PIO is the first benchmark to offer pixel-level grounding for ER, spanning diverse embodied tasks across multiple real-world scenarios.</p>
<p>INTRODUCTION</p>
<p>Large-scale vision-language models (VLMs) Achiam et al. (2023); Bai et al. (2025); Deitke et al. (2024); Google DeepMind (2024); Kavukcuoglu (2025) inherit the broad world knowledge and powerful instruction-following abilities of large language models (LLMs) while grounding them in visual inputs.Because these models can describe what they see and reason about how the world works, they have quickly become the backbone of many embodied-AI systems: e.g.robot manipulation Yuan et al. (2024b); Duan et al. (2024); Brohan et al. (2023); Open X-Embodiment Collaboration (2023); Huang et al. (2023a;2024;2023b), navigation Zhang et al. (2024); Shah et al. (2023); Yin et al. (2024) and autonomous-driving Pan et al. (2024); You et al. (2024); Long et al. (2024); You et al. (2024).</p>
<p>Despite the rapid adoption, there are still challenges in understanding the capacities of embodied reasoning (ER) of VLMs, particularly in tasks requiring fine-grained visual grounding.Existing benchmarks primarily focus on input-side understanding and perception, typically using usually evaluate models with either multiple-choice questions (MCQs), e.g., "Which of these trajectories reaches the mug?"Du et al. (2024); Team et al. (2025); Azzolini et al. (2025), or closed-set skill selection from predefined actions Yang et al. (2025); Li et al. (2024), or language based planning Yang et al. (2025); Ahn et al. (2022); Zhang et al. (2025).They either assume that the correct answers are in a list of choices or only provide language-based planning.However, they overlook the crucial step of grounding the outputs back into the visual space, which completes the perception-action loop.Without this visual grounding, it is difficult to assess whether a model can truly reason and act in the physical world.Precise visual grounding is therefore essential for evaluating embodied reasoning in a realistic and interpretable manner.Such MCQs and language-based evaluation fail to examine the VLM's capability for fine-grained visual grounding and precise planning, which is critical for ER.</p>
<p>Claim: Current Embodied Reasoning benchmarks (Table 1) offer partial insights by focusing on grounded inputs or language-based planning, but they overlook the need for precise pixel-level groundinga crucial step for making VLMs interpretable and actionable interfaces in real-world embodied tasks (Section 3).</p>
<p>To bridge this gap, we propose to include visual grounding Yu et al. (2016); Mao et al. (2016); Nagaraja et al. (2016) as a natural complement to language-based planning in embodied reasoning benchmarks.Here, we adapt the definition of visual grounding from Wang et al. (2024b) into embodied reasoning tasks: by prompting models to localize pixel-space bounding boxes, points, or trajectories based on language-described tasks, we directly assess their accuracy against ground-truth human annotations, providing a clear measure of their embodied reasoning capabilities under precise visual grounding settings.In this paper, we focus on 2D pixel coordinates because precise 2D visual grounding is a scalable, cost-effective proxy task that isolates core embodied reasoning from control dynamics, enabling efficient evaluation., and driving scenes ; (ii) the total number of tasks or questions; (iii) whether the benchmark requires pixel-grounded outputs (e.g., bounding boxes or keypoints); (iv) the presence of multi-modal input (e.g., vision and language); and (v) the question type or expected model output format.While prior work predominantly focuses on language-based or multiple-choice evaluation formats, PIO provides fine-grained, human-annotated pixel-level signal across diverse embodied domains and task types (S1, S2, S3; see Section 3).We propose PIO, a benchmark designed to systematically evaluate VLMs' embodied reasoning through precise visual grounding tasks across diverse real-world settings.PIO employs a hierarchical evaluation protocol that decomposes embodied reasoning into three stages of increasing complexity: (S1) referred object localization, (S2) task-driven pointing, and (S3) visual trace prediction for spatiotemporal grounding.This structure mirrors the natural complexity of embodied tasks progressively from simple object detection to more challenging tasks such as affordance prediction, spatial reasoning, and task understanding.We further divide S1 and S2 into finer sub-categories, with all labels annotated by humans, providing rich signals for assessing the ER capabilities of VLMs.</p>
<p>Our benchmark includes data from four key domains critical for embodied intelligence: household rooms, kitchen environments, driving scenes, and robotic manipulation tasks.These scenarios require varying degrees of perceptual grounding, object understanding, spatial navigation, and physical interaction, which are core capabilities for any vision-language agent operating in the real world.</p>
<p>We conduct extensive experiments across a wide range of state-of-the-art VLMs, including general VLM e.g.GPT-4o Achiam et al. (2023), Claude-3.7 Anthropic (2025), Gemini 2.0-flash Kavukcuoglu (2025), Qwen2.5-VLBai et al. (2025), MoLMo-7B Deitke et al. (2024);some strong reasoning models such as GPT-o3 OpenAI (2025) and Gemini-2.5Kavukcuoglu (2025).Also, we test models that are specifically fine-tuned on grounding tasks e.g RoboRefer Zhou et al. (2025) and MolmoAct Lee et al. (2025).Models explicitly trained for grounding tasks, such as Roborefer, Qwen2.5-VL and Molmo, consistently outperform more general-purpose VLMs, including GPT-o3 and Claude-3.7.For all models, our results reveal that there are still large performance gaps in precise visual grounding within embodied reasoning settings, particularly in tasks requiring fine-grained localization and reasoning about object affordances or physical contact.</p>
<p>In conclusion, our contributions are listed as follows:</p>
<ol>
<li>We introduce precise visual grounding as a critical and scalable proxy for embodied reasoning, addressing the limitations of existing benchmarks that primarily rely on multiple-choice evaluations (Table 1).(2015) serve as a useful reference for evaluating the localization capabilities of VLMs, they primarily focus on basic object-level grounding in everyday scenes.Moreover, they lack coverage of embodied scenarios that require more nuanced forms of grounding critical for task-related understanding: e.g., task-driven localization, affordance grounding, and visual trace prediction.</li>
</ol>
<p>BENCHMARKING VISION-LANGUAGE MODELS FOR EMBODIED REASONING</p>
<p>As Vision-Language Models (VLMs) are increasingly applied to embodied tasks, a growing number of benchmark studies have been introduced to evaluate their reasoning capabilities.However, as shown in Table 1, most existing benchmarks either rely on indirect evaluation formats such as multiple-choice questions Azzolini et al. (2025); Du et al. (2024);Team et al. (2025), generate high-level language-based plans Yang et al. (2025); Li et al. (2024), or reduce actions to predefined skill sets Yang et al. (2025).Localization in robotic scenarios has been explored by Lu et al. (2023); Yuan et al. (2024b), but these efforts are limited to indoor environments and focus only on simple object localization or vacant space detection, where we will show is not enough as visual grounding for embodied tasks (Section 3).The most recent benchmark is RefSpatial Zhou et al. (2025), but it focuses only on spatial relations, including localizing objects and placement.In our work, we aim to construct a hierarchical benchmark to evaluate critical visual grounding abilities essential for embodied reasoning, which provides rich and meaningful signal for the ability of current models.</p>
<p>HIERARCHICAL DEFINITION OF VISUAL GROUNDING FOR EMBODIED REASONING</p>
<p>In this section, we present a three-stage hierarchical framework that captures essential visual grounding capabilities for embodied reasoning.The stages are arranged in increasing complexity, with each level building upon previous ones.For example, the hierarchy derived from a household-robot task is shown in Figure 2, with additional examples from other domains illustrated in Figure 3.For each stage, we (i) define the specific visual grounding abilities it encompasses, (ii) provide relevant subclasses and scenario-specific examples, and (iii) highlight its importance by identifying existing embodied policy approaches that depend on these capabilities, either directly or indirectly.(2023).In practice, language often includes additional constraints to disambiguate the target object, such as spatial cues, color, or material properties.Moreover, references may vary in granularity, ranging from whole objects to object parts Wang et al. (2024a).For example, in the household task shown in Figure 2, "the middle pile of paper cups" includes a location-based constraint, while "the handle of the left cup" involves both part-level and spatial restrictions.We further divide S1 into three main categories: object without ambiguity (single object), object with constraints, and object part (See Figure 4 for examples of different subclasses).</p>
<p>S1 represents the most fundamental visual grounding capability required for embodied tasks: mapping language references to visual entities.It is an essential skill for nearly all open-vocabulary, languageguided policies, e.g.RT-series Brohan et al. (2022;2023); Open X-Embodiment Collaboration (2023), OpenVLA Kim et al. ( 2025), VIMA Jiang et al. (2022) andSayCan Ahn et al. (2022).As these models must first localize the referred object implicitly or explicitly before reasoning about or interacting with it.</p>
<p>S2: TASKED-DRIVEN GROUNDING</p>
<p>S2 goes beyond the explicit reference grounding in S1 by moving to a task-driven visual grounding: determining which object or part of an object is relevant for the task and pinpointing where to interact with it.Unlike S1, the entity to be localized may not be explicitly mentioned in the instruction, so it needs reasoning over target object and understanding of the action affordance.</p>
<p>S2 challenges the model to recognize action-relevant locations such as handles, buttons, or lids-based on contextual cues, even when these are not directly referred to in the instruction.For example, the command "open the top drawer" requires the model to (1) identify which drawer is being referred to (as in S1), and (2) localize the appropriate part to interact with (e.g., the handle).In another example, when given "I'm hungry, help me," the model must infer that a visible food item should be retrieved and localize where to grasp it.Thus, the essence of S2 lies in perceiving the affordances of objects and leveraging the task context to ground where to act.We further divide S2 into three categories: affordance, contact, and recommendation/safety grounding (See Figure 4).</p>
<p>Stage S2 highlights how embodied visual grounding differs from standard computer-vision grounding.</p>
<p>A model that excels here shows a basic sense of how to interact with the physical world, links its perception to the task, and reasons simply about where to act.S2 itself captures visual affordance understanding, a key ingredient for general-purpose manipulation Do et al. (2018); Mo et al. (2021); Huang et al. (2023a).Beyond that, it underpins many modern, versatile robot policies, e.g.VLAs Kim et al. ( 2025); Open X-Embodiment Collaboration (2023): even when the policy is not asked to output an affordance map, the robot still must know the right spot to act on.</p>
<p>S3: TASK-DRIVEN VISUAL TRACE PREDICTION</p>
<p>Building on the capabilities developed in S1 and S2, stage S3 assesses if VLM can plan accordingly to complete the instructed tasks.Given a task, the model must produce a coarse 2D visual trace</p>
<p>The license plate of the [car] in front of me</p>
<p>The [road sign] of my right lane.</p>
<p>Which [lane] should I switch to to turn right?</p>
<p>Which [car] will be in front of me if I switch to the right lane?</p>
<p>The [utensil] to the left of the white plate.</p>
<p>The [utensil] to the right of the white plate.</p>
<p>What may I use to hold some water to drink?</p>
<p>Where should I act to use the fork?</p>
<p>The trajectory to pick up the cup and put it on the plate.</p>
<p>The [carrot] on the table.</p>
<p>What can I use to clean the sink?</p>
<p>Where's my right hand holding the vegetable?</p>
<p>The trajectory to pick up the knife and put it on the cut board.</p>
<p>The trajectory to turn-off the [faucet].</p>
<p>The [soda can] on the table.</p>
<p>Where should I act to open the [top drawer]?</p>
<p>Which object is about to fall from the table?</p>
<p>The trajectory to pick up the object that is about to fall from the table.</p>
<p>S1: Localize Referred Object</p>
<p>Different constraints e.g.granularity, location, color, material</p>
<p>S2: Task-Driven Grounding</p>
<p>Rely on S1, task-driven reasoning e.g.affordance, recommendation, prediction  Where to pick up the black pot?</p>
<p>S3: Trajectory Prediction</p>
<p>Where am I holding the spatula?</p>
<p>Which object is about to fall from the table?</p>
<p>What is the gripper going to grasp?</p>
<p>Where can I pour the water into?</p>
<p>The white car in front of me.</p>
<p>The bowl on the upper left corner.</p>
<p>The lowest key of the piano.</p>
<p>The right wheel of the black car.</p>
<p>The backrest of the closest chair.</p>
<p>BENCHMARK CURATION, CANDIDATE MODELS AND EVALUATION METRICS</p>
<p>Our benchmark is made up of datapoints like (S, subclass, ⟨Img⟩, ⟨question⟩, ⟨mask⟩ , where S ∈ {S 1 , S 2 , S 3 }.subclass defines which specific subclass in the stage this datapoint belongs to.The left three attributes represent the input image, the question (description of the task), and the ground-truth polygon-based segmentation mask for the question.(2018).From these datasets, we extract image frames and select high-quality sample (e.g.filtering out those with motion blur or unclear visuals) o build our benchmark.</p>
<p>Annotation: For the first two stages, each datapoint is manually annotated using standard polygonbased segmentation tools Dwyer et al. (2024).Guided by example prompts and a predefined set of stages and subclasses for each dataset, human annotators generate a natural language question, assign the appropriate stage and subclass, and provide an accurate polygon-based mask as the ground-truth answer.To reduce potential bias in language descriptions, we use GPT-4o Achiam et al. (2023) to rewrite prompts in a clearer and more formal manner, helping to minimize ambiguity and errors.</p>
<p>For S1 and S2, we collect 501 question-answer (QA) pairs across five diverse datasets (around 230 for S1 and 270 for S2): Where2Place, Ego4D-EpicKitchen, BDD100K, AgiBot, and RT-1.For S3, we extract frames on the AgiBot, DROID, and RT-1 datasets, collecting 100 questions targeting visual trace prediction tasks for robotic arms in images.In addition to the question and the predicted visual trace, we annotate the starting 2D position of the robot arm to guide the model with the initial configuration.We put more details and more examples about the dataset in the appendix.</p>
<p>Candidates Models</p>
<p>The candidate models can be categorized by their output format.While most general instruction-following vision-language models are capable of both point and bounding box predictions, they tend to perform significantly better on one format over the other according to Yuan What can tell me whether it is allowed to turn right here?</p>
<p>Where should the right gripper grip to hand over the red chip?</p>
<p>Where is the right hand holding the pot?</p>
<p>Where can I act on to open the stove?</p>
<p>Where can I act on to pick up the rightmost mug?</p>
<p>The drawer on the top layer.(VLMs that Output Points): Recent models try to do fine-tuning based on point-based grounding due to its scalability Deitke et al. (2024).We include Molmo-7B-D Deitke et al. (2024) as a general-purpose-pointing model, and RoboPoint Yuan et al. (2024b), which is carefully finetuned using robotic data.For these models, we prompt them to output one or a few points that best correspond to the target described in the language input.All prompt templates used for evaluation are provided in the Appendix C.</p>
<p>Evaluation Metrics To ensure fair comparison across different output formats (points vs. bounding boxes), we propose a normalized IoU metric for S1 and S2 that accounts for the difficulty of covering irregular masks with rectangular boxes.This improves upon the biased point-in-mask evaluation used in prior work Yuan et al. (2024b).For S3, we evaluate visual trace quality using two methods: human ratings on a 1-5 scale and GPT-o4-mini based assessments guided by predefined criteria (e.g. the overall direction of the trajectory, the keypoint coverage, and the task feasibility).Further implementation details can be found in the Appendix D.</p>
<p>EXPERIMENTS</p>
<p>In this section, we present several key insights derived from quantitative evaluations and visualizations based on testing different VLMs on the PIO benchmark.We organize this section into blocks of key findings, each followed by supporting evidence and analysis.</p>
<p>Finding 1: For S1 and S2, models that incorporate explicit grounding supervision such as RoboRefer, MoLMO-7B-D, Gemini-2.5-Pro,and Qwen-2.5-VLconsistently achieve the highest, outperforming more general-purpose VLMs such as GPT-4o and Claude-3.7.This underscores the importance of grounding data when precise spatial reasoning is required .Figure 5 illustrates the overall performance of all candidate models on S1 (Referred Object Localization) and S2 (Task-Driven Localization).RoboRefer, MoLMO and Qwen consistently outperform other models across both tasks.Within the GPT series, GPT-o3 achieves the best results, likely due to its enhanced reasoning capabilities, whereas smaller variants such as GPT-o4 mini and GPT-4o mini perform noticeably worse.In the Gemini family, Gemini-2.5Pro significantly outperforms both Gemini-2.0Flash and Gemini-2.5Flash, and also slightly surpasses RoboPoint.</p>
<p>Although GPT-4o and Claude-3.7 have demonstrated strong performance in language-based embodied reasoning on prior benchmarks Li et al. (2024); Yang et al. (2025), they fall short on PIO where previse visual grounding is needed.</p>
<p>Finding 2: (i) Across all subclasses, every model exhibits a clear performance drop from S1 to S2. (ii) Two critical embodied skills suffer most: in S1 they often miss the correct object part to point to, and in S2 they struggle with affordance and contact prediction.</p>
<p>Looking at the performance of different subclasses in S1 and S2 (Figure 6), we find that most models do not show much performance drop on tasks that only require simple language reasoning-such as "object with restriction" in S1 and "recommendation" in S2.This is likely because the models can handle basic reasoning before predicting the bounding box or point.</p>
<p>However, when it comes to more detailed tasks like localizing object parts, most models perform significantly worse (left side of Figure 6).Even strong models like Qwen Bai et al. (2025) and MoLMO Deitke et al. ( 2024) score below 0.5.In S2, while most models handle recommendation well, they struggle with affordance and contact prediction.Although MoLMO is the best in affordance, it still scores below 0.4.</p>
<p>These findings suggest that more attention should be paid to improving the grounding abilities of models, especially for fine-grained tasks such as object part localization, affordance, and contact.These are crucial skills if vision-language models are to truly act as grounded agents capable of interacting with the real world.</p>
<p>Finding 3: S3 requires models to integrate single-target grounding into coherent visual trace generation.While S1 and S2 are necessary prerequisites, they are not sufficient for a model to succeed in S3.Gemini-2.5-proshows promising results in S3 and also performs well in S1 and S2.In contrast, MoLMO and Qwe-2.5-VL, the top-performing models in S1 and S2, fail in S3.</p>
<p>To evaluate the performance in S3 (visual trace prediction), we visualize model outputs together with human ratings and GPT-based assessments in Figure 8. Unlike S1 and S2, where fine-tuned models such as MoLMO and Qwen show strong grounding capabilities, these models underperform in visual trace generation.This suggests that although they can localize specific targets effectively, they struggle to extend this capability into multi-step, temporally coherent planning.</p>
<p>In contrast, we find that Gemini-2.5-ProKavukcuoglu (2025) and GPT-o3 Lee et al. (2025) (slightly outperforming GPT-4o) generate more reasonable trajectories that (1) follow the correct overall direction and (2) successfully reach the target objects.Gemini-2.5-Proachieves almost 4 out of 5 in Figure 8 (right), it can be attributed to the inclusion of embodied data and grounding data in the strong model Team et al. (2025).This indicates that strongly pre-trained VLMs such as GPT-based models excel at handling complex tasks that involve both grounding and planning, even without fine-tuning over 2D trajectory data.In comparison, smaller models like MoLMO-7B and Qwen, while effective at isolated grounding tasks, struggle to jointly perform grounding and visual trace planning, limiting their utility in more integrated embodied scenarios Li et al. (2025b); Gu et al. (2023); Lee et al. (2025).</p>
<p>Guideline for Model Users: For task does not require action generation (e.g.pick and place based on points), prioritize models that perform well on S1 and S2.If the goal is to train a robot policy using a VLM backbone, models that perform well on S3 -even if weaker on S1 / S2, could be better candidates.Furthermore, users might consider adding grounding data and fine-tuning to further improve performance for different stages.</p>
<p>CONCLUSIONS</p>
<p>In this work, we introduce PIO, a new benchmark designed to evaluate vision-language models (VLMs) in precise visual grounding and embodied reasoning tasks.By decomposing the evaluation into three stages, referred object localization (S1), task-driven localization (S2), and visual trace prediction (S3), our benchmark reveals fine-grained insights into embodied reasoning ability of different VLMs in regard of precise grounding.Through extensive quantitative and qualitative evaluations of over ten state-of-the-art VLMs, we uncover several key findings: models fine-tuned with grounding supervision excel at S1 and S2, but struggle with S3; in contrast, strong generalist models perform better in multi-step reasoning and planning tasks, despite weaker spatial precision.</p>
<p>A DETAILS OF BENCHMARK CURATION</p>
<p>We collect frames from the following datasets:</p>
<p>• Driving  For S3, we concentrate on robot manipulation, as it is the most suitable domain for visual trace generation.We collect roughly 50 tasks from three datasets: AgiBot Bu et al. ( 2025), DROID Khazatsky et al. (2024), and RT-1 Brohan et al. (2022).Each task is posed to the models evaluated as a question paired with an image; examples are given in Table 4.We did not include ground-truth trajectories because robot-manipulation problems are inherently multimodal: A single objective can be achieved through many valid trajectories.Providing a canonical answer would be potentially misleading.</p>
<p>B TESTED MODELS</p>
<p>We show all candidate models used in this paper in Table 5.For open-sourced models we ran them locally on one Nvidia-A6000 GPU, for close-sourced models we use the provided API.</p>
<p>-Please only output one bounding box !</p>
<p>S1 S2 Prompt -MoLMO</p>
<p>Point to one or more points that best correspond to the following question: {question} Note: The object referenced in the question always exists, so do not respond with "I don't know."/ "There are none"</p>
<p>Please DO NOT output any other texts besides points!</p>
<p>S1 S2 Prompt -RoboPoint</p>
<p>Please locate several points better fit the following question: { question}, Your answer should be formatted as a list of tuples, i.e. [(x1, y1), ( x2, y2), ...], where each tuple contains the x and y coordinates of a point satisfying the conditions above.The coordinates should be between 0 and 1, indicating the normalized pixel locations of the points in the image.</p>
<p>S3 Prompt -GPT</p>
<p>You are an agent to help me generate rough 2D visual trace to guided the robot to compelte the tasks.Given a observation image, you are told the current 2D location of the end-effector (marked in red in the image).You need to output a sequence of 2D points starting from the current position as the rough trajectory to complete the task.</p>
<h1>Return Format class ReturnedTrajectory(): explanations: str # analyzing process, e.g.decompose the task trajectory: [(float, float)] # normalized points starting from red marker # Some Tips:</h1>
<p>(1) take care of the contact point when interacting with the object, e.</p>
<p>S3 Prompt -Qwen</p>
<p>Given the task description and the image observation, you need to output a sequence of 2D points that can complete the task.</p>
<p>You will also be offered the starting points of the current end-effector (annotated in red marker in the given image) and its accurate 2D position.You need to return 5~10 points (x,y) as trajectory starting from the current position.Do not return anything but points!-Do not output "there are none", the trajectory always exists! ## Task Description: {task} ## Current End-effector 2D position (marked in red in the image): {eepose}</p>
<p>Please return your answer here, the format should be: {'explanation':'...', 'trajectory': [(x1, y1), (x2, y2), ...]} # Answer:</p>
<p>S3 Prompt -MoLMO</p>
<p>Based on the input image, points to a sequence of 2D points in format of html points as a rough trajectory to complete the following task: { task}, the starting point is annotated in red in the image, and the position is {eepose}.Your output trajectory should start from it.</p>
<p>Your answer is:  2024b), the authors evaluate performance using the percentage of predicted points that fall within annotated masks.For bounding boxes, they uniformly sample points within the box to enable comparison.Although it may not affect the final ranking in Yuan et al. (2024b), the metric itself is flawed: it introduces a bias toward point-based methods, the irregular shape of the mask makes it challenging for bounding boxes to achieve high scores, as they cannot fully cover the masked area.In our experiments for S1 and S2, we propose a normalized IoU metric to replace the evaluation method used in Yuan et al. (2024b).Specifically, we normalize the IoU score between the predicted bounding box and the ground-truth mask using a reference term: the IoU between the tightest bounding box enclosing the ground-truth mask and the mask itself.The final score is defined as: s = IoU(bbxpred,mask)</p>
<p>S3 AutoScore Prompt</p>
<p>IoU (bbxtight,mask) .This normalization accounts for the inherent difficulty of tightly covering irregular-shaped masks with rectangular boxes, allowing for a fairer comparison.</p>
<p>S3:</p>
<p>To evaluate the quality of the proposed trajectories generated in S3, we employ two evaluation metrics.First, human annotators are asked to score the outputs of various VLMs on a scale from 1 to 5. Second, we leverage GPT-o4-mini to assess and rate the trajectories based on prompts guided by predefined evaluation criteria.The prompts we used are in Section C.</p>
<p>E MORE EXAMPLES OF S1 S2 PREDICTIONS</p>
<p>We show more examples in Figure 9 for underperformed models like GPT-4o, GPT-4o-mini, Claude-3.7,Gemini-2.0-flash,GPT-4o and in Figure 10 for top rated models e.g.Qwen, Molmo and Gemini-2.5-pro.</p>
<p>F MORE EXAMPLES OF S3 RESULTS</p>
<p>In Figure 11, we show more model prediction visualizations of trajectory prediction in S3.</p>
<p>the [left cup].Where to act to open the [top Drawer] of the rightmost storage cabinet?The point to pick up the green [cup].I am hungry, what can help me?Open the second [drawer] from the bottom of the leftmost storage cabinet.</p>
<ol>
<li>1
1
S1: REFERRED OBJECT LOCALIZATION Stage S1 focuses on identifying and localizing the specific objects in a scene as referenced by the language instruction.S1 aligns closely with referring expression comprehension (REC) Xiao et al. (2024) tasks commonly studied in the literature Xiao et al. (2024); Kazemzadeh et al. (2014); Lu et al.</li>
</ol>
<p>Combine S1 and S2 to predict a trajectory to complete certain tasks</p>
<p>Figure 3 :
3
Figure 3: More Examples for Three-Stages Grounding across Embodied Tasks: We illustrate more examples across driving, kitchen, and robotic domains that align with our three-stage hierarchy.</p>
<p>Figure 4 :
4
Figure 4: Examples and Distributions of S1 and S2 Subclasses: here we show examples of subclasess for S1 (object w/o ambiguity, object part, and object with constraints in e.g.locations, color) and S2 (affordance, prediction, safety, contact and recommendation); and also the % of them in the each stage.</p>
<p>ForFigure 5 :Figure 6 :
56
Figure 5: Performance on S1 and S2 for Different VLMs.(Left) Model scores on S1 and S2 tasks.RoboRefer-SFT-8B, MoLMO-7B, Gemini-2.5-Pro,and Qwen-2.5-VLsignificantly outperform other models.(Right) Average scores across S1 and S2 of different model in four distinct scenarios.</p>
<p>Figure 7 :
7
Figure 7: Example predictions from different models on S1 and S2 tasks: here we show some visualization of predicted bounding boxes and points of different VLMs for S1 and S2.More visualizations of more models are put in the Appendix 9 and 10 .et al. (2024b).For instance, MoLMO Deitke et al. (2024) and RoboPoint Yuan et al. (2024b) excel at point prediction but struggle with bounding box outputs, whereas GPT-4o Achiam et al. (2023) more easily produces bounding boxes than points.(VLMs that Output BoundingBoxes): We have strong black-box models: GPT-4o/4omini Achiam et al. (2023), GPT-o3/o4-mini OpenAI (2025) (API version), Claude-3.7-SonnetAnthropic (2025), Gemini-2.0-FlashGoogle DeepMind (2024), Gemini-2.5-Flash/ProKavukcuoglu (2025); and also strong close sourced model like Qwen-2.5-VL-32B-InstructBai et al. (2025).For these models we prompt them to output only one bounding box that best match the location described in the language descrption.</p>
<p>Figure 8 :
8
Figure 8: Visualizations and Scores on S3: We present visualizations of S3 visual trace predictions from different models.The scores are shown on the right.Gemini-2.5-Pro,MolmoAct and GPT-o3 outperform other models, while MoLMO and Qwen, despite their strong performance on S1 and S2, struggle with temporal visual trace prediction compared to stronger general-purpose models.</p>
<p>-</p>
<p>BDD-100k Yu et al. (2018): front camera view of driving scenes, CC BY-NC • Household -Where2Place Yuan et al. (2024b): object placement in indoor scenes, Apache 2.0 • Kitchen -EPIC-Kitchens Damen et al. (2022): egocentric kitchen interactions, CC BY-NC 4.0 • Robot Manipulation -RT-1 Brohan et al. (2022): real-world robot arm manipulation, CC BY-NC-SA 4.0 -DROID Khazatsky et al. (2024): real-world robot arm manipulation, MIT License -AgiBot Bu et al. (2025): real-world humanoid robot manipulation, CC BY-NC-SA 4.0 A.1 DATA COLLECTION FOR S1 AND S2We use RoboFlowDwyer et al. (2024) to collect human-annotated masks in polygonal format, using images from BDD100K, Where2Place, EPIC-Kitchens, RT-1 and AgiBot.The initial prompts for S1 and S2 are generated by human annotators following a structured hierarchy and guided by subclass examples.</p>
<p>g .handle of cup and bottle (2) the trajectory should faithfully reflect the scale on the 2D pixel space (3) you can first detect the important point and put the reasoning process in the explanations # Task Description {task} # Current End-effector 2D position (marked in red in the image) {eepose} # You Answer as 2D Trajectory to Complete the Task</p>
<h1></h1>
<p>Image ** : Shows the current scene.-The task description is displayed at the top of the image.-The trajectory originates at the red point (robot gripper) and gradually shifts to blue as it progresses.trajectory -nonsensical path that cannot accomplish the task .| | 2 | Direction shows faint promise, but overall execution is unsatisfactory.| | 3 | General direction and keypoints are reasonable, yet important flaws remain.| | 4 | Largely correct; minor inaccuracies at some keypoints.| | 5 | Excellent -visits all required keypoints and should successfully complete the task.| S1 and S2: It is straight-forward to compare VLMs that output the same format (e.g.use IoU for boundingboxes), it need more careful design to handle cases where models have different formats of outputs.In Yuan et al. (</p>
<p>Figure 9 :
9
Figure9: Model prediction visualization for GPT-4o, GPT-4o-mini, Claude-3.7,Gemini-2.0-flash,GPT-4o, which underperform other models.</p>
<p>Figure 10 :
10
Figure 10: Model prediction visualization for top rated models, e.g.Qwen, Molmo and Gemini-2.5pro.</p>
<p>Figure 11 :
11
Figure 11: More model prediction visualizations of S3</p>
<p>Table 1 :
1
Comparison of PIO with Existing Embodied Reasoning Benchmarks: We compare benchmarks across five dimensions: (i) the range of scenarios covered, where icons denote robot manipulation , household environments , kitchens</p>
<p>A Hierarchical Framework for Visual Grounding in Embodied Reasoning.We propose a three-stage progression: S1 (object localization) localize objects explicited referred to in the text, with some conditions like granularity and appearance; S2 (task-driven grounding) builds on S1 to infer locations used in specific task, which may not be explicitly referred to in the text ; and S3 (visual trace prediction) combines S1 and S2 to generate executable motion plans.Underlined text denotes the referred object that needs to be localized (S1), while yellow highlights indicate task-contexts in task-oriented reasoning (S2/S3).
Class Ans(Object):Explanations: …Ans: …ProgressiveCalculate ScoresS1S2Figure 2: 2 BACKGROUND AND RELATED WORKS2.1 VISUAL GROUNDING OF VISION-LANGUAGE MODELSBefore large-scale Vision-Language Models (VLMs) emerged, visual grounding research is focusedon referring-expression comprehension (REC) Xiao et al. (2024); Yu et al. (2016); Mao et al. (2016).Pioneering datasets such as ReferItGame Kazemzadeh et al. (2014) and RefCOCO Yu et al. (2016)framed the task as localizing the image region that matches a unstructured language description.Modern VLMs have pushed REC performance to new heights thanks to their strong multi-modalunderstanding and instruction-follownig, making them much more generalized in REC tasks. Manyrecent VLMs now build visual grounding directly as an important training objectives: Kosmos-2 Penget al. (2023), Qwen-VL Bai et al. (2025), and Gemini Google DeepMind (2024); Kavukcuoglu (2025)are trained with bounding boxes annotations, whereas MoLMo Deitke et al. (2024) and RoboPointYuan et al. (2024b) specialise in point-based localization. While REC datasets Kazemzadeh et al.(2014); Yu et al. (2016); Mao et al. (2016); Wang et al. (2024a); Liu et al. (2024); Plummer et al.</p>
<p>Table 2 :
2
Table2summarizes the number of annotated images and question-answer (QA) pairs across different datasets.Also, Table3shows examples of questions of different stages and sub-classes.Number of annotated images and QA pairs for each dataset used in S1 and S2.
Dataset#Images #QAsWhere2Place50157Epic-Kitchen50108BDD-10k51135AgiBot3645RT-15456Total241501A.2 DATA COLLECTION FOR S3
. We introduce PIO, a three-stage hierarchical benchmark (Section
) spanning referred-object localization, task-driven pointing, and visual trace prediction. The benchmark includes over 600+ human-annotated datapoints across diverse embodied scenarios (Section
).3. We evaluate over ten candidate vision-language models (VLMs) and uncovering key limitations in their precise visual grounding capabilities for embodied reasoning. Our findings highlight the need for targeted data to improve model grounding-aware capabilities (Section
).
C PROMPTSHere we provide all the prompts used in this paper including:• S1, S2 prompts for different models • S3 prompts for selected models • S3 auto evaluation prompts
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Claude 3.7 sonnet and claude code. Anthropic, February 2025. February 24, 2025</p>
<p>Cosmos-reason1: From physical common sense to embodied reasoning. Alisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Francesco Ferroni, Rama Govindaraju, arXiv:2503.155582025arXiv preprint</p>
<p>Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, arXiv:2502.13923Qwen2.5-vl technical report. 2025arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Karol Hausman, arXiv:2212.068172022arXiv preprint</p>
<p>Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Danny Driess, arXiv:2307.15818arXiv:2503.066692023. 2025arXiv preprintRt-2: Vision-language-action models transfer web knowledge to robotic control</p>
<p>Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray, 10.1007/s11263-021-01531-2International Journal of Computer Vision (IJCV). 1302022</p>
<p>Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, arXiv:2409.171462024arXiv preprint</p>
<p>Affordancenet: An end-to-end deep learning approach for object affordance detection. Thanh-Toan Do, Anh Nguyen, Ian Reid, IEEE international conference on robotics and automation (ICRA). 2018. 2018IEEE</p>
<p>Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models. Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, Zhongyu Wei, arXiv:2406.057562024arXiv preprint</p>
<p>Manipulate-anything: Automating real-world robots using vision-language models. Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani, Dieter Fox, Ranjay Krishna, arXiv:2406.189152024arXiv preprint</p>
<p>Computer vision. B Dwyer, J Nelson, T Hansen, 2024version 1.0) [software</p>
<p>Introducing gemini 2.0: Our new ai model for the agentic era. Google Deepmind, -december-2024/, December 2024</p>
<p>Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, arXiv:2311.019772023arXiv preprint</p>
<p>Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei, Voxposer, arXiv:2307.05973Composable 3d value maps for robotic manipulation with language models. 2023aarXiv preprint</p>
<p>Grounded decoding: Guiding text generation with grounded models for robot control. Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey Levine, Karol Hausman, Brian Ichter, arXiv:2303.008552023barXiv preprint</p>
<p>Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, Li Fei-Fei, arXiv:2409.01652Rekep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation. 2024arXiv preprint</p>
<p>Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, arXiv:2502.21257A unified brain model for robotic manipulation from abstract to concrete. 2025arXiv preprint</p>
<p>Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, Linxi Fan, Vima, arXiv:2210.03094General robot manipulation with multimodal prompts. 202226arXiv preprint</p>
<p>Koray Kavukcuoglu, Gemini 2.5: Our most intelligent ai model. March 2025</p>
<p>ReferItGame: Referring to objects in photographs of natural scenes. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara L Berg, EMNLP. 2014</p>
<p>Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, arXiv:2403.12945A large-scale in-the-wild robot manipulation dataset. 2024arXiv preprint</p>
<p>Openvla: An opensource vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan P Rafailov, Foster, Quan Pannag R Sanketi, Thomas Vuong, Benjamin Kollar, Russ Burchfiel, Dorsa Tedrake, Sergey Sadigh, Percy Levine, Chelsea Liang, Finn, Proceedings of The 8th Conference on Robot Learning. The 8th Conference on Robot LearningPMLR2025270</p>
<p>Molmoact: Action reasoning models that can reason in space. Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jing Zhang, Y Raymond Wang, S Lee, W Han, arXiv:2508.07917Aug 2025arXiv preprint</p>
<p>Embodied agent interface: Benchmarking llms for embodied decision making. Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Erran Li Li, Ruohan Zhang, Advances in Neural Information Processing Systems. 202437</p>
<p>Yanbang Li, Ziyang Gong, Haoyang Li, Haoyang Li, Xiaoqi Huang, Haolan Kang, Guangping Bai, Xianzheng Ma, arXiv:2505.00693Robotic visual instruction. 2025aarXiv preprint</p>
<p>Yi Li, Yuquan Deng, Jesse Zhang, Joel Jang, Marius Memmel, Raymond Yu, Caelan Reed Garrett, Fabio Ramos, Dieter Fox, Anqi Li, arXiv:2502.05485Hierarchical action models for open-world robot manipulation. 2025barXiv preprint</p>
<p>Vlm-mpc: Vision language foundation model (vlm)-guided model predictive controller (mpc) for autonomous driving. Junzhuo Liu, Xuzheng Yang, Weiwei Li, Peng Wang, arXiv:2409.14750arXiv:2408.04821Finecops-ref: A new dataset and task for fine-grained compositional referring expression comprehension. Keke Long, Haotian Shi, Jiaxi Liu, Xiaopeng Li, 2024. 2024arXiv preprint</p>
<p>Vl-grasp: a 6-dof interactive grasp policy for language-oriented objects in cluttered indoor scenes. Yuhao Lu, Yixuan Fan, Beixing Deng, Fangfu Liu, Yali Li, Shengjin Wang, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023</p>
<p>Generation and comprehension of unambiguous object descriptions. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, Kevin Murphy, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2016</p>
<p>Where2act: From pixels to actions for articulated 3d objects. Kaichun Mo, Leonidas J Guibas, Mustafa Mukadam, Abhinav Gupta, Shubham Tulsiani, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Modeling context and ambiguity for referring expression understanding. Varun Kumar, Rao Nagaraja, Vlad I Morariu, Larry S Davis, ECCV. 2016</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models. arXiv:2310.088642023arXiv preprint</p>
<p>Introducing openai o3 and o4-mini. Openai, April 2025</p>
<p>Vlp: Vision language planning for autonomous driving. Chenbin Pan, Burhaneddin Yaman, Tommaso Nesti, Abhirup Mallik, Alessandro G Allievi, Senem Velipasalar, Liu Ren, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Kosmos-2: Grounding multimodal large language models to the world. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei, 10.48550/arXiv.2306.14824arXiv:2306.148242023arXiv preprint</p>
<p>Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. Liwei Bryan A Plummer, Chris M Wang, Juan C Cervantes, Julia Caicedo, Svetlana Hockenmaier, Lazebnik, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. Dhruv Shah, Błażej Osiński, Sergey Levine, Conference on robot learning. PMLR2023</p>
<p>Motion before action: Diffusing object motion as manipulation condition. Yue Su, Xinyu Zhan, Hongjie Fang, Yong-Lu Li, Cewu Lu, Lixin Yang, arXiv:2411.096582024arXiv preprint</p>
<p>Saminda Gemini Robotics Team, Joshua Abeyruwan, Jean-Baptiste Ainslie, Montserrat Alayrac, Travis Gonzalez Arenas, Ashwin Armstrong, Robert Balakrishna, Maria Baruch, Michiel Bauza, Blokzijl, arXiv:2503.20020Gemini robotics: Bringing ai into the physical world. 2025arXiv preprint</p>
<p>Unveiling parts beyond objects: Towards finer-granularity referring expression segmentation. Wenxuan Wang, Tongtian Yue, Yisi Zhang, Longteng Guo, Xingjian He, Xinlong Wang, Jing Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024a</p>
<p>Towards testing and evaluating vision-language-action models for robotic manipulation: An empirical study. Zhijie Wang, Zhehua Zhou, Jiayang Song, Yuheng Huang, Zhan Shu, Lei Ma, arXiv:2409.128942024barXiv preprint</p>
<p>Any-point trajectory modeling for policy learning. Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou, Yang Gao, Pieter Abbeel, arXiv:2401.000252023arXiv preprint</p>
<p>Linhui Xiao, Xiaoshan Yang, Xiangyuan Lan, Yaowei Wang, Changsheng Xu, arXiv:2412.20206Towards visual grounding: A survey. 2024arXiv preprint</p>
<p>Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, arXiv:2502.095602025arXiv preprint</p>
<p>Navigation with vlm framework: Go to any language. Zecheng Yin, Chonghao Cheng, arXiv:2410.027872024arXiv preprint</p>
<p>Junwei You, Haotian Shi, Zhuoyu Jiang, Zilin Huang, Rui Gan, Keshu Wu, Xi Cheng, arXiv:2408.09251Xiaopeng Li, and Bin Ran. V2x-vlm: End-to-end v2x cooperative autonomous driving through large vision-language models. 2024arXiv preprint</p>
<p>Bdd100k: A diverse driving video database with scalable annotation tooling. Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike Liao, Vashisht Madhavan, Trevor Darrell, arXiv:1805.04687201826arXiv preprint</p>
<p>Modeling context in referring expressions. Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, Tamara L Berg, ECCV. 2016</p>
<p>Chengbo Yuan, Chuan Wen, Tong Zhang, Yang Gao, arXiv:2401.11439General flow as foundation affordance for scalable robot learning. 2024aarXiv preprint</p>
<p>Robopoint: A vision-language model for spatial affordance prediction for robotics. Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, Dieter Fox, arXiv:2406.107212024barXiv preprint</p>
<p>Navid: Video-based vlm plans the next step for vision-andlanguage navigation. Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, He Wang, arXiv:2402.158522024arXiv preprint</p>
<p>Embodied-reasoner: Synergizing visual search, reasoning, and action for embodied interactive tasks. Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, arXiv:2503.216962025arXiv preprint</p>
<p>Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé, Iii , Andrey Kolobov, Furong Huang, Jianwei Yang, arXiv:2412.103452024arXiv preprint</p>
<p>Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, arXiv:2506.04308Towards spatial referring with reasoning in vision-language models for robotics. 2025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2731 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2731</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2731</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-260438426</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.01872v1.pdf" target="_blank">Thespian: Multi-Character Text Role-Playing Game Agents</a></p>
                <p><strong>Paper Abstract:</strong> Text-adventure games and text role-playing games are grand challenges for reinforcement learning game playing agents. Text role-playing games are open-ended environments where an agent must faithfully play a particular character. We consider the distinction between characters and actors, where an actor agent has the ability to play multiple characters. We present a framework we call a thespian agent that can learn to emulate multiple characters along with a soft prompt that can be used to direct it as to which character to play at any time. We further describe an attention mechanism that allows the agent to learn new characters that are based on previously learned characters in a few-shot fashion. We show that our agent outperforms the state of the art agent framework in multi-character learning and few-shot learning.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2731.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2731.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Thespian</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Thespian agent (Multi-Character Thespian Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single RL actor-critic agent that can emulate multiple characters via learned soft character prompts and character-specific action logits, and can learn new blended characters few-shot using an attention module over pretrained character logits while keeping core weights frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Thespian agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Built on the KG-A2C Advantage-Actor Critic framework; the core produces per-character stacks of action logits and per-character value estimates. It adds learned soft character prompt embeddings (one per character) concatenated with state embeddings to produce character-specific internal states, and a few-shot Thespian Attention module that (when learning a new character) learns to attend and linearly combine the frozen pretrained characters' raw action logits to produce actions for the new character.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>LIGHT (custom map)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>A grounded text-based role-playing environment with NPCs, objects and locations; experiments use handcrafted maps with thief- and adventurer-specific opportunities and an exit room; tasks involve choosing text actions (verbs + objects) in partially observable, long-horizon text environments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge graph (via underlying KG-A2C core)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Relational knowledge graph (nodes/edges representing rooms, objects, inventory items, relations) embedded with a graph-attention mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Facts and relations observed about rooms, objects in rooms, inventory items, and other world-state facts discovered during play.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Graph attention embedding: the knowledge graph is embedded via a graph-attention mechanism and incorporated into the agent's embedded observation; retrieval is implicit via the graph-attention embedding rather than explicit key-value lookup.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Updated online from observations: the state observation (look, inventory, previous action, feedback) is used to update the knowledge graph whenever new facts are observed.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Maintain and ground world state (what objects/rooms/inventory exist and relations) to inform action generation and planning in partially observable text worlds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Thespian (built on KG-A2C core) matched single-character baseline performance while being able to switch characters via prompts; in training it converged in ~4,000 episodes on the two-character task, whereas a baseline KG-A2C trained jointly struggled beyond 10,000 episodes. In few-shot experiments, the frozen-core Thespian + attention learned a new blended 'Rogue' character substantially faster (converged by ~1,500 steps in some maps) than an unfrozen agent (which required ~15,000 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Freezing the pretrained core (including the knowledge-graph-based world representation) and training a small attention module over raw action logits enabled effective few-shot adaptation to a new blended character while preserving prior character behaviors (prevents catastrophic forgetting). Operating attention over raw logits was more effective than attention over softmaxed action probabilities because softmax smooths logits and reduces discriminability.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Paper notes bias from pretraining (order of pretraining) can cause over-attention to certain character logits (e.g., overfit to 'thief'), slowing few-shot learning in some map configurations. Also, because the Critic is frozen during few-shot, predicted state-values can be inaccurate for actions not part of pretrained characters, requiring lower value-loss weighting and tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Thespian: Multi-Character Text Role-Playing Game Agents', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2731.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2731.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-A2C (Knowledge-Graph augmented Advantage Actor-Critic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An Advantage-Actor Critic RL architecture augmented with an explicit knowledge-graph memory of observed world facts that is embedded via graph-attention and combined with recurrent observation embeddings to generate actions and value estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Actor-critic model whose observation processing includes a GRU embedding of textual observations and a concurrently-updated knowledge graph of observed facts; the knowledge graph is embedded with a graph-attention network and used alongside other embeddings by the actor and critic.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>LIGHT (and other text-adventure/textworld settings referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-adventure / text role-playing environments where the agent must read room descriptions, manage inventory, and issue text commands (verbs + objects); partially observable with combinatorial state/action spaces and long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge graph (external structured memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Knowledge graph: nodes and relations representing rooms, objects, inventory, and observed relations; embedded via graph-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Observed facts and relations (e.g., object locations, inventory contents, room relations).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Graph attention embedding (the KG is embedded and that embedding is used by the policy network).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Incremental updates from observations (observations feed into updates of the knowledge graph as the agent explores).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Grounding world state to reduce partial observability, enable tracking of discovered objects/rooms and inform action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>When trained on a single character, KG-A2C attains high character-specific scores (single-character baselines in this paper achieved high thief/adventurer scores); however, when trained jointly on both characters in the two-character experimental setup, KG-A2C's performance degraded (authors report the jointly-trained KG-A2C gets trapped in a local maximum and performs worse than thespian). Specific table-reported examples: single-character KG-A2C thief-only and adventurer-only agents achieved near-high character scores, while a KG-A2C trained on both characters had degraded performance (text reports e.g., ~88.2% thief, ~68.6% adventurer in joint training -- reported in paper table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>The structured KG memory is part of the state representation that enables strong single-character performance; however, naive joint training on multiple reward functions (characters) without disentangling character-specific policies leads to poor multi-character behavior despite the KG memory. The paper leverages KG-A2C's KG-memory as a backbone for the thespian approach.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Thespian: Multi-Character Text Role-Playing Game Agents', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2731.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2731.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge-Graph Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-graph as external memory for text games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Explicit graph-structured external memory storing facts and relations observed during play (rooms, objects, inventory, relations) used to reduce partial observability and ground language-based decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Knowledge-graph memory (general)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An external structured memory in the form of a knowledge graph that is incrementally updated with observations and embedded (often via graph-attention) for downstream policy/value prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Text adventure / LIGHT / TextWorld (mentioned as general testbeds)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Partially-observable text environments where agents must maintain state about past observations, discovered objects and relations to plan and act effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge graph (semantic/world-state memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Graph (nodes/edges representing entities and relations); embedded with graph-attention networks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>World facts: room descriptions, objects in rooms, inventory items, relations among entities observed during exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Embedding via graph-attention; attention-weighted features from the KG combined with observation embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Updated when new facts are observed (during observation processing).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Track and ground world state for planning, to avoid forgetting discovered objects/locations and to inform action generation in long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Prior work cited (KG-A2C family) reports state-of-the-art performance on various text-game benchmarks when using KG-based memory; in this paper KG-memory is the backbone enabling the Thespian agent to operate across characters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Structured, explicit KG memory provides a compact, grounded representation of the world that supports decision making; when combined with disentangled per-character policies and attention over logits, it enables few-shot adaptation without catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Thespian: Multi-Character Text Role-Playing Game Agents', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2731.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2731.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Q*BERT (BERT-augmented KG-A2C extension)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior KG-A2C-family agent that further incorporates a pre-trained BERT language model into the architecture to improve text understanding for action selection in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Described in related work as an extension of KG-A2C that integrates BERT for richer textual embedding/understanding; referenced but not used in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Text-adventure games / TextWorld (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-based game environments requiring comprehension of room/object descriptions and production of text commands.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Thespian: Multi-Character Text Role-Playing Game Agents', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2731.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2731.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GATA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GATA (Graph-Augmented Transformer Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based agent that builds a knowledge-graph representation of the world and trains through a mix of reinforcement and self-supervised learning; referenced as an alternative KG-based approach to text-game play.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GATA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned in related work as a transformer agent that constructs a knowledge-graph world representation on top of a transformer backbone, trained with combined RL and self-supervised objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Text-based games (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-game environments where a KG-based world representation aids in generalization and action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Thespian: Multi-Character Text Role-Playing Game Agents', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dynamic knowledge graphs to generalize on text-based games <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds <em>(Rating: 2)</em></li>
                <li>Learning to speak and act in a fantasy text adventure game <em>(Rating: 2)</em></li>
                <li>Model ensemble instead of prompt fusion: a samplespecific knowledge transfer method for few-shot prompt tuning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2731",
    "paper_id": "paper-260438426",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "Thespian",
            "name_full": "Thespian agent (Multi-Character Thespian Agent)",
            "brief_description": "A single RL actor-critic agent that can emulate multiple characters via learned soft character prompts and character-specific action logits, and can learn new blended characters few-shot using an attention module over pretrained character logits while keeping core weights frozen.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Thespian agent",
            "agent_description": "Built on the KG-A2C Advantage-Actor Critic framework; the core produces per-character stacks of action logits and per-character value estimates. It adds learned soft character prompt embeddings (one per character) concatenated with state embeddings to produce character-specific internal states, and a few-shot Thespian Attention module that (when learning a new character) learns to attend and linearly combine the frozen pretrained characters' raw action logits to produce actions for the new character.",
            "base_model_size": null,
            "game_benchmark_name": "LIGHT (custom map)",
            "game_description": "A grounded text-based role-playing environment with NPCs, objects and locations; experiments use handcrafted maps with thief- and adventurer-specific opportunities and an exit room; tasks involve choosing text actions (verbs + objects) in partially observable, long-horizon text environments.",
            "uses_memory": true,
            "memory_type": "knowledge graph (via underlying KG-A2C core)",
            "memory_structure": "Relational knowledge graph (nodes/edges representing rooms, objects, inventory items, relations) embedded with a graph-attention mechanism.",
            "memory_content": "Facts and relations observed about rooms, objects in rooms, inventory items, and other world-state facts discovered during play.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Graph attention embedding: the knowledge graph is embedded via a graph-attention mechanism and incorporated into the agent's embedded observation; retrieval is implicit via the graph-attention embedding rather than explicit key-value lookup.",
            "memory_update_strategy": "Updated online from observations: the state observation (look, inventory, previous action, feedback) is used to update the knowledge graph whenever new facts are observed.",
            "memory_usage_purpose": "Maintain and ground world state (what objects/rooms/inventory exist and relations) to inform action generation and planning in partially observable text worlds.",
            "performance_with_memory": "Thespian (built on KG-A2C core) matched single-character baseline performance while being able to switch characters via prompts; in training it converged in ~4,000 episodes on the two-character task, whereas a baseline KG-A2C trained jointly struggled beyond 10,000 episodes. In few-shot experiments, the frozen-core Thespian + attention learned a new blended 'Rogue' character substantially faster (converged by ~1,500 steps in some maps) than an unfrozen agent (which required ~15,000 steps).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Freezing the pretrained core (including the knowledge-graph-based world representation) and training a small attention module over raw action logits enabled effective few-shot adaptation to a new blended character while preserving prior character behaviors (prevents catastrophic forgetting). Operating attention over raw logits was more effective than attention over softmaxed action probabilities because softmax smooths logits and reduces discriminability.",
            "memory_limitations": "Paper notes bias from pretraining (order of pretraining) can cause over-attention to certain character logits (e.g., overfit to 'thief'), slowing few-shot learning in some map configurations. Also, because the Critic is frozen during few-shot, predicted state-values can be inaccurate for actions not part of pretrained characters, requiring lower value-loss weighting and tuning.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2731.0",
            "source_info": {
                "paper_title": "Thespian: Multi-Character Text Role-Playing Game Agents",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "KG-A2C",
            "name_full": "KG-A2C (Knowledge-Graph augmented Advantage Actor-Critic)",
            "brief_description": "An Advantage-Actor Critic RL architecture augmented with an explicit knowledge-graph memory of observed world facts that is embedded via graph-attention and combined with recurrent observation embeddings to generate actions and value estimates.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "KG-A2C",
            "agent_description": "Actor-critic model whose observation processing includes a GRU embedding of textual observations and a concurrently-updated knowledge graph of observed facts; the knowledge graph is embedded with a graph-attention network and used alongside other embeddings by the actor and critic.",
            "base_model_size": null,
            "game_benchmark_name": "LIGHT (and other text-adventure/textworld settings referenced)",
            "game_description": "Text-adventure / text role-playing environments where the agent must read room descriptions, manage inventory, and issue text commands (verbs + objects); partially observable with combinatorial state/action spaces and long horizons.",
            "uses_memory": true,
            "memory_type": "knowledge graph (external structured memory)",
            "memory_structure": "Knowledge graph: nodes and relations representing rooms, objects, inventory, and observed relations; embedded via graph-attention.",
            "memory_content": "Observed facts and relations (e.g., object locations, inventory contents, room relations).",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Graph attention embedding (the KG is embedded and that embedding is used by the policy network).",
            "memory_update_strategy": "Incremental updates from observations (observations feed into updates of the knowledge graph as the agent explores).",
            "memory_usage_purpose": "Grounding world state to reduce partial observability, enable tracking of discovered objects/rooms and inform action selection.",
            "performance_with_memory": "When trained on a single character, KG-A2C attains high character-specific scores (single-character baselines in this paper achieved high thief/adventurer scores); however, when trained jointly on both characters in the two-character experimental setup, KG-A2C's performance degraded (authors report the jointly-trained KG-A2C gets trapped in a local maximum and performs worse than thespian). Specific table-reported examples: single-character KG-A2C thief-only and adventurer-only agents achieved near-high character scores, while a KG-A2C trained on both characters had degraded performance (text reports e.g., ~88.2% thief, ~68.6% adventurer in joint training -- reported in paper table).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "The structured KG memory is part of the state representation that enables strong single-character performance; however, naive joint training on multiple reward functions (characters) without disentangling character-specific policies leads to poor multi-character behavior despite the KG memory. The paper leverages KG-A2C's KG-memory as a backbone for the thespian approach.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2731.1",
            "source_info": {
                "paper_title": "Thespian: Multi-Character Text Role-Playing Game Agents",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Knowledge-Graph Memory",
            "name_full": "Knowledge-graph as external memory for text games",
            "brief_description": "Explicit graph-structured external memory storing facts and relations observed during play (rooms, objects, inventory, relations) used to reduce partial observability and ground language-based decision making.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Knowledge-graph memory (general)",
            "agent_description": "An external structured memory in the form of a knowledge graph that is incrementally updated with observations and embedded (often via graph-attention) for downstream policy/value prediction.",
            "base_model_size": null,
            "game_benchmark_name": "Text adventure / LIGHT / TextWorld (mentioned as general testbeds)",
            "game_description": "Partially-observable text environments where agents must maintain state about past observations, discovered objects and relations to plan and act effectively.",
            "uses_memory": true,
            "memory_type": "knowledge graph (semantic/world-state memory)",
            "memory_structure": "Graph (nodes/edges representing entities and relations); embedded with graph-attention networks.",
            "memory_content": "World facts: room descriptions, objects in rooms, inventory items, relations among entities observed during exploration.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Embedding via graph-attention; attention-weighted features from the KG combined with observation embeddings.",
            "memory_update_strategy": "Updated when new facts are observed (during observation processing).",
            "memory_usage_purpose": "Track and ground world state for planning, to avoid forgetting discovered objects/locations and to inform action generation in long-horizon tasks.",
            "performance_with_memory": "Prior work cited (KG-A2C family) reports state-of-the-art performance on various text-game benchmarks when using KG-based memory; in this paper KG-memory is the backbone enabling the Thespian agent to operate across characters.",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Structured, explicit KG memory provides a compact, grounded representation of the world that supports decision making; when combined with disentangled per-character policies and attention over logits, it enables few-shot adaptation without catastrophic forgetting.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2731.2",
            "source_info": {
                "paper_title": "Thespian: Multi-Character Text Role-Playing Game Agents",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Q*BERT",
            "name_full": "Q*BERT (BERT-augmented KG-A2C extension)",
            "brief_description": "A prior KG-A2C-family agent that further incorporates a pre-trained BERT language model into the architecture to improve text understanding for action selection in text games.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Q*BERT",
            "agent_description": "Described in related work as an extension of KG-A2C that integrates BERT for richer textual embedding/understanding; referenced but not used in experiments in this paper.",
            "base_model_size": null,
            "game_benchmark_name": "Text-adventure games / TextWorld (referenced)",
            "game_description": "Text-based game environments requiring comprehension of room/object descriptions and production of text commands.",
            "uses_memory": null,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": null,
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2731.3",
            "source_info": {
                "paper_title": "Thespian: Multi-Character Text Role-Playing Game Agents",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GATA",
            "name_full": "GATA (Graph-Augmented Transformer Agent)",
            "brief_description": "A transformer-based agent that builds a knowledge-graph representation of the world and trains through a mix of reinforcement and self-supervised learning; referenced as an alternative KG-based approach to text-game play.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "GATA",
            "agent_description": "Mentioned in related work as a transformer agent that constructs a knowledge-graph world representation on top of a transformer backbone, trained with combined RL and self-supervised objectives.",
            "base_model_size": null,
            "game_benchmark_name": "Text-based games (referenced)",
            "game_description": "Text-game environments where a KG-based world representation aids in generalization and action selection.",
            "uses_memory": null,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": null,
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2731.4",
            "source_info": {
                "paper_title": "Thespian: Multi-Character Text Role-Playing Game Agents",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on text-based games",
            "rating": 2,
            "sanitized_title": "learning_dynamic_knowledge_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "How to avoid being eaten by a grue: Structured exploration strategies for textual worlds",
            "rating": 2,
            "sanitized_title": "how_to_avoid_being_eaten_by_a_grue_structured_exploration_strategies_for_textual_worlds"
        },
        {
            "paper_title": "Learning to speak and act in a fantasy text adventure game",
            "rating": 2,
            "sanitized_title": "learning_to_speak_and_act_in_a_fantasy_text_adventure_game"
        },
        {
            "paper_title": "Model ensemble instead of prompt fusion: a samplespecific knowledge transfer method for few-shot prompt tuning",
            "rating": 1,
            "sanitized_title": "model_ensemble_instead_of_prompt_fusion_a_samplespecific_knowledge_transfer_method_for_fewshot_prompt_tuning"
        }
    ],
    "cost": 0.0162045,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Thespian: Multi-Character Text Role-Playing Game Agents</p>
<p>Christopher Cui 
Georgia Institute of Technology</p>
<p>Xiangyu Peng 
Georgia Institute of Technology</p>
<p>Mark Riedl riedl@gatech.edu 
Georgia Institute of Technology</p>
<p>Thespian: Multi-Character Text Role-Playing Game Agents
C23503E270E2CD567976C20DCE3B5577
Text-adventure games and text role-playing games are grand challenges for reinforcement learning game playing agents.Text role-playing games are open-ended environments where an agent must faithfully play a particular character.We consider the distinction between characters and actors, where an actor agent has the ability to play multiple characters.We present a framework we call a thespian agent that can learn to emulate multiple characters along with a soft prompt that can be used to direct it as to which character to play at any time.We further describe an attention mechanism that allows the agent to learn new characters that are based on previously learned characters in a few-shot fashion.We show that our agent outperforms the state of the art agent framework in multi-character learning and few-shot learning.</p>
<p>Introduction</p>
<p>Text adventure games are those in which a player can only interact with an interactive environment through reading text descriptions of the environment and acting by typing descriptions of actions.Text games present a grand challenge for AI because they (a) are partially observable; (b) have combinatorially large state spaces consisting of all possible descriptive text strings; (c) have combinatorially large action spaces in the order of billions of possible text commands; (d) require reasoning about long-horizon causal dependencies; and (e) require commonsense and narrative trope reasoning (Hausknecht et al. 2020).Text adventure game playing has become a benchmark challenge for reinforcement Learning (RL) agents (Hausknecht et al. 2020;Narasimhan, Kulkarni, and Barzilay 2015;Ammanabrolu and Riedl 2019;Ammanabrolu and Hausknecht 2020;Ammanabrolu et al. 2020;Adhikari et al. 2020), which play by exploring the environment and receiving score based on how far they make it through the game.</p>
<p>Relatedly, table-top role playing games, such as Dungeons &amp; Dragons, involve multiple players that interact with textual descriptions of the environment as well as dialogue with other players.While players may be motivated by a quest or mission, table-top role playing games are fundamentally open-ended, meaning that players can interact with the environment and with each other in ways that are not strictly dictated by a quest, mission, or set of puzzles.Openended role-playing extends the same challenges of text ad-</p>
<p>Dungeon</p>
<p>You are in the Dungeon.It is dark and gloomy... ...There is a skeleton here.There is a path south, a path east, and a path west.You have a sword, a shield, and a tattered map.Previous action: Go west  venture games but removes the environmentally-dictated reward structure.The predominant question for open-ended role-playing is whether an agent acts consistently with a given character definition.</p>
<p>Because there may be no explicit reward associated with progression in open-ended role playing games, an agent must instead be trained to, at least, emulate particular character types such as "thief" or an "adventurer", each of which has different preferences for different actions depending on arXiv:2308.01872v1[cs.AI] 3 Aug 2023 the situation. 1n this paper we consider the distinction between character agents and actor agents.A character agent is trained to act like one specific character; for all intents and purposes it is that character and knows nothing else but how to be that character.In contrast, an actor agent has knowledge about how to play many different characters and can receive instruction from an external source (for example a movie director or a dungeon master) about which character type to play.Furthermore, an actor can leverage the character knowledge to learn to blend characters with only a small amount of additional practice (e.g., few-shot learning) without exhaustively re-training from scratch.We will refer to actor agents as thespian agents to distinguish between agents that learn to enact multiple characters from the actor-critic reinforcement learning architecture.This paper considers two challenges.The first is to train a single reinforcement learning agent model that can switch between character types with a simple instruction.We present a new RL agent that can learn to emulate multiple characters simultaneously with an updated policy model that generates |C| sets of action distributions, where C is a set of character classes.The agent also learns a soft prompt that can later be provided as a cue to emulate a specific character.</p>
<p>The second challenge is to be able to train a thespian agent to learn new characters in a faction of the training time while maintaining performance in the previously trained characters.We achieve this by adding an attention mechanism to the outputs of the the thespian agent, which can choose which can learn how to blend the action probabilities of different characters, thus learning a new character and a new soft prompt.</p>
<p>To return to our character vs. actor metaphor, we now have an thespian model that can simultaneously generate different actions for different characters.This is equivalent to a thespian thinking about how different characters will respond to the same situation.The thespian agent receives direction in the form of a prompt indicating what character to play.If the thespian needs to play a new character that it has never played before it can learn a new prompt for the new character much faster than if it had to learn from scratch by leveraging what it already knows about playing other characters.</p>
<p>We conduct experiments across two original character types, a "thief" and an "adventurer" and demonstrate the ability of a single thespian agent trained on both characters to perform as well as separate baseline models trained to emulate individual characters.We show that we can use a novel attention mechanism to learn a third character that is a blend of the previously trained characters in a few-shot fashion.This few-shot character learning is 10x faster than baseline alternatives and doesn't degrade the performance of original characters.</p>
<p>The distinction between characters and actors have been made before.Louchart and Aylett (2007) consider an actor agent one that makes a secondary assessment of its own cognitive and emotional state.Riedl (2003) consider an actor agent one that doesn't just reason about the best action to convey a character but also incorporates directorial goals.Si, Marsella, and Pynadath (2005) consider an actor agent one that reasons about the cognitive state of other interlocutors in an interactive game; they also referred to their agent as a thespian.These prior works looked at acting as meta-cognition, but agents could not represent more than one character without retraining or reprogramming.While our work can also be considered a form of meta-cognition, our focus is on a single model trained to be able to reason about and enact different characters.</p>
<p>Text Adventure Game Playing Agents</p>
<p>Text adventures are games in which the player must read textual descriptions of the environment and describe their actions with short text commands.Most text adventure games have a narrative progression through puzzles toward an ultimate goal or conclusion.Text based games have shown great potential for use as Reinforcement Learning benchmark environments (Hausknecht et al. 2020;Narasimhan, Kulkarni, and Barzilay 2015).Ammanabrolu and Riedl (2019) proposed augmenting reinforcement learning with knowledge graphs as external memory about world state.Ammanabrolu and Hausknecht (2020) proposed KG-A2C, which integrates knowledge graphs into the actor-critic (Bahdanau et al. 2016) RL framework.The Q*BERT agent (Ammanabrolu et al. 2020) further extended KG-A2C to incorporate the BERT (Devlin et al. 2019a) language model into the model architecture.We build on top of the KG-A2C family of models since they have shown state-of-theart performance.Other techniques for playing text-based games include GATA (Adhikari et al. 2020), which builds a knowledge-graph based representation of the world on top of a transformer-based agent, training through a combination of RL and self-supervised learning.</p>
<p>Text-based Role Playing Agents</p>
<p>Whereas text adventure games have pre-defined progression toward a goal state, table-top role playing games involve open-ended game play.We refer to text-based environments that support open-ended game play as text-based role playing to signify the interaction with the environment through reading and writing text instead of verbal interactions with other players and game masters.</p>
<p>The LIGHT environment (Urbanek et al. 2019) is a crowdsourced text-based role playing game with a rich environment with interactable NPCs, objects and locations, each with a short paragraph description, demonstrating the value of grounding in training agents that can not only act but also converse successfully.Ammanabrolu et al. (2021) propose agents that can switch seamlessly between generating natural language and action declarations.These agents can learn to play different characters when given a motivation that includes character type and goal as part of the input world state.This work is most similar to ours, except our agents do not require explicit motivations or goals beyond a learned character prompt.</p>
<p>Story Shaping (Peng et al. 2023) is a technique for training RL agents to play text role-playing games wherein a story is converted into a rich reward signal.The technique can be used to train different characters, but can only train a single agent to emulate a single character.Our character-based reward strategy is related, but our rewards are manually crafted instead of inferred from stories.</p>
<p>Few-Shot Adaptation</p>
<p>Large pre-trained Language models have emerged as extremely powerful tools for NLP tasks (Devlin et al. 2019b;Raffel et al. 2020;Brown et al. 2020).However, a limitation of these powerful models is their size, some with parameters numbering in the billions (Brown et al. 2020).This makes them prohibitively expensive when it comes to further training or fine-tuning.Low-Rank Adaptation (LoRA) circumvents this by keeping the model frozen and introducing trainable rank decomposition matrices.Our proposed technique also freezes the core model and trains additional layers on top, though the specific mechanics needed for reinforcement learning are different.</p>
<p>Prompt-tuning also avoids the need to do further training on the model itself by introducing trainable, soft prompts that learn an ideal input based on the desired output (Lester, Al-Rfou, and Constant 2021).(Peng et al. 2022) proposes pairing soft prompts with an attention module to induce language models to perform different tasks.Using knowledge from a previously trained task to improve learning on a new task has also been explored by (Zhao, Sun, and Ma 2021), their approach more focused on generalization across simpler objectives and adaptation to unseen environments.</p>
<p>Preliminaries</p>
<p>Textworlds as RL Testbeds</p>
<p>A text-adventure or text-based role playing game can be modeled as a partially-observable Markov decision process (POMDP) M = S, T, A, , O, R,  where S is the set of ground truth world states, A is the set of actions, T is the probability of transitioning from one state to another given an executed action, R is a reward function, O is the set of possible observations,  is the probability of observations given the ground truth world state, and  is a parameter estimating the reward horizon (Hausknecht et al. 2020).In our setting, we will use a deterministic transition function T , which is common in text-based games.However, nothing in our proposed technique strictly requires it.The objective of reinforcement learning is to learn a policy,  : S  A that maps states to actions, such that taking the action mapped to the current state and following the policy henceforth maximizes expected reward.</p>
<p>LIGHT</p>
<p>Our agent is trained in the LIGHT environment (Urbanek et al. 2019), a text world environment with a database of 1775 Non-Player Characters (NPCs), 663 locations, and 3462 objects with rich text descriptions.Game maps can also be handcraft with specifically placed NPCs, locations and objects.We create a map for our experiments such that multiple character types can have relevant activities to perform, including interacting with objects and NPCs.For example there are dragons for an "adventurer" character to slay, and armor to don, whereas a "thief" character can take money from the donations receptacle in a sanctuary.</p>
<p>Our experiments use base character types of "Thief" and "Adventurer".We also associate rewards to different actions for each character type.For example, a "Thief" character agent is rewarded for obtaining a hidden dagger, stealing, and other thief-like actions.Likewise, an "Adventurer" character agent is rewarded for obtaining a sword and armor from the armory and killing monsters, and other adventurerlike actions.There is no requirement that an agent do particular actions and no prescribed order.This is equivalent to the Story Shaping technique (Peng et al. 2023) , except the rewards are manual, which is done to make more controlled experiments.Regardless of character type, all games terminate when the agent enters a particular, preset "goal room", at which time the agent receives a final reward that is smaller than the others.The entire game map is provided in the appendix.</p>
<p>KG-A2C</p>
<p>We build off the KG-A2C agent framework (Ammanabrolu and Hausknecht 2020), an Advantage-Actor Critic architecture augmented with a knowledge-graph based attention.KG-A2C's space of observations includes (a) text description of the room the agent is in via the "look" command, (b) text descriptions of the character's inventory via the "inventory" command, (c) the agent's last command, and (d) feedback from the last command.The state observations are concatenated and embedded using a recurrent GRU.</p>
<p>Simultaneously, the state observation is used to update a knowledge graph of facts about the world that have been observed to date.This includes facts and relations about rooms, objects in rooms, inventory items, etc.This knowledge graph is then embedded using a graph attention mechanism (Velikovi et al. 2018).</p>
<p>Advantage-actor critic networks (Mnih et al. 2016) have two heads.The actor head generates logit scores, one for each possible action, which can be converted to a probability distribution via softmax and sampled to determine which action the agent takes.The critic head estimates the utility of the state.Actions are made up of verbs and optional object names.The KG-A2C agent generates a verb, which maps to a pre-defined template, and the generated object name is used to populate the template.</p>
<p>The Thespian Agent</p>
<p>Building off the basic framework of KG-A2C we describe how a single agent policy model can learn to emulate multiple characters.To train an single model to emulate different characters, it must be rewarded differently for each character, which can confuse an agent unless it has a way of disentangling the characters.Our thespian agent architecture addresses this challenge in two ways.First, we provide a means shows the thespian agent, focusing on the these two aspects.</p>
<p>Character Prompts</p>
<p>First, we allow for a soft character prompt to be learned.Each prompt is associated with different character the model has been trained to emulate and induces the agent to generate behavior that is consistent with the associated character.This is similar to the notion of the soft prompt(Lester, Al-Rfou, and Constant 2021), which is like a regular prompt for LLMs but given as an embedding instead of natural language.The soft character prompt vector of values can be interpreted as an instruction analogous to saying "I am in state x and I am a Thief.My next action would be..." at the embedding level.Let P = p1 ... pn be a set of soft character prompts for each character c i  C and let o be the embedded current state observation.Initially, the prompts p i are empty, initialized with random numbers.The internal state representation s i for character c i is:
s i = W T i  cat(o, p i ) (1)
where W is a set of trainable weights.</p>
<p>The soft character prompts are learned as follows.During training, the agent will engage in reinforcement learning games as normal.In each game, the agent will be provided with a different reward function for each character c i .That is, a thief will be rewarded for certain actions and an adventurer will be rewarded for different actions.The character, corresponding character reward function, and character prompt p i are rotated each game to balance the training of multiple characters.Over time, each soft prompt is updated via gradient flow through W such that each unique prompt is associated with a particular way in which the agent is rewarded.</p>
<p>Character-Specific Action Scores</p>
<p>We also modify the agent model's actor and critic modules.The standard A2C framework produces logit scores for each action.This vector of logit scores is traditionally converted to a probability distribution with a softmax layer and sampled to determine which action the agent takes.Our thespian agent model instead produces a stack of action logit scores.A softmax over this stack of logits produces n probability distributions, for n characters.</p>
<p>The critic head is likewise modified to produce n predicted utility scores, one for each character.</p>
<p>Thus, the agent is simultaneously determining which action is best for each character and how good the current state is from the perspective of each character.</p>
<p>At training time, the characters are rotated each game and the ith set of logit scores is sampled to determine the agent's action, and the ith utility value is used to compute character-specific advantage loss.The loss is backpropagated through only the logits and utility used.</p>
<p>Thespian Agent Experiments</p>
<p>In this section we evaluate the thespian agent without the additional few-shot learning attention mechanism in order to determine the extent to which the agent can learn more than one character at a time.In this experiment we train a single agent to emulate two characters: thief and adventurer.We execute the agent in the same general environment that has multiple opportunities for thief-specific actions and adventurer-specific actions.The environment (see Figure 5 in the Appendix) has a common starting room and an exit room that terminates the game when the agent enters it.There are a cluster of thief-specific and adventurer-specific rewards clustered near the starting room.The environment then branches with one branch heading to areas that only contain thief-specific rewards and another branch heading to areas that only contain adventurer-specific actions.</p>
<p>The thespian agent is trained as follows.We create empty prompts for thief and adventurer.We train on one character reward, accompanied by the character prompt, for two games, then switch to the net character reward and character prompt for two more games.A game completes when the agent navigates to the exit room as described in Section 3.2.We train for a total of 10,000 games and use the checkpoint with the highest performance on 20 test game runs, split equally between each character.</p>
<p>We evaluate the agent in the same environment, executing the agent with with each character prompt one at a time.We measure the percentage of total character-specific action opportunities the agent takes.We run each character prompt for 100 games with different initialization seeds and take the average result.</p>
<p>We compare to a baseline KG-A2C trained with the same training method (but without the prompts since the base KG-A2C architecture would not understand them), as well as the thespian agent with a prompt made of random numbers.</p>
<p>Table 1 shows the results.The base KG-A2C when trained only on thief rewards or adventurer rewards is able to achieve most of the character-specific score.The base agent trained on one character rarely attempts to perform actions specific to another character, which is to be expected and demonstrates that the environment setting is fair if the objective were to only train one character at a time.However, when the base KG-A2C is trained with both character rewards, the agent's performance relative to both characters suffers.The resulting agent also attempts to get all rewards, regardless of character, thus failing to differentiate between characters.</p>
<p>In comparison, thespian agent uses a single model and that single model scores well has a high thief score when given the thief prompt and a high adventurer score when given the adventurer prompt.The thespian agent rarely attempts actions that are specific to a non-prompted character.Despite being trained on multiple character rewards, the thespian agent achieves performance equivalent to the base model trained on only one character.Figure 3 shows the learning curve of the single thespian agent training on both</p>
<p>Score Episodes</p>
<p>Thespian Agent KG-A2C</p>
<p>Figure 3: We see the thespian agent achieves convergence after about 4,000 episodes where the KG-A2C still struggles to perform even after 10,000 episodes.</p>
<p>characters versus a single base KG-A2C training on both characters using the same character rotation scheme.As can be seen KG-A2C gets trapped in a local maximum.</p>
<p>When the thespian agent is given a random prompt, it scores poorly as either character.There may be a bias in the environment that leads the agent to prefer the branch that contains more adventurer score, explaining why the agent obtains more adventurer rewards.</p>
<p>Few-Shot Learning with Thespian Attention</p>
<p>The thespian agent is a single agent that can be trained to emulate many different characters by providing one of the learned prompts as a cue for how to behave in an open-ended fashion.In this section we consider the question of whether a pre-trained thespian agent can learn a new character that draws on knowledge of previously learned characters.</p>
<p>Given a thespian agent that has been trained on n characters, training the n + 1th character poses challenges.Training on the n + 1th character, with a new reward runs the risk that the agent forgets the previous n characters.This is a commonly known phenomenon with fine-tuning any type of model.It is typically a desired phenomenon when we wish to update the model to a new behavior that overwrites the pre-trained behavior.However, in this case, we wish to preserve the ability to execute previous behavior while adding new behavior.</p>
<p>Our approach is to freeze the thespian agent model and add a module (see Figure 2 right, yellow) with learnable weights that operate on the original, frozen model's outputs.Since we seek to teach the agent a new character that is a blend of existing characters, we apply an attention mechanism across the action logit scores for each character.This attention module learns to blend the raw logit scores for each
h O = LN (W T FF2  (W T FF1  O))(2)
with  as a non-linear activation function, W FF1 and W FF2 as trainable weights, and LN () is a Layer Norm (Ba, Kiros, and Hinton 2016).</p>
<p>The action logits a i for all characters c i  C produced by the frozen thespian agent are stacked as A = a1 ... an and also fed through a feed-forward network identical to Equation 2 to obtain h A .To obtain the final set of attention scores for each observation we perform a matrix multiplication between h O and h A .We divide by a constant m that applies a temperature-like smoothing before applying a softmax layer to obtain the matrix of attention scores,
S = softmax h O  h A m(3)
with c being some character (pre-trained or being learned few-shot).</p>
<p>The final weighted averaged logits for the action is:
p final = softmax  obs  S T  A (4)
where  obs is a vector of scaling coefficients for each of o look , o inv , o prev , and o fback , the look, inventory, previous action, and previous action feedback components of the state observation, respectively. obs is a hyperparameter that allows us increase the influence of different parts of the observation.They can be equal and sum to one to have a uniform averaging effect, or be used to increase or decrease the contribution of each component of the state observation.Setting the coefficients &gt; 1.0 loads greater probability mass onto the highest-scoring action score logits.This has the effect of making the agent more "exploitative" when sampling from the probability distribution over actions.</p>
<p>The result is that the thespian attention learns the optimal weights to calculate the contribution of each pre-trained character in determining an action for the new character in the current state with respect to each observation tensor.</p>
<p>Since the KG-A2C base splits action generation into verb and object selection, the above process is repeated for the verb and the object to produce one probability distribution for the verb and one distribution for the object.The sampled verb and sampled object are combined using the KG-A2C template approach described in Section 3.3.</p>
<p>Few-Shot Training</p>
<p>The traditional actor-critic loss is computed as the difference between the agent's predicted value of an action and the true expected value.However, the thespian agent produces a realnumbered utility value prediction for each character.Rather than perform a weighted average with the attention scores as we did for the action logits, we take the average of the predicted values of the state from the new character's perspective and the predicted value of the most influential pretrained character.This is the pre-existing character that the agent thinks has the best chance of receiving reward even though the reward function is for a new character.Thus loss is a function of how much better the thespian attention can pick an action for the new character over the best chance if it had to play a pre-existing character.</p>
<p>The thespian agent can now be trained as before, by providing a new character reward and an empty prompt.With the core thespian agent weights frozen, the agent will retain the ability to respond to existing character prompts.The thespian agent will learn new weights in the feed-forward networks that combine the existing characters action logits.We no longer need to specify which set of character action logits to sample from.It will also learn a new prompt for the new character.</p>
<p>Few-shot Experiments</p>
<p>The thespian attention uses far fewer parameters than the core agent.Therefore we test the ability to train the thespian attention module to learn a new character in fewer training steps versus training from scratch.Given a frozen thespian agent pre-trained to respond to the thief and adventurer prompts, we train a new character-a "Rogue"-that excels at both thieving and adventuring.To demonstrate few-shot learning, we limit the total training steps to 3,000.</p>
<p>Thief-First</p>
<p>Adventurer-First Alternating</p>
<p>Steps Score Thespian Attention Unfrozen Thief Score Unfrozen Adventurer Score Unfrozen New Character Score Thief Max Score Adventurer Max Score New Character Max Score</p>
<p>Figure 4: Scores of the thespian attention versus unfrozen agents for each game.The red and blue lines represent the score of the unfrozen agent on the "Thief" and "Adventurer" games when provided the respective prompt.We see that in all cases, the unfrozen agent's performance in the pre-trained characters suffers.In the Adventurer-First and Alternating maps, the thespian attention converge far sooner than the unfrozen agent.</p>
<p>While not yet converged, we also see the thespian attention train faster than the unfrozen agent.</p>
<p>We created three variations of the environment:  Thief-first map: all the thief-specific activities are arranged close to the starting room while all the adventurerspecific activities are closer to the exit room. Adventurer-first map: all the adventurer-specific activities are arranged close to the starting room while all the thief-specific activities are closer to the exit room. Alternating map: the character-specific activities alternate between thief and adventurer as the agent progresses farther from the starting room.These alternative maps demonstrate robustness to alternating conditions in the environment that require either knowledge about how to act as a thief or knowledge about how to act as an adventurer.For all characters, the maximum score the agent can achieve is 47 and all characters share the same exit room.We use the total score achieved as a measure of how well the thespian attention allows the agent to learn a new character based on the pre-trained characters.</p>
<p>Baselines</p>
<p>We compare two agents:</p>
<p> Thespian attention agent: a pre-trained thespian agent with frozen weights and the few-shot attention mechanism. Unfrozen thespian agent: the same pre-trained thespian agent but with unfrozen weights and no attention mechanism.Both agents are trained on a new "Rogue" reward, which rewards the agent for the union of thief-specific and adventurer-specific actions.</p>
<p>For the thespian attention agent, we measure the total "rogue" game score after each step.For the unfrozen agent, we measure the total "rogue" game score as well as just the thief score and just the adventurer score.Whereas the thespian attention agent is frozen and cannot lose its ability to emulate a thief or adventurer (character prompt and internal weights are unchanged), the unfrozen agent may lose its ability to emulate the thief and adventurer as it trains on the "rogue" reward.</p>
<p>Results</p>
<p>Figure 4 shows the total cumulative score for the thespian attention agent and unfrozen agent, averaged across five training runs each.In all three maps, the thespian attention agent training a new "rogue" prompt outperforms the unfrozen agent training a new "rogue" prompt.In the adventurer-first and alternating maps the thespian attention agent has converged by 1,500 steps.</p>
<p>The unfrozen agent training a new "rogue" prompt fails to converge within the allotted time.The unfrozen agent converges after 15,000 steps, which is 10x slower than the frozen thespian agent with attention mechanism, though it does match the performance eventually.However, we also see that the unfrozen agent quickly loses its ability to emulate the plain thief and plain adventurer.The unfrozen agent can be trained using a rotation of games for all three characters.When this is done it takes in excess of 40,000 steps to before the it converges on a model that can play all three characters.</p>
<p>The reason the thespian attention agent does not do as well on the thief-first map as the others is because of bias introduced in the pre-training.Because the training regimen alternates characters, it trains on the "thief" character last.This makes the thespian agent slightly overfit to the thief character (relative to the adventurer).While this might seem like it might give it an advantage on the thief-first map, in means that it takes longer to encounter non-thief "rogue" rewards; the encounter of early thief rewards reinforces this by placing more attention weight on thief action logits.</p>
<p>Ablation Studies</p>
<p>We investigate three alternative ways to incorporate attention into the thespian agent:</p>
<p> Attention over a direct weighted average of character prompts. Attention over a weighted average of the soft character prompt plus state observation. Attention over action probabilities instead of raw logits.</p>
<p>The first two, which focused on attention over the soft character prompts in various ways, resulted in agents that failed to learn a new character.The agent would choose actions that went with the most attended prompt and would never achieve blending.This is because the attention layer would just act as a scalar on the inputs.</p>
<p>The third alternative would have used a softmax layer to convert action logits to a probability distribution before being fed into the attention mechanism.In all cases, this variation was inferior to operating on raw logits.The softmax conversion of raw logits to a probability distribution smooths the values, making it harder to discriminate between actions.Manipulating the logits allows for the biases of the individual character prompts to be more faithfully preserved.</p>
<p>Conclusions</p>
<p>In this paper, we make the distinction between character agents and actor agents.A character agent learns a model of a single character.An actor, or thespian, agent learns a model of multiple characters and can take direction through a soft prompt about which character to emulate at any given time.Our formulation of a thespian agent is further able to reason about which actions would be appropriate to each character.</p>
<p>The production of different action logit scores for different characters allows us to add an additional attention mechanism that learn new characters that remix previously known characters in a few-shot fashion.This is shown by training a new character that can take on the behavioral characteristics of previously known characters to respond to new circumstances in the environment.</p>
<p>In the context of text role-playing games, a grand challenge for AI (Callison-Burch et al. 2022), this work presents a step toward open-ended agents with disentanglable behavior policies.</p>
<p>A Appendix</p>
<p>A.1 LIGHT Map</p>
<p>Figure 5 shows the entire LIGHT map layout used for experimentation.</p>
<p>A.2 Training Details</p>
<p>While most other hyperparameters are kept the same, we increase the learning rate while decreasing the value loss for the thespian attention.Despite the new prompt and the Attention Module having comparably a smaller number of trainable parameters, we also train over a much smaller number of steps to emulate Few-Shot training.Where thespian agent allowed to train to completion over 10,000 games, we constrain the thespian attention to only 3000 steps, which for a well performing agent could be potentially 150 games but could also potentially only be 40 games for a nonperforming agent, depending on the number of steps the agent takes within a game.While we found a higher learning rate hinders the thespian agent, for the thespian attention the higher learning rate benefited the agent due to the agent having already learned and being constrained to a smaller, more optimal set of actions.</p>
<p>We also lower the coefficient of the value loss as well as changing how the value is calculated.As the Critic is frozen, we know it will always output the wrong reward value for any "Adventurer" or "Thief" action that isn't included in the new character.This results in large amounts of unnecessary loss that throws off the fusion agent during training.However, the value loss cannot be removed completely as it comprises the vast majority of the loss due to the pre-training of the thespian agent prior to the thespian attention.</p>
<p>Prompt: &lt;0.1656, -0.2891, -0.2251, ..., -0.3157, 0.2262, 0.2698&gt; You are an adventurer You hit the skeleton!The skeleton died! ... As an adventurer, the best action for me to take is hit</p>
<p>Figure 1 :
1
Figure 1: A Thespian Agent is capable of acting out a number of different characters by being provided a prompt that indicates which character it should emulate at the time.</p>
<p>Figure 5 :
5
Figure 5: LIGHT Map</p>
<p>The Thespian Attention takes in the embedded observations and the stacked logits, calculating an attention score per character for each observation.When training the Thespian Attention, the blue-shaded boxes indicate frozen modules with red-shaded boxes being trainable modules.
Thespian AgentThespian AttentionEmbeddingsRoom DescriptionInventoryFeedforwardThief Attention ScoresGame FeedbackNetworkAdv. Attention ScoresPrevious Action+SoftmaxOther Attention Scores... Other Attention Scores... Other AttentionXSoftmaxSampleScores...Room Description Inventory Game Feedback Previous Action+Embedding+Linear LayerThief Action Logits Adv. Action Logits Learnable Action Other Action Logits... Other Action Logits... Other Action Logits...Feedforward NetworkLearnable Attention ScoresComplete ActionLogitsFeedforwardThief Attention ScoresNetworkThief PromptRLThief Object LogitsAdv. Attention ScoresAdv. PromptAgentAdv. Object Logits+SoftmaxOther Attention Scores... Other Attention Scores... Other AttentionXSoftmaxSampletrained Prompts... Other Pre-Other Object Logits... Learnable Object Other Object Logits... Other Object Logits...Feedforward NetworkScores... Scores Learnable AttentionKnowledge GraphPrompt LearnableLogitsThief ValueAdv. ValueSelect most influentialOther P-T Value Other P-T Value Other P-T ValuepersonaLearnable ValueAverageCritic ValueFigure 2:
to learn soft character prompts.These are unique codes that are associated with different characters and can be provided as input to indicate which character the agent should emulate.Second, we change the actor and critic heads to generate sets of logit scores for all learned characters.Thus the agent can reason about which actions are best for each character, and we can sample from the set of logits for which ever character we want to execute.Figure2(left, green box)</p>
<p>Table 1 :
1
Performance of the base KG-A2C trained in different conditions and a single thespian model responding to different prompts.
ExperimentThief score %Thief std.devAdv score %Adv std.devAvg. Game StepsBase KG-A2CThief-Only 93.29.33.8024.7Adv-Only 3.8099.34.833.4Both trained 88.210.368.67.833.1Thespian agentThief Prompt92.917.43.8022.1Adv. Prompt 4.3099.111.132.2Rand. Prompt10.712.872.737.526.5characters to produce a single final action probability distri-bution.Specifically, we adapt the attention module from Penget al. (2022)-which is used for few-shot learning inLLMs-to the reinforcement learning setting. 26.1 Thespian AttentionLet O =o look oinv oprevrepresent the stacked observation compo-o fbacknent embeddings, which is fed through a feed-forward net-work, projecting it to a new, non-linear representation space,
This is a simplification of table-top role-playing games that can also feature distinct character personalities and back-stories.
In place of the embedded token sequence, we use the embedded observation tensors ot but do not perform a maxpool over the embedded observations as they are much smaller than the token sequences used in Peng et all's model ensemble</p>
<p>A Adhikari, X Yuan, M.-A Ct, M Zelinka, M.-A Rondeau, R Laroche, P Poupart, J Tang, A Trischler, W L Hamilton, Learning dynamic knowledge graphs to generalize on text-based games. 2020</p>
<p>Graph constrained reinforcement learning for natural language action spaces. P Ammanabrolu, M Hausknecht, P Ammanabrolu, M Riedl, arXiv:2001.08837Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2020. 20191arXiv preprintPlaying Text-Adventure Games with Graph-Based Deep Reinforcement Learning</p>
<p>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. P Ammanabrolu, E Tien, M Hausknecht, M O Riedl, arXiv:2006.074092020arXiv preprint</p>
<p>How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds. P Ammanabrolu, J Urbanek, M Li, A Szlam, T Rocktschel, J Weston, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterAssociation for Computational Linguistics2021</p>
<p>An actorcritic algorithm for sequence prediction. J L Ba, J R Kiros, G E Hinton, D Bahdanau, P Brakel, K Xu, A Goyal, R Lowe, J Pineau, A Courville, Y Bengio, arXiv:1607.06450arXiv:1607.07086Layer normalization. 2016. 2016arXiv preprint</p>
<p>Language models are few-shot learners. Advances in neural information processing systems. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Conference on Empirical Methods in Natural Language Processing (EMNLP). Abu Dhabi, UAEACL2020. 202233Dungeons and Dragons as a Dialogue Challenge for Artificial Intelligence</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterAssociation for Computational Linguistics2019a1</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies2019b1</p>
<p>Al-Rfou, R.; and Constant, N. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. M Hausknecht, P Ammanabrolu, M.-A Ct, X Yuan, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingLester, B202034Proceedings of the AAAI Conference on Artificial Intelligence</p>
<p>From synthetic characters to virtual actors. S Louchart, R Aylett, Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment. the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment20073</p>
<p>Language Understanding for Text-based Games using Deep Reinforcement Learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, K Narasimhan, T Kulkarni, R Barzilay, X Peng, C Cui, W Zhou, R Jia, M Riedl, arXiv:2301.10107Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2016. 2015. 2023arXiv preprintStory Shaping: Teaching Agents Human-like Behavior with Stories</p>
<p>Model ensemble instead of prompt fusion: a samplespecific knowledge transfer method for few-shot prompt tuning. X Peng, C Xing, P K Choubey, C.-S Wu, C Xiong, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Actor conference: character-focused narrative planning. M O Riedl, 2003Mateas and Sengers10</p>
<p>Thespian: Using multi-agent fitting to craft interactive drama. M Si, S C Marsella, D V Pynadath, Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems. the fourth international joint conference on Autonomous agents and multiagent systems2005</p>
<p>Learning to speak and act in a fantasy text adventure game. J Urbanek, A Fan, S Karamcheti, S Jain, S Humeau, E Dinan, T Rocktschel, D Kiela, A Szlam, J Weston, arXiv:1903.030942019arXiv preprint</p>
<p>Meta-Reinforcement Learning for Mastering Multiple Skills and Generalizing across Environments in Text-based Games. P Velikovi, G Cucurull, A Casanova, A Romero, P Li, Y Bengio, Z Zhao, M Sun, X Ma, Proceedings of the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing. the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing2018. 2021International Conference on Learning Representations</p>            </div>
        </div>

    </div>
</body>
</html>
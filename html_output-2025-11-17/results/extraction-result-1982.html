<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1982 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1982</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1982</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-278394531</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.04769v1.pdf" target="_blank">Vision-Language-Action Models: Concepts, Progress, Applications and Challenges</a></p>
                <p><strong>Paper Abstract:</strong> Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents. This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence.>Vision-language-action, Agentic AI, AI Agents, Vision-language Models</p>
                <p><strong>Cost:</strong> 0.041</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1982.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1982.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A visuomotor framework that uses CLIP-derived semantic embeddings to condition pixel-level manipulation policies, enabling language-conditioned manipulation without explicit language parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cliport: What and where pathways for robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conditions visuomotor policies on CLIP visual-text embeddings and decodes pixel-wise transport/action via a convolutional decoder (transport/lingUNet-style), enabling semantic grounding for manipulation without explicit language parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>CLIP ResNet-based encoder (e.g., ResNet-50 CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>CLIP-style contrastive pretraining on large web imageâ€“text corpora (web-scale image-text pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Semantic conditioning: policy input is conditioned on CLIP image/text embeddings (embedding-level conditioning) which ground instructions to image regions used by the pixel decoder</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level / spatial transport maps (image patches pixels)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit pixel-space spatial maps (transport maps); spatial grounding via pixel coordinates</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>robotic tabletop manipulation (CLIPort benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>robot-camera tabletop images (simulation/real robot tabletop setups)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>conditioning of policy on CLIP embeddings (embedding-level concatenation/conditioning)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Using CLIP embeddings enables semantic grounding for pixel-level manipulation and avoids explicit parsing, simplifying the visuomotor pipeline and enabling language-conditioned policies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1982.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1982.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based VLA that processes object-centric visual tokens alongside instruction tokens to enable few-shot generalization across spatial reasoning and manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VIMA: General robot manipulation with multimodal prompts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses a transformer encoder to jointly process object-centric visual tokens (from detectors / segmentation) and language tokens (T5), aligning object-centric perception with instruction tokens to produce action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Vision Transformer (ViT) plus Mask R-CNN for object-centric token extraction</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Standard pretrained ViT and Mask R-CNN backbones (not further specified in review; typically ImageNet/COCO pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Object-centric cross-modal fusion: transformer jointly attends to object-centric visual tokens and instruction tokens to ground language to object instances and spatial relations</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>object-centric / region-level</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit object-centric spatial tokens (object bounding boxes / regions)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation / spatial reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>few-shot spatial reasoning/manipulation benchmarks (VIMA data tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>tabletop/robotic manipulation scenes (robot camera views)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>few-shot generalization / success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>joint transformer processing of object tokens and language tokens (cross-attention within transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Object-centric tokenization combined with joint transformer processing enables few-shot generalization on spatial reasoning tasks by grounding language to object instances.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1982.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1982.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2 (Robotic Transformer 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified VLA that treats action generation as autoregressive token generation by fusing visual patch tokens and language tokens in a transformer; discretizes actions for tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-2: Vision-language-action models transfer web knowledge to robotic control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unified transformer that fuses ViT image patch tokens and language embeddings and autoregressively generates action tokens; actions are discretized (DCT compression + BPE-like encoding) so control is treated analogously to text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Vision Transformer (ViT) producing image patch tokens</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>ViT / vision-language pretraining on web-scale image-text corpora and co-finetuning on robot demonstration datasets (RT-1/RT-X style datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Cross-attention / joint tokenization in a unified transformer: language and vision tokens are aligned in a shared latent space and condition autoregressive action token generation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>image patch-level (ViT patch tokens) / tokenized action space</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit via patch token positions (no explicit 3D coordinates described)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>manipulation and generalist robot control</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>RT-1/RT-2 robotics benchmarks (robot demonstration datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>real-world robot camera streams and demonstration videos</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>relative improvement on novel objects</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>63% improvement in performance on novel objects (relative improvement reported)</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Reported +63% improvement on novel-object performance when using RT-2 style tokenized action grounding and co-training</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Co-finetuning on web-scale vision-language and robot trajectory data to align semantic priors with physical actions (mitigates some domain shift)</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Reported substantial improvement (+63%) for novel object handling</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Leverages large web-scale and robot demonstration data; report emphasizes benefit of web-scale priors for zero-shot generalization</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>joint tokenization and cross-attention within a unified transformer (vision+language+state tokens condition action token decoding)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Treating action generation as autoregressive token generation with joint vision-language tokenization enables strong generalization to novel objects when pretrained on web-scale multimodal data plus robot demonstrations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1982.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1982.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EF-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Early-Fusion VLA (EF-VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early-fusion VLA variant that keeps frozen CLIP encoders and fuses image-text embeddings early in the transformer backbone, preserving semantic alignment and improving compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Early fusion helps vision language action models generalize better</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EF-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encodes image and text with frozen CLIP encoders and fuses their embeddings early in the transformer's backbone (prior to action prediction), maintaining CLIP-learned semantic alignment while training the policy head.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Frozen CLIP encoders (CLIP backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>CLIP pretraining on large web image-text pairs (contrastive)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Early fusion of pre-aligned CLIP image and text embeddings into the policy backbone (preserves pretraining alignment rather than fine-tuning vision-language modules)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>global embedding / multi-level (preserves CLIP semantic alignment across image-text)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit (no explicit 3D or per-patch augmentation described)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>manipulation, compositional tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>compositional manipulation benchmarks (reported in ICLR 2025 workshop work)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>robotic manipulation scenes (tabletop/bench)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate / task performance</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported +20% performance improvement on compositional manipulation tasks; reached 85% success on previously unseen goal descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Approximately 20% lower success on compositional manipulation when not using early fusion (reported as the ablation/comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>+20% success on compositional tasks; 85% success on unseen goal descriptions versus lower baseline</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Frozen CLIP encoders preserved semantic consistency; EF-VLA outperformed approaches that fine-tuned vision-language modules for compositional generalization while maintaining computational efficiency</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Preserves pretrained alignment to reduce overfitting on robot data; early fusion improves robustness to distributional shifts in instructions</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Achieved high success (85%) on previously unseen goal descriptions (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Shows benefit of freezing VLM (CLIP) and early-fusing embeddings rather than fine-tuning the VLM in-domain (improves generalization and efficiency)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Relies on large CLIP pretraining to supply robust semantic priors used by the early-fusion strategy</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>early fusion (embed then fuse before policy layers)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Early fusion of frozen CLIP visual-text embeddings preserves semantic priors from pretraining, yielding substantial improvements in compositional generalization and success on unseen instructions with lower compute and less catastrophic forgetting.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1982.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1982.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Groot N1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gr00t N1 (Dual-System VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-system VLA combining a fast diffusion-based low-level controller (System 1) and a slow LLM-based planner (System 2) to provide multi-timescale grounding and safer, more adaptive control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gr00t n1: An open foundation model for generalist humanoid robots</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Groot N1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dual-system architecture: System 1 is a low-latency diffusion policy for reactive, fine-grained control; System 2 is an LLM planner for high-level task decomposition and sequencing. The planner issues subgoals that are grounded by the fast controller into motor actions.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Hierarchical grounding: high-level language-grounded planning by an LLM decomposes tasks into subgoals; subgoals are grounded into actions by a diffusion-based low-level controller that uses perception inputs for reactive control</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>hierarchical: high-level symbolic/subgoal tokens + low-level continuous action representations</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit multi-level (planner-level semantics; controller-level kinematics); no single explicit 3D format described</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>multi-stage household manipulation / whole-body control</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>multi-stage household manipulation benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>real-world humanoid robot environments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate / collision failures</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Outperformed monolithic models by +17% success rate and reduced collision failures by ~28% (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>+17% SR and -28% collision failures relative to monolithic single-network baselines</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Temporal/synchronization mismatch between System 2 (LLM planner) inference (~800 ms) and System 1 control loops (~10 ms) can cause jerkiness and suboptimal real-time behavior</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Asynchronous interaction between planner and controller can produce jerky or suboptimal movements in sensitive tasks; reported tradeoff between latency and trajectory smoothness when parallel decoding is used</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>hierarchical communication between planner (LLM) and low-level diffusion controller; coordinated subgoal grounding</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Dual-system architectures improve multi-timescale grounding and safety (better success and fewer collisions) but require careful synchronization to avoid motion jerkiness and timing-induced failures.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1982.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1982.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SC-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correcting VLA (SC-VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-correcting VLA framework that detects execution failures and invokes a secondary chain-of-thought reasoning path (LLM) to diagnose and recover, improving recoverability in cluttered or adversarial settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SC-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-path execution loop: a fast inference path (lightweight transformer) for normal operation, and a slow chain-of-thought correction path that uses an internal LLM to diagnose failures (e.g., occlusion misidentification) and propose corrective strategies (viewpoint change, regrasp), then re-invoke the controller.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Failure-triggered LLM-assisted grounding: when the fast path detects failure (e.g., unsuccessful grasp), an LLM is queried to re-ground the instruction and propose corrective perceptual/action strategies</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>multi-level (fast reactive action tokens + LLM-level diagnostic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit; corrective actions may include viewpoint or pose adjustments</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>manipulation in cluttered/adversarial environments</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>closed-loop manipulation benchmarks (self-correction experiments described in review)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>real-world cluttered manipulation scenes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task failure rate / recoverability</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported reduction in task failure rates by 35% and notable improvement in recoverability in cluttered/adversarial environments</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Reduced failure rates by 35% when invoking LLM-based correction path</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Occluded object references and misidentifications drive many failures; SC-VLA specifically addresses occlusion-induced failures by recommending active perceptual strategies</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Frequent failures arise from occlusion and repeated misidentification; SC-VLA recovers by suggesting viewpoint changes or reorientations</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Invokes corrective strategies at runtime rather than relying solely on pretraining; helps adapt to unanticipated real-world perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>fast transformer fusion for normal operation + LLM-based diagnostic reasoning invoked on failure</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Self-correction via internal chain-of-thought diagnostics increases recoverability from grounding failures (e.g., occlusion) and reduces overall task failure rates substantially.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1982.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1982.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter open-source VLA co-finetuned on large robot demonstration datasets that uses dual vision encoders and a Llama-2 language backbone to align semantic priors with robot control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenVLA: An open-source vision-language-action model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A co-fine-tuned VLA (7B) using dual vision encoders (DINOv2 and SigLIP) with Llama-2 language model, trained on ~970k real-world robot demonstrations and web-scale vision-language corpora to align semantic knowledge with action policies.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Dual vision encoders: DINOv2 and SigLIP (frozen or co-finetuned depending on run)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>DINOv2 (self-supervised on large image collections), SigLIP (vision-language pretrained variant); co-finetuned on robot demonstration datasets (970k demos)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Co-fine-tuning and joint cross-attention: align web-scale semantic priors from VLMs with robot trajectory/action decoder via co-fine-tuning and joint token-level fusion</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>multi-level (vision tokens + language tokens + action tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit via vision tokens; no single explicit 3D format described though dual encoders provide complementary spatial/semantic cues</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>robotic manipulation and generalist embodied tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>real-world robot demonstration benchmarks (OpenVLA benchmark suite)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>real-world robot demonstrations (diverse scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported 16.5% higher success rate than a 55B-parameter RT-2 variant on cited benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Co-fine-tuning VLMs with robot data yields +16.5% success over a much larger RT-2 variant (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Dual-encoder (DINOv2 + SigLIP) co-fine-tuned configuration outperformed a larger RT-2-X 55B model; co-fine-tuning of VLMs with robot demos yields superior sample-efficient grounding</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Handles domain shift via co-fine-tuning on large robot demonstration datasets and synthetic augmentation (e.g., UniSim) to expose occlusions and lighting variations</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Co-fine-tuning of visual encoders and language backbone is a core design choice (report indicates co-fine-tuning leads to strong generalization versus frozen-only approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Combines web-scale VLM pretraining with large robot demo finetuning; shows co-finetuning yields strong generalization with moderate parameter counts</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>joint token fusion / cross-attention between vision and language tokens during co-fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Demonstrated strong performance with 970k real-world demonstrations co-finetuned (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Co-fine-tuning VLMs with large robot demonstration datasets aligns semantic priors to action spaces effectively and can outperform much larger models when grounding is properly aligned.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1982.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1982.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ObjectVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ObjectVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA aimed at open-world object manipulation that uses advanced vision-language grounding but shows limited generalization to novel objects in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ObjectVLA: End-to-end open-world object manipulation without demonstration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ObjectVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>End-to-end open-world object manipulation model that employs hybrid vision-language grounding schemes to map linguistic references to object affordances and actions in the scene.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Hybrid vision-language grounding (details not fully enumerated in review): aligns linguistic references to perceived object regions/affordances to enable manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>object-centric / scene-level</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit object-region representations (bounding boxes / affordance points)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>open-world object manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>open-world manipulation evaluation (ObjectVLA benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>real-world open-world scenes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>generalization to novel objects (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Generalizes to ~64% of novel objects (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Limited open-world robustness: only ~64% generalization to novel objects indicates grounding/visual generalization is a bottleneck</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Failures concentrated on out-of-distribution objects and rare/unseen object categories</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>~64% success on novel objects (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>hybrid vision-language grounding (object-level matching + semantic alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Even specialized hybrid grounding schemes struggle with open-world object generalization; ObjectVLA attains only ~64% success on novel objects, highlighting a key grounding limitation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1982.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1982.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Occllama</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Occllama</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative occupancy-language-action world model that specifically addresses occluded object references via attention-based mechanisms to improve grounding under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Occllama: An occupancy-language-action generative world model for autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Occllama</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Integrates occupancy/3D scene representations with language-conditioned action generation; employs attention-based mechanisms to reason about occluded object references and maintain grounding under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Attention-based mechanisms over occupancy/scene representations to disambiguate occluded references and ground language to likely object locations</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>scene-level / 3D occupancy</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>occupancy grids / 3D-space reasoning (explicit occupancy representation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>autonomous driving / navigation under occlusion</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Autonomous driving scenarios with occlusions</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>egocentric driving camera + LiDAR-like occupancy representations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Designed specifically to address occlusion-induced grounding failures by modeling occupancy and using attention to resolve hidden references</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Targets failure mode where occluded objects are misreferenced; uses generative occupancy reasoning to mitigate</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>cross-attention between language tokens and occupancy/scene representations</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Explicit occupancy/scene representations combined with attention help resolve occlusion-driven grounding failures in navigation/driving contexts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1982.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1982.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CogACT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CogACT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An industrial VLA framework that uses a diffusion-based action transformer to model action sequences robustly and to ground vision-language inputs for high-precision industrial manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CogACT: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CogACT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Modular VLA with a visual-language encoder feeding a diffusion-based action transformer (DiT-Base) that generates robust, adaptive motor actions; designed to generalize to unseen tools and layouts in industrial settings.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>High-level visual-language encoder (e.g., Prismatic-7B mentioned in review) - specific backbone not strictly enumerated in review</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Large vision-language pretraining (Prismatic-style) and finetuning on industrial datasets; exact datasets not enumerated in review</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Diffusion-based action transformer grounds high-level VLM embeddings into action sequences; modular separation preserves interpretability and robustness to unseen tools</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>scene-level / object-centric to action sequence representation</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit via visual-language encoder; action transformer encodes temporal-spatial constraints</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>industrial robotic manipulation (assembly, screw fastening, part sorting)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>industrial manipulation benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>factory/assembly-line real-world images and robot sensor streams</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported to outperform OpenVLA by over 59% in real-world task success rates for high-precision industrial tasks (reported in review)</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Large improvement (+59% reported) over prior OpenVLA baseline in high-precision industrial tasks, attributed to diffusion action modeling and modular grounding</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Efficient fine-tuning and modular adaptation allow CogACT to transfer across robot embodiments</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Strong generalization to unseen tools and part layouts reported</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>visual-language encoder to diffusion transformer bridging (cross-modal conditioning into diffusion-based action generator)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Diffusion-based action transformers conditioned on robust VLM embeddings produce highly adaptive, generalizable action sequences for industrial manipulation, improving real-world success markedly.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1982.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1982.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diffusion Policy / Pi-0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diffusion Policy (Pi-0 / Pi-0 Fast variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Diffusion-based action policy family that models multi-modal action distributions for richer action generation but at higher computational cost; Pi-0 Fast uses compressed action tokenization for real-time performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diffusion policy: Visuomotor policy learning via action diffusion</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Diffusion Policy (Pi-0 / Pi-0 Fast)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Diffusion-based generative policies model stochastic, multi-modal action distributions for control (Pi-0 family); Pi-0 Fast combines diffusion-based modeling with FAST action tokenization to enable high-frequency control.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>ResNet / ViT style encoders in cited works (varies by implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Varies; diffusion policy works often trained on robot trajectory datasets and vision backbones pretrained on standard image datasets</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Condition action diffusion sampling on fused vision-language-state embeddings to produce diverse action candidates; token compression (FAST) used in Pi-0 Fast to reduce latency</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>low-level continuous action distributions / trajectory chunks</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit in action-conditioned state/pose tokens; no single 3D representation specified</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>dexterous manipulation / general robot control</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>manipulation benchmarks using diffusion policy families (e.g., Pi-0 evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>real-world and simulated robotic demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>action diversity / success rate / latency (Hz)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Diffusion policies provide richer multimodal action representations but are ~3Ã— more computationally expensive; Pi-0 Fast reported 15Ã— faster inference with compressed tokenization enabling 200 Hz control (reported for Pi-0 Fast variant)</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Improves capability to represent multiple viable action modes compared to single-mode autoregressive decoders; trade-off is higher compute</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Higher compute makes real-time deployment challenging unless token compression or hardware acceleration used</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>conditioning diffusion sampling on fused multimodal embeddings (cross-attention like conditioning)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Diffusion-based policies better capture multimodal action distributions useful for ambiguous manipulation tasks, but require careful efficiency strategies (e.g., token compression) for real-time grounding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1982.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e1982.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FAST tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FAST (Efficient Action Tokenization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A compressed action tokenization method that reformulates continuous action outputs into frequency-domain tokens to drastically reduce autoregressive decoding steps and enable high-frequency control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fast: Efficient action tokenization for vision-language-action models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FAST tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Compresses continuous action sequences using frequency-domain representations (e.g., DCT-like) into a small number of discrete tokens representing longer action windows; reduces autoregressive steps and enables higher control loop rates.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Not a visual grounding mechanism per se; reduces action decoding bottleneck so grounded policies can run at higher frequency by compressing action tokens</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>action-token level (compressed trajectory tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>high-frequency manipulation and control</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>used as component in Pi-0 Fast and other VLA models (high-frequency control tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>inference speed / control Hz</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported up to 15Ã— faster inference for Pi-0 Fast variant; tokenizing 1000 ms action windows into 16 tokens enabled 200 Hz control (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Indirect: reduces decoding latency enabling grounded action policies to meet real-time control rates (e.g., 200 Hz)</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Compression can trade off some trajectory granularity / smoothness for speed; reported minimal accuracy loss in many tasks</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>action-token compression / chunking (not a vision-language fusion mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Compressed action tokenization (FAST) is an effective mitigation for the autoregressive decoding bottleneck, enabling grounded VLAs to run at control frequencies required by real robots with small loss in trajectory fidelity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1982.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e1982.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low-Rank Adaptation (LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-efficient fine-tuning method that injects small low-rank adapter matrices into frozen transformer layers, enabling large VLA backbones to be adapted for robotic grounding with few trainable parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lora: Low-rank adaptation of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LoRA adapters</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Insert lightweight low-rank adapter matrices into frozen transformer layers so a large VLM/LLM backbone can be adapted with a small number of trainable parameters (e.g., tens of millions) for robot grounding tasks while keeping base weights frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>applies to transformer-based encoders/decoders (VLMs/LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>leverages pretrained VLM/LLM backbones (e.g., CLIP, DINOv2, LLaMA) and adapts via LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Parameter-efficient adaptation preserves pretrained visual-text alignment while allowing task-specific grounding by training only adapter matrices that modulate attention/FFN layers</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>model-weight adaptation (affects embedding and attention representations)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>general VLA adaptation across manipulation, navigation, and embodied tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>used in multiple VLA models (OpenVLA, Pi-0 Fast, TinyVLA) as adaptation method</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>applies across domains (simulation and real robot data)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>compute cost / adaptation speed / retained performance</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported cut in GPU compute by ~70% for OpenVLA LoRA adaptation (20M adapter params on 7B backbone) with preserved high-level performance (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Enables efficient grounding adaptation with minimal computeâ€”preserves pretrained semantic grounding and reduces catastrophic forgetting</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>LoRA enables practical finetuning on in-domain robot data to reduce domain shift with low compute cost</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>LoRA is a middle ground between frozen and full fine-tuning: adapters are trained while backbone stays frozen; reported to retain >95% of full-finetune performance in many cases</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>model adaptation (does not change fusion mechanism directly but allows efficient co-finetuning of fusion layers)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>LoRA enables cost-effective adaptation of large VLM/LLM backbones for embodied grounding, preserving pretrained semantic alignment while dramatically reducing compute and trainable parameter count.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1982.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e1982.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PerceptionBottlenecks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perception and Grounding Bottlenecks Identified in VLAs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey-level synthesis of major perception/grounding failure modes and bottlenecks limiting VLA performance in embodied tasks (real-time latency, occlusion, limited object generalization, dataset bias, and energy constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>analysis / diagnostic summary</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Aggregated analysis from the review identifying common grounding bottlenecks: sequential autoregressive decoding latency (3â€“5 Hz typical vs. required 100+ Hz), high token and memory bandwidth requirements, occlusion/missing references, dataset bias, limited novel-object generalization, and synchronization issues in hierarchical systems.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>N/A (analysis of mechanisms and bottlenecks across models, not a single model)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>covers pixel-, region-, object-, and scene-level representation bottlenecks</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>notes missing/existing use of explicit spatial representations (depth, occupancy) as gap area</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>manipulation, navigation, autonomous driving, humanoid control</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>multiple benchmarks discussed across review (manipulation, navigation, driving)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>simulation and real-world robotics domains</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative reported bottlenecks: typical autoregressive VLA decoding at ~3â€“5 Hz (insufficient vs 100+ Hz required for precise control); embedded memory demand example: ~1.2 GB/s for 400 vision tokens Ã— 512 dims; emergency-stop latencies 200â€“500 ms reported for some safety checks; OpenVLA reported overlooking ~23% of object references; dataset biases ~17% skew reported in web datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Key issues: autoregressive decoding latency (3â€“5 Hz vs needed >100 Hz), high token/memory bandwidth (e.g., 400 tokens Ã— 512 dims â‰ˆ 1.2 GB/s), occlusion and partial observability, dataset bias (â‰ˆ17% skew), and limited novel-object generalization (ObjectVLA ~64% success; OpenVLA misses ~23% object refs).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Frequent failure modes include occlusion-induced misidentification, viewpoint sensitivity, limited out-of-distribution object handling, synchronization mismatch between planner and low-level control, and safety/latency-related collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Surveyed solutions include co-finetuning on robot demonstrations, synthetic augmentation (UniSim), domain randomization, and sim-to-real fine-tuning; performance drops noted when not applied (up to ~32% drop in sim-to-real as cited for some models).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Multiple reports: ObjectVLA ~64% generalization; RT-2 reports +63% improvement on novel objects with its approach; OpenVLA still misses ~23% object refs in novel scenes per review.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Discussion: frozen VLMs (early fusion) can preserve semantic priors and improve compositional generalization; co-finetuning can better align semantics to control but needs parameter-efficient methods (LoRA) to be tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Review notes benefits of web-scale pretraining (CLIP, LAION) combined with robot demo finetuning; OpenVLA shows co-finetuning uses fewer params but outperforms larger models, indicating scale interacts with alignment quality.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Surveyed fusion mechanisms include cross-attention, early fusion (embedding-level), concatenation, token unification, and object-centric attention; no single dominant solution â€” tradeoffs exist between preserving semantic priors vs. adaptability.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Noted techniques improving sample efficiency: co-finetuning, LoRA, retrieval-augmentation, imitation learning (behavior cloning), and diffusion-based policies for diverse action modeling; reported sample reductions e.g., LoRA reduces GPU compute rather than dataset size.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Grounding effectiveness hinges on (1) alignment between pretrained semantic priors and robot dynamics (co-finetuning helps), (2) representation choice (object-centric and explicit 3D/occupancy aids spatial grounding), and (3) inference/decoder efficiency (compressed tokens, parallel decoding) to meet real-time control requirements; occlusion, dataset bias and novel-object generalization remain primary failure drivers.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Rt-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 2)</em></li>
                <li>Cliport: What and where pathways for robotic manipulation <em>(Rating: 2)</em></li>
                <li>VIMA: General robot manipulation with multimodal prompts <em>(Rating: 2)</em></li>
                <li>OpenVLA: An open-source vision-language-action model <em>(Rating: 2)</em></li>
                <li>ObjectVLA: End-to-end open-world object manipulation without demonstration <em>(Rating: 2)</em></li>
                <li>Early fusion helps vision language action models generalize better <em>(Rating: 2)</em></li>
                <li>Diffusion policy: Visuomotor policy learning via action diffusion <em>(Rating: 2)</em></li>
                <li>Fast: Efficient action tokenization for vision-language-action models <em>(Rating: 2)</em></li>
                <li>Occllama: An occupancy-language-action generative world model for autonomous driving <em>(Rating: 1)</em></li>
                <li>CogACT: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1982",
    "paper_id": "paper-278394531",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "CLIPort",
            "name_full": "CLIPort",
            "brief_description": "A visuomotor framework that uses CLIP-derived semantic embeddings to condition pixel-level manipulation policies, enabling language-conditioned manipulation without explicit language parsing.",
            "citation_title": "Cliport: What and where pathways for robotic manipulation",
            "mention_or_use": "mention",
            "model_name": "CLIPort",
            "model_description": "Conditions visuomotor policies on CLIP visual-text embeddings and decodes pixel-wise transport/action via a convolutional decoder (transport/lingUNet-style), enabling semantic grounding for manipulation without explicit language parsing.",
            "visual_encoder_type": "CLIP ResNet-based encoder (e.g., ResNet-50 CLIP)",
            "visual_encoder_pretraining": "CLIP-style contrastive pretraining on large web imageâ€“text corpora (web-scale image-text pairs)",
            "grounding_mechanism": "Semantic conditioning: policy input is conditioned on CLIP image/text embeddings (embedding-level conditioning) which ground instructions to image regions used by the pixel decoder",
            "representation_level": "pixel-level / spatial transport maps (image patches pixels)",
            "spatial_representation": "implicit pixel-space spatial maps (transport maps); spatial grounding via pixel coordinates",
            "embodied_task_type": "object manipulation",
            "embodied_task_name": "robotic tabletop manipulation (CLIPort benchmarks)",
            "visual_domain": "robot-camera tabletop images (simulation/real robot tabletop setups)",
            "performance_metric": "success rate",
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": "",
            "failure_mode_analysis": "",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "conditioning of policy on CLIP embeddings (embedding-level concatenation/conditioning)",
            "sample_efficiency": null,
            "key_findings_grounding": "Using CLIP embeddings enables semantic grounding for pixel-level manipulation and avoids explicit parsing, simplifying the visuomotor pipeline and enabling language-conditioned policies.",
            "uuid": "e1982.0"
        },
        {
            "name_short": "VIMA",
            "name_full": "VIMA",
            "brief_description": "A transformer-based VLA that processes object-centric visual tokens alongside instruction tokens to enable few-shot generalization across spatial reasoning and manipulation tasks.",
            "citation_title": "VIMA: General robot manipulation with multimodal prompts",
            "mention_or_use": "mention",
            "model_name": "VIMA",
            "model_description": "Uses a transformer encoder to jointly process object-centric visual tokens (from detectors / segmentation) and language tokens (T5), aligning object-centric perception with instruction tokens to produce action sequences.",
            "visual_encoder_type": "Vision Transformer (ViT) plus Mask R-CNN for object-centric token extraction",
            "visual_encoder_pretraining": "Standard pretrained ViT and Mask R-CNN backbones (not further specified in review; typically ImageNet/COCO pretraining)",
            "grounding_mechanism": "Object-centric cross-modal fusion: transformer jointly attends to object-centric visual tokens and instruction tokens to ground language to object instances and spatial relations",
            "representation_level": "object-centric / region-level",
            "spatial_representation": "implicit object-centric spatial tokens (object bounding boxes / regions)",
            "embodied_task_type": "object manipulation / spatial reasoning",
            "embodied_task_name": "few-shot spatial reasoning/manipulation benchmarks (VIMA data tasks)",
            "visual_domain": "tabletop/robotic manipulation scenes (robot camera views)",
            "performance_metric": "few-shot generalization / success rate",
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": "",
            "failure_mode_analysis": "",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "joint transformer processing of object tokens and language tokens (cross-attention within transformer)",
            "sample_efficiency": null,
            "key_findings_grounding": "Object-centric tokenization combined with joint transformer processing enables few-shot generalization on spatial reasoning tasks by grounding language to object instances.",
            "uuid": "e1982.1"
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2 (Robotic Transformer 2)",
            "brief_description": "A unified VLA that treats action generation as autoregressive token generation by fusing visual patch tokens and language tokens in a transformer; discretizes actions for tokenization.",
            "citation_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "mention_or_use": "mention",
            "model_name": "RT-2",
            "model_description": "Unified transformer that fuses ViT image patch tokens and language embeddings and autoregressively generates action tokens; actions are discretized (DCT compression + BPE-like encoding) so control is treated analogously to text generation.",
            "visual_encoder_type": "Vision Transformer (ViT) producing image patch tokens",
            "visual_encoder_pretraining": "ViT / vision-language pretraining on web-scale image-text corpora and co-finetuning on robot demonstration datasets (RT-1/RT-X style datasets)",
            "grounding_mechanism": "Cross-attention / joint tokenization in a unified transformer: language and vision tokens are aligned in a shared latent space and condition autoregressive action token generation",
            "representation_level": "image patch-level (ViT patch tokens) / tokenized action space",
            "spatial_representation": "implicit via patch token positions (no explicit 3D coordinates described)",
            "embodied_task_type": "manipulation and generalist robot control",
            "embodied_task_name": "RT-1/RT-2 robotics benchmarks (robot demonstration datasets)",
            "visual_domain": "real-world robot camera streams and demonstration videos",
            "performance_metric": "relative improvement on novel objects",
            "performance_value": "63% improvement in performance on novel objects (relative improvement reported)",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": "Reported +63% improvement on novel-object performance when using RT-2 style tokenized action grounding and co-training",
            "has_encoder_comparison": null,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": "",
            "failure_mode_analysis": "",
            "domain_shift_handling": "Co-finetuning on web-scale vision-language and robot trajectory data to align semantic priors with physical actions (mitigates some domain shift)",
            "novel_object_performance": "Reported substantial improvement (+63%) for novel object handling",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Leverages large web-scale and robot demonstration data; report emphasizes benefit of web-scale priors for zero-shot generalization",
            "fusion_mechanism": "joint tokenization and cross-attention within a unified transformer (vision+language+state tokens condition action token decoding)",
            "sample_efficiency": null,
            "key_findings_grounding": "Treating action generation as autoregressive token generation with joint vision-language tokenization enables strong generalization to novel objects when pretrained on web-scale multimodal data plus robot demonstrations.",
            "uuid": "e1982.2"
        },
        {
            "name_short": "EF-VLA",
            "name_full": "Early-Fusion VLA (EF-VLA)",
            "brief_description": "An early-fusion VLA variant that keeps frozen CLIP encoders and fuses image-text embeddings early in the transformer backbone, preserving semantic alignment and improving compositional generalization.",
            "citation_title": "Early fusion helps vision language action models generalize better",
            "mention_or_use": "mention",
            "model_name": "EF-VLA",
            "model_description": "Encodes image and text with frozen CLIP encoders and fuses their embeddings early in the transformer's backbone (prior to action prediction), maintaining CLIP-learned semantic alignment while training the policy head.",
            "visual_encoder_type": "Frozen CLIP encoders (CLIP backbone)",
            "visual_encoder_pretraining": "CLIP pretraining on large web image-text pairs (contrastive)",
            "grounding_mechanism": "Early fusion of pre-aligned CLIP image and text embeddings into the policy backbone (preserves pretraining alignment rather than fine-tuning vision-language modules)",
            "representation_level": "global embedding / multi-level (preserves CLIP semantic alignment across image-text)",
            "spatial_representation": "implicit (no explicit 3D or per-patch augmentation described)",
            "embodied_task_type": "manipulation, compositional tasks",
            "embodied_task_name": "compositional manipulation benchmarks (reported in ICLR 2025 workshop work)",
            "visual_domain": "robotic manipulation scenes (tabletop/bench)",
            "performance_metric": "success rate / task performance",
            "performance_value": "Reported +20% performance improvement on compositional manipulation tasks; reached 85% success on previously unseen goal descriptions",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Approximately 20% lower success on compositional manipulation when not using early fusion (reported as the ablation/comparison)",
            "grounding_improvement": "+20% success on compositional tasks; 85% success on unseen goal descriptions versus lower baseline",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Frozen CLIP encoders preserved semantic consistency; EF-VLA outperformed approaches that fine-tuned vision-language modules for compositional generalization while maintaining computational efficiency",
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": "",
            "failure_mode_analysis": "",
            "domain_shift_handling": "Preserves pretrained alignment to reduce overfitting on robot data; early fusion improves robustness to distributional shifts in instructions",
            "novel_object_performance": "Achieved high success (85%) on previously unseen goal descriptions (reported)",
            "frozen_vs_finetuned": "Shows benefit of freezing VLM (CLIP) and early-fusing embeddings rather than fine-tuning the VLM in-domain (improves generalization and efficiency)",
            "pretraining_scale_effect": "Relies on large CLIP pretraining to supply robust semantic priors used by the early-fusion strategy",
            "fusion_mechanism": "early fusion (embed then fuse before policy layers)",
            "sample_efficiency": null,
            "key_findings_grounding": "Early fusion of frozen CLIP visual-text embeddings preserves semantic priors from pretraining, yielding substantial improvements in compositional generalization and success on unseen instructions with lower compute and less catastrophic forgetting.",
            "uuid": "e1982.3"
        },
        {
            "name_short": "Groot N1",
            "name_full": "Gr00t N1 (Dual-System VLA)",
            "brief_description": "A dual-system VLA combining a fast diffusion-based low-level controller (System 1) and a slow LLM-based planner (System 2) to provide multi-timescale grounding and safer, more adaptive control.",
            "citation_title": "Gr00t n1: An open foundation model for generalist humanoid robots",
            "mention_or_use": "mention",
            "model_name": "Groot N1",
            "model_description": "Dual-system architecture: System 1 is a low-latency diffusion policy for reactive, fine-grained control; System 2 is an LLM planner for high-level task decomposition and sequencing. The planner issues subgoals that are grounded by the fast controller into motor actions.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Hierarchical grounding: high-level language-grounded planning by an LLM decomposes tasks into subgoals; subgoals are grounded into actions by a diffusion-based low-level controller that uses perception inputs for reactive control",
            "representation_level": "hierarchical: high-level symbolic/subgoal tokens + low-level continuous action representations",
            "spatial_representation": "implicit multi-level (planner-level semantics; controller-level kinematics); no single explicit 3D format described",
            "embodied_task_type": "multi-stage household manipulation / whole-body control",
            "embodied_task_name": "multi-stage household manipulation benchmarks",
            "visual_domain": "real-world humanoid robot environments",
            "performance_metric": "success rate / collision failures",
            "performance_value": "Outperformed monolithic models by +17% success rate and reduced collision failures by ~28% (reported)",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": "+17% SR and -28% collision failures relative to monolithic single-network baselines",
            "has_encoder_comparison": null,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Temporal/synchronization mismatch between System 2 (LLM planner) inference (~800 ms) and System 1 control loops (~10 ms) can cause jerkiness and suboptimal real-time behavior",
            "failure_mode_analysis": "Asynchronous interaction between planner and controller can produce jerky or suboptimal movements in sensitive tasks; reported tradeoff between latency and trajectory smoothness when parallel decoding is used",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "hierarchical communication between planner (LLM) and low-level diffusion controller; coordinated subgoal grounding",
            "sample_efficiency": null,
            "key_findings_grounding": "Dual-system architectures improve multi-timescale grounding and safety (better success and fewer collisions) but require careful synchronization to avoid motion jerkiness and timing-induced failures.",
            "uuid": "e1982.4"
        },
        {
            "name_short": "SC-VLA",
            "name_full": "Self-Correcting VLA (SC-VLA)",
            "brief_description": "A self-correcting VLA framework that detects execution failures and invokes a secondary chain-of-thought reasoning path (LLM) to diagnose and recover, improving recoverability in cluttered or adversarial settings.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "SC-VLA",
            "model_description": "Two-path execution loop: a fast inference path (lightweight transformer) for normal operation, and a slow chain-of-thought correction path that uses an internal LLM to diagnose failures (e.g., occlusion misidentification) and propose corrective strategies (viewpoint change, regrasp), then re-invoke the controller.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Failure-triggered LLM-assisted grounding: when the fast path detects failure (e.g., unsuccessful grasp), an LLM is queried to re-ground the instruction and propose corrective perceptual/action strategies",
            "representation_level": "multi-level (fast reactive action tokens + LLM-level diagnostic reasoning)",
            "spatial_representation": "implicit; corrective actions may include viewpoint or pose adjustments",
            "embodied_task_type": "manipulation in cluttered/adversarial environments",
            "embodied_task_name": "closed-loop manipulation benchmarks (self-correction experiments described in review)",
            "visual_domain": "real-world cluttered manipulation scenes",
            "performance_metric": "task failure rate / recoverability",
            "performance_value": "Reported reduction in task failure rates by 35% and notable improvement in recoverability in cluttered/adversarial environments",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": "Reduced failure rates by 35% when invoking LLM-based correction path",
            "has_encoder_comparison": null,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Occluded object references and misidentifications drive many failures; SC-VLA specifically addresses occlusion-induced failures by recommending active perceptual strategies",
            "failure_mode_analysis": "Frequent failures arise from occlusion and repeated misidentification; SC-VLA recovers by suggesting viewpoint changes or reorientations",
            "domain_shift_handling": "Invokes corrective strategies at runtime rather than relying solely on pretraining; helps adapt to unanticipated real-world perturbations",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "fast transformer fusion for normal operation + LLM-based diagnostic reasoning invoked on failure",
            "sample_efficiency": null,
            "key_findings_grounding": "Self-correction via internal chain-of-thought diagnostics increases recoverability from grounding failures (e.g., occlusion) and reduces overall task failure rates substantially.",
            "uuid": "e1982.5"
        },
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA",
            "brief_description": "A 7B-parameter open-source VLA co-finetuned on large robot demonstration datasets that uses dual vision encoders and a Llama-2 language backbone to align semantic priors with robot control.",
            "citation_title": "OpenVLA: An open-source vision-language-action model",
            "mention_or_use": "mention",
            "model_name": "OpenVLA",
            "model_description": "A co-fine-tuned VLA (7B) using dual vision encoders (DINOv2 and SigLIP) with Llama-2 language model, trained on ~970k real-world robot demonstrations and web-scale vision-language corpora to align semantic knowledge with action policies.",
            "visual_encoder_type": "Dual vision encoders: DINOv2 and SigLIP (frozen or co-finetuned depending on run)",
            "visual_encoder_pretraining": "DINOv2 (self-supervised on large image collections), SigLIP (vision-language pretrained variant); co-finetuned on robot demonstration datasets (970k demos)",
            "grounding_mechanism": "Co-fine-tuning and joint cross-attention: align web-scale semantic priors from VLMs with robot trajectory/action decoder via co-fine-tuning and joint token-level fusion",
            "representation_level": "multi-level (vision tokens + language tokens + action tokens)",
            "spatial_representation": "implicit via vision tokens; no single explicit 3D format described though dual encoders provide complementary spatial/semantic cues",
            "embodied_task_type": "robotic manipulation and generalist embodied tasks",
            "embodied_task_name": "real-world robot demonstration benchmarks (OpenVLA benchmark suite)",
            "visual_domain": "real-world robot demonstrations (diverse scenes)",
            "performance_metric": "success rate",
            "performance_value": "Reported 16.5% higher success rate than a 55B-parameter RT-2 variant on cited benchmarks",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": "Co-fine-tuning VLMs with robot data yields +16.5% success over a much larger RT-2 variant (reported)",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Dual-encoder (DINOv2 + SigLIP) co-fine-tuned configuration outperformed a larger RT-2-X 55B model; co-fine-tuning of VLMs with robot demos yields superior sample-efficient grounding",
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": "",
            "failure_mode_analysis": "",
            "domain_shift_handling": "Handles domain shift via co-fine-tuning on large robot demonstration datasets and synthetic augmentation (e.g., UniSim) to expose occlusions and lighting variations",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "Co-fine-tuning of visual encoders and language backbone is a core design choice (report indicates co-fine-tuning leads to strong generalization versus frozen-only approaches)",
            "pretraining_scale_effect": "Combines web-scale VLM pretraining with large robot demo finetuning; shows co-finetuning yields strong generalization with moderate parameter counts",
            "fusion_mechanism": "joint token fusion / cross-attention between vision and language tokens during co-fine-tuning",
            "sample_efficiency": "Demonstrated strong performance with 970k real-world demonstrations co-finetuned (reported)",
            "key_findings_grounding": "Co-fine-tuning VLMs with large robot demonstration datasets aligns semantic priors to action spaces effectively and can outperform much larger models when grounding is properly aligned.",
            "uuid": "e1982.6"
        },
        {
            "name_short": "ObjectVLA",
            "name_full": "ObjectVLA",
            "brief_description": "A VLA aimed at open-world object manipulation that uses advanced vision-language grounding but shows limited generalization to novel objects in practice.",
            "citation_title": "ObjectVLA: End-to-end open-world object manipulation without demonstration",
            "mention_or_use": "mention",
            "model_name": "ObjectVLA",
            "model_description": "End-to-end open-world object manipulation model that employs hybrid vision-language grounding schemes to map linguistic references to object affordances and actions in the scene.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Hybrid vision-language grounding (details not fully enumerated in review): aligns linguistic references to perceived object regions/affordances to enable manipulation",
            "representation_level": "object-centric / scene-level",
            "spatial_representation": "implicit object-region representations (bounding boxes / affordance points)",
            "embodied_task_type": "open-world object manipulation",
            "embodied_task_name": "open-world manipulation evaluation (ObjectVLA benchmarks)",
            "visual_domain": "real-world open-world scenes",
            "performance_metric": "generalization to novel objects (percentage)",
            "performance_value": "Generalizes to ~64% of novel objects (reported)",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Limited open-world robustness: only ~64% generalization to novel objects indicates grounding/visual generalization is a bottleneck",
            "failure_mode_analysis": "Failures concentrated on out-of-distribution objects and rare/unseen object categories",
            "domain_shift_handling": null,
            "novel_object_performance": "~64% success on novel objects (reported)",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "hybrid vision-language grounding (object-level matching + semantic alignment)",
            "sample_efficiency": null,
            "key_findings_grounding": "Even specialized hybrid grounding schemes struggle with open-world object generalization; ObjectVLA attains only ~64% success on novel objects, highlighting a key grounding limitation.",
            "uuid": "e1982.7"
        },
        {
            "name_short": "Occllama",
            "name_full": "Occllama",
            "brief_description": "A generative occupancy-language-action world model that specifically addresses occluded object references via attention-based mechanisms to improve grounding under partial observability.",
            "citation_title": "Occllama: An occupancy-language-action generative world model for autonomous driving",
            "mention_or_use": "mention",
            "model_name": "Occllama",
            "model_description": "Integrates occupancy/3D scene representations with language-conditioned action generation; employs attention-based mechanisms to reason about occluded object references and maintain grounding under partial observability.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Attention-based mechanisms over occupancy/scene representations to disambiguate occluded references and ground language to likely object locations",
            "representation_level": "scene-level / 3D occupancy",
            "spatial_representation": "occupancy grids / 3D-space reasoning (explicit occupancy representation)",
            "embodied_task_type": "autonomous driving / navigation under occlusion",
            "embodied_task_name": "Autonomous driving scenarios with occlusions",
            "visual_domain": "egocentric driving camera + LiDAR-like occupancy representations",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Designed specifically to address occlusion-induced grounding failures by modeling occupancy and using attention to resolve hidden references",
            "failure_mode_analysis": "Targets failure mode where occluded objects are misreferenced; uses generative occupancy reasoning to mitigate",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "cross-attention between language tokens and occupancy/scene representations",
            "sample_efficiency": null,
            "key_findings_grounding": "Explicit occupancy/scene representations combined with attention help resolve occlusion-driven grounding failures in navigation/driving contexts.",
            "uuid": "e1982.8"
        },
        {
            "name_short": "CogACT",
            "name_full": "CogACT",
            "brief_description": "An industrial VLA framework that uses a diffusion-based action transformer to model action sequences robustly and to ground vision-language inputs for high-precision industrial manipulation.",
            "citation_title": "CogACT: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation",
            "mention_or_use": "mention",
            "model_name": "CogACT",
            "model_description": "Modular VLA with a visual-language encoder feeding a diffusion-based action transformer (DiT-Base) that generates robust, adaptive motor actions; designed to generalize to unseen tools and layouts in industrial settings.",
            "visual_encoder_type": "High-level visual-language encoder (e.g., Prismatic-7B mentioned in review) - specific backbone not strictly enumerated in review",
            "visual_encoder_pretraining": "Large vision-language pretraining (Prismatic-style) and finetuning on industrial datasets; exact datasets not enumerated in review",
            "grounding_mechanism": "Diffusion-based action transformer grounds high-level VLM embeddings into action sequences; modular separation preserves interpretability and robustness to unseen tools",
            "representation_level": "scene-level / object-centric to action sequence representation",
            "spatial_representation": "implicit via visual-language encoder; action transformer encodes temporal-spatial constraints",
            "embodied_task_type": "industrial robotic manipulation (assembly, screw fastening, part sorting)",
            "embodied_task_name": "industrial manipulation benchmarks",
            "visual_domain": "factory/assembly-line real-world images and robot sensor streams",
            "performance_metric": "task success rate",
            "performance_value": "Reported to outperform OpenVLA by over 59% in real-world task success rates for high-precision industrial tasks (reported in review)",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": "Large improvement (+59% reported) over prior OpenVLA baseline in high-precision industrial tasks, attributed to diffusion action modeling and modular grounding",
            "has_encoder_comparison": null,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": "",
            "failure_mode_analysis": "",
            "domain_shift_handling": "Efficient fine-tuning and modular adaptation allow CogACT to transfer across robot embodiments",
            "novel_object_performance": "Strong generalization to unseen tools and part layouts reported",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "visual-language encoder to diffusion transformer bridging (cross-modal conditioning into diffusion-based action generator)",
            "sample_efficiency": null,
            "key_findings_grounding": "Diffusion-based action transformers conditioned on robust VLM embeddings produce highly adaptive, generalizable action sequences for industrial manipulation, improving real-world success markedly.",
            "uuid": "e1982.9"
        },
        {
            "name_short": "Diffusion Policy / Pi-0",
            "name_full": "Diffusion Policy (Pi-0 / Pi-0 Fast variants)",
            "brief_description": "Diffusion-based action policy family that models multi-modal action distributions for richer action generation but at higher computational cost; Pi-0 Fast uses compressed action tokenization for real-time performance.",
            "citation_title": "Diffusion policy: Visuomotor policy learning via action diffusion",
            "mention_or_use": "mention",
            "model_name": "Diffusion Policy (Pi-0 / Pi-0 Fast)",
            "model_description": "Diffusion-based generative policies model stochastic, multi-modal action distributions for control (Pi-0 family); Pi-0 Fast combines diffusion-based modeling with FAST action tokenization to enable high-frequency control.",
            "visual_encoder_type": "ResNet / ViT style encoders in cited works (varies by implementation)",
            "visual_encoder_pretraining": "Varies; diffusion policy works often trained on robot trajectory datasets and vision backbones pretrained on standard image datasets",
            "grounding_mechanism": "Condition action diffusion sampling on fused vision-language-state embeddings to produce diverse action candidates; token compression (FAST) used in Pi-0 Fast to reduce latency",
            "representation_level": "low-level continuous action distributions / trajectory chunks",
            "spatial_representation": "implicit in action-conditioned state/pose tokens; no single 3D representation specified",
            "embodied_task_type": "dexterous manipulation / general robot control",
            "embodied_task_name": "manipulation benchmarks using diffusion policy families (e.g., Pi-0 evaluations)",
            "visual_domain": "real-world and simulated robotic demonstrations",
            "performance_metric": "action diversity / success rate / latency (Hz)",
            "performance_value": "Diffusion policies provide richer multimodal action representations but are ~3Ã— more computationally expensive; Pi-0 Fast reported 15Ã— faster inference with compressed tokenization enabling 200 Hz control (reported for Pi-0 Fast variant)",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": "Improves capability to represent multiple viable action modes compared to single-mode autoregressive decoders; trade-off is higher compute",
            "has_encoder_comparison": null,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": "",
            "failure_mode_analysis": "Higher compute makes real-time deployment challenging unless token compression or hardware acceleration used",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "conditioning diffusion sampling on fused multimodal embeddings (cross-attention like conditioning)",
            "sample_efficiency": null,
            "key_findings_grounding": "Diffusion-based policies better capture multimodal action distributions useful for ambiguous manipulation tasks, but require careful efficiency strategies (e.g., token compression) for real-time grounding.",
            "uuid": "e1982.10"
        },
        {
            "name_short": "FAST tokenization",
            "name_full": "FAST (Efficient Action Tokenization)",
            "brief_description": "A compressed action tokenization method that reformulates continuous action outputs into frequency-domain tokens to drastically reduce autoregressive decoding steps and enable high-frequency control.",
            "citation_title": "Fast: Efficient action tokenization for vision-language-action models",
            "mention_or_use": "mention",
            "model_name": "FAST tokenization",
            "model_description": "Compresses continuous action sequences using frequency-domain representations (e.g., DCT-like) into a small number of discrete tokens representing longer action windows; reduces autoregressive steps and enables higher control loop rates.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Not a visual grounding mechanism per se; reduces action decoding bottleneck so grounded policies can run at higher frequency by compressing action tokens",
            "representation_level": "action-token level (compressed trajectory tokens)",
            "spatial_representation": null,
            "embodied_task_type": "high-frequency manipulation and control",
            "embodied_task_name": "used as component in Pi-0 Fast and other VLA models (high-frequency control tasks)",
            "visual_domain": null,
            "performance_metric": "inference speed / control Hz",
            "performance_value": "Reported up to 15Ã— faster inference for Pi-0 Fast variant; tokenizing 1000 ms action windows into 16 tokens enabled 200 Hz control (reported)",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": "Indirect: reduces decoding latency enabling grounded action policies to meet real-time control rates (e.g., 200 Hz)",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": "",
            "failure_mode_analysis": "Compression can trade off some trajectory granularity / smoothness for speed; reported minimal accuracy loss in many tasks",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "action-token compression / chunking (not a vision-language fusion mechanism)",
            "sample_efficiency": null,
            "key_findings_grounding": "Compressed action tokenization (FAST) is an effective mitigation for the autoregressive decoding bottleneck, enabling grounded VLAs to run at control frequencies required by real robots with small loss in trajectory fidelity.",
            "uuid": "e1982.11"
        },
        {
            "name_short": "LoRA",
            "name_full": "Low-Rank Adaptation (LoRA)",
            "brief_description": "A parameter-efficient fine-tuning method that injects small low-rank adapter matrices into frozen transformer layers, enabling large VLA backbones to be adapted for robotic grounding with few trainable parameters.",
            "citation_title": "Lora: Low-rank adaptation of large language models",
            "mention_or_use": "mention",
            "model_name": "LoRA adapters",
            "model_description": "Insert lightweight low-rank adapter matrices into frozen transformer layers so a large VLM/LLM backbone can be adapted with a small number of trainable parameters (e.g., tens of millions) for robot grounding tasks while keeping base weights frozen.",
            "visual_encoder_type": "applies to transformer-based encoders/decoders (VLMs/LLMs)",
            "visual_encoder_pretraining": "leverages pretrained VLM/LLM backbones (e.g., CLIP, DINOv2, LLaMA) and adapts via LoRA",
            "grounding_mechanism": "Parameter-efficient adaptation preserves pretrained visual-text alignment while allowing task-specific grounding by training only adapter matrices that modulate attention/FFN layers",
            "representation_level": "model-weight adaptation (affects embedding and attention representations)",
            "spatial_representation": null,
            "embodied_task_type": "general VLA adaptation across manipulation, navigation, and embodied tasks",
            "embodied_task_name": "used in multiple VLA models (OpenVLA, Pi-0 Fast, TinyVLA) as adaptation method",
            "visual_domain": "applies across domains (simulation and real robot data)",
            "performance_metric": "compute cost / adaptation speed / retained performance",
            "performance_value": "Reported cut in GPU compute by ~70% for OpenVLA LoRA adaptation (20M adapter params on 7B backbone) with preserved high-level performance (reported)",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": "Enables efficient grounding adaptation with minimal computeâ€”preserves pretrained semantic grounding and reduces catastrophic forgetting",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": "",
            "failure_mode_analysis": "",
            "domain_shift_handling": "LoRA enables practical finetuning on in-domain robot data to reduce domain shift with low compute cost",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "LoRA is a middle ground between frozen and full fine-tuning: adapters are trained while backbone stays frozen; reported to retain &gt;95% of full-finetune performance in many cases",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "model adaptation (does not change fusion mechanism directly but allows efficient co-finetuning of fusion layers)",
            "sample_efficiency": null,
            "key_findings_grounding": "LoRA enables cost-effective adaptation of large VLM/LLM backbones for embodied grounding, preserving pretrained semantic alignment while dramatically reducing compute and trainable parameter count.",
            "uuid": "e1982.12"
        },
        {
            "name_short": "PerceptionBottlenecks",
            "name_full": "Perception and Grounding Bottlenecks Identified in VLAs",
            "brief_description": "Survey-level synthesis of major perception/grounding failure modes and bottlenecks limiting VLA performance in embodied tasks (real-time latency, occlusion, limited object generalization, dataset bias, and energy constraints).",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "analysis / diagnostic summary",
            "model_description": "Aggregated analysis from the review identifying common grounding bottlenecks: sequential autoregressive decoding latency (3â€“5 Hz typical vs. required 100+ Hz), high token and memory bandwidth requirements, occlusion/missing references, dataset bias, limited novel-object generalization, and synchronization issues in hierarchical systems.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "N/A (analysis of mechanisms and bottlenecks across models, not a single model)",
            "representation_level": "covers pixel-, region-, object-, and scene-level representation bottlenecks",
            "spatial_representation": "notes missing/existing use of explicit spatial representations (depth, occupancy) as gap area",
            "embodied_task_type": "manipulation, navigation, autonomous driving, humanoid control",
            "embodied_task_name": "multiple benchmarks discussed across review (manipulation, navigation, driving)",
            "visual_domain": "simulation and real-world robotics domains",
            "performance_metric": null,
            "performance_value": "Representative reported bottlenecks: typical autoregressive VLA decoding at ~3â€“5 Hz (insufficient vs 100+ Hz required for precise control); embedded memory demand example: ~1.2 GB/s for 400 vision tokens Ã— 512 dims; emergency-stop latencies 200â€“500 ms reported for some safety checks; OpenVLA reported overlooking ~23% of object references; dataset biases ~17% skew reported in web datasets.",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Key issues: autoregressive decoding latency (3â€“5 Hz vs needed &gt;100 Hz), high token/memory bandwidth (e.g., 400 tokens Ã— 512 dims â‰ˆ 1.2 GB/s), occlusion and partial observability, dataset bias (â‰ˆ17% skew), and limited novel-object generalization (ObjectVLA ~64% success; OpenVLA misses ~23% object refs).",
            "failure_mode_analysis": "Frequent failure modes include occlusion-induced misidentification, viewpoint sensitivity, limited out-of-distribution object handling, synchronization mismatch between planner and low-level control, and safety/latency-related collisions.",
            "domain_shift_handling": "Surveyed solutions include co-finetuning on robot demonstrations, synthetic augmentation (UniSim), domain randomization, and sim-to-real fine-tuning; performance drops noted when not applied (up to ~32% drop in sim-to-real as cited for some models).",
            "novel_object_performance": "Multiple reports: ObjectVLA ~64% generalization; RT-2 reports +63% improvement on novel objects with its approach; OpenVLA still misses ~23% object refs in novel scenes per review.",
            "frozen_vs_finetuned": "Discussion: frozen VLMs (early fusion) can preserve semantic priors and improve compositional generalization; co-finetuning can better align semantics to control but needs parameter-efficient methods (LoRA) to be tractable.",
            "pretraining_scale_effect": "Review notes benefits of web-scale pretraining (CLIP, LAION) combined with robot demo finetuning; OpenVLA shows co-finetuning uses fewer params but outperforms larger models, indicating scale interacts with alignment quality.",
            "fusion_mechanism": "Surveyed fusion mechanisms include cross-attention, early fusion (embedding-level), concatenation, token unification, and object-centric attention; no single dominant solution â€” tradeoffs exist between preserving semantic priors vs. adaptability.",
            "sample_efficiency": "Noted techniques improving sample efficiency: co-finetuning, LoRA, retrieval-augmentation, imitation learning (behavior cloning), and diffusion-based policies for diverse action modeling; reported sample reductions e.g., LoRA reduces GPU compute rather than dataset size.",
            "key_findings_grounding": "Grounding effectiveness hinges on (1) alignment between pretrained semantic priors and robot dynamics (co-finetuning helps), (2) representation choice (object-centric and explicit 3D/occupancy aids spatial grounding), and (3) inference/decoder efficiency (compressed tokens, parallel decoding) to meet real-time control requirements; occlusion, dataset bias and novel-object generalization remain primary failure drivers.",
            "uuid": "e1982.13"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 2
        },
        {
            "paper_title": "Cliport: What and where pathways for robotic manipulation",
            "rating": 2
        },
        {
            "paper_title": "VIMA: General robot manipulation with multimodal prompts",
            "rating": 2
        },
        {
            "paper_title": "OpenVLA: An open-source vision-language-action model",
            "rating": 2
        },
        {
            "paper_title": "ObjectVLA: End-to-end open-world object manipulation without demonstration",
            "rating": 2
        },
        {
            "paper_title": "Early fusion helps vision language action models generalize better",
            "rating": 2
        },
        {
            "paper_title": "Diffusion policy: Visuomotor policy learning via action diffusion",
            "rating": 2
        },
        {
            "paper_title": "Fast: Efficient action tokenization for vision-language-action models",
            "rating": 2
        },
        {
            "paper_title": "Occllama: An occupancy-language-action generative world model for autonomous driving",
            "rating": 1
        },
        {
            "paper_title": "CogACT: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation",
            "rating": 1
        }
    ],
    "cost": 0.040547,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges
7 May 2025</p>
<p>Ranjan Sapkota 
Cornell University, Biological &amp; Environmental Engineering
IthacaNew YorkUSA</p>
<p>Yang Cao 
Department of Computer Science and Engineering
Department of Informatics and Telecommunications
The Hong Kong University of Science and Technology
Hong Kong c University of the Peloponnese
Greece</p>
<p>Konstantinos I Roumeliotis 
Manoj Karkee 
Cornell University, Biological &amp; Environmental Engineering
IthacaNew YorkUSA</p>
<p>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges
7 May 202568F7A2F0A1D6055674A12EB96B06E5FEarXiv:2505.04769v1[cs.CV]Preprint submitted to Proceedings of the IEEE May 9, 2025Vision-Language-ActionVLAArtificial IntelligenceRoboticsVision-Language ModelsAI AgentsAgentic AI Contents
Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework.This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field.We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers.Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years.Key progress areas include architectural innovations, parameterefficient training strategies, and real-time inference accelerations.We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation.The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks.Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning.In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents.This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence.</p>
<p>Control Behavior</p>
<p>Integrate vision, language understanding, and motor actions, enabling robots to perceive, reason, and act coherently Vision-Language-Action Models</p>
<p>Understand Text
Understand Image
Figure 1: Evolution from Isolated Modalities to Unified Vision-Language-Action Models.This figure illustrates the transition from separate vision, language, and action systems-each limited to its own domain-to integrated VLA models.VLA models enable robots to jointly perceive, understand language, and act, overcoming the fragmentation of earlier approaches and marking a major step toward adaptive, generalizable, and intelligent embodied agents.</p>
<p>Introduction</p>
<p>Before Vision-Language-Action (VLA) models were developed, progress in robotics and artificial intelligence happened mostly in separate areas: vision systems that could see and recognize images [44,69], language systems that could understand and generate text [164,137], and action systems that could control movement [49].These systems worked well on their own, but they struggled to work together or handle new and unpredictable situations [46,21].As a result, 'erstand complex environments or respond flexibly to real-world challenges.</p>
<p>As illustrated in Figure 1, traditional computer vision models primarily based on convolutional neural networks (CNNs) , were tailored for narrowly specified tasks such as object detection or classification, requiring extensive labeled datasets and cumbersome retraining for even slight shifts in environment or objectives [156,62].These vision models could "see" (e.g., identifying apples in an orchard, as shown in Figure 1) but lacked any understanding of language or the ability to convert visual insights into purposeful actions.Language models, particularly large language models (LLMs), revolutionized text-based understanding and generation [23]; however, they remained restricted to processing language without the capability to perceive or reason about the physical world [76] ("Ripe apples in orchard" in Figure 1 exemplifies this limitation).Meanwhile, action-based systems in robotics, relying heavily on hand-crafted policies or reinforcement learning [122], enabled specific behaviors like object manipulation but demanded painstaking engineering and failed to generalize beyond narrowly scripted scenarios [119].</p>
<p>Despite progress with VLMs, which achieved impressive multimodal understanding by combining vision and language [149,25,148], there remained a conspicuous integration gap: the inability to generate or execute coherent actions based on multimodal input [121,107].As further visualized in Figure 1 , most AI systems specialized at most in two modalities-visionlanguage, vision-action, or language-action-but struggled to fully integrate all three into a unified, end-to-end framework.Consequently, robots could recognize objects visually ("apple"), understand a corresponding textual instruction ("pick the apple"), or perform a predefined motor action (grasping), yet orchestrating these abilities into fluid, adaptable behavior was beyond reach.The result was a fragmented pipeline architecture that could not flexibly adapt to new tasks or environments, leading to brittle generalization and labor-intensive engineering efforts.This highlighted a critical bottleneck in embodied AI: without systems that could jointly perceive, understand, and act, intelligent autonomous behavior remained an elusive goal.</p>
<p>The pressing need to bridge these gaps catalyzed the emergence of VLA models.VLA models, conceptualized around 2021-2022, and pioneered by efforts such as Google Deep-Mind's Robotic Transformer 2 (RT-2) [224], introduced a transformative architecture that unified perception, reasoning, and control within a single framework.As a solution to the limitations outlined in Figure 1, VLAs integrate vision inputs, language comprehension, and motor control capabilities, enabling embodied agents to perceive their surroundings, under-stand complex instructions, and execute appropriate actions dynamically.Early VLA approaches achieved this integration by extending vision-language models to include action tokens-numerical or symbolic representations of robot motor commands, thereby allowing the model to learn from paired vision, language, and trajectory data [121].This methodological innovation dramatically improved robots' ability to generalize to unseen objects, interpret novel language commands, and perform multi-step reasoning in unstructured environments [83].</p>
<p>VLA models represent a transformative step in the pursuit of unified multimodal intelligence, overcoming the long-standing limitations of treating vision, language, and action as separate domains [121].By leveraging internet-scale datasets that integrate visual, linguistic, and behavioral information, VLAs empower robots to not only recognize and describe their environments but also to reason contextually and execute appropriate actions in complex, dynamic settings [196].The progression illustrated in Figure 1 from isolated vision, language, and action systems to an integrated VLA paradigm-captures a fundamental shift toward the development of truly adaptive and generalizable embodied agents.Given the profound implications of this innovation, it is crucial to undertake a thorough and systematic review that draws from a comprehensive body of literature and critical analysis.First, such a review is necessary to clarify the foundational concepts and architectural principles that distinguish VLAs from their predecessors.Second, it provides a structured account of the rapid progress and key milestones in the field, enabling researchers and practitioners to appreciate the trajectory of technological advancements.Third, an in-depth review is essential for mapping the diverse range of real-world applications-from household robotics to industrial automation and assistive technologies-where VLAs are already demonstrating transformative potential.Furthermore, by critically examining the current challenges, such as data efficiency, safety, generalization, and ethical considerations, the review identifies barriers that must be addressed for widespread deployment.And Fifth, synthesizing these insights helps to inform the broader AI and robotics communities about emerging research directions and practical considerations, fostering collaboration and innovation.</p>
<p>In this review, we systematically analyze the foundational principles, developmental progress, and technical challenges associated with VLA models.Our objective is to consolidate the current understanding of VLAs while identifying limitations and proposing future directions for their evolution.The review begins with a detailed examination of key conceptual foundations (Figure 2), including what constitutes a VLA model, its historical evolution, multimodal integration mechanisms, and language-based tokenization and encoding strategies.These conceptual components set the stage for understanding how VLAs are structured and function across modalities.</p>
<p>Building upon this, we present a unified view of recent progress and training efficiency strategies (Figure 3).This includes architectural innovations that have enabled more capable and generalizable VLA models, as well as data-efficient learning frameworks, parameter-efficient modeling techniques, and  model acceleration strategies designed to reduce computational overhead without compromising performance.These advancements are critical for scaling VLA systems to real-world applications.</p>
<p>Following this, we delve into a comprehensive discussion of the current limitations faced by VLA systems (Figure 4).These include inference bottlenecks, safety concerns, high computational demands, limited generalization, and ethical implications.We not only highlight these pressing challenges but also provide an analytical discussion of potential solutions to address them.</p>
<p>Together, these three figures offer a visual framework that supports the textual analysis of this review.By outlining the conceptual landscape, recent innovations, and open challenges, this work aims to guide future research and encourage the development of more robust, efficient, and ethically grounded Figure 4: Mindmap -VLA Challenges.This diagram highlights key barriers to robust VLA deployment, including inference limitations, bias, system complexity, generalization gaps, and ethical concerns.It also motivates the need for innovative solutions and future research directions to overcome these challenges.models typically employ multimodal fusion techniques-such as cross-attention, concatenated embeddings, or token unification-to align sensory observations with textual instructions.</p>
<p>Unlike traditional visuomotor pipelines, VLAs support semantic grounding, enabling context-aware reasoning, affordance detection, and temporal planning.A typical VLA model observes the environment through camera or sensor data, interprets goals expressed in language (e.g., "pick up the red apple") (Figure 5), and outputs low-level or high-level action sequences.Recent advancements integrate imitation learning, reinforcement learning, or retrieval-augmented modules to improve sample efficiency and generalization.This review examines how VLA models have evolved from foundational fusion architectures to general-purpose agents capable of real-world deployment across robotics, navigation, and human-AI collaboration.</p>
<p>VLA models are multimodal artificial intelligence systems that unify visual perception, language comprehension, and physical action generation into a single framework.These models enable robots or AI agents to interpret sensory inputs (e.g., images, text), understand contextual meaning, and autonomously execute tasks in real-world environments-all through end-to-end learning rather than isolated subsystems.As shown conceptually in Figure 5, VLA models bridge the historical disconnect between visual recognition, language comprehension, and motor execution that limited the capabilities of earlier robotic and AI systems.</p>
<p>Evolution and Timeline</p>
<p>The rapid development of VLA models from 2022-2025 demonstrates three distinct evolutionary phases:</p>
<ol>
<li>Foundational Integration (2022-2023).Early VLAs established basic visuomotor coordination through multimodal fusion architectures.[157] first combined CLIP embeddings with motion primitives, while [141] demonstrated generalist capabilities across 604 tasks.[18] achieved 97% success rates in manipulation through scaled imitation learning, and [86] introduced temporal reasoning via transformer-based planners.By 2023, [224] enabled visual chain-of-thought reasoning, and [34] advanced stochastic action prediction through diffusion processes.These foundations addressed low-level control but lacked compositional reasoning [216], prompting innovations in affordance grounding [78].</li>
</ol>
<p>Specialization and Embodied Reasoning (2024).</p>
<p>Second-generation VLAs incorporated domain-specific inductive biases.[202] enhanced few-shot adaptation through retrieval-augmented training, while [210] optimized navigation via 3D scene-graph integration.[39] introduced reversible architectures for memory efficiency, and [183] addressed partial observability with physics-informed attention.Simultaneously, [5] improved compositional understanding through object-centric disentanglement, and [220] extended applications to autonomous driving via multi-modal sensor fusion.These advances required new benchmarking methodologies [196].</p>
<p>Generalization and Safety-Critical Deployment (2025).</p>
<p>Current systems prioritize robustness and human alignment.[205] integrated formal verification for risk-aware decisions, while [42] demonstrated whole-body control through hierarchical VLAs.[19] optimized compute efficiency for embedded deployment, and [102] combined neural-symbolic reasoning for causal inference.Emerging paradigms like [100]'s affordance chaining and [13]'s simto-real transfer learning address cross-embodiment challenges, while [108] bridges VLAs with human-in-the-loop interfaces through natural language grounding.</p>
<p>Figure 6 presents a comprehensive timeline highlighting the evolution of 47 VLA models developed between 2022 and 2025.The earliest VLA systems, including CLIPort [157], Gato [141], RT-1 [18], and VIMA [86], laid the foundation by combining pretrained vision-language representations with task-conditioned policies for manipulation and control.These were followed by ACT [216], RT-2 [224], and Vox-Poser [78], which integrated visual chain-of-thought reasoning and affordance grounding.Models like Diffusion Policy [34] and Octo [167] introduced stochastic modeling and scalable data pipelines.In 2024, systems such as Deer-VLA [202], ReVLA [39], and Uni-NaVid [210] added domain specialization and memory-efficient designs, while Occllama [183] and ShowUI [108] tackled partial observability and user interaction.The trajectory continued with robotics-focused VLAs like Quar-VLA [43] and RoboMamba [111].Recent innovations emphasize generalization and deployment: SafeVLA [205], Humanoid-VLA [42], and MoManipVLA [190] incorporate verification, full-body control, and memory systems.Models such as Gr00t N1 [13] and SpatialVLA [136] further bridge sim-to-real transfer and spatial grounding.This timeline illustrates how VLAs have advanced from modular learning to general-purpose, safe, and embodied intelligence.</p>
<p>Multimodal Integration: From Isolated Pipelines to Unified Agents</p>
<p>A central advancement in the emergence of VLA models lies in their ability to perform multimodal integration, the joint processing of vision, language, and action within a unified architecture.Traditional robotic systems treated perception, natural language understanding, and control as discrete modules, often linked through manually defined interfaces or data transformations [109,20,168].For instance, classic pipelinebased frameworks required a perception model to output symbolic labels, which were then mapped by a planner to specific actions-frequently with domain-specific hand engineering [138,90].These approaches lacked adaptability, failed in ambiguous or novel environments, and could not generalize instructions beyond pre-encoded templates.</p>
<p>In contrast, modern VLAs fuse modalities end-to-end using large-scale pretrained encoders and transformer-based architectures [188].This shift enables the model to interpret visual observations and linguistic instructions within the same computational space, allowing flexible, context-aware reasoning [99].For example, in the task "Pick up the red ripe apple," (Figure 5) the vision encoder-typically a Vision Transformer (ViT) or ConvNeXt-segments and classifies objects in the scene (e.g., apples, leaves, background), identifying color and ripeness attributes [187].Meanwhile, the language model, often a variant of T5, GPT, or BERT, encodes the instruction into a highdimensional embedding.These representations are then fused via cross-attention or joint tokenization schemes, producing a unified latent space that informs the action policy [68].</p>
<p>This multimodal synergy was first effectively demonstrated in CLIPort [157], which used CLIP embeddings for semantic grounding and a convolutional decoder for pixel-level manipulation.CLIPort bypassed the need for explicit language parsing and directly conditioned visuomotor policies on natural language.Similarly, VIMA [86] advanced this approach by employing a transformer encoder to jointly process object-centric visual tokens and instruction tokens, enabling few-shot generalization across spatial reasoning tasks.</p>
<p>Recent developments push this fusion further by incorporating temporal and spatial grounding.VoxPoser [78] employs voxel-level reasoning to resolve ambiguities in 3D object selection, while RT-2 [224] fuses visual-language tokens into a unified transformer that supports zero-shot generalization to unseen instructions.Another noteworthy contribution is Octo [167], which introduces a memory-augmented transformer that enables long-horizon decision-making across diverse scenes, demonstrating the scalability of joint perceptionlanguage-action learning.</p>
<p>Crucially, VLAs offer robust solutions to challenges in realworld grounding.For example, Occllama [183] handles occluded object references through attention-based mechanisms, while ShowUI [108] demonstrates natural language interfaces that allow non-expert users to command agents through voice or typed input.These capabilities are only possible because the integration is not limited to surface-level fusion; rather, it captures semantic, spatial, and temporal alignment across modalities.</p>
<p>Tokenization and Representation: How VLAs Encode the World</p>
<p>A core innovation that sets VLA models apart from conventional vision-language architectures lies in their token-based representation framework, which enables holistic reasoning over perceptual [125,215], linguistic, and physical action spaces [106].Inspired by autoregressive generative models like transformers, modern VLAs encode the world using discrete tokens that unify all modalities-vision, language, state, and action into a shared embedding space [110].This allows the model to not only understand "what needs to be done" (semantic reasoning), but also "how to do it" (control policy execution) in a fully learnable and compositional way [192,117,170].</p>
<p>â€¢ Prefix Tokens: Encoding Context and Instruction:Prefix tokens serve as the contextual backbone of VLA models [195,83].These tokens encode the environmental scene (via images or video) and the accompanying natural language instruction into compact embeddings that prime the model's internal representations [16].</p>
<p>For instance, as depicted in Figure 7in a task such as "stack the green blocks on the red tray," the image of a cluttered tabletop is processed through a vision encoder like ViT or ConvNeXt, while the instruction is embedded by a large language model (e.g., T5 or LLaMA).These are then 2022 2023 CLIPort [157] Gato [141] RT-1 [18] VIMA [86] ACT [216] RT-2 [224] VoxPoser [78] Diffusion Policy [34] Octo [167] OpenVLA [94] 2024 2024</p>
<p>Deer-VLA [202] Uni-NaVid [210] ReVLA [39] Occllama [183] Pi-0 [14] RDT-1B [112] CogAct [102] EdgeVLA [19] ShowUI [108] NaviLa [32] Quar-VLA [43] Bi-VLA [59] RoboMamba [111] Otter [75] PointVLA [96] CombatVLA [29] HybridVLA [110] 2025 2025</p>
<p>CoVLA [5] OpenDriveVLA [220] ORION [56] ObjectVLA [223] ConRFT [31] Hi Robot [155] TLA [70] RaceVLA [153] DexVLA [185] Humanoid-VLA [42] SafeVLA [205] MoManipVLA [190] VLA-Cache [195] TinyVLA [186] Gr00t N1 [13] NORA [79] SpatialVLA [136] MoLe-VLA [213] Figure 6: Comprehensive timeline of Vision-Language-Action models (2022-2025), showing evolution from foundation to 45 specialized VLA systems.Organized chronologically with thematic grouping.</p>
<p>transformed into a sequence of prefix tokens that establish the model's initial understanding of the goal and environmental layout.This shared representation enables crossmodal grounding, allowing the system to resolve spatial references (e.g., "on the left," "next to the blue cup") and object semantics ("green blocks") across both modalities.</p>
<p>â€¢ State Tokens: Embedding the Robot's Configuration:</p>
<p>In addition to perceiving external stimuli, VLAs must be aware of their internal physical state [186,111].This is achieved through the use of state tokens, which encode real-time information about the agent's configuration-joint positions, force-torque readings, gripper status, end-effector pose, and even the locations of nearby objects [97].These tokens are crucial for ensuring situational awareness and safety, especially during manipulation or locomotion [163,81].</p>
<p>Figure 8 illustrates how VLA models utilize state tokens to enable dynamic, context-aware decision-making in both manipulation and navigation settings.In Figure 8a, a robot arm is shown partially extended near a fragile object.In such scenarios, state tokens play a critical role by encoding real-time proprioceptive information, such as joint angles, gripper pose, and end-effector proximity.These tokens are continuously fused with visual and languagebased prefix tokens, allowing the transformer to reason about physical constraints.The model can thus infer that a collision is imminent and adjust the motor commands accordingly-e.g., rerouting the arm trajectory or modulating force output.In mobile robotic platforms, as depicted in Figure 8b, state tokens encapsulate spatial features such as odometry, LiDAR scans, and inertial sen-sor data.These are essential for terrain-aware locomotion and obstacle avoidance.The transformer model integrates this state representation with environmental and instructional context to generate navigation actions that dynamically adapt to changing surroundings.Whether grasping objects in cluttered environments or autonomously navigating uneven terrain, state tokens provide a structured mechanism for situational awareness, enabling the autoregressive decoder to produce precise, context-informed action sequences that reflect both internal robot configuration and external sensory data.</p>
<p>â€¢ Action Tokens: Autoregressive Control Generation:</p>
<p>The final layer of the VLA token pipeline involves action tokens [93,94], which are autoregressively generated by the model to represent the next step in motor control [186].Each token corresponds to a low-level control signal, such as joint angle updates, torque values, wheel velocities, or high-level movement primitives [64].During inference, the model decodes these tokens one step at a time, conditioned on prefix and state tokens, effectively turning VLA models into language-driven policy generators [54,161].This formulation allows seamless integration with real-world actuation systems, supports variablelength action sequences [10,77], and enables model finetuning via reinforcement or imitation learning frameworks [214].Notably, models like RT-2 [224] and PaLM-E [47] exemplify this design, where perception, instruction, and embodiment are merged into a unified token stream.</p>
<p>For instance, in the apple-picking task as depicted in Figure 9, the model may receive prefix tokens that include the image of the orchard and the text instruction.The state tokens describe the robot's current arm posture and whether the gripper is open or closed.Action tokens are then predicted step by step to guide the robotic arm toward the apple, adjust the gripper orientation, and execute a grasp with appropriate force.The beauty of this approach is that it allows transformers, which are traditionally used for text generation, to now generate sequences of physical actions in a manner similar to generating a sentence-only here, the sentence is the motion.</p>
<p>To operationalize the VLA paradigm in robotics, we present in Figure 9 a structured pipeline that demonstrates how multimodal information-specifically vision, language, and proprioceptive state-is encoded, fused, and converted into executable action sequences.This end-to-end loop allows a robot to interpret complex tasks like "pick the ripe apple near the green leaf" and execute precise, context-sensitive manipulations.The system begins with multimodal input acquisition, where three distinct data streams are collected: visual observations (e.g., RGB-D frames), natural language commands, and real-time robot state information (e.g., joint angles or velocity).These are independently tokenized into discrete embeddings using pretrained modules [41,212].As depicted in the diagram, the image is processed through a Vision Transformer (ViT) backbone to generate vision tokens, the instruction is parsed by a Figure 9: Illustrating the process of how VLAs Encode the World.VLAs encode the world by converting vision, language, and sensor inputs into tokens, fusing them through cross-attention, predicting action sequences via transformers, and executing tasks with real-time feedback-enabling robots to interpret scenes, follow instructions, and adapt actions dynamically.language model such as BERT or T5 to produce language tokens, and state inputs are transformed via a lightweight MLP encoder into compact state tokens.</p>
<p>These tokens are then fused using a cross-modal attention mechanism, where the model jointly reasons over object semantics, spatial layout, and physical constraints [61].This fused representation forms the contextual basis for decision-making [74,116].In Figure 9, this is denoted as the multimodal fusion step.The fused embedding is passed into an autoregressive decoder-typically a transformer-that generates a series of action tokens.These tokens may correspond to joint displacements, gripper force modulation, or high-level motor primitives (e.g., "move to grasp pose", "rotate wrist").The action tokens are subsequently translated into control commands and passed to the execution loop, which closes the perception-action cycle by feeding back the robot's updated state, thus informing the next inference step.This closed-loop mechanism enables the model to dynamically adapt to perturbations, object shifts, or occlusions in real time [206,120,194].</p>
<p>To offer concrete implementation details, Algorithm 1 formalizes the VLA tokenization process.Given an RGB-D frame I, natural language instruction T , and joint angle vector Î¸, the algorithm produces a set of action tokens that can be executed in sequence.The image I is processed via a ViT to produce V, a set of 400 visual tokens.In parallel, the instruction T is encoded by a BERT model to yield L, a sequence of 12 semantic language tokens.Simultaneously, robot state Î¸ is passed through a multilayer perceptron to generate a 64-dimensional state embedding S .These tokens are then fused via a cross-attention module to produce a shared 512-dimensional representation F, capturing the semantics, intent, and situational awareness needed for grounded action.Finally, a policy decoder such as FAST [133] maps the fused features into 50 discrete action tokens, which can then be decoded into motor commands Ï„ 1:N .</p>
<p>The decoding process is implemented using a transformerbased architecture, as shown in the code snippet titled Action Prediction Code.A 'Transformer' object is initialized with 12 layers, a model dimension of 512, and 8 attention heads.The fused tokens are passed to the decoder, which autoregressively predicts the next most likely action token conditioned on previous tokens and context.The final motor command sequence is obtained by detokenizing the output.This implementation mirrors how text generation works in large language models, but here the "sentence" is a motion trajectory-a novel repurposing of natural language generation techniques for physical action synthesis.</p>
<p>Together, Figure 9, Algorithm 1, and the pseudocode illustrate how VLAs unify perception, instruction, and embodiment within a coherent and interpretable token space.This modularity allows the framework to generalize across tasks and robot morphologies, facilitating rapid deployment in real-world applications like apple picking, household tasks, and mobile navigation.Importantly, the clarity and separability of the tokenization steps make the architecture extensible, enabling further research on token learning, hierarchical planning, or symbolic grounding in VLA systems.gies Training VLA models requires a hybrid learning paradigm that integrates both semantic knowledge from the web and taskgrounded information from robotics datasets [30].As shown in prior sections, the multimodal architecture of VLAs must be exposed to diverse forms of data that support language understanding, visual recognition, and motor control.This is typically achieved through two primary data sources.</p>
<p>First, as depicted in figure 10, large-scale internet-derived corpora form the backbone of the model's semantic prior.These datasets include image-caption pairs (e.g., COCO, LAION-400M), instruction-following datasets (e.g., HowTo100M, We-bVid), and visual question-answering corpora (e.g., VQA, GQA).Such datasets enable pretraining of the visual and language encoders, helping the model acquire general representations of objects, actions, and concepts [2].This phase often uses contrastive or masked modeling objectives, such as CLIP-style contrastive learning or language modeling losses, to align vision and language modalities within a shared embedding space [146,199].Importantly, this stage gives VLAs a foundational "understanding of the world" that facilitates compositional generalization, object grounding, and zero-shot transfer [28,15].However, semantic understanding alone is insufficient for physical task execution [36,178,107].Thus, the second phase focuses on grounding the model in embodied experience [178].Robot trajectory datasets-collected either from realworld robots or high-fidelity simulators-are used to teach the model how language and perception translate into action [54].These include datasets like RoboNet [37], BridgeData [50], and RT-X [175], which provide video-action pairs, joint trajectories, and environment interactions under natural language instructions [123].Demonstration data may come from kinesthetic teaching, teleoperation, or scripted policies [89,12].This phase typically employs supervised learning (e.g., behavior cloning) [55], reinforcement learning (RL), or imitation learning to train the autoregressive policy decoder to predict action tokens based on fused visual-language-state embeddings [65].</p>
<p>Recent works increasingly adopt multistage or multitask training strategies.For example, models are often pretrained on vision-language datasets using masked language modeling, then fine-tuned on robot demonstration data using token-level autoregressive loss [94,221,195].Others use curriculum learning, where simpler tasks (e.g., object pushing) precede more complex ones (e.g., multistep manipulation) [217].Some approaches further leverage domain adaptation such as in Open-VLA [94] or sim-to-real transfer to bridge the gap between synthetic and real-world distributions [96].By unifying semantic priors with task execution data, these learning paradigms allow VLA models to generalize across tasks, domains, and embodiments-forming the backbone of scalable, instructionfollowing agents capable of robust real-world operation.</p>
<p>Through co-fine-tuning, these datasets are brought into alignment [179,52].The model learns to map from visual and linguistic inputs to appropriate action sequences [136].This training paradigm not only helps the model understand object affordances (e.g., apples can be grasped) and action outcomes (e.g., lifting requires force and trajectory), but also promotes generalization to novel scenarios [100].A model trained on kitchen manipulation tasks may be able to infer how to pick an apple in an outdoor orchard if it has learned general principles of object localization, grasping, and following language directives.</p>
<p>Recent architectures, such as Google DeepMind's RT-2 (Robotic Transformer 2) [224], have demonstrated this principle in action.RT-2 treats action generation as a form of text generation, where each action token corresponds to a discrete command in a robot's control space.Because the model is trained on both web-scale multimodal data and thousands of robot demonstrations, it can flexibly interpret novel instructions and perform zero-shot generalization to new objects and tasks-something that was largely impossible with traditional control systems or even early multimodal models.</p>
<p>Adaptive Control and Real-Time Execution</p>
<p>Another strength of VLAs lies in their ability to perform adaptive control, using real-time feedback from sensors to adjust behavior on the fly [153].This is particularly important in dynamic, unstructured environments like orchards, homes, or hospitals, where unexpected changes (e.g., wind moving an apple, lighting changes, human presence) can alter the task parameters.During execution, state tokens are updated in real time, reflecting sensor inputs and joint feedback [195].The model can then revise its planned actions accordingly.For instance, in the apple-picking scenario, if the target apple shifts slightly or another apple enters the field of view, the model dynamically reinterprets the scene and adjusts the grasp trajectory.This capability mimics human-like adaptability and is a core advantage of VLA systems over pipeline-based robotics.</p>
<p>Progress in Vision-Language-Action Models</p>
<p>The inception of VLA models was catalyzed by the remarkable success of transformer-based LLMs, notably ChatGPT, released in November 2022, which demonstrated unprecedented semantic reasoning capabilities (ChatGPT) [139].This breakthrough inspired researchers to extend language models to multimodal domains, integrating perception and action for robotics.By 2023, GPT-4 introduced multimodal capabilities, processing both text and images, which spurred efforts to incorporate physical actions (GPT-4) [1].Concurrently, VLMs like CLIP (2022) [157] and Flamingo (2022) [3] had established robust visualtext alignment through contrastive learning, enabling zero-shot object recognition and laying the groundwork for VLA models (CLIP).These models leveraged large-scale web datasets to align images with textual descriptions, a critical precursor to integrating actions.</p>
<p>A pivotal development was the creation of large-scale robotic datasets, such as RT-1's 130,000 demonstrations, which provided action-grounding data essential for co-training vision, language, and action components [18].These datasets captured diverse tasks and environments, enabling models to learn generalizable behaviors.Architectural breakthroughs followed with Google's RT-2 in 2023 [17], a landmark VLA model that unified vision, language, and action tokens, treating robotic control as an autoregressive sequence prediction task (RT-2 Blog).RT-2 discretized actions using Discrete Cosine Transform (DCT) compression and Byte-Pair Encoding (BPE), achieving a 63% improvement in performance on novel objects.Multimodal fusion techniques, such as cross-attention transformers, integrated Vision Transformer (ViT)-processed images (e.g., 400 patch tokens) with language embeddings, enabling robots to execute complex commands like "Pick the red cup left of the bowl."Additionally, UC Berkeley's Octo model (2023) introduced an open-source approach with 93M parameters and diffusion decoders, trained on 800,000 robot demonstrations from the OpenX-Embodiment Dataset, further broadening the research landscape [167].</p>
<p>Architectural Innovations in VLA Models</p>
<p>From 2023 to 2024, VLA models underwent significant architectural advancements and refined training methodologies.Dual-system architectures emerged as a key innovation, exemplified by NVIDIA's Groot N1 (2025) [13], which combined System 1 (fast diffusion policies with 10ms latency for lowlevel control) and System 2 (LLM-based planners for highlevel task decomposition).This separation enabled efficient coordination between strategic planning and real-time execution, enhancing adaptability in dynamic environments.Other models, like Stanford's OpenVLA (2024) [94], introduced a 7B-parameter open-source VLA trained on 970k real-world robot demonstrations, using dual vision encoders (DINOv2 [128] and SigLIP [204]) and a Llama 2 language model [172], outperforming larger models like RT-2-X (55B) [94].Training paradigms evolved to leverage co-fine-tuning on web-scale vision-language data (e.g., LAION-5B) [152] and robotic trajectory data (e.g., RT-X) [175], aligning semantic knowledge with physical constraints [152].Synthetic data generation tools like UniSim addressed data scarcity by creating photorealistic scenarios, such as occluded objects, crucial for robust training (UniSim [200]).Parameter efficiency was enhanced through Low-Rank Adaptation (LoRA) adapters [72], which allowed domain adaptation without full retraining, reducing GPU hours by 70%.The introduction of diffusion-based policies, as seen in Physical Intelligence's pi 0 model (2024) [14], offered improved action diversity but required significant computational resources.These advancements democratized VLA technology, fostering collaboration and accelerating innovation.</p>
<p>Recent VLA models have converged toward three major architectural paradigms that balance efficiency, modularity, and robustness: early fusion models, dual-system architectures, and self-correcting frameworks.Each of these innovations addresses specific challenges in grounding, generalization, and action reliability in real-world robotic systems.</p>
<ol>
<li>
<p>Early Fusion Models: One class of approaches focuses on fusing vision and language representations at the input stage before passing them to the policy module.Huang et al.'s EF-VLA model [74], presented at ICLR 2025, exemplifies this trend by retaining the representational alignment established by CLIP [157].EF-VLA accepts image-text pairs, encodes them with CLIP's frozen encoders, and fuses the resulting embeddings early in the transformer backbone-prior to action prediction.This design ensures that the semantic consistency learned during CLIP pretraining is preserved, reducing overfitting and enhancing generalization.Notably, EF-VLA demonstrated a 20% performance improvement on compositional manipulation tasks and reached 85% success on previously unseen goal descriptions.By avoiding fine-tuning of the vision-language modules, this approach also preserves computational efficiency and prevents catastrophic forgetting during domain-specific training.</p>
</li>
<li>
<p>Dual-System Architectures: Inspired by dual-process theories of human cognition, models like NVIDIA's Groot N1 (2025) [13] implement two complementary subsystems: a fastreactive module (System 1) and a slow-reasoning planner (System 2).System 1 comprises a diffusion-based control policy that operates at 10 ms latency, ideal for fine-grained, low-level control such as end-effector stabilization or adaptive grasping.In contrast, System 2 uses a LLM for task planning, skill composition, and high-level sequencing.The planner parses longhorizon goals (e.g., "clean the table") into atomic subtasks, while the low-level controller ensures real-time execution.This decomposition enables multi-timescale reasoning and improved safety, especially in environments where rapid reaction and deliberation must co-exist.In benchmark tests on multi-stage household manipulation, Groot N1 outperformed monolithic models by 17% in success rate and reduced collision failures by 28 3. Self-Correcting Frameworks: A third architectural evolution is the development of self-correcting VLA models, designed to detect and recover from failure conditions without external supervision.SC-VLA (2024) introduces a hybrid execution loop featuring a fast inference path and a slow correction path.In this framework, the default behavior is to predict poses or actions directly from the fused embedding using a lightweight transformer.When failures are detected-e.g., unsuccessful grasps or obstacle collisions-the model invokes a secondary process that performs chain-of-thought reasoning [211,203].This path queries an internal LLM (or external expert system) to diagnose failure modes and generate correction strategies [48].For example, if the robot repeatedly misidentifies an occluded object, the LLM may suggest an active viewpoint change or gripper reorientation.In closed-loop experiments, SC-VLA reduced task failure rates by 35% and significantly improved recoverability in cluttered and adversarial environments.</p>
</li>
</ol>
<p>VLA models exhibit a rich diversity of architectural designs and functional emphases, which can be systematically organized along the dimensions of end-to-end versus modular pipelines, hierarchical versus flat policy structures, and the balance between low-level control and high-level planning (Table 1).End-to-end VLAs, such as CLIPort [157], RT-1 [18], and OpenVLA [94], process raw sensory inputs directly into motor commands via a single unified network.By contrast, component-focused models like VLATest [182] and Chain-of-Affordance [100] decouple perception, language grounding, and action modules, enabling targeted improvements in individual submodules.</p>
<p>Hierarchical architectures have emerged to tackle complex, long-horizon tasks by separating strategic decision making from reactive control.For instance, CogACT [102] and NaV-ILA [32] employ a two-tier hierarchy where an LLM-based planner issues subgoals to a low-level controller, thereby combining the strengths of System 2 reasoning and System 1 execution.Similarly, ORION [56] integrates a QT-Former for longterm context aggregation with a generative trajectory planner in a cohesive framework.</p>
<p>Low-level policy emphasis is typified by diffusion-based controllers (e.g.Pi-0 [14], DexGraspVLA [219]), which excel at producing smooth, diverse motion distributions but often incur higher computational cost.In contrast, high-level planners (e.g.FAST Pi-0 Fast [133], CoVLA [5]) focus on rapid subgoal generation or coarse trajectory prediction, delegating fine-grained control to specialized modules or traditional motion planners.End-to-end dual-system models like HybridVLA [110] and Helix [166] blur these distinctions by jointly training both components while preserving modular interpretability.</p>
<p>Table 1 further highlights how recent VLAs balance these trade-offs.Models such as OpenDriveVLA [220] and Combat-VLA [29] prioritize hierarchical planning in dynamic, safetycritical domains, whereas lightweight, edge-targeted systems like Edge VLA [19] and TinyVLA [186] emphasize real-time low-level policies at the expense of high-level reasoning.This classification framework not only clarifies the design space of VLAs but also guides future development by pinpointing underexplored combinations-such as fully end-to-end, hierarchical models optimized for embedded deployment-that promise to advance both the capabilities and the applicability of VLA systems across robotics, autonomous driving, and beyond.</p>
<p>The classification in Table 1 is significant because it provides a clear framework for comparing diverse VLA architectures, highlighting how design choices-such as end-to-end integration versus hierarchical decomposition-impact task performance, scalability, and adaptability.By categorizing models along dimensions like low-level policy execution and highlevel planning, researchers can pinpoint strengths and limitations of existing approaches and identify opportunities for innovation.This taxonomy aids in selecting appropriate architectures for specific applications (e.g., real-time control vs. strategic reasoning) and guides future development toward hybrid systems that balance responsiveness with cognitive planning, ultimately accelerating progress in embodied AI.Additionally, to synthesize recent advancements in VLA models, Table 2 presents a comparative summary of notable systems developed from 2022 through 2025.Building upon architectural innovations such as early fusion, dual-system processing, and selfcorrecting feedback loops, these models incorporate diverse design philosophies and training strategies.Each entry highlights the model's key components-vision and language encoders, action decoders-and the datasets used to ground their capabilities.Models like CLIPort [157] and RT-2 [224] laid early foundations by aligning semantic embeddings with action policies, while more recent frameworks like Pi-Zero, CogACT [102], and Groot N1 [13] introduce scalable architectures with diffusion-based or high-frequency controllers.Several models leverage multimodal pretraining with internet-scale visionlanguage corpora and robot trajectory datasets, enhancing generalization and zero-shot capabilities [223,219,218,198].This tabulated comparison serves as a reference point for researchers seeking to understand the functional diversity, domain applicability, and emerging trends in VLA design across real and simulated environments.</p>
<p>Table 1: Taxonomy of VLA models showing structured classification based on architectural paradigms and scientific priorities.We differentiate models by their support for end-to-end execution, hierarchical planning-control decomposition, or component-focused modularity, and further by their emphasis on low-level motor policies versus high-level task planners.</p>
<p>Model Name</p>
<p>Year Endto-End Gato [141] â€¢ Vision Encoder: ViT
Hie rarc hi- cal Comp onent Fo- cused Low- Level Pol- icy High- Level Plan- ner CLIPort [157] âœ“ âœ— âœ— âœ“ âœ— RT-1 [18] âœ“ âœ— âœ— âœ“ âœ— Gato [141] âœ“ âœ— âœ— âœ“ âœ— VIMA [86] âœ“ âœ— âœ— âœ“ âœ— Diffusion Policy [34] âœ“ âœ— âœ— âœ“ âœ— ACT [216] âœ“ âœ— âœ— âœ“ âœ— VoxPoser [78] âœ“ âœ— âœ— âœ“ âœ— Seer [63] âœ“ âœ— âœ— âœ“ âœ— Octo [167] âœ“ âœ— âœ— âœ“ âœ— OpenVLA [94] âœ“ âœ— âœ— âœ“ âœ— CogACT [102] âœ— âœ“ âœ— âœ“ âœ“ VLATest [182] âœ— âœ— âœ“ âœ— âœ— NaVILA [32] âœ— âœ“ âœ— âœ“ âœ“ RoboNurse- VLA [103] âœ“ âœ— âœ— âœ“ âœ— Mobility VLA [35] âœ— âœ“ âœ— âœ“ âœ“ RevLA [39] âœ— âœ— âœ“ âœ— âœ— Uni-NaVid [210] âœ— âœ“ âœ— âœ“ âœ“ RDT-1B [112] âœ“ âœ— âœ— âœ“ âœ— RoboMamba [111] âœ“ âœ— âœ— âœ“ âœ— Chain-of- Affordance [100] âœ— âœ— âœ“ âœ— âœ— Edge VLA [19] âœ— âœ— âœ“ âœ— âœ— ShowUI-2B [108] âœ“ âœ— âœ— âœ“ âœ— Pi-0 [14] âœ“ âœ— âœ— âœ“ âœ— FAST (Pi-0 Fast) [133] âœ— âœ— âœ“ âœ“ âœ— OpenVLA-OFT [93] âœ“ âœ— âœ— âœ“ âœ— CoVLA [5] âœ— âœ“ âœ— âœ“ âœ“ OpenDriveVLA [220] âœ— âœ“ âœ— âœ“ âœ“ ORION [56] âœ— âœ“ âœ— âœ“ âœ“ UAV-VLA [150] âœ— âœ“ âœ— âœ“ âœ“ CombatVLA [29] âœ“ âœ— âœ— âœ“ âœ— HybridVLA [110] âœ— âœ“ âœ— âœ“ âœ“ NORA [79] âœ“ âœ— âœ— âœ“ âœ— SpatialVLA [136] âœ— âœ— âœ“ âœ“ âœ— MoLe-VLA [213] âœ— âœ— âœ“ âœ“ âœ— JARVIS-VLA [101] âœ“ âœ— âœ— âœ“ âœ— UP-VLA [209] âœ“ âœ— âœ— âœ“ âœ— Shake-VLA [92] âœ— âœ— âœ“ âœ“ âœ— DexGraspVLA [219] âœ— âœ“ âœ— âœ“ âœ“ DexVLA [185] âœ— âœ“ âœ— âœ“ âœ“ Humanoid-VLA [42] âœ“ âœ— âœ— âœ“ âœ— ObjectVLA [223] âœ“ âœ— âœ— âœ“ âœ—
â€¢ Language Encoder: Sen-tencePiece
â€¢ Action Decoder: Trans- former Self- collected [SC]
Generalist agent handling Atari, captioning, and robotics through unified tokenization.</p>
<p>VIMA [86] â€¢ Vision Encoder: ViT + Mask R-CNN</p>
<p>â€¢ Language Encoder: T5</p>
<p>â€¢ Action Decoder: Transformer</p>
<p>VIMA-Data [SC]</p>
<p>Multi-modal prompt handling with 6 types of vision-language grounding tasks.</p>
<p>ACT [216] â€¢ Vision Encoder: ResNet-18</p>
<p>â€¢ Language Encoder: -
â€¢ Action Decoder: CVAE- Transformer ALOHA [SC]
Temporal ensembling for smooth bimanual manipulation with 0.1mm precision.</p>
<p>Octo [167] â€¢ Vision Encoder: CNN</p>
<p>â€¢ Language Encoder: T5base</p>
<p>â€¢ Action Decoder: Diffusion Transformer Open X-Embodiment</p>
<p>First policy trained on 4M+ robot trajectories from 22 robot types.</p>
<p>VoxPoser [78] â€¢ Vision Encoder: ViLD + MDETR</p>
<p>â€¢ Language Encoder: GPT-4</p>
<p>â€¢ Action Decoder: MPC Zero-shot LLM+VLM composition for constraintaware motion planning without training.</p>
<p>Diffusion</p>
<p>Policy [34] â€¢ Vision Encoder: ResNet-18 â€¢ Data-Efficient Learning.</p>
<p>-Co-fine-tuning on massive vision-language corpora (e.g.LAION-5B) and robotic trajectory collections (e.g.Open X-Embodiment) aligns semantic understanding with motor skills.OpenVLA (7 B params) achieves a 16.5 % higher success rate than a 55 Bparameter RT-2 variant, demonstrating that co-finetuning yields strong generalization with fewer parameters [152,175,94].</p>
<p>-Synthetic Data Generation via UniSim produces photorealistic scenes-including occlusions and dynamic lighting-to augment rare edge-case scenarios, improving model robustness in cluttered environments by over 20 % [200,167].</p>
<p>-Self-Supervised Pretraining adopts contrastive objectives (Ã  la CLIP) to learn joint visual-text embeddings before action fine-tuning, reducing reliance on task-specific labels.Qwen2-VL leverages self-supervised alignment to accelerate downstream grasp-and-place convergence by 12 % [137,76].</p>
<p>â€¢ Parameter-Efficient Adaptation.Low-Rank Adaptation (LoRA) inserts lightweight adapter matrices into frozen transformer layers, cutting trainable weights by up to 70 % while retaining performance [72].The Pi-0 Fast variant uses merely 10 M adapter parameters atop a static backbone to deliver continuous 200 Hz control with negligible accuracy loss [133].</p>
<p>â€¢ Inference Acceleration.</p>
<p>-Compressed Action Tokens (FAST) and Parallel Decoding in dual-system frameworks (e.g.Groot N1) yield 2.5Ã— faster policy steps, achieving sub-5 ms latencies at a modest cost to trajectory smoothness [13,161].</p>
<p>-Hardware-Aware Optimizations-including tensorcore quantization and pipelined attention kernels-shrink runtime memory footprints below 8 GB and enable real-time inference on embedded GPUs [93].</p>
<p>Together, these methods have transformed VLAs into practical agents capable of handling language-conditioned, visionguided tasks in dynamic, real-world settings.</p>
<p>Parameter-Efficient Methods and Acceleration Techniques</p>
<p>in VLA Models Building on advances in data-efficient training, recent work has focused on reducing the parameter footprint and improving inference speed of VLA models-critical for deployment on resource-constrained robotic platforms.The iRe-VLA framework alternates between reinforcement learning (RL) in simulation and supervised finetuning on human demonstrations to stabilize policy updates.By leveraging Direct Preference Optimization (DPO) to shape reward models and Conservative Q-Learning to avoid extrapolation error, iRe-VLA reduces sample complexity by 60 % versus pure RL, while maintaining the semantic fidelity imparted by languageconditioned priors [123,65].This hybrid approach yields robust policies for tasks with sparse feedback, such as dynamic obstacle avoidance.7. Hardware-Aware Optimizations.Compiler-level graph rewrites and kernel fusion (e.g. via NVIDIA TensorRT-LLM) exploit target hardware features-tensor cores, fused attention, and pipelined memory transfers-to accelerate both transformer inference and diffusion sampling.</p>
<p>In OpenVLA-OFT, such optimizations reduced inference latency by 30 % on RTX A2000 GPUs and lowered energy per inference by 25 % compared to standard PyTorch execution [93].This makes real-time VLAs feasible on mobile robots and drones with strict power budgets.</p>
<p>Discussion.Parameter-efficient adaptation and inference acceleration techniques collectively democratize VLA deployment:</p>
<p>â€¢ LoRA and quantization empower smaller labs to fine-tune and operate billion-parameter VLAs on consumer-grade hardware, unlocking cutting-edge semantic understanding for robots [72,94].</p>
<p>â€¢ Pruning and FAST tokenization compress model and action representations, enabling sub-4 GB, sub-5 ms control loops without sacrificing precision in dexterous tasks [112,133].</p>
<p>â€¢ Parallel decoding and action chunking overcome sequential bottlenecks of autoregressive policies, supporting 100-200 Hz decision rates needed for agile manipulation and legged locomotion [13,161].</p>
<p>â€¢ Hybrid RL-SL training stabilizes exploration in complex environments, while hardware-aware compilation ensures real-time performance on edge accelerators [123,93].</p>
<p>Together, these advances make it practical to embed VLA models across industrial manipulators, assistive drones, and consumer robots, bridging the gap from research prototypes to real-world autonomy.</p>
<p>Applications of Vision-Language-Action Models</p>
<p>VLA models are rapidly emerging as foundational building blocks for embodied intelligence, integrating perception, natural language understanding, and motor control within a unified architecture.By encoding visual and linguistic modalities into shared semantic spaces and generating contextually grounded actions, VLA models enable seamless interaction between agents and their environments [102,220].This multimodal capacity has positioned VLAs as transformative agents across a wide spectrum of real-world applications.In humanoid robotics, systems like Helix and RoboNurse-VLA combine vision, language, and dexterous manipulation to assist with domestic tasks and surgical operations, demonstrating real-time reasoning and safety-aware control [103,186].In autonomous vehicles, models such as OpenDriveVLA and ORION process dynamic visual streams and natural language instructions to make transparent, adaptive driving decisions in complex urban environments [56,220].Industrial deployments leverage VLA architectures for high-precision assembly, inspection, and collaborative manufacturing [102].In agriculture, VLA-powered robotic systems enable vision-guided fruit harvesting, plant monitoring, and anomaly detection, reducing labor dependency and increasing sustainability.Furthermore, recent advances in interactive augmented reality systems utilize VLA models for real-time, language-conditioned spatial navigation, guiding users in indoor and outdoor settings based on voice or visual cues [150,59].Across these domains, VLAs offer a unified framework for robust, adaptable, and semantically aligned task execution, marking a pivotal shift toward embodied generalist agents.</p>
<p>Table 3 in the appendix shows the recent VLA models by summarizing their methodologies, application domains, and key innovations.</p>
<p>The following subsections chronologically explore the application areas in depth as shown in Figure 11.</p>
<p>Applications of VLA</p>
<p>Humanoid Robotics</p>
<p>Autonomous Vehicle Systems</p>
<p>Industrial Robotics</p>
<p>Healthcare &amp; Medical Robotics</p>
<p>Precision &amp; Automated Agriculture</p>
<p>Interactive AR Navigation</p>
<p>Figure 11: Mind-map of application domains for Vision-Language-Action models.</p>
<p>Humanoid Robotics</p>
<p>Humanoid robots, designed to mimic the form and functionality of the human body, represent one of the most demanding yet impactful domains for the deployment of VLA models.These platforms must seamlessly perceive complex environments, understand spoken or written natural language, and perform intricate physical tasks with human-level dexterity [144,22].The core strength of VLA models lies in their ability to unify perception, cognition, and control into a single, endto-end trainable framework-allowing humanoid robots to interpret visual inputs (e.g., RGB-D imagery of cluttered scenes), comprehend linguistic instructions (e.g., "place the spoon in the drawer"), and generate precise motor trajectories [118,222].</p>
<p>Recent advances have significantly accelerated the deployment of VLAs in humanoid robotics.For example, Helix 2 , a humanoid robot developed by Figure AI, leverages a fully integrated VLA model to perform full-body manipulation at high frequency, controlling arms, hands, torso, and even fine-grained finger motion in real time.The architecture follows a dualsystem design: a multimodal transformer processes inputs such as language commands and vision streams, while a real-time motor policy outputs dense action vectors at 200 Hz.This allows Helix to generalize across previously unseen objects and tasks, adapting fluidly to changing environments without the need for task-specific retraining.The key advantage of VLAs in humanoid systems is their ability to scale across diverse tasks using shared representations [8].Unlike traditional robotic systems that rely on taskspecific programming or modular pipelines, VLA-powered humanoids operate under a unified token-based framework.Vision inputs are encoded via pretrained vision-language models like DINOv2 or SigLIP, while instructions are processed using large language models such as Llama-2 or GPT-style encoders.These representations are fused into prefix tokens that capture the full context of the scene and task.Action tokens are then generated autoregressively, similar to language decoding, but represent motor commands for the robot's joints and effectors.</p>
<p>This capability enables humanoid robots to operate effectively in human-centric spaces, such as households, hospitals, and retail environments.In domestic settings, VLA-powered robots can clean surfaces, prepare simple meals, or organize objects simply by interpreting voice commands [118,222].In healthcare, systems like RoboNurse-VLA [103] have demonstrated the ability to perform precise instrument handovers to surgeons using real-time voice and visual cues.In retail, humanoid platforms equipped with VLAs can assist with customer queries, restock shelves, and navigate store layouts without explicit pre-programming [8].</p>
<p>What distinguishes modern humanoid VLAs is their ability to run on embedded, low-power hardware, making real-world deployment viable.For instance, systems such as TinyVLA [186] and MoManipVLA [190] demonstrate efficient inference pipelines that run on Jetson-class GPUs, enabling mobile deployment without compromising performance.These models exploit techniques like diffusion-based policies, LoRA-based fine-tuning, and dynamic token caching to minimize compute cost while retaining high precision and generalization.</p>
<p>In logistics and manufacturing, humanoid VLAs are already making a commercial impact.Robots like Figure 01 are deployed in warehouses to perform repetitive, physically intensive tasks-such as picking, sorting, and shelving-alongside human workers.Their ability to handle novel object categories and dynamically changing scenes is powered by continual learning and robust multimodal grounding [195,102].</p>
<p>As VLA models continue to advance in their capacity for diverse action generation, spatial reasoning, and real-time adaptation, humanoid robots are emerging as highly capable assistants across homes, industrial settings, and public spaces.Their strength lies in their ability to unify perception, language comprehension, and motor control through a shared token-based architecture-enabling seamless, context-aware behavior in unstructured human environments.</p>
<p>For example, as depicted in the figure 12, consider 'Helix', a state-of-the-art humanoid robot equipped with a nextgeneration VLA model.When instructed verbally, "Please take the water bottle from the fridge," Helix activates its integrated perception system, where a foundation vision-language model (e.g., SigLIP or DINOv2) segments the visual scene to identify the refrigerator, its handle, and the bottle.The language input is processed by a large language model such as LLaMA-2, which tokenizes the instruction and fuses it with the visual context.This fused representation is passed to a hierarchical controller: the high-level policy plans the task sequence (locate handle, pull door, identify bottle, grasp), while a mid-level planner defines motor primitives, such as grasp type and joint trajectories.The low-level VLA controller-often based on diffusion policy networks-executes these actions with sub-second latency.Upon encountering variations (e.g., a tilted bottle or slippery grip), Helix's agentic AI module performs micro-policy refinement in real time, adjusting its grip based on feedback.This example illustrates the transformative potential of humanoid VLAs.From kitchens to clinics, these systems not only interpret complex instructions and execute physical tasks with dexterity but also adapt to environmental unpredictability.By embedding agentic reasoning and safety alignment mechanisms, modern humanoid robots powered by VLAs are transitioning from narrow-task performers to generalist, trustworthy collaborators.As energy-efficient models like TinyVLA and MoMa-nipVLA mature, deployment on mobile, low-power platforms becomes increasingly practical-ushering in a new era of embodied, socially aligned AI.</p>
<p>Autonomous Vehicle Systems</p>
<p>Autonomous vehicles (AVs), including self-driving cars, trucks, and aerial drones, represent a frontier application domain for VLA models, where safety-critical decision-making demands tightly coupled perception, semantic understanding, and real-time action generation.Unlike traditional modular AV pipelines that decouple perception, planning, and control, VLA frameworks offer an integrated architecture that processes multimodal inputs-including visual streams, natural language instructions, and internal state information-within a unified autoregressive model capable of outputting precise control signals.</p>
<p>VLA models empower AVs to comprehend complex environments beyond pixel-level object recognition.For instance, a self-driving car navigating an urban setting must detect traffic signs, understand pedestrian behavior, and interpret navigation commands such as "take the second right after the gas station."These tasks involve fusing visual and linguistic signals to understand spatial relationships, predict intent, and generate context-aware driving actions.VLAs encode this information through token-based representations, where visual encoders (e.g., ViT, CLIP), language models (e.g., LLaMA-2), and trajectory decoders operate in a coherent semantic space, enabling the vehicle to reason about high-level goals and translate them into low-level motion.</p>
<p>A notable contribution in this direction is CoVLA [5], which provides a comprehensive dataset pairing over 80 hours of realworld driving videos with synchronized sensor streams (e.g., LiDAR, odometry), detailed natural language annotations, and high-resolution driving trajectories.This dataset enables training VLA models to align perceptual and linguistic features with physical actions.CoVLA employs CLIP for visual grounding, LLaMA-2 for instruction embedding, and trajectory decoders for motion prediction.This configuration allows AVs to interpret verbal cues (e.g., "yield to ambulance") and environmental conditions (e.g., merging traffic) to make transparent and safe driving decisions.</p>
<p>OpenDriveVLA [220] advances the state of VLA modeling by integrating hierarchical alignment of 2D/3D multi-view vision tokens with natural language inputs.Its architecture leverages both egocentric spatial perception and external scene understanding to construct a dynamic agent-environment-ego interaction model.Through autoregressive decoding, Open-DriveVLA generates both action plans (e.g., steering angle, acceleration) and trajectory visualizations interpretable to humans.Its end-to-end framework achieves state-of-the-art performance on planning benchmarks and question-answering tasks related to driving scenarios, demonstrating its robustness in urban navigation and behavioral prediction.</p>
<p>Another seminal model, ORION [56], pushes the boundaries of closed-loop autonomous driving by incorporating a QT-Former to retain long-horizon visual context, a large language model for reasoning over traffic narratives, and a generative trajectory planner.ORION excels at aligning the discrete reasoning space of vision-language models with the continuous control space of AV motion.This unified optimization results in accurate visual question answering (VQA) and trajectory planning, crucial for scenarios involving ambiguous human instructions or occluded obstacles (e.g., "take the exit after the red truck").</p>
<p>For example, as depicted in Figure 13 consider an autonomous delivery vehicle, "AutoNav," operating in a dense urban environment using a next-generation VLA architecture.As AutoNav receives a cloud-based instruction-"Drop off the package near the red awning beside the bakery, then return to base avoiding construction zones"-its onboard VLM (e.g., CLIP or SigLIP) parses the visual stream from multiple cameras, identifying dynamic landmarks such as bakery signs, red awnings, and traffic cones.Simultaneously, the LLM module grounded in LLaMA-2 decodes the instruction and fuses it with real-time sensory context including LiDAR, GPS, and inertial odometry.A hierarchical control stack processes these multimodal signals via an autoregressive VLA decoder that integrates egocentric views and world-centric maps to plan adaptive paths.As the vehicle approaches the delivery location, unexpected pedestrian activity prompts an agentic submodule to trigger trajectory re-planning using a reinforcement learning-inspired policy refinement routine.At the same time, AutoNav audibly warns pedestrians and recalibrates its speed to maintain safety margins.This interplay of semantic understanding, perceptual grounding, and adaptive control exemplifies the power of VLA-based systems in achieving interpretable, human-aligned behavior in safety-critical scenarios.It also demonstrates how such integration can surpass traditional perception-planning-control pipelines in autonomy, transparency, and decision-making agility.</p>
<p>In aerial robotics, VLAs enhance the capabilities of delivery drones and UAVs.Models such as UAV-VLA [150] combine satellite imagery, natural language mission descriptions, and onboard sensing to execute high-level commands (e.g., "deliver to the rooftop pad with the blue tarp").These systems use modular VLA architectures, where a vision-language planner parses global context and a flight controller executes precise waypoints, supporting applications in logistics, disaster response, and military reconnaissance.</p>
<p>As autonomous systems increasingly operate in unstructured environments, VLAs provide a scalable, interpretable, and dataefficient alternative to traditional pipelines.By learning from large-scale multimodal datasets and modeling decision-making as token prediction, VLAs align human-level semantics with robotic motion, paving the way for safer, smarter autonomous</p>
<p>VLA Driving</p>
<p>Agentic AI VLA decoder Figure 13: This illustration depicts an autonomous delivery vehicle powered by a VLA system, integrating VLMs for visual grounding, LLMs for instruction parsing, and a VLA decoder for path planning.Agentic AI enables adaptive trajectory refinement in dynamic environments, exemplifying how multimodal integration drives safe, interpretable, and autonomous decision-making in realworld navigation tasks.</p>
<p>driving and navigation technologies.</p>
<p>Industrial Robotics</p>
<p>Industrial robotics is undergoing a paradigm shift with the integration of VLA models, enabling a new generation of intelligent robots capable of high-level reasoning, flexible task execution, and natural communication with human operators [27,7].Traditional industrial robots typically operate in highly structured environments using rigid programming, often requiring extensive reconfiguration and manual intervention when adapting to new assembly lines or product variants [6,142].Such systems lack the semantic grounding and adaptability required for modern dynamic manufacturing settings.</p>
<p>VLA models, by contrast, offer a more human-interpretable and generalizable framework.Through the joint embedding of visual inputs (e.g., component layout or conveyor belt state), natural language instructions (e.g., "tighten the screw on the red module"), and robot state, VLAs can infer context and execute appropriate control commands in real-time [105,58,121].Vision transformers (e.g., ViT, DINOv2), large language models (e.g., LLaMA-2), and autoregressive or diffusion-based action decoders form the backbone of these systems, allowing the robot to parse multimodal instructions and perform actions grounded in its environment.</p>
<p>One of the most significant contributions in this domain is CogACT [102], a componentized VLA framework explicitly designed for industrial robotic manipulation.Unlike early VLAs that relied on frozen language-vision embeddings followed by direct action quantization, CogACT introduces a diffusion-based action transformer that models action sequences more robustly and adaptively.The system uses a visual-language encoder (e.g., Prismatic-7B) to extract highlevel scene and instruction embeddings, which are then passed to a diffusion transformer (DiT-Base) to generate fine-grained motor actions.This modular separation enables superior generalization to unseen tools, parts, and layouts while preserving interpretability and robustness under real-world constraints.</p>
<p>Furthermore, CogACT demonstrates rapid adaptation across different robot embodiments-such as 6-DoF arms or bimanual systems-through efficient fine-tuning, making it suitable for deployment across heterogeneous factory environments [102].Empirical evaluations show that CogACT outperforms prior models like OpenVLA by over 59% in real-world task success rates, especially in complex, high-precision tasks such as multistep assembly, screw fastening, and part sorting.</p>
<p>As manufacturing shifts toward Industry 4.0 paradigms, VLAs promise to reduce programming overhead, support voice-commanded robot programming, and facilitate real-time human-robot collaboration on mixed-initiative tasks.While execution precision, safety guarantees, and latency optimizations remain areas of active research, the use of VLA models in industrial robotics marks a substantial step toward autonomous, intelligent, and adaptable robotic factories.</p>
<p>Healthcare and Medical Robotics</p>
<p>Healthcare and medical robotics represent a high-stakes domain where precision, safety, and adaptability are paramount-qualities that VLA models are increasingly well-suited to provid [103,151].Traditional medical robotic systems rely heavily on teleoperation or pre-programmed behaviors [130,158], limiting their autonomy and responsiveness in dynamic surgical or care environments.In contrast, VLA models offer a flexible framework that integrates real-time visual perception, language comprehension, and fine-grained motor control, enabling medical robots to understand high-level instructions and autonomously perform intricate procedures or assistance tasks [102,43,174].</p>
<p>In surgical robotics, VLAs can dramatically enhance capabilities in minimally invasive operations [40,177].These systems can fuse laparoscopic video feeds [98], anatomical maps [114,40], and voice commands into a unified tokenized representation using vision encoders (e.g., ViT, SAM-2) and language models (e.g., LLaMA, T5) [181].For instance, as depicted in Figure 14a, in a task like "apply a suture to the left coronary artery," the vision module identifies the anatomical target, while the language module contextualizes the instruction.The action decoder then translates the fused semantic embedding into stepwise motion commands with sub-millimeter precision.This enables the robot to adaptively reposition tools, apply dynamic force feedback, and avoid critical structures, reducing the need for surgeon micromanagement and minimizing risk of human error.</p>
<p>Beyond the operating room, VLA models are powering a new generation of patient-assistive robots in eldercare, rehabilitation, and hospital logistics.These systems can autonomously perceive patient behavior, understand spoken or gestural input, and execute responsive tasks such as retrieving medication, guiding mobility aids, or notifying caregivers during emergencies.For example, as depicted in Figure 14b, a VLA-enabled robot can visually detect a patient attempting to rise from bed,</p>
<p>(a) (b)</p>
<p>Figure 14: a) This figure illustrates a VLA surgical system executing the task "apply a suture to the left coronary artery."The vision module identifies anatomical targets, the language model interprets the instruction, and the action decoder generates precise motor commands, enabling adaptive tool control, real-time feedback, and safe autonomous operation; b) A VLA-powered assistive robot perceives patient behavior, processes verbal requests (e.g., "bring my walker"), and autonomously executes context-aware motion plans, enabling real-time assistance in eldercare, rehabilitation, and hospital logistics without relying on predefined scripts or manual oversight.interpret a verbal request such as "bring my walker," and generate a context-appropriate motion plan to assist-without predefined scripts or constant supervision.</p>
<p>Recent VLA frameworks such as RoboNurse-VLA [103] highlight the real-world feasibility of this approach.RoboNurse employs SAM-2 for semantic scene segmentation and LLaMA-2 for command comprehension, integrated into a real-time voice-to-action pipeline that enables robots to assist with surgical instrument handovers in operating rooms [103].The system demonstrates robustness to diverse tools, varied lighting conditions, and noisy environments-common challenges in clinical settings.</p>
<p>Additionally, VLA architectures offer advantages in explainability and auditability, both critical in regulated medical domains [173,113].Scene grounding and trajectory prediction can be visualized and reviewed post-hoc [208], which could facilitate clinical trust and enabling FDA-style validation pipelines.LoRA-based fine-tuning allows adaptation to specific hospital environments or procedural workflows with minimal data and compute [9,176,114].</p>
<p>Importantly, the multimodal foundation of VLA models enables cross-domain transferability: the same model trained on surgical tool manipulation can be adapted to patient mobility tasks with modest retraining [45].This modularity significantly reduces development time and cost compared to task-specific automation systems [73].As medical robotics transitions from teleoperated assistance to semi-autonomous and collaborative systems, VLA models stand at the core of this transformation.</p>
<p>By combining high-level semantic understanding with lowlevel control, VLAs provide a unified solution for scalable, human-aligned, and adaptive robotic healthcare [193,221,209].As healthcare systems face increasing demand and workforce shortages, VLA-driven robotics will play a crucial role in enhancing medical precision, operational efficiency, and patient-centered care.</p>
<p>Precision and Automated Agriculture</p>
<p>As illustrated in Figure 15, VLA models are emerging as transformative tools in precision and automated agriculture, offering intelligent, adaptive solutions for labor-intensive tasks across diverse farming landscapes [57,150].Unlike traditional agricultural automation systems that depend on rigid, sensordriven pipelines and require manual reprogramming for each task or environmental variation [169,84], VLAs integrate multimodal perception, natural language understanding, and realtime action generation within a unified framework [131,66].This enables autonomous ground robots and drones to interpret complex field scenes, follow spoken or text-based farming instructions, and generate context-aware actions such as selective fruit picking or adaptive irrigation.The ability of VLAs to dynamically adjust to occlusions, terrain irregularities, or varying crop types-combined with training on synthetic, photorealistic datasets-allows them to generalize across geographies and seasons.By leveraging action tokenization [189], transformer-based policy generation [11,67], and techniques like LoRA fine-tuning [72], these systems are redefining the scalability and intelligence of agricultural robotics for sustainable and precision-driven farming.</p>
<p>In modern orchards and crop fields, VLAs can process visual inputs from RGB-D cameras, multispectral sensors, or drones to monitor plant growth, detect diseases, and identify nutrient deficiencies.Vision transformers (e.g., ConvNeXt, DINOv2) encode spatial and semantic information from visual scenes, while large language models (e.g., T5, LLaMA) parse natural language commands-such as "inspect the east plot for powdery mildew" or "harvest ripe apples near the irrigation trench."Through token fusion, these modalities are aligned in a shared representation space, allowing robots to execute fine-grained, context-aware actions with precision.</p>
<p>For instance, in fruit-picking tasks, as illustrated in Figure 15, a VLA-equipped ground robot can identify ripe produce using image-based ripeness cues, interpret user-specified criteria such as "pick only Grade A fruits," and execute motion sequences via action tokens that control its end-effector.This approach ensures minimal crop damage, optimizes pick rates, and allows real-time adaptation to unexpected variables like occlusions or terrain shifts.In irrigation management, drones guided by VLA models can interpret field maps and verbal instructions to selectively water stressed zones, reducing water usage by up to 30%.</p>
<p>Moreover, VLA models support dynamic reconfiguration and lifelong learning.With access to synthetic training datasets generated from photorealistic simulations of crop environments (e.g., 3D orchard renderings), models can be trained to recognize pests, weeds, and crop maturity stages without extensive manual annotation.Techniques like LoRA adapters and diffusion-based policy tuning further enhance generalization to novel crops, seasons, and geographical regions.</p>
<p>The integration of VLAs into agricultural workflows offers significant benefits: reduced dependence on skilled labor, higher yield through targeted intervention, and enhanced environmental sustainability through optimized input usage.As global food systems grapple with climate variability and resource constraints, VLA-enabled agriculture will play a pivotal role in advancing scalable, intelligent, and sustainable farming practices tailored to real-world complexity.</p>
<p>Interactive AR Navigation with Vision-Language-Action</p>
<p>Models Interactive Augmented Reality (AR) navigation represents a frontier where the VLA models can significantly enhance human-environment interaction by providing intelligent, context-aware guidance in real-time [26,80,197].In this paradigm, VLAs process continuous streams of visual data from AR-enabled devices-such as smart glasses or smartphones-alongside natural language queries to generate dynamic navigational cues overlaid directly onto the user's view of the physical world.Unlike traditional GPS-based systems that rely on rigid maps and limited user input [24,159], VLAbased AR agents interpret complex visual scenes (e.g., intersections, indoor hallways, signage) and respond to free-form instructions such as "take me to the nearest pharmacy with a wheelchair ramp" or "show the quietest route to the conference room."</p>
<p>Technically, these models integrate a vision encoder (e.g., ViT, DINOv2) that extracts scene representations from firstperson RGB frames, a language encoder (e.g., T5 or LLaMA) that processes user prompts or voice commands, and an action decoder that predicts tokenized navigation cues such as directional overlays, waypoints, or voice instructions.A transformer-based architecture fuses these modalities to reason about both the spatial layout and semantic intent, allowing the AR agent to adaptively highlight paths, landmarks, and hazards directly within the user's field of view [163,129].For example, as shown in Figure 16, in a crowded airport, the VLA agent could visually identify escalators, gates, or baggage claims while understanding a query like "how do I reach Gate 22 without stairs?",adjusting the route in response to real-time occupancy and obstacles.</p>
<p>VLAs also support interaction loops that enable users to refine instructions (e.g., "avoid busy areas" or "take the scenic route") and receive context-aware feedback, improving accessibility for the visually impaired or cognitively challenged.In logistics and indoor navigation, these systems can be integrated with IoT sensors and digital twins to guide warehouse workers, maintenance teams, or delivery robots through complex environments.Furthermore, personalized navigation can be achieved through continual fine-tuning, where VLA models learn user preferences and local spatial layouts over time.</p>
<p>As AR hardware becomes more affordable and integrated into daily life, VLA-powered navigation systems will enable seamless spatial understanding, multimodal interaction, and autonomous guidance in public, industrial, and assistive contexts-redefining how humans perceive, explore, and interact with physical spaces.</p>
<p>Challenges and Limitations of Vision-Language-Action Models</p>
<p>VLA models face a spectrum of interrelated challenges that impede their translation from research prototypes to robust, real-world systems.First, achieving real-time, resource-aware inference remains difficult: models like DeeR-VLA leverage dynamic early-exit architectures to cut computation 5-6Ã— on manipulation benchmarks while preserving accuracy, yet their gains diminish in complex scenarios [202].Similarly, Uni-NaVid compresses egocentric video tokens for 5 Hz navigation but still struggles under highly ambiguous instructions and In dynamic environments such as airports, VLAs interpret user queries like "avoid stairs to Gate 22," analyze visual scenes (e.g., detecting escalators), and adjust navigational paths accordingly, supporting personalized, accessible, and context-aware mobility guidance.</p>
<p>longer horizons [210].Coupled with limited object generalization, even advanced hybrid vision-language grounding schemes (e.g., ObjectVLA) generalize to only 64 % of novel objects, un-derscoring persistent gaps in open-world robustness [223].</p>
<p>Second, adapting VLA models with minimal supervision and ensuring stable policy updates under scarce, noisy data is nontrivial.ConRFT combines behavior cloning and Qlearning with human-in-the-loop fine-tuning to rapidly converge to 96.3% success over eight contact-rich tasks, yet it relies heavily on expert interventions and reward shaping [31].Hierarchical frameworks such as Hi Robot decouple high-level reasoning from low-level execution to improve instruction fidelity, but coordinating these modules and grounding ambiguous feedback remains challenging [155].Likewise, TLA's fusion of tactile streams with language commands achieves over 85 % success on unseen peg-in-hole tasks, but dataset breadth and real-time multi-step decoding still limit broader generalization [70].</p>
<p>Furthermore, ensuring safety, generalization, and end-to-end reliability in dynamic environments demands new modeling and evaluation standards.Occupancy-Language-Action models like OccLLaMA unify 3D scene understanding with action planning, yet they must scale to richer scene dynamics and semantic consistency across modalities [183].RaceVLA pushes high-speed drone navigation via quantized, iterative control loops, but its visual-physical generalization trails larger VLAs and dedicated reasoners [153].Model-merging strategies in ReVLA recover lost out-of-domain visual robustness-improving OOD grasp success by up to 77 %-but introduce extra computation and complexity [39].Finally, SafeVLA formulates constraints via constrained Markov decision processes to cut unsafe behavior by over 80 %, yet defining comprehensive, non-restrictive safety rules for diverse real-world tasks remains an open problem [205].Addressing these intersecting limitations is critical for VLA models to achieve reliable, autonomous operation against the full complexity of realworld robotics.</p>
<p>Building upon the critical limitations outlined above, it is imperative to map each challenge to targeted mitigation strategies and forecast their system-level impact.Table 4 distills this mapping into three columns-identifying core limitations, proposing concrete technical remedies drawn from recent advances, and articulating the anticipated benefits for real-world VLA deployment.For instance, tackling real-time inference constraints leverages parallel decoding and quantized transformer pipelines with hardware acceleration (e.g., TensorRT) to sustain control loop rates in drones and manipulators [100,94,60,110].Addressing multimodal action representation via hybrid diffusion-autoregressive policies enriches a model's capacity to produce varied, context-sensitive motor commands for complex tasks [133,121].To guarantee safety in open worlds, dynamic risk assessment modules and adaptive planning layers can be integrated, ensuring robust emergency stop behaviors in unpredictable settings [143,180,87].Similarly, dataset bias and grounding are countered through curated debiased corpora and advanced contrastive fine-tuning, bolstering fairness and semantic fidelity when generalizing to novel objects and scenes [145,16,136].Together, these solution pathways-and others spanning simulation-to-real transfer, tactile integration, and energy-efficient architectures-frame a com-prehensive roadmap for transitioning VLA research into reliable, scalable autonomy.</p>
<p>The remainder of this section is organized into five focused subsections, each examining a distinct cluster of VLA challenges identified in the literature.First, we analyze real-time inference constraints and the emerging methods that address them.Next, we delve into multimodal action representation alongside safety assurance in open-world settings.We then discuss dataset bias, grounding strategies, and generalization to unseen tasks, followed by an exploration of system integration complexity and computational demands.Finally, we consider robustness and the ethical implications of deploying VLAs in real-world applications.</p>
<p>Real-Time Inference Constraints</p>
<p>Real-time inference remains a significant limitation in deploying VLA models, particularly in latency-critical applications like robotic manipulation, autonomous driving, and drone control.VLAs typically depend on autoregressive decoding strategies, which sequentially generate action tokens based on previous predictions.While effective for many tasks, this method severely restricts inference speed, typically achieving only 3-5 Hz.This rate falls dramatically short of the 100 Hz or greater frequency required by robotic systems for precise and fluid real-time control.For instance, when a robotic arm manipulates delicate objects, frequent positional updates are essential to maintain accuracy and prevent damage.Models such as OpenVLA [94] and Pi-0 [14] face inherent challenges with this sequential token generation approach, thereby limiting their effectiveness in dynamic environments.</p>
<p>Emerging solutions such as parallel decoding, exemplified by NVIDIA's Groot N1 model [13], aim to accelerate inference by predicting multiple tokens simultaneously.Groot N1 achieves approximately a 2.52Ã— speedup over traditional decoding methods; however, this parallelism often introduces tradeoffs in trajectory smoothness, resulting in jerky or suboptimal robot movements.Such movements are undesirable in sensitive applications like surgical robotics, where precision and fluidity are paramount.Thus, achieving rapid inference without compromising output quality remains an open challenge.</p>
<p>Additionally, hardware limitations exacerbate real-time inference constraints.For example, processing high-dimensional visual embeddings, typically involving over 400 vision tokens at 512 dimensions each, requires approximately 1.2 GB/s memory bandwidth.This demand significantly exceeds the capacity of current embedded systems or edge-AI hardware such as NVIDIA Jetson platforms, thereby restricting practical deployment.Even with efficient quantization techniques, which reduce the precision of floating-point operations to alleviate memory constraints, models frequently experience accuracy degradation, especially in tasks demanding sub-millimeter precision, such as bimanual robotic manipulation or medical robotics.</p>
<p>Multimodal Action Representation and Safety Assurance</p>
<p>Multimodal Action Representation: One significant limitation of current VLA models is accurately representing mul-timodal actions, particularly in scenarios requiring continuous and nuanced control [51,38].Traditional discrete tokenization methods, such as those dividing actions into 256 distinct bins, inherently lack precision, creating substantial errors in fine-grained tasks like delicate robotic grasping or intricate surgical procedures [133].For instance, during precise robotic manipulation in assembly tasks, discrete representations can result in misaligned or imprecise actions, undermining performance and reliability.On the other hand, continuous multilayer perceptron (MLP) based approaches face the risk of mode collapse [126,179], where models converge prematurely to single action trajectories, despite multiple viable paths available.This diminishes the flexibility necessary for adaptive decisionmaking in highly dynamic environments.Emerging diffusionbased policies, exemplified by models like Pi-Zero and RDT-1B [112], offer richer multimodal action representation capable of capturing diverse action possibilities.However, their substantial computational overhead-approximately three times that of conventional transformer-based decoders-renders them impractical for real-time deployment.Consequently, VLA models currently struggle with complex dynamic tasks, such as robotic navigation in densely crowded spaces or sophisticated bimanual manipulations [59,191], where multiple strategic actions may be equally valid and contextually dependent.</p>
<p>Safety Assurance in Open Worlds: Another critical challenge facing VLAs is ensuring robust safety in dynamic, unpredictable environments characteristic of real-world scenarios [33,205].Many current implementations depend heavily on predefined hardcoded force and torque thresholds, significantly constraining their adaptability in encountering unforeseen or novel conditions, such as unexpected obstacles or sudden environmental changes [121].Models used for collision prediction typically attain only about 82% accuracy in cluttered and dynamic spaces, posing serious risks in applications such as warehouse logistics or domestic robotics, where safety margins are minimal [217,94].Moreover, the essential safety mechanisms like emergency stops incorporate substantial latency-often between 200 and 500 milliseconds-due to comprehensive safety verifications [132,94].This delay, although seemingly minor, can prove hazardous in high-speed operations or critical interventions, such as automated driving or emergency robotic responses.</p>
<p>Dataset Bias, Grounding, and Generalization to Unseen</p>
<p>Tasks A significant obstacle limiting the effectiveness of VLA models is the pervasive presence of dataset bias and grounding deficiencies.Current training datasets, predominantly sourced from web-crawled repositories, frequently exhibit inherent biases [165,91].Studies indicate that approximately 17% of associations within standard datasets are skewed toward stereotypical interpretations, such as disproportionately associating terms like "doctor" with male figures [171,95].These biases propagate through training, resulting in VLAs that produce semantically misaligned or contextually inappropriate responses when deployed in diverse environments.For instance, models such as OpenVLA have been documented to overlook ap-proximately 23% of object references in novel settings, significantly impairing their practical utility in real-world applications where accurate interpretation of instructions is critical [94].This grounding issue also extends to challenges in compositional generalization, where VLAs often falter when encountering rare or unconventional combinations, such as interpreting a phrase like "yellow horse" because of underrepresentation in training corpora.These shortcomings highlight an urgent need for carefully curated, balanced, and comprehensive datasets, coupled with advanced grounding algorithms designed to mitigate biases and enhance semantic alignment across varied contexts.</p>
<p>Complementing the challenges posed by dataset bias is the broader issue of generalization to unseen tasks, a critical barrier for the practical deployment of VLAs.While existing models demonstrate proficiency in familiar environments or tasks similar to their training scenarios, their performance significantly degrades-often by as much as 40%-when encountering entirely novel tasks or unfamiliar variations.For example, a VLA trained specifically on domestic tasks may struggle or outright fail when introduced into industrial or agricultural settings, largely due to discrepancies in object types, environmental dynamics, and operational constraints.This limitation arises primarily from overfitting to narrowly scoped training distributions and insufficient exposure to diverse task representations.Consequently, current VLAs exhibit limited proficiency in zero-shot or few-shot learning scenarios, impeding their adaptability and scalability.</p>
<p>System Integration Complexity and Computational Demands</p>
<p>Integrating VLA models within dual-system architectures, which combine high-level cognitive planning (System 2) and real-time physical control (System 1), presents significant complexity in robotic applications.A primary challenge arises from temporal mismatches between these two systems.Typically, System 2 leverages large language models (LLMs) such as GPT or Llama-2 for complex task decomposition and strategic planning.These models, due to their substantial computational requirements, often exhibit processing times of approximately 800 ms or more per inference cycle.Conversely, System 1 components, tasked with executing rapid, low-level motor actions through control loops, operate on millisecond timescales-often around 10 ms intervals.This stark discrepancy in operational cadence leads to synchronization difficulties, causing delays and potentially suboptimal execution trajectories.For example, NVIDIA's Groot N1 model demonstrates an effective integration of these two systems but still suffers from occasional jerkiness in motion due to asynchronous interaction, highlighting this intrinsic challenge.</p>
<p>Furthermore, the feature space misalignment between highdimensional vision encoders, such as Vision Transformers (ViT), and lower-dimensional action decoders exacerbates integration complexity.When attempting to reconcile these disparate embeddings, the coherence between perceptual understanding and actionable commands can deteriorate significantly.OpenVLA [94] and RoboMamba [111], which utilize transformer-based visual processing and subsequent action decoding, illustrate these integration challenges-resulting in diminished performance when ported from simulation environments to physical hardware deployments.Such discrepancies manifest as high as a 32% reduction in performance, primarily due to mismatches between simulated dynamics and real-world sensor noise or calibration issues.</p>
<p>Energy and compute demands constitute another significant barrier for VLA deployment, particularly in edge computing contexts typical of autonomous drones, mobile robots, and wearable robotic systems.The substantial parameter counts typical of advanced VLAs-for instance, models possessing upwards of 7 billion parameters-necessitate computational resources often exceeding 28 GB of VRAM in their native form.These requirements vastly outpace the capabilities of most current edge-oriented processors and GPUs, restricting the practical applicability of sophisticated VLAs outside specialized, high-resource environments.</p>
<p>Robustness and Ethical Challenges in VLA Deployment</p>
<p>The practical deployment of VLA models faces substantial challenges regarding robustness to environmental variability and ethical considerations.Environmental robustness refers to the VLA's capacity to maintain stable and accurate performance across dynamically changing conditions.Real-world environments frequently introduce unpredictable variations such as fluctuating lighting, weather conditions, or partial occlusions.For instance, vision modules within VLAs, such as those employed by OpenDriveVLA [220], exhibit accuracy reductions of approximately 20-30% under low-contrast or shadow-heavy scenarios due to inadequate processing capabilities of current visual encoders.Similarly, linguistic comprehension in VLAs like CoVLA [5] is adversely affected in acoustically noisy or ambiguous contexts, where instructions can become difficult to interpret accurately, leading to task execution errors.Additionally, robotic manipulation tasks using VLA-equipped systems such as RoboMamba [111] frequently struggle with cluttered environments, misjudging positions or orientations of partially occluded objects, thereby compromising task success.</p>
<p>Discussion</p>
<p>As illustrated in Figure 17, VLA models face a multifaceted set of challenges that span algorithmic, computational, and ethical dimensions.First, achieving real-time inference on resource-constrained hardware remains difficult due to the sequential nature of autoregressive decoders and the high dimensionality of multimodal inputs.Second, fusing vision, language, and action into coherent policies introduces safety vulnerabilities when encountering unanticipated environmental changes.Third, dataset bias and grounding errors compromise generalization, often causing models to fail on out-of-distribution tasks.Fourth, integrating diverse components-perception, reasoning, control-yields complex architectures that are hard to optimize and maintain.Fifth, the energy and compute demands of large VLA systems hinder de-ployment on embedded or mobile platforms.Finally, robustness to environmental variability and ethical considerations, such as privacy and bias mitigation, add layers of societal and regulatory concern.Collectively, these limitations constrain the practical adoption of VLA models in real-world robotics, autonomous systems, and interactive applications.The potential solutions to these challenges are discussed in the below points.</p>
<p>Potential Solutions</p>
<p>â€¢ Real-Time Inference Constraints.Future research must develop VLA architectures that harmonize latency, throughput, and task-specific accuracy.One promising direction is the integration of specialized hardware accelerators-such as FPGA-based vision processors and tensor cores optimized for sparse matrix operations-to execute convolutional and transformer layers at sub-millisecond scales [94,100].Model compression techniques like Low-Rank Adaptation (LoRA) [72] and knowledge distillation can shrink parameter counts by up to 90%, reducing both memory footprint and inference time while retaining over 95% of original performance on benchmark tasks.Progressive quantization strategies that combine mixed-precision arithmetic (e.g., FP16/INT8) with blockwise calibration can further cut computation by 2-4Ã— with minimal accuracy loss [93].Adaptive inference architectures that dynamically adjust network depth or width based on input complexity-akin to early-exit branches in DeeR-VLA [202]-can reduce average compute by selectively bypassing transformer layers when visual scenes or linguistic commands are simple.Finally, efficient tokenization schemes leveraging subword patch embeddings and dynamic vocabulary allocation can compress visual and linguistic input into compact representations, minimizing token counts without sacrificing semantic richness [133].Together, these innovations can enable sub-50 ms end-to-end inference on commodity edge GPUs, paving the way for latency-sensitive applications in autonomous drone flight, real-time teleoperation, and collaborative manufacturing.</p>
<p>â€¢ Multimodal Action Representation and Safety Assurance.Addressing multimodal action representation and robust safety requires end-to-end frameworks that unify perception, reasoning, and control under stringent safety constraints.Hybrid policy architectures combining diffusion-based sampling for low-level motion primitives [34] with autoregressive high-level planners [186] enable compact stochastic representations of diverse action trajectories, improving adaptability in dynamic environments.Safety can be enforced via real-time risk assessment modules that ingest multi-sensor fusion streams-visual, depth, and proprioceptive data-to predict collision probability and joint stress thresholds, triggering emergency stop circuits when predefined safety envelopes are breached [143,180].Reinforcement learning algorithms augmented with constrained optimization (e.g., Lagrangian methods in SafeVLA [205]) can learn  policies that maximize task success while strictly respecting safety constraints.Online model adaptation techniques-such as rule-based RL (GRPO) and Direct Preference Optimization (DPO)-further refine action selection under new environmental conditions, ensuring consistent safety performance across scenarios [87].Crucially, embedding formal verification layers that symbolically analyze planner outputs before execution can guarantee compliance with safety invariants, even for neuralnetwork-based controllers.Integrating these methodologies will produce VLA systems that not only execute complex, multimodal actions but do so with provable safety in unstructured, real-world settings.</p>
<p>Challenges Solutions</p>
<p>â€¢ Dataset Bias, Grounding, and Generalization to Unseen Tasks.</p>
<p>Robust generalization demands both broadened data diversity and advanced learning paradigms.Curating large-scale, debiased multimodal datasets-combining web-scale image-text corpora like LAION-5B [152] with robot-centric trajectory archives such as Open X-Embodiment [175]-lays the groundwork for equitable semantic grounding.Hard-negative sampling and contrastive fine-tuning of vision-language backbones (e.g., CLIP variants) can mitigate spurious correlations and enhance semantic fidelity [16,212].Meta-learning frameworks enable rapid adaptation to novel tasks by learning shared priors across task families, as demonstrated in vision-language robotic navigation models [136].Continual learning algorithms-with replay buffers and regularization strategies-preserve old knowledge while integrating new concepts, addressing catastrophic forgetting in VLA models [39].Transfer learning from 3D perception domains (e.g., point cloud reasoning in 3D-VLA [217]) can imbue models with spatial inductive biases, improving out-of-distribution robustness.Finally, simulation-to-real (sim2real) fine-tuning with domain randomization and real-world calibration-such as dynamic lighting, texture, and physics variations-ensures that policies learned in synthetic environments transfer effectively to physical robots [4,53].These combined strategies will empower VLAs to generalize confidently to unseen objects, scenes, and tasks in real-world deployments.</p>
<p>â€¢ System Integration Complexity and Computational Demands.To manage the intricate orchestration of multimodal pipelines under tight compute budgets, researchers must embrace model modularization and hardware-software co-design.Low-Rank Adaptation (LoRA) adapters can be injected into pre-trained transformer layers, enabling task-specific fine-tuning without modifying core weights [72].Knowledge distillation from large "teacher" VLAs into lightweight "student" networks-using student-teacher mutual information objectives-yields compact models with 5-10Ã— fewer parameters while retaining 90-95% task performance [93].Mixed-precision quantization augmented by quantizationaware training can compress weights to 4-8 bits, cutting memory bandwidth and energy consumption by over 60% [94].Hardware accelerators tailored for VLA workloads-supporting sparse tensor operations, dynamic token routing, and fused vision-language kernels-can deliver sustained 100+ TOPS throughput within a 20-30 W power envelope, meeting the demands of embedded robotic platforms [133,186].Toolchains like TensorRT-LLM [100] and TVM can optimize end-to-end VLA graphs for specific edge devices, fusing layers and precomputing static subgraphs.Emerging architectures such as TinyVLA demonstrate that sub-1 B parameter VLAs can achieve near-state-of-the-art performance on manipulation benchmarks with real-time inference, charting a path for widespread deployment in resource-constrained settings.</p>
<p>â€¢ Robustness and Ethical Challenges in VLA Deployment.Ensuring VLA robustness and ethical integrity requires both technical and governance measures.Domain randomization and synthetic augmentation pipelines-like UniSim's closed-loop sensor simulator-generate photorealistic variations in lighting, occlusion, and sensor noise, enhancing model resilience to environmental shifts [200].Adaptive recalibration modules, which adjust perception thresholds and control gains based on real-time feedback, further mitigate drift and sensor degradation over prolonged operation.On the ethical front, bias auditing tools must scan training datasets for skewed demographic or semantic distributions, followed by corrective fine-tuning using adversarial debiasing and counterfactual augmentation [145,212].Privacy-preserving inferencing-via on-device processing, homomorphic encryption for sensitive data streams, and differential privacy during training-safeguards user data in applications like healthcare and smart homes [124,140].Socioeconomic impacts can be managed through transparent impact assessments and stakeholder engagement, ensuring that VLA adoption complements human labor through upskilling programs rather than displacing workers en masse.Finally, establishing regulatory frameworks and industry standards for VLA safety and accountability will underpin responsible innovation, balancing technical capabilities with societal values.</p>
<p>Future Roadmap</p>
<p>The future of VLA models lies at the intersection of increasingly powerful multimodal foundations, agentic reasoning, and embodied continual learning.Over the next decade, we anticipate several converging trends that will propel VLAs from capable but narrow task specialists toward the core of truly generalist robotic intelligence.</p>
<ol>
<li>Multimodal Foundation Models as the "Cortex."Today's VLAs typically couple a vision-language backbone with task-specific policy heads.Tomorrow, we expect a single, massive multimodal foundation model-trained on web-scale image, video, text, and affordance data-to serve as a shared perceptual and conceptual "cortex."This foundation will encode not only static scenes but also dynamics, physics, and common-sense world knowledge, enabling downstream action learners to tap into a unified representational substrate rather than reinventing basic perceptual skills for every robot or domain.2. Agentic, Self-Supervised Lifelong Learning.Rather than static pretraining, future VLAs will engage in continual, self-supervised interaction with their environments.Agentic frameworks-where the model generates its own exploration objectives, hypothesizes outcomes, and selfcorrects via simulated or real rollouts-will drive rapid skill acquisition.By formulating internal sub-goals ("learn to open drawers," "map furniture affordances") and integrating reinforcement-style feedback, a VLA-driven humanoid could autonomously expand its capabilities over years of deployment, much like a human apprentice.3. Hierarchical, Neuro-Symbolic Planning.To scale from low-level motor primitives to high-level reasoning, VLAs will adopt hierarchical control architectures.A top-level language-grounded planner (perhaps an LLM variant finetuned for affordance reasoning) will decompose complex instructions ("prepare a cup of tea") into sequences of sub-tasks ("fetch kettle," "fill water," "heat water," "steep tea bag").Mid-level modules will translate these into parameterized motion plans, and low-level diffusion or transformer-based controllers will generate smooth, compliant trajectories in real time.This neuro-symbolic blend ensures both the interpretability of structured plans and the flexibility of learned policies.4. Real-Time Adaptation via World Models.Robustness in unstructured settings demands that VLAs maintain an internal, predictive world model-an up-to-date simulation of objects, contacts, and agent dynamics.As the robot acts, it will continuously reconcile its predictions with sensor feedback, using model-based corrective actions when discrepancies arise (e.g., slipping grasp).Advances in differentiable physics and video-to-state encoders will make these world models both accurate and efficient enough for on-board, real-time use.Cross-Embodiment and Transfer Learning: The era of training separate VLAs for each robot morphology will give way to embodiment-agnostic policies.By encoding actions in an abstract, kinematicagnostic space (e.g., "apply grasp force at these affordance points"), future VLAs will transfer skills seamlessly between wheeled platforms, quadrupeds, and humanoids.Combined with meta-learning, a new robot can bootstrap prior skills with only a few minutes of calibration data.Safety, Ethics, and Human-Centered Alignment As VLAs gain autonomy, built-in safety and value alignment become non-negotiable.Future systems will integrate real-time risk estimators-assessing potential harm to humans or property before executing high-risk maneuvers-and seek natural language consent for ambiguous situations.Regulatory constraints and socially aware policies will be baked into the VLA stack, ensuring that robots defer to human preferences and legal norms.As illustrated in Figure 18, the future of VLA-based robotics lies in the integration of three foundational components: Vision-Language Models (VLMs), VLA architectures, and agentic AI systems.Consider "Eva," a generalist humanoid assistant operating in a household.At the perception layer, Eva's foundation VLM interprets multimodal inputs by segmenting visual scenes into discrete object-level representations, predicting affordances (e.g., graspable, fragile), and simulating dynamic behaviors through an internal world model.This VLM layer enables high-level visual understanding grounded in language semantics and physical properties.Upon receiving a user command such as "Eva, clean the coffee spill and water the plants," the VLA module activates.This core architecture combines tokenized language inputs and sensory feedback to perform hierarchical task planning.A high-level planner decomposes the instruction into actionable subtasks (e.g., locate cloth, wipe spill, retrieve watering can), which are then converted into motion trajectories via a mid-level policy module.These plans are passed to a low-level diffusion-policy controller, responsible for generating smooth, physics-aware joint movements tailored to the robot's embodiment.Complementing these is Eva's agentic AI module, which supports continual learning and adaptation.When confronted with unexpected challenges-like a sticky stain-Eva invokes an internal selfimprovement loop, running real-time simulated variations to refine its wiping strategy without human supervision.Safety and alignment are ensured through human-aware policies: proxim-ity sensors, real-time monitoring, and verbal confirmations before high-risk actions.Overnight, Eva performs autonomous review of performance logs, refining sub-policies via simulated rollouts.Together, this VLM-VLA-agentic triad marks a significant leap toward embodied AGI.It enables robots like Eva to perceive, plan, act, adapt, and safely coexist with humans, ultimately transforming how intelligent systems interact with real-world environments in robust, interpretable, and humanaligned ways.</li>
</ol>
<p>Conclusion</p>
<p>In this comprehensive review, we systematically evaluated the recent developments, methodologies, and applications of VLA models published over the last three years.Our analysis began with the foundational concepts of VLAs, defining their role as multimodal systems that unify visual perception, natural language understanding, and action generation in physical or simulated environments.We traced their evolution and timeline, detailing key milestones that marked the transition from isolated perception-action modules to fully unified, instructionfollowing robotic agents.We highlighted how multimodal integration has matured-from loosely coupled pipelines to transformer-based architectures that enable seamless coordination between modalities.</p>
<p>Next, we examined tokenization and representation techniques, focusing on how VLAs encode visual and linguistic information, including action primitives and spatial semantics.We explored learning paradigms, detailing the datasets and training strategies-from supervised learning and imitation learning to reinforcement learning and multimodal pretraining-that have shaped VLA performance.In our section on adaptive control and real-time execution, we addressed how modern VLAs are optimized for dynamic environments, discussing policies that support latency-sensitive tasks.We then categorized major architectural innovations, surveying over 50 recent VLA models.This included advancements in model design, memory systems, and interaction fidelity.We further studied training and efficiency strategies, including parameterefficient methods like LoRA, quantization, and model pruning, alongside acceleration techniques such as parallel decoding and hardware-aware inference.Our analysis continued with realworld applications of VLA models, showcasing their deployment across six domains: humanoid robotics, autonomous vehicles, industrial automation, healthcare, agriculture, and augmented reality (AR) navigation.Each application was reviewed with examples of model performance, domain-specific challenges, and generalizability.</p>
<p>In addressing challenges and limitations, we focused on five core areas: real-time inference, multimodal action representation and safety, bias and generalization, system integration and compute constraints, and ethical deployment.We proposed potential solutions drawn from current literature, including model compression, cross-modal grounding, domain adaptation, and agentic learning frameworks.Finally, our discussion and future roadmap articulated how the convergence of VLMs, VLA architectures, and agentic AI systems is steering robotics toward artificial general intelligence (AGI).This review provides a unified understanding of VLA advancements, identifies unresolved challenges, and outlines a structured path forward for developing intelligent, embodied, and human-aligned agents.Componentized VLA with specialized action module using diffusion transformers Industrial robotics, languageguided manipulation Robust action modeling, rapid adaptation, strong generalization, much higher task success rates VLATest [182] &amp; 2024 Automated framework for large-scale VLA model testing in manipulation Robotic manipulation: benchmarking VLA robustness and reliability Diverse scene generation, multi-model/task evaluation, reveals robustness gaps, guides VLA improvement NaVILA [32] &amp; 2024 Two-level VLA: high-level vision-language generates mid-level nav commands, RL locomotion executes Legged robot navigation via natural language in cluttered, realworld scenes Modular mid/low-level split, strong generalization, 88% real-world success, robust to diverse terrains RoboNurse-VLA [103] &amp; 2024 SAM 2 vision, Llama 2 language, real-time voiceto-action pipeline Surgical assistance: precise instrument grasp and handover in OR Accurate, real-time handover, robust to unseen tools, excels at complex, dynamic surgical tasks</p>
<p>Real-Time Inference Constraints</p>
<p>Adopt parallel decoding, quantized transformers, and hardware acceleration <a href="e.g., Ten-sorRT">100,94</a>; minimize autoregressive overhead [60,110] Supports real-time robotic control and broader deployment in time-sensitive domains [194,150] (e.g., drones, manipulators) Multimodal Action Representation Hybrid tokenization using diffusion and autoregressive policies [133]; train on diverse demonstrations and multi-modal outputs [121] Improves handling of complex, dynamic manipulation tasks with multiple viable solutions [59] Safety Assurance in Open Worlds Integrate dynamic risk assessment modules [143,180]; low-latency emergency stop circuits and adaptive planning layers [87] Ensures reliability and safety in unpredictable environments (homes, factories, healthcare settings) Dataset Bias and Grounding Curate diverse, debiased datasets [145]; apply improved grounding techniques such as CLIP finetuning with hard negatives [212,16] Enhances model fairness, semantic fidelity [85], and generalizability to novel realworld inputs [175,217,136] Limited 3D Perception and Reasoning Integrate 3D sensors (e.g., depth, LiDAR), develop 3D-aware architectures, and leverage point cloud fusion with vision-language inputs Enables more accurate spatial reasoning, manipulation, and navigation in complex real-world environments [100] Cross-Embodiment Generalization Train with diverse robot types and morphologies, use embodiment-agnostic representations, and apply cross-domain adaptation techniques [201] Facilitates transfer of policies and knowledge across different robot platforms and configurations [209,94] Annotation Complexity and Cost Employ weak supervision, active learning, and synthetic data generation to reduce reliance on extensive manual annotation [115] Lowers development costs and accelerates scaling to new tasks and domains [180,215] Sim-to-Real Transfer Gap Use domain adaptation, sim-to-real fine-tuning, and real-world calibration strategies [162,104] Improves reliability and consistency of VLA models when deployed outside simulation environments [4,53] Integration of Physical Knowledge Incorporate physics-based priors, simulation environments, and dynamics modeling into training pipelines [42] Enhances the model's ability to predict and plan actions that respect real-world physical constraints [82] Multi-Modal Integration (e.g., tactile, audio) Fuse additional sensory modalities (tactile, audio) with vision and language [88]; develop multimodal transformers Expands task repertoire and robustness to ambiguous or visually occluded scenarios [61,106,71] Handling Long-Horizon, Multi-Stage Tasks Design hierarchical policies, memory-augmented networks, and trajectory planning modules [106] Improves performance on complex, sequential tasks requiring planning and memory [102,215,175,136] System Integration Complexity Develop unified Transformer backbones [218]; incorporate temporal alignment layers and sim-toreal transfer learning strategies [127,207] Enables seamless planning-control coordination and robust transfer to physical robots [147,160] Energy and Compute Demands Apply model pruning, LoRA adapters, quantization-aware training, and deployment on low-power accelerators Facilitates scalable, efficient deployment of VLAs in embedded and mobile platforms [195,213,96,190] Generalization to Unseen Tasks Use compositional generalization, few-shot metalearning, and task-agnostic pretraining pipelines [135,113] Reduces task-specific overfitting, enabling robust zero-and few-shot adaptation [75,186,215] Robustness to Environmental Variability Use domain randomization, sensor fusion, and real-time recalibration of perception-action pipelines [121] Enhances performance in changing or cluttered environments with minimal degradation [214,186] Ethical and Societal Implications Enforce privacy via on-device processing and anonymization [116,154,195,29]; audit model fairness; build regulatory frameworks for trust Promotes equitable and trustworthy VLA adoption across social, medical, and labor domains [124,140,166,134]</p>
<p>1 Introduction 2 2 10 3 21 4
121021
Concepts of Vision-Language-Action Models 3 2.1 Evolution and Timeline . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 Multimodal Integration: From Isolated Pipelines to Unified Agents . . . . 4 2.3 Tokenization and Representation: How VLAs Encode the World . . . . . 5 2.4 Learning Paradigms: Data Sources and Training Strategies . . . . . . . .8 2.5 Adaptive Control and Real-Time Execution . . . . . . . . . . . . . . . .Progress in Vision-Language-Action Models 10 3.1 Architectural Innovations in VLA Models . . . . . . . . . . . . . . . . . 10 3.2 Training and Efficiency Advancements in Vision-Language-Action Models 15 3.3 Parameter-Efficient Methods and Acceleration Techniques in VLA Models 15 3.4 Applications of Vision-Language-Action Models . . . . . . . . . . . . .16 3.4.1 Humanoid Robotics . . . . . . . . . . . . . . . . . . . . . . . .17 3.4.2Autonomous Vehicle Systems . . . . . . . . . . . . . . . . . . .18 3.4.3Industrial Robotics . . . . . . . . . . . . . . . . . . . . . . . . .19 3.4.4Healthcare and Medical Robotics . . . . . . . . . . . . . . . . .20 3.4.5Precision and Automated Agriculture . . . . . . . . . . . . . . .21 3.4.6Interactive AR Navigation with Vision-Language-Action Models Challenges and Limitations of Vision-Language-Action Models 22 4.1 Real-Time Inference Constraints . . . . . . . . . . . . . . . . . . . . . .23 4.2 Multimodal Action Representation and Safety Assurance . . . . . . . . .23 4.3 Dataset Bias, Grounding, and Generalization to Unseen Tasks . . . . . .24 4.4 System Integration Complexity and Computational Demands . . . . . . .24 4.5 Robustness and Ethical Challenges in VLA Deployment . . . . . . . . . 25 5 Discussion 25 5.1 Potential Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.2 Future Roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .27 6 Conclusion 28 * Ranjan Sapkota Email address: rs2672@cornell.edu(Manoj Karkee)</p>
<p>Figure 2 :
2
Figure 2: Mindmap for VLA Concepts.This diagram outlines the foundational components of Vision-Language-Action models, including their definitions, historical development, integration of multimodal signals, tokenization techniques, and adaptive execution.It sets the conceptual stage for understanding the structure and purpose of VLAs.</p>
<p>Figure 5 :
5
Figure 5: Foundational Concept of VLA Models (in an Apple-Picking Scenario) This illustration depicts a robotic arm autonomously picking a ripe apple in an orchard, guided by a VLA model.On the right, a flowchart outlines the four key stages of VLA models: Multimodal Integration, Tokenization and Representation, Learning Paradigms, and Adaptive Control and Real-Time Execution.</p>
<p>Figure 7 :Figure 8 :
78
Figure 7: The diagram illustrates the end-to-end tokenization and representation process in VLA models.Visual input (e.g., cluttered tabletop) is encoded by a vision encoder (e.g., ViT), while natural language instructions (e.g., "stack the green blocks") are processed by a language encoder (e.g., T5).The system fuses prefix, state, and action tokens through a transformer and autoregressively predicts motor actions.</p>
<p>Figure 10 :
10
Figure 10: Learning Paradigms: Data Sources and Training Strategies for VLAs.</p>
<p>2</p>
<p>https://www.figure.ai/news/helix</p>
<p>Figure 12 :
12
Figure12: This figure illustrates "Helix," a next-generation humanoid robot executing a household task using a VLA framework.Upon receiving a verbal command, Helix integrates a vision-language model (e.g., SigLIP) and a language model (e.g., LLaMA-2) to jointly perceive and interpret the environment.A hierarchical VLA controller plans and executes sub-tasks-opening the fridge, grasping a bottle-while an agentic AI module adapts actions in real time.This demonstrates VLA-based generalist robotics with dynamic task adaptation and safe, semantically grounded manipulation.</p>
<p>Figure 15 :
15
Figure15: This diagram illustrates the application of VLA models in precision and automated agriculture.A ground robot uses vision encoders to detect ripe fruits and interprets instructions such as "pick only Grade A fruits" through language encoders.Action tokens then guide robotic manipulators for efficient, damage-free picking.Drones leverage VLA models to analyze aerial imagery and verbal commands for targeted irrigation.Synthetic training environments and LoRA-based adaptation enable models to generalize across crop types, environmental conditions, and geographies.This VLA-driven pipeline promotes sustainable agriculture by improving productivity, reducing manual labor, and enhancing decision-making through multimodal perception and control.</p>
<p>Figure 16 :
16
Figure 16: Showing how VLA models enable interactive AR navigation by fusing real-time visual perception, language understanding, and action planning.In dynamic environments such as airports, VLAs interpret user queries like "avoid stairs to Gate 22," analyze visual scenes (e.g., detecting escalators), and adjust navigational paths accordingly, supporting personalized, accessible, and context-aware mobility guidance.</p>
<p>Figure 17 :
17
Figure 17: Figure maps six core VLA challenges-real-time inference, multimodal fusion safety, dataset bias, integration complexity, compute demands, and robustness/ethics-against six targeted solutions: adaptive pruning, hybrid policy architectures, meta/transfer learning, LoRA/quantization, domain randomization, and ethical oversight.This systematic alignment clarifies pathways to robust, efficient, and safe VLA deployment across broader real-world robotic domains.</p>
<p>Figure 18 :
18
Figure18: This conceptual illustration presents "Eva," a future humanoid assistant powered by Vision-Language Models (VLMs), VLA frameworks, and agentic AI systems.VLMs enable semantic scene understanding and object affordance prediction, while VLAs translate language-grounded instructions into hierarchical motor plans.Agentic AI modules ensure adaptive learning, selfrefinement, and interactive decision-making in open-ended environments.Together, these components represent a foundational blueprint for Artificial General Intelligence (AGI) in robotics, where perception, language understanding, planning, and safe autonomous behavior converge in real-world, socially aware tasks.</p>
<p>long-context VLM for multimodal goal localization, topological graph for lowlevel navigationMultimodal instruction navigation with demonstration tours (MINT) in real-world environments High success on complex language+image tasks, robust to novel queries, leverages demonstration videos, scalable to large spaces CoVLA[5] &amp; 2025 CLIP for vision, Llama-2 for language, trajectory prediction for action Autonomous driving, dataset for VLA model training Large-scale, richly annotated dataset; enables interpretable scene understanding and robust path planning OpenDriveVLA[220] &amp; 2025 Hierarchical alignment of 2D/3D visual tokens and language embeddings; autoregressive agent-envego modeling End-to-end autonomous driving Unified semantic space, dynamic interaction modeling, state-of-the-art planning and QA performance ORION[56] &amp; 2025QT-Former for history context, LLM for reasoning, generative planner for trajectory predictionHolistic E2E autonomous drivingAligns reasoning and action spaces, unified optimization for VQA and planning, superior closed-loop results QUAR-VLA[43] &amp; 2025 QUART model fuses vision and language for action generation Quadruped robots: navigation, manipulation, whole-body tasks Tight vision-language-action integration, fine-grained instruction alignment, strong sim-to-real generalization TinyVLA[186] &amp; 2025Compact VLA with fast multimodal backbone, diffusion policy decoder Robotic manipulation: fast, data-efficient visuomotor control No pre-training needed, faster inference, strong generalization, outperforms OpenVLA on efficiency and accuracy UAV-VLA [150] &amp; 2025 Modular VLA: GPT for goal extraction, VLM for object search, GPT for action generation Large-scale UAV mission planning from natural language and satellite imagery Efficient flight path/action plan generation, no prior training, intuitive human-UAV interaction, benchmarked performance Bi-VLA [59] &amp; 2025 Multimodal transformer links vision, language, and bimanual action modules Bimanual dexterous manipulation for household tasks Accurate code/action generation, high adaptability, strong vision-language-action integration, robust realworld performance ChatVLA [221] &amp; 2025 Phased Alignment Training, Mixture-of-Experts for vision-language-action integration Unified multimodal understanding and robot control Minimizes forgetting/interference, excels at VQA and manipulation, efficient, outperforms SOTA VLA models RoboMamba [111] &amp; 2025 Mamba-based VLA: vision encoder co-trained with SSM for reasoning and SE(3) action Robotic reasoning and manipulation, efficient pose prediction Linear-complexity inference, minimal fine-tuning, fast and accurate reasoning and manipulation, SOTA efficiency OTTER [75] &amp; 2025 Text-aware visual feature extraction with frozen pre-trained VLMs Robotic manipulation: zero-shot generalization to novel tasks Preserves semantic alignment, no VLM fine-tuning, task-relevant feature selection, strong zero-shot performance PointVLA [96] &amp; 2025 Injects 3D point cloud features into frozen pretrained VLA via modular skip-blocks Robotic manipulation: spatial reasoning, few-shot, longhorizon tasks No retraining, preserves 2D knowledge, strong 3D spatial reasoning, excels at few-shot and dynamic tasks VLA-Cache [195] &amp; 2025 Adaptive token caching with selective reuse of static visual tokens Robotic manipulation: efficient, real-time VLA inference 40-50% faster, minimal accuracy loss, dynamic layerwise token reuse, practical for real-world robots CombatVLA [29] &amp; 2025 Trains on video-action AoT sequences, integrates truncated AoT for fast inference 3D ARPGs: real-time combat understanding and tactical action 50x faster inference, outperforms all baselines, surpasses human success rate, strong tactical reasoning HybridVLA [110] &amp; 2025 Unified LLM with collaborative diffusion and autoregressive action policies Robotic manipulation: singlearm, dual-arm, diverse real/sim tasks Adaptive action ensemble, robust control, strong generalization, outperforms SOTA on complex manipulations NORA [79] &amp; 2025 3B-parameter VLA using Qwen-2.5-VL-3Bbackbone and FAST+ tokenizer Generalist embodied robotics: efficient real-world and simulated task execution Low computational overhead, strong visual reasoning, fast action decoding, outperforms larger VLA models SpatialVLA [136] &amp; 2025 Ego3D Position Encoding and Adaptive Action Grids for spatially-aware VLA Generalist robot manipulation: cross-robot, multi-task, zeroshot control 3D spatial integration, adaptive action discretization, strong generalization and transfer, open-sourced code MoLe-VLA [213] &amp; 2025 Mixture-of-Layers with dynamic layer-skipping via STAR router and CogKD Efficient robot manipulation: RLBench and real-world tasks Selective layer activation, 5.6x faster, preserves cognition, +8% mean success, brain-inspired efficiency JARVIS-VLA [101] &amp; 2025 Post-trains large VLMs with visual-language guidance and action head for keyboard/mouse control Visual games (Minecraft): 1k+ tasks, open-world, instruction following Self-supervised post-training, 40%+ over baselines, strong world knowledge, state-of-the-art generalization, open-sourced UP-VLA [209] &amp; 2025 Unified VLA with joint multi-modal understanding and future prediction objectives Embodied agents: manipulation tasks, precise spatial reasoning Captures both high-level semantics and low-level spatial dynamics, 33% better on Calvin ABC-D, excels at realworld tasks needing fine spatial control Shake-VLA [92] &amp; 2025 Modular VLA system with vision, speech-to-text, RAG, anomaly detection, and bimanual arms Bimanual robotic cocktail mixing: ingredient detection, recipe adaptation, liquid measurement 100% task success, robust in noisy/cluttered settings, accurate ingredient handling, flexible recipe adaptation, real-world deployment MoRE [214] &amp; 2025 Sparse Mixture-of-Experts (MoE) with LoRA modules, RL-based Q-function training Quadruped robots: multi-task navigation, locomotion, and manipulation Scalable RL fine-tuning on mixed-quality data, strong multi-task and OOD generalization, outperforms baselines in sim and real-world DexGraspVLA [219] &amp; 2025 Hierarchical VLA: pre-trained vision-language planner + diffusion-based low-level controller General dexterous grasping: robust across diverse objects, lighting, and backgrounds Iterative domain-invariant representation, strong zeroshot generalization, 90%+ success on unseen scenarios, consistent performance across variations DexVLA [185] &amp; 2025 Plug-in billion-param diffusion action expert, embodiment curriculum learning General robot control: singlearm, bimanual, dexterous hand, long-horizon tasks Cross-embodiment action modeling, efficient curriculum training, rapid adaptation, SOTA on complex tasks without task-specific tuning</p>
<p>Table 2 :
2
Summary of VLA models, detailing each model's name, architecture features, training dataset, and highlighting their key strengths or unique capabilities in robotics and AI tasks.
ModelArchitecture ComponentsTrainingKey Strength /(Refer-DatasetUniquenessence)CLIPort [157]â€¢ Vision Encoder: CLIP-Self-Combines semanticResNet50 + Transporter-collectedCLIP features withResNet[SC]spatial Transporternetwork for preciseâ€¢ Language Encoder: CLIP-SE(2) manipulation.GPTâ€¢ Action Decoder: LingUNetRT-1 [18]â€¢ Vision Encoder: Efficient-RT-1-Pioneering Trans-NetKitchenformer architecture[SC]with discretized ac-â€¢ Language Encoder: Uni-tions for multi-taskversal Sentence Encoderkitchen manipula-tion.â€¢ Action Decoder: Trans-formerRT-â€¢ Vision Encoder: ViT-VQA +First large VLA co-2 [224]22B/ViT-4BRT-1-finetuned on internetKitchenVQA data and robotâ€¢ Language Encoder: PaLI-data for emergentX/PaLM-Ecapabilities.â€¢ Action Decoder: Symbol-tuning</p>
<p>[133]102]ning.Structured pruning removes entire attention heads or feed-forward sublayers identified as redundant.While less explored in VLA than in pure vision or language models, early studies on Diffusion Policy demonstrate that pruning up to 20 % of ConvNet-based vision encoders yields negligible performance degradation in grasp stability[34].Similar schemes applied to transformer-based VLAs (e.g.RDT-1B) can reduce memory footprint by 25 % with under 2 % drop in task success, paving the way for sub-4 GB deployments[112,102]. 4. Compressed Action Tokenization (FAST).FAST reformulates continuous action outputs as frequency-domain tokens, compressing long control sequences into concise descriptors.The Pi-0 Fast variant achieved 15Ã— faster inference with a 300 M-parameter diffusion head by tokenizing 1000 ms action windows into 16 discrete tokens, enabling 200 Hz policy rates on desktop GPUs[133].This approach trades minimal trajectory granularity for large speedups, suited for high-frequency control in dynamic tasks like bimanual assembly.5. Parallel Decoding and Action Chunking.Autoregressive VLAs traditionally decode actions token by token, incurring sequential latency.Parallel decoding architectures
1. Low-Rank Adaptation (LoRA). LoRA injects smalltrainable rank-decomposition matrices into frozen trans-former layers, enabling fine-tuning of billion-parameterVLAs with only a few million additional weights. InOpenVLA, LoRA adapters (20 M parameters) tuned a 7B-parameter backbone on commodity GPUs in under 24h, cutting GPU compute by 70 % compared to full back-propagation [72, 94]. Crucially, LoRA-adapted models re-tain their high-level language grounding and visual rea-soning capabilities while adapting to new robotic manipu-lation tasks (e.g. novel object shapes), making large VLAsaccessible to labs without supercomputing resources.2. Quantization. Reducing weight precision to 8-bit integers(INT8) shrinks model size by half and doubles on-chipthroughput. OpenVLA experiments show that INT8 quan-tization on Jetson Orin maintains 97 % of full-precisiontask success across pick-and-place benchmarks, with onlya 5 % drop in fine-grained dexterity tasks [152, 94]. Com-
[86]161]ry methods such as post-training quantization with per-channel calibration further minimize accuracy loss in high-dynamic-range sensor inputs[128].These optimizations allow continuous control loops at 30 Hz on 50 W edge modules.3.(e.g. in Groot N1) decode groups of spatial-temporal tokens concurrently, achieving a 2.5Ã— reduction in end-toend latency on 7-DoF arms at 100 Hz, with less than 3 mm positional error increase[13,161].Action chunking further abstracts multi-step routines into single tokens (e.g."pick-and-place-cup"), cutting inference steps by up to 40 % in long-horizon tasks like kitchen workflows[86].6. Reinforcement Learning-Supervised Hybrid Training.</p>
<p>Table 3 :
3
Comparison of VLA methodologies, application areas, and innovations.This comprehensive table compares cutting-edge VLA models by summarizing their methodologies, application domains, and key innovations.
Reference&amp;VLA MethodologyVLA Application AreaStrength and Key InnovationYearCogACT [102] &amp;2024</p>
<p>Table 4 :
4
Challenges, Potential Solutions, and Expected Impact of VLA Models
Challenge / Limita-Potential SolutionExpected Impacttion
AcknowledgementThis work was supported by the National Science Foundation and the United States Department of Agriculture, National Institute of Food and Agriculture through the "Artificial Intelligence (AI) Institute for Agriculture" Program under Award AWD003473, and AWD004595, Accession Number 1029004, "Robotic Blossom Thinning with Soft Manipulators".DeclarationsThe authors declare no conflicts of interest.Statement on AI Writing AssistanceChatGPT and Perplexity were utilized to enhance grammatical accuracy and refine sentence structure; all AI-generated revisions were thoroughly reviewed and edited for relevance.Additionally, ChatGPT-4o was employed to generate realistic visualizations.Appendix TableThe following appendix tables present a comprehensive overview of recent developments and challenges in VLA models.Table3systematically compares state-of-the-art VLA methodologies, their application domains, and key innovations across robotics, autonomous systems, and embodied AI platforms.This comparative summary highlights core architectural advances, deployment contexts, and technical contributions-providing valuable insight into the evolving landscape of generalist and task-specific VLA models.Meanwhile, Table4presents a structured synthesis of the major technical and practical challenges facing VLA model deployment, alongside potential solutions and their expected impact.This includes limitations such as real-time inference constraints, multimodal integration issues, and ethical concerns, with proposed resolutions ranging from architectural innovations to scalable training techniques and cross-modal alignment strategies.Together, these tables serve as a detailed reference for researchers, developers, and practitioners aiming to understand both the current capabilities and outstanding barriers in VLA-based intelligent systems.
J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>From methods to datasets: A survey on image-caption generators. L Agarwal, B Verma, Multimedia Tools and Applications. 832024</p>
<p>Flamingo: a visual language model for few-shot learning. J B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, Advances in neural information processing systems. 202235</p>
<p>Sim-to-real transfer for vision-and-language navigation. P Anderson, A Shrivastava, J Truong, A Majumdar, D Parikh, D Batra, S Lee, Conference on Robot Learning, PMLR. 2021</p>
<p>Covla: Comprehensive vision-language-action dataset for autonomous driving. H Arai, K Miwa, K Sasaki, K Watanabe, Y Yamaguchi, S Aoki, I Yamamoto, 2025 IEEE/CVF Winter Conference on Applications of Computer Vision. IEEE2025</p>
<p>Rapid and automated configuration of robot manufacturing cells. S Asif, M Bueno, P Ferreira, P Anandan, Z Zhang, Y Yao, G Ragunathan, L Tinkler, M Sotoodeh-Bahraini, N Lohse, Robotics and Computer-Integrated Manufacturing. 922025. 102862</p>
<p>State-of-the-art and challenges of engineering ml-enabled software systems in the deep learning era. G Assres, G Bhandari, A Shalaginov, T M Gronli, G Ghinea, 2025ACM Computing Surveys</p>
<p>Human-robot interaction through joint robot planning with large language models. K Asuzu, H Singh, M Idrissi, Intelligent Service Robotics. 2025</p>
<p>Medvlm: Medical vision-language model for consumer devices. M Ayaz, M Khan, M Saqib, A Khelifi, M Sajjad, A Elsaddik, 2024IEEE Consumer Electronics Magazine</p>
<p>Vavim and vavam: Autonomous driving through video generative modeling. F Bartoccioni, E Ramzi, V Besnier, S Venkataramanan, T H Vu, Y Xu, L Chambon, S Gidaris, S Odabas, D Hurych, arXiv:2502.156722025arXiv preprint</p>
<p>Policy learning-based image captioning with vision transformer. N V Bathula, I Paleti, S Pagidi, S S Akkumahanthi, N T Guduru, 2024 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS). IEEE2024</p>
<p>S Belkhale, T Ding, T Xiao, P Sermanet, Q Vuong, J Tompson, Y Chebotar, D Dwibedi, D Sadigh, arXiv:2403.01823Rt-h: Action hierarchies using language. 2024arXiv preprint</p>
<p>J Bjorck, F CastaÃ±eda, N Cherniadev, X Da, R Ding, L Fan, Y Fang, D Fox, F Hu, S Huang, arXiv:2503.14734Gr00t n1: An open foundation model for generalist humanoid robots. 2025arXiv preprint</p>
<p>K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, arXiv:2410.24164Pi-0: A visionlanguage-action flow model for general robot control. 2024arXiv preprint</p>
<p>D Bolya, P Y Huang, P Sun, J H Cho, A Madotto, C Wei, T Ma, J Zhi, J Rajasegaran, H Rasheed, arXiv:2504.13181Perception encoder: The best visual embeddings are not at the output of the network. 2025arXiv preprint</p>
<p>An introduction to vision-language modeling. F Bordes, R Y Pang, A Ajay, A C Li, A Bardes, S Petryk, O MaÃ±as, Z Lin, A Mahmoud, B Jayaraman, arXiv:2405.172472024arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Edgevla: Efficient vision-language-action models. environments. P Budzianowski, W Maa, M Freed, J Mo, A Xie, V Tipnis, B Bolte, 2024203</p>
<p>Integration of action and language knowledge: A roadmap for developmental robotics. A Cangelosi, G Metta, G Sagerer, S Nolfi, C Nehaniv, K Fischer, J Tani, T Belpaeme, G Sandini, F Nori, IEEE Transactions on Autonomous Mental Development. 22010</p>
<p>Behind the scene: Revealing the secrets of pre-trained vision-and-language models. J Cao, Z Gan, Y Cheng, L Yu, Y C Chen, J Liu, Computer Vision-ECCV 2020: 16th European Conference. UKSpringer2020. August 23-28, 2020Proceedings, Part VI 16</p>
<p>L Cao, arXiv:2405.15775Ai robots and humanoid ai: Review, perspectives and directions. 2024arXiv preprint</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, ACM transactions on intelligent systems and technology. 152024</p>
<p>Mobile augmented reality survey: From where we are to where we go. D Chatzopoulos, C Bermejo, Z Huang, P Hui, Ieee Access. 52017</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. B Chen, Z Xu, S Kirmani, B Ichter, D Sadigh, L Guibas, F Xia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024a</p>
<p>Augmented reality, deep learning and vision-language query system for construction worker safety. H Chen, L Hou, S Wu, G Zhang, Y Zou, S Moon, M Bhuiyan, 2024b. 105158157</p>
<p>Human-in-the-loop robot learning for smart manufacturing: A human-centric perspective. H Chen, S Li, J Fan, A Duan, C Yang, D Navarro-Alarcon, P Zheng, IEEE Transactions on Automation Science and Engineering. 2025a</p>
<p>H Chen, B Liu, S Wang, X Wang, W Han, Y Zhu, X Wang, Y Bi, arXiv:2501.13628Language modulates vision: Evidence from neural networks and human brain-lesion models. 2025barXiv preprint</p>
<p>Combatvla: An efficient visionlanguage-action model for combat tasks in 3d action role-playing games. P Chen, P Bu, Y Wang, X Wang, Z Wang, J Guo, Y Zhao, Q Zhu, J Song, S Yang, arXiv:2503.095272025carXiv preprint</p>
<p>Vision-semantics-label: A new two-step paradigm for action recognition with large language model. X Chen, W Xu, S Kan, L Zhang, Y Jin, Y Cen, Y Li, 2025d</p>
<p>Y Chen, S Tian, S Liu, Y Zhou, H Li, D Zhao, arXiv:2502.05450Conrft: A reinforced fine-tuning method for vla models via consistency policy. 2025earXiv preprint</p>
<p>A C Cheng, Y Ji, Z Yang, Z Gongye, X Zou, J Kautz, E BÄ±yÄ±k, H Yin, S Liu, X Wang, arXiv:2412.04453Navila: Legged robot visionlanguage-action model for navigation. 2024aarXiv preprint</p>
<p>Manipulation facing threats: Evaluating physical vulnerabilities in end-to-end vision language action models. H Cheng, E Xiao, C Yu, Z Yao, J Cao, Q Zhang, J Wang, M Sun, K Xu, J Gu, arXiv:2409.131742024barXiv preprint</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. C Chi, Z Xu, S Feng, E Cousineau, Y Du, B Burchfiel, R Tedrake, S Song, The International Journal of Robotics Research. 027836492412736682023</p>
<p>Mobility vla: Multimodal instruction navigation with long-context vlms and topological graphs. H T L Chiang, Z Xu, Z Fu, M G Jacob, T Zhang, T W E Lee, W Yu, C Schenck, D Rendleman, D Shah, arXiv:2407.077752024arXiv preprint</p>
<p>Ecbench: Can multi-modal foundation models understand the egocentric world? a holistic embodied cognition benchmark. R Dang, Y Yuan, W Zhang, Y Xin, B Zhang, L Li, L Wang, Q Zeng, X Li, L Bing, arXiv:2501.050312025arXiv preprint</p>
<p>S Dasari, F Ebert, S Tian, S Nair, B Bucher, K Schmeckpeper, S Singh, S Levine, C Finn, arXiv:1910.11215Robonet: Large-scale multi-robot learning. 2019arXiv preprint</p>
<p>Graspvla: a grasping foundation model pre-trained on billion-scale synthetic action data. S Deng, M Yan, S Wei, H Ma, Y Yang, J Chen, Z Zhang, T Yang, X Zhang, H Cui, Z Zhang, H Wang, arXiv:2505.032332025</p>
<p>Revla: Reverting visual domain limitation of robotic foundation models. S Dey, J N Zaech, N Nikolov, L Van Gool, D P Paudel, arXiv:2409.152502024arXiv preprint</p>
<p>Visual question answering in robotic surgery: A comprehensive review. D Ding, T Yao, R Luo, X Sun, 2025aIEEE Access</p>
<p>Understanding world or predicting future? a comprehensive survey of world models. J Ding, Y Zhang, Y Shang, Y Zhang, Z Zong, J Feng, Y Yuan, H Su, N Li, N Sukiennik, arXiv:2411.144992024aarXiv preprint</p>
<p>P Ding, J Ma, X Tong, B Zou, X Luo, Y Fan, T Wang, H Lu, P Mo, J Liu, arXiv:2502.14795Humanoid-vla: Towards universal humanoid control with visual integration. 2025barXiv preprint</p>
<p>Quar-vla: Vision-language-action model for quadruped robots. P Ding, H Zhao, W Zhang, W Song, M Zhang, S Huang, N Yang, D Wang, European Conference on Computer Vision. Springer2024b</p>
<p>Long-term recurrent convolutional networks for visual recognition and description. J Donahue, Anne Hendricks, L Guadarrama, S Rohrbach, M Venugopalan, S Saenko, K Darrell, T , Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>Advances in multimodal adaptation and generalization: From traditional approaches to foundation models. H Dong, M Liu, K Zhou, E Chatzi, J Kannala, C Stachniss, O Fink, arXiv:2501.185922025arXiv preprint</p>
<p>Teaching structured vision &amp; language concepts to vision &amp; language models. S Doveh, A Arbelle, S Harary, E Schwartz, R Herzig, R Giryes, R Feris, R Panda, S Ullman, L Karlinsky, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, A Wahid, J Tompson, Q Vuong, T Yu, W Huang, 2023Openreview</p>
<p>Aha: A visionlanguage-model for detecting and reasoning over failures in robotic manipulation. J Duan, W Pumacay, N Kumar, Y R Wang, S Tian, W Yuan, R Krishna, D Fox, A Mandlekar, Y Guo, arXiv:2410.003712024arXiv preprint</p>
<p>Action anticipation: Reading the intentions of humans and robots. N F Duarte, M RakoviÄ‡, J Tasevski, M I Coco, A Billard, J Santos-Victor, IEEE Robotics and Automation Letters. 32018</p>
<p>Bridge data: Boosting generalization of robotic skills with cross-domain datasets. F Ebert, Y Yang, K Schmeckpeper, B Bucher, G Georgakis, K Daniilidis, C Finn, S Levine, arXiv:2109.133962021arXiv preprint</p>
<p>Interleave-vla: Enhancing robot manipulation with interleaved image-text instructions. C Fan, X Jia, Y Sun, Y Wang, J Wei, Z Gong, X Zhao, M Tomizuka, X Yang, J Yan, M Ding, arXiv:2505.021522025</p>
<p>Language reasoning in vision-language-action model for robotic grasping. L Fan, K Chen, Z Xu, M Yuan, P Huang, W Huang, 20242024 China Automation Congress (CAC), IEEE</p>
<p>Rebot: Scaling robot learning with real-to-sim-to-real robotic video synthesis. Y Fang, Y Yang, X Zhu, K Zheng, G Bertasius, D Szafir, M Ding, arXiv:2503.145262025arXiv preprint</p>
<p>Foundation models in robotics: Applications, challenges, and the future. R Firoozi, J Tucker, S Tian, A Majumdar, J Sun, W Liu, Y Zhu, S Song, A Kapoor, K Hausman, The International Journal of Robotics Research. 2023. 02783649241281508</p>
<p>Is behavior cloning all you need? understanding horizon in imitation learning. D J Foster, A Block, D Misra, arXiv:2407.150072024arXiv preprint</p>
<p>Orion: A holistic end-to-end autonomous driving framework by vision-language instructed action generation. H Fu, D Zhang, Z Zhao, J Cui, D Liang, C Zhang, D Zhang, H Xie, B Wang, X Bai, arXiv:2503.197552025arXiv preprint</p>
<p>A vision-language model for predicting potential distribution land of soybean double cropping. B Gao, Y Liu, Y Li, H Li, M Li, W He, Frontiers in Environmental Science. 1215157522025a</p>
<p>J Gao, S Belkhale, S Dasari, A Balakrishna, D Shah, D Sadigh, arXiv:2503.01238A taxonomy for evaluating generalist robot policies. 2025barXiv preprint</p>
<p>Bi-vla: Vision-language-action model-based system for bimanual robotic dexterous manipulations. K F Gbagbe, SMCM A Cabrera, SMCA Alabbas, SMCO Alyunes, SMCA Lykov, SMCD Tsetserukou, SMC2024 IEEE International Conference on Systems, Man, and Cybernetics. IEEE2024</p>
<p>Bringing generative ai to edge devices through interoperable compute cores. R Geens, Flanders AI Research Day2024GhentLocation</p>
<p>Exploring the frontier of vision-language models: A survey of current methodologies and future directions. A Ghosh, A Acharya, S Saha, V Jain, A Chadha, arXiv:2404.072142024arXiv preprint</p>
<p>Recent advances in convolutional neural networks. J Gu, Z Wang, J Kuen, L Ma, A Shahroudy, B Shuai, T Liu, X Wang, G Wang, J Cai, Pattern recognition. 772018</p>
<p>Seer: Language instructed video prediction with latent diffusion models. X Gu, C Wen, W Ye, J Song, Y Gao, arXiv:2303.148972023arXiv preprint</p>
<p>Humanoid locomotion and manipulation: Current progress and challenges in control, planning, and learning. Z Gu, J Li, W Shen, W Yu, Z Xie, S Mccrory, X Cheng, A Shamsah, R Griffin, C K Liu, arXiv:2501.021162025arXiv preprint</p>
<p>Improving vision-language-action model with online reinforcement learning. Y Guo, J Zhang, X Chen, X Ji, Y J Wang, Y Hu, J Chen, arXiv:2501.166642025arXiv preprint</p>
<p>Benchmarking vision, language, &amp; action models on robotic learning tasks. P Guruprasad, H Sikka, J Song, Y Wang, P P Liang, arXiv:2411.058212024arXiv preprint</p>
<p>Baku: An efficient transformer for multi-task policy learning. S Haldar, Z Peng, L Pinto, arXiv:2406.075392024arXiv preprint</p>
<p>A review of large language models: Fundamental architectures, key technological evolutions, interdisciplinary technologies integration, optimization and compression techniques, applications, and challenges. S Han, M Wang, J Zhang, D Li, J Duan, 2024135040</p>
<p>The visions image-understanding system. A Hanson, E Riseman, Advances in Computer Vision. Psychology Press2014</p>
<p>P Hao, C Zhang, D Li, X Cao, X Hao, S Cui, S Wang, arXiv:2503.08548Tla: Tactile-language-action model for contact-rich manipulation. 2025arXiv preprint</p>
<p>Building 3D Foundation Models for the Embodied Minds. Y Hong, 2025Los AngelesUniversity of CaliforniaPh.D. thesis</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, 202213</p>
<p>Vision-based multimodal interfaces: A survey and taxonomy for enhanced context-aware system design. Y Hu, J Tang, X Gong, Z Zhou, S Zhang, D S Elvitigala, F Mueller, W Hu, A J Quigley, arXiv:2501.134432025arXiv preprint</p>
<p>Early fusion helps vision language action models generalize better. H Huang, F Liu, L Fu, T Wu, M Mukadam, J Malik, K Goldberg, P Abbeel, 20241st Workshop on X-Embodiment Robot Learning</p>
<p>Otter: A vision-language-action model with textaware visual feature extraction. H Huang, F Liu, L Fu, T Wu, M Mukadam, J Malik, K Goldberg, P Abbeel, arXiv:2503.037342025aarXiv preprint</p>
<p>Language is not all you need: Aligning perception with language models. S Huang, L Dong, W Wang, Y Hao, S Singhal, S Ma, T Lv, L Cui, O K Mohammed, B Patra, Advances in Neural Information Processing Systems. 362023a</p>
<p>Decision spikeformer: Spike-driven transformer for decision making. W Huang, Q Gu, N Ye, arXiv:2504.038002025barXiv preprint</p>
<p>W Huang, C Wang, R Zhang, Y Li, J Wu, L Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023barXiv preprint</p>
<p>Nora: A small open-sourced generalist vision language action model for embodied tasks. C Y Hung, Q Sun, P Hong, A Zadeh, C Li, U Tan, N Majumder, S Poria, arXiv:2504.198542025arXiv preprint</p>
<p>Marcer: Multimodal augmented reality for composing and executing robot tasks. B Ikeda, M Gramopadhye, L Nekervis, D Szafir, 20th ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE2025. 2025</p>
<p>Foundation models in robotics. A Imran, K Gopalakrishnan, AI for Robotics: Toward Embodied and General Intelligence in the Physical World. Springer2025</p>
<p>5: a vision-language-action model with open-world generalization. P Intelligence, K Black, N Brown, J Darpinian, K Dhabalia, D Driess, A Esmail, M Equi, C Finn, N Fusai, arXiv:2504.1605420250arXiv preprint</p>
<p>A survey of robot intelligence with large language models. H Jeong, H Lee, C Kim, S Shin, Applied Sciences. 1488682024</p>
<p>A comprehensive review on automation in agriculture using artificial intelligence. K Jha, A Doshi, P Patel, M Shah, Artificial Intelligence in Agriculture. 22019</p>
<p>Solami: Social vision-language-action modeling for immersive interaction with 3d autonomous characters. J Jiang, W Xiao, Z Lin, H Zhang, T Ren, Y Gao, Z Lin, Z Cai, L Yang, Z Liu, arXiv:2412.001742024arXiv preprint</p>
<p>Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, L Fan, arXiv:2210.030942Vima: General robot manipulation with multimodal prompts. 20226arXiv preprint</p>
<p>Y Jiang, R Zhang, J Wong, C Wang, Y Ze, H Yin, C Gokmen, S Song, J Wu, L Fei-Fei, arXiv:2503.05652Behavior robot suite: Streamlining real-world whole-body manipulation for everyday household activities. 2025arXiv preprint</p>
<p>Beyond sight: Finetuning generalist robot policies with heterogeneous sensors via language grounding. J Jones, O Mees, C Sferrazza, K Stachowicz, P Abbeel, S Levine, arXiv:2501.046932025arXiv preprint</p>
<p>Learning visually guided latent actions for assistive teleoperation. S Karamcheti, A J Zhai, D P Losey, D Sadigh, 2021</p>
<p>A Model-Driven Framework for Domain-Specific Adaptation of Time Series Forecasting Pipeline. N Katiyar, 2023McGill University (Canada)</p>
<p>Visiongpt: Vision-language understanding agent using generalized multimodal framework. C Kelly, L Hu, B Yang, Y Tian, D Yang, C Yang, Z Huang, Z Li, J Hu, Y Zou, arXiv:2403.090272024arXiv preprint</p>
<p>M H Khan, S Asfaw, D Iarchuk, M A Cabrera, L Moreno, I Tokmurziyev, D Tsetserukou, arXiv:2501.06919Shake-vla: Vision-language-action model-based system for bimanual robotic manipulations and liquid mixing. 2025arXiv preprint</p>
<p>Fine-tuning vision-languageaction models: Optimizing speed and success. M J Kim, C Finn, P Liang, arXiv:2502.196452025arXiv preprint</p>
<p>Openvla: An open-source vision-language-action model. M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>N Lee, Y Bang, H Lovenia, S Cahyawijaya, W Dai, P Fung, arXiv:2309.14381Survey of social bias in vision-language models. 2023arXiv preprint</p>
<p>C Li, J Wen, Y Peng, Y Peng, F Feng, Y Zhu, arXiv:2503.07511Pointvla: Injecting the 3d world into vision-language-action models. 2025aarXiv preprint</p>
<p>What foundation models can bring for robot learning in manipulation: A survey. D Li, Y Jin, Y Sun, H Yu, J Shi, X Hao, P Hao, H Liu, F Sun, J Zhang, arXiv:2404.182012024aarXiv preprint</p>
<p>J Li, G Skinner, G Yang, B R Quaranto, S D Schwaitzberg, P C Kim, J Xiong, arXiv:2408.07981Llava-surg: towards multimodal surgical assistant via structured surgical video learning. 2024barXiv preprint</p>
<p>Intentqa: Context-aware video intent reasoning. J Li, P Wei, W Han, L Fan, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Improving vision-language-action models via chain-of-affordance. J Li, Y Zhu, Z Tang, J Wen, M Zhu, X Liu, C Li, R Cheng, Y Peng, F Feng, arXiv:2412.204512024carXiv preprint</p>
<p>M Li, Z Wang, K He, X Ma, Y Liang, arXiv:2503.16365Jarvis-vla: Posttraining large-scale vision language models to play visual games with keyboards and mouse. 2025barXiv preprint</p>
<p>Cogact: A foundational visionlanguage-action model for synergizing cognition and action in robotic manipulation. Q Li, Y Liang, Z Wang, L Luo, X Chen, M Liao, F Wei, Y Deng, S Xu, Y Zhang, arXiv:2411.196502024darXiv preprint</p>
<p>S Li, J Wang, R Dai, W Ma, W Y Ng, Y Hu, Z Li, arXiv:2409.19590Robonurse-vla: Robotic scrub nurse system based on vision-languageaction model. 2024earXiv preprint</p>
<p>Hamster: Hierarchical action models for open-world robot manipulation. Y Li, Y Deng, J Zhang, J Jang, M Memmel, R Yu, C R Garrett, F Ramos, D Fox, A Li, arXiv:2502.054852025carXiv preprint</p>
<p>Y Li, Z Gong, H Li, X Huang, H Kang, G Bai, X Ma, arXiv:2505.00693Robotic visual instruction. 2025darXiv preprint</p>
<p>Y Li, Z Lai, W Bao, Z Tan, A Dao, K Sui, J Shen, D Liu, H Liu, Y Kong, arXiv:2501.02765Visual large language models for generalized and specialized applications. 2025earXiv preprint</p>
<p>Benchmark evaluations, applications, and challenges of large vision language models: A survey. Z Li, X Wu, H Du, H Nghiem, G Shi, arXiv:2501.0218912025farXiv preprint</p>
<p>Showui: One vision-language-action model for gui visual agent. K Q Lin, L Li, D Gao, Z Yang, S Wu, Z Bai, W Lei, L Wang, M Z Shou, arXiv:2411.174652024arXiv preprint</p>
<p>Automatic sorting system for industrial robot with 3d visual perception and natural language interaction. Y Lin, H Zhou, M Chen, H Min, Measurement and Control. 522019</p>
<p>J Liu, H Chen, P An, Z Liu, R Zhang, C Gu, X Li, Z Guo, S Chen, M Liu, arXiv:2503.10631Hybridvla: Collaborative diffusion and autoregression in a unified vision-language-action model. 2025aarXiv preprint</p>
<p>Robomamba: Efficient vision-languageaction model for robotic reasoning and manipulation. J Liu, M Liu, Z Wang, P An, X Li, K Zhou, S Yang, R Zhang, Y Guo, S Zhang, Advances in Neural Information Processing Systems. 2024a37</p>
<p>S Liu, L Wu, B Li, H Tan, H Chen, Z Wang, K Xu, H Su, J Zhu, arXiv:2410.07864Rdt-1b: a diffusion foundation model for bimanual manipulation. 2024barXiv preprint</p>
<p>From screens to scenes: A survey of embodied ai in healthcare. Y Liu, X Cao, T Chen, Y Jiang, J You, M Wu, X Wang, M Feng, Y Jin, J Chen, arXiv:2501.074682025barXiv preprint</p>
<p>Y Liu, X Cao, T Chen, Y Jiang, J You, M Wu, X Wang, M Feng, Y Jin, J Chen, arXiv:2501.07468A survey of embodied ai in healthcare: Techniques, applications, and opportunities. 2025carXiv preprint</p>
<p>Synthvlm: High-efficiency and high-quality synthetic data for vision language models. Z Liu, H Liang, X Huang, W Xiong, Q Yu, L Sun, C Chen, C He, B Cui, W Zhang, arXiv:2407.207562024carXiv preprint</p>
<p>Probing a vision-language-action model for symbolic states and integration into a cognitive architecture. H Lu, H Li, P S Shahani, S Herbers, M Scheutz, arXiv:2502.045582025arXiv preprint</p>
<p>Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. J Lu, C Clark, S Lee, Z Zhang, S Khosla, R Marten, D Hoiem, A Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Towards happy housework: Scenario-based experience design for a household cleaning robotic system. Y Lu, Z Liao, EAI Endorsed Transactions on Scalable Information Systems. 102023</p>
<p>Precise and dexterous robotic manipulation via human-in-the-loop reinforcement learning. J Luo, C Xu, J Wu, S Levine, arXiv:2410.218452024arXiv preprint</p>
<p>J Lyu, Z Li, X Shi, C Xu, Y Wang, H Wang, arXiv:2503.16806Dywa: Dynamics-adaptive world action model for generalizable non-prehensile manipulation. 2025arXiv preprint</p>
<p>A survey on vision-language-action models for embodied ai. Y Ma, Z Song, Y Zhuang, J Hao, I King, arXiv:2405.140932024arXiv preprint</p>
<p>Review of deep reinforcement learning-based object grasping: Techniques, open challenges, and recommendations. M Q Mohammed, K L Chung, C S Chyi, Ieee Access. 82020</p>
<p>Integrating reinforcement learning with foundation models for autonomous robotics: Methods and perspectives. A Moroncelli, V Soni, A A Shahid, M Maccarini, M Forgione, D Piga, B Spahiu, L Roveda, arXiv:2410.164112024arXiv preprint</p>
<p>Large language models for artificial general intelligence (agi): A survey of foundational principles and approaches. A Mumuni, F Mumuni, arXiv:2501.031512025arXiv preprint</p>
<p>Peria: Perceive, reason, imagine, act via holistic language and vision planning for manipulation. F Ni, J Hao, S Wu, L Kou, Y Yuan, Z Dong, J Liu, M Li, Y Zhuang, Y Zheng, Advances in Neural Information Processing Systems. 372024</p>
<p>Y Nie, L Li, Z Gan, S Wang, C Zhu, M Zeng, Z Liu, M Bansal, L Wang, arXiv:2112.04453Mlp architectures for vision-and-language modeling: An empirical study. 2021arXiv preprint</p>
<p>From abstraction to reality: Darpa's vision for robust sim-to-real autonomy. E Noorani, Z Serlin, B Price, A Velasquez, arXiv:2503.110072025arXiv preprint</p>
<p>M Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Fernandez, D Haziza, F Massa, A El-Nouby, arXiv:2304.07193Dinov2: Learning robust visual features without supervision. 2023arXiv preprint</p>
<p>Towards cognition-augmented human-centric assembly: A visual computation perspective. J Pang, P Zheng, J Fan, T Liu, Robotics and Computer-Integrated Manufacturing. 911028522025</p>
<p>Robotassisted surgery in space: pros and cons. a review from the surgeon's point of view. D Pantalone, G S Faini, F Cialdai, E Sereni, S Bacci, D Bani, M Bernini, C Pratesi, P StefÃ no, L Orzalesi, 202156npj Microgravity 7</p>
<p>Visual language integration: A survey and open challenges. S M Park, Y G Kim, Computer Science Review. 482023. 100548</p>
<p>Pretrained language models as visual planners for human assistance. D Patel, H Eghbalzadeh, N Kamra, M L Iuzzolino, U Jain, R Desai, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Fast: Efficient action tokenization for vision-language-action models. K Pertsch, K Stachowicz, B Ichter, D Driess, S Nair, Q Vuong, O Mees, C Finn, S Levine, arXiv:2501.097472025arXiv preprint</p>
<p>A Plaat, M Van Duijn, N Van Stein, M Preuss, P Van Der Putten, K J Batenburg, arXiv:2503.23037Agentic large language models, a survey. 2025arXiv preprint</p>
<p>A Polubarov, N Lyubaykin, A Derevyagin, I Zisman, D Tarasov, A Nikulin, V Kurenkov, arXiv:2501.19400Vintix: Action model via in-context reinforcement learning. 2025arXiv preprint</p>
<p>D Qu, H Song, Q Chen, Y Yao, X Ye, Y Ding, Z Wang, J Gu, B Zhao, D Wang, arXiv:2501.15830Spatialvla: Exploring spatial representations for visual-language-action model. 2025arXiv preprint</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>An Intelligent Versatile Pipeline for 6D Localization of Industrial Components in a Production Environment. P K Rawal, 2025Fraunhofer VerlagPh.D. thesis</p>
<p>Chatgpt: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. P P Ray, ternet of Things and Cyber-Physical Systems. 32023</p>
<p>Who is responsible? the data, models, users or regulations? responsible generative ai for a sustainable future. S Raza, R Qureshi, A Zahid, J Fioresi, F Sadak, M Saeed, R Sapkota, A Jain, A Zafar, M U Hassan, arXiv:2502.086502025arXiv preprint</p>
<p>S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, arXiv:2205.06175A generalist agent. 2022arXiv preprint</p>
<p>Human-robot interaction review: Challenges and solutions for modern industrial environments. D Rodriguez-Guerra, G Sorrosal, I Cabanes, C Calleja, Ieee Access. 92021</p>
<p>Integrating advanced vision-language models for context recognition in risks assessment. J Rodriguez-Juan, D Ortiz-Perez, J Garcia-Rodriguez, D TomÃ¡s, G J Nalepa, Neurocomputing. 6181291312025</p>
<p>Perception for humanoid robots. A Roychoudhury, S Khorshidi, S Agrawal, M Bennewitz, Current Robotics Reports. 42023</p>
<p>Scaling for fairness? analyzing model size, data composition, and multilinguality in vision-language bias. Z A Sahili, I Patras, M Purver, arXiv:2501.132232025arXiv preprint</p>
<p>Building vision-language models on solid foundations with masked distillation. S Sameni, K Kafle, H Tan, S Jenni, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Scalable, trainingfree visual language robotics: a modular multi-model framework for consumer-grade gpus. M Samson, B Muraccioli, F Kanehiro, 2025 IEEE/SICE International Symposium on System Integration (SII). IEEE2025</p>
<p>Object detection with multimodal large vision-language models: An in-depth review. R Sapkota, M Karkee, SSRN 52339532025</p>
<p>A review of 3d object detection with vision-language models. R Sapkota, K I Roumeliotis, R H Cheppally, M F Calero, M Karkee, arXiv:2504.187382025arXiv preprint</p>
<p>Uav-vla: Vision-language-action system for large scale aerial mission generation. O Sautenkov, Y Yaqoot, A Lykov, M A Mustafa, G Tadevosyan, A Akhmetkazy, M A Cabrera, M Martynov, S Karaf, D Tsetserukou, arXiv:2501.050142025arXiv preprint</p>
<p>S Schmidgall, J Cho, C Zakka, W Hiesinger, arXiv:2407.19305Gp-vls: A general-purpose vision language model for surgery. 2024arXiv preprint</p>
<p>Laion-5b: An open large-scale dataset for training next generation image-text models. C Schuhmann, R Beaumont, R Vencu, C Gordon, R Wightman, M Cherti, T Coombes, A Katta, C Mullis, M Wortsman, Advances in neural information processing systems. 352022</p>
<p>V Serpiva, A Lykov, A Myshlyaev, M H Khan, A A Abdulkarim, O Sautenkov, D Tsetserukou, arXiv:2503.02572Racevla: Vla-based racing drone navigation with human-like behaviour. 2025arXiv preprint</p>
<p>A Sharshar, L U Khan, W Ullah, M Guizani, arXiv:2502.07855Vision-language models for edge networks: A comprehensive survey. 2025arXiv preprint</p>
<p>Hi robot: Open-ended instruction following with hierarchical vision-language-action models. L X Shi, B Ichter, M Equi, L Ke, K Pertsch, Q Vuong, J Tanner, A Walling, H Wang, N Fusai, arXiv:2502.194172025arXiv preprint</p>
<p>Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and transfer learning. H C Shin, H R Roth, M Gao, L Lu, Z Xu, I Nogues, J Yao, D Mollura, R M Summers, IEEE transactions on medical imaging. 352016</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Conference on robot learning, PMLR. 2022</p>
<p>A review on manipulation skill acquisition through teleoperation-based learning from demonstration. W Si, N Wang, C Yang, Cognitive Computation and Systems. 32021</p>
<p>Augmented reality and gps-based resource efficient navigation system for outdoor environments: Integrating device camera, sensors, and storage. S Singh, J Singh, B Shah, S S Sehra, F Ali, 2022. 12720Sustainability 14</p>
<p>A survey on diffusion policy for robotic manipulation: Taxonomy, analysis, and future directions. M Song, X Deng, Z Zhou, J Wei, W Guan, L Nie, 2025aAuthorea Preprints</p>
<p>Accelerating vision-language-action model integrated with action chunking via parallel decoding. W Song, J Chen, P Ding, H Zhao, W Zhao, Z Zhong, Z Ge, J Ma, H Li, arXiv:2503.023102025barXiv preprint</p>
<p>Prism: Projection-based reward integration for scene-aware real-to-sim-to-real transfer with few demonstrations. H Sun, H Wang, C Ma, S Zhang, J Ye, X Chen, X Lan, arXiv:2504.205202025aarXiv preprint</p>
<p>A review of embodied grasping. J Sun, P Mao, L Kong, J Wang, Sensors. 8522025b</p>
<p>Generating text with recurrent neural networks. I Sutskever, J Martens, G E Hinton, Proceedings of the 28th international conference on machine learning (ICML-11). the 28th international conference on machine learning (ICML-11)2011</p>
<p>Grounding multimodal large language models in actions. A Szot, B Mazoure, H Agrawal, R D Hjelm, Z Kira, A Toshev, Advances in Neural Information Processing Systems. 202437</p>
<p>G R Team, S Abeyruwan, J Ainslie, J B Alayrac, M G Arenas, T Armstrong, A Balakrishna, R Baruch, M Bauza, M Blokzijl, arXiv:2503.20020Gemini robotics: Bringing ai into the physical world. 2025arXiv preprint</p>
<p>O M Team, D Ghosh, H Walke, K Pertsch, K Black, O Mees, S Dasari, J Hejna, T Kreiman, C Xu, arXiv:2405.12213Octo: An opensource generalist robot policy. 2024arXiv preprint</p>
<p>Robots that use language. S Tellex, N Gopalan, H Kress-Gazit, C Matuszek, Annual Review of Control, Robotics, and Autonomous Systems. 32020</p>
<p>Computer vision technology in agricultural automation-a review. H Tian, T Wang, Y Liu, X Qiao, Y Li, 20207Information processing in agriculture</p>
<p>Visual autoregressive modeling: Scalable image generation via next-scale prediction. K Tian, Y Jiang, Z Yuan, B Peng, L Wang, Advances in neural information processing systems. 372024</p>
<p>A comprehensive analysis of gender, racial, and prompt-induced biases in large language models. N Torres, C Ulloa, I Araya, M Ayala, S Jara, International Journal of Data Science and Analytics. 2024</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Explainable ai for industry 5.0: vision, architecture, and potential directions. C Trivedi, P Bhattacharya, V K Prasad, V Patel, A Singh, S Tanwar, R Sharma, S Aluvala, G Pau, G Sharma, IEEE Open Journal of Industry Applications. 2024</p>
<p>Perception and control with large language models in robotic manipulation. L Verbaan, 2024TU Delft Library</p>
<p>Open xembodiment: Robotic learning datasets and rt-x models. Q Vuong, S Levine, H R Walke, K Pertsch, A Singh, R Doshi, C Xu, J Luo, L Tan, D Shah, Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition@ CoRL2023. 2023</p>
<p>Surgical-lvlm: Learning to adapt large vision-language model for grounded visual question answering in robotic surgery. G Wang, L Bai, W J Nah, J Wang, Z Zhang, Z Chen, J Wu, M Islam, H Liu, H Ren, arXiv:2405.109482024aarXiv preprint</p>
<p>Non-invasive to invasive: Enhancing ffa synthesis from cfp with a benchmark dataset and a novel network. H Wang, Z Xing, W Wu, Y Yang, Q Tang, M Zhang, Y Xu, L Zhu, Proceedings of the 1st International Workshop on Multimedia Computing for Health and Medicine. the 1st International Workshop on Multimedia Computing for Health and Medicine2024b</p>
<p>Where to learn: Embodied perception learning planned by vision-language models. J Wang, D Guo, H Liu, IEEE Transactions on Cognitive and Developmental Systems. 2025a</p>
<p>Roboflamingo-plus: Fusion of depth and rgb perception with vision-language models for enhanced robotic manipulation. S Wang, arXiv:2503.195102025arXiv preprint</p>
<p>Exploring the adversarial vulnerabilities of vision-language-action models in robotics. T Wang, C Han, J C Liang, W Yang, D Liu, L X Zhang, Q Wang, J Luo, R Tang, arXiv:2411.135872024carXiv preprint</p>
<p>Multimodal chain-of-thought reasoning: A comprehensive survey. Y Wang, S Wu, Y Zhang, S Yan, Z Liu, J Luo, H Fei, arXiv:2503.126052025barXiv preprint</p>
<p>Towards testing and evaluating vision-language-action models for robotic manipulation: An empirical study. Z Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, arXiv:2409.128942024darXiv preprint</p>
<p>Occllama: An occupancy-language-action generative world model for autonomous driving. J Wei, S Yuan, P Li, Q Hu, Z Gan, W Ding, arXiv:2409.032722024arXiv preprint</p>
<p>Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression. J Wen, M Zhu, Y Zhu, Z Tang, J Li, Z Zhou, C Li, X Liu, Y Peng, C Shen, arXiv:2412.032932024arXiv preprint</p>
<p>Dexvla: Vision-language model with plug-in diffusion expert for general robot control. J Wen, Y Zhu, J Li, Z Tang, C Shen, F Feng, arXiv:2502.058552025aarXiv preprint</p>
<p>Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. J Wen, Y Zhu, J Li, M Zhu, Z Tang, K Wu, Z Xu, N Liu, R Cheng, C Shen, IEEE Robotics and Automation Letters. 2025b</p>
<p>Convnext v2: Co-designing and scaling convnets with masked autoencoders. S Woo, S Debnath, R Hu, X Chen, Z Liu, I S Kweon, S Xie, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2023</p>
<p>Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks. J Wu, M Zhong, S Xing, Z Lai, Z Liu, Z Chen, W Wang, X Zhu, L Lu, T Lu, Advances in Neural Information Processing Systems. 372024a</p>
<p>Smart: scalable multiagent real-time motion generation via next-token prediction. W Wu, X Feng, Z Gao, Y Kan, Advances in Neural Information Processing Systems. 372024b</p>
<p>Momanipvla: Transferring vision-language-action models for general mobile manipulation. Z Wu, Y Zhou, X Xu, Z Wang, H Yan, arXiv:2503.134462025arXiv preprint</p>
<p>T Y Xiang, A Q Jin, X H Zhou, M J Gui, X L Xie, S Q Liu, S Y Wang, S B Duang, S C Wang, Z Lei, arXiv:2503.04163Vla modelexpert collaboration for bi-directional manipulation learning. 2025arXiv preprint</p>
<p>J Xiong, G Liu, L Huang, C Wu, T Wu, Y Mu, Y Yao, H Shen, Z Wan, J Huang, arXiv:2411.05902Autoregressive models in vision: A survey. 2024arXiv preprint</p>
<p>Mlevlm: Improve multi-level progressive capabilities based on multimodal large language model for medical visual question answering. D Xu, Y Chen, J Wang, Y Huang, H Wang, Z Jin, H Wang, W Yue, J He, H Li, Findings of the Association for Computational Linguistics ACL 2024. 2024a</p>
<p>When embodied ai meets industry 5.0: human-centered smart manufacturing. J Xu, Q Sun, Q L Han, Y Tang, IEEE/CAA Journal of Automatica Sinica. 122025a</p>
<p>Vlacache: Towards efficient vision-language-action model via adaptive token caching in robotic manipulation. S Xu, Y Wang, C Xia, D Zhu, T Huang, C Xu, arXiv:2502.021752025barXiv preprint</p>
<p>Z Xu, K Wu, J Wen, J Li, N Liu, Z Che, J Tang, arXiv:2402.02385A survey on robotics with foundation models: toward embodied ai. 2024barXiv preprint</p>
<p>H Xue, J Ren, W Chen, G Zhang, Y Fang, G Gu, H Xu, C Lu, arXiv:2503.02881Reactive diffusion policy: Slow-fast visual-tactile policy learning for contact-rich manipulation. 2025arXiv preprint</p>
<p>R Yang, G Chen, C Wen, Y Gao, arXiv:2503.08950Fp3: A 3d foundation policy for robotic manipulation. 2025arXiv preprint</p>
<p>Attentive mask clip. Y Yang, W Huang, Y Wei, H Peng, X Jiang, H Jiang, F Wei, Y Wang, H Hu, L Qiu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023a</p>
<p>Unisim: A neural closed-loop sensor simulator. Z Yang, Y Chen, J Wang, S Manivasagam, W C Ma, A J Yang, R Urtasun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023b</p>
<p>S Ye, J Jang, B Jeon, S Joo, J Yang, B Peng, A Mandlekar, R Tan, Y W Chao, B Y Lin, arXiv:2410.11758Latent action pretraining from videos. 2024arXiv preprint</p>
<p>Deer-vla: Dynamic inference of multimodal large language models for efficient robot execution. Y Yue, Y Wang, B Kang, Y Han, S Wang, S Song, J Feng, G Huang, Advances in Neural Information Processing Systems. 202437</p>
<p>Robotic control via embodied chain-of-thought reasoning. M Zawalski, W Chen, K Pertsch, O Mees, C Finn, S Levine, arXiv:2407.086932024arXiv preprint</p>
<p>Sigmoid loss for language image pre-training. X Zhai, B Mustafa, A Kolesnikov, L Beyer, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>B Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, arXiv:2503.03480Safevla: Towards safety alignment of vision-language-action model via safe reinforcement learning. 2025aarXiv preprint</p>
<p>Gevrm: Goalexpressive video generation model for robust visual manipulation. H Zhang, P Ding, S Lyu, Y Peng, D Wang, arXiv:2502.092682025barXiv preprint</p>
<p>Slim: Sim-to-real legged instructive manipulation via long-horizon visuomotor learning. H Zhang, H Yu, L Zhao, A Choi, Q Bai, B Yang, W Xu, arXiv:2501.099052025carXiv preprint</p>
<p>H Zhang, N Zantout, P Kachana, Z Wu, J Zhang, W Wang, arXiv:2411.03540Vla-3d: A dataset for 3d semantic scene understanding and navigation. 2024aarXiv preprint</p>
<p>Upvla: A unified understanding and prediction model for embodied agent. J Zhang, Y Guo, Y Hu, X Chen, X Zhu, J Chen, arXiv:2501.188672025darXiv preprint</p>
<p>Uni-navid: A video-based vision-languageaction model for unifying embodied navigation tasks. J Zhang, K Wang, S Wang, M Li, H Liu, S Wei, Z Wang, Z Zhang, H Wang, arXiv:2412.062242024barXiv preprint</p>
<p>Learning manipulation skills through robot chain-of-thought with sparse failure guidance. K Zhang, Z H Yin, W Ye, Y Gao, arXiv:2405.135732024carXiv preprint</p>
<p>K Zhang, P Yun, J Cen, J Cai, D Zhu, H Yuan, C Zhao, T Feng, M Y Wang, Q Chen, arXiv:2503.03464Generative artificial intelligence in robotic manipulation: A survey. 2025earXiv preprint</p>
<p>Mole-vla: Dynamic layer-skipping vision language action model via mixture-of-layers for efficient robot manipulation. R Zhang, M Dong, Y Zhang, L Heng, X Chi, G Dai, L Du, D Wang, Y Du, S Zhang, arXiv:2503.203842025farXiv preprint</p>
<p>More: Unlocking scalability in reinforcement learning for quadruped vision-language-action models. H Zhao, W Song, D Wang, X Tong, P Ding, X Cheng, Z Ge, arXiv:2503.080072025aarXiv preprint</p>
<p>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. Q Zhao, Y Lu, M J Kim, Z Fu, Z Zhang, Y Wu, Z Li, Q Ma, S Han, C Finn, arXiv:2503.220202025barXiv preprint</p>
<p>Learning finegrained bimanual manipulation with low-cost hardware. T Z Zhao, V Kumar, S Levine, C Finn, arXiv:2304.137052023arXiv preprint</p>
<p>H Zhen, X Qiu, P Chen, J Yang, X Yan, Y Du, Y Hong, C Gan, arXiv:2403.096313d-vla: A 3d vision-language-action generative world model. 2024arXiv preprint</p>
<p>Universal actions for enhanced embodied foundation models. J Zheng, J Li, D Liu, Y Zheng, Z Wang, Z Ou, Y Liu, J Liu, Y Q Zhang, X Zhan, arXiv:2501.101052025arXiv preprint</p>
<p>Dexgraspvla: A vision-language-action framework towards general dexterous grasping. Y Zhong, X Huang, R Li, C Zhang, Y Liang, Y Yang, Y Chen, arXiv:2502.209002025arXiv preprint</p>
<p>X Zhou, X Han, F Yang, Y Ma, A C Knoll, arXiv:2503.23463Opendrivevla: Towards end-to-end autonomous driving with large vision language action model. 2025aarXiv preprint</p>
<p>Z Zhou, Y Zhu, M Zhu, J Wen, N Liu, Z Xu, W Meng, R Cheng, Y Peng, C Shen, arXiv:2502.14420Chatvla: Unified multimodal understanding and robot control with vision-language-action model. 2025barXiv preprint</p>
<p>Robot with humanoid hands cooks food better? effect of robotic chef anthropomorphism on food quality prediction. D H Zhu, Y P Chang, International Journal of Contemporary Hospitality Management. 322020</p>
<p>M Zhu, Y Zhu, J Li, Z Zhou, J Wen, X Liu, C Shen, Y Peng, F Feng, arXiv:2502.19250Objectvla: End-to-end open-world object manipulation without demonstration. 2025arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. B Zitkovich, T Yu, S Xu, P Xu, T Xiao, F Xia, J Wu, P Wohlhart, S Welker, A Wahid, Conference on Robot Learning, PMLR. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
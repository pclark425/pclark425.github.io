<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4258 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4258</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4258</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-273850480</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.03484v1.pdf" target="_blank">Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature</a></p>
                <p><strong>Paper Abstract:</strong> Automated knowledge extraction from scientific literature can potentially accelerate materials discovery. We have investigated an approach for extracting synthesis protocols for reticular materials from scientific literature using large language models (LLMs). To that end, we introduce a Knowledge Extraction Pipeline (KEP) that automatizes LLM-assisted paragraph classification and information extraction. By applying prompt engineering with in-context learning (ICL) to a set of open-source LLMs, we demonstrate that LLMs can retrieve chemical information from PDF documents, without the need for fine-tuning or training and at a reduced risk of hallucination. By comparing the performance of five open-source families of LLMs in both paragraph classification and information extraction tasks, we observe excellent model performance even if only few example paragraphs are included in the ICL prompts. The results show the potential of the KEP approach for reducing human annotations and data curation efforts in automated scientific knowledge extraction.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4258.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4258.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KEP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Extraction Pipeline (KEP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-independent pipeline that uses open-source LLMs (via prompt engineering and in‑context learning) to extract structured synthesis protocols (JSON) from unstructured PDF scientific articles about reticular materials.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>flan-t5-xxl-11b, flan-ul2-20b, granite-20b-code-instruct, granite-34b-code-instruct, llama-3-70b-instruct, llama-3.1-405b-instruct, mistral-large, mixtral-8x7b-instruct-v01</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B, 20B, 20B, 34B, 70B, 405B, (mistral-large size not specified), 47B (mixtral reported total param count in text)</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Knowledge Extraction Pipeline (KEP)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>KEP processes PDFs with a DS4SD extractor to obtain paragraphs, then applies two LLM-powered modules: (1) Paragraph classification — LLM instructed (ICL/few-shot) to label paragraphs as synthesis-relevant ('S') or irrelevant ('I'), with prompts containing an instruction and a small set of example paragraphs (in experiments: 5 examples); (2) Information extraction — LLM instructed (ICL/few-shot) to parse synthesis paragraphs and emit structured JSON objects encoding product description, material type, conditions (temperature, time, etc.), reactants (quantities, units), and solvents; prompts included 1–2 paragraph+JSON examples. An Examples selection phase systematically tests candidate example sets (random sampling, repeated runs) to choose the set that maximizes model performance. No model fine-tuning was performed; all extraction relies on prompt engineering, few-shot in-context learning, and constrained output formats (JSON) to reduce hallucination. Multiple runs (100 per model/experiment) and randomized example selection were used to robustly evaluate prompt sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>305 papers processed (sampled from 2,287 CC-BY candidate papers); corpus originally retrieved: 6,669 articles, filtered to 2,287 CC-BY papers</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Materials science / chemistry (reticular materials: MOFs, ZIFs, COFs, zeolites)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Procedural syntheses and empirical synthesis patterns — i.e., structured procedural knowledge and empirical generalizations about synthesis conditions, reagents, solvents, stoichiometries and workflows (practical design/recipe patterns rather than abstract mechanistic laws).</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>1) Synthesis recipe for Zn-MOF: Bis(imidazol-1-yl)methane (0.02 mmol), terephthalic acid (0.04 mmol), and Zn(NO3)2·6H2O (0.02 mmol) dissolved in DMF/EtOH/H2O (2:1:1, vol.) (1 mL) and heated at 100 °C for 24 h (structured into JSON fields: product, conditions, reactants, solvents). 2) Generic pattern: reaction temperature and time associated with solvothermal vial reactions (e.g., 100°C, 24 h) and common solvent mixtures (DMF/EtOH/H2O) as recurring procedural motifs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparison against an SME-annotated golden dataset: human experts (11 research scientists, including 2 SMEs) annotated paragraphs and produced ground-truth JSONs; experiments used cross-validation-style selection of example prompts and tested on held-out paragraphs. Performance measured by structural comparison of JSON key/value presence and by classification label agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paragraph classification: best F1 up to 0.98 (models: llama-3-70b-instruct and mistral-large achieved F1=0.98 using 5-shot prompts). Information extraction: best accuracy up to 0.96 (llama-3.1-405b-instruct on held-out JSON-annotated paragraphs using 2-shot prompts). Other reported values: several top models achieved accuracy ≥0.93; smallest tested model achieved ~0.84 accuracy on information extraction. Models exhibited wide variance between best and worst prompt-example selections (e.g., mixtral ranged from F1=0.61 to F1=1.0 in paragraph classification during example-selection experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No direct quantitative comparison to non-LLM extraction pipelines in the experiments; comparisons to prior work are qualitative in the paper (prior pipelines used heavy annotation and classical NLP/ML stacks). Within the study the approach was compared across multiple LLM families and across many prompt-example selections (best vs worst prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Off-the-shelf open-source LLMs can accurately extract complex synthesis information from PDFs without fine-tuning when guided by carefully engineered few-shot prompts (ICL). 2) Example selection (which examples to include in the prompt) strongly affects performance; different models often require different example sets for optimal results. 3) Small numbers of examples (5 for classification, 2 for JSON extraction) were sufficient to reach high performance for several large models. 4) Larger model parameter count does not universally guarantee superior performance (some large models underperformed due to hallucination or prompt constraints). 5) Constraining output to a machine-readable format (JSON) and providing examples reduces hallucination and eases downstream knowledge representation.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Hallucination (models sometimes appended explanations despite instruction to only provide labels), token/context limitations for some models (prompt+example size constraints prevented testing some models in certain configurations), sensitivity to example selection (variability across prompts), lack of semantic-level JSON comparison metrics (current evaluation was structural), scalability to process entire corpora and full workflow extraction (step-by-step workflows not yet extracted), and dependence on SME-created golden datasets for evaluation. The pipeline extracts procedural/empirical patterns but does not infer mechanistic theories or causal scientific laws beyond documented recipes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4258.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4258.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Polak2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extracting accurate materials data from research papers with conversational language models and prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work that uses conversational language models and prompt engineering to extract materials data (material names, values, units) from research papers; cited as related work in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extracting accurate materials data from research papers with conversational language models and prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>conversational language models (unspecified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Described in this paper as a pipeline using conversational LLMs and prompt engineering to extract simple entities (material, numeric value, unit) from materials science literature; characterized as focusing on simpler extraction tasks than KEP (no complex synthesis JSON outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Materials science / chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Not applicable — reported work focused on entity/value/unit extraction rather than distilling qualitative laws or principles.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as demonstrating that conversational LLMs with prompt engineering can extract material/value/unit information from papers, but targeted simpler extraction tasks than the synthesis-protocol extraction performed in KEP.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Not detailed in this paper beyond the comment that the cited work focused on simpler extraction tasks (material, value, unit) and used zero-shot methods for relevance determination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4258.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4258.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-bot (ref)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT-based bot for extracting MOF synthesis information (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced demonstration that a ChatGPT-style chatbot interface, guided by prompt engineering, can assist in extracting MOF synthesis information and answer synthesis-related questions from heterogeneous sources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatgpt chemistry assistant for text mining and the prediction of mof synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (unspecified version)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Described here as a bot-like interface leveraging prompt engineering to extract and answer questions about synthesis procedures and chemical reactions; presented as a proof-of-concept for assistant-style extraction rather than a fully automated pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Materials chemistry (MOFs)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Primarily extraction of procedural synthesis information and interactive question answering rather than formal qualitative laws or theories.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that conversational LLM-based assistants can accelerate materials discovery tasks and act as lab assistants, but typically require human-in-the-loop and are not fully automated extraction pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Characterized in the paper as not fully automated and used for interactive assistance rather than bulk automated extraction; details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Extracting accurate materials data from research papers with conversational language models and prompt engineering <em>(Rating: 2)</em></li>
                <li>Chatgpt chemistry assistant for text mining and the prediction of mof synthesis <em>(Rating: 2)</em></li>
                <li>Chatmof: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models <em>(Rating: 2)</em></li>
                <li>Comparative analysis of automatic literature review using mistral large language model and human reviewers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4258",
    "paper_id": "paper-273850480",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "KEP",
            "name_full": "Knowledge Extraction Pipeline (KEP)",
            "brief_description": "A domain-independent pipeline that uses open-source LLMs (via prompt engineering and in‑context learning) to extract structured synthesis protocols (JSON) from unstructured PDF scientific articles about reticular materials.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "flan-t5-xxl-11b, flan-ul2-20b, granite-20b-code-instruct, granite-34b-code-instruct, llama-3-70b-instruct, llama-3.1-405b-instruct, mistral-large, mixtral-8x7b-instruct-v01",
            "model_size": "11B, 20B, 20B, 34B, 70B, 405B, (mistral-large size not specified), 47B (mixtral reported total param count in text)",
            "method_name": "Knowledge Extraction Pipeline (KEP)",
            "method_description": "KEP processes PDFs with a DS4SD extractor to obtain paragraphs, then applies two LLM-powered modules: (1) Paragraph classification — LLM instructed (ICL/few-shot) to label paragraphs as synthesis-relevant ('S') or irrelevant ('I'), with prompts containing an instruction and a small set of example paragraphs (in experiments: 5 examples); (2) Information extraction — LLM instructed (ICL/few-shot) to parse synthesis paragraphs and emit structured JSON objects encoding product description, material type, conditions (temperature, time, etc.), reactants (quantities, units), and solvents; prompts included 1–2 paragraph+JSON examples. An Examples selection phase systematically tests candidate example sets (random sampling, repeated runs) to choose the set that maximizes model performance. No model fine-tuning was performed; all extraction relies on prompt engineering, few-shot in-context learning, and constrained output formats (JSON) to reduce hallucination. Multiple runs (100 per model/experiment) and randomized example selection were used to robustly evaluate prompt sensitivity.",
            "number_of_papers": "305 papers processed (sampled from 2,287 CC-BY candidate papers); corpus originally retrieved: 6,669 articles, filtered to 2,287 CC-BY papers",
            "domain_or_field": "Materials science / chemistry (reticular materials: MOFs, ZIFs, COFs, zeolites)",
            "type_of_laws_extracted": "Procedural syntheses and empirical synthesis patterns — i.e., structured procedural knowledge and empirical generalizations about synthesis conditions, reagents, solvents, stoichiometries and workflows (practical design/recipe patterns rather than abstract mechanistic laws).",
            "example_laws_extracted": "1) Synthesis recipe for Zn-MOF: Bis(imidazol-1-yl)methane (0.02 mmol), terephthalic acid (0.04 mmol), and Zn(NO3)2·6H2O (0.02 mmol) dissolved in DMF/EtOH/H2O (2:1:1, vol.) (1 mL) and heated at 100 °C for 24 h (structured into JSON fields: product, conditions, reactants, solvents). 2) Generic pattern: reaction temperature and time associated with solvothermal vial reactions (e.g., 100°C, 24 h) and common solvent mixtures (DMF/EtOH/H2O) as recurring procedural motifs.",
            "evaluation_method": "Comparison against an SME-annotated golden dataset: human experts (11 research scientists, including 2 SMEs) annotated paragraphs and produced ground-truth JSONs; experiments used cross-validation-style selection of example prompts and tested on held-out paragraphs. Performance measured by structural comparison of JSON key/value presence and by classification label agreement.",
            "performance_metrics": "Paragraph classification: best F1 up to 0.98 (models: llama-3-70b-instruct and mistral-large achieved F1=0.98 using 5-shot prompts). Information extraction: best accuracy up to 0.96 (llama-3.1-405b-instruct on held-out JSON-annotated paragraphs using 2-shot prompts). Other reported values: several top models achieved accuracy ≥0.93; smallest tested model achieved ~0.84 accuracy on information extraction. Models exhibited wide variance between best and worst prompt-example selections (e.g., mixtral ranged from F1=0.61 to F1=1.0 in paragraph classification during example-selection experiments).",
            "comparison_baseline": "No direct quantitative comparison to non-LLM extraction pipelines in the experiments; comparisons to prior work are qualitative in the paper (prior pipelines used heavy annotation and classical NLP/ML stacks). Within the study the approach was compared across multiple LLM families and across many prompt-example selections (best vs worst prompts).",
            "key_findings": "1) Off-the-shelf open-source LLMs can accurately extract complex synthesis information from PDFs without fine-tuning when guided by carefully engineered few-shot prompts (ICL). 2) Example selection (which examples to include in the prompt) strongly affects performance; different models often require different example sets for optimal results. 3) Small numbers of examples (5 for classification, 2 for JSON extraction) were sufficient to reach high performance for several large models. 4) Larger model parameter count does not universally guarantee superior performance (some large models underperformed due to hallucination or prompt constraints). 5) Constraining output to a machine-readable format (JSON) and providing examples reduces hallucination and eases downstream knowledge representation.",
            "challenges_limitations": "Hallucination (models sometimes appended explanations despite instruction to only provide labels), token/context limitations for some models (prompt+example size constraints prevented testing some models in certain configurations), sensitivity to example selection (variability across prompts), lack of semantic-level JSON comparison metrics (current evaluation was structural), scalability to process entire corpora and full workflow extraction (step-by-step workflows not yet extracted), and dependence on SME-created golden datasets for evaluation. The pipeline extracts procedural/empirical patterns but does not infer mechanistic theories or causal scientific laws beyond documented recipes.",
            "uuid": "e4258.0",
            "source_info": {
                "paper_title": "Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Polak2024",
            "name_full": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "brief_description": "A referenced work that uses conversational language models and prompt engineering to extract materials data (material names, values, units) from research papers; cited as related work in the paper.",
            "citation_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "mention_or_use": "mention",
            "model_name": "conversational language models (unspecified in this paper)",
            "model_size": null,
            "method_name": null,
            "method_description": "Described in this paper as a pipeline using conversational LLMs and prompt engineering to extract simple entities (material, numeric value, unit) from materials science literature; characterized as focusing on simpler extraction tasks than KEP (no complex synthesis JSON outputs).",
            "number_of_papers": null,
            "domain_or_field": "Materials science / chemistry",
            "type_of_laws_extracted": "Not applicable — reported work focused on entity/value/unit extraction rather than distilling qualitative laws or principles.",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Cited as demonstrating that conversational LLMs with prompt engineering can extract material/value/unit information from papers, but targeted simpler extraction tasks than the synthesis-protocol extraction performed in KEP.",
            "challenges_limitations": "Not detailed in this paper beyond the comment that the cited work focused on simpler extraction tasks (material, value, unit) and used zero-shot methods for relevance determination.",
            "uuid": "e4258.1",
            "source_info": {
                "paper_title": "Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "ChatGPT-bot (ref)",
            "name_full": "ChatGPT-based bot for extracting MOF synthesis information (referenced work)",
            "brief_description": "A referenced demonstration that a ChatGPT-style chatbot interface, guided by prompt engineering, can assist in extracting MOF synthesis information and answer synthesis-related questions from heterogeneous sources.",
            "citation_title": "Chatgpt chemistry assistant for text mining and the prediction of mof synthesis",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (unspecified version)",
            "model_size": null,
            "method_name": null,
            "method_description": "Described here as a bot-like interface leveraging prompt engineering to extract and answer questions about synthesis procedures and chemical reactions; presented as a proof-of-concept for assistant-style extraction rather than a fully automated pipeline.",
            "number_of_papers": null,
            "domain_or_field": "Materials chemistry (MOFs)",
            "type_of_laws_extracted": "Primarily extraction of procedural synthesis information and interactive question answering rather than formal qualitative laws or theories.",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Cited as evidence that conversational LLM-based assistants can accelerate materials discovery tasks and act as lab assistants, but typically require human-in-the-loop and are not fully automated extraction pipelines.",
            "challenges_limitations": "Characterized in the paper as not fully automated and used for interactive assistance rather than bulk automated extraction; details not provided in this paper.",
            "uuid": "e4258.2",
            "source_info": {
                "paper_title": "Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "rating": 2,
            "sanitized_title": "extracting_accurate_materials_data_from_research_papers_with_conversational_language_models_and_prompt_engineering"
        },
        {
            "paper_title": "Chatgpt chemistry assistant for text mining and the prediction of mof synthesis",
            "rating": 2,
            "sanitized_title": "chatgpt_chemistry_assistant_for_text_mining_and_the_prediction_of_mof_synthesis"
        },
        {
            "paper_title": "Chatmof: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models",
            "rating": 2,
            "sanitized_title": "chatmof_an_artificial_intelligence_system_for_predicting_and_generating_metalorganic_frameworks_using_large_language_models"
        },
        {
            "paper_title": "Comparative analysis of automatic literature review using mistral large language model and human reviewers",
            "rating": 1,
            "sanitized_title": "comparative_analysis_of_automatic_literature_review_using_mistral_large_language_model_and_human_reviewers"
        }
    ],
    "cost": 0.01359925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature</p>
<p>Viviane Torres Da Silva vivianet@br.ibm.com 
IBM Research</p>
<p>Krystelle Lionti klionti@us.ibm.com 
IBM Research</p>
<p>Ronaldo Giro rgiro@br.ibm.com 
IBM Research</p>
<p>Geisa Lima geisa.lima@ibm.com 
IBM Research</p>
<p>Sandro Fiorini srfiorini@ibm.com 
IBM Research</p>
<p>Marcelo Archanjo marcelo.archanjo@ibm.com 
IBM Research</p>
<p>Breno W Carvalho brenow@ibm.com 
IBM Research</p>
<p>Rodrigo Neumann rneumann@br.ibm.com 
IBM Research</p>
<p>Anaximandro Souza anaximandrosouza@ibm.com 
IBM Research</p>
<p>João Pedro Souza 
Idiap Research Institute</p>
<p>Gabriela De Valnisio gvalnisio@ibm.com 
IBM Research</p>
<p>Carmen Nilda Paz cpaz@br.ibm.com 
IBM Research</p>
<p>Renato Cerqueira 
IBM Research</p>
<p>Mathias Steiner mathiast@br.ibm.com 
IBM Research</p>
<p>Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature
EEFCCF60195A40E36BE21F34D8F4F7DA
Automated knowledge extraction from scientific literature can potentially accelerate materials discovery.We have investigated an approach for extracting synthesis protocols for reticular materials from scientific literature using large language models (LLMs).To that end, we introduce a Knowledge Extraction Pipeline (KEP) that automatizes LLM-assisted paragraph classification and information extraction.By applying prompt engineering with in-context learning (ICL) to a set of opensource LLMs, we demonstrate that LLMs can retrieve chemical information from PDF documents, without the need for fine-tuning or training and at a reduced risk of hallucination.By comparing the performance of five open-source families of LLMs in both paragraph classification and information extraction tasks, we observe excellent model performance even if only few example paragraphs are included in the ICL prompts.The results show the potential of the KEP approach for reducing human annotations and data curation efforts in automated scientific knowledge extraction.</p>
<p>Introduction</p>
<p>Reticular materials are a class of crystalline, porous materials made of molecular building blocks that are linked by strong chemical bonds [1].They exhibit exceptional properties due to their highly porous structure, high surface area, tunable pore sizes and morphologies [2].Their versatility is evidenced by a broad range of industrial applications, among them heterogeneous catalysis [3], energy 38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2411.03484v1[cond-mat.mtrl-sci]5 Nov 2024 storage [4], water treatment [5], chemical sensing [6], heat transfer [7], gas capture [8] and drug delivery [9].Following recent advances in generative AI, several models have been proposed to explore the large chemical space covered by reticular materials [10][11][12][13][14].These models aim to generate reticular structures with optimized properties.Such structures are hypothetical as they have not yet been synthesised and tested in the lab.Devising a synthesis protocol for computationally generated structures requires a subject matter expert (SME).This is, however a challenging task given the large number of possible structures.An AI model that correlates a computationally discovered material with a lab synthesis protocol is, therefore, highly desirable.A first step towards the creation of such a model is building a database of existing synthesis protocols.</p>
<p>One approach for creating such database is applying information extraction techniques to the existing body of scientific literature.A large number of reticular materials have been reported in the literature alongside their respective synthesis protocols [15,16].It is worth noting, however, that overlapping discoveries are common, given that the same material can be produced by means of different synthesis protocols [17].Transfer learning has been suggested as means to improve information extraction on existing corpora of scientific texts related to materials [18].For example, fine-tuning techniques allow for adapting existing general-purpose AI models to specific tasks in domains for which comparatively little data exists.However, recent developments in LLMs have enabled information extraction based on prompt engineering and few-shot learning tasks [19].</p>
<p>In this paper, we propose using large language models (LLMs), without the need for additional training or fine-tuning, for extracting synthesis protocols of reticular materials from scientific literature, i.e., unstructured PDF documents.We use prompt engineering with in-context learning (ICL) [20] for providing in the prompt all the context needed by the LLM to process the instructions.Together with instructions and input data, we provide examples that guide the LLM output production.This technique reduces the risk of hallucination, since all the context needed to execute the instruction is provided within the prompt.Also, it accelerates the process of information extraction because it does not require SME-based annotation of thousands of sentences/paragraphs for fine-tuning the models.</p>
<p>Our domain-independent Knowledge Extraction Pipeline (KEP) uses LLMs for extracting relevant information from PDF documents.The pipeline is composed of four main modules: (i) PDF extractor: processes the PDF to extract the text; (ii) Paragraph classification: processes the text in order to select only the relevant paragraphs (i.e., paragraphs that have the information the user is interested in); (iii) Information extraction: processes the relevant paragraphs and extract the relevant information; and (iv) Knowledge representation: interprets and assigns meaning to the information while representing the related knowledge.The pipeline uses LLMs with prompt-engineering and ICL in two modules, namely paragraph classification and information extraction, which are the focus of this paper.In addition, for identifying the best set of examples to be used in the prompts of these two modules, we propose the Examples selection phase.This phase measures the performance of the LLMs in a given task and, by using different sets of examples, identifies the set to be used for optimal LLM performance.</p>
<p>We have used five families of LLMs in both paragraph classification and information extraction modules and have compared their performance.We note that these open-source LLMs are not domain-specific and were not fine-tuned for our tasks.Our experiments indicate that: (i) even without fine-tuning or training, some of these models have achieved high performance in case ICL was used to provide examples in the prompt; (ii) the examples used in the prompt affect model performance and, hence, must be chosen carefully; and (iii) the same set of examples may lead to varying results if used in different models.Some recent papers share our work's objectives, however, they differ methodologically [19,[21][22][23].For example, Polak et al. (2024) [19] reported a pipeline for extracting information from unstructured text in the material discovery domain using language models.However, the cited work focused on simple extraction tasks, e.g., material, value and unit, while our pipeline is aimed at complex information associated with synthesis protocols that require additional classification.Unlike in our approach which is based on few-shot prompts providing examples for facilitating the information extraction, the cited work applies zero-shot methods for determining the relevance of sentences or paragraphs.Huo et al. (2019) [21] introduced a semi-supervised machine learning approach for classifying inorganic materials synthesis steps in scientific papers.The authors used the Latent Dirichlet Allocation (LDA) unsupervised topic modeling algorithm for clustering terms that are typically used in synthesis descriptions.A random forest classifier, based on annotations of hundreds of paragraphs, categorized the occurring synthesis types.This approach also used a Markov chain for modeling the sequence of steps, creating flowcharts of synthesis procedures.</p>
<p>In Kononova et al. (2019) [22], the authors generated a dataset with "codified recipes" for solid-state synthesis which was automatically extracted from scientific publications using traditional text mining and natural language processing approaches.The authors used the two-step paragraph classification approach described in Huo et al. (2019) [21] for finding paragraphs on solid-state synthesis.The extraction pipeline consisted of several algorithms (BiLSTM-CRF, Material Parser, etc.) for identifying materials related information, including synthesis steps and conditions.Compared to our method, the cited work required considerable annotation effort and employed a less straightforward extraction pipeline.We note that our method relies primarily on the LLM capabilities for text understanding, without specialized tokenizers or entity recognizers.Finally, Park et al. (2022) [23] created a four-step pipeline, with text extraction from XML/HTML or PDF files and classifying relevant paragraphs, performing named entity recognition and, a fully connected multi-layer with dropout as classifier.</p>
<p>Another promising, less related approach is using "AI chatbot agents" for assisting materials scientists in specific pipeline tasks.In reference [24], the authors used prompt engineering for guiding a ChatGPT-based bot to extract MOF synthesis information from various sources.The authors leveraged a bot-like interface for answering questions about synthesis procedures and chemical reactions.In reference [25], the authors leveraged multiple AI assistants, such as LLMs and specific ML algorithms, as lab assistants to support a human SME, enabling productivity levels similar to those of an entire research team.While the approach was not fully automated, it provided a proof-of-concept of how language models can be leveraged for accelerating materials discovery.</p>
<p>The remainder of this paper is organized as follows.Section 2 introduces the use case, Section 3 describes in details the pipeline applied to the use case and Section 4 presents our experiments.Section 5 concludes and presents some future work.</p>
<p>Use Case: Synthesis Protocols of Reticular Materials</p>
<p>With the goal of extracting knowledge about the synthesis of reticular materials, i.e., MOFs, ZIFs, COFs and zeolites, we have searched the scientific literature by using Elsevier's API 1 and downloaded full-text PDFs from the SCOPUS database. 2.Our approach is based on extracting information from PDFs, and not XMLs, since not always a XML file will be available for a given document.Notice that our extraction pipeline (see Section 3) was not created to manipulate only documents available in Elsevier, where their XML files are also provided, but to process any PDF document (including those that are images).</p>
<p>Our search employed the following keywords and wildcard terms to capture relevant references: 'MOF', 'metal organic framework', 'metal-organic framework', 'metal-organic-framework', 'COF', 'covalent organic', 'covalent-organic', 'ZIF', and 'zeolit<em> imidazol</em>'.We further limited the search to articles published in journals within Chemistry, Chemical Engineering, Materials Science, Energy, Engineering, Environmental Science, Physics and Astronomy, and Biochemistry, Genetics, and Molecular Biology, retrieving 6,669 articles.</p>
<p>The results were then filtered, by using the filter provided in the Elsevier API, to include only open-access articles with DOI identifiers from the following publishers: Elsevier (10.1016),Wiley Blackwell (10.1002),The Royal Society of Chemistry (10.1039),American Chemical Society (10.1021), Springer-Verlag (10.1007),Nature Publishing Group (10.1038), and MDPI (10.3390).</p>
<p>To create a public dataset, we finally kept only articles under the CC-BY-4.0 or CC-BY-3.0 licenses, resulting in 2,032 CC-BY-4.0 articles and 255 CC-BY-3.0 articles.These CCBY license papers were selected by performing web-scrapping from the list of DOIs provided by the Elsevier API.Since we are considering only papers with CCBY 3.0 and 4.0 licenses, everyone can retrieve the PDFs.</p>
<p>After collecting the data, we randomly selected 305 articles in PDF format 3 .We then extracted from these PDFs 188 paragraphs describing synthesis protocols, and 137 examples of paragraphs not describing synthesis protocols (a total of 325 paragraphs).This curated set of paragraphs constitutes our golden collection of classified paragraphs.For details about how those paragraphs were extracted, see Section 3.</p>
<p>Subsequently, a team of eleven research scientists (composed of 2 SMEs) annotated each of the synthesis-related paragraphs on a case-by-case basis for extracting the following information: (i) the description of the synthesis product; (ii) the equipment used as an energy source; (iii) the conditions under which the synthesis occurred (e.g., reaction time, reaction temperature, current density); and (iv) the reactants and solvents used, including their descriptions, quantities, and units of measurement.</p>
<p>Intentionally, some paragraphs were selected for annotation by multiple SMEs, leading to some inconsistencies.These inconsistencies were then used to refine the annotation guidelines.The data was reviewed on a case-by-case basis by SMEs using a custom-built graphical interface and compiled in a final set of 131 syntheses descriptions encoded in a JSON format, thereby creating our golden dataset of annotated synthesis information.Table 1 summarizes the data in our golden dataset.</p>
<p>Knowledge Extraction Pipeline (KEP)</p>
<p>KEP is a domain-independent pipeline that helps extract knowledge from unstructured data.It is composed of four main modules: PDF extractor, Paragraph classification, Information extraction and Knowledge representation, as shown in Figure 1.The PDF extractor processes the PDF to extract paragraphs, since we assume that SMEs are interested in paragraphs containing specific information.</p>
<p>The Paragraph classification classifies the extracted paragraphs into relevant or irrelevant, according to the task the SME is interested in.When applying this module to our use case, relevant paragraphs are those describing synthesis protocols of reticular materials.</p>
<p>Information extraction processes the relevant paragraphs and extracts the relevant information.When applying this module to our use case, the relevant information is the synthesis details such as the description of the synthesis product, the experimental conditions (such as reaction time and temperature), and the reagents and solvents used in the synthesis.The final module, Knowledge representation, interprets and assigns meaning to the extracted information while creates the knowledge representation.In the synthesis protocol use case, the knowledge representation is characterized by (i) the normalization of the unities; (ii) by the instantiation of entities of different kinds (such as productions, reactants and solvents), and (iii) by the instantiation of the relationships (such as used-reactant and used-solvent) that link the entities to the synthesis where they take part.For instance, it is possible to represent that the same reactant is being used in syntheses of two different products and that same product can be synthesized by two different synthesis.</p>
<p>The PDF extractor was implemented using the DS4SD open-source tool 4 that converts unstructured PDF documents into JSON files containing the document elements such as section titles, paragraphs, footnotes, headers, figure captions and tables, etc. DS4SD is also able to process PDFs that are indeed images since it uses an OCR engine to extract text-snippets from those images.</p>
<p>Paragraph classification</p>
<p>Since the goal of this module is the classification of paragraphs as relevant or irrelevant, the prompt to be used in this model should describe the difference between a relevant and an irrelevant paragraph.</p>
<p>In addition, a sentence explicitly instructing the LLM that it should not provide an explanation together with the classification may be required.</p>
<p>Since we are not using zero-shot prompting but ICL prompting, we not only provide the LLM with the aforementioned instructions, but also give it several examples of paragraphs and their corresponding classifications.In Section 4 we demonstrate that, by providing just a few examples in the prompt, the performance of the LLMs tends to increase significantly.Below is an example of instructions used, along with an example of paragraph 5 and its corresponding classification, also provided in the prompt.This paragraph was classified as "S" meaning it is a paragraph describing a synthesis protocol.</p>
<p>Instruction: You are assisting a chemist in classifying paragraphs from scientific articles.Mark the paragraph as 'S' if it describes the components of synthesis protocols for reticular materials, or 'I' if it does not include a synthesis description.After reviewing the examples, classify the given paragraph.Do not add any information or explanation besides 'S' or 'I' in the answer.</p>
<p>Example: "Synthesis of Zn-MOF: Bis(imidazole-1-yl)methane was synthesized analogously to a the procedure reported in [43].All other materials were obtained from commercial sources and were used as received.{[Zn(bim)(bdc)]0.8DMF0.4EtOH0.1H 2 O} n (Zn-MOF).Bis(imidazol-1-yl)methane (bim) (3.0 mg, 0.02 mmol), terephthalic acid (6.6 mg, 0.04 mmol), and Zn(NO3)2•6H2O (7.6 mg, 0.02 mmol) were dissolved in DMF/EtOH/H2O (2:1:1, vol.) mixture (1 mL), placed in a 4 mL screw-cap vial, and heated to 100 °C for 24 h."</p>
<p>Classification: S</p>
<p>Information extraction</p>
<p>The prompt used in the Information extraction module should inform to the LLM the kind of knowledge that should be extracted.In case of a complex structure, the prompt should suggest to the LLM to represent the extracted information following a given schema in well-known format, such as JSON [27].It is reasonable to assume that the LLM will be able to parse this format since it is a commonly used data format that appeared in several documents used to train the LLM.In order to exemplify, find below the instruction we used and the JSON annotation related to the synthesis paragraph presented in Section 3.1.</p>
<p>Instruction: You are assisting a chemist in identifying and extracting descriptions of the synthesis of reticular materials from paragraphs.For each synthesis described in a paragraph, your task is to produce a JSON object that encodes the components involved in the synthesis, following the format provided in the examples.After reviewing the examples, carefully analyze the last paragraph and create a JSON object for each synthesis you find, ensuring that it adheres to the structure and conventions demonstrated.</p>
<p>Example: "Synthesis of Zn-MOF: Bis(imidazole-1-yl)methane was synthesized analogously to a . . .screw-cap vial, and heated to 100 °C for 24 h."</p>
<p>{"output": { "product": { "description": "Zn−MOF", "material_type": "MOF", "conditions": [ {"description": "reaction temperature", "value": 100 , "unit": "oC"}, {"description": "reaction time", "value": 24, "unit": "h"} ] }, "reactants": [ {"description": "Bis(imidazol−1−yl)methane (bim)", "value": 0.02, "unit": "mmol"}, {"description": "terephthalic acid", "value": 0.04, "unit": "mmol"}, {"description": "Zn(NO3)2−6H2O", "value": 0.02, "unit": "mmol"} ], "solvents": [ {"description": "DMF/EtOH/H2O (2:1:1, vol)", "value": 1.0, "unit": "mL"} ] }}</p>
<p>Examples selection</p>
<p>It is well-known that the performance of LLMs to execute a given task is significantly influenced by the set of examples provided in the prompt.In addition, due to the different characteristics of how the LLMs were trained, it is expected that different LLMs will require different sets of examples to achieve their highest performance when executing the same task.</p>
<p>Therefore, the Examples selection step was included and associated with each KEP module that uses LLMs to help on the selection of the best set of prompt examples to be used.Examples selection receives as input the model to be tested, a golden dataset and the number of examples to be selected as examples.It randomly selects from the dataset some instances to be used as examples in the prompt, and all other instances are used to measure the performance of the model.This step is executed for all possible combinations of examples or until the user is satisfied with the performance of the model in one of the executions.The set of examples that leads the LLM to achieve the highest performance is the one selected to be used in the associated KEP module.</p>
<p>Experiments</p>
<p>This section presents the experiments we ran with 5 families of open-source LLMs.None of them were trained or fine-tuned to extract synthesis details from paragraphs or to execute any specific task in the Material Discovery domain.We selected 2 models of each family 6 , prioritized the models that have been fine-tuned using a collection of instructions (not related to our tasks) and chosen the last released ones 7 .Ultimately, the selected models were: (i) flan: flan-t5-xxl-11b, flan-ul2-20b; (ii) granite: granite-20b-code-instruct, granite-34b-code-instruct; (iii) llama: llama-3-70b-instruct, llama-3.1-405b-instruct;(iv) mistral: mistral-large; and (v) mixtral: mixtral-8x7b-instruct-v01. See the description of each model in Appendix A.</p>
<p>Examples selection</p>
<p>Paragraph classification: From the original set of 325 classified paragraphs, we reduced the golden dataset by downselecting only 50 paragraphs to demonstrate that, even when testing the prompt examples selection in a small dataset, it is possible to achieve a good performance on a majority of the tested models.In addition, the use of a small dataset helps demonstrate that the approach does not require the manual classification/annotation of thousands or hundreds of examples.</p>
<p>In the set of 50 paragraphs we ensure that 25 paragraphs are relevant (i.e., classified with "S" and mentioning synthesis protocol) and 25 are irrelevant (i.e., classified with "I" and not mentioning synthesis protocols).We fixed the number of examples to be provided in the prompt to 5, since paragraphs describing synthesis protocols are typically very large and the prompts have a limited number of tokens.Our goal is to find the best set of 5 examples used in the prompt that helps the models achieve their highest performance.The accuracy of each model was measured by using the F1 metric.</p>
<p>For each model, we executed 100 runs by providing in the prompt the instruction mentioned in Section 3.1 and 5 examples randomly selected from 50 possibilities.We tested the output with the remaining 45 paragraphs not provided in the prompt.Table 2 presents the result of our experiments.For each one of the models, the table indicates the number of paragraphs mentioning synthesis protocols ("S") and the number of irrelevant paragraphs ("I") used in both the worst and best prompt together with the F1 value for each case.The models with highest performance were flan-t5-xxl-11b, llama-3-70b-instruct, mistral-large and mixtral-8x7b-instruct-v01.Although llama-3-70b-instruct and mistral-large used the same number of relevant paragraphs and the same number of irrelevant paragraphs in their best cases, their prompts share only one paragraph (see Table 6 in Appendix B).When testing the best prompt for mistral-large in llama-3-70b-instruct by using the same 45 testing examples, the performance of the model did not achieve F1=1.0, but F1=0.98.Although it is a small difference, it demonstrate that, different LLMs may need different examples in their prompts to achieve their highest performance.The models with worst performance were flan-ul2-20b and llama-3.1-405b-instruct.Although we included in the prompt a sentence stating that the answer should only include "S" or "I", their answers often also include an explanation; which we considered to be a hallucination and, thus, an incorrect answer.</p>
<p>Information extraction:</p>
<p>The golden dataset used in this step is the 25 paragraphs mentioning synthesis protocols used in the previous step together with their coresponding JSON annotations.Different from the previous step, here we fixed the number of examples used in the prompt to 2, since the JSON annotation is being provided together with the paragraph, which significantly increases the number of tokens.Even with only 2 examples, flan-t5-xxl and flan-ul2 could not be tested since their prompt+result do not accept so many tokens 8 .</p>
<p>The experiment begun by randomly selecting 2 paragraphs+JSON to be used in the prompt for each one of the 6 models.For each model, we executed 100 runs by providing in the prompt the instructions mentioned in Section 3.2 and the 2 examples of paragraph+JSON randomly selected from 25 possibilities.We tested the performance of the model with each prompt by using the 23 paragraphs that were not provided as examples in the prompt.The results are presented in Table 3.</p>
<p>To compare the JSON annotations provided by the LLM with the JSON annotations included in the golden dataset, a structure analysis based on each JSON key (i.e., name/value pair) was defined 9 .The models that achieved the highest accuracy were llama-3-70b-instruct, llama-3.1-405b-instructand mistral-large.However, it is important to notice that all of them achieved an accuracy higher than 0.84 even using only two examples in the prompt.Similar to what happened in the previous step, the experiments illustrate the influence of the examples in the accuracy of the model (E.g.llama-3.1-405binstructworst case was 0.53 and best case was 0.94).In addition, one of the paragraphs presented in the worst case of mistral-large appeared in the best case of mixtral-8x7b-instruct-v01 (see Table 7 in Appendix B).Two related models that have the same example in opposite scenarios.Moreover, it is important to highlight that the two granites, the two llamas, and mixtral-8x7b-instruct-v01 included in their worst scenarios the same paragraph (see Table 7 in Appendix B).It may indicate that there are examples that really do not help the models on executing their tasks.</p>
<p>Paragraph classification</p>
<p>After selecting the final set of five examples that maximize the performance of each model, the paragraph classification module was tested by using the entire golden dataset of 275 paragraphs (325 minus the 50 used for prompt selection).For each model, the prompt was composed of the instructions mentioned in Section 3.1 and the best set of examples selected for that model, as presented in Section 4.1.Table 4 summarizes the results for each model in terms of Precision, Recall, and F1 achieved with the best prompt.Llama-3-70b-instruct and mistral-large achieved F1=0.98.Although llama-3.1-405b-instruct and flan-ul2-20b have more parameters than the other model of their families, their performances were worse.It occurred due the hallucination mentioned in the Example section step.Excluding granite-20b-code-instruct, all the models achieved F1&gt;0.84, which is very good accuracy given that only five examples were provided in the prompt to these models.</p>
<p>Information extraction</p>
<p>This module was tested by using the golden dataset of 106 annotated paragraphs (131 minus the 25 used for prompt selection).For each model, the prompt was composed of the instructions mentioned in Section 3.2 and the best set of examples selected for that model, as presented in Section 4.1.</p>
<p>Table 5 summarizes the results of our experiments.The model that achieved the highest accuracy (0.96) was llama-3.1-405b-instruct,which is the biggest one.Other four models also achieved a very similar and high performance (mixtral-8x7b-instruct-v01, mistral-large, llama-3-70b-instruct and granite-34b-code-instruct).Notice that the smallest model (granite-20b-code-instruct) was the one that achieved the lower performance.The high accuracy achieved by the biggest models when compared to the smallest one is expected due to the complex of the task that involves the creation of a correct JSON.When considering both the Paragraph classification and Information extraction modules, the three models with highest performance and, thus, those that should be considered to be used in KEP to process all the selected papers mention in Section 2 are: llama-3-70b-instruct, mistral-large and mixtral-8x7b-instruct-v01.</p>
<p>Table 5: Experiments for the Information extraction module (best results highlighted in bold).Model Accuracy granite-34b-code-instruct 0.93 granite-20b-code-instruct 0.84 llama-3-70b-instruct 0.93 llama-3.1-405b-instruct0.96 mistral-large 0.95 mixtral-8x7b-instruct-v01 0.94</p>
<p>Conclusions and Future Research</p>
<p>In summary, we present a knowledge extraction pipeline for synthesis protocols of reticular materials that significantly reduces SME based classification and annotation tasks related to the training or finetuning of machine learning models.Our experimental results indicate that LLMs can achieve high performance with a limited set of examples within the prompt, even without training or fine-tuning the models for the specific domain.For example, by including five representative paragraphs in the prompt, we have reproducibly achieved F1=0.98 in paragraph classification tasks.In information extraction tasks, we have used two paragraphs + JSON and llama-3.1-405b-instructfor achieving Accuracy=0.96.</p>
<p>Our results highlight the necessity of testing different examples to be used in the prompt as this variation strongly influences model performance.For instance, in the Paragraph classification module, the performance of mixtral-8x7b-instruct-v01, one of the best models in our study, ranges from F1=0.61 to F1=1.0.In addition, the experiments show that different LLMs may require different sets of examples for achieving top performance.Although both llama-3-70b-instruct and mistral-large included four synthesis paragraphs and one irrelevant paragraph in their best set of examples, llama-3-70b-instruct has not achieved its highest performance with the best prompt chosen for mistral-large.Finally, a huge number of parameters in the model does not necessarily guarantee a superior model performance.Both flan-ul2 and llama-3.1-405b-instructfailed to achieve top performance in the classification of paragraphs if compared to other models of the same family.</p>
<p>Future research work should include comparative analyses with nonLLM methods in view of extraction time and quality, as well as measuring LLMs' performance for different materials applications.For creating a dataset of synthesis protocols for reticular materials, future research should address the following: (i) refine JSONs comparison: The creation of metrics for semantically comparing JSONs is needed to validate if the output of the model is structurally comparable with the golden dataset, and if it should be considered a valid JSON; (ii) workflow extraction: The extension of the Information extraction module for extracting the synthesis workflow step-by-step; and (iii) increase use case coverage: The application of KEP to all paragraphs extracted from the selected 2,287 papers.Once processed, the resulting data set should be explored for analyzing the distributions of synthesis details made available in the scientific literature.</p>
<p>[</p>
<p>A Models Description</p>
<p>Flan-T5 [28]is a variant of the T5 (Text-to-Text Transfer Transformer) model, further fine-tuned using a mixture of instruction-based learning tasks.Like the original T5, Flan-T5 leverages a transformer architecture, specifically designed for text-to-text tasks, which means it treats both the input and output as text sequences, regardless of the task (e.g., translation, summarization, question-answering).</p>
<p>The "Flan" component (Fine-tuned LAnguage Net) introduces instruction tuning, where the model is exposed to a variety of natural language instructions during its fine-tuning phase.This method allows the model to generalize better across tasks by learning to follow explicit human instructions.</p>
<p>In essence, Flan-T5 adapts the standard pre-training and fine-tuning methods of T5 but adds an additional layer of task diversity through its instruction-based training.This approach enhances its performance on zero-shot and few-shot learning tasks, making it more versatile for a wide range of NLP applications.</p>
<p>Flan-UL2 (Unified Language Learner) [29] is a variant of the UL2 architecture, designed for improved instruction-based fine-tuning similar to Flan-T5.UL2 is an advanced architecture that introduces a novel pre-training method utilizing a mixture of denoising tasks with different difficulty levels.This approach allows the model to adapt to a wider range of NLP tasks by balancing between simple and complex learning objectives.In the case of Flan-UL2, this model takes UL2 and further enhances it with instruction tuning, similar to the Flan-T5 approach.It is trained on a large variety of instruction tasks, making it highly proficient at zero-shot and few-shot learning across many tasks, such as summarization, translation, and question answering.The model's ability to generalize across these tasks is further improved by the fine-tuning process with diverse datasets of instructions, allowing it to better understand human-like queries and execute complex tasks.This makes Flan-UL2 particularly powerful for applications requiring high versatility and adaptability in natural language understanding.</p>
<p>Granite-20B-Code-Instruct and Granite-34B-Code-Instruct [30] are part of the Granite family of large language models (LLMs) designed specifically for code-related tasks.Both models are finetuned versions of their respective base models, Granite-20B-Code-Base and Granite-34B-Code-Base, using instruction-based datasets to improve their ability to follow natural language instructions.These models, developed by IBM Research, are built for tasks such as code generation, bug fixing, code explanation, and translation across a wide range of programming languages, making them versatile tools for code-centric applications.Granite-20B-Code-Instruct, with 20 billion parameters, was trained on trillions of tokens from various sources, including high-quality code, mathematical data, and instructional prompts.Its fine-tuning emphasizes logical reasoning and problem-solving, with a focus on generating and explaining code, alongside supporting tasks like API calling and debugging .Granite-34B-Code-Instruct, with 34 billion parameters, extends these capabilities by being a more computationally powerful model, trained on a larger and more diverse dataset of code instructions.It can handle more complex coding tasks and demonstrates state-of-the-art performance across benchmarks for code synthesis, explanation, and debugging .Both models are decoder-only architectures, optimized for generating human-readable code outputs from natural language inputs, and are trained with instruction tuning to improve their accuracy in code-based applications.</p>
<p>Llama-3-70B-Instruct [31] is part of Meta's Llama 3 family of large language models, specifically designed for instruction-following tasks.The model contains 70 billion parameters and is optimized for generating text in response to user prompts.It is a decoder-only model, which uses an optimized transformer architecture.The instruction-tuned version of Llama-3-70B benefits from Supervised Fine-Tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF) to align its outputs with human preferences for helpfulness and safety.This fine-tuning process makes it particularly suitable for assistant-like applications, such as chatbots and task-oriented dialogue systems.Llama- Mixtral-8x7B-Instruct-v0. 1 [34] is an advanced sparse mixture-of-experts (SMoE) model developed by Mistral AI.It incorporates a unique architecture where each layer contains eight experts (feedforward blocks), but only two are activated for each token during inference.This selective processing allows the model to manage a large number of parameters-47 billion in total-while only using 13 billion active parameters per token, which significantly reduces computation costs during inference.Mixtral-8x7B-Instruct has been fine-tuned for instruction-following tasks through a combination of supervised fine-tuning (SFT) and Direct Preference Optimization (DPO).This model excels in benchmarks such as MMLU and GSM8K, matching or outperforming larger models like GPT-3.5 Turbo and Llama 2 70B in several areas, particularly code generation, reasoning, and multilingual tasks.Its ability to handle long sequences with a 32k token context window makes it highly effective for long-range information retrieval and complex prompts.</p>
<p>B Examples selection</p>
<p>Paragraph classification Table 6 shows excerpts of JSONs with best and worst paragraphs selected as examples for each model.It is possible to see that few paragraphs appear in more than one prompt.Information extraction Table 7 shows the JSONs that include the best and worst paragraphs selected as examples for each model.</p>
<p>Figure 1 :
1
Figure 1: Knowledge Extraction Pipeline (KEP) with the four KEP modules highlighted in gray color.Also shown are the respective inputs and outputs</p>
<p>Table 1 :
1
Overview of golden dataset
Synthesis Not Synthesisparagraphs classified 188137annotated paragraphs 131-</p>
<p>Table 2 :
2
The best-case (highlighted in bold) and worst-case (underlined) scenarios in the selection of examples to be used in the prompt of the Paragraph classification module.
ModelWorstBest#S#I F1#S#IF1flan-t5-xxl-11b140.93 321.0flan-ul2-20b320.0140.98granite-34b-code-instruct 140.30 230.92granite-20b-code-instruct 230.32 230.74llama-3-70b-instruct140.71 411.0llama-3.1-405b-instruct320.0320.95mistral-large230.76 411.0mixtral-8x7b-instruct-v01 320.61 321.0</p>
<p>Table 3 :
3
The best-case and worst-case scenarios in the selection of examples of the Information extraction module.The best results are highlighted in bold and the worst results are underlined.
ModelWorst accuracy Best accuracygranite-34b-code-instruct 0.700.93granite-20b-code-instruct 0.650.84llama-3-70b-instruct0.540.95llama-3.1-405b-instruct0.530.94mistral-large0.220.94mixtral-8x7b-instruct-v01 0.700.93</p>
<p>Table 4 :
4
Experiments for the Paragraph classification module (best results highlighted in bold).
ModelPrecision Recall F1flan-t5-xxl-11b0.980.960.97flan-ul2-20b0.960.960.96granite-34b-code-instruct 0.870.830.84granite-20b-code-instruct 0.750.700.72llama-3-70b-instruct0.980.980.98llama-3.1-405b-instruct0.980.830.88mistral-large0.980.980.98mixtral-8x7b-instruct-v01 0.950.930.94</p>
<p>33] H.-C. Tsai, Y.-F.Huang, and C.-W. Kuo, "Comparative analysis of automatic literature review using mistral large language model and human reviewers," Sciety, 2024.[Online].Available: https://sciety.org/articles/activity/10.21203/rs.3.rs-4022248/v1[34] M. AI, "Sparse mixture of experts in large language models: Mixtral 8x7b," arXiv preprint arXiv:2401.04088,2024.[Online].Available: https://arxiv.org/abs/2401.04088</p>
<p>[32]B-Instruct was trained on an extensive corpus of 15 trillion tokens from publicly available datasets and supports a wide range of use cases, including multilingual text generation and code-related tasks.It incorporates improvements like Grouped-Query Attention (GQA) for faster inference and an expanded 8,192 token context window, allowing it to handle longer inputs effectively.The model has been tested extensively for safety, and Meta has integrated safeguards to limit misuse, including rigorous red teaming and cybersecurity assessments.The model is available under the Meta Llama 3 Community License for both commercial and research applications.It's praised for outperforming other models in several benchmarks, demonstrating significant advancements in multilingual dialogue capabilities and code generation.Llama 3.1-405B-Instruct[32]is the largest model in the Llama 3.1 series by Meta, designed to provide state-of-the-art performance in multilingual dialogue and complex instruction-following tasks.With 405 billion parameters, it utilizes a transformer-based, decoder-only architecture optimized for extensive text generation tasks.It introduces enhancements in context handling, supporting up to 128,000 tokens, which makes it ideal for tasks like document summarization and long-context conversation .This model is fine-tuned using a combination of Supervised Fine-Tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF), enabling it to align better with human preferences and improve the safety and helpfulness of its outputs .Llama 3.1-405B was trained on a mixture of publicly available datasets containing approximately 15 trillion tokens, and its fine-tuning included more than 25 million synthetically generated instruction-based examples .Furthermore, it offers improved multilingual support beyond English, covering languages like German, French, Italian, Portuguese, Hindi, Spanish, and Thai .The model is open-source and available under Meta's custom open model license, encouraging use in both research and commercial applications .Mistral AI's large language models, particularly Mistral Large 2 [33], represent significant advancements in both computational efficiency and reasoning capabilities.This model, featuring 123 billion parameters, is designed for tasks that require extensive reasoning, such as multilingual text processing, code generation, and mathematical problem-solving.With support for over 80 coding languages and a context window of 128,000 tokens, it excels in handling large documents and long, complex inputs.Mistral Large 2 is particularly strong in benchmarks like MMLU (Massive Multitask Language Understanding), where it achieves an accuracy of 84</p>
<p>Table 6 :
6
(continued)</p>
<p>https://github.com/ElsevierDev/elsapy
https://www.scopus.com
articles with at least one paragraph describing a synthesis protocol and 134 articles without any synthesis protocol description.4 https://ds4sd.github.io/
Paragraph extracted from[26].
Exceptions: mistral and mixtral
Exception: llama-3-70b-instruct selected instead of llama-3-1-70b-instruct since it has a highest performance in the tasks we are testing.
Both flan models accept only 4,096 when comparing to llama that accepts8,192 <br />
To create a more fine-grained comparison between the JSONs, it would be necessary to compare their semantics and not only their structures, as different structures could have the same meaning.</p>
<p>Reticular synthesis and the design of new materials. O M Yaghi, M O'keeffe, N W Ockwig, H K Chae, M Eddaoudi, J Kim, Nature. 42369412003</p>
<p>Digital reticular chemistry. H Lyu, Z Ji, S Wuttke, O M Yaghi, Chem. 692020</p>
<p>Metal-organic frameworks in heterogeneous catalysis: recent progress, new trends, and future perspectives. A Bavykina, N Kolobov, I S Khan, J A Bau, A Ramirez, J Gascon, Chemical reviews. 120162020</p>
<p>Metal-organic frameworks for batteries. R Zhao, Z Liang, R Zou, Q Xu, Joule. 2112018</p>
<p>Metal-organic frameworks as platforms for the removal of per-and polyfluoroalkyl substances from contaminated waters. R Li, N N Adarsh, H Lu, M Wriedt, Matter. 5102022</p>
<p>Metalorganic framework materials as chemical sensors. L E Kreno, K Leong, O K Farha, M Allendorf, R P Van Duyne, J T Hupp, Chemical reviews. 11222012</p>
<p>High-throughput screening of hypothetical metal-organic frameworks for thermal conductivity. M Islamov, H Babaei, R Anderson, K B Sezginel, J R Long, A J Mcgaughey, D A Gomez-Gualdron, C E Wilmer, Computational Materials. 91112023</p>
<p>Progress toward the computational discovery of new metal-organic framework adsorbents for energy applications. P Z Moghadam, Y G Chung, R Q Snurr, Nature Energy. 922024</p>
<p>Porous metal-organic-framework nanoscale carriers as a potential platform for drug delivery and imaging. P Horcajada, T Chalati, C Serre, B Gillet, C Sebrie, T Baati, J F Eubank, D Heurtaux, P Clayette, C Kreuz, Nature materials. 922010</p>
<p>Inverse design of nanoporous crystalline reticular materials with deep generative models. Z Yao, B Sánchez-Lengeling, N S Bobbitt, B J Bucior, S G H Kumar, S P Collins, T Burns, T K Woo, O K Farha, R Q Snurr, Nature Machine Intelligence. 312021</p>
<p>Inverse design of metal-organic frameworks for direct air capture of co 2 via deep reinforcement learning. H Park, S Majumdar, X Zhang, J Kim, B Smit, Digital Discovery. 342024</p>
<p>A generative artificial intelligence framework based on a molecular diffusion model for the design of metal-organic frameworks for carbon capture. H Park, X Yan, R Zhu, E A Huerta, S Chaudhuri, D Cooper, I Foster, E Tajkhorshid, Communications Chemistry. 71212024</p>
<p>Chatmof: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models. Y Kang, J Kim, Nature Communications. 15147052024</p>
<p>Discovery of novel reticular materials for carbon dioxide capture using gflownets. F Cipcigan, J Booth, R N B Ferreira, C R Santos, M Steiner, Digital Discovery. 332024</p>
<p>Development of a cambridge structural database subset: a collection of metal-organic frameworks for past, present, and future. P Z Moghadam, A Li, S B Wiggin, A Tao, A G Maloney, P A Wood, S C Ward, D Fairen-Jimenez, Chemistry of Materials. 2972017</p>
<p>Targeted classification of metal-organic frameworks in the cambridge structural database (csd). P Z Moghadam, A Li, X.-W Liu, R Bueno-Perez, S.-D Wang, S B Wiggin, P A Wood, D Fairen-Jimenez, Chemical science. 11322020</p>
<p>Advances, updates, and analytics for the computation-ready, experimental metal-organic framework database: Core mof 2019. Y G Chung, E Haldoupis, B J Bucior, M Haranczyk, S Lee, H Zhang, K D Vogiatzis, M Milisavljevic, S Ling, J S Camp, Journal of Chemical &amp; Engineering Data. 64122019</p>
<p>Data-driven materials research enabled by natural language processing and information extraction. E A Olivetti, J M Cole, E Kim, O Kononova, G Ceder, T Y , .-J Han, A M Hiszpanski, 10.1063/5.0021106Applied Physics Reviews. 744131712 2020</p>
<p>Extracting accurate materials data from research papers with conversational language models and prompt engineering. M P Polak, D Morgan, 10.1038/s41467-024-45914-8Nature Communications. 15115692024</p>
<p>The learnability of in-context learning. N Wies, Y Levine, A Shashua, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336651</p>
<p>Semi-supervised machine-learning classification of materials synthesis procedures. H Huo, Z Rong, O Kononova, W Sun, T Botari, T He, V Tshitoyan, G Ceder, 10.1038/s41524-019-0204-1Computational Materials. 5162Jul 2019</p>
<p>Text-mined dataset of inorganic materials synthesis recipes. O Kononova, H Huo, T He, Z Rong, T Botari, W Sun, V Tshitoyan, G Ceder, 10.1038/s41597-019-0224-1Scientific Data. Oct 20196203</p>
<p>Mining insights on metal-organic framework synthesis from scientific literature texts. H Park, Y Kang, W Choe, J Kim, Journal of Chemical Information and Modeling. 6252022</p>
<p>Chatgpt chemistry assistant for text mining and the prediction of mof synthesis. Z Zheng, O Zhang, C Borgs, J T Chayes, O M Yaghi, Journal of the American Chemical Society. 14532622023</p>
<p>Chatgpt research group for optimizing the crystallinity of mofs and cofs. Z Zheng, O Zhang, H L Nguyen, N Rampal, A H Alawadhi, Z Rong, T Head-Gordon, C Borgs, J T Chayes, O M Yaghi, ACS Central Science. 9112023</p>
<p>Synthesis, crystal structure, and luminescent sensing properties of a supramolecular 3d zinc(ii) metal-organic framework with terephthalate and bis(imidazol-1-yl)methane linkers. V V Matveevskaya, D I Pavlov, A A Ryadun, V P Fedin, A S Potapov, 10.3390/inorganics11070264Inorganics. 117264Jun. 2023</p>
<p>ECMA-404: The JSON data interchange syntax, ECMA International Std. 2017404</p>
<p>Scaling instruction-finetuned language models. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, E Li, X Wang, M Dehghani, S Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>Ul2 20b: An open-source unified language learner model. Y Tay, H W Chung, L Hou, B Zoph, S Borgeaud, P He, S Narang, W Fedus, D G Patil, arXiv:2301.075202023arXiv preprint</p>
<p>Granite code models: A family of open foundation models for code intelligence. I Research, 2024IBM Documentation</p>
<p>. Meta 405b, Std, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-513 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-513</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-513</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-f80550ebe259c328bccbcbe837df23b4da705acd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f80550ebe259c328bccbcbe837df23b4da705acd" target="_blank">Paired Recurrent Autoencoders for Bidirectional Translation Between Robot Actions and Linguistic Descriptions</a></p>
                <p><strong>Paper Venue:</strong> IEEE Robotics and Automation Letters</p>
                <p><strong>Paper TL;DR:</strong> A novel deep learning framework for bidirectional translation between robot actions and their linguistic descriptions that learns to encode action sequences as fixed-dimensional vectors in a way that allows the sequences to be reproduced from the vectors by its decoder.</p>
                <p><strong>Paper Abstract:</strong> We propose a novel deep learning framework for bidirectional translation between robot actions and their linguistic descriptions. Our model consists of two recurrent autoencoders (RAEs). One RAE learns to encode action sequences as fixed-dimensional vectors in a way that allows the sequences to be reproduced from the vectors by its decoder. The other RAE learns to encode descriptions in a similar way. In the learning process, in addition to reproduction losses, we create another loss function whereby the representations of an action and its corresponding description approach each other in the latent vector space. Across the shared representation, the trained model can produce a linguistic description given a robot action. The model is also able to generate an appropriate action by receiving a linguistic instruction, conditioned on the current visual input. Visualization of the latent representations shows that the robot actions are embedded in a semantically compositional way in the vector space by being learned jointly with descriptions.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e513.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e513.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Paired RAE (NAO experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paired Recurrent Autoencoders (action RAE + description RAE) — NAO cube manipulation experiment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bidirectional pair of recurrent autoencoders that (i) encodes robot action sequences (joint angles + pretrained visual features) and (ii) encodes linguistic descriptions into a shared latent space; decoders reproduce sequences and enable translation between modalities conditioned on visual input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Paired RAE (aRAE + dRAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two recurrent autoencoders: the action-RAE (aRAE) encodes sequences of joint angles concatenated with visual feature vectors into a fixed-length vector; the description-RAE (dRAE) encodes word sequences (one-hot) into fixed-length vectors. Both decoders reconstruct sequences from the shared latent vector. Additionally, a binding loss forces paired action/description encodings to be close and unpaired encodings apart (margin-based). Visual inputs are provided as pretrained CAE features and fed into the aRAE decoder at each step (teacher forcing).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cube manipulation (push/pull/slide) with NAO robot</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Robot performs 12 action types (push/pull/slide × left/right × slow/fast) on two differently colored cubes arranged in fixed positions (6 permutations). Each action sequence is recorded as 10 joint angles and synchronized images; corresponding natural-language descriptions are 3-word sentences (verb + object color + adverb), where the object word depends on cube arrangement.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+object-relational (action sequences grounded to object identity and semantic descriptions); spatial knowledge is implicit in visual features (object positions) but not represented as an explicit map</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>supervised learning from paired action-description data (preprocessed visual features from a convolutional autoencoder), i.e., fine-tuning on embodied task data</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>training with reconstruction losses and binding loss; at inference, encode description with dRAE and decode action with aRAE conditioned on visual features (forward pass), and vice versa for action→description</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>shared continuous latent vector space (fixed-dimensional embeddings) where verbs/objects/adverbs compose additively in latent space; action sequences embedded as fixed vectors (z_act) and descriptions as z_dsc; visual context incorporated by concatenating pretrained visual feature vectors to action inputs and feeding visual features into decoder at each timestep</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy for description generation (exact sentence match), success rate for action execution (object moved >3 cm in correct direction, and timing criteria for fast/slow), dynamic time warping (DTW) similarity for trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Action→description: 54/54 trained patterns and 18/18 untrained patterns correct (100% exact-match by their criterion). Description→action: 36/54 trained patterns successful (~66.7%) and 12/18 untrained patterns successful (~66.7%); failed cases still produced trajectories most similar (by DTW) to the intended reference trajectory but slight deviations prevented correct contact.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>1) Compositional encoding of sentence parts — verb, object (color), and adverb occupy systematic dimensions in the shared latent space; 2) Action→description translation generalizes to untrained combinations (compositional generalization); 3) Description→action produces appropriate trajectories when visual context disambiguates object location and decoder integrates semantic latent vector with visual input.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>1) Small trajectory deviations caused the robot to miss contacting the intended cube side (leading to failed object movement despite trajectory similarity); 2) Model sometimes failed to alter trajectory sufficiently to physically contact the intended object even though latent semantics were correct; 3) reliance on pretrained visual features introduced biases (lighting) and not trained end-to-end, which may have limited robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>aRAE trained alone (no binding): action encodings clustered by raw joint-trajectory type (12 clusters) and were not semantically organized; performance on translation tasks not comparable because unbound model lacks cross-modal mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Training the aRAE alone (no binding loss) produced latent representations organized by trajectory rather than by semantic meaning (visual object identity had little influence), showing binding loss is necessary to ground actions semantically; no direct numeric ablation in NAO experiment beyond this representation comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Joint training of action and language autoencoders with an explicit binding loss embeds actions and descriptions into a shared, semantically compositional latent space: verb/object/adverb factors are disentangled in the space, enabling perfect action→description mapping and substantially enabling description→action when visual context is provided. Visual context (pretrained CAE features) is required at decode time to resolve situational ambiguity; however, pretraining visual features separately creates biases and limits end-to-end robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Paired Recurrent Autoencoders for Bidirectional Translation Between Robot Actions and Linguistic Descriptions', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e513.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e513.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Paired RAE (KIT motion-language)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paired Recurrent Autoencoders (action RAE + description RAE) — KIT motion-language (no-vision) experiment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The same paired-RAE architecture scaled up and applied to the KIT motion-language dataset (human whole-body motions paired with natural-language annotations), evaluated without visual input to test binding between motion and language when the model operates without direct sensory input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Paired RAE (scaled)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Scaled architecture: bidirectional LSTM encoders and multilayer LSTM decoders; binding layer enlarged (512 nodes); word embeddings used instead of one-hot; action decoder outputs modeled as mixture of Gaussians (20 components). Binding loss uses negative cosine similarity. Trained on paired motion-description data (no visual inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Motion-to-language and language-to-motion on KIT motion-language dataset</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Dataset contains ~3,911 human whole-body motion sequences (44 joint angles) and 6,278 natural-language annotations. Tasks: (a) generate textual descriptions from motions (motion→language), and (b) generate motions from descriptions (language→motion); here no visual / external sensory inputs are available — the model must rely solely on latent embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction/motion generation / bidirectional mapping between language and motion</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (motion semantics and action types); primarily procedural since dataset lacks external spatial sensory streams; embedding captures semantic clustering of motion types and keywords</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>supervised training on paired motion-language dataset (fine-tuning on embodied task data); knowledge is learned from motion trajectories and textual annotations</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>joint supervised training with reconstruction and binding loss; beam search (width 5) used for language generation at inference; no external sensory conditioning during generation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>shared latent vector space where motions and sentences are embedded close if paired; decoders produce sequences (text via softmax over vocabulary, motions via mixture of Gaussians outputs); negative cosine similarity used for binding objective to align modalities</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>corpus-level BLEU score (modified, uniformly weighted 1-4 gram with smoothing) for motion→language; qualitative assessment and visual inspection for language→motion generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>BLEU (test set, max [mean]): full bidirectional model .303 [.278]; without discriminative term .186 [.151]; only discriminative term .300 [.282]; unidirectional motion→language .328 [.318]. Language→motion: qualitative samples show plausible common motions (walking) but many generated motions are less sophisticated; quantitative motion generation metrics not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>1) Motions and descriptions are embedded semantically and cluster by keywords/topics (t-SNE visualizations); 2) Common motion types (e.g., walking) and simple sentences produced reasonably well in language→motion generation; 3) Binding enables cross-modal retrieval and generation even without sensory input, by relying on learned semantic embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>1) Bidirectional models sometimes generated grammatically or semantically incorrect sentences and lower BLEU than unidirectional model; 2) Generated motions beyond common types were often noisy or semantically mismatched, indicating decoder capacity/optimization was a bottleneck; 3) absence of visual context means spatial situational grounding is not modeled (no explicit spatial maps).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Unidirectional motion→language model achieved higher BLEU (.328 [.318]) than full bidirectional model (.303 [.278]); ablation removing discriminative term produced much worse BLEU (.186 [.151]).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Three comparisons: (i) full model, (ii) w/o second (discriminative) term of binding loss — performance dropped substantially (BLEU .186 vs .303), (iii) only discriminative term (first term removed) — BLEU .300 close to full; conclusion: the discriminative (margin) term of the binding loss is critical to good generation performance. Also demonstrated that replacing Euclidean distance with negative cosine similarity was used at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When operating without direct sensory input, the paired-RAE binding still learns a shared semantic latent space that clusters motions and their natural-language annotations, enabling cross-modal generation; however, decoder capacity and the discriminative component of the binding loss are critical — removing the discriminative term severely degrades language generation (BLEU), and a unidirectional specialized model can outperform the bidirectional scheme on BLEU, indicating trade-offs between joint shared representations and task-specialized decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Paired Recurrent Autoencoders for Bidirectional Translation Between Robot Actions and Linguistic Descriptions', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e513.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e513.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Binding loss (L_shr)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representation binding loss (pairwise closeness + discrimination margin)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pairwise loss that (1) pulls paired action and description encodings together and (2) enforces that each action encoding is farther from unpaired descriptions by a margin, implemented with a distance/dissimilarity function (Euclidean or negative cosine) and a hinge-style discriminative term.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Paired RAE (binding loss)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Binding loss L_shr = sum_i psi(z_act_i, z_dsc_i) + sum_i sum_{j!=i} max{0, Delta + psi(z_act_i, z_dsc_i) - psi(z_act_i, z_dsc_j)} where psi is a distance (Euclidean in NAO experiments; negative cosine similarity in KIT experiments) and Delta is a margin; hyperparameters weight this term relative to reconstruction losses.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cross-modal representation binding for action↔language translation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used during training to align latent embeddings of paired action sequences and linguistic descriptions so that decoding across modalities is possible; the discriminative second term separates representations of unrelated pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>representation learning for cross-modal translation / grounding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+object-relational+semantic (encodes action sequence semantics and object identity mentioned in language into the latent space)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>learned from paired datasets during supervised training (joint objective alongside reconstruction losses)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>used as an explicit loss term during gradient-based optimization (backpropagation through time) to shape the latent space; no special inference-time procedure required (one forward pass suffices to get latent vector)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>reshapes latent vector space so paired items are nearby and unpaired are separated by a margin; encourages semantic compositionality (verbs/objects/adverbs occupy systematic structure) rather than raw low-level trajectory clusters</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>indirect: effect measured via downstream metrics (exact-match description accuracy, BLEU for motion→language, task success rate for instruction→action)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Including both terms yields best overall performance; ablation removing the discriminative margin term caused large BLEU drop (from .303 to .186 in KIT experiment), while keeping only discriminative term gave similar performance to full (.300), indicating the discriminative term is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Binding loss causes action encodings to organize semantically, enabling compositional generalization and cross-modal decoding; discriminative margin prevents collapse and ensures unpaired items are separated.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Point-to-point binding still enforces one-to-one closeness and does not model many-to-many mappings (multiple descriptions ↔ multiple actions) well; model may need probabilistic latent methods (e.g., variational approaches) to capture inherent ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to no binding (only reconstruction), binding achieved semantically organized embeddings and cross-modal translation capability; compared to only discriminative term, full loss similar but discriminative term alone already provides most benefit in KIT experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing the discriminative term (second term) severely degrades downstream language generation (BLEU .186). Using only the discriminative term yields BLEU .300, close to the full model (.303), highlighting that discrimination is the dominant contributor among binding components in large-scale data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>An explicit margin-based discriminative binding term is critical for shaping a shared latent space that supports cross-modal translation between language and actions; it enforces semantic clustering and prevents trivial closeness while standard reconstruction losses alone do not produce semantically compositional action embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Paired Recurrent Autoencoders for Bidirectional Translation Between Robot Actions and Linguistic Descriptions', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks <em>(Rating: 2)</em></li>
                <li>Two-way translation of compound sentences and arm motions by recurrent neural networks <em>(Rating: 1)</em></li>
                <li>Text2Action: Generative adversarial synthesis from language to action <em>(Rating: 1)</em></li>
                <li>Grounded language learning in a simulated 3D world <em>(Rating: 1)</em></li>
                <li>See, hear, and read: Deep aligned representations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-513",
    "paper_id": "paper-f80550ebe259c328bccbcbe837df23b4da705acd",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "Paired RAE (NAO experiment)",
            "name_full": "Paired Recurrent Autoencoders (action RAE + description RAE) — NAO cube manipulation experiment",
            "brief_description": "A bidirectional pair of recurrent autoencoders that (i) encodes robot action sequences (joint angles + pretrained visual features) and (ii) encodes linguistic descriptions into a shared latent space; decoders reproduce sequences and enable translation between modalities conditioned on visual input.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Paired RAE (aRAE + dRAE)",
            "model_size": null,
            "model_description": "Two recurrent autoencoders: the action-RAE (aRAE) encodes sequences of joint angles concatenated with visual feature vectors into a fixed-length vector; the description-RAE (dRAE) encodes word sequences (one-hot) into fixed-length vectors. Both decoders reconstruct sequences from the shared latent vector. Additionally, a binding loss forces paired action/description encodings to be close and unpaired encodings apart (margin-based). Visual inputs are provided as pretrained CAE features and fed into the aRAE decoder at each step (teacher forcing).",
            "task_name": "Cube manipulation (push/pull/slide) with NAO robot",
            "task_description": "Robot performs 12 action types (push/pull/slide × left/right × slow/fast) on two differently colored cubes arranged in fixed positions (6 permutations). Each action sequence is recorded as 10 joint angles and synchronized images; corresponding natural-language descriptions are 3-word sentences (verb + object color + adverb), where the object word depends on cube arrangement.",
            "task_type": "object manipulation / instruction following",
            "knowledge_type": "procedural+object-relational (action sequences grounded to object identity and semantic descriptions); spatial knowledge is implicit in visual features (object positions) but not represented as an explicit map",
            "knowledge_source": "supervised learning from paired action-description data (preprocessed visual features from a convolutional autoencoder), i.e., fine-tuning on embodied task data",
            "has_direct_sensory_input": true,
            "elicitation_method": "training with reconstruction losses and binding loss; at inference, encode description with dRAE and decode action with aRAE conditioned on visual features (forward pass), and vice versa for action→description",
            "knowledge_representation": "shared continuous latent vector space (fixed-dimensional embeddings) where verbs/objects/adverbs compose additively in latent space; action sequences embedded as fixed vectors (z_act) and descriptions as z_dsc; visual context incorporated by concatenating pretrained visual feature vectors to action inputs and feeding visual features into decoder at each timestep",
            "performance_metric": "accuracy for description generation (exact sentence match), success rate for action execution (object moved &gt;3 cm in correct direction, and timing criteria for fast/slow), dynamic time warping (DTW) similarity for trajectories",
            "performance_result": "Action→description: 54/54 trained patterns and 18/18 untrained patterns correct (100% exact-match by their criterion). Description→action: 36/54 trained patterns successful (~66.7%) and 12/18 untrained patterns successful (~66.7%); failed cases still produced trajectories most similar (by DTW) to the intended reference trajectory but slight deviations prevented correct contact.",
            "success_patterns": "1) Compositional encoding of sentence parts — verb, object (color), and adverb occupy systematic dimensions in the shared latent space; 2) Action→description translation generalizes to untrained combinations (compositional generalization); 3) Description→action produces appropriate trajectories when visual context disambiguates object location and decoder integrates semantic latent vector with visual input.",
            "failure_patterns": "1) Small trajectory deviations caused the robot to miss contacting the intended cube side (leading to failed object movement despite trajectory similarity); 2) Model sometimes failed to alter trajectory sufficiently to physically contact the intended object even though latent semantics were correct; 3) reliance on pretrained visual features introduced biases (lighting) and not trained end-to-end, which may have limited robustness.",
            "baseline_comparison": "aRAE trained alone (no binding): action encodings clustered by raw joint-trajectory type (12 clusters) and were not semantically organized; performance on translation tasks not comparable because unbound model lacks cross-modal mapping.",
            "ablation_results": "Training the aRAE alone (no binding loss) produced latent representations organized by trajectory rather than by semantic meaning (visual object identity had little influence), showing binding loss is necessary to ground actions semantically; no direct numeric ablation in NAO experiment beyond this representation comparison.",
            "key_findings": "Joint training of action and language autoencoders with an explicit binding loss embeds actions and descriptions into a shared, semantically compositional latent space: verb/object/adverb factors are disentangled in the space, enabling perfect action→description mapping and substantially enabling description→action when visual context is provided. Visual context (pretrained CAE features) is required at decode time to resolve situational ambiguity; however, pretraining visual features separately creates biases and limits end-to-end robustness.",
            "uuid": "e513.0",
            "source_info": {
                "paper_title": "Paired Recurrent Autoencoders for Bidirectional Translation Between Robot Actions and Linguistic Descriptions",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "Paired RAE (KIT motion-language)",
            "name_full": "Paired Recurrent Autoencoders (action RAE + description RAE) — KIT motion-language (no-vision) experiment",
            "brief_description": "The same paired-RAE architecture scaled up and applied to the KIT motion-language dataset (human whole-body motions paired with natural-language annotations), evaluated without visual input to test binding between motion and language when the model operates without direct sensory input.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Paired RAE (scaled)",
            "model_size": null,
            "model_description": "Scaled architecture: bidirectional LSTM encoders and multilayer LSTM decoders; binding layer enlarged (512 nodes); word embeddings used instead of one-hot; action decoder outputs modeled as mixture of Gaussians (20 components). Binding loss uses negative cosine similarity. Trained on paired motion-description data (no visual inputs).",
            "task_name": "Motion-to-language and language-to-motion on KIT motion-language dataset",
            "task_description": "Dataset contains ~3,911 human whole-body motion sequences (44 joint angles) and 6,278 natural-language annotations. Tasks: (a) generate textual descriptions from motions (motion→language), and (b) generate motions from descriptions (language→motion); here no visual / external sensory inputs are available — the model must rely solely on latent embeddings.",
            "task_type": "instruction/motion generation / bidirectional mapping between language and motion",
            "knowledge_type": "procedural + object-relational (motion semantics and action types); primarily procedural since dataset lacks external spatial sensory streams; embedding captures semantic clustering of motion types and keywords",
            "knowledge_source": "supervised training on paired motion-language dataset (fine-tuning on embodied task data); knowledge is learned from motion trajectories and textual annotations",
            "has_direct_sensory_input": false,
            "elicitation_method": "joint supervised training with reconstruction and binding loss; beam search (width 5) used for language generation at inference; no external sensory conditioning during generation",
            "knowledge_representation": "shared latent vector space where motions and sentences are embedded close if paired; decoders produce sequences (text via softmax over vocabulary, motions via mixture of Gaussians outputs); negative cosine similarity used for binding objective to align modalities",
            "performance_metric": "corpus-level BLEU score (modified, uniformly weighted 1-4 gram with smoothing) for motion→language; qualitative assessment and visual inspection for language→motion generation",
            "performance_result": "BLEU (test set, max [mean]): full bidirectional model .303 [.278]; without discriminative term .186 [.151]; only discriminative term .300 [.282]; unidirectional motion→language .328 [.318]. Language→motion: qualitative samples show plausible common motions (walking) but many generated motions are less sophisticated; quantitative motion generation metrics not reported.",
            "success_patterns": "1) Motions and descriptions are embedded semantically and cluster by keywords/topics (t-SNE visualizations); 2) Common motion types (e.g., walking) and simple sentences produced reasonably well in language→motion generation; 3) Binding enables cross-modal retrieval and generation even without sensory input, by relying on learned semantic embeddings.",
            "failure_patterns": "1) Bidirectional models sometimes generated grammatically or semantically incorrect sentences and lower BLEU than unidirectional model; 2) Generated motions beyond common types were often noisy or semantically mismatched, indicating decoder capacity/optimization was a bottleneck; 3) absence of visual context means spatial situational grounding is not modeled (no explicit spatial maps).",
            "baseline_comparison": "Unidirectional motion→language model achieved higher BLEU (.328 [.318]) than full bidirectional model (.303 [.278]); ablation removing discriminative term produced much worse BLEU (.186 [.151]).",
            "ablation_results": "Three comparisons: (i) full model, (ii) w/o second (discriminative) term of binding loss — performance dropped substantially (BLEU .186 vs .303), (iii) only discriminative term (first term removed) — BLEU .300 close to full; conclusion: the discriminative (margin) term of the binding loss is critical to good generation performance. Also demonstrated that replacing Euclidean distance with negative cosine similarity was used at scale.",
            "key_findings": "When operating without direct sensory input, the paired-RAE binding still learns a shared semantic latent space that clusters motions and their natural-language annotations, enabling cross-modal generation; however, decoder capacity and the discriminative component of the binding loss are critical — removing the discriminative term severely degrades language generation (BLEU), and a unidirectional specialized model can outperform the bidirectional scheme on BLEU, indicating trade-offs between joint shared representations and task-specialized decoders.",
            "uuid": "e513.1",
            "source_info": {
                "paper_title": "Paired Recurrent Autoencoders for Bidirectional Translation Between Robot Actions and Linguistic Descriptions",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "Binding loss (L_shr)",
            "name_full": "Representation binding loss (pairwise closeness + discrimination margin)",
            "brief_description": "A pairwise loss that (1) pulls paired action and description encodings together and (2) enforces that each action encoding is farther from unpaired descriptions by a margin, implemented with a distance/dissimilarity function (Euclidean or negative cosine) and a hinge-style discriminative term.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Paired RAE (binding loss)",
            "model_size": null,
            "model_description": "Binding loss L_shr = sum_i psi(z_act_i, z_dsc_i) + sum_i sum_{j!=i} max{0, Delta + psi(z_act_i, z_dsc_i) - psi(z_act_i, z_dsc_j)} where psi is a distance (Euclidean in NAO experiments; negative cosine similarity in KIT experiments) and Delta is a margin; hyperparameters weight this term relative to reconstruction losses.",
            "task_name": "Cross-modal representation binding for action↔language translation",
            "task_description": "Used during training to align latent embeddings of paired action sequences and linguistic descriptions so that decoding across modalities is possible; the discriminative second term separates representations of unrelated pairs.",
            "task_type": "representation learning for cross-modal translation / grounding",
            "knowledge_type": "procedural+object-relational+semantic (encodes action sequence semantics and object identity mentioned in language into the latent space)",
            "knowledge_source": "learned from paired datasets during supervised training (joint objective alongside reconstruction losses)",
            "has_direct_sensory_input": null,
            "elicitation_method": "used as an explicit loss term during gradient-based optimization (backpropagation through time) to shape the latent space; no special inference-time procedure required (one forward pass suffices to get latent vector)",
            "knowledge_representation": "reshapes latent vector space so paired items are nearby and unpaired are separated by a margin; encourages semantic compositionality (verbs/objects/adverbs occupy systematic structure) rather than raw low-level trajectory clusters",
            "performance_metric": "indirect: effect measured via downstream metrics (exact-match description accuracy, BLEU for motion→language, task success rate for instruction→action)",
            "performance_result": "Including both terms yields best overall performance; ablation removing the discriminative margin term caused large BLEU drop (from .303 to .186 in KIT experiment), while keeping only discriminative term gave similar performance to full (.300), indicating the discriminative term is critical.",
            "success_patterns": "Binding loss causes action encodings to organize semantically, enabling compositional generalization and cross-modal decoding; discriminative margin prevents collapse and ensures unpaired items are separated.",
            "failure_patterns": "Point-to-point binding still enforces one-to-one closeness and does not model many-to-many mappings (multiple descriptions ↔ multiple actions) well; model may need probabilistic latent methods (e.g., variational approaches) to capture inherent ambiguity.",
            "baseline_comparison": "Compared to no binding (only reconstruction), binding achieved semantically organized embeddings and cross-modal translation capability; compared to only discriminative term, full loss similar but discriminative term alone already provides most benefit in KIT experiments.",
            "ablation_results": "Removing the discriminative term (second term) severely degrades downstream language generation (BLEU .186). Using only the discriminative term yields BLEU .300, close to the full model (.303), highlighting that discrimination is the dominant contributor among binding components in large-scale data.",
            "key_findings": "An explicit margin-based discriminative binding term is critical for shaping a shared latent space that supports cross-modal translation between language and actions; it enforces semantic clustering and prevents trivial closeness while standard reconstruction losses alone do not produce semantically compositional action embeddings.",
            "uuid": "e513.2",
            "source_info": {
                "paper_title": "Paired Recurrent Autoencoders for Bidirectional Translation Between Robot Actions and Linguistic Descriptions",
                "publication_date_yy_mm": "2018-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks",
            "rating": 2
        },
        {
            "paper_title": "Two-way translation of compound sentences and arm motions by recurrent neural networks",
            "rating": 1
        },
        {
            "paper_title": "Text2Action: Generative adversarial synthesis from language to action",
            "rating": 1
        },
        {
            "paper_title": "Grounded language learning in a simulated 3D world",
            "rating": 1
        },
        {
            "paper_title": "See, hear, and read: Deep aligned representations",
            "rating": 1
        }
    ],
    "cost": 0.0127865,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Paired Recurrent Autoencoders for Bidirectional Translation Between Robot Actions and Linguistic Descriptions</h1>
<p>Tatsuro Yamada ${ }^{\circledR}$, Hiroyuki Matsunaga, and Tetsuya Ogata ${ }^{\circledR}$</p>
<h4>Abstract</h4>
<p>We propose a novel deep learning framework for bidirectional translation between robot actions and their linguistic descriptions. Our model consists of two recurrent autoencoders (RAEs). One RAE learns to encode action sequences as fixed-dimensional vectors in a way that allows the sequences to be reproduced from the vectors by its decoder. The other RAE learns to encode descriptions in a similar way. In the learning process, in addition to reproduction losses, we create another loss function whereby the representations of an action and its corresponding description approach each other in the latent vector space. Across the shared representation, the trained model can produce a linguistic description given a robot action. The model is also able to generate an appropriate action by receiving a linguistic instruction, conditioned on the current visual input. Visualization of the latent representations shows that the robot actions are embedded in a semantically compositional way in the vector space by being learned jointly with descriptions.</p>
<p>Index Terms-Deep learning in robotics and automation, AI-based methods, neurorobotics.</p>
<h2>I. INTRODUCTION</h2>
<p>ROBOTS in everyday environments are required to work while communicating with people in a linguistic way. Unlike most situations in industrial factories, our living environment is highly changeable; the current situation almost always differs from the previous ones. Therefore, robots have to flexibly ground linguistic descriptions in their own actions conditioned on the current situation [1]. One linguistic skill required of robots is obviously the ability to generate actions that are instructed by people in a linguistic way. Meanwhile, a robot's paired skill is to describe the current situation, event, or action with a linguistic phrase or a sentence. In particular,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the ability of a robot to describe its own actions is essential for it to communicate with people effectively; this ability makes it easier to interpret the produced actions. Just as people can translate linguistic descriptions into actions and vice versa, so should robots have equivalent bidirectional linguistic skills.</p>
<p>This study proposes a novel deep recurrent neural network (RNN) architecture that can learn (i) to produce linguistic descriptions from robot actions and conversely (ii) to generate robot actions from linguistic commands conditioned on the current visual input. The key idea is making vector representations that encode robot actions and their descriptions by paired recurrent autoencoders (RAEs). One RAE encodes action sequences as fixed-dimensional vectors in a way that allows the sequences to be reproduced from the vectors by its decoder. The other RAE encodes descriptions in a similar way. We propose to train the paired RAEs with an additional loss function that forcibly binds the representations of actions and their paired descriptions. Thanks to this additional loss, the representations of an action and its corresponding description are embedded in an area close to each other in the shared space. Across the shared representation, the trained model can produce a linguistic description from a given robot action. The model is also able to generate an appropriate action upon receiving a linguistic instruction. In other words, through this shared space, these two modalities are bidirectionally translatable.</p>
<p>This paper is organized as follows. In Section II, we make a short survey of related work. In Section III, we describe our proposed neural architecture and the learning algorithm. In Section IV, we present the results of a robot learning experiment to evaluate our proposed method. We also visualize the acquired latent representations of the actions and their descriptions. In Section V, we perform another experiment with a larger dataset of annotated human body motions to evaluate the scalability of the proposed method. We discuss the results and our method in Section VI and finally conclude our study in Section VII.</p>
<h2>II. Related Work</h2>
<h2>A. Encoder-Decoder Model</h2>
<p>Many studies have endeavored to train machine learning models to convert linguistic instructions into robot or simulated agent actions [2]-[10], to convert actions into descriptions [11], or to convert both bidirectionally [12]-[14]. In recent years, the deep RNN model has achieved impressive performance thanks</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Model overview. The model consists of paired RAEs, one for descriptions (lower part) and the other for robot actions (upper part). Each learns to minimize the reconstruction error of the original sequence. In this process, each RAE acquires the ability to encode sequences as fixed-dimensional vectors (z<sup>frec</sup> or z<sup>act</sup>) in a way that allows the original sequences to be reproduced by its decoder. In addition to the reconstruction errors, we create an additional loss function that forces the representation of an action and its corresponding description to be close to each other. By these two types of loss (i.e., reconstruction error and representation distance) converging sufficiently, it is expected that bidirectional translation between robot actions and their descriptions would be achieved.</p>
<p>to it being highly capable of dealing with long-term dependency, which is required to process both linguistic sequences and robot actions. One common architecture is the recurrent encoder–decoder structure, also known as the sequence-to-sequence model [15]. In this structure, the encoder RNN first embeds a sequence (e.g., a linguistic instruction) into a fixed-dimensional representation, whereupon the decoder RNN produces a corresponding sequence (e.g., a robot action) by decoding the latent representation. This architecture is very general and thus is used not only for converting between language and agent control [11], [16] but also for such tasks as translating from one language to another [15], [17], translating from human body gestures to robot actions [18], predicting future images from previous ones [19], and speech recognition [20]. Some recent studies have applied a learning framework of generative adversarial networks [21] to the encoder–decoder architecture in order to generate more diverse and realistic samples [4].</p>
<p>From another perspective, the recurrent encoder–decoder architecture can be used as a type of autoencoder for sequential or temporal data [22], [23]. When this is done, the encoder–decoder architecture is called an RAE. The loss function is defined as the distance between the original input sequence and the model's output sequence. In other words, the model is trained so that its decoder can reconstruct an original input sequence from the fixed-dimensional representation of that sequence encoded by its encoder. In this training process, important features or patterns of sequential data can be extracted in an unsupervised manner. The present study uses two RAEs, one to make representations of robot actions and the other for descriptions.</p>
<h3><em>B. Representation Sharing</em></h3>
<p>As mentioned above, we are able to extract important features of sequential data as fixed-dimensional vectors by training (deep) neural networks. As a way to exploit such representations, there have been some studies on cross-modal retrieval in which the paired representations of different modalities (text, sound, video) are bound with each other by creating an additional loss function to make the representations of corresponding audio, visual, and textual information close to each other [24]–[26]. Through such shared representations, each modality can be retrieved from the others. However, the previous research dealt with retrieval only and thus did not generate novel tokens from the representations. Our proposed model can produce sequences of these modalities directly from the representations because in the learning process the shared representations are structured for decoding by the RAE decoders. Through the representations, the model can translate bidirectionally between robot actions and their descriptions.</p>
<h3>III. PROPOSED METHOD</h3>
<h4><em>A. Model Overview</em></h4>
<p>This study proposes a neural network architecture and learning algorithm that learns from a paired dataset of robot action sequences and their linguistic descriptions to enable a robot both to (i) produce actions in response to linguistic instructions and, in contrast, to (ii) produce linguistic descriptions given its own actions. In this study, we define robot actions as sequences of joint angle values, and we suppose that the appropriate choice of translation depends on the situation, which is given as visual information (explained in detail in Section III-C).</p>
<p>As shown in Fig. 1, our model consists of paired RAEs [22], one for descriptions (i.e., word sequences) and the other for robot action sequences. Each RAE learns to minimize the reconstruction error of the original sequences. In this process, each RAE acquires the ability to encode sequences as fixed-dimensional vectors (z<sup>frec</sup> or z<sup>act</sup>) in a way that allows the original sequences to be reproduced by its decoder.</p>
<p>Here, the characteristics of action sequences and those of their descriptions may differ, something that Saussure [27] described as "arbitrariness" of language. Therefore, the shapes of their embedding spaces would also differ. To bind them, we create an additional loss function that forces the representation of an action and that of its corresponding description to be close to each other in the vector space.</p>
<p>By these two types of loss (i.e., reconstruction error and representation distance) converging sufficiently, it is expected that the two aforementioned capabilities would be achieved. Produc-</p>
<p>ing actions in response to linguistic instructions is realized by using the encoder of the description RAE (dRAE) to encode a description and using the decoder of the action RAE (aRAE) to expand the representation. By contrast, producing linguistic descriptions given actions is realized by having the encoder of the aRAE encode an action sequence and having the decoder of the dRAE expand the representation.</p>
<h2>B. RAE for Descriptions</h2>
<p>To encode descriptions, we use the dRAE (the lower part of Fig. 1). The dRAE consists of an encoder RNN and a decoder RNN. The encoder RNN embeds a description of length $T_{\mathrm{d}}$, $\left(x_{1}, x_{2}, \ldots, x_{T_{\mathrm{d}}}\right)$ into a fixed-dimensional vector $z^{\text {dsc }}$ as follows:</p>
<p>$$
\begin{aligned}
&amp; h_{t}^{\mathrm{enc}}=\operatorname{EncCell}\left(x_{t}, h_{t-1}^{\mathrm{enc}}\right) \quad\left(1 \leq t \leq T_{\mathrm{d}}\right) \
&amp; z^{\mathrm{dsc}}=W^{\mathrm{enc}} h_{T_{\mathrm{d}}}+b^{\mathrm{enc}}
\end{aligned}
$$</p>
<p>where the function of $\operatorname{EncCell}$ represents any type of trainable recurrent cell (e.g., a gated recurrent unit [GRU] [28] or long short-term memory [LSTM] [29]) and $h_{t}^{\text {enc }}$ is the encoder cell state at time step $t . W^{\text {enc }}$ and $b^{\text {enc }}$ are a trainable weight matrix and a bias vector, respectively, to project the final state $h_{T_{\mathrm{d}}}$ of the encoder onto the shared space. We set $h_{0}$ as a zero vector.</p>
<p>After encoding, the decoder RNN generates a sequence by recursively expanding the vector representation $z^{\text {dsc }}$ as follows:</p>
<p>$$
\begin{aligned}
h_{0}^{\mathrm{dec}} &amp; =W^{\mathrm{dec}} z^{\mathrm{dsc}}+b^{\mathrm{dec}} \
h_{t}^{\mathrm{dec}} &amp; =\operatorname{DecCell}\left(y_{t-1}, h_{t-1}^{\mathrm{dec}}\right) &amp; \left(1 \leq t \leq T_{\mathrm{d}}-1\right) \
y_{t} &amp; =f\left(W^{\mathrm{out}} h_{t}^{\mathrm{dec}}+b^{\mathrm{out}}\right) &amp; \left(1 \leq t \leq T_{\mathrm{d}}-1\right)
\end{aligned}
$$</p>
<p>where the function DecCell represents any type of trainable recurrent cell in a similar way to EncCell, and $h_{t}^{\text {dec }}$ is the decoder cell state at time step $t . W^{\text {dec }}$ and $b^{\text {dec }}$ are a trainable weight matrix and a bias vector, respectively, to project the representation $z^{\text {dec }}$ onto the decoder initial cell state. Likewise, $W^{\text {out }}$ and $b^{\text {out }}$ are a trainable weight matrix and a bias vector, respectively, to project the decoder cell state onto the vocabulary size output. $f$ is an activation function. In this study, we represent each word as a one-hot vector that has the value 1 at the element corresponding to the word and 0 at the other elements. We therefore choose the softmax function for $f$. We give a vector representing the $&lt;$ beginning of sequence (BOS) $&gt;$ symbol instead of $y_{0}$.</p>
<p>The target function of training is for the decoder to generate an original description received by the encoder, in other words, minimization of the cross entropy between the input and output:</p>
<p>$$
L_{\mathrm{dsc}}=\frac{1}{T_{\mathrm{d}}-1} \sum_{t=1}^{T_{\mathrm{d}}-1}\left(-\sum_{w}^{W} x_{t+1}(w) \log y_{t}(w)\right)
$$</p>
<p>Here, $W$ is the vocabulary size. We optimize all the trainable parameters by the gradient descent method. The derivative of $L_{\text {dsc }}$ with respect to each parameter can be calculated by the back-propagation through time algorithm [30]. In this unsupervised learning, a vector space that effectively embeds the given description set would be learned.</p>
<h2>C. RAE for Robot Actions</h2>
<p>To encode robot actions, we use the aRAE (the upper part of Fig. 1). Like the dRAE, the aRAE consists of an encoder RNN and a decoder RNN. An action sequence consists of a series of length $T_{\mathrm{a}},\left(j_{1}, j_{2}, \ldots, j_{T_{\mathrm{a}}}\right)$ of robot joint angles and a visual stream $\left(v_{1}, v_{2}, \ldots, v_{T_{\mathrm{a}}}\right)$ accompanying it. Here, to start learning from some advantageous stage, we extract low-dimensional visual features from raw images in advance (shown as a visual feature extractor in Fig. 1). The type of extractor can be chosen depending on the tasks. For example, in Section IV, we use a convolutional neural network. The encoder of the aRAE encodes a sequence $\left(\left(j_{1} ; v_{1}\right),\left(j_{2} ; v_{2}\right), \ldots,\left(j_{T_{\mathrm{a}}} ; v_{T_{\mathrm{a}}}\right)\right)$ that concatenates joint angles and visual features into a fixed-dimensional vector $z^{\text {act }}$. The architecture and the behavior of the encoder are almost the same, as follows:</p>
<p>$$
\begin{aligned}
h_{t}^{\mathrm{enc}} &amp; =\operatorname{EncCell}\left(v_{t}, j_{t}, h_{t-1}^{\mathrm{enc}}\right) \quad\left(1 \leq t \leq T_{\mathrm{a}}\right) \
z^{\mathrm{act}} &amp; =W^{\mathrm{enc}} h_{T_{\mathrm{a}}}^{\mathrm{enc}}+b^{\mathrm{enc}}
\end{aligned}
$$</p>
<p>However, the input/output of the decoder is somewhat different:</p>
<p>$$
\begin{array}{ll}
h_{0}^{\mathrm{dec}}=W^{\mathrm{dec}} z^{\mathrm{act}}+b^{\mathrm{dec}} \
h_{t}^{\mathrm{dec}}=\operatorname{DecCell}\left(v_{t}, \widehat{j}<em t-1="t-1">{t}, h</em>-1\right) \
\widehat{j}}^{\mathrm{dec}}\right) &amp; \left(1 \leq t \leq T_{\mathrm{a}<em t="t">{t+1}=f\left(W^{\mathrm{out}} h</em>-1\right)
\end{array}
$$}^{\mathrm{dec}}+b^{\mathrm{out}}\right) &amp; \left(1 \leq t \leq T_{\mathrm{a}</p>
<p>Equations (10) and (11) mean that the decoder generates only $\widehat{j}<em t_1="t+1">{t+1}$ as a prediction of joint angles at the next time step $j</em>$. The loss function for the aRAE is the mean squared error between the prediction and the target, namely,}$. At each time step, the visual information is given as an external input, as in teacher forcing. In contrast, joint angles predicted by the decoder itself are given to the joint input nodes at the next time step (i.e., it is free running). At the first step only, we give the initial robot posture $j_{1}$ instead of $\widehat{j}_{1</p>
<p>$$
L_{\mathrm{act}}=\frac{1}{T_{\mathrm{a}}-1} \sum_{t=1}^{T_{\mathrm{a}}-1}\left|j_{t+1}-\widehat{j}<em 2="2">{t+1}\right|</em>
$$}^{2</p>
<p>We build the model in this way, which predicts only joint angles, not vision, because we want it to be able to deal with context-dependent actions. In other words, the ambiguities between instructions and robot actions will be resolved by receiving visual information. The following explanation is in the form of a concrete example. Action sequences that actualize the description "push the red ball" can differ from each of the other tokens depending on the position of the red ball. As described in Section III-D, the representations of these various action sequences that are encoded by the encoder of the aRAE are bound with the unique representation of the description "push the red ball" that is encoded by the encoder of the dRAE. As a result, these various action sequences are not represented as ones that differ from each other but are embedded close to each other as sequences that have the same meaning, namely "push the red ball." Therefore, the decoder of the aRAE produces an appropriate action sequence by integrating such a semantic</p>
<h2>Algorithm 1: Learning of Paired RAEs.</h2>
<p>Require: $X^{\mathrm{dsc}}, X^{\mathrm{act}}$ : paired dataset
Require: $\alpha, \beta, \gamma, A$ : hyperparameters
randomly initialize learnable parameters: $\theta$
while not done do
Sample a random batch $\left{x_{i}^{\mathrm{dsc}}, x_{i}^{\mathrm{act}}\right}$ from $X^{\mathrm{dsc}}, X^{\mathrm{act}}$
Calculate $L_{\mathrm{dsc}}, L_{\mathrm{act}}, L_{\mathrm{shr}}$ by forward-path
Compute total loss: $L_{\mathrm{all}} \Leftarrow \alpha L_{\mathrm{dsc}}+\beta L_{\mathrm{act}}+\gamma L_{\mathrm{shr}}$
Compute gradients $\nabla_{\theta} L_{\mathrm{all}}$ by backward-path
Apply the gradients $\theta \Leftarrow \theta-A \nabla_{\theta} L_{\mathrm{all}}$
end while
representation and the current situation given as external visual input during decoding.</p>
<h2>D. Representation Binding</h2>
<p>To bind the encodings of robot actions and their corresponding descriptions, we use the other loss function $L_{\mathrm{shr}}$ in addition to the reconstruction losses $L_{\mathrm{dsc}}, L_{\mathrm{act}}$. We denote a batch of encodings of robot actions as $\left{z_{i}^{\text {act }} \mid 1 \leq i \leq N\right}$ and the encodings of the corresponding descriptions as $\left{z_{i}^{\mathrm{dsc}} \mid 1 \leq i \leq N\right}$, where $N$ is the batch size. The binding loss is as follows:</p>
<p>$$
\begin{aligned}
L_{\mathrm{shr}}= &amp; \sum_{i}^{N} \psi\left(z_{i}^{\mathrm{act}}, z_{i}^{\mathrm{dsc}}\right) \
&amp; +\sum_{i}^{N} \sum_{j \neq i} \max \left{0, \Delta+\psi\left(z_{i}^{\mathrm{act}}, z_{i}^{\mathrm{dsc}}\right)-\psi\left(z_{i}^{\mathrm{act}}, z_{j}^{\mathrm{dsc}}\right)\right}
\end{aligned}
$$</p>
<p>Here, $\psi$ is some function that calculates the distance or dissimilarity between two variables. The first term makes the representation of an action (resp., description) be close to that of its paired description (resp., action). By contrast, the second term makes the representation of an action (resp., description) be far from that of its unpaired description (resp., action) if the distance between them is less than that from the paired description (resp., action). The scalar $\Delta$ is the margin added to the distance between paired representations to enhance the loss.</p>
<h2>E. Learning Procedure</h2>
<p>All the learnable parameters are optimized by the gradient descent method with a random batch sampled from the paired dataset as described in Algorithm 1. Here, we control the importance of each loss function by introducing hyperparameters $\alpha, \beta$, and $\gamma$. The term $A$ controls the learning rate; we can choose any constant or adaptive rate, such as one controlled by Adam [31].</p>
<h2>IV. EXPERIMENT I</h2>
<h2>A. Task Design</h2>
<p>To evaluate the bidirectional translation capability of our proposed method, we performed a robot learning experiment using a small humanoid robot NAO. We put two colored cubes (red,
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Some examples of actions (only left-arm joints are plotted).</p>
<p>TABLE I
LIST OF ACTIONS</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Action name</th>
<th style="text-align: left;">Details</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PUSH-L-SLOW</td>
<td style="text-align: left;">Push the left cube slowly</td>
</tr>
<tr>
<td style="text-align: left;">PUSH-L-FAST</td>
<td style="text-align: left;">Push the left cube fast</td>
</tr>
<tr>
<td style="text-align: left;">PUSH-R-SLOW</td>
<td style="text-align: left;">Push the right cube slowly</td>
</tr>
<tr>
<td style="text-align: left;">PUSH-R-FAST</td>
<td style="text-align: left;">Push the right cube fast</td>
</tr>
<tr>
<td style="text-align: left;">PULL-L-SLOW</td>
<td style="text-align: left;">Pull the left cube slowly</td>
</tr>
<tr>
<td style="text-align: left;">PULL-L-FAST</td>
<td style="text-align: left;">Pull the left cube fast</td>
</tr>
<tr>
<td style="text-align: left;">PULL-R-SLOW</td>
<td style="text-align: left;">Pull the right cube slowly</td>
</tr>
<tr>
<td style="text-align: left;">PULL-R-FAST</td>
<td style="text-align: left;">Pull the right cube fast</td>
</tr>
<tr>
<td style="text-align: left;">SLIDE-L-SLOW</td>
<td style="text-align: left;">Slide the left cube to the right slowly</td>
</tr>
<tr>
<td style="text-align: left;">SLIDE-L-FAST</td>
<td style="text-align: left;">Slide the left cube to the right fast</td>
</tr>
<tr>
<td style="text-align: left;">SLIDE-R-SLOW</td>
<td style="text-align: left;">Slide the right cube to the left slowly</td>
</tr>
<tr>
<td style="text-align: left;">SLIDE-R-FAST</td>
<td style="text-align: left;">Slide the right cube to the left fast</td>
</tr>
</tbody>
</table>
<p>green, or yellow; the cubes were always of different colors) in front of the robot (Fig. 2) in fixed positions; the number of possible cube arrangements was ${ }<em 2="2">{3} P</em>=6$. For each arrangement, the robot could perform the 12 actions listed in Table I.</p>
<p>We made the description corresponding to each action depend on the cube arrangement. More precisely, each descriptive sentence consists of a verb, an object, and an adverb, in that order. The verb and adverb do not depend on the cube arrangement, only on the action type, whereas the object does depend on the cube arrangement. The color subject to the action is assigned to the object. For example, if a red cube was placed on the left and a green cube was placed on the right, the description of the PUSH-L-SLOW action was "push red slowly" and that of the SLIDE-R-FAST action was "slide green fast". Therefore, there are $3 \times 3 \times 2=18$ possible sentences, each a combination of push/pull/slide + red/green/yellow + slowly/fast. Note that the numbers of possible actions and possible sentences are not the same due to this task setting.</p>
<h2>B. Data and Parameters</h2>
<p>We predesigned the action trajectories on a computer in advance. For each of the six cube arrangements, we made the robot produce each of the 12 action sequences while we recorded 10 joint angles on the robot arms as well as images (H: 120, W: 160, RGB) from a head-mounted camera every 300 ms . FAST actions and SLOW actions took approximately 26 and 39 time steps, respectively. We collected all 72 patterns (six arrangements with 12 actions each) six times.</p>
<p>TABLE II
Detailed Architecture of the Visual Feature Extractor</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Layer type</th>
<th style="text-align: center;">Layers</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Input (height, width, n_channels)</td>
<td style="text-align: center;">$(120,160,3)$</td>
</tr>
<tr>
<td style="text-align: center;">Convolution <br> (n_channels, kernel size, stride)</td>
<td style="text-align: center;">$-(8,4,2)-(16,4,2)$ <br> $-(32,4,2)-(64,8,5)$</td>
</tr>
<tr>
<td style="text-align: center;">Fully connected (n_units)</td>
<td style="text-align: center;">-(384)-(192)-(10)-(192)-(384)</td>
</tr>
<tr>
<td style="text-align: center;">Deconvolution <br> (n_channels, kernel size, stride)</td>
<td style="text-align: center;">$-(32,8,5)-(16,4,2)$ <br> $-(8,4,2)$ - output $(3,4,2)$</td>
</tr>
</tbody>
</table>
<p>After recording, we trained a convolutional autoencoder (CAE) from scratch with the collected images to use it as a visual feature extractor. We extracted 10-dimensional visual features from its centrer layer and used them as visual inputs for the aRAE. Table II explains the details of the CAE architecture.</p>
<p>The description sentences were represented as a series of onehot vectors. We pre-/post-fixed $&lt;$ BOS/EOS $&gt;$ symbols to every sentence.</p>
<p>Finally, we divided the 72 possible patterns into two sets: 54 for training and 18 for testing ${ }^{1}$. The encoder and decoder of the dRAE were each a one-layer LSTM with 100 nodes, and those of the aRAE were each a two-layer LSTM with 100 nodes per layer. The dimensionality of the binding layer was also 100. For binding loss, we used Euclidean distance as the distance function $\psi$. The margin $\Delta$ was 1.0 . The mixing rates of losses $\alpha, \beta$, and $\gamma$ were all 1.0. We used Adam as the optimizer (learning rate: 0.001 ), the batch size was 50 , and the number of learning iterations was 20,000 . With this hyper parameter set, we trained the model on a single GPU (Nvidia Titan X Pascal), which took 2,260 s. The source code of our model is available at https://github.com/ogata-lab/PRAE.</p>
<h2>C. Result 1: Translation From Actions to Descriptions</h2>
<p>First, we evaluated the model's ability to produce descriptions of given actions. As mentioned in Section III, this translation is performed by embedding an action sequence with the encoder of the aRAE and expanding the representation with the decoder of the dRAE. Here, the dRAE outputs the probabilistic distributions over the vocabulary as given by its softmax activation. We interpreted a word that corresponded to the element that took the maximum value as the model's output at each time step. If the output sentence was perfectly the same as the correct sentence, we judged the model to have succeeded in describing the action. For this criterion, the model succeeded to produce the correct descriptions for all the 54 trained patterns and the 18 untrained patterns.</p>
<h2>D. Result 2: Translation From Descriptions to Actions</h2>
<p>Next, we evaluated whether the model could produce appropriate robot actions by receiving an instruction and visual information. This translation is performed by embedding an</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Cases in which the robot failed to produce the appropriate action. The solid lines indicate the joint angles produced by the model; the broken lines indicate the predesigned target trajectories. Only five joints on the arm that moved are plotted. Even in these failed cases, the model seems to have been able to produce joint trajectories that were rather similar to the target trajectories.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. [Left] Visualization of description encoding. [Center] Visualization of action encoding for being jointly learned with descriptions. The action sequences are bound with their descriptions and are thus represented semantically and compositionally. [Right] Visualization of action encoding for being learned alone. In this case, the encodings of the action sequences are not semantically organized.
instruction with the encoder of the dRAE and by decoding the representation with the decoder of the aRAE, conditioned on the visual information. We judged the action production to have been successful if the robot moved the cube indicated by the object word in the direction indicated by the verb more than 3 cm . Here, we also regarded the produced action as having been the FAST one if the robot returned to the neighborhood of its initial posture within 30 time steps; otherwise, we regarded it as the SLOW one.</p>
<p>For this criterion, the model produced appropriate actions for 36 out of the 54 trained patterns and for 12 out of the 18 untrained patterns. Fig. 3 shows the failed examples of joint trajectories produced by the model. Even in the failed cases, the model seems to have been able to produce trajectories that were rather similar to the predesigned reference trajectories even though the object was not moved in the correct direction because the produced trajectory differed slightly and the robot did not touch the correct side of the cube. For a detailed evaluation, we employed the dynamic time warping (DTW) that is used to measure similarities between time-series data. With DTW, we confirmed that in each failed case the produced trajectory was most similar to the correct one of the 12 reference trajectories.</p>
<h2>E. Analysis of Shared Representations</h2>
<p>Finally, we visualized the latent representations of the actions and their descriptions by using principal component analysis. First, the left panel of Fig. 4 shows the encodings of all 18</p>
<p>possible sentences. The symbol shape, color, and fill express the verb, object, and adverb, respectively. This panel shows that each part of speech was embedded systematically. The compositional structure of the sentences (i.e., verb, object, and adverb) is strongly represented in this space.</p>
<p>The center panel of Fig. 4 shows the encodings of the action sequences projected onto the same two-dimensional space. This panel shows that the action sequences were bound with their descriptions as expected, and thus represented compositionally. Here, note that the same type of action could be bound with different descriptions. For example, the symbol indicated by (A) represents an encoding of PUSH-R-SLOW in a situation in which there was a yellow cube on the left and a red one on the right; therefore, the action was bound with the description "push red slowly." Meanwhile, the symbol indicated by (B) also represents an encoding of PUSH-R-SLOW in a situation in which there was a green cube on the left and a yellow one on the right, and thus the action was bound with the description "push yellow slowly." Although (A) and (B) represent the same action type, they are far from each other because they are semantically different because of the differing cube arrangements. Likewise, different types of action can be bound with the same description.</p>
<p>To verify that these semantic representations truly arose from the joint learning with their descriptions, we performed an additional experiment in which the aRAE was trained alone (i.e., only the reconstruction error was used). In this case, the encodings of the action sequences were organized as 12 clusters that basically depended on the joint trajectories, as shown in the right panel of Fig. 4. The visual information (i.e., which color was acted upon) did not influence the latent representation greatly. This comparison reveals that the binding loss can substantially influence the latent representations of the action sequences to be grounded semantically in the paired descriptions.</p>
<h2>V. EXPERIMENT II</h2>
<p>In the previous experiment, the task space had to be limited because we used a real robot for the phases from data collection to evaluation. In this section, we evaluate the scalability of the proposed method and perform an ablation study to clarify which term of the binding loss function Eq. (13) is important. However, to our knowledge, there is no suitable large dataset of robot actions with paired descriptions. We instead use the KIT motion-language dataset [32], which consists of human whole-body motions and their annotations.</p>
<h2>A. Data and Parameters</h2>
<p>The KIT motion-language dataset contains 3,911 human whole-body motion sequences, comprising 44 joint angles and 6,278 annotations in natural language ${ }^{2}$ (vocabulary size: 1,345). Note that this dataset does not include visual information. We therefore evaluate the proposed model on binding motions and descriptions only, without the visual condition. In accordance</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>TABLE III
LEARNING DETAIL OF THE PAIRED RAES</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: right;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Description encoder</td>
<td style="text-align: right;">Bidirectional LSTM (2 layers, 64 nodes)</td>
</tr>
<tr>
<td style="text-align: left;">Description decoder</td>
<td style="text-align: right;">LSTM (2 layers, 128 nodes)</td>
</tr>
<tr>
<td style="text-align: left;">Action encoder</td>
<td style="text-align: right;">Bidirectional LSTM (2 layers, 64 nodes)</td>
</tr>
<tr>
<td style="text-align: left;">Action decoder</td>
<td style="text-align: right;">LSTM (3 layers, 400 nodes)</td>
</tr>
<tr>
<td style="text-align: left;">Binding layer</td>
<td style="text-align: right;">512 nodes</td>
</tr>
<tr>
<td style="text-align: left;">Margin $(\Delta)$</td>
<td style="text-align: right;">0.7</td>
</tr>
<tr>
<td style="text-align: left;">Batchsize</td>
<td style="text-align: right;">128</td>
</tr>
<tr>
<td style="text-align: left;">Learning epochs</td>
<td style="text-align: right;">100 (38 batches per epoch)</td>
</tr>
<tr>
<td style="text-align: left;">Word embedding dim.</td>
<td style="text-align: right;">64</td>
</tr>
<tr>
<td style="text-align: left;">Mixture components</td>
<td style="text-align: right;">20 Gaussians</td>
</tr>
</tbody>
</table>
<p>TABLE IV
BLEU SCORES FOR TEST DATA (MAX [MEAN])</p>
<table>
<thead>
<tr>
<th style="text-align: center;">(i) full</th>
<th style="text-align: center;">(ii) w/o discrim.</th>
<th style="text-align: center;">(iii) only discrim.</th>
<th style="text-align: center;">(iv) unidir.(m2l)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">.303 [.278]</td>
<td style="text-align: center;">.186 [.151]</td>
<td style="text-align: center;">.300 [.282]</td>
<td style="text-align: center;">.328 [.318]</td>
</tr>
</tbody>
</table>
<p>with the dataset size, we also scaled up the model. The changed hyper parameters are listed in Table III. We determined the model scale by referring to a model used in [12], although these are not precisely the same. We also changed the word representation form from one-hot to word embedding which was learned together, and changed the output form of the decoder of the aRAE into a mixture of Gaussians (for details, refer to [33]). At last, we replaced the Euclidian distance for the binding loss with the negative cosine similarity, following [24].</p>
<p>In this experiment, we compare five models: (i) the proposed method (full); (ii) the proposed method without the second term of Eq. (13) (without discriminating) and (iii) without the first term of the Eq. (13) (only discriminating); (iv) a unidirectional model from motion to language (motion2language); and (v) a unidirectional model from language to motion (language2motion) ${ }^{3}$. We left out $10 \%$ of the dataset; this omitted part was used for testing. We trained each model five times from differently initialized parameters.</p>
<h2>B. Results</h2>
<p>We first quantitatively evaluated the ability to generate descriptions from motions ${ }^{4}$. Similarly to [12], we evaluated the performance by calculating the corpus-level BLEU score, which is modified, uniformly weighted from 1-gram to 4-gram, and smoothed. Table IV shows the results. We also show the samples generated by the full model (case (i)) and the unidirectional model (case (iv)). See Table V. Although the bidirectional models could not outperform the unidirectional model on BLEU score and sometimes generated grammatically and/or semantically wrong sentences, these models still succeeded in generating appropriate sentences in other examples. Moreover, from the</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>TABLE V
GENERATED DESCRIPTION SAMPLES</p>
<table>
<thead>
<tr>
<th style="text-align: left;">reference</th>
<th style="text-align: left;">(i) full</th>
<th style="text-align: left;">(iv) unidir.(m2l)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">A person walks <br> 3 fourths of a <br> circle to the left.</td>
<td style="text-align: left;">A person walks <br> in a circle <br> counterclockwise.</td>
<td style="text-align: left;">A person walks <br> in a circle <br> to the left.</td>
</tr>
<tr>
<td style="text-align: left;">A person stand still <br> and then gets <br> pushed from behind.</td>
<td style="text-align: left;">A person is <br> pushed back.</td>
<td style="text-align: left;">A person is pushed <br> in the back and <br> therefore makes a <br> few steps forward.</td>
</tr>
<tr>
<td style="text-align: left;">A person waves <br> a few times <br> with both hands.</td>
<td style="text-align: left;">A person waves <br> with the right <br> hand.</td>
<td style="text-align: left;">A person waves <br> with both hands.</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Generated action samples for full model.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. t-SNE visualization of the shared representations. [Left] Each plot represents a sentence, with color differences reflecting differences in included keywords. [Right] Each plot represents a motion.
comparison among cases (i)-(iii), it seems that in the binding loss, the second term for discrimination has a critical effect on achieving better generation.</p>
<p>We next show the samples of motions generated from descriptions by the full model (case (i); see Fig. 5). We qualitatively observed that walking motions, which are included in the dataset more than any other motion, and some other basic motions described in a simple sentence were generated relatively well. Other motions were not very sophisticated: they sometimes looked meaningless or differed from the described motion type. We also visualize some generated samples in a supplementary video:
https://youtu.be/BN5vWy73vPA. However, in the current experiment, even in the case of a unidirectional model (case (v)), the quality of the generated samples looked quite similar. This implies that the learning of the decoder in the aRAE was insufficient, rather than the binding. Fig. 6 shows the representations of motions and descriptions in the binding space, projected by t-SNE. This visualization shows that the motions and their corresponding descriptions are embedded close to each other and semantically clustered. This suggests that the proposed binding method has the potential to be applied to large-scale datasets if the decoder module could be optimized together. Scalability should be explored in more detail in future work.</p>
<h2>VI. DISCUSSION</h2>
<p>We proposed a novel deep encoder-decoder architecture to bidirectionally translate between robot actions and their linguistic descriptions. In previous studies, the encoder-decoder models were mainly used to translate sequences into different sequences in a unidirectional and cross-modal way. By contrast, our paired RAE architecture and binding method can learn bidirectional translation between robot actions and their descriptions from paired action-description datasets. Moreover, although we did not perform such evaluation, our model would be able to translate an action sequence directly to another action sequence that has the same meaning through the semantic representation. For example, the PUSH-L-FAST action when there is a red cube on the left can be embedded as the semantic representation "push red fast." Therefore, if the red cube was to be relocated on the right in the decoding phase, the model would be able to convert the representation into the PUSH-R-FAST action.</p>
<p>Ogata et al. [14] proposed a neural network architecture to bidirectionally translate between robot actions and their descriptions. However, their model requires an iterative backpropagation calculation to get a latent representation with which to produce an action sequence given a sentence, or vice versa. Therefore, it takes time and the convergence might be unstable. In contrast, our model requires only one forward-path calculation to get a latent vector.</p>
<p>Plappert et al. [12] also proposed a method for bidirectional mapping between human body motions and their descriptions. Their model consisted of two distinct unidirectional models. This method always requires a parallel corpus of motions and descriptions. In contrast, our model has the advantage that we can first train the two RAEs independently with single modal datasets and then fine-tune them with a parallel corpus by using binding loss.</p>
<p>However, the present model has some limitations. The most important one is that it binds an action and its description in a point-to-point way. In essence, people can express the same action by various phrases and use the same description for various actions. Even if the situation remains perfectly the same, the relationships between actions and descriptions can still be many to many. One way to deal with this inevitable ambiguity would be to introduce a Bayesian method similar to the variational autoencoder [34].</p>
<p>A second limitation is that the model scalability is not well understood. Although in the second experiment the proposed method sometimes translated between motions and descriptions appropriately, the performance should be improved. The scalability in the case with visual information should also be investigated.</p>
<p>The third one is that in the first experiment we preprocessed the raw images before the learning of the RAEs. As a preliminary check, we analyzed the visual feature compressed by the pretrained CAE. Doing so revealed that the light condition during the data collection had a large bias on the vision data space, although it did not have serious effects in the current experiment. In future work, to provide the model with more-effective and noise-robust representations, the training should be performed in an end-to-end manner from raw images to robot control.</p>
<h2>VII. CONCLUSION</h2>
<p>We proposed a novel paired RAE architecture in which the vector representations of robot actions and their descriptions are bound with each other. This binding enables the model to bidirectionally translate between the actions and the descriptions, conditioned on the current visual input. Visualizations of the shared representations showed that the robot actions were represented semantically and compositionally in the vector space by being learned jointly with descriptions. In future work, we will evaluate the model capacity for tasks that are more complex. We also must consider how to deal with the intrinsic ambiguity of linguistic expressions.</p>
<h2>REFERENCES</h2>
<p>[1] S. Harnad, "The symbol grounding problem," Phys. D:, vol. 42, no. 1-3, pp. 335-346, 1990.
[2] J. Hatori, Y. Kikuchi, S. Kobayashi, and K. Takahashi, "Interactively picking real-world objects with unconstrained spoken language instructions," in Proc. IEEE Int. Conf. Robot. Autom., 2018, pp. 3774-3781.
[3] A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra, "Embodied question answering," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 1-10.
[4] H. Ahn, T. Ha, Y. Choi, H. Yoo, and S. Oh, "Text2Action: Generative adversarial synthesis from language to action," in Proc. IEEE Int. Conf. Robot. Autom., 2018, pp. 5915-5920. http://arxiv.org/abs/1710.05298
[5] K. M. Hermann et al., "Grounded language learning in a simulated 3D world," arXiv:1706.06551, 2017.
[6] D. S. Chaplot, K. M. Sathyendra, R. K. Pasumarthi, D. Rajagopal, and R. Salakhutdinov, "Gated-attention architectures for task-oriented language grounding," in Proc. 32nd AAAI Conf. Artif. Intell., 2018, pp. 2819-2826. http://arxiv.org/abs/1706.07230
[7] H. Mei, M. Bansal, and M. R. Walter, "Listen, attend, and walk: Neural mapping of navigational instructions to action sequences," in Proc. Natl. Conf. Artif. Intell., 2016, pp. 2772-2778.
[8] D. Misra, J. Langford, and Y. Artzi, "Mapping instructions and visual observations to actions with reinforcement learning," in Proc. Conf. Empir. Methods Natural Lang. Process., 2017, pp. 1004-1015. http://arxiv.org/abs/1704.08795
[9] T. Yamada, S. Murata, H. Arie, and T. Ogata, "Dynamical integration of language and behavior in a recurrent neural network for humanrobot interaction," Frontiers Neurorobotics, vol. 10, no. 5, pp. 1-17, 2016.
[10] E. Tuci, T. Ferrauto, A. Zeschel, G. Massera, and S. Nolfi, "An experiment on behavior generalization and the emergence of linguistic compositionality in evolving robots," IEEE Trans. Auton. Mental Develop., vol. 3, no. 2, pp. 176-189, Jun. 2011.
[11] S. Heinrich and S. Wermter, "Interactive language understanding with multiple timescale recurrent neural networks," in Artificial Neural Networks and Machine Learning - ICANN 2014(Lecture Notes in Computer Science 8681), Wermter S. et al., Eds. Berlin, Germany: Springer-Verlag, 2014, pp. 193-200.
[12] M. Plappert, C. Mandery, and T. Asfour, "Learning a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks," arXiv:1705.06400, 2017. http://arxiv.org/abs/1705.06400
[13] W. Takano, S. Hamano, and Y. Nakamura, "Correlated space formation for human whole-body motion primitives and descriptive word labels," Robot. Auton. Syst., vol. 66, pp. 35-43, 2015. http://dx.doi.org/10.1016/j.robot.2014.11.020
[14] T. Ogata, M. Murase, J. Tani, K. Komatani, and H. G. Okuno, "Twoway translation of compound sentences and arm motions by recurrent neural networks," in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2007, pp. 1858-1863.
[15] I. Sutskever, O. Vinyals, and V. Q. Le, "Sequence to sequence learning with neural networks," in Proc. Neural Inf. Process. Syst., 2014, pp. 3104-3112.
[16] P. Anderson, D. Teney, J. Bruce, M. Johnson, S. Niko, and I. Reid, "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 3674-3683.
[17] D. Bahdanau, K. Cho, and Y. Bengio, "Neural machine translation by jointly learning to align and translate," in Proc. Int. Conf. Learn. Representations, 2015, pp. 1-15. http://arxiv.org/pdf/1409.0473v6.pdf
[18] G. Park and J. Tani, "Development of compositional and contextual communicable congruence in robots by using dynamic neural network models," Neural Netw., vol. 72, pp. 109-122, 2015. http://dx.doi.org/10.1016/j.neunet.2015.09.004
[19] N. Srivastava, "Unsupervised learning of video representations using LSTMs," Proc. 32nd Int. Conf. Mach. Learn., 2015, pp. 843-852.
[20] L. Lu, X. Zhang, K. Cho, and S. Renals, "A study of the recurrent neural network encoder-decoder for large vocabulary speech recognition," in Proc. INTERSPEECH 16th Annu. Conf. Int. Speech Commun. Assoc., 2015, pp. 3249-3253.
[21] I. Goodfellow et al., "Generative adversarial nets," Adv. Neural Inf. Process. Syst. 27, 2014, pp. 2672-2680. http://papers.nips.cc/papez/5423-generative-adversarial-nets.pdf
[22] O. Fabius and J. R. V. Amersfoort, "Variational recurrent auto-encoders," in Proc. Int. Conf. Learn. Representations Workshop, 2015, pp. 1-5.
[23] A. Tikhonov and I. P. Yamshchikov, "Music generation with variational recurrent autoencoder supported by history," in Proc. 13th Int. Symp. Comput. Music Multidiscip. Res., 2017, pp. 527-537.
[24] Y. Aytar, C. Vondrick, and A. Torralba, "See, hear, and read: Deep aligned representations," arXiv:1706.00932, 2017. https:// arxiv.org/pdf/1706.00932.pdf
[25] A. Duarte, D. Surís, A. Salvador, and X. Giró, "Temporal-aware crossmodal embeddings for video and audio retrieval," in Proc. Neural Inf. Process. Syst., 2017. http://arxiv.org/abs/1609.08675
[26] R. Arandjelović and A. Zisserman, "Objects that Sound," arXiv:1712.06651, 2017. http://arxiv.org/abs/1712.06651
[27] F. Saussure, Course in General Linguistics. New York, NY, USA: Philosophical Library, 1959.
[28] K. Cho et al., "Learning phrase representations using RNN encoderdecoder for statistical machine translation," in Proc. Conf. Empir. Methods Natural Lang. Process., 2014, pp. 1724-1734.
[29] F. A. Gers and J. Schmidhuber, "Recurrent nets that time and count," Proc. IEEE-INNS-ENNS Int. Joint Conf. Neural Netw., 2000, vol. 3, pp. 189-194.
[30] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, "Learning internal representations by error propagation," in Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Cambridge, MA, USA: MIT Press, 1986, pp. 318-362.
[31] D. Kingma and J. Ba, "Adam: A method for stochastic optimization," in Proc. Int.Conf. Learn. Representations, Dec. 2015, pp. 1-15. http://arxiv.org/abs/1412.6980
[32] M. Plappert, C. Mandery, and T. Asfour, "The KIT motion-language dataset," Big Data, Mary Ann Liebert, Inc., vol. 4, no. 4, Dec., 2016, doi: 10.1089/big.2016.0028.
[33] A. Graves, "Generating sequences with recurrent neural networks," arXiv:1308.0850, pp. 1-43, 2013.
[34] D. P. Kingma and M. Welling, "Auto-encoding variational Bayes," in Proc. Int. Conf. Learn. Represent., 2014, pp. 1-14.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Similarly to [12], we use only 2,846 motions that have a duration of less than 30 s ( 300 time steps) and 6,187 annotations (max length is 41 words). The data representation form is also the same as that used in [12].&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ Here, unlike the bidirectional models that have no explicit experiences of translation during learning phase, the unidirectional models explicitly learn to translate sequences to their corresponding ones.
${ }^{4}$ In this experiment, we used the beam search (width: 5) to obtain candidates, and finally chose the sentence with the highest probability as the model output.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
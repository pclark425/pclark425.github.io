<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4807 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4807</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4807</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-39e0bf77300bb6df8716ce83eb8a3f6a5e3d6b20</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/39e0bf77300bb6df8716ce83eb8a3f6a5e3d6b20" target="_blank">Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> The Chain-of-Table framework is proposed, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts as a proxy for intermediate thoughts, and achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.</p>
                <p><strong>Paper Abstract:</strong> Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4807.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4807.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-Table (CHAIN-OF-TABLE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CHAIN-OF-TABLE: Evolving Tables in the Reasoning Chain for Table Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting framework that has an LLM iteratively plan and execute atomic table operations to produce a chain of intermediate tables that act as structured scratchpad (intermediate thoughts) to solve table-based QA and fact verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Table: Evolving Tables in the REASONING CHAIN FOR TABLE UNDERSTANDING</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CHAIN-OF-TABLE</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A system composed of an LLM (PaLM 2 / GPT-3.5 / LLaMA 2 in experiments) that performs DynamicPlan and GenerateArgs prompting to choose atomic table operations (e.g., add_column, select_row, select_column, group_by, sort_by). The chosen operations are executed programmatically to transform the input table; the transformed table(s) are fed back to the LLM for subsequent planning and final answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>structured external scratchpad (tabular intermediate states)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory is implemented as a sequence of transformed table states produced by executing atomic operations; each intermediate table stores results of prior steps and is provided as updated context to the LLM for planning the next operation and for final querying. The chain of operations is also tracked explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Table-based reasoning (question answering and fact verification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks require multi-step reasoning over semi-structured tables: short-answer QA (WikiTQ), free-form long-answer QA (FeTaQA), and binary fact verification (TabFact). Problems often need selecting relevant rows/columns, aggregation, sorting, and extraction of values across rows.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WikiTQ, TabFact, FeTaQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>PaLM 2: TabFact 86.61% acc, WikiTQ 67.31% acc; GPT-3.5: TabFact 80.20% acc, WikiTQ 59.94% acc; LLaMA 2 (17B): TabFact 67.24% acc, WikiTQ 42.61% acc; FeTaQA (PaLM 2): BLEU 32.61, ROUGE-1 0.66, ROUGE-2 0.44, ROUGE-L 0.56.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Representative non-tabular-memory baselines reported in paper: End-to-End QA (PaLM 2: TabFact 77.92% acc, WikiTQ 60.59%); Chain-of-Thought (PaLM 2: TabFact 79.05% acc, WikiTQ 60.43%); program-aided Dater (PaLM 2: TabFact 84.63% acc, WikiTQ 61.48%). (These are the comparisons used in the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Using structured intermediate tables as memory (CHAIN-OF-TABLE) substantially improves performance over generic textual CoT and many program-aided baselines across datasets and backbones. Example: on PaLM 2 CHAIN-OF-TABLE outperforms the next-best (Dater) on TabFact (86.61% vs 84.63%) and on WikiTQ (67.31% vs 61.48%). The paper shows consistent gains across model sizes and that CHAIN-OF-TABLE degrades more gracefully on larger tables and on problems requiring longer operation chains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not framed as a generic episodic memory system; limitations reported include: (1) performance still decreases for larger/longer inputs (though more gracefully than baselines); (2) reliance on a predefined pool of atomic operations—ablation shows removing certain operations (select_row/select_column/group_by) degrades performance; (3) DynamicPlan generates a whole chain but the implementation only trusts/use the first generated operation because later generated ops may be based on stale intermediate tables (potential planning inconsistency); (4) method requires in-context demo examples and prompt engineering; (5) it is a greedy planning procedure (no full search/self-consistency), which can be both an efficiency advantage and a potential suboptimality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Structured, executable intermediate representations (tables) function as a more effective and reliable memory / scratchpad for tabular reasoning than free-form textual chains or single-pass program generation. Dynamically planning operations conditioned on the latest table state and feeding back the transformed table enables multi-step, adaptive reasoning and better handling of long/complex tables. Different atomic operations matter for different tasks (e.g., group_by important for WikiTQ; select_row/column important for TabFact). Greedy iterative execution over structured memory yields good efficiency (fewer LLM calls) while maintaining or improving accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4807.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4807.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (textual CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique where the LLM is prompted to produce intermediate reasoning steps as free-form text (a textual scratchpad) before generating the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM prompting strategy that elicits multi-step reasoning by asking the model to output intermediate textual reasoning steps prior to the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>textual scratchpad (implicit internal/exposed reasoning trace)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory is the generated chain-of-thought text tokens that record intermediate steps; these tokens act as an exposed scratchpad during model generation but are unstructured (free text) rather than structured tables.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Table-based reasoning (QA and fact verification) as a baseline</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same table-based QA and verification tasks (WikiTQ, TabFact, FeTaQA) used as baselines in the paper to compare textual reasoning chains versus structured tabular chains.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WikiTQ, TabFact, FeTaQA (used as baselines in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>PaLM 2 (CoT baseline reported): TabFact 79.05% acc, WikiTQ 60.43% acc (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>End-to-End QA (no explicit chain): PaLM 2 End-to-End TabFact 77.92% acc, WikiTQ 60.59% acc (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Textual CoT gives modest gains over end-to-end on some tasks/models but is less effective than CHAIN-OF-TABLE for complex tabular reasoning. The paper reports CoT does not close the gap to table-specialized structured intermediate representations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Free-form textual chains are less well suited for complex semi-structured tables because they do not provide structured intermediate results; CoT can also hurt performance on some table tasks versus end-to-end (reported decrease on WikiTQ with PaLM 2 when vanilla CoT is used).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>While CoT helps in generic reasoning, structured memory that directly represents transformed tabular state is a better fit for table understanding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4807.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4807.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dater</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dater (table decomposition for LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that decomposes large tables into sub-tables and sub-questions as a preprocessing/decomposition step to help LLMs reason over tables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Dater</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A pipeline that decomposes tables and questions into smaller sub-tables/sub-queries (fixed decomposition strategy) and uses LLMs to answer per-subproblem; motivated primarily to make large tables tractable for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>modified tabular context (pre-processed external context / decomposed evidence)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Dater creates a set of sub-tables/sub-questions by pre-processing the input table; these decomposed contexts act as an external contextual memory but are a fixed, once-off decomposition rather than an evolving intermediate scratchpad tailored dynamically to the question.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Table-based reasoning (QA and fact verification) as a baseline</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same datasets (WikiTQ, TabFact, FeTaQA) used as a program-aided baseline that guards against large-table issues by decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WikiTQ, TabFact, FeTaQA (used as a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>PaLM 2 (Dater reported in paper): TabFact 84.63% acc, WikiTQ 61.48% acc; FeTaQA (PaLM 2): BLEU 29.47, ROUGE metrics similar to End-to-End.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>End-to-End QA (PaLM 2): TabFact 77.92% acc, WikiTQ 60.59% acc.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Dater's fixed table decomposition improves over naive end-to-end prompting on many table tasks but is outperformed by CHAIN-OF-TABLE's dynamic evolving-table memory, especially on complex tables and longer reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Dater's decomposition is fixed and limited to selection-type operations; it functions more as preprocessing than as a dynamic part of the reasoning chain and therefore cannot adaptively modify the table during multi-step reasoning. It also requires more sampled generations (higher query count) in original instantiation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Preprocessing/decomposition of table context helps scale LLMs to bigger tables, but dynamic, question-adaptive, evolving tabular memory (as in CHAIN-OF-TABLE) provides better accuracy and efficiency for multi-step tabular reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning <em>(Rating: 2)</em></li>
                <li>Binding language models in symbolic languages <em>(Rating: 2)</em></li>
                <li>PAL: Program-aided language models <em>(Rating: 1)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4807",
    "paper_id": "paper-39e0bf77300bb6df8716ce83eb8a3f6a5e3d6b20",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "CoT-Table (CHAIN-OF-TABLE)",
            "name_full": "CHAIN-OF-TABLE: Evolving Tables in the Reasoning Chain for Table Understanding",
            "brief_description": "A prompting framework that has an LLM iteratively plan and execute atomic table operations to produce a chain of intermediate tables that act as structured scratchpad (intermediate thoughts) to solve table-based QA and fact verification.",
            "citation_title": "Chain-of-Table: Evolving Tables in the REASONING CHAIN FOR TABLE UNDERSTANDING",
            "mention_or_use": "use",
            "agent_name": "CHAIN-OF-TABLE",
            "agent_description": "A system composed of an LLM (PaLM 2 / GPT-3.5 / LLaMA 2 in experiments) that performs DynamicPlan and GenerateArgs prompting to choose atomic table operations (e.g., add_column, select_row, select_column, group_by, sort_by). The chosen operations are executed programmatically to transform the input table; the transformed table(s) are fed back to the LLM for subsequent planning and final answer generation.",
            "memory_type": "structured external scratchpad (tabular intermediate states)",
            "memory_description": "Memory is implemented as a sequence of transformed table states produced by executing atomic operations; each intermediate table stores results of prior steps and is provided as updated context to the LLM for planning the next operation and for final querying. The chain of operations is also tracked explicitly.",
            "task_name": "Table-based reasoning (question answering and fact verification)",
            "task_description": "Tasks require multi-step reasoning over semi-structured tables: short-answer QA (WikiTQ), free-form long-answer QA (FeTaQA), and binary fact verification (TabFact). Problems often need selecting relevant rows/columns, aggregation, sorting, and extraction of values across rows.",
            "benchmark_name": "WikiTQ, TabFact, FeTaQA",
            "performance_with_memory": "PaLM 2: TabFact 86.61% acc, WikiTQ 67.31% acc; GPT-3.5: TabFact 80.20% acc, WikiTQ 59.94% acc; LLaMA 2 (17B): TabFact 67.24% acc, WikiTQ 42.61% acc; FeTaQA (PaLM 2): BLEU 32.61, ROUGE-1 0.66, ROUGE-2 0.44, ROUGE-L 0.56.",
            "performance_without_memory": "Representative non-tabular-memory baselines reported in paper: End-to-End QA (PaLM 2: TabFact 77.92% acc, WikiTQ 60.59%); Chain-of-Thought (PaLM 2: TabFact 79.05% acc, WikiTQ 60.43%); program-aided Dater (PaLM 2: TabFact 84.63% acc, WikiTQ 61.48%). (These are the comparisons used in the paper.)",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Using structured intermediate tables as memory (CHAIN-OF-TABLE) substantially improves performance over generic textual CoT and many program-aided baselines across datasets and backbones. Example: on PaLM 2 CHAIN-OF-TABLE outperforms the next-best (Dater) on TabFact (86.61% vs 84.63%) and on WikiTQ (67.31% vs 61.48%). The paper shows consistent gains across model sizes and that CHAIN-OF-TABLE degrades more gracefully on larger tables and on problems requiring longer operation chains.",
            "limitations_or_challenges": "Not framed as a generic episodic memory system; limitations reported include: (1) performance still decreases for larger/longer inputs (though more gracefully than baselines); (2) reliance on a predefined pool of atomic operations—ablation shows removing certain operations (select_row/select_column/group_by) degrades performance; (3) DynamicPlan generates a whole chain but the implementation only trusts/use the first generated operation because later generated ops may be based on stale intermediate tables (potential planning inconsistency); (4) method requires in-context demo examples and prompt engineering; (5) it is a greedy planning procedure (no full search/self-consistency), which can be both an efficiency advantage and a potential suboptimality.",
            "key_insights": "Structured, executable intermediate representations (tables) function as a more effective and reliable memory / scratchpad for tabular reasoning than free-form textual chains or single-pass program generation. Dynamically planning operations conditioned on the latest table state and feeding back the transformed table enables multi-step, adaptive reasoning and better handling of long/complex tables. Different atomic operations matter for different tasks (e.g., group_by important for WikiTQ; select_row/column important for TabFact). Greedy iterative execution over structured memory yields good efficiency (fewer LLM calls) while maintaining or improving accuracy.",
            "uuid": "e4807.0",
            "source_info": {
                "paper_title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Chain-of-Thought (textual CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique where the LLM is prompted to produce intermediate reasoning steps as free-form text (a textual scratchpad) before generating the final answer.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "agent_name": "Chain-of-Thought (CoT) prompting",
            "agent_description": "An LLM prompting strategy that elicits multi-step reasoning by asking the model to output intermediate textual reasoning steps prior to the final answer.",
            "memory_type": "textual scratchpad (implicit internal/exposed reasoning trace)",
            "memory_description": "Memory is the generated chain-of-thought text tokens that record intermediate steps; these tokens act as an exposed scratchpad during model generation but are unstructured (free text) rather than structured tables.",
            "task_name": "Table-based reasoning (QA and fact verification) as a baseline",
            "task_description": "Same table-based QA and verification tasks (WikiTQ, TabFact, FeTaQA) used as baselines in the paper to compare textual reasoning chains versus structured tabular chains.",
            "benchmark_name": "WikiTQ, TabFact, FeTaQA (used as baselines in this paper)",
            "performance_with_memory": "PaLM 2 (CoT baseline reported): TabFact 79.05% acc, WikiTQ 60.43% acc (Table 1).",
            "performance_without_memory": "End-to-End QA (no explicit chain): PaLM 2 End-to-End TabFact 77.92% acc, WikiTQ 60.59% acc (Table 1).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Textual CoT gives modest gains over end-to-end on some tasks/models but is less effective than CHAIN-OF-TABLE for complex tabular reasoning. The paper reports CoT does not close the gap to table-specialized structured intermediate representations.",
            "limitations_or_challenges": "Free-form textual chains are less well suited for complex semi-structured tables because they do not provide structured intermediate results; CoT can also hurt performance on some table tasks versus end-to-end (reported decrease on WikiTQ with PaLM 2 when vanilla CoT is used).",
            "key_insights": "While CoT helps in generic reasoning, structured memory that directly represents transformed tabular state is a better fit for table understanding tasks.",
            "uuid": "e4807.1",
            "source_info": {
                "paper_title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Dater",
            "name_full": "Dater (table decomposition for LLMs)",
            "brief_description": "A prior method that decomposes large tables into sub-tables and sub-questions as a preprocessing/decomposition step to help LLMs reason over tables.",
            "citation_title": "Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning",
            "mention_or_use": "use",
            "agent_name": "Dater",
            "agent_description": "A pipeline that decomposes tables and questions into smaller sub-tables/sub-queries (fixed decomposition strategy) and uses LLMs to answer per-subproblem; motivated primarily to make large tables tractable for LLMs.",
            "memory_type": "modified tabular context (pre-processed external context / decomposed evidence)",
            "memory_description": "Dater creates a set of sub-tables/sub-questions by pre-processing the input table; these decomposed contexts act as an external contextual memory but are a fixed, once-off decomposition rather than an evolving intermediate scratchpad tailored dynamically to the question.",
            "task_name": "Table-based reasoning (QA and fact verification) as a baseline",
            "task_description": "Same datasets (WikiTQ, TabFact, FeTaQA) used as a program-aided baseline that guards against large-table issues by decomposition.",
            "benchmark_name": "WikiTQ, TabFact, FeTaQA (used as a baseline)",
            "performance_with_memory": "PaLM 2 (Dater reported in paper): TabFact 84.63% acc, WikiTQ 61.48% acc; FeTaQA (PaLM 2): BLEU 29.47, ROUGE metrics similar to End-to-End.",
            "performance_without_memory": "End-to-End QA (PaLM 2): TabFact 77.92% acc, WikiTQ 60.59% acc.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Dater's fixed table decomposition improves over naive end-to-end prompting on many table tasks but is outperformed by CHAIN-OF-TABLE's dynamic evolving-table memory, especially on complex tables and longer reasoning chains.",
            "limitations_or_challenges": "Dater's decomposition is fixed and limited to selection-type operations; it functions more as preprocessing than as a dynamic part of the reasoning chain and therefore cannot adaptively modify the table during multi-step reasoning. It also requires more sampled generations (higher query count) in original instantiation.",
            "key_insights": "Preprocessing/decomposition of table context helps scale LLMs to bigger tables, but dynamic, question-adaptive, evolving tabular memory (as in CHAIN-OF-TABLE) provides better accuracy and efficiency for multi-step tabular reasoning.",
            "uuid": "e4807.2",
            "source_info": {
                "paper_title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning",
            "rating": 2
        },
        {
            "paper_title": "Binding language models in symbolic languages",
            "rating": 2
        },
        {
            "paper_title": "PAL: Program-aided language models",
            "rating": 1
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 1
        }
    ],
    "cost": 0.015302749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Chain-of-Table: Evolving Tables in the REASONING CHAIN FOR TABLE UNDERSTANDING</h1>
<p>Zilong Wang ${ }^{1 *}$ Hao Zhang ${ }^{3}$ Chun-Liang Li ${ }^{2}$ Julian Martin Eisenschlos ${ }^{3}$<br>Vincent Perot ${ }^{3}$ Zifeng Wang ${ }^{2}$ Lesly Miculicich ${ }^{2}$ Yasuhisa Fujii ${ }^{3}$<br>Jingbo Shang ${ }^{1}$ Chen-Yu Lee ${ }^{2}$ Tomas Pfister ${ }^{2}$<br>${ }^{1}$ University of California, San Diego ${ }^{2}$ Google Cloud AI Research ${ }^{3}$ Google Research</p>
<h4>Abstract</h4>
<p>Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the CHAIN-OF-TABLE framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. CHAIN-OF-TABLE achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.</p>
<h2>1 INTRODUCTION</h2>
<p>Tables are a popular data format and widely used in daily life (Cafarella et al., 2008). Understanding tabular data with language models can benefit various downstream tasks, such as table-based fact verification (Chen et al., 2019), and table-based question answering (Jin et al., 2022). Distinct from pure text, tables deliver rich information through the interaction between rows and columns in the tabular structure, which enhances the data capacity but also increases the difficulty for language models to understand them. Thus, reasoning over the tabular data is an important direction in natural language processing and attracts increasing attention from both academia and industry.</p>
<p>In recent years, several approaches have been suggested to tackle the problem of table understanding by training language models. One common direction is to add specialized embedding layers or attention mechanisms into language models and pre-train the models by recovering table cells or segments (Herzig et al., 2020; Wang et al., 2021; Gu et al., 2022; Andrejczuk et al., 2022). In this way, the pre-trained models are aware of the tabular structure. Another direction is to synthesize SQL query-response pairs and pre-train an encoder-decoder model as a neural SQL executor (Eisenschlos et al., 2020; Liu et al., 2021; Jiang et al., 2022).</p>
<p>Recently, large language models (LLMs) achieve outstanding performance across diverse tasks solely by prompting, thanks to the massive scale of pre-training (Brown et al., 2020; Kojima et al., 2022). As series of works on prompting techniques have further improved the reliability of LLMs by designing reasoning chains, such as Chain-of-Thought (Wei et al., 2022), Least-to-Most (Zhou et al., 2022), Program-of-Thought (Chen et al., 2022) and Tree-of-Thought (Yao et al., 2023). Different works have also explored the possibility of using LLMs to solve table-based problems (Chen,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the comparison between (a) generic reasoning, (b) program-aided reasoning, and (c) the proposed Chain-of-Table. Given a complex table where a cyclist's nationality and name are in the same cell, (a) is unable to provide the correct answer through multi-step reasoning due to the complexity; (b) generates and executes programs (e.g. SQL queries) to deliver the answer, but it also falls short in accurately parsing the name and nationality in the table. In contrast, (c) Chain-of-Table iteratively samples a chain of operations that effectively transform the complex table into a version specifically tailored to the question. With the assistance of CHAIN-OF-TABLE, the LLM can arrive at the correct answer.</p>
<p>2023; Cheng et al., 2022; Ye et al., 2023). However, these approaches (Hsieh et al., 2023) often represent reasoning steps in free-form text or code, which are not ideally suited for addressing scenarios involving complex tables, as shown in Figure 1(a) and Figure 1(b).</p>
<p>On the other hand, inference on tables typically involves a series of intermediate reasoning steps and each of them aligns with specific tabular operations. We propose ChAIN-OF-TABLE, where we conduct step-by-step reasoning as step-by-step tabular operations to form a chain of tables. The tables in the chain are the transformed tables by the tabular operations, representing the intermediate reasoning results. This procedure resembles the thought of reasoning in Chain-of-Thought (Wei et al., 2022). Specifically, we define a set of table operations, such as adding columns, selecting rows, grouping, and more, which are commonly-used in SQL and DataFrame development (Pönighaus, 1995; Shi et al., 2020; Katsogiannis-Meimarakis &amp; Koutrika, 2023). We then prompt LLMs to conduct step-by-step reasoning. In each step, the LLM dynamically generates an operation as the next step along with its required arguments, and then we execute the operation on the table programmatically. This operation can either enrich the table by adding detailed intermediate results or condense it by removing irrelevant information. Intuitively, visualizing the intermediate results is essential for reaching correct predictions. We feed the transformed table back for the next step. This iterative process continues until an ending state is achieved. We argue that the tables obtained during the reasoning steps are better structured representations of the intermediate thoughts than free-form text. Finally, the ChAIN-OF-TABLE reasoning results in tables from which it is easier for LLMs to derive a final answer to the question.</p>
<p>We validate ChAIN-OF-TABLE with three tabular benchmarks to evaluate table-based reasoning: WikiTQ (Pasupat &amp; Liang, 2015), TabFact (Chen et al., 2019), and FeTaQA (Nan et al., 2022). We conduct our experiments using proprietary PaLM 2 (Anil et al., 2023) and GPT-3.5 (Brown et al., 2020; OpenAI, 2023), and open-sourced LLaMA 2 (Touvron et al., 2023), to demonstrate that our proposed method ChAIN-OF-TABLE is able to generalize to various LLM options. We summarize our contribution as follows:</p>
<ul>
<li>We extend the concept of Chain-of-Thought to the tabular setting, where we transform the input table to store intermediate results. This multi-step tabular reasoning approach with table evolution leads to more accurate table understanding.</li>
<li>Extensive experiments on table-based fact verification and question answering show that CHAIN-OF-TABLE archives state-of-the-art performance in WikiTQ, TabFact, and FeTaQA datasets.</li>
</ul>
<h1>2 Related Work</h1>
<p>Fine-tuning Language Model for Table Understanding Tables are effective in organizing, storing, and analyzing information. Efforts have been made to fine-tune language models (LMs) to tackle table understanding tasks. Following the successful mask language modeling (MLM) proposed in BERT (Devlin et al., 2019), TaPas (Herzig et al., 2020) adopts this approach and asks the model to reconstruct certain cells in the table during pre-training. Pasta (Gu et al., 2022) and TUTA (Wang et al., 2021) further propose to mask the entire columns or segments in the table. On the other hand, TAPEX (Liu et al., 2021) pre-trains an encoder-decoder model with a large synthetic SQL dataset so that it can perform as a SQL executor to better understand the tabular structure. Eisenschlos et al. (2020) and Jiang et al. (2022) also leverage synthesized SQL with additional consideration of the alignment between SQL and natural language questions by pre-training the model with both natural and synthetic data.</p>
<p>Prompting Language Model for Table Understanding LLMs can learn from a few samples as prompts through in-context learning. This strategy is widely used to give models additional instructions to better solve downstream tasks. Chain-of-Thought (CoT) (Wei et al., 2022) proposes to generate reasoning steps before answering instead of directly generating an end-to-end answer. Following CoT, Least-to-Most (Zhou et al., 2022) and DecomP (Khot et al., 2022) propose to break down the question into subproblems in the reasoning chain. During reasoning, the latter steps are aware of the previous ones. Such iterative chains with task decomposition further improve the results on complex problems by leveraging the intermediate results from solving subproblems. Jin \&amp; Lu (2023) enhances CoT through a table-filling procedure, with a primary focus on text-based tasks where the input and output are in textual format. However, the line of works following CoT is not specifically designed for tabular data. As reported in Chen (2023), large language models with these generic reasoning methods can achieve decent results, but there are still gaps between these methods and those specialized for table scenarios (Cheng et al., 2022; Ye et al., 2023). We propose CHAIN-OF-TABLE to fill the gap by directly incorporating intermediate tables from tabular operations as a proxy of intermediate thoughts.</p>
<p>To better solve table-based tasks with LLMs, researchers go beyond general text and resort to using external tools. Chen et al. (2022); Gao et al. (2023) propose solving reasoning tasks by generating Python programs, which are then executed using the Python interpreter. This approach greatly improves the performance of arithmetic reasoning. In the scenario of table understanding, Text-toSQL with LLMs (Rajkumar et al., 2022) is a straightforward application of this idea. To further push the limits of programs, Binder (Cheng et al., 2022) generates SQL or Python programs and extends their capabilities by calling LLMs as APIs in the programs. LEVER (Ni et al., 2023) also proposes solving the table-based tasks with programs but with the additional step of verifying the generated programs with their execution results. However, the assistant programs in these programaided methods still fall short in solving difficult cases that involve complex tables. These limitations are primarily due to the constraints of the single-pass generation process, where the LLMs lack the capability to modify the table in response to a specific question, requiring them to perform reasoning over a static table. Our method, on the contrary, is a multi-step reasoning framework that conducts tabular reasoning step by step. It transforms the tables tailored to the given question.</p>
<p>To the best of our knowledge, Dater (Ye et al., 2023) is the only model that modifies the tabular context while solving table-based tasks. However, the table decomposition in Dater is motivated by the idea that tables could be too large for LLMs to conduct reasoning. It is, therefore, more similar to an LLM-aided data pre-processing than to a part of the reasoning chain since the tabular operations are limited to column and row selections, and fixed for all tables and questions. In contrast, our CHAIN-OF-TABLE generalizes a larger set of generic table operations and dynamically generates reasoning chains in an adaptive way based on the inputs, leveraging the planning ability (Valmeekam et al., 2022; Hao et al., 2023) of LLMs.</p>
<h1>3 Chain-of-Table ReASONing</h1>
<p>Problem Formulation. In table-based reasoning, each entry can be represented as a triplet $(T, Q, A)$, where $T$ stands for the table, $Q$ represents a question or statement related to the table, and $A$ is the expected answer. Particularly, in the table-based question answering task, $Q$ and $A$ are the question and expected answer in natural language form; in the table-based fact verification task, $Q$ is a statement about the table contents and $A \in{$ True, False $}$ is a Boolean value that indicates the statement's correctness. The objective is to predict the answer $A$ given the question $Q$ and the table $T$. To facilitate table-based reasoning within the same paradigm employed for generic reasoning, we convert all data values, including tables, into textual representations (see Appendix D for the tabular format encoding method).</p>
<h3>3.1 OVERVIEW</h3>
<p>CHAIN-OF-TABLE enables LLMs to dynamically plan a chain of operations over a table $T$ in response to a given question $Q$. It utilizes atomic tool-based operations to construct the table chain. These operations include adding columns, selecting rows or columns, grouping, and sorting, which are common in SQL and DataFrame development (see Appendix A for more details).</p>
<p>Previously, Dater (Ye et al., 2023) employs a dedicated yet fixed procedure for decomposing tables and questions, which limits its compatibility with new operations. Also, Binder (Cheng et al., 2022), while potentially compatible with new operations, is restricted to those that work with code interpreters such as SQL or Python. In contrast, our framework is extendable and can incorporate operations from a wide range of tools thanks to the flexible in-context learning capability to sample and execute effective operations.</p>
<p>As illustrated in Algorithm 1, at each iteration, we prompt the LLM to sample one of the pre-defined atomic operations denoted as $f$ using the corresponding question $Q$, the latest table state $T$, and the operation chain chain (Line 4). Then, we query the LLM to generate the required arguments args for $f$ (Line 5) and execute it to transform the table $T$ (Line 6). We keep track of the operation f performed on the table in the operation chain chain (Line 7). The process finishes when the ending tag $[E]$ is generated (Line 8). Finally, we feed the latest table into the LLM to predict the answer (Line 9). This series of operations serves as the reasoning steps leading LLMs to understand the input table and better generate the final answer.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="err">:</span><span class="w"> </span><span class="n">CHAIN</span><span class="o">-</span><span class="k">OF</span><span class="o">-</span><span class="nc">TABLE</span><span class="w"> </span><span class="n">Prompting</span>
<span class="w">    </span><span class="k">Data</span><span class="err">:</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="n">T</span><span class="p">,</span><span class="w"> </span><span class="n">Q</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="nc">table</span><span class="o">-</span><span class="n">question</span><span class="w"> </span><span class="n">pair</span><span class="p">.</span>
<span class="w">    </span><span class="k">Result</span><span class="err">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">hat</span><span class="err">{</span><span class="n">A</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">predicted</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">question</span><span class="p">.</span>
<span class="w">    </span><span class="k">Function</span><span class="w"> </span><span class="n">Chain</span><span class="o">-</span><span class="k">of</span><span class="o">-</span><span class="nc">Table</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="n">T</span><span class="p">,</span><span class="w"> </span><span class="n">Q</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">:</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">chain</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="o">[</span><span class="n">([B</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">phi</span><span class="p">),</span><span class="err">]\</span><span class="p">)</span>
<span class="w">        </span><span class="n">repeat</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">f</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">DynamicPlan</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="n">T</span><span class="p">,</span><span class="w"> </span><span class="n">Q</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="n">chain</span><span class="w"> </span><span class="err">\</span><span class="p">()</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="n">args</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">GenerateArgs</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="n">T</span><span class="p">,</span><span class="w"> </span><span class="n">Q</span><span class="p">,</span><span class="w"> </span><span class="n">f</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">T</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">T</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="n">args</span><span class="w"> </span><span class="err">\</span><span class="p">()</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="n">chain</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">chain</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">))</span>
<span class="w">        </span><span class="n">until</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">f</span><span class="o">=[</span><span class="n">E</span><span class="o">]</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">hat</span><span class="err">{</span><span class="n">A</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">Query</span><span class="err">}</span><span class="p">(</span><span class="n">T</span><span class="p">,</span><span class="w"> </span><span class="n">Q</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">hat</span><span class="err">{</span><span class="n">A</span><span class="err">}\</span><span class="p">)</span>
</code></pre></div>

<h3>3.2 DYNAMIC PLANNING</h3>
<p>CHAIN-OF-TABLE instructs the LLM to dynamically plan the next operation by in-context learning. As shown in Figure 2(a), DynamicPlan involves three components: the most recent intermediate table $T$ (Figure 2(a)(i)), the history of the previous operations chain chain (Figure 2(a)(ii)), and the question $Q$ (Figure 2(a)(iii)). We guide the LLM to select the subsequent operation $f$ from the operation pool given $(T$, chain, $Q)$. The LLM is then able to dynamically plan the next operation and build a tabular reasoning chain step by step. See Appendix E. 1 for detailed prompts.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of DynamicPlan $(T, Q$, chain $)$ and GenerateArgs $(T, Q, f)$ in the proposed Chain-of-Table, where $T$ is a intermediate table; $Q$ is the question; chain is a list of operations already performed on the table; $f$ is the operation selected by DynamicPlan. Left: DynamicPlan samples the next operation from the operation pool, according to ( $T$, chain, $Q$ ). Right: GenerateArgs takes the selected operation $f$ as input and generates its arguments based on $(T, f, Q)$. The operations, along with their arguments, act as a proxy of the tabular reasoning process to effectively tackle table understanding tasks.</p>
<h1>3.3 ARGUMENT GENERATION</h1>
<p>The next step, GenerateArgs, involves generating arguments for the selected table operation $f$ sampled by DynamicPlan, as depicted in Figure 2. GenerateArgs involves three key components: the most recent intermediate table $T$ (Figure 2(b)(i)), the selected operation $f$ along with its arguments args (Figure 2(b)(ii)), and the question (Figure 2(b)(iii)). We employ simple regular expressions to account for varying number of arguments required by different operations (see Appendix E. 2 for more details). Finally, we apply programming languages to execute the operation and create the corresponding intermediate tables.</p>
<h3>3.4 FINAL QUERY</h3>
<p>We transform the table through dynamic planning (Section 3.2) and argument generation (Section 3.3). During this process, we create a chain of operations that acts as a proxy for the tabular reasoning steps. These operations generate intermediate tables that store and present the results of each step to the LLM. Consequently, the output table from this chain of operations contains comprehensive information about the intermediate phases of tabular reasoning. We then employ this output table in formulating the final query. As illustrated in Figure 1 (bottom right), we input both the output table and the question into the LLM, which provides the final answer to the question (see Line 9 in Algorithm 1).</p>
<h2>4 EXPERIMENTS</h2>
<p>We evaluate the proposed CHAIN-OF-TABLE on three public table understanding benchmarks: WikiTQ (Pasupat \&amp; Liang, 2015), FeTaQA (Nan et al., 2022), and TabFact (Chen et al., 2019). WikiTQ and FeTaQA are datasets focused on table-based question answering. They require complex tabular reasoning over the provided table to answer questions. WikiTQ typically requires short text span answers, whereas FeTaQA demands longer, free-form responses. TabFact, on the other hand, is a table-based binary fact verification benchmark. The task is to ascertain the truthfulness of a given statement based on the table. For WikiTQ evaluation, we use the official denotation accuracy (Pasupat \&amp; Liang, 2015), and for TabFact, we employ the binary classification accuracy. Given the nature of FeTaQA, which involves comparing predictions with longer target texts, we utilize BLEU (Papineni et al., 2002), ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) for assessment. In our experiments, we use PaLM 2-S ${ }^{1}$, GPT 3.5 (turbo-16k-0613) ${ }^{2}$, and LLaMA 2 (Llama-2-17B-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Table understanding results on WikiTQ and TabFact with PaLM 2, GPT 3.5, and LLaMA 2. (underline denotes the second-best performance; bold denotes the best performance; the improvement is measured against the second-best performing method.)</p>
<table>
<thead>
<tr>
<th>Prompting</th>
<th>PaLM 2</th>
<th></th>
<th>GPT 3.5</th>
<th></th>
<th>LLaMA 2</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>TabFact</td>
<td>WikiTQ</td>
<td>TabFact</td>
<td>WikiTQ</td>
<td>TabFact</td>
<td>WikiTQ</td>
</tr>
<tr>
<td>Generic Reasoning</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>End-to-End QA</td>
<td>77.92</td>
<td>60.59</td>
<td>70.45</td>
<td>51.84</td>
<td>44.86</td>
<td>23.90</td>
</tr>
<tr>
<td>Few-Shot QA</td>
<td>78.06</td>
<td>60.33</td>
<td>71.54</td>
<td>52.56</td>
<td>62.01</td>
<td>35.52</td>
</tr>
<tr>
<td>Chain-of-Thought (Wei et al., 2022)</td>
<td>79.05</td>
<td>60.43</td>
<td>65.37</td>
<td>53.48</td>
<td>60.52</td>
<td>36.05</td>
</tr>
<tr>
<td>Program-aided Reasoning</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Text-to-SQL (Rajkumar et al., 2022)</td>
<td>68.37</td>
<td>52.42</td>
<td>64.71</td>
<td>52.90</td>
<td>64.03</td>
<td>36.14</td>
</tr>
<tr>
<td>Binder (Cheng et al., 2022)</td>
<td>76.98</td>
<td>54.88</td>
<td>79.17</td>
<td>56.74</td>
<td>62.76</td>
<td>30.92</td>
</tr>
<tr>
<td>Dater (Ye et al., 2023)</td>
<td>84.63</td>
<td>61.48</td>
<td>78.01</td>
<td>52.81</td>
<td>65.12</td>
<td>41.44</td>
</tr>
<tr>
<td>CHAIN-OF-TABLE (ours)</td>
<td>86.61 (+1.98)</td>
<td>67.31 (+5.83)</td>
<td>80.20 (+1.03)</td>
<td>59.94 (+3.20)</td>
<td>67.24 (+2.12)</td>
<td>42.61 (+1.17)</td>
</tr>
</tbody>
</table>
<p>chat $)^{3}$ as the backbone LLMs. We incorporate few-shot demo samples from the training set into the prompts to perform in-context learning. Examples of these prompts can be found in Appendix E. Details regarding the LLM inference parameters and the number of demonstration samples used are provided in Appendix C.</p>
<h1>4.1 BASELINES</h1>
<p>The baseline methods are categorized into two groups: (a) generic reasoning, which includes End-to-End QA, Few-Shot QA, Chain-of-Thought (Wei et al., 2022); and (b) program-aided reasoning, which includes Text-to-SQL (Rajkumar et al., 2022), Binder (Cheng et al., 2022), Dater (Ye et al., 2023)). Detailed descriptions of these baseline methods are provided below.</p>
<p>Generic Reasoning End-to-End QA guides the LLM to directly produce the answer when provided with a table and a question as input prompts. Few-Shot QA operates similarly, but it includes few-shot examples of (Table, Question, Answer) triplets in the prompt, as detailed in Brown et al. (2020). We select these examples from the training set, and the model also outputs the answer directly. Chain-of-Thought (Wei et al., 2022) prompts the LLM to articulate its reasoning process in text format before delivering the question. See Appendix F for the prompts of baselines.</p>
<p>Program-aided Reasoning Text-to-SQL (Rajkumar et al., 2022) utilizes in-context samples to guide LLMs in generating SQL queries for answering questions. This approach follows the concepts introduced by Chen et al. (2022); Gao et al. (2023). Binder (Cheng et al., 2022) integrates a language model API with programming languages such as SQL or Python. This integration prompts the LLM to produce executable programs that perform table reasoning tasks on the given table and question. Dater (Ye et al., 2023) employs few-shot samples for efficient deconstruction of table contexts and questions, enhancing end-to-end table reasoning with decomposed sub-tables and sub-questions.</p>
<h3>4.2 RESULTS</h3>
<p>We compare Chain-of-Table with generic reasoning methods and program-aided reasoning methods on three datasets: WikiTQ, TabFact, and FeTaQA. The results on WikiTQ and TabFact are presented in Table 1. We have additional results on FeTaQA in Appendix B. We follow the previous works and report the performance using the official evaluation pipeline ${ }^{4}$.</p>
<p>Table 1 shows that CHAIN-OF-TABLE significantly outperforms all generic reasoning methods and program-aided reasoning methods on TabFact and WikiTQ across PaLM 2, GPT 3.5, and LLaMA</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance of Chain-of-Thought, Dater, and the proposed Chain-of-Table on WikiTQ for questions that require an operation chain of varying lengths. Our proposed atomic operations allow our proposed method CHAIN-OF-TABLE to dynamically transform the input table through multiple reasoning iterations. This significantly improves performance over generic and programaided reasoning counterparts.</p>
<p>Table 2: Distribution of the number of samples v.s. the required length of operation chain in CHAIN-OF-TABLE with PaLM 2 on WikiTQ and TabFact datasets. We observe that the majority of samples need 2 to 4 operations to generate the final output.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Length of operation chain</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$\mathbf{1}$</td>
<td>$\mathbf{2}$</td>
<td>$\mathbf{3}$</td>
<td>$\mathbf{4}$</td>
<td>$\mathbf{5}$</td>
</tr>
<tr>
<td>WikiTQ</td>
<td>95</td>
<td>1308</td>
<td>1481</td>
<td>1084</td>
<td>341</td>
</tr>
<tr>
<td>TabFact</td>
<td>4</td>
<td>547</td>
<td>732</td>
<td>517</td>
<td>223</td>
</tr>
</tbody>
</table>
<ol>
<li>This is attributed to the dynamically sampled operations and the informative intermediate tables in CHAIN-OF-TABLE. CHAIN-OF-TABLE iteratively generates operations that act as proxies for tabular reasoning steps. These operations produce and present tailored intermediate tables to the LLM, conveying essential intermediate thoughts (see the example in Figure 4). With the support of CHAIN-OF-TABLE, the LLM can reliably reach the correct answer.</li>
</ol>
<p>From the results, we observe a performance decrease on WikiTQ due to the complexity of tabular structure when vanilla Chain-of-Thought is introduced to End-to-End QA using PaLM 2. In contrast, our proposed CHAIN-OF-TABLE consistently enhances End-to-End QA performance by $8.69 \%$ on TabFact and $6.72 \%$ on WikiTQ with PaLM 2.</p>
<p>We also observe that our proposed CHAIN-OF-TABLE is effective across all backbone models experimented, while other competing methods, such as Binder, perform better on larger LLMs but its performance decreases with smaller LLaMA 2 (Llama-2-17B-chat). We attribute this decline to Binder's single-pass generation process. While Binder does incorporate API calls within its framework, it lacks the capability to modify and observe the transformed tables. Consequently, Binder can only perform the tabular reasoning over a static table, making it challenging to solve complicated cases with smaller LLMs.</p>
<h1>4.3 Performance Analysis under Different Operation Chain Lengths</h1>
<p>In CHAIN-OF-TABLE, the selection of each operation is dynamically determined based on the difficulty and complexity of the questions and their corresponding tables. Therefore, we conduct a detailed study on the performance under different numbers of operations by categorizing the test samples according to their operation lengths. We report the distribution of the number of samples v.s. the required length of operation chain in Table 2. This analysis focuses on samples that require operations in the reasoning process. We use the results with PaLM 2 as an example. Our observations reveal that the majority of samples require 2 to 4 operations to generate the final output.</p>
<p>For each chain length, we further compare CHAIN-OF-TABLE with Chain-of-Thought and Dater, as representative generic and program-aided reasoning methods, respectively. We illustrate this using results from PaLM 2 on WikiTQ. We plot the accuracy of all methods using bar charts in Figure 3,</p>
<p>Table 3: Performance of Binder, Dater, and the proposed Chain-of-Table on small ( $&lt;2000$ tokens), medium ( 2000 to 4000 tokens), large ( $&gt;4000$ tokens) tables from WikiTQ. We observe that the performance decreases with larger input tables while CHAIN-OF-TABLE diminishes gracefully, achieving significant improvements over competing methods. (underline denotes the second-best performance; bold denotes the best performance; the improvement is measured against the secondbest performing method.)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompting</th>
<th style="text-align: left;">Table Size</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Small $(&lt;2 \mathrm{k})$</td>
<td style="text-align: left;">Medium $(2 \mathrm{k} \sim 4 \mathrm{k})$</td>
<td style="text-align: left;">Large $(&gt;4 \mathrm{k})$</td>
</tr>
<tr>
<td style="text-align: left;">Binder (Cheng et al., 2022)</td>
<td style="text-align: left;">56.54</td>
<td style="text-align: left;">26.13</td>
<td style="text-align: left;">6.41</td>
</tr>
<tr>
<td style="text-align: left;">Dater (Ye et al., 2023)</td>
<td style="text-align: left;">$\underline{62.50}$</td>
<td style="text-align: left;">$\underline{42.34}$</td>
<td style="text-align: left;">$\underline{34.62}$</td>
</tr>
<tr>
<td style="text-align: left;">CHAIN-OF-TABLE (ours)</td>
<td style="text-align: left;">$\mathbf{6 8 . 1 3}(+5.63)$</td>
<td style="text-align: left;">$\mathbf{5 2 . 2 5}(+9.91)$</td>
<td style="text-align: left;">$\mathbf{4 4 . 8 7}(+10.25)$</td>
</tr>
</tbody>
</table>
<p>Table 4: Number of samples generated for a single question in Binder, Dater, and the proposed Chain-of-Table on the WikiTQ dataset. Notably, Chain-of-Table generates the fewest samples among the baselines $-50 \%$ less than Binder and $75 \%$ less than Dater. For a detailed description of the steps involved in Binder and Dater, please refer to the corresponding papers.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompting</th>
<th style="text-align: center;">Total # of <br> generated samples</th>
<th style="text-align: left;"># of generated samples <br> in each steps</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Binder (Cheng et al., 2022)</td>
<td style="text-align: center;">50</td>
<td style="text-align: left;">Generate Neural-SQL: 50</td>
</tr>
<tr>
<td style="text-align: left;">Dater (Ye et al., 2023)</td>
<td style="text-align: center;">100</td>
<td style="text-align: left;">Decompose Table: 40; Generate Cloze: 20; <br> Generate SQL: 20; Query: 20</td>
</tr>
<tr>
<td style="text-align: left;">CHAIN-OF-TABLE (ours)</td>
<td style="text-align: center;">$\leq \mathbf{2 5}$</td>
<td style="text-align: left;">DynamicPlan: $\leq 5$; GenerateArgs: $\leq 19$; <br> Query: 1</td>
</tr>
</tbody>
</table>
<p>highlighting the gap between the compared methods and our method. Notably, Chain-of-Table consistently surpasses both baseline methods across all operation chain lengths, with a significant margin up to $11.6 \%$ compared with Chain-of-Thought, and up to $7.9 \%$ compared with Dater.</p>
<p>Generally, the performance of these methods decreases as the number of tabular operations required in the tabular reasoning chain increases due to higher difficulty and complexity of questions and tables. Nevertheless, our proposed CHAIN-OF-TABLE declines gracefully compared to other baseline methods. For example, CHAIN-OF-TABLE exhibits only a minimal decrease in performance when the number of operations increases from four to five.</p>
<h1>4.4 Performance Analysis under Different Table Sizes</h1>
<p>Large tables present significant challenges to LLMs since LLMs often struggle to interpret and integrate contexts in long input prompts (Liu et al., 2023a; Ye et al., 2023). To assess the performance on tables of various sizes, we categorize the input tables from WikiTQ into 3 groups based on token count: small ( $&lt;2000$ tokens), medium ( 2000 to 4000 tokens) and large ( $&gt;4000$ tokens). We then compare Chain-of-Table with Dater (Ye et al., 2023) and Binder (Cheng et al., 2022), the two latest and strongest baselines, as representative methods. Detailed results are presented in Table 3.</p>
<p>As anticipated, the performance decreases with larger input tables, as models are required to process and reason through longer contexts. Nevertheless, the performance of the proposed CHAIN-OF-TABLE diminishes gracefully, achieving a significant $10+\%$ improvement over the second best competing method when dealing with large tables. This demonstrates the efficacy of the reasoning chain in handling long tabular inputs.</p>
<h3>4.5 Efficiency Analysis of Chain-of-Table</h3>
<p>We analyze the efficiency of Chain-of-Table by evaluating the number of required generated samples. We compare Chain-of-Table with Binder (Cheng et al., 2022) and Dater (Ye et al., 2023), the two latest and most competitive baseline method. The analysis results on WikiTQ are presented in Table 4. Binder generates Neural-SQL queries, requiring 50 samples for self-consistent</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Illustration of the tabular reasoning process in Chain-of-Table. This iterative process involves dynamically planning an operation chain and accurately storing intermediate results in the transformed tables. These intermediate tables serve as tabular thought process that can guide the LLM to land to the correct answer more reliably.
results. Dater involves multiple delicate yet fixed steps, such as decomposing the tables and generating cloze queries for the questions. In each step, Dater also employs self-consistency to improve accuracy of the LLM outputs, leading to a high number of required generated samples. For a detailed description of these frameworks, please refer to the corresponding papers, Ye et al. (2023) and Cheng et al. (2022).</p>
<p>Unlike these previous methods, our proposed CHAIN-OF-TABLE employs a greedy search strategy in its tabular reasoning process, instead of relying on self-consistency sampling for boosting performance. This approach results in a reduced query count for our method, despite CHAIN-OF-TABLE adopting an iterative reasoning process. To be more specific, we observe that the number of queries needed by CHAIN-OF-TABLE is the lowest among the most recent baselines $-50 \%$ less than Binder and $75 \%$ less than Dater. We attribute the query efficiency of our method to the proposed dynamic operation execution through the tabular reasoning. The model is able to find an effective reasoning process that reaches the final output quicker and more reliably.</p>
<h1>4.6 CASE Study</h1>
<p>In Figure 4, we illustrate the tabular reasoning process by CHAIN-OF-TABLE. The question is based on a complex table and requires multiple reasoning steps to 1) identify the relevant columns, 2) conduct aggregation, and 3) reorder the aggregated intermediate information. Our proposed CHAIN-OF-TABLE involves dynamically planning an operation chain and accurately storing intermediate results in the transformed tables. These intermediate tables serve as tabular thought process that can guide the LLM to land to the correct answer more reliably.</p>
<h2>5 CONCLUSION</h2>
<p>Our proposed CHAIN-OF-TABLE enhances the reasoning capability of LLMs by leveraging the tabular structure to express intermediate thoughts for table-based reasoning. It instructs LLMs to dynamically plan an operation chain according to the input table and its associated question. This evolving table design sheds new light on the understanding of prompting LLMs for table understanding.</p>
<h2>6 REPRODUCIbility STATEMENT</h2>
<p>We include the prompt examples of DynamicPlan $(T, Q$, chain) in Appendix E.1, the demo examples of GenerateArgs $(T, Q, f)$ in Appendix E.2, the prompt examples of Query $(T, Q)$ in Appendix E.3. We run the generic reasoning methods (End-to-End QA, FewShot QA, Chain-of-Thought) using the prompts reported in Appendix F. We run Text-to-SQL and Binder using the official open-sourced code and prompts in https://github.com/HKUNLP/Binder. We run Dater using the official open-sourced code and prompts in https://github.com/ AlibabaResearch/DAMO-ConvAI. We revise the code to use publicly available GPT 3.5, PaLM 2, and LLaMA 2 (Section 4) as the LLM backbone instead of the OpenAI Codex due to its inaccessibility.</p>
<h1>REFERENCES</h1>
<p>Ewa Andrejczuk, Julian Eisenschlos, Francesco Piccinno, Syrine Krichene, and Yasemin Altun. Table-to-text generation and pre-training with TabT5. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 6758-6766, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.503. 1</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. 2</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. 1, 2,6</p>
<p>Michael J. Cafarella, Alon Halevy, Daisy Zhe Wang, Eugene Wu, and Yang Zhang. Webtables: Exploring the power of tables on the web. Proc. VLDB Endow., 1(1):538-549, aug 2008. ISSN 2150-8097. doi: $10.14778 / 1453856.1453916 .1$</p>
<p>Wenhu Chen. Large language models are few(1)-shot table reasoners. In Findings of the Association for Computational Linguistics: EACL 2023, pp. 1120-1130, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-eacl.83. 1, 3, 17</p>
<p>Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. Tabfact: A large-scale dataset for table-based fact verification. In International Conference on Learning Representations, 2019. 1, 2, 5</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022. 1, 3, 6</p>
<p>Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, et al. Binding language models in symbolic languages. In International Conference on Learning Representations, 2022. 2, 3, 4, 6, 8, 9, 16</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, 2019. 3</p>
<p>Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William Cohen. Handling divergent reference texts when evaluating table-to-text generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4884-4895, 2019. 14</p>
<p>Julian Eisenschlos, Syrine Krichene, and Thomas Müller. Understanding tables with intermediate pre-training. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 281-296, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.findings-emnlp.27. 1, 3</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: Program-aided language models. In International Conference on Machine Learning, pp. 10764-10799. PMLR, 2023. 3, 6</p>
<p>Zihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiaoman Zhao, and Xiaoyong Du. PASTA: Tableoperations aware fact verification via sentence-table cloze pre-training. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 4971-4983, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.331. 1, 3</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023. 3</p>
<p>Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Eisenschlos. TaPas: Weakly supervised table parsing via pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4320-4333, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.398. 1, 3</p>
<p>Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. In Findings of the Association for Computational Linguistics: ACL 2023. Association for Computational Linguistics, 2023. 2</p>
<p>Shima Imani, Liang Du, and Harsh Shrivastava. MathPrompter: Mathematical reasoning using large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track), pp. 37-42, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-industry.4. 13</p>
<p>Zhengbao Jiang, Yi Mao, Pengcheng He, Graham Neubig, and Weizhu Chen. OmniTab: Pretraining with natural and synthetic data for few-shot table-based question answering. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 932-942, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.68. 1, 3, 16</p>
<p>Nengzheng Jin, Joanna Siebert, Dongfang Li, and Qingcai Chen. A survey on table question answering: recent advances. In China Conference on Knowledge Graph and Semantic Computing, pp. 174-186. Springer, 2022. 1</p>
<p>Ziqi Jin and Wei Lu. Tab-cot: Zero-shot tabular chain of thought. arXiv preprint arXiv:2305.17812, 2023. 3</p>
<p>George Katsogiannis-Meimarakis and Georgia Koutrika. A survey on deep learning approaches for text-to-sql. The VLDB Journal, pp. 1-32, 2023. 2</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. In International Conference on Learning Representations, 2022. 3</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, 2022. 1</p>
<p>Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. 5,14</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023a. 8</p>
<p>Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. TAPEX: Table pre-training via learning a neural sql executor. In International Conference on Learning Representations, 2021. 1, 3, 16</p>
<p>Qian Liu, Fan Zhou, Zhengbao Jiang, Longxu Dou, and Min Lin. From zero to hero: Examining the power of symbolic tasks in instruction tuning. arXiv preprint arXiv:2304.07995, 2023b. 16</p>
<p>Joshua Maynez, Priyanka Agrawal, and Sebastian Gehrmann. Benchmarking large language model capabilities for conditional generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9194-9213, 2023. 14</p>
<p>Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryściński, Hailey Schoelkopf, Riley Kong, Xiangru Tang, Mutethia Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, Dragomir Radev, and Dragomir Radev. FeTaQA: Free-form table question answering. Transactions of the Association for Computational Linguistics, 10:35-49, 2022. doi: 10.1162/tacl_a_00446. 2, 5</p>
<p>Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pp. 26106-26128. PMLR, 2023. 3</p>
<p>OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. 2
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. 5</p>
<p>Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1470-1480, Beijing, China, July 2015. Association for Computational Linguistics. doi: $10.3115 / \mathrm{v} 1 / \mathrm{P} 15-1142.2,5$</p>
<p>Richard Pönighaus. 'favourite'sql-statements-an empirical analysis of sql-usage in commercial applications. In International Conference on Information Systems and Management of Data, pp. 75-91. Springer, 1995. 2</p>
<p>Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. Evaluating the text-to-sql capabilities of large language models. arXiv preprint arXiv:2204.00498, 2022. 3, 6</p>
<p>Tianze Shi, Chen Zhao, Jordan Boyd-Graber, Hal Daumé III, and Lillian Lee. On the potential of lexico-logical alignments for semantic parsing to sql queries. Findings of the Association for Computational Linguistics: EMNLP 2020, 2020. 2</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2</p>
<p>Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still can't plan (a benchmark for llms on planning and reasoning about change). In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022. 3</p>
<p>Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, and Dongmei Zhang. TUTA: Treebased transformers for generally structured table pre-training. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining, pp. 1780-1790, 2021. 1, 3</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022. 1, 2, 3, 6</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023. 1</p>
<p>Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning. arXiv preprint arXiv:2301.13808, 2023. 2, 3, 4, 6, 8, 9, 13, 14, 16</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, et al. Least-to-most prompting enables complex reasoning in large language models. In International Conference on Learning Representations, 2022. 1, 3</p>
<h1>Appendix</h1>
<h2>A Atomic Operations in Chain-of-Table</h2>
<h2>A. 1 INTRODUCTION</h2>
<p>In this study, we adopt a set of five table operations, which are commonly-used in SQL and DataFrame development, as an example. We note that our framework can trivially accommodate additional operations, which we leave for future work.</p>
<ul>
<li>f_add_column() adds a new column to the table to store intermediate reasoning or computational results.</li>
<li>f_select_row() selects a subset of rows that are relevant to the question. Tables may contain irrelevant information for the given question (Ye et al., 2023). This operation helps locate the necessary context.</li>
<li>f_select_column() selects a subset of columns. A column usually corresponds to an attribute in the table. This operation allows the model to locate the necessary attributes to answer the question.</li>
<li>f_group_by() groups the rows by the contents of a specific column and provides the count of each enumeration value in that column. Many table-based questions or statements involve counting, but LLMs are not proficient at this task (Imani et al., 2023).</li>
<li>f_sort_by() sorts the rows based on the contents of a specific column. When dealing with questions or statements involving comparison or extremes, LLMs can utilize this operation to rearrange the rows. The relationship can be readily inferred from the order of the sorted rows.</li>
</ul>
<h2>A. 2 Ablation Study</h2>
<p>To demonstrate the effectiveness of our proposed atomic operations, we perform an ablation study by creating five leave-one-out variants of our method, each of which removes one of the pre-defined operations from the pre-defined operation pool. For example, w/o f_add_column() means f_add_column() is removed from the operation pool. As a result, the LLM is only able to plan from the remaining four operations (f_select_column, f_select_row, f_group_by, and f_sort_by) to construct operation chains. We report the results of the ablation study in Table 5.</p>
<p>Table 5: Ablation study of the atomic operations used in CHAIN-OF-TABLE with PaLM 2 on WikiTQ and TabFact datasets. We observe that row selection and group-by operations have the biggest impact on the final table understanding performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompting</th>
<th style="text-align: left;">TabFact</th>
<th style="text-align: left;">WikiTQ</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Accuracy</td>
<td style="text-align: left;">Accuracy</td>
</tr>
<tr>
<td style="text-align: left;">CHAIN-OF-TABLE</td>
<td style="text-align: left;">$\mathbf{8 6 . 6 1}$</td>
<td style="text-align: left;">$\mathbf{6 7 . 3 1}$</td>
</tr>
<tr>
<td style="text-align: left;">w/o f_add_column()</td>
<td style="text-align: left;">$85.23(-1.38)$</td>
<td style="text-align: left;">$65.88(-1.43)$</td>
</tr>
<tr>
<td style="text-align: left;">w/o f_select_column()</td>
<td style="text-align: left;">$82.61(-4.00)$</td>
<td style="text-align: left;">$65.68(-1.63)$</td>
</tr>
<tr>
<td style="text-align: left;">w/o f_select_row()</td>
<td style="text-align: left;">$82.21(-4.40)$</td>
<td style="text-align: left;">$65.06(-2.25)$</td>
</tr>
<tr>
<td style="text-align: left;">w/o f_group_by()</td>
<td style="text-align: left;">$84.78(-1.83)$</td>
<td style="text-align: left;">$61.88(-5.43)$</td>
</tr>
<tr>
<td style="text-align: left;">w/o f_sort_by()</td>
<td style="text-align: left;">$86.21(-0.40)$</td>
<td style="text-align: left;">$65.85(-1.46)$</td>
</tr>
</tbody>
</table>
<p>As shown in Table 5, all five operations contribute to the final state-of-the-art performance of CHAIN-OF-TABLE, as removing any operation results in a decrease in performance. In particular, we observe that f_select_row() and f_select_column() contribute the most on TabFact, while f_group_by() contributes the most on WikiTQ. This suggests that different tasks require different operations to help the LLM determine the correct answer. Therefore, leveraging the LLM to design custom operation chains through dynamic planning naturally fits different tasks, resulting in superior performance of our method.</p>
<h1>B EXPERIMENTS OF CHAIN-OF-TABLE ON FeTAQA</h1>
<p>Table 6 shows that CHAIN-OF-TABLE also improves the performance of free-form question answering on FeTaQA across all metrics, whereas Dater (Ye et al., 2023) fails to improve the ROUGE scores compared with End-to-End QA. We also observe the marginal improvement of CHAIN-OF-TABLE compared with the baseline methods. We attribute this to the nature of the n-gram text similarity metrics of ROUGE-1/2/L (Lin, 2004). As discussed in Maynez et al. (2023); Dhingra et al. (2019), these metrics are known to be insensitive to capturing improvements when using in-context learning since the model is unable to learn the expected style of the long form text just from an instruction or a few examples. We sample several cases from FeTaQA as shown in Figure 5 where the ROUGE metrics assign low scores; however, upon review, we observe that the generated answers were correct.</p>
<p>Table 6: Table understanding results on the FeTaQA benchmark using PaLM 2 with the best results in bold and improvements over Dater (Ye et al., 2023) reported. (underline denotes the second-best performance; bold denotes the best performance; the improvement is measured against the secondbest performing method.)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompting</th>
<th style="text-align: left;">FeTaQA</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">BLEU</td>
<td style="text-align: left;">ROUGE-1</td>
<td style="text-align: left;">ROUGE-2</td>
<td style="text-align: left;">ROUGE-L</td>
</tr>
<tr>
<td style="text-align: left;">End-to-End QA</td>
<td style="text-align: left;">28.37</td>
<td style="text-align: left;">0.63</td>
<td style="text-align: left;">0.41</td>
<td style="text-align: left;">0.53</td>
</tr>
<tr>
<td style="text-align: left;">Dater (Ye et al., 2023)</td>
<td style="text-align: left;">$\underline{29.47}$</td>
<td style="text-align: left;">$\underline{0.63}$</td>
<td style="text-align: left;">$\underline{0.41}$</td>
<td style="text-align: left;">$\underline{0.53}$</td>
</tr>
<tr>
<td style="text-align: left;">CHAIN-OF-TABLE (ours)</td>
<td style="text-align: left;">$\mathbf{3 2 . 6 1}(+3.14)$</td>
<td style="text-align: left;">$\mathbf{0 . 6 6}(+0.83)$</td>
<td style="text-align: left;">$\mathbf{0 . 4 4}(+0.83)$</td>
<td style="text-align: left;">$\mathbf{0 . 5 6}(+0.83)$</td>
</tr>
</tbody>
</table>
<h2>Example from FeTaQA</h2>
<p>Question: Who were the last two finishers in the 2000 Summer Olympics Mens 100 metre freestyle?
Answer: Russia's Denis Pimankov (49.36) and Australia's Chris Fydler (49.44) rounded out the finale.
Prediction: The last two finishers in the 2000 Summer Olympics Mens 100 metre freestyle were Chris Fydler and Denis Pimankov.
Results: ROUGE-1=0.33; ROUGE-2=0.12; ROUGE-L=0.11
Explanation: The generated response correctly answers the question but the sentence styles are different. From the metrics, we can see the ROUGE scores are below the average.</p>
<p>Figure 5: Result example of CHAIN-OF-TABLE on FeTaQA using the ROUGE scores as metrics, where the ROUGE metrics assign very low scores but the generated answers were correct.</p>
<h1>C Inference Parameters and Number of Demo Samples of Chain-of-Table</h1>
<p>We report the parameters and demo sample numbers we used in CHAIN-OF-TABLE in Table 7, 8 and 9. Overall, we annotate 29 samples and use them across different datasets. There are a large overlapping between the usage on different functions. For example, we use the same demo sample to introduce how to use f_add_column in the function DynamicPlan across different datasets. We guarantee that all demo samples are from the training set so they are unseen during testing. We argue that this further demonstrates our framework does not rely on a specific set of demos and can be well generalized to new datasets with the same prompts.</p>
<p>Table 7: LLM parameters and number of demo samples in CHAIN-OF-TABLE on WikiTQ</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Function</th>
<th style="text-align: center;">WikiTQ</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">temperature</td>
<td style="text-align: center;">top_p</td>
<td style="text-align: center;">decode_steps</td>
<td style="text-align: center;">n_samples</td>
<td style="text-align: center;">n_demos</td>
</tr>
<tr>
<td style="text-align: left;">DynamicPlan()</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">f_add_column()</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">f_select_row()</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">f_select_column()</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">f_group_by()</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">f_sort_by()</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">query()</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Table 8: LLM parameters and number of demo samples in CHAIN-OF-TABLE on TabFact</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Function</th>
<th style="text-align: center;">TabFact</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">temperature</td>
<td style="text-align: center;">top_p</td>
<td style="text-align: center;">decode_steps</td>
<td style="text-align: center;">n_samples</td>
<td style="text-align: center;">n_demos</td>
</tr>
<tr>
<td style="text-align: left;">DynamicPlan()</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">f_add_column()</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: left;">f_select_row()</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">f_select_column()</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">f_group_by()</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">f_sort_by()</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">query()</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4</td>
</tr>
</tbody>
</table>
<p>Table 9: LLM parameters and number of demo samples in CHAIN-OF-TABLE on FeTaQA</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Function</th>
<th style="text-align: center;">FeTaQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">temperature</td>
<td style="text-align: center;">top_p</td>
<td style="text-align: center;">decode_steps</td>
<td style="text-align: center;">n_samples</td>
<td style="text-align: center;">n_demos</td>
</tr>
<tr>
<td style="text-align: left;">DynamicPlan()</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">f_add_column()</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">f_select_row()</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">f_select_column()</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">f_group_by()</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">f_sort_by()</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">query()</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">8</td>
</tr>
</tbody>
</table>
<h1>D Tabular Format Encoding Comparison</h1>
<p>In alignment with prior studies Liu et al. (2023b; 2021); Jiang et al. (2022) and the baseline methods Cheng et al. (2022); Ye et al. (2023), we adopt PIPE encoding in Chain-of-TABLE (as shown in Appendix E). This decouples the performance gains of the proposed tabular CoT with atomic operations from the influence of various table formatting choices.</p>
<p>To further understand the impact of different encoding methods on table understanding performance, we conduct additional experiments using 3 additional table representations: HTML, TSV, and Markdown. For these experiments, we use End-to-End QA on WikiTQ with PaLM 2 as a running example. The results are shown in Table 10. These findings show that different tabular format encoding methods lead to different outcomes. Notably, the PIPE format adopted in our study yields the highest performance among the four encoding methods tested.</p>
<p>Table 10: Tabular format encoding comparison on WikiTQ with PaLM 2</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompting</th>
<th style="text-align: center;">Tabular Format Encoding</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PIPE</td>
<td style="text-align: center;">HTML</td>
<td style="text-align: center;">TSV</td>
<td style="text-align: center;">Markdown</td>
</tr>
<tr>
<td style="text-align: center;">End-to-End QA</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">58.0</td>
</tr>
</tbody>
</table>
<h2>E Prompts in Chain-of-Table</h2>
<h2>E. 1 DynamicPlan</h2>
<p>We illustrate the prompting method used by DynamicPlan(T,Q, chain) in Figure 6 where $T$ is the latest intermediate table and $Q$ is its corresponding question; chain is the list of operations performed on the table.</p>
<p>With DynamicPlan, the LLM can generate the rest of the operation chain for the current sample (Figure 6(c)). We denote the generated operations as $f_{i+1}\left(\right.$ args. $\left.<em i="i">{i+1}\right) \rightarrow \ldots \rightarrow$ E given that $f</em>$ is the most reliable generation among all operations in the generated chain. See Figure 9 for more detailed prompts.}$ is the last operation of the input open-ended operation chain. Although a complete chain is generated, we only consider the first generated operation, $f_{i+1}$, and ignore the rest of the generation including the arguments and remaining operations. $f_{i+1}$ is generated based on the latest intermediate table from the previous operations, while the generation of subsequent operations are not based on the most up-to-date intermediate table so there could be mistakes in the generated contents. Therefore, we believe $f_{i+1</p>
<h2>E. 2 GenerateArgs</h2>
<p>We illustrate the demonstration examples and prompts used by GenerateArgs (T, Q, f) in Figure 7 where $T$ is the latest intermediate table and $Q$ is its corresponding question; $f$ is the selected tabular operations. The detailed prompts for each operation and the regular expressions for extracting the generated arguments are as follows.</p>
<ul>
<li>f_add_column: See Figure 10.</li>
<li>f_select_row: See Figure 12.</li>
<li>f_select_column: See Figure 11.</li>
<li>f_group_by: See Figure 13.</li>
<li>f_sort_by: See Figure 14.</li>
</ul>
<h2>E. 3 Query</h2>
<p>We illustrate the prompts used by Query(T,Q) in Figure 8 where $T$ is the resulting table from CHAIN-OF-TABLE and $Q$ is the question. See Figure 15 for more detailed prompts.</p>
<h1>F IMPLEMENTATION DETAILS OF BASELINE METHODS</h1>
<p>We run Text-to-SQL and Binder using the official open-sourced code and prompts in https:// github.com/HKUNLP/Binder. We run Dater using the official open-sourced code and prompts in https://github.com/A1ibabaResearch/DAMO-ConvAI. We revise the code to use publicly available GPT 3.5, PaLM 2, and LLaMA 2 (Section 4) as the LLM backbone instead of the OpenAI Codex due to its inaccessibility. We report the detailed prompts used in other baseline methods as follows.</p>
<ul>
<li>End-to-End QA: See Figure 16.</li>
<li>Few-Shot QA: See Figure 17.</li>
<li>Chain-of-Thought: The demonstration samples of Chain-of-Thought for WikiTQ and TabFact are from Chen (2023) (https://github.com/wenhuchen/TableCoT). See Figure 18.
<img alt="img-4.jpeg" src="img-4.jpeg" /></li>
</ul>
<p>Figure 6: Illustration of DynamicPlan( $T, Q$, chain). Left: Overall prompt template and expected generation, including (a) demonstration of how atomic operations work, (b) demonstration of how to generate a complete operation chain to answer a given question, and (c) prompt for actual input table and its question, and its expected generation from the LLM (highlighted in green). Right: Examples and brief explanations of each part in the prompt and generation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt Template of GenerateArgs(T, Q, f)</th>
<th style="text-align: center;">// Parse LLM generation with pre-defined regular expressions to extract arguments.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(OPERATION_INSTRUCTION (f))</td>
<td style="text-align: center;">When $\mathbf{f}=\mathbf{f}$, add_column():</td>
</tr>
<tr>
<td style="text-align: center;">(SERIALISED_TABLE (T))</td>
<td style="text-align: center;">The answer is: f_add_column(Country). The value: ESP, RUS, ...</td>
</tr>
<tr>
<td style="text-align: center;">Question: (QUESTION (Q))</td>
<td style="text-align: center;">When $\mathbf{f}=\mathbf{f}$, select_row():</td>
</tr>
<tr>
<td style="text-align: center;">Explanation:</td>
<td style="text-align: center;">The answer is: f_select_row(row 1, row 2, row 3...)</td>
</tr>
<tr>
<td style="text-align: center;">(EXECARATION)</td>
<td style="text-align: center;">When $\mathbf{f}=\mathbf{f}$, select_column():</td>
</tr>
<tr>
<td style="text-align: center;">The answer is: (OPERATION_AND_ARGUMENTS)</td>
<td style="text-align: center;">The answer is: f_select_column(Country, Rank)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">When $\mathbf{f}=\mathbf{f}$, sort_by():</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The answer is: f_sort_by(Count). The order is from-large-to-small.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">When $\mathbf{f}=\mathbf{f}$, group_by():</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The answer is: f_group_by(Country)</td>
</tr>
</tbody>
</table>
<p>Figure 7: Illustration of GenerateArgs $(T, Q, f)$. After a specific operation $f$ is sampled by the LLM as the next operation, we ask the LLM to generate the required arguments by calling GenerateArgs. Then we parse the generation results of the LLM according to the pre-defined templates to extract the arguments.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: Illustration of Query $(T, Q)$. The resulting table from the operation chain serves as a proxy for the intermediate thoughts of reasoning, allowing us to directly generate the answer without providing the reasoning chain in textual format.</p>
<div class="codehilite"><pre><span></span><code><span class="o">=======================================</span><span class="w"> </span><span class="nt">Prompt</span><span class="w"> </span><span class="o">=========================================</span>
<span class="nt">If</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">table</span><span class="w"> </span><span class="nt">only</span><span class="w"> </span><span class="nt">needs</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">few</span><span class="w"> </span><span class="nt">rows</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">answer</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">question</span><span class="o">,</span><span class="w"> </span><span class="nt">we</span><span class="w"> </span><span class="nt">use</span><span class="w"> </span><span class="nt">f_select_row</span><span class="o">()</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">select</span>
<span class="nt">these</span><span class="w"> </span><span class="nt">rows</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">it</span><span class="o">.</span><span class="w"> </span><span class="nt">For</span><span class="w"> </span><span class="nt">example</span><span class="o">,</span>
<span class="c">/*</span>
<span class="c">col : Home team | Home Team Score | Away Team | Away Team Score | Venue | Crowd</span>
<span class="c">row 1 : st kilda | 13.12 (90) | melbourne | 13.11 (89) | moorabbin oval | 18836</span>
<span class="c">row 2 : south melbourne | 9.12 (66) | footscray | 11.13 (79) | lake oval | 9154</span>
<span class="c">row 3 : richmond | 20.17 (137) | fitzroy | 13.22 (100) | mcg | 27651</span>
<span class="c">*/</span>
<span class="nt">Question</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">Whose</span><span class="w"> </span><span class="nt">home</span><span class="w"> </span><span class="nt">team</span><span class="w"> </span><span class="nt">score</span><span class="w"> </span><span class="nt">is</span><span class="w"> </span><span class="nt">higher</span><span class="o">,</span><span class="w"> </span><span class="nt">richmond</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">st</span><span class="w"> </span><span class="nt">kilda</span><span class="o">?</span>
<span class="nt">Function</span><span class="o">:</span><span class="w"> </span><span class="nt">f_select_row</span><span class="o">(</span><span class="nt">row</span><span class="w"> </span><span class="nt">1</span><span class="o">,</span><span class="w"> </span><span class="nt">row</span><span class="w"> </span><span class="nt">3</span><span class="o">)</span>
<span class="nt">Explanation</span><span class="o">:</span><span class="w"> </span><span class="nt">The</span><span class="w"> </span><span class="nt">question</span><span class="w"> </span><span class="nt">asks</span><span class="w"> </span><span class="nt">about</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">home</span><span class="w"> </span><span class="nt">team</span><span class="w"> </span><span class="nt">score</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">richmond</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">st</span><span class="w"> </span><span class="nt">kilda</span><span class="o">.</span><span class="w"> </span><span class="nt">We</span><span class="w"> </span><span class="nt">need</span>
<span class="nt">to</span><span class="w"> </span><span class="nt">know</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">information</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">richmond</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">st</span><span class="w"> </span><span class="nt">kilda</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">row</span><span class="w"> </span><span class="nt">1</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">row</span><span class="w"> </span><span class="nt">3</span><span class="o">.</span><span class="w"> </span><span class="nt">We</span><span class="w"> </span><span class="nt">select</span><span class="w"> </span><span class="nt">row</span><span class="w"> </span><span class="nt">1</span>
<span class="nt">and</span><span class="w"> </span><span class="nt">row</span><span class="w"> </span><span class="nt">3</span><span class="o">.</span>
<span class="nt">If</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">table</span><span class="w"> </span><span class="nt">only</span><span class="w"> </span><span class="nt">needs</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">few</span><span class="w"> </span><span class="nt">columns</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">answer</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">question</span><span class="o">,</span><span class="w"> </span><span class="nt">we</span><span class="w"> </span><span class="nt">use</span>
<span class="nt">f_select_column</span><span class="o">()</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">select</span><span class="w"> </span><span class="nt">these</span><span class="w"> </span><span class="nt">columns</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">it</span><span class="o">.</span><span class="w"> </span><span class="nt">For</span><span class="w"> </span><span class="nt">example</span><span class="o">,</span>
<span class="nt">If</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">question</span><span class="w"> </span><span class="nt">asks</span><span class="w"> </span><span class="nt">about</span><span class="w"> </span><span class="nt">items</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">same</span><span class="w"> </span><span class="nt">value</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">number</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">these</span><span class="w"> </span><span class="nt">items</span><span class="o">,</span><span class="w"> </span><span class="nt">we</span><span class="w"> </span><span class="nt">use</span>
<span class="nt">f_group_by</span><span class="o">()</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">group</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">items</span><span class="o">.</span><span class="w"> </span><span class="nt">For</span><span class="w"> </span><span class="nt">example</span><span class="o">,</span>
<span class="nt">If</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">question</span><span class="w"> </span><span class="nt">asks</span><span class="w"> </span><span class="nt">about</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">order</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">items</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">column</span><span class="o">,</span><span class="w"> </span><span class="nt">we</span><span class="w"> </span><span class="nt">use</span><span class="w"> </span><span class="nt">f_sort_by</span><span class="o">()</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">sort</span>
<span class="nt">the</span><span class="w"> </span><span class="nt">items</span><span class="o">.</span><span class="w"> </span><span class="nt">For</span><span class="w"> </span><span class="nt">example</span><span class="o">,</span>
<span class="nt">Here</span><span class="w"> </span><span class="nt">are</span><span class="w"> </span><span class="nt">examples</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">using</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">operations</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">answer</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">question</span><span class="o">.</span>
<span class="c">/*</span>
<span class="c">col : Date | Division | League | Regular Season | Playoffs | Open Cup</span>
<span class="c">row 1 : 2001/01/02 | 2 | USL A-League | 4th, Western | Quarterfinals | Did not qualify</span>
<span class="c">row 2 : 2002/08/06 | 2 | USL A-League | 2nd, Pacific | 1st Round | Did not qualify</span>
<span class="c">row 5 : 2005/03/24 | 2 | USL First Division | 5th | Quarterfinals | 4th Round</span>
<span class="c">*/</span>
<span class="nt">Question</span><span class="o">:</span><span class="w"> </span><span class="nt">what</span><span class="w"> </span><span class="nt">was</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">last</span><span class="w"> </span><span class="nt">year</span><span class="w"> </span><span class="nt">where</span><span class="w"> </span><span class="nt">this</span><span class="w"> </span><span class="nt">team</span><span class="w"> </span><span class="nt">was</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">part</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">usl</span><span class="w"> </span><span class="nt">a-league</span><span class="o">?</span>
<span class="nt">Function</span><span class="w"> </span><span class="nt">Chain</span><span class="o">:</span><span class="w"> </span><span class="nt">f_add_column</span><span class="o">(</span><span class="nt">Year</span><span class="o">)</span><span class="w"> </span><span class="nt">-</span><span class="o">&gt;</span><span class="w"> </span><span class="nt">f_select_row</span><span class="o">(</span><span class="nt">row</span><span class="w"> </span><span class="nt">1</span><span class="o">,</span><span class="w"> </span><span class="nt">row</span><span class="w"> </span><span class="nt">2</span><span class="o">)</span><span class="w"> </span><span class="nt">-</span><span class="o">&gt;</span>
<span class="nt">f_select_column</span><span class="o">(</span><span class="nt">Year</span><span class="o">,</span><span class="w"> </span><span class="nt">League</span><span class="o">)</span><span class="w"> </span><span class="nt">-</span><span class="o">&gt;</span><span class="w"> </span><span class="nt">f_sort_by</span><span class="o">(</span><span class="nt">Year</span><span class="o">)</span><span class="w"> </span><span class="nt">-</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="nt">END</span><span class="o">&gt;</span>
<span class="c">/*</span>
<span class="c">col : Rank | Cyclist | Team | Time | UCI ProTour; Points | Country</span>
<span class="c">row 1 : 1 | Alejandro Valverde (ESP) | Caisse d&#39;Epargne | 5h 29&#39; 10&quot; | 40 | ESP</span>
<span class="c">row 2 : 2 | Alexandr Kolobnev (RUS) | Team CSC Saxo Bank | s.t. | 30 | RUS</span>
<span class="c">row 3 : 3 | Davide Rebellin (ITA) | Gerolsteiner | s.t. | 25 | ITA</span>
<span class="c">row 4 : 4 | Paolo Bettini (ITA) | Quick Step | s.t. | 20 | ITA</span>
<span class="c">row 5 : 5 | Franco Pellizotti (ITA) | Liquigas | s.t. | 15 | ITA</span>
<span class="c">row 6 : 6 | Denis Menchov (RUS) | Rabobank | s.t. | 11 | RUS</span>
<span class="c">row 7 : 7 | Samuel Sánchez (ESP) | Euskaltel-Euskadi | s.t. | 7 | ESP</span>
<span class="c">row 8 : 8 | Stéphane Goubert (FRA) | Ag2r-La Mondiale | + 2&quot; | 5 | FRA</span>
<span class="c">row 9 : 9 | Haimar Zubeldia (ESP) | Euskaltel-Euskadi | + 2&quot; | 3 | ESP</span>
<span class="c">row 10 : 10 | David Moncoutié (FRA) | Cofidis | + 2&quot; | 1 | FRA</span>
<span class="c">*/</span>
<span class="nt">Question</span><span class="o">:</span><span class="w"> </span><span class="nt">which</span><span class="w"> </span><span class="nt">country</span><span class="w"> </span><span class="nt">had</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">most</span><span class="w"> </span><span class="nt">cyclists</span><span class="w"> </span><span class="nt">finish</span><span class="w"> </span><span class="nt">within</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">top</span><span class="w"> </span><span class="nt">10</span><span class="o">?</span>
<span class="nt">The</span><span class="w"> </span><span class="nt">next</span><span class="w"> </span><span class="nt">operation</span><span class="w"> </span><span class="nt">must</span><span class="w"> </span><span class="nt">be</span><span class="w"> </span><span class="nt">one</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">f_select_row</span><span class="o">()</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">f_select_column</span><span class="o">()</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">f_group_by</span><span class="o">()</span>
<span class="nt">or</span><span class="w"> </span><span class="nt">f_sort_by</span><span class="o">().</span>
<span class="nt">Function</span><span class="w"> </span><span class="nt">Chain</span><span class="o">:</span><span class="w"> </span><span class="nt">f_add_column</span><span class="o">(</span><span class="nt">Country</span><span class="o">)</span><span class="w"> </span><span class="o">~&gt;</span>
<span class="o">=====================================</span><span class="w"> </span><span class="nt">Completion</span><span class="w"> </span><span class="o">=======================================</span>
<span class="nt">f_select_row</span><span class="o">(</span><span class="nt">row</span><span class="w"> </span><span class="nt">1</span><span class="o">,</span><span class="w"> </span><span class="nt">row</span><span class="w"> </span><span class="nt">10</span><span class="o">)</span><span class="w"> </span><span class="nt">-</span><span class="o">&gt;</span><span class="w"> </span><span class="nt">f_select_column</span><span class="o">(</span><span class="nt">Country</span><span class="o">)</span><span class="w"> </span><span class="nt">-</span><span class="o">&gt;</span><span class="w"> </span><span class="nt">f_group_by</span><span class="o">(</span><span class="nt">Country</span><span class="o">)</span><span class="w"> </span><span class="nt">-</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="nt">END</span><span class="o">&gt;</span>
</code></pre></div>

<p>Figure 9: DynamicPlan( $T, Q$, chain) Prompt used for WikiTQ</p>
<p>To answer the question, we can first use f_add_column() to add more columns to the table.
The added columns should have these data types:</p>
<ol>
<li>Numerical: the numerical strings that can be used in sort, sum</li>
<li>Datetype: the strings that describe a date, such as year, month, day</li>
<li>String: other strings
/<em>
col : Week | When | Kickoff | Opponent | Results; Final score | Results; Team record
row 1 : 1 | Saturday, April 13 | 7:00 p.m. | at Rhein Fire | W 27-21 | 1-0
row 2 : 2 | Saturday, April 20 | 7:00 p.m. | London Monarchs | W 37-3 | 2-0
row 3 : 3 | Sunday, April 28 | 6:00 p.m. | at Barcelona Dragons | W 33-29 | 3-0
</em>/
Question: what is the date of the competition with highest attendance?
The existing columns are: "Week", "When", "Kickoff", "Opponent", "Results; Final score", "Results; Team record", "Game site", "Attendance".
Explanation: the question asks about the date of the competition with highest score. Each row is about one competition. We extract the value from column "Attendance" and create a different column "Attendance number" for each row. The datatype is Numerical.
Therefore, the answer is: f_add_column(Attendance number). The value: 32092 | 34186 | 17503
/<em>
col : Rank | Lane | Player | Time
row 1 : | 5 | Olga Tereshkova (KAZ) | 51.86
row 2 : | 6 | Manjeet Kaur (IND) | 52.17
row 3 : | 3 | Asami Tanno (JPN) | 53.04
</em>/
Question: tell me the number of athletes from japan.
The existing columns are: Rank, Lane, Player, Time.
Explanation: the question asks about the number of athletes from japan. Each row is about one athlete. We need to know the country of each athlete. We extract the value from column "Player" and create a different column "Country of athletes" for each row. The datatype is String.
Therefore, the answer is: f_add_column(Country of athletes). The value: KAZ | IND | JPN
Figure 10: Demos used for GenerateArgs(T, Q, f_add_column). We use the regular expression: f_add_column( (. * )). The value: (. *) to extract the arguments from the generated text.</li>
</ol>
<p>Use f_select_column() to filter out useless columns in the table according to information in the statement and the table.
/<em>
{
"table_caption": "south wales derby",
"columns": ["competition", "total matches", "cardiff win", "draw", "swansea win"],
"table_column_priority": [
["competition", "league", "fa cup", "league cup"],
["total matches", "55", "2", "5"],
["cardiff win", "19", "0", "2"],
["draw", "16", "27", "0"],
["swansea win", "20", "2", "3"]
]
</em>/
statement : there are no cardiff wins that have a draw greater than 27.
similar words link to columns :
no cardiff wins $-&gt;$ cardiff win
a draw $-&gt;$ draw
column value link to columns :
$27-&gt;$ draw
semantic sentence link to columns :
None
The answer is : f_select_column([cardiff win, draw])
Figure 11: Demos used for GenerateArgs(T, Q, f_select_column). We use the regular expression: f_select_column([(. *)]) to extract the arguments from the generated text.</p>
<p>Using f_select_row() to select relevant rows in the given table that support or oppose the statement.
Please use f_select_row([+]) to select all rows in the table.
/<em>
table caption : 1972 vfl season.
col : home team | home team score | away team | away team score | venue | crowd
row 1 : st kilda | 13.12 (90) | melbourne | 13.11 (89) | moorabbin oval | 18836
row 2 : south melbourne | 9.12 (66) | footscray | 11.13 (79) | lake oval | 9154
row 3 : richmond | 20.17 (137) | fitzroy | 13.22 (100) | mcg | 27651
row 4 : geelong | 17.10 (112) | collingwood | 17.9 (111) | kardinia park | 23108
row 5 : north melbourne | 8.12 (60) | carlton | 23.11 (149) | arden street oval | 11271
row 6 : hawthorn | 15.16 (106) | essendon | 12.15 (87) | vfl park | 36749
</em>/
statement : what is the away team with the highest score?
explain : the statement want to ask the away team of highest away team score. the highest
away team score is 23.11 (149). it is on the row 5 .so we need row 5 .
The answer is : f_select_row([row 5])</p>
<p>Figure 12: Demos used for GenerateArgs(T, Q, f_select_row). We use the regular expression: f_select_row([(.*)]) to extract the arguments from the generated text.</p>
<p>To answer the question, we can first use f_group_by() to group the values in a column.
/<em>
col : Rank | Lane | Athlete | Time | Country
row 1 : 1 | 6 | Manjeet Kaur (IND) | 52.17 | IND
row 2 : 2 | 5 | Olga Tereshkova (KAZ) | 51.86 | KAZ
row 3 : 3 | 4 | Pinki Pramanik (IND) | 53.06 | IND
row 4 : 4 | 1 | Tang Xiaoyin (CHN) | 53.66 | CHN
row 5 : 5 | 8 | Marina Maslyonko (KAZ) | 53.99 | KAZ
</em>/
Question: tell me the number of athletes from japan.
The existing columns are: Rank, Lane, Athlete, Time, Country.
Explanation: The question asks about the number of athletes from India. Each row is about an athlete. We can group column "Country" to group the athletes from the same country. Therefore, the answer is: f_group_by(Country).</p>
<p>Figure 13: Demos used for GenerateArgs(T, Q, f_group_by). We use the regular expression: f_group_by((. *)) to extract the arguments from the generated text.</p>
<p>To answer the question, we can first use f_sort_by() to sort the values in a column to get the
order of the items. The order can be "large to small" or "small to large".
The column to sort should have these data types:</p>
<ol>
<li>Numerical: the numerical strings that can be used in sort</li>
<li>DateType: the strings that describe a date, such as year, month, day</li>
<li>String: other strings
/<em>
col : Position | Club | Played | Points | Wins | Draws | Losses | Goals for | Goals against
row 1 : 1 | Malaga CF | 42 | 79 | 22 | 13 | 7 | 72 | 47
row 10 : 10 | CP Merida | 42 | 59 | 15 | 14 | 13 | 48 | 41
row 3 : 3 | CD Numancia | 42 | 73 | 21 | 10 | 11 | 68 | 40
</em>/
Question: what club placed in the last position?
The existing columns are: Position, Club, Played, Points, Wins, Draws, Losses, Goals for, Goals against
Explanation: the question asks about the club in the last position. Each row is about a club. We need to know the order of position from last to front. There is a column for position and the column name is Position. The datatype is Numerical.
Therefore, the answer is: f_sort_by(Position), the order is "large to small".</li>
</ol>
<p>Figure 14: Demos used for GenerateArgs (T, Q, f_sort_by). We use the regular expression: f_sort_by((. <em>)), the order is "(. </em>)". to extract the arguments from the generated text.</p>
<p>Here is the table to answer this question. Please understand the table and answer the question:</p>
<div class="codehilite"><pre><span></span><code><span class="o">/*</span>
<span class="n">col</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">Rank</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">City</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Passengers</span><span class="w"> </span><span class="n">Number</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Ranking</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Airline</span>
<span class="n">row</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">United</span><span class="w"> </span><span class="n">States</span><span class="p">,</span><span class="w"> </span><span class="n">Los</span><span class="w"> </span><span class="n">Angeles</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">14749</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Alaska</span><span class="w"> </span><span class="n">Airlines</span>
<span class="n">row</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">United</span><span class="w"> </span><span class="n">States</span><span class="p">,</span><span class="w"> </span><span class="n">Houston</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">5465</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">United</span><span class="w"> </span><span class="n">Express</span>
<span class="n">row</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Canada</span><span class="p">,</span><span class="w"> </span><span class="n">Calgary</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">3761</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Air</span><span class="w"> </span><span class="n">Transat</span><span class="p">,</span><span class="w"> </span><span class="n">WestJet</span>
<span class="n">row</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Canada</span><span class="p">,</span><span class="w"> </span><span class="n">Saskatoon</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">2282</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="o">|</span>
<span class="n">row</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Canada</span><span class="p">,</span><span class="w"> </span><span class="n">Vancouver</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">2103</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Air</span><span class="w"> </span><span class="n">Transat</span>
<span class="n">row</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">United</span><span class="w"> </span><span class="n">States</span><span class="p">,</span><span class="w"> </span><span class="n">Phoenix</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">1829</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">US</span><span class="w"> </span><span class="n">Airways</span>
<span class="n">row</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Canada</span><span class="p">,</span><span class="w"> </span><span class="n">Toronto</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">1202</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Air</span><span class="w"> </span><span class="n">Transat</span><span class="p">,</span><span class="w"> </span><span class="n">CanJet</span>
<span class="n">row</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Canada</span><span class="p">,</span><span class="w"> </span><span class="n">Edmonton</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">110</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">|</span>
<span class="n">row</span><span class="w"> </span><span class="mi">9</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">9</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">United</span><span class="w"> </span><span class="n">States</span><span class="p">,</span><span class="w"> </span><span class="n">Oakland</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">107</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">|</span>
<span class="o">*/</span>
<span class="n">Question</span><span class="p">:</span><span class="w"> </span><span class="n">how</span><span class="w"> </span><span class="n">many</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">passengers</span><span class="w"> </span><span class="n">flew</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">los</span><span class="w"> </span><span class="n">angeles</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">saskatoon</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">manzanillo</span>
<span class="n">airport</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="mi">20137</span>
<span class="n">The</span><span class="w"> </span><span class="n">anwser</span><span class="w"> </span><span class="n">is</span><span class="p">:</span><span class="w"> </span><span class="mi">12467</span>
<span class="n">Here</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">table</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">question</span><span class="p">.</span><span class="w"> </span><span class="n">Please</span><span class="w"> </span><span class="n">understand</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">table</span><span class="w"> </span><span class="nb">and</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">the</span>
<span class="n">question</span><span class="p">:</span>
<span class="o">/*</span>
<span class="n">col</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">Rank</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Country</span>
<span class="n">row</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">ESP</span>
<span class="n">row</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">RUS</span>
<span class="n">row</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">ITA</span>
<span class="n">row</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">ITA</span>
<span class="n">row</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">ITA</span>
<span class="n">row</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">RUS</span>
<span class="n">row</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">ESP</span>
<span class="n">row</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">FRA</span>
<span class="n">row</span><span class="w"> </span><span class="mi">9</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">9</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">ESP</span>
<span class="n">row</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">FRA</span>
<span class="o">*/</span>
<span class="n">Group</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">rows</span><span class="w"> </span><span class="n">according</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">column</span><span class="w"> </span><span class="s">&quot;Country&quot;</span><span class="p">:</span>
<span class="o">/*</span>
<span class="n">Group</span><span class="w"> </span><span class="n">ID</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Country</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Count</span>
<span class="mi">1</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">ITA</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">3</span>
<span class="mi">2</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">ESP</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">3</span>
<span class="mi">3</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">RUS</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">2</span>
<span class="mi">4</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">FRA</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">2</span>
<span class="o">*/</span>
<span class="n">Question</span><span class="p">:</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">country</span><span class="w"> </span><span class="n">had</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">most</span><span class="w"> </span><span class="n">cyclists</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">top</span><span class="w"> </span><span class="mi">10</span>?
<span class="n">The</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">is</span><span class="p">:</span>
<span class="o">======================================</span><span class="w"> </span><span class="n">Completion</span><span class="w"> </span><span class="o">======================================</span>
<span class="n">Italy</span><span class="p">.</span>
</code></pre></div>

<p>Figure 15: Prompt Example used for Query(T, Q)</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://ai.meta.com/llama/
${ }^{4}$ Dater Ye et al. (2023) with OpenAI Codex LLM achieves $65.9 \%$ and $85.6 \%$ accuracy on WikiTQ and TabFact, respectively. It also achieves 27.96 in BLEU, 0.62 in ROUGE-1, 0.40 in ROUGE-2, and 0.52 in ROUGE-L on FeTaQA. However, because Codex is no longer publicly available, we do not compare CHAIN-OF-TABLE with Dater with Codex.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
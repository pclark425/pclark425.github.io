<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2078 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2078</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2078</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-281666918</p>
                <p><strong>Paper Title:</strong> Streamline automated biomedical discoveries with agentic bioinformatics</p>
                <p><strong>Paper Abstract:</strong> Abstract The emergence of artificial intelligence agents powered by large language models marks a transformative shift in computational biology. In this new paradigm, autonomous, adaptive, and intelligent agents are deployed to tackle complex biological challenges, leading to a new research field named agentic bioinformatics. Here, we explore the core principles, evolving methodologies, and diverse applications of agentic bioinformatics. We examine how agentic bioinformatics systems work synergistically to facilitate data-driven decision-making and enable self-directed exploration of biological datasets. Furthermore, we highlight the integration of agentic frameworks in key areas such as personalized medicine, drug discovery, and synthetic biology, illustrating their potential to revolutionize healthcare and biotechnology. In addition, we address the ethical, technical, and scalability challenges associated with agentic bioinformatics, identifying key opportunities for future advancements. By emphasizing the importance of interdisciplinary collaboration and innovation, we envision agentic bioinformatics as a major force in overcoming the grand challenges of modern biology, ultimately advancing both research and clinical applications.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2078.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2078.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioMANIA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioMANIA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-augmented pipeline that integrates large language models with existing Python bioinformatics APIs to automate high-throughput sequencing data analysis, enabling code-free workflows but remaining sensitive to API quality and LLM hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BioMANIA</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM + API orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>genomics / high-throughput sequencing analysis</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>analysis code, analysis tables, processed omics results</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>incremental (automation / synthesis of existing tools rather than creation of novel biological entities)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-driven interpretation of user instructions to generate and sequence API calls and code using existing bioinformatics libraries</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Relies on downstream third-party tool execution and user inspection; no standardized ground-truth validation reported</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Qualitative reports of simplifying complex pipelines and enabling code-free analyses; no quantitative success rates provided</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not reported quantitatively; limitations noted when API docs are poor or ambiguous, implying validation degrades with tool/annotation novelty</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper reports generation (code/API calls) can succeed syntactically but validation is vulnerable to external tool failures and LLM hallucination during API prediction; generation outpaces robust validation when APIs are unreliable</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified; described susceptibility to failures when confronted with poorly documented or novel APIs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Relies on successful execution of third-party tools and absence of runtime errors as a proxy for validity</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Often required (manual intervention common when APIs fail or when LLM hallucinates)</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (bioinformatics pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>No rigorous mitigation reported beyond recommending high-quality documentation and careful tool selection; notes susceptibility to LLM hallucinations</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors note hallucinations during API prediction and dependency on external tool quality leading to validation failures despite generated code</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None provided</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2078.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2078.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BIA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioInformatics Agent (BIA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive LLM-based agent for automated single-cell RNA-seq (scRNA-seq) analysis that interacts via text, executing pipelines from data retrieval to report generation but exhibits incomplete experimental designs and unstable tool recommendations in zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BioInformatics Agent (BIA): unleashing the power of large language models to reshape bioinformatics workflow</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BIA</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-driven agent</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>single-cell transcriptomics (scRNA-seq)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>analysis pipelines, figures, reports, experimental design suggestions</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel (automating end-to-end scRNA-seq workflows and reporting)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM planning and API invocation to select and run scRNA-seq processing tools</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Autonomous execution followed by internal LLM-based summary; manual expert review and intervention recommended due to incomplete designs</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Qualitatively able to run entire pipelines; exhibits frequent generation of incomplete protocols (e.g., missing subcluster annotation)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Zero-shot dynamic workflows produce more incomplete or incorrect outputs, indicating validation/performance degrades on novel/unseen workflows</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation can propose full pipelines but validation/self-correction is weak; generation capability exceeds autonomous validation and refinement</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Demonstrated instability in zero-shot scenarios and inconsistent tool recommendations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Implicitly uses successful execution and generated summaries as proxies; lacks rigorous quantitative validation</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Frequent; persistent need for manual intervention</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Noted need for improved autonomous refinement capabilities and domain knowledge injection; no validated mitigation tested</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Reported frequent incomplete experimental designs and inconsistent recommendations requiring manual correction</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None provided</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2078.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2078.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoBA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoBA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous system for multi-omics data analysis that proposes analysis plans, generates and executes code, and includes an automated code repairing module, reported robust across a set of test cases but lacking broad generalizability validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoBA</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-driven pipeline with automated code generation/repair</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-omics data analysis</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>analysis plans, executable code, processed results and tables (e.g., differential expression tables)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>incremental to moderately novel (automated end-to-end analysis and self-repair of code)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM planning + code generation + automated code repairing and execution</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Execution of generated pipelines on provided datasets and inspection of outputs; reported robustness across test cases but no external benchmark comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported robust performance across 40 test cases (qualitative); no numeric accuracy metrics provided</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Authors caution that generalizability requires further validation across diverse tasks; performance may drop with tasks outside tested set</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation (code + plan) is strong on tested cases but validation/generalizability across broader bioinformatics tasks is unproven, indicating potential generation>validation gap for novel tasks</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported quantitatively; generalizability not established</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Success on internal test cases used as proxy for validity</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended for novel datasets/tasks</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Automated code repair module to reduce runtime failures; no rigorous external validation reported</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors note need for further validation due to diversity of bioinformatics tasks despite internal robustness</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Performance reported as robust across 40 cases, suggesting generation and execution can be reliable within tested scope</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2078.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2078.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BRAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BRAD</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automation system comprising modules for literature search, software generation/execution, and database searches to automate tasks like gene enrichment and biomarker pipeline creation; effectiveness remains unquantified due to absence of benchmark comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic biomarker discovery and enrichment with BRAD</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BRAD</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-driven modular automation system</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomarker discovery and enrichment analysis</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>literature summaries, code, biomarker candidate lists and enrichment analyses</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>incremental (automation and orchestration of standard analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Specialized modules (LAB NOTEBOOK, SOFTWARE, DIGITAL LIBRARY) coordinated by LLM agents to generate code and search results</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not reported; no benchmark comparisons provided to quantify accuracy or success rate</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported functional automation of tasks in examples; lacks quantitative metrics</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not evaluated; absence of benchmarks prevents assessment</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation capability described, but validation performance unquantified, indicating uncertain gap</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>None reported</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended given lack of quantified validation</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Not provided; authors highlight absence of benchmarks as limitation</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Explicit statement that effectiveness remains unquantified due to absence of benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None provided</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2078.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2078.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-ABC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LM-ABC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-assisted system for enzyme engineering that dynamically selects specialized tools for tasks like binding site extraction and molecular dynamics, accelerating enzyme engineering workflows but facing implementation challenges when generated code or workflows fail due to integration issues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A language model assistant for biocatalysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LM-ABC</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM + tool selection orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>enzyme engineering / protein design</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>tool chains, code snippets, engineered enzyme suggestions and optimizations</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel (automated orchestration across specialized enzyme engineering tools)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-driven dynamic tool selection and pipeline generation combining domain-specific modules</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validation depends on execution of selected external tools and integration tests; many failures originate from integration rather than conceptual generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported acceleration of workflows qualitatively; no quantitative success metrics provided</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Implementation reliability (validation) is sensitive to tool integration and may degrade for novel/uncommon tool combinations</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation of strategies is strong but validation often fails due to incomplete or erroneous tool integration, creating a practical gap</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Successful execution of downstream simulations or MD runs used as a proxy</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Often required when integration issues arise</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical with physical simulation components</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Recommend robust interoperability validation and error handling for generated code/workflows</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors report failures stemming from incomplete or erroneous tool integration despite plausible generated workflows</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None provided</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2078.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2078.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRISPR-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CRISPR-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent to automate CRISPR experimental design including guide RNA design, delivery method suggestions, and validation protocols; democratizes design but depends on curated genomic resources and faces species-specific data gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CRISPR-GPT: an LLM agent for automated design of gene-editing experiments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CRISPR-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-driven design agent</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>genome editing / CRISPR experimental design</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>gRNA designs, delivery strategy suggestions, validation protocol outlines</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel (automated design assistance for CRISPR experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-guided retrieval plus heuristic generation informed by curated biological databases</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Relies on curated databases and possibly in silico checks; empirical/experimental validation limited and contingent on resource completeness</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported to streamline design for nonexperts; no numerical success metrics provided</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Performance tied closely to coverage of underlying databases; for underrepresented species or novel genes, reliability decreases</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation can produce plausible designs, but validation capacity is limited by database completeness and species-specific annotations, creating a gap for novel targets</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degrades for underrepresented organisms or emerging gene targets</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Database coverage and annotation completeness used implicitly as proxies for validation</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended especially for non-model organisms and novel targets</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (experimental biology)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Iterative updates to knowledge resources and curation recommended</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors note reliance on curated databases introduces limitations for species-specific incomplete genomic data</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None provided</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2078.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2078.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpatialAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SpatialAgent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous agent for spatial biology that designs experiments, analyzes multimodal spatial data, and generates hypotheses; performs strongly on large-scale tissue and niche annotation but tends to rely on common training-data annotation patterns rather than fully adapting to novel contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SpatialAgent: an autonomous AI agent for spatial biology</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SpatialAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-driven autonomous analysis agent (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>spatial genomics / spatial biology</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>tissue/cell niche annotations, experimental designs, multimodal analyses, hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel (automated annotation and hypothesis generation in spatial contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Adaptive reasoning engine with dynamic tool integration and retrieval from external databases</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparative assessment against expert annotation; qualitative performance description</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported to perform strongly in large-scale spatially resolved cell and tissue niche annotation (qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Tendency to reproduce common annotation patterns from training data suggests validation degrades for truly novel biological contexts</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generative annotation is strong on in-distribution patterns, but validation/adaptation to novel contexts lags (generation biased by training priors)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Relies on training distribution; limited adaptation to novel biological contexts reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Agreement with domain experts used implicitly for validation</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended especially for novel tissue types or atypical samples</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Adaptive reasoning and dynamic tool integration to improve context adaptation; still shows training-data bias</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Paper notes reliance on common annotation patterns rather than true adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Strong performance on large-scale annotation tasks indicates generation can match expert-level outputs in-distribution</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2078.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2078.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CellAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CellAgent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven multi-agent framework for automated single-cell analysis with Planner, Executor, and Evaluator roles and a self-iterative optimization mechanism; faces limitations in memory size, LLM performance on long contexts, and restricted self-evaluation flexibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CellAgent: an LLM-driven multi-agent framework for automated single-cell data analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CellAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent LLM framework</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>single-cell transcriptomics</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>analysis workflows, processed results, evaluations and optimization suggestions</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel (multi-agent orchestration and self-optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Planner designs workflows, Executor runs tasks, Evaluator performs assessments and guides self-iterative optimization</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>GPT-4-based evaluation for autonomous optimization and automated evaluations incorporated into self-iteration; limited flexibility for diverse objectives</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Capable of full pipeline orchestration; self-optimization improves over iterations but constrained by evaluator scope</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Evaluator provides autonomous assessments but lacks flexibility; not quantified numerically</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Performance and self-evaluation are less effective for specialized or novel analytic objectives; long-context memory issues degrade performance</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation and planning are advanced, but autonomous validation/evaluation capabilities are restricted, requiring manual tuning for specialized tasks</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Memory and evaluator limitations reduce robustness for OOD tasks</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Automated evaluator scores (GPT-4-based) used as proxy for correctness</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Often required for specialized objectives and to adjust evaluator settings</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Self-iterative optimization and automated evaluation modules; limited effectiveness without human tuning</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Restricted self-evaluation capability and need for manual adjustment shows generation outpacing validation flexibility</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Self-optimization mechanism demonstrates some capacity to close gaps within scoped tasks</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2078.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2078.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bio-Copilot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bio-Copilot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent copilot architecture that distributes tasks based on agent expertise with a coordination layer and real-time feedback, improving collaborative efficiency but incurring high computational and energy costs and facing multi-agent prompt engineering challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bio-Copilot</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent LLM orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>large-scale omics analysis / general bioinformatics workflows</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>task plans, generated code, analysis outputs and coordination decisions</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>incremental to moderately novel (multi-agent orchestration at scale)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Expertise-based task distribution and coordination with real-time feedback loops</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Cross-evaluation among agents and coordination feedback; details of quantitative validation not provided</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported improved collaborative efficiency qualitatively; no numeric metrics provided</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Prompt engineering and coordination degrade when instructions are ambiguous or context-poor, implying validation suffers for novel/ambiguous tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Parallel task execution and cross-evaluation improve internal validation but at high computational cost; generation may outpace reliable validation for resource-constrained deployments</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified; prompt sensitivity suggests fragility OOD</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Agent cross-evaluation and real-time feedback used as proxy metrics</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended, especially for ambiguous prompts and high-stakes tasks</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Multi-agent cross-evaluation and coordination layers proposed; trade-offs with computational cost noted</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors highlight computational/energy cost and prompt engineering challenges that limit validation robustness</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Cross-evaluation mechanism provides some internal validation gains</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>High (authors note significant computational and energy costs for competitive learning/cross-evaluation), but no numeric ratio provided</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2078.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2078.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProtAgent(s)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProtAgent / ProtAgents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multi-agent frameworks aimed at protein design and discovery that combine role specialization with automatic tool selection; effective for single-objective predictions but constrained when optimizing competing design requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ProtAgent(s)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent LLM + predictive models + physics-informed components</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>protein design / structural biology</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>designed protein sequences/constructs, predicted properties, suggested assays</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel to highly novel depending on task (novel sequences possible)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Planner-Executor-Critic structure selecting external tools and employing data-driven end-to-end deep learning models for prediction</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Tool-based predictions, simulation (physics or ML-based), and critic assessment; limited multiobjective optimization validation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Capable of producing designs and predictions; constrained in multiobjective optimization scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Effective for in-scope properties but struggles when designs must balance competing objectives (novel complex designs reduce validation reliability)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation of candidate designs can outpace ability to validate multiple competing objectives simultaneously, creating a gap for complex novel designs</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified; multiobjective and OOD challenges noted</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Predictive scores from models and simulation plausibility checks</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended for multiobjective or high-novelty designs</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical with physics-informed components</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Dynamic tool selection and critic evaluation; limited in addressing multiobjective optimization challenges</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors explicitly state difficulty in optimizing competing design requirements simultaneously</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Successful single-objective predictions demonstrate generation and validation align for simpler tasks</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2078.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2078.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>The Virtual Lab</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Virtual Lab</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collaborative multi-agent system led by a principal investigator AI agent and human researcher that designed and experimentally validated engineered nanobodies against SARS-CoV-2 variants, demonstrating real wet-lab validation of AI-generated molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The virtual lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>The Virtual Lab</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent LLM orchestration + human-in-loop + wet-lab validation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>protein binder (nanobody) design / experimental molecular biology</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>engineered nanobody sequences and designs, experimental assays/results</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>highly novel (designs validated experimentally against emerging viral variants)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Multi-agent decomposition (chemistry, CS agents etc.) with design proposals synthesized into candidate nanobodies</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical wet-lab validation including expression, solubility assays, and binding assays against spike RBD variants</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Generated a set of engineered nanobodies; 92 engineered nanobodies were validated experimentally</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Over 90% of the 92 validated nanobodies exhibited expression and solubility; two showed unique binding to JN.1 and KP.3 spike RBD variants</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>High in-distribution success (expression/solubility) but system required repeated refinements and human intervention for optimal results; novel-binding discoveries (2 nanobodies) indicate capability for true novelty with experimental confirmation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>This system demonstrates generation accompanied by direct experimental validation; however, generation required iterative refinement and human oversight to reach high experimental validation rates, indicating generation alone was insufficient without human-guided validation loops</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Demonstrated ability to find binders to recent variants (OOD relative to earlier training data), but needed human corrections for deprecated tool choices and code fixes</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Expression and solubility assays, binding assays used as direct validation metrics rather than proxies</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Frequent: repeated refinements and human intervention were required during optimization and to correct deprecated tool recommendations</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (experimental molecular biology)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Human-in-the-loop oversight, iterative refinement cycles, and repeated experimental testing to ensure robustness</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Agents sometimes recommended outdated tools and required human intervention to correct deprecated code; repeated refinements increased time cost</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>High experimental validation rates (>90% expression/solubility) show that generation plus iterative validation can yield robust experimental outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2078.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2078.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioDiscoveryAgent / Biodiscoveryagent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioDiscoveryAgent (Biodiscoveryagent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI agent for designing genetic perturbation experiments that automates aspects of experimental design and reasoning about outcomes; mentioned as a recent system with limited reported quantitative validation in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Biodiscoveryagent: An AI agent for designing genetic perturbation experiments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BioDiscoveryAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-driven design agent</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>genetic perturbation design / experimental planning</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>experimental designs, suggested perturbations, predicted outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel (automated experimental design)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-based reasoning and planning over genetic perturbation hypothesis space</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not detailed in review; likely depends on experimental follow-up or domain expert review</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not quantified in the review</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Mentioned as capable of design but no detailed validation metrics provided, suggesting potential generation-validation gap not quantified</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Not detailed</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Lack of reported validation metrics indicates validation capabilities are not well quantified in literature cited</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None provided</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2078.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e2078.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>General LLM agents (LLMs / LLM-driven agents)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model-driven AI agents (e.g., GPT-4, GPT-4o, Claude, Llama variants) as used across agentic systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs serve as foundational reasoning and planning modules in many agentic bioinformatics systems; they generate hypotheses, code, experimental designs and narrative explanations but are prone to hallucination and temporal knowledge cutoff issues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM (e.g., GPT-4, GPT-4o, Claude 3.5 Sonnet, Llama3 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model / transformer</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>broad (bioinformatics, experimental design, molecular design, spatial biology)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual hypotheses, code, experiment plans, molecule/protein design proposals, annotations</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>varies from in-distribution synthesis to moderately/highly novel outputs depending on prompting and retrieval augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Pattern extrapolation and recombination of learned features with retrieval-augmented generation and multi-agent orchestration in many systems</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Varies by systemcommon methods include tool execution, simulation, multi-agent cross-validation, expert review, and empirical wet-lab testing in some cases</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>High fluency and ability to produce plausible outputs; specific success rates depend on downstream system integration</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Hallucination and factual errors increase for novel or OOD prompts; temporal knowledge cutoffs cause outdated tool recommendations</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Consistently reported that generation capabilities exceed robust autonomous validation; many systems require human oversight or additional grounding</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Some systems propose confidence scoring and uncertainty quantification as needed, but adoption is inconsistent across cited systems</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Generally poor when faced with novel biological claims; medical LLMs shown to produce clinically unsupported content with fluent language (cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degrades; cited studies and examples (e.g., outdated tool recommendations, reliance on common annotation patterns) illustrate fragility OOD</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Plausibility, coherence, API execution success, and agreement across agents often used as proxies</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Frequent for high-stakes or novel outputs</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>varies by application but typically empirical in bio domains</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Retrieval-augmented generation, multi-agent cross-validation, explainability tools, retrieval from curated databases, and human-in-loop processes recommended</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Multiple examples across survey where hallucinations, outdated tool recommendations, and insufficient autonomous refinement are reported</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Systems like The Virtual Lab show that with iterative refinement and wet-lab validation, LLM-generated designs can achieve high empirical success</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The virtual lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation <em>(Rating: 2)</em></li>
                <li>ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning <em>(Rating: 2)</em></li>
                <li>CellAgent: an LLM-driven multi-agent framework for automated single-cell data analysis <em>(Rating: 2)</em></li>
                <li>CRISPR-GPT: an LLM agent for automated design of gene-editing experiments <em>(Rating: 2)</em></li>
                <li>Automatic biomarker discovery and enrichment with BRAD <em>(Rating: 1)</em></li>
                <li>A language model assistant for biocatalysis <em>(Rating: 1)</em></li>
                <li>An AI agent for fully automated multi-omic analyses <em>(Rating: 2)</em></li>
                <li>SpatialAgent: an autonomous AI agent for spatial biology <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2078",
    "paper_id": "paper-281666918",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "BioMANIA",
            "name_full": "BioMANIA",
            "brief_description": "An LLM-augmented pipeline that integrates large language models with existing Python bioinformatics APIs to automate high-throughput sequencing data analysis, enabling code-free workflows but remaining sensitive to API quality and LLM hallucinations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "BioMANIA",
            "system_type": "LLM + API orchestration",
            "scientific_domain": "genomics / high-throughput sequencing analysis",
            "output_type": "analysis code, analysis tables, processed omics results",
            "novelty_level": "incremental (automation / synthesis of existing tools rather than creation of novel biological entities)",
            "generation_method": "LLM-driven interpretation of user instructions to generate and sequence API calls and code using existing bioinformatics libraries",
            "validation_method": "Relies on downstream third-party tool execution and user inspection; no standardized ground-truth validation reported",
            "generation_performance": "Qualitative reports of simplifying complex pipelines and enabling code-free analyses; no quantitative success rates provided",
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not reported quantitatively; limitations noted when API docs are poor or ambiguous, implying validation degrades with tool/annotation novelty",
            "generation_validation_comparison": "Paper reports generation (code/API calls) can succeed syntactically but validation is vulnerable to external tool failures and LLM hallucination during API prediction; generation outpaces robust validation when APIs are unreliable",
            "uncertainty_quantification": null,
            "calibration_quality": null,
            "out_of_distribution_performance": "Not quantified; described susceptibility to failures when confronted with poorly documented or novel APIs",
            "validation_proxy_metrics": "Relies on successful execution of third-party tools and absence of runtime errors as a proxy for validity",
            "human_validation_required": true,
            "human_validation_frequency": "Often required (manual intervention common when APIs fail or when LLM hallucinates)",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (bioinformatics pipelines)",
            "gap_mitigation_strategies": "No rigorous mitigation reported beyond recommending high-quality documentation and careful tool selection; notes susceptibility to LLM hallucinations",
            "evidence_supporting_gap": "Authors note hallucinations during API prediction and dependency on external tool quality leading to validation failures despite generated code",
            "evidence_contradicting_gap": "None provided",
            "computational_cost_ratio": null,
            "uuid": "e2078.0"
        },
        {
            "name_short": "BIA",
            "name_full": "BioInformatics Agent (BIA)",
            "brief_description": "An interactive LLM-based agent for automated single-cell RNA-seq (scRNA-seq) analysis that interacts via text, executing pipelines from data retrieval to report generation but exhibits incomplete experimental designs and unstable tool recommendations in zero-shot settings.",
            "citation_title": "BioInformatics Agent (BIA): unleashing the power of large language models to reshape bioinformatics workflow",
            "mention_or_use": "mention",
            "system_name": "BIA",
            "system_type": "LLM-driven agent",
            "scientific_domain": "single-cell transcriptomics (scRNA-seq)",
            "output_type": "analysis pipelines, figures, reports, experimental design suggestions",
            "novelty_level": "moderately novel (automating end-to-end scRNA-seq workflows and reporting)",
            "generation_method": "LLM planning and API invocation to select and run scRNA-seq processing tools",
            "validation_method": "Autonomous execution followed by internal LLM-based summary; manual expert review and intervention recommended due to incomplete designs",
            "generation_performance": "Qualitatively able to run entire pipelines; exhibits frequent generation of incomplete protocols (e.g., missing subcluster annotation)",
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Zero-shot dynamic workflows produce more incomplete or incorrect outputs, indicating validation/performance degrades on novel/unseen workflows",
            "generation_validation_comparison": "Generation can propose full pipelines but validation/self-correction is weak; generation capability exceeds autonomous validation and refinement",
            "uncertainty_quantification": null,
            "calibration_quality": null,
            "out_of_distribution_performance": "Demonstrated instability in zero-shot scenarios and inconsistent tool recommendations",
            "validation_proxy_metrics": "Implicitly uses successful execution and generated summaries as proxies; lacks rigorous quantitative validation",
            "human_validation_required": true,
            "human_validation_frequency": "Frequent; persistent need for manual intervention",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical",
            "gap_mitigation_strategies": "Noted need for improved autonomous refinement capabilities and domain knowledge injection; no validated mitigation tested",
            "evidence_supporting_gap": "Reported frequent incomplete experimental designs and inconsistent recommendations requiring manual correction",
            "evidence_contradicting_gap": "None provided",
            "computational_cost_ratio": null,
            "uuid": "e2078.1"
        },
        {
            "name_short": "AutoBA",
            "name_full": "AutoBA",
            "brief_description": "An autonomous system for multi-omics data analysis that proposes analysis plans, generates and executes code, and includes an automated code repairing module, reported robust across a set of test cases but lacking broad generalizability validation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "AutoBA",
            "system_type": "LLM-driven pipeline with automated code generation/repair",
            "scientific_domain": "multi-omics data analysis",
            "output_type": "analysis plans, executable code, processed results and tables (e.g., differential expression tables)",
            "novelty_level": "incremental to moderately novel (automated end-to-end analysis and self-repair of code)",
            "generation_method": "LLM planning + code generation + automated code repairing and execution",
            "validation_method": "Execution of generated pipelines on provided datasets and inspection of outputs; reported robustness across test cases but no external benchmark comparisons",
            "generation_performance": "Reported robust performance across 40 test cases (qualitative); no numeric accuracy metrics provided",
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Authors caution that generalizability requires further validation across diverse tasks; performance may drop with tasks outside tested set",
            "generation_validation_comparison": "Generation (code + plan) is strong on tested cases but validation/generalizability across broader bioinformatics tasks is unproven, indicating potential generation&gt;validation gap for novel tasks",
            "uncertainty_quantification": null,
            "calibration_quality": null,
            "out_of_distribution_performance": "Not reported quantitatively; generalizability not established",
            "validation_proxy_metrics": "Success on internal test cases used as proxy for validity",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended for novel datasets/tasks",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical",
            "gap_mitigation_strategies": "Automated code repair module to reduce runtime failures; no rigorous external validation reported",
            "evidence_supporting_gap": "Authors note need for further validation due to diversity of bioinformatics tasks despite internal robustness",
            "evidence_contradicting_gap": "Performance reported as robust across 40 cases, suggesting generation and execution can be reliable within tested scope",
            "computational_cost_ratio": null,
            "uuid": "e2078.2"
        },
        {
            "name_short": "BRAD",
            "name_full": "BRAD",
            "brief_description": "An automation system comprising modules for literature search, software generation/execution, and database searches to automate tasks like gene enrichment and biomarker pipeline creation; effectiveness remains unquantified due to absence of benchmark comparisons.",
            "citation_title": "Automatic biomarker discovery and enrichment with BRAD",
            "mention_or_use": "mention",
            "system_name": "BRAD",
            "system_type": "LLM-driven modular automation system",
            "scientific_domain": "biomarker discovery and enrichment analysis",
            "output_type": "literature summaries, code, biomarker candidate lists and enrichment analyses",
            "novelty_level": "incremental (automation and orchestration of standard analyses)",
            "generation_method": "Specialized modules (LAB NOTEBOOK, SOFTWARE, DIGITAL LIBRARY) coordinated by LLM agents to generate code and search results",
            "validation_method": "Not reported; no benchmark comparisons provided to quantify accuracy or success rate",
            "generation_performance": "Reported functional automation of tasks in examples; lacks quantitative metrics",
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not evaluated; absence of benchmarks prevents assessment",
            "generation_validation_comparison": "Generation capability described, but validation performance unquantified, indicating uncertain gap",
            "uncertainty_quantification": null,
            "calibration_quality": null,
            "out_of_distribution_performance": "Not reported",
            "validation_proxy_metrics": "None reported",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended given lack of quantified validation",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical",
            "gap_mitigation_strategies": "Not provided; authors highlight absence of benchmarks as limitation",
            "evidence_supporting_gap": "Explicit statement that effectiveness remains unquantified due to absence of benchmarks",
            "evidence_contradicting_gap": "None provided",
            "computational_cost_ratio": null,
            "uuid": "e2078.3"
        },
        {
            "name_short": "LM-ABC",
            "name_full": "LM-ABC",
            "brief_description": "An LLM-assisted system for enzyme engineering that dynamically selects specialized tools for tasks like binding site extraction and molecular dynamics, accelerating enzyme engineering workflows but facing implementation challenges when generated code or workflows fail due to integration issues.",
            "citation_title": "A language model assistant for biocatalysis",
            "mention_or_use": "mention",
            "system_name": "LM-ABC",
            "system_type": "LLM + tool selection orchestration",
            "scientific_domain": "enzyme engineering / protein design",
            "output_type": "tool chains, code snippets, engineered enzyme suggestions and optimizations",
            "novelty_level": "moderately novel (automated orchestration across specialized enzyme engineering tools)",
            "generation_method": "LLM-driven dynamic tool selection and pipeline generation combining domain-specific modules",
            "validation_method": "Validation depends on execution of selected external tools and integration tests; many failures originate from integration rather than conceptual generation",
            "generation_performance": "Reported acceleration of workflows qualitatively; no quantitative success metrics provided",
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Implementation reliability (validation) is sensitive to tool integration and may degrade for novel/uncommon tool combinations",
            "generation_validation_comparison": "Generation of strategies is strong but validation often fails due to incomplete or erroneous tool integration, creating a practical gap",
            "uncertainty_quantification": null,
            "calibration_quality": null,
            "out_of_distribution_performance": "Not reported",
            "validation_proxy_metrics": "Successful execution of downstream simulations or MD runs used as a proxy",
            "human_validation_required": true,
            "human_validation_frequency": "Often required when integration issues arise",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical with physical simulation components",
            "gap_mitigation_strategies": "Recommend robust interoperability validation and error handling for generated code/workflows",
            "evidence_supporting_gap": "Authors report failures stemming from incomplete or erroneous tool integration despite plausible generated workflows",
            "evidence_contradicting_gap": "None provided",
            "computational_cost_ratio": null,
            "uuid": "e2078.4"
        },
        {
            "name_short": "CRISPR-GPT",
            "name_full": "CRISPR-GPT",
            "brief_description": "An LLM-based agent to automate CRISPR experimental design including guide RNA design, delivery method suggestions, and validation protocols; democratizes design but depends on curated genomic resources and faces species-specific data gaps.",
            "citation_title": "CRISPR-GPT: an LLM agent for automated design of gene-editing experiments",
            "mention_or_use": "mention",
            "system_name": "CRISPR-GPT",
            "system_type": "LLM-driven design agent",
            "scientific_domain": "genome editing / CRISPR experimental design",
            "output_type": "gRNA designs, delivery strategy suggestions, validation protocol outlines",
            "novelty_level": "moderately novel (automated design assistance for CRISPR experiments)",
            "generation_method": "LLM-guided retrieval plus heuristic generation informed by curated biological databases",
            "validation_method": "Relies on curated databases and possibly in silico checks; empirical/experimental validation limited and contingent on resource completeness",
            "generation_performance": "Reported to streamline design for nonexperts; no numerical success metrics provided",
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Performance tied closely to coverage of underlying databases; for underrepresented species or novel genes, reliability decreases",
            "generation_validation_comparison": "Generation can produce plausible designs, but validation capacity is limited by database completeness and species-specific annotations, creating a gap for novel targets",
            "uncertainty_quantification": null,
            "calibration_quality": null,
            "out_of_distribution_performance": "Degrades for underrepresented organisms or emerging gene targets",
            "validation_proxy_metrics": "Database coverage and annotation completeness used implicitly as proxies for validation",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended especially for non-model organisms and novel targets",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (experimental biology)",
            "gap_mitigation_strategies": "Iterative updates to knowledge resources and curation recommended",
            "evidence_supporting_gap": "Authors note reliance on curated databases introduces limitations for species-specific incomplete genomic data",
            "evidence_contradicting_gap": "None provided",
            "computational_cost_ratio": null,
            "uuid": "e2078.5"
        },
        {
            "name_short": "SpatialAgent",
            "name_full": "SpatialAgent",
            "brief_description": "An autonomous agent for spatial biology that designs experiments, analyzes multimodal spatial data, and generates hypotheses; performs strongly on large-scale tissue and niche annotation but tends to rely on common training-data annotation patterns rather than fully adapting to novel contexts.",
            "citation_title": "SpatialAgent: an autonomous AI agent for spatial biology",
            "mention_or_use": "mention",
            "system_name": "SpatialAgent",
            "system_type": "LLM-driven autonomous analysis agent (multimodal)",
            "scientific_domain": "spatial genomics / spatial biology",
            "output_type": "tissue/cell niche annotations, experimental designs, multimodal analyses, hypotheses",
            "novelty_level": "moderately novel (automated annotation and hypothesis generation in spatial contexts)",
            "generation_method": "Adaptive reasoning engine with dynamic tool integration and retrieval from external databases",
            "validation_method": "Comparative assessment against expert annotation; qualitative performance description",
            "generation_performance": "Reported to perform strongly in large-scale spatially resolved cell and tissue niche annotation (qualitative)",
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Tendency to reproduce common annotation patterns from training data suggests validation degrades for truly novel biological contexts",
            "generation_validation_comparison": "Generative annotation is strong on in-distribution patterns, but validation/adaptation to novel contexts lags (generation biased by training priors)",
            "uncertainty_quantification": null,
            "calibration_quality": null,
            "out_of_distribution_performance": "Relies on training distribution; limited adaptation to novel biological contexts reported",
            "validation_proxy_metrics": "Agreement with domain experts used implicitly for validation",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended especially for novel tissue types or atypical samples",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical",
            "gap_mitigation_strategies": "Adaptive reasoning and dynamic tool integration to improve context adaptation; still shows training-data bias",
            "evidence_supporting_gap": "Paper notes reliance on common annotation patterns rather than true adaptation",
            "evidence_contradicting_gap": "Strong performance on large-scale annotation tasks indicates generation can match expert-level outputs in-distribution",
            "computational_cost_ratio": null,
            "uuid": "e2078.6"
        },
        {
            "name_short": "CellAgent",
            "name_full": "CellAgent",
            "brief_description": "An LLM-driven multi-agent framework for automated single-cell analysis with Planner, Executor, and Evaluator roles and a self-iterative optimization mechanism; faces limitations in memory size, LLM performance on long contexts, and restricted self-evaluation flexibility.",
            "citation_title": "CellAgent: an LLM-driven multi-agent framework for automated single-cell data analysis",
            "mention_or_use": "mention",
            "system_name": "CellAgent",
            "system_type": "multi-agent LLM framework",
            "scientific_domain": "single-cell transcriptomics",
            "output_type": "analysis workflows, processed results, evaluations and optimization suggestions",
            "novelty_level": "moderately novel (multi-agent orchestration and self-optimization)",
            "generation_method": "Planner designs workflows, Executor runs tasks, Evaluator performs assessments and guides self-iterative optimization",
            "validation_method": "GPT-4-based evaluation for autonomous optimization and automated evaluations incorporated into self-iteration; limited flexibility for diverse objectives",
            "generation_performance": "Capable of full pipeline orchestration; self-optimization improves over iterations but constrained by evaluator scope",
            "validation_performance": "Evaluator provides autonomous assessments but lacks flexibility; not quantified numerically",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Performance and self-evaluation are less effective for specialized or novel analytic objectives; long-context memory issues degrade performance",
            "generation_validation_comparison": "Generation and planning are advanced, but autonomous validation/evaluation capabilities are restricted, requiring manual tuning for specialized tasks",
            "uncertainty_quantification": null,
            "calibration_quality": null,
            "out_of_distribution_performance": "Memory and evaluator limitations reduce robustness for OOD tasks",
            "validation_proxy_metrics": "Automated evaluator scores (GPT-4-based) used as proxy for correctness",
            "human_validation_required": true,
            "human_validation_frequency": "Often required for specialized objectives and to adjust evaluator settings",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical",
            "gap_mitigation_strategies": "Self-iterative optimization and automated evaluation modules; limited effectiveness without human tuning",
            "evidence_supporting_gap": "Restricted self-evaluation capability and need for manual adjustment shows generation outpacing validation flexibility",
            "evidence_contradicting_gap": "Self-optimization mechanism demonstrates some capacity to close gaps within scoped tasks",
            "computational_cost_ratio": null,
            "uuid": "e2078.7"
        },
        {
            "name_short": "Bio-Copilot",
            "name_full": "Bio-Copilot",
            "brief_description": "A multi-agent copilot architecture that distributes tasks based on agent expertise with a coordination layer and real-time feedback, improving collaborative efficiency but incurring high computational and energy costs and facing multi-agent prompt engineering challenges.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Bio-Copilot",
            "system_type": "multi-agent LLM orchestration",
            "scientific_domain": "large-scale omics analysis / general bioinformatics workflows",
            "output_type": "task plans, generated code, analysis outputs and coordination decisions",
            "novelty_level": "incremental to moderately novel (multi-agent orchestration at scale)",
            "generation_method": "Expertise-based task distribution and coordination with real-time feedback loops",
            "validation_method": "Cross-evaluation among agents and coordination feedback; details of quantitative validation not provided",
            "generation_performance": "Reported improved collaborative efficiency qualitatively; no numeric metrics provided",
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Prompt engineering and coordination degrade when instructions are ambiguous or context-poor, implying validation suffers for novel/ambiguous tasks",
            "generation_validation_comparison": "Parallel task execution and cross-evaluation improve internal validation but at high computational cost; generation may outpace reliable validation for resource-constrained deployments",
            "uncertainty_quantification": null,
            "calibration_quality": null,
            "out_of_distribution_performance": "Not quantified; prompt sensitivity suggests fragility OOD",
            "validation_proxy_metrics": "Agent cross-evaluation and real-time feedback used as proxy metrics",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended, especially for ambiguous prompts and high-stakes tasks",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical",
            "gap_mitigation_strategies": "Multi-agent cross-evaluation and coordination layers proposed; trade-offs with computational cost noted",
            "evidence_supporting_gap": "Authors highlight computational/energy cost and prompt engineering challenges that limit validation robustness",
            "evidence_contradicting_gap": "Cross-evaluation mechanism provides some internal validation gains",
            "computational_cost_ratio": "High (authors note significant computational and energy costs for competitive learning/cross-evaluation), but no numeric ratio provided",
            "uuid": "e2078.8"
        },
        {
            "name_short": "ProtAgent(s)",
            "name_full": "ProtAgent / ProtAgents",
            "brief_description": "Multi-agent frameworks aimed at protein design and discovery that combine role specialization with automatic tool selection; effective for single-objective predictions but constrained when optimizing competing design requirements.",
            "citation_title": "ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning",
            "mention_or_use": "mention",
            "system_name": "ProtAgent(s)",
            "system_type": "multi-agent LLM + predictive models + physics-informed components",
            "scientific_domain": "protein design / structural biology",
            "output_type": "designed protein sequences/constructs, predicted properties, suggested assays",
            "novelty_level": "moderately novel to highly novel depending on task (novel sequences possible)",
            "generation_method": "Planner-Executor-Critic structure selecting external tools and employing data-driven end-to-end deep learning models for prediction",
            "validation_method": "Tool-based predictions, simulation (physics or ML-based), and critic assessment; limited multiobjective optimization validation",
            "generation_performance": "Capable of producing designs and predictions; constrained in multiobjective optimization scenarios",
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Effective for in-scope properties but struggles when designs must balance competing objectives (novel complex designs reduce validation reliability)",
            "generation_validation_comparison": "Generation of candidate designs can outpace ability to validate multiple competing objectives simultaneously, creating a gap for complex novel designs",
            "uncertainty_quantification": null,
            "calibration_quality": null,
            "out_of_distribution_performance": "Not quantified; multiobjective and OOD challenges noted",
            "validation_proxy_metrics": "Predictive scores from models and simulation plausibility checks",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended for multiobjective or high-novelty designs",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical with physics-informed components",
            "gap_mitigation_strategies": "Dynamic tool selection and critic evaluation; limited in addressing multiobjective optimization challenges",
            "evidence_supporting_gap": "Authors explicitly state difficulty in optimizing competing design requirements simultaneously",
            "evidence_contradicting_gap": "Successful single-objective predictions demonstrate generation and validation align for simpler tasks",
            "computational_cost_ratio": null,
            "uuid": "e2078.9"
        },
        {
            "name_short": "The Virtual Lab",
            "name_full": "The Virtual Lab",
            "brief_description": "A collaborative multi-agent system led by a principal investigator AI agent and human researcher that designed and experimentally validated engineered nanobodies against SARS-CoV-2 variants, demonstrating real wet-lab validation of AI-generated molecules.",
            "citation_title": "The virtual lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation",
            "mention_or_use": "mention",
            "system_name": "The Virtual Lab",
            "system_type": "multi-agent LLM orchestration + human-in-loop + wet-lab validation",
            "scientific_domain": "protein binder (nanobody) design / experimental molecular biology",
            "output_type": "engineered nanobody sequences and designs, experimental assays/results",
            "novelty_level": "highly novel (designs validated experimentally against emerging viral variants)",
            "generation_method": "Multi-agent decomposition (chemistry, CS agents etc.) with design proposals synthesized into candidate nanobodies",
            "validation_method": "Empirical wet-lab validation including expression, solubility assays, and binding assays against spike RBD variants",
            "generation_performance": "Generated a set of engineered nanobodies; 92 engineered nanobodies were validated experimentally",
            "validation_performance": "Over 90% of the 92 validated nanobodies exhibited expression and solubility; two showed unique binding to JN.1 and KP.3 spike RBD variants",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "High in-distribution success (expression/solubility) but system required repeated refinements and human intervention for optimal results; novel-binding discoveries (2 nanobodies) indicate capability for true novelty with experimental confirmation",
            "generation_validation_comparison": "This system demonstrates generation accompanied by direct experimental validation; however, generation required iterative refinement and human oversight to reach high experimental validation rates, indicating generation alone was insufficient without human-guided validation loops",
            "uncertainty_quantification": null,
            "calibration_quality": null,
            "out_of_distribution_performance": "Demonstrated ability to find binders to recent variants (OOD relative to earlier training data), but needed human corrections for deprecated tool choices and code fixes",
            "validation_proxy_metrics": "Expression and solubility assays, binding assays used as direct validation metrics rather than proxies",
            "human_validation_required": true,
            "human_validation_frequency": "Frequent: repeated refinements and human intervention were required during optimization and to correct deprecated tool recommendations",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (experimental molecular biology)",
            "gap_mitigation_strategies": "Human-in-the-loop oversight, iterative refinement cycles, and repeated experimental testing to ensure robustness",
            "evidence_supporting_gap": "Agents sometimes recommended outdated tools and required human intervention to correct deprecated code; repeated refinements increased time cost",
            "evidence_contradicting_gap": "High experimental validation rates (&gt;90% expression/solubility) show that generation plus iterative validation can yield robust experimental outcomes",
            "computational_cost_ratio": null,
            "uuid": "e2078.10"
        },
        {
            "name_short": "BioDiscoveryAgent / Biodiscoveryagent",
            "name_full": "BioDiscoveryAgent (Biodiscoveryagent)",
            "brief_description": "An AI agent for designing genetic perturbation experiments that automates aspects of experimental design and reasoning about outcomes; mentioned as a recent system with limited reported quantitative validation in the review.",
            "citation_title": "Biodiscoveryagent: An AI agent for designing genetic perturbation experiments",
            "mention_or_use": "mention",
            "system_name": "BioDiscoveryAgent",
            "system_type": "LLM-driven design agent",
            "scientific_domain": "genetic perturbation design / experimental planning",
            "output_type": "experimental designs, suggested perturbations, predicted outcomes",
            "novelty_level": "moderately novel (automated experimental design)",
            "generation_method": "LLM-based reasoning and planning over genetic perturbation hypothesis space",
            "validation_method": "Not detailed in review; likely depends on experimental follow-up or domain expert review",
            "generation_performance": "Not quantified in the review",
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not reported",
            "generation_validation_comparison": "Mentioned as capable of design but no detailed validation metrics provided, suggesting potential generation-validation gap not quantified",
            "uncertainty_quantification": null,
            "calibration_quality": null,
            "out_of_distribution_performance": "Not reported",
            "validation_proxy_metrics": null,
            "human_validation_required": true,
            "human_validation_frequency": "Recommended",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical",
            "gap_mitigation_strategies": "Not detailed",
            "evidence_supporting_gap": "Lack of reported validation metrics indicates validation capabilities are not well quantified in literature cited",
            "evidence_contradicting_gap": "None provided",
            "computational_cost_ratio": null,
            "uuid": "e2078.11"
        },
        {
            "name_short": "General LLM agents (LLMs / LLM-driven agents)",
            "name_full": "Large Language Model-driven AI agents (e.g., GPT-4, GPT-4o, Claude, Llama variants) as used across agentic systems",
            "brief_description": "LLMs serve as foundational reasoning and planning modules in many agentic bioinformatics systems; they generate hypotheses, code, experimental designs and narrative explanations but are prone to hallucination and temporal knowledge cutoff issues.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLM (e.g., GPT-4, GPT-4o, Claude 3.5 Sonnet, Llama3 variants)",
            "system_type": "large language model / transformer",
            "scientific_domain": "broad (bioinformatics, experimental design, molecular design, spatial biology)",
            "output_type": "textual hypotheses, code, experiment plans, molecule/protein design proposals, annotations",
            "novelty_level": "varies from in-distribution synthesis to moderately/highly novel outputs depending on prompting and retrieval augmentation",
            "generation_method": "Pattern extrapolation and recombination of learned features with retrieval-augmented generation and multi-agent orchestration in many systems",
            "validation_method": "Varies by systemcommon methods include tool execution, simulation, multi-agent cross-validation, expert review, and empirical wet-lab testing in some cases",
            "generation_performance": "High fluency and ability to produce plausible outputs; specific success rates depend on downstream system integration",
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Hallucination and factual errors increase for novel or OOD prompts; temporal knowledge cutoffs cause outdated tool recommendations",
            "generation_validation_comparison": "Consistently reported that generation capabilities exceed robust autonomous validation; many systems require human oversight or additional grounding",
            "uncertainty_quantification": "Some systems propose confidence scoring and uncertainty quantification as needed, but adoption is inconsistent across cited systems",
            "calibration_quality": "Generally poor when faced with novel biological claims; medical LLMs shown to produce clinically unsupported content with fluent language (cited works)",
            "out_of_distribution_performance": "Degrades; cited studies and examples (e.g., outdated tool recommendations, reliance on common annotation patterns) illustrate fragility OOD",
            "validation_proxy_metrics": "Plausibility, coherence, API execution success, and agreement across agents often used as proxies",
            "human_validation_required": true,
            "human_validation_frequency": "Frequent for high-stakes or novel outputs",
            "formal_verification_used": false,
            "domain_formalization_level": "varies by application but typically empirical in bio domains",
            "gap_mitigation_strategies": "Retrieval-augmented generation, multi-agent cross-validation, explainability tools, retrieval from curated databases, and human-in-loop processes recommended",
            "evidence_supporting_gap": "Multiple examples across survey where hallucinations, outdated tool recommendations, and insufficient autonomous refinement are reported",
            "evidence_contradicting_gap": "Systems like The Virtual Lab show that with iterative refinement and wet-lab validation, LLM-generated designs can achieve high empirical success",
            "computational_cost_ratio": null,
            "uuid": "e2078.12"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The virtual lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation",
            "rating": 2
        },
        {
            "paper_title": "ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning",
            "rating": 2
        },
        {
            "paper_title": "CellAgent: an LLM-driven multi-agent framework for automated single-cell data analysis",
            "rating": 2
        },
        {
            "paper_title": "CRISPR-GPT: an LLM agent for automated design of gene-editing experiments",
            "rating": 2
        },
        {
            "paper_title": "Automatic biomarker discovery and enrichment with BRAD",
            "rating": 1
        },
        {
            "paper_title": "A language model assistant for biocatalysis",
            "rating": 1
        },
        {
            "paper_title": "An AI agent for fully automated multi-omic analyses",
            "rating": 2
        },
        {
            "paper_title": "SpatialAgent: an autonomous AI agent for spatial biology",
            "rating": 2
        }
    ],
    "cost": 0.021369,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Streamline automated biomedical discoveries with agentic bioinformatics</p>
<p>Juexiao Zhou 
Syneron Opal
10281Cayman Island</p>
<p>Computer Science Program
Computer, Electrical and Mathematical Sciences and Engineering Division
King Abdullah University of Science and Technology (KAUST)
23955-6900ThuwalKingdom of Saudi Arabia</p>
<p>Center of Excellence for Smart Health
King Abdullah University of Science and Technology (KAUST)
23955-6900ThuwalKingdom of Saudi Arabia</p>
<p>Center of Excellence on Generative AI
King Abdullah University of Science and Technology (KAUST)
23955-6900ThuwalKingdom of Saudi Arabia</p>
<p>School of Data Science
The Chinese University of Hong Kong
518172Shenzhen (CUHK-Shenzhen), GuangdongP.R. China</p>
<p>Jindong Jiang 
Department of Statistics
Nanjing University
210008NanjingChina</p>
<p>Zhongyi Han 
Computer Science Program
Computer, Electrical and Mathematical Sciences and Engineering Division
King Abdullah University of Science and Technology (KAUST)
23955-6900ThuwalKingdom of Saudi Arabia</p>
<p>Center of Excellence for Smart Health
King Abdullah University of Science and Technology (KAUST)
23955-6900ThuwalKingdom of Saudi Arabia</p>
<p>Center of Excellence on Generative AI
King Abdullah University of Science and Technology (KAUST)
23955-6900ThuwalKingdom of Saudi Arabia</p>
<p>School of Software
Shandong University
250101JinanChina</p>
<p>Zijian Wang 
School of Data Science
The Chinese University of Hong Kong
518172Shenzhen (CUHK-Shenzhen), GuangdongP.R. China</p>
<p>Xin Gao xin.gao@kaust.edu.sa 
Computer Science Program
Computer, Electrical and Mathematical Sciences and Engineering Division
King Abdullah University of Science and Technology (KAUST)
23955-6900ThuwalKingdom of Saudi Arabia</p>
<p>Center of Excellence for Smart Health
King Abdullah University of Science and Technology (KAUST)
23955-6900ThuwalKingdom of Saudi Arabia</p>
<p>Center of Excellence on Generative AI
King Abdullah University of Science and Technology (KAUST)
23955-6900ThuwalKingdom of Saudi Arabia</p>
<p>Computer Science Program
Computer, Electrical and Mathematical Sciences and Engineering Division
Kingdom of Saudi Arabia
King Abdullah University of Science and Technology (KAUST)
23955-6900Thuwal</p>
<p>Jindong Jiang and Zhongyi Han contributed equally
Juexiao Zhou</p>
<p>Streamline automated biomedical discoveries with agentic bioinformatics
5103ECA8547EC4A82C7DF771BEB8426810.1093/bib/bbaf505Received: April 30, 2025. Revised: August 14, 2025. Accepted: August 31, 2025AI agentsbioinformaticslarge language models
The emergence of artificial intelligence agents powered by large language models marks a transformative shift in computational biology.In this new paradigm, autonomous, adaptive, and intelligent agents are deployed to tackle complex biological challenges, leading to a new research field named agentic bioinformatics.Here, we explore the core principles, evolving methodologies, and diverse applications of agentic bioinformatics.We examine how agentic bioinformatics systems work synergistically to facilitate data-driven decision-making and enable self-directed exploration of biological datasets.Furthermore, we highlight the integration of agentic frameworks in key areas such as personalized medicine, drug discovery, and synthetic biology, illustrating their potential to revolutionize healthcare and biotechnology.In addition, we address the ethical, technical, and scalability challenges associated with agentic bioinformatics, identifying key opportunities for future advancements.By emphasizing the importance of interdisciplinary collaboration and innovation, we envision agentic bioinformatics as a major force in overcoming the grand challenges of modern biology, ultimately advancing both research and clinical applications.</p>
<p>Main</p>
<p>Bioinformatics, a discipline rooted at the intersection of biology, computer science, and mathematics, has undergone remarkable evolution since its inception in the mid-20th century [1].Initially emerging as a computational response to the increasing complexity of biological data [2], bioinformatics was propelled by breakthroughs in molecular biology and the development of sequencing technologies [3].Early milestones, such as the development of the sequence alignment algorithms [4] and the establishment of pioneering genetic databases [5,6], laid the foundation for a field that would transform our understanding of life.Over the decades, bioinformatics has expanded its scope far beyond genomics, encompassing proteomics, transcriptomics, metabolomics, and systems biology [7][8][9][10].This growth has been paralleled by significant advancements in computational power, algorithm design, and data storage, enabling the integration of machine learning and high-performance computing into bioinformatics workf lows.These technological innovations have facilitated the exploration of intricate biological networks and the modeling of cellular systems, ushering in an era of predictive and system-level analyses [11,12].Today, bioinformatics stands as the cornerstone of modern biology and medicine, driving discoveries in areas ranging from evolutionary biology and structural genomics to personalized medicine and synthetic biology [13].The rapid evolution of artificial intelligence (AI) and deep learning (DL) has profoundly affected numerous scientific disciplines [14], with bioinformatics being no exception [15][16][17].From the early conceptualization of AI to the emergence of large language models (LLMs) [18] and LLM-driven AI agents [19], AI has transitioned from rulebased systems to advanced models capable of understanding, reasoning, and interacting with complex datasets.LLM-driven AI agents, which are autonomous systems designed to perceive environments, make decisions, and execute actions, represent a critical milestone in this evolution [20].Their integration into bioinformatics marks a paradigm shift, characterized by the use of intelligent agents to autonomously generate, analyze, and interpret biological data.This approach addresses the growing scale and complexity of biological research, unlocking new possibilities for innovation and collaboration [21].</p>
<p>The use of agent-based approaches in bioinformatics has a well-established history.As early as the NETTAB 2001 and 2002 workshops [22,23], researchers explored intelligent agents to enhance interoperability, data integration, and decision-making across distributed biological databases.Notably, Merelli et al. [24] introduced one of the first agent-based systems for automating workf lows and processing DNA microarray data.While today's focus has shifted toward LLM-based agents capable of natural language reasoning and high-level task planning, these modern systems still build upon the original vision of autonomous, interoperable agents in computational biology [25].</p>
<p>Conventional bioinformatics approaches have relied heavily on manual curation and rigid workf lows [26][27][28].While these methods have delivered significant insights, they are increasingly limited by their scalability, adaptability, and ability to manage the rapidly expanding volume and heterogeneity of biological datasets, as well as the demands of emerging research pipelines [29].These challenges highlight the pressing need for more dynamic and intelligent solutions.AI agents, with their transformative potential, offer a promising path forward.They enable capabilities such as real-time data generation, autonomous experimental design, and high-dimensional data analysis, addressing the limitations of traditional methods and unlocking new possibilities in bioinformatics [21,29,30].</p>
<p>Thus, we define and envision agentic bioinformatics as a novel and transformative paradigm within bioinformatics, wherein intelligent AI agents are strategically integrated throughout the entire research process to optimize, automate, and innovate biological data analysis.Agentic bioinformatics goes beyond the mere application of LLMs or isolated AI agents in bioinformatics workf lows.It emphasizes end-to-end integration of intelligent, autonomous agents that can reason, plan, adapt, and collaborate across the entire scientific process.These agents leverage advanced machine learning, natural language processing, and autonomous decision-making techniques to assume diverse, dynamic roles across the bioinformatics pipeline, ranging from data preprocessing and analysis to result interpretation and hypothesis generation.These agents are designed not just to execute predefined tasks, but to dynamically coordinate with one another, make independent decisions under uncertainty, and engage in long-horizon planning tailored to the complexities of biological systems.By facilitating end-to-end automation and offering innovative solutions, agentic bioinformatics enables more efficient and insightful scientific discovery in biology.</p>
<p>Agentic bioinformatics represents a fundamental shift that introduces a multi-agent, adaptive framework that aligns more closely with the open-ended, exploratory nature of biological research, where goals evolve and data contexts shift.Tasks that once demanded extensive human expertise and time are now streamlined, facilitating rapid hypothesis iteration and validation.Furthermore, this paradigm promotes inclusivity by democratizing access to advanced analytical tools, allowing researchers with varying levels of computational expertise to leverage stateof-the-art methods.In contrast to traditional LLM-based workf low automation, agentic bioinformatics entails a system-level rethinking of how biological knowledge is generated, transforming agents from tools into autonomous collaborators capable of hypothesis generation, experimental design, and iterative refinement.This systemic view enables enhanced scalability, inclusivity, and accessibility, allowing researchers with diverse backgrounds to leverage sophisticated methods through intelligent mediation.By bridging disciplines, agentic bioinformatics could empower collaborative efforts across the biological sciences, fostering innovation and accelerating discovery.</p>
<p>Here, we comprehensively explore the concept and implications of agentic bioinformatics, with key terminology defined in Table 1.We begin with an overview of existing operational frameworks of AI agents in bioinformatics, distinguishing between single-agent and multi-agent systems.Single-agent systems focus on specialized, independent tasks, while multi-agent systems involve collaboration and task distribution across agents.The discussion emphasizes how these systems can tackle complex biological problems more effectively than traditional methods.Next, we present a forward-looking vision for a fully automated, end-to-end laboratory powered by agentic bioinformatics.In such a laboratory, intelligent agents manage every stage of the research process, from hypothesis generation and experimental design to data analysis, interpretation, and reporting.This vision integrates wet-lab robotics, dry-lab computational agents, and real-time decision-making systems, revolutionizing the pace and scope of biological discovery.Finally, we address the opportunities and challenges associated with agentic bioinformatics.Opportunities include enhancing reproducibility, scalability, and innovation in research, while challenges span ethical considerations, data privacy, bias mitigation, the development of robust, interpretable AI systems, among others.Our objective is to underscore the transformative potential of AI agents in bioinformatics and their critical role in advancing biological research.Agentic bioinformatics is not merely an incremental advancement, it signifies a paradigm shift that redefines the boundaries of what is possible in the life sciences.By exploring the applications, frameworks, and future directions of agentic bioinformatics, we aim to inspire new research, foster interdisciplinary collaboration, and provide a roadmap for realizing the full potential of this emerging field.</p>
<p>Agentic bioinformatics</p>
<p>As illustrated in Fig. 1, intelligent agents in bioinformatics can function in a variety of roles, each tailored to address specific aspects of biological research and data analysis.These roles include, but are not limited to Brainstorming Agents, which assist in hypothesis generation and ideation; Experimental Design Agents, which optimize research workf lows and suggest experimental parameters; Reasoning Agents, which draw inferences and establish causal relationships from data; Wet-lab AI agents, which control and automate laboratory equipment for physical experiments; Dry-lab AI agents, which focus on computational analysis and simulations; and Innovative AI agents, which explore novel strategies and generate creative solutions to complex problems.Together, these agents form a versatile and adaptive toolkit for agentic bioinformatics, enabling researchers to address diverse challenges across the biological research spectrum.</p>
<p>Agents in bioinformatics operate under two primary paradigms: single-agent systems and multi-agent systems.Each paradigm serves distinct purposes and offers unique advantages depending on the complexity and scope of the task.Single-agent systems comprise a stand-alone AI agent that executes specific tasks independently.They are designed for high specialization and focus, excelling in tasks that require in-depth expertise or precision.Such systems are particularly effective for compartmentalized tasks that can be solved in isolation, offering simplicity and efficiency in implementation.However, their stand-alone nature limits their ability to address interconnected or multifaceted problems.</p>
<p>In contrast, multi-agent systems consist of multiple intelligent agents working collaboratively to tackle complex challenges.</p>
<p>Reasoning agent</p>
<p>An advanced dry-lab AI agent that applies logic, inference, and probabilistic modeling to derive insights from complex datasets and guide decision-making.</p>
<p>These systems are characterized by their ability to distribute responsibilities, coordinate actions, and adapt dynamically to evolving tasks.The adaptability and scalability of multiagent systems make them ideal for addressing the intricate and dynamic nature of modern bioinformatics challenges.They enable researchers to model complex systems, explore multidimensional datasets, and solve problems that exceed the capabilities of stand-alone agents.By leveraging the strengths of both single-agent and multiagent systems, bioinformatics researchers can tailor their approaches to the specific demands of their work, ensuring optimal efficiency and innovation.</p>
<p>Single AI agent in bioinformatics</p>
<p>Single agent in bioinformatics is typically designed to handle specific tasks such as data analysis or experiment control.These agents have proven highly effective in managing welldefined tasks, offering significant advantages in simplicity, costeffectiveness, and task specialization, as shown in Table 2.A comparative analysis of their shared and distinct characteristics is provided in Table 3.</p>
<p>Traditional bioinformatics data analysis requires multiple tools and substantial programming expertise, which limits accessibility to experimental researchers due to the steep learning curve.BioMANIA [31] combines LLMs with Application Programming Interfaces (APIs) from established Python libraries to streamline high-throughput sequencing data analysis, addressing the complexities and technical challenges inherent in these tasks.By interpreting user instructions and automatically executing bioinformatics workf lows, BioMANIA enables code-free biological analyses, simplifying omics data exploration and accelerating research.Despite these advantages, BioMANIA still faces limitations with ambiguous instructions and heavily relies on highquality documentation.Its performance is limited by reliance on the quality of third-party tools, encountering challenges such as installation failures, undocumented dependencies, and inconsistencies between API documentation and code.The system also struggles with poorly designed APIs, characterized by ambiguous names and excessive parameters, and unreliable tutorials, while remaining susceptible to LLM hallucinations during API prediction.</p>
<p>BIA [32] provides an interactive, automated solution for single-cell RNA sequencing (scRNA-seq) analysis, leveraging text-based interactions with LLMs to facilitate data extraction, analysis, and report generation through dynamic user dialog.The agent executes the entire single-cell analysis pipeline, from data retrieval to invoking the necessary APIs for processing, and autonomously plans and compiles conclusions.However, the system's performance in zero-shot dynamic workf lows reveals notable limitations, including frequent generation of incomplete experimental designs that omit critical steps like subcluster annotation because of the lack of professional knowledge, inconsistent tool recommendations for identical queries indicating underlying stability issues, and a persistent need for manual intervention due to inadequate autonomous refinement capabilities.</p>
<p>AutoBA [18] autonomously handles multi-omics data analysis with remarkable ease of use.By requiring minimal user input, such as the data path, description, and analysis goal, AutoBA autonomously proposes an analysis plan, generates and executes the necessary code, and performs the subsequent data analysis.For example, in a task focused on identifying differentially expressed genes, the user provides RNA-Seq data, along with its description and analysis goal.AutoBA then automatically performs the required preprocessing steps and generates an analysis table as output.Its ability to adapt and self-design analysis processes based on variations in input data and the novel automated code repairing module further enhances its f lexibility and versatility.While demonstrating robust performance across 40 Figure 1.Conceptual framework for agentic bioinformatics.We present an interdisciplinary framework for agentic bioinformatics, which integrates AI-driven agents into both dry-lab and wet-lab environments to revolutionize bioinformatics workf lows.The framework is organized into functional sections that highlight the key roles and interactions of autonomous AI agents across various stages of bioinformatics research.Brainstorming and experiment design: the research process begins with brainstorming and experiment design, facilitated by specialized AI agents such as Search Agents and Literature Review Agents.These agents retrieve, curate, and synthesize scientific knowledge from diverse databases, enabling researchers to generate hypotheses efficiently.By providing contextualized insights from the latest research, these tools streamline the initial stages of scientific inquiry and ensure that experiments are grounded in up-to-date evidence.Wet-lab AI Agents: wet-lab AI agents represent a class of physical or semi-physical AI systems that interact directly with laboratory equipment to execute experimental procedures.Examples include polymerase chain reactions (PCR) machines, microscopes, and animal modeling agents, which perform tasks such as PCR, high-resolution imaging, and in vivo modeling.These agents enhance precision, reduce human error, and accelerate workf lows, enabling researchers to focus on higher level decision-making and interpretation.Dry-lab AI Agents: dry-lab AI agents, such as Database Agents and Reasoning Agents, are pivotal in managing, analyzing, and interpreting complex datasets, including genomic, imaging, and multi-omics data.Advanced AI-driven tools like AutoBA and CellAgent further refine data interpretation, extracting meaningful biological insights and uncovering patterns that may not be apparent through traditional methods.These agents bridge the gap between raw data and actionable knowledge, enabling researchers to make data-driven decisions with confidence.Innovative AI agents: the framework also emphasizes the role of forward-thinking AI applications, such as the Design of Novel Algorithms and the Development of Novel Applications.These innovative agents exemplify the creative potential of AI in bioinformatics, enabling the exploration of uncharted territories and the development of groundbreaking tools.By pushing the boundaries of what is possible, these agents facilitate discoveries and applications that were previously unattainable.The seamless integration of wet-lab and dry-lab agents creates a collaborative ecosystem where AI accelerates the pace of bioinformatics research.By synergizing embodied agents (wet-lab) and computational agents (dry-lab), the framework enables a continuous f low from data generation to analysis and interpretation.This holistic approach addresses complex biological questions with unprecedented sophistication, paving the way for transformative advancements in the field.test cases, the system's generalizability requires further validation given the vast diversity of tasks in classical bioinformatics analysis.</p>
<p>BRAD [33] is used for automation, performing tasks ranging from gene enrichment and archival searches to automatically generating code for biomarker identification pipelines.It is organized into several specialized modules: the LAB NOTEBOOK module for literature searches, the SOFTWARE module for software generation and execution, and the DIGITAL LIBRARY module for database and web searches.Bioinformatics tools like LM-ABC [34] not only accelerate enzyme engineering workf lows through dynamic selection of specialized tools for tasks such as binding site extraction, catalytic activity optimization, and molecular dynamics simulations but also face implementation challenges when generated code snippets or workf lows encounter failures stemming from incomplete or erroneous tool integration.By combining AI-guided molecular design with robust interoperability validation, these systems aim to balance automated experimental optimization with technical reliability, enabling researchers to navigate both biological complexity and computational constraints in enzyme engineering pipelines.But its effectiveness remains unquantified due to the absence of benchmark comparisons that would establish its success rate, accuracy, and other key performance indicators.</p>
<p>CRISPRGPT [35] streamlines gene-editing research by automating the design of CRISPR systems, guide RNAs, delivery methods, and validation protocols, while its reliance on curated biological databases introduces limitations in scenarios where speciesspecific genomic data or gene annotations are incomplete.The system's ability to democratize experimental design for nonexperts is inherently tied to the coverage and accuracy of its underlying knowledge resources, necessitating iterative updates</p>
<p>MDCrow</p>
<p>to address gaps in underrepresented organisms or emerging gene targets.This integration of automation with domain-specific constraints highlights both the transformative potential and contextdependent challenges of AI-augmented workf lows in genome engineering.Safety and ethical concerns may arise when using AI tools to guide genome editing, including risks such as unauthorized modification of human genomes and privacy breaches involving users' genomic data.</p>
<p>In spatial genomics research, autonomous agents hold significant potential to drive innovation.SpatialAgent [36] exemplifies this promise by autonomously executing various spatial biology tasks, from experimental design and multimodal data analysis to hypothesis generation.Its adaptive reasoning engine and dynamic tool integration enable it to work effectively across diverse datasets, tissue types, and biological questions.Notably, it preforms strongly in large-scale, spatially resolved cell and tissue niche annotation.However, evaluation results suggest a tendency to rely on common annotation patterns from its training data rather than fully adapting to novel biological contexts.This contrasts with domain experts, who often incorporate subtle, context-specific cues during manual annotation.</p>
<p>However, while the simplicity and specialization of single agents offer clear benefits, they also present distinct challenges.Focusing on specific tasks can limit their ability to handle complex, multifaceted problems that require cross-domain knowledge or collaboration between multiple agents.Moreover, the lack of synergy between isolated agents can lead to inefficiencies, particularly when dealing with tasks that require multiple steps or diverse methodologies.</p>
<p>Multi AI agents in bioinformatics</p>
<p>Multi-agent systems have emerged as indispensable tools for tackling complex biological problems.These challenges include data processing in genomics, drug discovery, and protein design.By collaborating, different agents can effectively share tasks, improve productivity, and adapt to new challenges.This section will explore the collaboration models of multi-agent systems in bioinformatics, focusing on collaborative task-solving, dynamic task allocation, and continuous learning and adaptability (Table 4).The comparative analysis of their shared and distinct characteristics is provided in Table 5.</p>
<p>Multiple agents work together to solve complex tasks, often by decomposing them into smaller sub-tasks and distributing them among different agents.In the CellAgent [37] framework, three agents, the Planner, Executor, and Evaluator, each take on different roles to collaboratively analyze scRNA-seq data.The Planner designs a comprehensive analysis workf low, the Executor executes the designated tasks, and the Evaluator ensures accuracy and biological relevance by conducting rigorous assessments.BioMaster [38] employs a similar multi-agent design, where specialized agents collaborate to plan, code, and debug, enabling seamless execution of complex tasks such as Hi-C data processing.</p>
<p>In complex bioinformatics tasks, new challenges and unforeseen issues may arise during the experiment.Multi-agent systems are equipped with continuous learning and adaptability mechanisms to optimize their strategies and behaviors over time.Cel-lAgent also incorporates a self-iterative optimization mechanism that integrates automated evaluation results and adjusts the execution strategy to address potential technical problems or experimental anomalies.This self-improvement capability enables the system to adapt to changes and maintain high efficiency in execution quickly.However, CellAgent also faces several challenges.Its Table 3. Feature comparison of single-agent systems for agentic bioinformatics.Automated: the agent completes a self-contained task without human intervention.Human interaction: the agent follows user commands or instructions.Experiment design: the agent is capable of independently designing experimental protocols relevant to its task.Data analysis: the agent performs statistical computations, pattern recognition, and derives insights from raw datasets</p>
<p>Method name Automated Human interaction Experiment design Data analysis
BioMANIA   AutoBA   BioInformatics Agent (BIA)  BRAD   LM-ABC    CRISPR-GPT  BioDiscoveryAgent  SpatialAgent MDCrow  
memory systems can become excessively large, leading to performance degradation in LLMs when handling overly long contexts.CellAgent's main limitation lies in its restricted self-evaluation capability.While the GPT-4-based evaluation enables autonomous optimization, it currently lacks f lexibility for diverse analytical objectives, requiring manual adjustment for specialized needs.Multi-agent systems often need to dynamically allocate tasks based on the complexity, priority, and real-time needs of the tasks, enhancing system efficiency and ensuring optimal resource utilization.In the Bio-Copilot framework [39], the task distribution is based on the expertise of each agent and the current demands of the system, with a coordination layer ensuring smooth execution and real-time feedback allowing adjustments to task allocation for improved collaborative efficiency.Bio-Copilot's multiagent architecture presents two key limitations.First, its competitive learning mechanism, while effective for performance optimization through parallel task execution and cross-evaluation, incurs significant computational costs in resource allocation and energy consumption.Second, the system's prompt engineering faces challenges in multi-agent coordination, where ambiguous or context-poor instructions can compromise decision-making accuracy.</p>
<p>Multi-agent systems have been successfully applied to a wide range of bioinformatics tasks.For example, ProtAgent [40] employs a multi-agent framework to streamline protein design and analysis through role specialization while automating the selection and deployment of external bioinformatics tools.Each agent has a specific function: the Planner formulates strategies, the Executor automatically identifies and applies appropriate tools from external libraries for specific bioinformatics tasks, and the Critic continuously assesses the effectiveness and appropriateness of the tools being used.This system is engineered to dynamically select and evaluate the most suitable tools based on task requirements, ensuring that ProtAgent not only adapts seamlessly across various protein design scenarios but also enhances other functionalities integral to the protein engineering process.ProAgent demonstrates constrained capabilities in multiobjective protein design, as its current architecture primarily employs data-driven end-to-end DL models.While effective for individual characteristic prediction, the system struggles to optimize competing design requirements simultaneously.This limitation becomes particularly evident when addressing complex biological specifications that demand concurrently balancing multiple structural and functional parameters.</p>
<p>The Virtual Lab [41] functions as a collaborative platform where AI and human expertise converge.Guided by a principal investigator AI agent, the system coordinates a diverse team of AI agents with expertise in fields like chemistry and computer science, complemented by a human researcher who provides highlevel feedback.This collaborative setup is structured through a series of team and individual meetings.Team meetings facilitate collaborative discussions on overarching scientific goals, whereas individual meetings focus on delegating and clarifying agentspecific tasks.The Virtual Lab effectively addresses complex, real-world scientific challenges, such as the development of nanobody binders targeting new SARS-CoV-2 variants.In the experiments, it successfully validated 92 engineered nanobodies, with over 90% exhibiting expression and solubility.In particular, two of these nanobodies showed unique binding properties to recent JN.1 and KP.3 spike RBD variants, highlighting the Virtual Lab's innovative capacity to drive scientific breakthroughs.But the Virtual Lab requires repeated refinements to achieve optimal results, increasing time costs and facing challenges with efficient resource utilization and ambiguous prompts.The Virtual Lab's effectiveness is constrained by inherent LLM limitations, particularly in temporal knowledge currency and decision-making precision.Its AI agents occasionally recommend outdated tools (like AlphaFold-Multimer instead of AlphaFold3) due to training data cutoffs, and require human intervention to correct deprecated code implementations.Furthermore, the system exhibits prompt sensitivity, when faced with binary design choices (e.g.nanobody modification vs. de novo design), agents often default to noncommittal responses unless explicitly constrained.These constraints currently necessitate significant human work to maintain scientific rigor.</p>
<p>Challenges of agentic bioinformatics</p>
<p>Despite its transformative potential, agentic bioinformatics faces several critical challenges that must be addressed to ensure its effective deployment in biological research.These challenges span technical, ethical, and collaborative aspects, each of which requires targeted solutions.Below, we outline the key challenges specific to agentic bioinformatics.</p>
<p>Integration and standardization</p>
<p>Agentic bioinformatics systems rely on a diverse ecosystem of AI agents, bioinformatics tools, and laboratory automation devices.However, the lack of standardized protocols for communication between AI-driven components impedes seamless interoperability.Existing bioinformatics workf lows use heterogeneous data</p>
<p>High-dimensional biological data and multimodal integration</p>
<p>Biological data are inherently high-dimensional, spanning genomics, transcriptomics, proteomics, metabolomics, and imaging modalities [43].Agentic bioinformatics systems must efficiently process and integrate multi-modal datasets, each with different statistical distributions and noise characteristics.</p>
<p>Traditional bioinformatics pipelines often rely on domain-specific preprocessing steps that may not generalize well to AI-driven workf lows.AI agents must be equipped with adaptive data preprocessing, feature selection, and dimensionality reduction techniques to extract meaningful patterns from complex biological datasets.</p>
<p>Out-of-distribution generalization and anomaly detection</p>
<p>A significant challenge for AI-driven bioinformatics is generalization to unseen data distributions, particularly when analyzing datasets from different species, cell types, or experimental conditions [44].AI agents trained on specific datasets may fail to generalize when applied to new biological contexts, leading to unreliable predictions.Out-of-distribution generalization and anomaly detection methods must be incorporated into agentic bioinformatics workf lows to ensure robustness [45,46].This is particularly critical in clinical applications where AI-driven diagnostics and personalized medicine decisions must be validated across diverse patient populations [47].</p>
<p>Hallucinations in AI-driven bioinformatics</p>
<p>AI agents, particularly LLMs and generative AI systems, may produce biologically implausible results or hallucinations when making predictions or generating hypotheses.In bioinformatics, hallucinations could manifest as incorrect sequence alignments, erroneous functional annotations, or false protein-ligand interactions.Unlike hallucinations in natural language processing, which are often a minor inconvenience, incorrect AI-driven bioinformatics predictions can mislead experimental designs and result in wasted resources.Implementing confidence scoring mechanisms, uncertainty quantification, and expert validation is crucial to mitigating AI hallucinations in bioinformatics [18].Recent studies have demonstrated that medical LLMs can produce clinically unsupported content with high linguistic f luency [48,49].</p>
<p>In domains like protein design, such hallucinations, e.g.inaccurate structural or binding predictions, could lead to costly downstream failures in wet-lab experiments.To mitigate these risks, methods such as retrieval-augmented generation [50] and multi-agent cross-validation [51] have been proposed, introducing external factual grounding and structured reasoning workf lows to enhance reliability.</p>
<p>Bias, inclusivity, and security</p>
<p>Bioinformatics datasets are often biased due to limited sample diversity in publicly available repositories.AI agents trained on biased datasets may produce unfair or misleading conclusions, particularly in applications such as disease biomarker discovery and drug response predictions.Bias mitigation strategies, such as adversarial debiasing and fairness-aware learning, are necessary Table 5. Feature comparison of multi-agent systems for agentic bioinformatics.Specialized roles: agents are assigned distinct, complementary roles within the system.Self-optimization: the system utilizes performance feedback to enable self-tuning and adaptive optimization.Human-guided collaboration: the system incorporates scientific expertise through structured mechanisms for human-agent interaction</p>
<p>Method name Specialized roles Self-optimization Human-guided collaboration
CellAgent  The Virtual Lab Bio-Copilot ProtAgents  BioMaster  BioAgents 
to ensure that AI-driven insights are equitable across diverse populations [52].In particular, ancestry-related biases in genomic databases have been widely reported to reduce predictive performance for underrepresented populations [53].In multi-agent settings, fairness-aware coordination strategies [54] can help ensure equitable outcomes across subgroup-specific agents.Additionally, AI-driven bioinformatics workf lows must address security concerns, including adversarial attacks on biological datasets and unauthorized data access.This is especially important in federated or cloud-deployed agentic systems, where data poisoning or backdoor attacks could compromise downstream scientific outcomes [55].</p>
<p>Data privacy and limited annotations</p>
<p>Many bioinformatics applications involve sensitive genomic and clinical data, raising concerns about data privacy and regulatory compliance.Ensuring that AI agents comply with data protection regulations (e.g.GDPR, HIPAA) is crucial for their adoption in biomedical research.Federated learning and privacy-preserving AI techniques, such as differential privacy and homomorphic encryption, can help mitigate data privacy risks [56].Recent advancements demonstrate secure multiparty computation frameworks for genome-wide association studies that preserve individual privacy without sacrificing utility [57].Additionally, limited annotations in biological datasets pose challenges for supervised learning models, necessitating the development of weakly supervised, self-supervised, and semi-supervised learning approaches.Agentic frameworks can benefit from selfsupervised pretraining on large unlabeled datasets followed by few-shot fine-tuning for specialized downstream tasks [58].</p>
<p>Interpretability and explainability in AI-driven bioinformatics</p>
<p>Interpretability remains a major bottleneck in deploying AI-driven bioinformatics models in real-world applications.Many DL-based bioinformatics models function as black boxes, making it difficult for researchers to understand the reasoning behind their predictions.AI agents used in bioinformatics must incorporate explainability techniques to improve model transparency [59].For instance, model attribution tools such as SHAP and Integrated Gradients are increasingly used to explain omics-based classifiers [60,61].In multi-agent settings, it is equally important to provide step-wise provenance logs that allow users to trace decisions across the agent graph.Explainable AI (XAI) is particularly crucial for applications like drug discovery, where the rationale behind molecule-target interactions must be validated before proceeding to experimental validation.</p>
<p>Ethical considerations</p>
<p>The integration of AI agents in bioinformatics raises several ethical concerns.AI-generated biological hypotheses and experimental designs must be evaluated to ensure that they do not lead to unintended consequences, such as unethical genetic modifications or dual-use research applications.Ethical AI frameworks should be developed to guide the responsible use of AI agents in bioinformatics research, ensuring alignment with bioethical principles [62].For example, recent work has demonstrated how generative models can be repurposed to design toxic molecules in silico [63], raising the urgency of adding explicit safeguards.</p>
<p>Integrating kill switches, access constraints, and auditability are necessary for safe agent deployment [21].</p>
<p>Human-AI collaboration in bioinformatics</p>
<p>AI-driven bioinformatics should augment, rather than replace human expertise.However, achieving effective human-AI collaboration remains a challenge due to the steep learning curve associated with AI-driven tools.Researchers must be trained to interact with AI agents, interpret AI-driven insights, and validate computational predictions experimentally.Additionally, AI systems must be designed to communicate their findings in a manner that is comprehensible to domain experts with varying levels of computational expertise [64].Interactive agents that support natural language explanation, backtracking, and what-if analysis have shown promise in reducing cognitive barriers and improving user trust [65].In particular, explainable dialog agents that allow clinicians or biologists to question AI decisions are gaining attention in biomedical domains [21].</p>
<p>Methods</p>
<p>Selection of agentic systems</p>
<p>As the notion of agentic systems in bioinformatics is still nascent but rapidly advancing, the body of literature explicitly using this terminology remains limited.Therefore, we adopted an exploratory and integrative survey methodology aimed at capturing the current landscape of relevant technologies.We performed a targeted search of relevant literature from January 2020 to March 2025 using major academic and preprint databases, including PubMed, arXiv, bioRxiv, Google Scholar, and IEEE Xplore.Our search employed combinations of the following keywords: "AI agent," "autonomous system," "LLM in biology," "wet-lab automation," "robot scientist," "bioinformatics pipeline," "multi-agent system," and "agent-based AI in science".</p>
<p>We included studies, tools, and platforms that met the following criteria:</p>
<ol>
<li>Explicit or implicit use of autonomous or semi-autonomous AI systems that support tasks across the bioinformatics workf low.2. Relevance to biological data acquisition, analysis, or interpretation in either wet-lab or dry-lab contexts.3. Availability through peer-reviewed publications, preprints, or reputable open-source platforms with community recognition.4. Systems that exhibit agentic characteristics, such as goaldirected behavior, contextual reasoning, perception-action loops, or integration with physical or digital environments.</li>
</ol>
<p>Discussion</p>
<p>The prospect of an end-to-end automated biological discovery system powered by agentic bioinformatics represents a revolutionary shift in how biological research is conducted.In this forward-thinking model, intelligent AI agents are fully integrated throughout the research pipeline, autonomously managing every phase, from data generation and experimental design to execution, data acquisition, and analysis.This transformation has the potential to significantly reduce the time and resources needed for scientific discovery while increasing precision, reproducibility, and scalability.However, realizing this vision requires addressing the diversity of biological research, the iterative nature of experimental design, and the technical and ethical challenges of integrating AI into laboratory workf lows.</p>
<p>The AI-driven laboratory: a vision for automation</p>
<p>In the envisioned AI-driven laboratory, intelligent agents operate across all stages of the experimental workf low [66,67].They can autonomously generate synthetic biological data to simulate complex systems, which is particularly valuable when realworld data are scarce or difficult to obtain.AI agents can also design experiments by selecting optimal methodologies, determining experimental parameters, and refining protocols based on prior results [68,69].During the execution phase, AI-controlled robotic systems automate tasks such as liquid handling, sample preparation, and real-time imaging [70].This level of automation ensures precision and reduces human error.It also enables highthroughput experimentation crucial for large-scale studies.Once experiments are conducted, AI agents immediately process and analyze the resulting data, using machine learning algorithms to identify patterns, uncover novel insights, and suggest new avenues for exploration [29].This seamless integration reduces the time between data collection and actionable results, enabling faster hypothesis testing and refinement.</p>
<p>Types of experiments amenable to automation</p>
<p>While the potential of AI-driven laboratories is immense, it is important to recognize that biological research is highly diverse, and not all experiments are equally suited to automation.Highthroughput approaches, such as multi-omics profiling, are particularly well-suited for automation due to their parallelizable nature and reliance on large-scale data generation [71].These experiments benefit significantly from AI agents, which can efficiently manage repetitive tasks, process vast datasets, and identify patterns that human researchers might miss [29,72].In contrast, low-throughput experiments often require customized skills and designs, making them less amenable to full automation.For example, experiments involving complex organismal behavior or rare biological phenomena may still rely heavily on human expertise and manual intervention.</p>
<p>Moreover, the iterative nature of biological research poses a unique challenge for automation.Experimental design often depends on the results of prior experiments, requiring a feedback loop where hypotheses are refined and protocols are adjusted based on new data.AI agents can play a critical role in this process by rapidly analyzing results, suggesting modifications, and optimizing experimental parameters in real-time.However, this requires robust integration between AI systems and laboratory equipment, as well as the ability to adapt to unexpected outcomes [73].</p>
<p>Multi-agent systems: collaboration and scalability</p>
<p>A key development in this paradigm is the use of multi-agent systems, where multiple AI agents collaborate to accomplish complex tasks.In such systems, each agent focuses on a specific aspect of the research process, such as data collection, analysis, or resource management.For instance, one agent may preprocess data from biological samples, while another runs machine learning algorithms to identify key biomarkers, and yet another optimizes experimental protocols in real-time.This collaborative approach allows for the efficient execution of multifaceted research tasks that would be impossible for a single agent to accomplish in isolation.</p>
<p>Multi-agent systems also enhance the adaptability of the research process.As new challenges or research questions arise, agents can autonomously adjust their behavior or cooperate to address emerging issues without significant human intervention.This adaptability is particularly valuable in dynamic research environments, where experimental conditions and objectives may evolve over time.</p>
<p>Technical challenges and solutions</p>
<p>Despite the promise of AI-driven laboratories, several technical challenges must be addressed to realize their full potential.One major challenge is the integration and standardization of diverse AI systems [74].Effective collaboration between agents requires a unified platform that can accommodate different types of AI, such as machine learning, reinforcement learning, and natural language processing.Standardized protocols for data exchange and communication between agents are essential to ensure seamless interoperability [75].</p>
<p>Another challenge is the integration of AI agents with physical laboratory equipment.Ensuring that AI systems can control and interact with devices such as robotic arms, sensors, and microscopes requires robust hardware-software interfaces and realtime feedback mechanisms.Advances in the Internet of Things (IoT) and edge computing may provide solutions to these challenges, enabling seamless communication between AI agents and laboratory devices.</p>
<p>Ethical and regulatory considerations</p>
<p>The adoption of AI-driven laboratories also raises important ethical and regulatory questions.Issues such as data privacy, the interpretability of experimental results, and the transparency of AI decision-making must be carefully addressed.For example, AI-generated insights must be explainable and reproducible to maintain scientific rigor and public trust.Additionally, the use of AI in sensitive areas such as personalized medicine and genetic engineering requires robust ethical guidelines to prevent misuse and ensure equitable access to technological advancements.</p>
<p>Implications for scientific discovery</p>
<p>The implications of fully automated, multi-agent research environments are profound.By automating routine and laborintensive tasks, researchers can focus on higher level scientific thinking and hypothesis generation.The speed and efficiency of AI-driven experiments would enable the exploration of previously infeasible research questions, accelerating the pace of scientific discovery.Moreover, the scalability of automated systems means that the volume and complexity of experiments that can be conducted would increase exponentially, potentially revolutionizing fields such as genomics, drug discovery, and personalized medicine.</p>
<p>This new paradigm also has the potential to democratize access to cutting-edge experimental capabilities.Researchers with limited expertise in computational methods could harness the power of AI for their work, broadening participation in scientific research and accelerating progress across the biological sciences.By combining the strengths of AI-driven automation with human creativity and insight, the future of biological discovery holds unprecedented promise.</p>
<p>Conclusion</p>
<p>Agentic bioinformatics has emerged as a groundbreaking paradigm that integrates AI agents into the bioinformatics pipeline, ushering in a new era of precision, efficiency, and innovation in biological research.AI agents are now playing a key role in various aspects of bioinformatics, from data generation and experimental design to data analysis and interpretation.Their ability to autonomously perform complex tasks has the potential to drastically reduce human error, accelerate discovery, and enable researchers to address questions that were previously out of reach.This development marks a significant shift in how biological research is conducted, with AI agents now complementing, and in some cases replacing, traditional humancentered workf lows.</p>
<p>Despite the remarkable advancements, there remain significant challenges to overcome in the field of agentic bioinformatics.One of the primary hurdles is the continued improvement of AI agents' intelligence and capabilities.Current systems often face limitations, such as biases in algorithms, poor-quality data, and challenges in generalizing across diverse biological contexts.Enhancing the robustness, adaptability, and accuracy of these AI systems will be critical in ensuring their reliability and scalability for a wide range of biological research applications.Additionally, as AI agents become more integral to bioinformatics, there is an increasing need for more advanced integration techniques that allow these systems to work seamlessly across various domains, from machine learning and data analysis to laboratory automation.</p>
<p>The future of agentic bioinformatics also lies in fostering crossdisciplinary collaboration.The convergence of AI, computational biology, and experimental sciences is essential for advancing the field.Realizing the full potential of AI-driven research requires close collaboration among bioinformaticians, data scientists, experimental biologists, and AI experts.This collaboration must ensure AI systems incorporate biological understanding while experimental workf lows optimize AI agents' utilization.This collaborative effort will be pivotal in overcoming the current technological limitations and will lead to the development of more intuitive, f lexible, and powerful systems.</p>
<p>Looking ahead, we can anticipate that AI agents will become increasingly ubiquitous in bioinformatics over the next few years.</p>
<p>As the technology continues to evolve, the role of AI agents in the field will expand from supporting individual tasks to orchestrating entire research workf lows, leading to a future where end-to-end automated biological discovery is a reality.This evolution will likely revolutionize not only research but also clinical applications, particularly in areas like personalized medicine, drug discovery, and disease modeling.By automating complex processes and offering more precise, data-driven insights, AI agents will empower researchers to make faster and more informed decisions, ultimately leading to breakthroughs that have profound implications for human health and our understanding of biology.</p>
<p>In conclusion, agentic bioinformatics holds great promise, but achieving its full potential will require overcoming technical, ethical, and collaborative challenges.As AI technologies advance and integrate further into biological research, the field will undoubtedly experience a transformation.AI agents will play a critical role in the next wave of scientific discovery and clinical innovation.</p>
<p>Key Points</p>
<p> Agentic bioinformatics is an emerging paradigm that leverages AI agents powered by large language models (LLMs) to autonomously analyze, interpret, and explore complex biological data. Agentic bioinformatics represents a shift from traditional, static bioinformatics workf lows toward dynamic, adaptive, and scalable systems capable of self-directed biological discovery. Agentic bioinformatics has promising applications in personalized medicine, drug discovery, and synthetic biology, where autonomous decision-making can accelerate innovation. This review outlines key technical, ethical, and scalability challenges, emphasizing the need for robust infrastructure and responsible deployment of agentic systems.</p>
<p>Table 1 .
1
Glossary of key terms in agentic bioinformatics
TermDefinitionAI agentAn autonomous or semi-autonomous computational entity capable of perceiving its environment, processing information,and taking actions to achieve specific goals. In the context of bioinformatics, AI agents operate across both wet-lab anddry-lab settings.LLMA type of AI model trained on large corpora of text to perform natural language processing tasks. In this framework,LLMs are used as foundational components for agents such as Literature Review Agents and Reasoning Agents, enablingsophisticated language understanding and generation.Agentic bioinformaticsAn interdisciplinary paradigm that integrates autonomous AI agents into the bioinformatics lifecycle, from hypothesisgeneration and experimental execution to data analysis and interpretation, across both wet-lab and dry-lab environments.Wet-lab AI agentAn AI-driven system, often embodied in hardware or interfaced with physical lab equipment, that performs experimentaltasks such as sample preparation, PCR, microscopy, or animal testing. These agents support experimental throughput,precision, and reproducibility.Dry-lab AI agentA software-based AI entity that handles computational tasks, including data mining, statistical analysis, machine learning,and hypothesis generation, using digital datasets such as genomic sequences or imaging data.Search agentA type of dry-lab agent designed to retrieve relevant scientific information from structured and unstructured databases,assisting in the knowledge-gathering phase of research.Literature review agentAn AI agent powered by LLMs that synthesizes insights from scientific literature to support experiment planning andcontextualization of results.Database agentA dry-lab AI agent that manages and queries large-scale biological databases, enabling efficient access to structuredbiological data.Innovative AI agentAn AI agent that can design forward-thinking AI applications, such as the design of novel algorithms and the developmentof novel applications.</p>
<p>Table 2 .
2
Single-agent systems for agentic bioinformatics.An asterisk ( * ) indicates a non-peer-reviewed paper
Reproducibilityhttps://github.com/batmen-lab/BioMANIAhttps://github.com/JoshuaChou2018/AutoBAhttps://github.com/biagent-dev/biahttps://github.com/jpickard1/bradhttps://github.com/GT4SD/lm-assistant-for-biocatalysishttps://github.com/cong-lab/crispr-gpt-pubhttps://github.com/snap-stanford/BioDiscoveryAgenthttps://github.com/Genentech/SpatialAgenthttps://github.com/ur-whitelab/Task Key featureomics data analysis A chatbot generation pipeline with a user-friendly back-endservice for seamless interactionomics data analysis Automated multi-omics analysis with minimal user input whileproviding detailed, step-by-step analysis plansscRNA-seq data analysis Autonomous bioinformatic analysis through natural languageVarious biological tasks Enable tasks such as Retrieval-Augmented Generation, searchesacross bioinformatics databases, and the execution of softwarepipelinesenzyme engineering A computational tool that merges LLMs with biocatalysis-specificmodules to streamline enzyme engineeringCRISPR genome engineering Assist nonexpert researchers in designing gene-editingexperiments and validates effectiveness in real-world use casesdesign genetic perturbation Design new experiments, reasons about their outcomes, andexperiments efficiently navigates the hypothesis space to reach desiredsolutionsconduct spatial genomics Process multimodal inputs, incorporate external databases, andresearch support human-in-the-loop interactions, enabling both fullyautomated and collaborative discoveryMolecular dynamics Consist of an environment of tools that emit observations and ansimulations LLM that selects actionsJournal ModelbioRxiv *  GPT4Advanced Multiple LLMsSciencebioRxiv *  GPT4Bioinformatics GPT4bioRxiv *  GPT4bioRxiv *  GPT4ICLR Claude 3.5SonnetbioRxiv *  GPT-4o, Claude3.5 SonnetarXiv *  GPT-4o,llama3-405bMethod name YearBioMANIA 2023AutoBA 2023BioInformatics Agent (BIA) 2024BRAD 2024LM-ABC 2024CRISPR-GPT 2024BioDiscoveryAgent 2024SpatialAgent 2025MDCrow 2025</p>
<p>Table 4 .
4
Multi-agent systems for agentic bioinformatics.An asterisk ( * ) indicates a non-peer-reviewed paper
Key featureTaskModelJournalYearMethod name
Data availabilityNo new data were generated or analysed in support of this research.FundingThis work is supported by the King Abdullah University of Science and Technology (KAUST) Office of Research Administration (ORA) under Award No REI/1/5234-01-01, REI/1/5414-01-01, REI/1/5289-01-01, REI/1/5404-01-01, REI/1/5992-01-01, URF/1/4663-01-01, Center of Excellence for Smart Health (KCSH), under award number 5932, and Center of Excellence on Generative AI, under award number 5940.This work is supported by The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), under Award No UDF01004172.Author contributionsJuexiao
Early bioinformatics: the birth of a discipline-a personal view. C A Ouzounis, A Valencia, 10.1093/bioinformatics/btg309Bioinformatics. 192003</p>
<p>. B Hesper, P Hogeweg, Bioinformatica: Een werkconcept. Kameleon. 11970</p>
<p>DNA sequencing at 40: past, present and future. J Shendure, S Balasubramanian, G M Church, 10.1038/nature24286Nature. 5502017</p>
<p>Multiple sequence alignment with Clustal x. F Jeanmougin, J D Thompson, M Gouy, 10.1016/S0968-0004(98)01285-7S0968-0004(98)01285-7Trends Biochem Sci. 231998</p>
<p>The national DNA database. D J Werrett, 10.1016/S0379-0738(97)00081-9S0379-0738(97)00081-9Forensic Sci Int. 881997</p>
<p>The Ensembl genome database project. T Hubbard, D Barker, E Birney, 10.1093/nar/30.1.38Nucleic Acids Res. 302002</p>
<p>. M Vihinen, 10.1016/S1389-0344(01)00099-5Bioinformatics in proteomics. Biomol Eng. 182001</p>
<p>Deep learning in bioinformatics. S Min, B Lee, S Yoon, 10.1093/bib/bbw068Brief Bioinform. 182017</p>
<p>A brief history of bioinformatics. J Gauthier, A T Vincent, S J Charette, 10.1093/bib/bby063Brief Bioinform. 202019</p>
<p>Combined mechanistic modeling and machine-learning approaches in systems biology-a systematic literature review. A Procopio, G Cesarelli, L Donisi, 10.1016/j.cmpb.2023.107681Comput Methods Programs Biomed. 2401076812023</p>
<p>Applications of transformerbased language models in bioinformatics: a survey. S Zhang, Fan R Liu, Y , 10.1093/bioadv/vbad001Bioinf Adv. 312023</p>
<p>Diffusion models in bioinformatics and computational biology. Z Guo, J Liu, Y Wang, 10.1038/s44222-023-00114-9Nat Rev Bioeng. 22024</p>
<p>Bioinformatics: An Introduction. Springer Nature. J Ramsden, 10.1007/978-3-030-45607-82023</p>
<p>Deep learning. Y Lecun, Y Bengio, G Hinton, 10.1038/nature14539Nature. 5212015</p>
<p>Ensemble deep learning in bioinformatics. Y Cao, T A Geddes, Jyh Yang, 10.1038/s42256-020-0217-yNat Mach Intell. 22020</p>
<p>Leveraging transformers-based language models in proteome bioinformatics. Nqk Le, 10.1002/pmic.202300011Proteomics. 2323000112023</p>
<p>Sa-TTCA: an SVM-based approach for tumor t-cell antigen classification using features extracted from biological sequencing and natural language processing. T-O Tran, Nqk Le, 10.1016/j.compbiomed.2024.108408Comput Biol Med. 1741084082024</p>
<p>Large language models in biomedicine and healthcare. J Zhou, H Li, Chen S , Caduceus. 2977</p>
<p>Exploring large language model based intelligent agents: definitions, methods, and prospects. Y Cheng, C Zhang, Z Zhang, 2024</p>
<p>LLM-based agentic systems in medicine and healthcare. J Qiu, K Lam, G Li, 10.1038/s42256-024-00944-1Nat Mach Intell. 62024</p>
<p>Empowering biomedical discovery with ai agents. S Gao, A Fang, Y Huang, 10.1016/j.cell.2024.09.022Cell. 1872024</p>
<p>N O Committee, Nettab, Workshop on Agents in Bioinformatics. 2001. 12 July 2024</p>
<p>N O Committee, Nettab 2002 Workshop: Agents in Bioinformatics. 2002/2002. 12 July 2024</p>
<p>Agents in bioinformatics, computational and systems biology. E Merelli, G Armano, N Cannata, 10.1093/bib/bbl014Brief Bioinform. 82007</p>
<p>Agent-based models for detecting the driving forces of biomolecular interactions. S Maestri, E Merelli, M Pettini, 10.1038/s41598-021-04205-8Sci Rep. 1218782022</p>
<p>Taverna: a tool for the composition and enactment of bioinformatics workflows. T Oinn, M Addis, J Ferris, 10.1093/bioinformatics/bth361Bioinformatics. 202004</p>
<p>Snakemake-a scalable bioinformatics workflow engine. J Kster, S Rahmann, 10.1093/bioinformatics/bts480Bioinformatics. 282012</p>
<p>Reproducible, scalable, and shareable analysis pipelines with bioinformatics workflow managers. L Wratten, A Wilm, J Gke, 10.1038/s41592-021-01254-9Nat Methods. 182021</p>
<p>An AI agent for fully automated multi-omic analyses. J Zhou, B Zhang, G Li, 10.1002/advs.202407094Adv Sci. 11e24070942024</p>
<p>On the responsible use of chatbots in bioinformatics. G Hu, L Liu, D Xu, 10.1093/gpbjnl/qzae002Genomics Proteomics Bioinf. 22e0022024</p>
<p>Simplifying bioinformatics data analysis through conversation. Z Dong, H Zhou, Y Jiang, 10.1101/2023.10.29.564479</p>
<p>Bioinformatics agent (BIA): unleashing the power of large language models to reshape bioinformatics workflow. Q Xin, Q Kong, Ji H , 10.1101/2024.05.22.595240</p>
<p>Automatic biomarker discovery and enrichment with BRAD. J Pickard, R Prakash, M A Choi, 10.1093/bioinformatics/btaf159Bioinformatics. 411592025</p>
<p>A language model assistant for biocatalysis. Nana Teukam, Y G Grisoni, F Manica, M , 10.1101/2024.11.15.623739</p>
<p>CRISPR-GPT: an LLM agent for automated design of gene-editing experiments. Y Qu, K Huang, H Cousins, 10.1101/2024.04.25.591003</p>
<p>SpatialAgent: an autonomous AI agent for spatial biology. H Wang, Y He, P P Coelho, </p>
<p>CellAgent: an LLM-driven multi-agent framework for automated single-cell data analysis. Y Xiao, J Liu, Y Zheng, </p>
<p>. 10.1101/2024.05.13.593861</p>
<p>BioMaster: multi-agent system for automated bioinformatics analysis workflow. H Su, W Long, Y Zhang, </p>
<p>A data-intelligence-intensive bioinformatics copilot system for large-scale omics researches and scientific insights. Y Liu, R Shen, L Zhou, 10.1101/2024.05.19.594895</p>
<p>ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning. A Ghafarollahi, M J Buehler, </p>
<p>The virtual lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation. K Swanson, W Wu, N L Bulaong, 10.1101/2024.11.11.623004</p>
<p>A new AI-assisted data standard accelerates interoperability in biomedical research. R A Long, S Ballard, S Shah, medRxiv. 2024</p>
<p>A review on machine learning principles for multi-view biological data integration. Y Li, F-X Wu, A Ngom, 10.1093/bib/bbw113Brief Bioinform. 192018</p>
<p>Out-ofdistribution generalization from labelled and unlabelled gene expression data for drug response prediction. H Sharifi-Noghabi, P A Harjandi, O Zolotareva, 10.1038/s42256-021-00408-wNat Mach Intell. 32021</p>
<p>Transferable discriminative learning for medical open-set domain adaptation: application to pneumonia classification. W Su, Wang F Han, Z , 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE2022</p>
<p>Visual out-of-distribution detection in open-set noisy environments. R He, Z Han, X Nie, 10.1007/s11263-024-02139-yInt J Comput Vision. 1322024</p>
<p>How well does GPT-4V (ision) adapt to distribution shifts? A preliminary investigation. Z Han, G Zhou, R He, 2023</p>
<p>Can large language models reason about medical questions?. V Livin, C E Hother, A G Motzfeldt, 10.1016/j.patter.2024.100943Patterns. 51009432024</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, 10.1145/3571730ACM Comput Surv. 552023</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. P Lewis, E Perez, A Piktus, Advances in neural information processing. 2020</p>
<p>ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning. A Ghafarollahi, M J Buehler, 10.1039/D4DD00013GDigital Discovery. 32024</p>
<p>A survey on bias and fairness in machine learning. N Mehrabi, F Morstatter, N Saxena, 10.1145/3457607ACM Comput Surv. 542021</p>
<p>Genomics of disease risk in globally diverse populations. D Gurdasani, I Barroso, E Zeggini, 10.1038/s41576-019-0144-0Nat Rev Genet. 202019</p>
<p>Learning fairness in multi-agent systems. J Jiang, Z Lu, Advances in Neural Information Processing Systems. 322019</p>
<p>A survey of adversarial machine learning in cyber warfare. V Duddu, 10.14429/dsj.68.12371Def Sci J. 683562018</p>
<p>Privacy-preserving machine learning: threats and solutions. M Al-Rubaie, J M Chang, 10.1109/MSEC.2018.2888775IEEE Security Privacy. 172019</p>
<p>Secure genome-wide association analysis using multiparty computation. H Cho, D J Wu, B Berger, 10.1038/nbt.4108Nat Biotechnol. 362018</p>
<p>Foundation models for bioinformatics. Z Chen, L Wei, G Gao, 10.1002/qub2.69Quant Biol. 122024</p>
<p>An explainable AI-driven biomarker discovery framework for non-small cell lung cancer classification. K Dwivedi, A Rajpal, S Rajpal, 10.1016/j.compbiomed.2023.106544Comput Biol Med. 1531065442023</p>
<p>A unified approach to interpreting model predictions. S Lundberg, S-I Lee, Advances in Neural Information Processing Systems. 302017</p>
<p>Interpretable deep learning for chromatin-informed inference of transcriptional programs driven by somatic alterations across cancers. Y Tao, X Ma, D Palmer, 10.1093/nar/gkac881Nucleic Acids Res. 50108692022</p>
<p>The global landscape of AI ethics guidelines. A Jobin, M Ienca, E Vayena, 10.1038/s42256-019-0088-2Nat Mach Intell. 12019</p>
<p>Dual-use of artificialintelligence-powered drug discovery. F Urbina, F Lentzos, C Invernizzi, 10.1038/s42256-022-00465-9Nat Mach Intelli. 42022</p>
<p>Identifying challenges and opportunities in human-AI collaboration in healthcare. S Y Park, P-Y Kuo, A Barbarin, 10.1145/3311957.3359433?casa_token=kfDuyl1xCgAAAAAA:vNJU_OTPsphSXmNkWG3DXkiUJqgJbTs0P_VdzAWwpLHjlU3ZazdlGV_pMiHaYh6lK8w8MCzZff6NCompanion Publication of the 2019 Conference on Computer Supported Cooperative Work and Social Computing. 2019</p>
<p>Scientific exploration and explainable artificial intelligence. C Zednik, H Boelsen, 10.1007/s11023-021-09583-6Minds Mach. 322022</p>
<p>AI-driven robotic laboratories show promise. S O'neill, 10.1016/j.eng.2021.08.006Engineering. 72021</p>
<p>National Academies of Sciences et al. AIenabled biological design and the risks of synthetic biology. C Science, T Board, E , The Age of AI in the Life Sciences: Benefits and Biosecurity Considerations. USNational Academies Press2025</p>
<p>Biodiscoveryagent: An AI agent for designing genetic perturbation experiments. Y Roohani, A Lee, Q Huang, 2024</p>
<p>The virtual lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation. K Swanson, W Wu, N L Bulaong, bioRxiv. 2024. 2024</p>
<p>Real-time AIdriven quality control for laboratory automation: a novel computer vision solution for the opentrons OT-2 liquid handling robot. S U Khan, V K Mller, Rjn Frandsen, 10.1007/s10489-025-06334-3Appl Intell. 552025</p>
<p>Revolutionizing multi-omics analysis with artificial intelligence and data processing. A Yetgin, 10.1002/qub2.70002Quant Biol. 13e700022025</p>
<p>TransAgent: dynamizing transcriptional regulation analysis via multi-omics-aware AI agent. G Zhang, C Song, L Liu, bioRxiv. 2025</p>
<p>The automated lab of tomorrow. D Adam, 10.1073/pnas.2406320121Proc Natl Acad Sci. 121e24063201212024</p>
<p>Artificial intelligence standardization is a key challenge for the technologies of the future. V Golenkov, N Guliakina, V Golovko, 10.1007/978-3-030-60447-9_1International Conference on Open Semantic Technologies for Intelligent Systems. Springer2020https</p>
<p>Model context protocol (MCP): landscape, security threats, and future research directions. X Hou, Y Zhao, Wang S , 2025</p>
<p>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. Author The, 10.1093/bib/bbaf505Briefings in Bioinformatics. 26520252025Oxford University PressPublished by. This is an Open Access article distributed under the terms of the Creative Commons Attribution License</p>            </div>
        </div>

    </div>
</body>
</html>
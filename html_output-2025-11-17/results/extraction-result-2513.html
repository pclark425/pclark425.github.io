<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2513 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2513</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2513</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-270786925</p>
                <p><strong>Paper Title:</strong> Automation and machine learning augmented by large language models in a catalysis study</p>
                <p><strong>Paper Abstract:</strong> Recent advancements in artificial intelligence and automation are transforming catalyst discovery and design from traditional trial-and-error manual mode into intelligent, high-throughput digital methodologies. This transformation is driven by four key components, including high-throughput information extraction, automated robotic experimentation, real-time feedback for iterative optimization, and interpretable machine learning for generating new knowledge. These innovations have given rise to the development of self-driving labs and significantly accelerated materials research. Over the past two years, the emergence of large language models (LLMs) has added a new dimension to this field, providing unprecedented flexibility in information integration, decision-making, and interacting with human researchers. This review explores how LLMs are reshaping catalyst design, heralding a revolutionary change in the fields.</p>
                <p><strong>Cost:</strong> 0.033</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2513.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2513.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian Optimization (BO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic optimization framework that builds a surrogate model of an expensive-to-evaluate objective and uses an acquisition function (e.g., EI, PI, UCB) to select new experiments that trade off exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian optimization (general framework)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BO constructs a surrogate (commonly a Gaussian process or other regressor) to approximate a black-box objective (e.g., catalytic activity). An acquisition function scores candidate points according to expected utility; the highest-scoring points are selected for true evaluation (experiment or high-fidelity simulation). Iteratively, observations update the surrogate and the process repeats.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Catalyst discovery, materials optimization, experimental parameter tuning (electrocatalysis, photocatalysis, alloy composition optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate experimental/simulation budget to candidate points maximizing a chosen acquisition function (e.g., Expected Improvement, Probability of Improvement, Upper Confidence Bound), thereby selecting points that balance predicted high performance and model uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Reported/used metrics in examples: number of physical experiments or number of DFT simulations (counts); retrospective comparisons use wall-clock experimental time reductions (e.g., 'experiments per day' or 'days of autonomous running'). FLOPs/dollars not reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition functions that implicitly capture expected improvement/uncertainty reduction (EI, PI, UCB); Bayesian inference updating the surrogate posterior acts as the information-update mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit via acquisition functions: EI/PI/UCB trade off exploitation of high predicted objective values and exploration of high-uncertainty regions; surrogate posterior variance drives exploration component.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No general, explicit diversity-promoting mechanism is mandated by BO itself in the paper; extensions cited (e.g., Gryffin, Phoenics) address categorical/structured variables but explicit diversity heuristics are not central in baseline BO examples.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of experiments or compute-limited (number of DFT runs); time-limited laboratory runs (days/hours).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>BO inherently prioritizes highest-utility experiments under a limited budget (select top acquisition-scoring points iteratively); practitioners cap iterations or stop when improvement saturates or budget exhausted.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Task-dependent performance metrics such as peak current density (electro-oxidation), C2 yield, Faradaic efficiency, activity volcano metrics; breakthroughs identified by surpassing prior best performance thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples reported in paper: Yamauchi et al. found optimal PtPdAu composition in 47 experiments (<1% of composition space); Cooper's mobile chemist ran 688 experiments in 8 days (photocatalyst discovery); Rossmeisl et al. BO ran ~150 iterations and reported up to 20x acceleration vs grid search in one study. Metrics are counts of experiments/iterations and task-specific performance (current density, yields).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Grid search, manual experimentation, Gaussian-process baselines, random or uniform sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BO substantially reduced required experiments relative to grid or uniform search: e.g., Yamauchi: optimal found in 47 experiments (<1% of space); Rossmeisl: up to ~20x faster than grid search; Cooper: autonomous BO pipeline produced 60x reduction in experimental time compared to manual operations (paper's reported figure).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported examples: discovery using <1% of search space (Yamauchi), up to 20x fewer iterations to reach optimum (Rossmeisl), and ~60x reduction in experimental time for autonomous BO-driven runs (Cooper mobile chemist example in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The review emphasizes BO's formal balancing of exploration and exploitation via acquisition functions and discusses limits: surrogate models interpolate better than extrapolate, choice of descriptors affects efficiency, and token/representation limits (when using LLM surrogates) can impede performance. It notes that adding domain knowledge (e.g., via LLMs) can improve allocation efficiency beyond pure BO.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Principles: use probabilistic surrogates with acquisition functions to maximize expected utility per experiment; incorporate domain knowledge (descriptors, physics) to improve surrogate quality; restrict search via informed descriptors to reduce wasted budget; stop when acquisition improvement plateaus. For categorical/discrete domains use specialized BO variants (e.g., Gryffin).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2513.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2513.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active Learning (AL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Learning (uncertainty/information-driven sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adaptive sampling approach where the learning model selects new data points to label (perform simulation/experiment) that are expected to most improve model performance, typically by targeting high uncertainty or high expected error.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Active learning frameworks for catalyst discovery</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AL workflows train an initial surrogate on a small set of evaluated points, then iteratively select next candidates using heuristics such as model uncertainty (ensemble variance), predicted error estimators, or information-theoretic criteria; the new labeled points are measured with high-fidelity experiments/DFT and used to retrain the model.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>High-throughput DFT screening, electrocatalyst discovery (CO2RR, HER), materials property prediction, reaction yield prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate expensive labels (DFT calculations or experiments) to points of highest expected information gain — e.g., highest prediction variance from an ensemble, highest predicted model error, or that maximize a proxy for uncertainty-reduction per cost.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of expensive labels performed (count of DFT calculations or physical experiments); wall-clock time for experiments/DFT is mentioned qualitatively; no unified FLOPs/dollar metric presented.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uncertainty reduction (ensemble prediction variance), predicted error from an error model (KRR error predictor), and related information-theoretic heuristics are used as proxies for information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration via selecting high-uncertainty/unlabeled regions; exploitation arises when selecting points with promising predicted performance as part of optimization-oriented AL; choice depends on objective (model-building vs global optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Some AL implementations implicitly promote diversity by choosing high-uncertainty candidates across the space and by using ensemble-based selection; agent-based cyclic workflows (Ulissi et al.) select across diverse candidate classes, but explicit diversity-penalty mechanisms are not standardized across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of expensive labels (DFT runs / experiments); compute and chemical consumption limits.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>AL prioritizes samples expected to maximize model improvement per label; ensembles and error predictors estimate where labels are most valuable, and iterations continue until budget is consumed or model convergence criteria met.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Identification of candidates with superior computed properties (e.g., favorable adsorption energies) or experimentally validated high activity; breakthrough defined by exceeding performance thresholds or finding previously unanticipated high-performing materials.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples: Jung et al. reported an ensemble/KRR AL pipeline with best model RMSE ~0.05 eV for *CO binding energy predictions; Ulissi-style cyclic workflows reduced DFT workload by focusing on promising candidates (search over 1499 intermetallics reduced to handfuls); Sargent et al. performed ~300 ML regression cycles and guided DFT for ~4000 adsorption sites out of a much larger search space to identify Cu-Al catalysts with experimentally validated >80% Faraday efficiency (vs ∼66% for pure Cu).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random sampling, exhaustive DFT screening, uniform sampling, and non-adaptive ML-assisted selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>AL drastically reduced required expensive labels relative to exhaustive search: e.g., instead of evaluating thousands-to-hundreds-of-thousands of sites, guided AL focused DFT on ~4000 sites and still identified high-performance catalysts; reported RMSEs and discovery rates improved relative to unguided sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Examples: Sargent et al. guided ~4000 DFT calculations rather than enumerating all ~228k adsorption sites, enabling practical screening; Jung et al. achieved low RMSE with far fewer ab initio calculations via AL-driven selection.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The review notes tradeoffs: AL reduces expensive label cost but depends on surrogate generality (interpolation vs extrapolation limits); descriptor choice affects cost-effectiveness; there is a tension between maximizing immediate discovery (optimization) and improving the global model (information collection).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Effective allocation uses uncertainty- or error-based selection, ensemble methods for uncertainty estimation, and combines cheap descriptors or physics-based surrogates to reduce expensive evaluations; integrating domain knowledge (e.g., theory-based descriptors or LLM output) further improves allocation efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2513.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2513.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mobile Robotic Chemist (Cooper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A mobile robotic chemist (AGV-based autonomous mobile robot system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated mobile-robot laboratory that autonomously executes high-throughput experiments across distributed workstations and uses Bayesian algorithms to select experiments for photocatalyst discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A mobile robotic chemist</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AGV-based autonomous mobile robotic chemist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A mobile robotic platform with an autonomous guided vehicle (AGV) and multiple workstations (solid/liquid dispensing, sonication, photolysis, GC, storage). It executes automated experiments and uses Bayesian optimization to choose experimental conditions; data are fed back iteratively to the BO loop.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Photocatalyst optimization for water-splitting / hydrogen production.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Bayesian optimization-driven selection of next experiments within a 10-variable design space; acquisition function selects promising compositions/conditions to evaluate experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Experimental counts and wall-clock lab time (e.g., 688 experiments over 8 days). Computational costs of BO surrogate not quantified in FLOPs or dollars in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicit via the BO acquisition function (which uses surrogate predictive mean and variance to estimate expected improvement/information value).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>BO acquisition function balances exploring uncertain regions vs exploiting high predicted photocatalytic activity.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-promoting mechanism described beyond the BO acquisition-driven sampling; the system does not incorporate domain theoretical models in its decision-making (paper notes this limitation).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Time-limited experimental run and consumables (lab time, reagents); implicit fixed experiment count per campaign.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>BO selects experiments expected to yield maximal improvement per experiment under the campaign time window; practical constraints handled by scheduling across workstations.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Photocatalytic activity metrics such as hydrogen production rate/photocatalytic current density and identification of formulations with substantially higher activity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Example: Autonomous run completed 688 experiments in 8 days and discovered improved photocatalysts; the platform reportedly delivered ~60x reduction in experimental time cost compared to manual operations (as reported in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Manual experimental screening and non-autonomous workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Significant speed-up: ~60x reduction in experimental time vs manual operations; discovery of higher-activity formulations that would have been difficult for chemists to predict.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>~60x reduction in experimental time; discovered high-performing photocatalysts within an automated campaign.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper highlights tradeoffs: robotic mobility and flexibility vs throughput; lack of domain-model integration limits chemical insight; BO reduces experiments but may miss opportunities without embedded chemical knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Empirical finding: autonomous BO-guided experimentation across distributed stations can accelerate discovery dramatically; integrating chemical/domain knowledge into the acquisition or surrogate could further improve allocation efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2513.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2513.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BEAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Experiments for Autonomous Researchers (BEAR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that combines Bayesian optimization with high-throughput automated experimentation to close the loop for self-driving material discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Bayesian experimental autonomous researcher for mechanical design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BEAR (Bayesian experimental autonomous researcher)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BEAR integrates BO with automated experimental platforms to autonomously propose and execute experiments, collect measurements, and update the surrogate model iteratively, enabling end-to-end autonomous material discovery cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Autonomous experimental design across materials and mechanical design contexts; exemplified in materials discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Use BO acquisition functions to prioritize experiments that maximize expected improvement/information per experiment; execution is automated by high-throughput hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Count of executed experiments and total autonomous runtime; specific computational cost metrics are not provided in detail in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition functions grounded in Bayesian posterior (expected improvement or related criteria) which implicitly measure expected utility/information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition-function-driven trade-off between sampling high-mean and high-uncertainty regions; iteratively updated surrogate directs subsequent experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not described as having explicit diversity-promoting sub-algorithms in the review; diversity arises indirectly from the acquisition function and problem geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experimental budget (number of automated experiments) and physical lab resource constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>BO-driven prioritization of experiments to maximize gain under the available automation/trial budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Task-specific improvement in objective (e.g., mechanical performance or materials property); the framework targets largest expected improvement per automated trial.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The review cites BEAR as an example of BO+automation enabling self-driven discovery; explicit numeric outcomes for BEAR within this review are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Manual experiment planning and non-autonomous optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not numerically quantified in this review; BEAR is presented as a paradigm combining BO with automation to accelerate discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not specified numerically in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>BEAR reduces human planning overhead and trial counts via BO but inherits the surrogate/model limitations discussed for BO and AL (descriptor dependence, interpolation limits).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Combining BO with automation yields efficient allocation of physical experiments; careful surrogate and acquisition choices are critical to maximize benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2513.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2513.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM surrogate BO (White et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian optimization using Large Language Models as surrogate models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach using language models (GPT-family) as the surrogate predictive model inside a BO loop, using in-context learning or fine-tuning and special prompting to extract uncertainty estimates for acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian Optimization of Catalysts With In-context Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-as-surrogate Bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLMs (e.g., GPT-4 or other OpenAI models) accept natural-language descriptions of experimental procedures and output predicted property values; uncertainty is induced via multiple-choice prompts or top-k completions to produce discrete output distributions which are used by BO acquisition functions to select experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Catalyst synthesis/procedure → property prediction tasks (example: oxidative coupling of methane C2 yield prediction from synthesis procedure text).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>BO with LLM surrogate: form discrete predictive distributions from LLM outputs (via multiple-choice or repeated top-k completions) and feed these into acquisition function scoring to allocate experiments expected to yield high objective or information.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of samples (experimental trials) and LLM query counts; token size limitations noted as practical constraints; explicit FLOPs/dollars not reported but token budget and prompt length highlighted as limiting factors.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition functions operate on the surrogate's predictive distribution derived from LLM outputs; implicit expected improvement/uncertainty measures from the discrete output distribution are used.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Same BO mechanics: acquisition function uses surrogate mean/uncertainty (derived from LLM output distributions) to select candidates that balance exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not explicitly described; LLM sampling diversity (top-k restarts) can produce an ensemble-like spread but no intentional diversity-penalty algorithm is described.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Limited prompt/context size (token limit), limited number of experiment queries; human/robot execution constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>The surrogate (LLM) is queried sparsely due to token/context limitations; BO terminates when quantiles saturate or when budget exhausted; fine-tuning can reduce per-query costs by improving single-query accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>C2 yield prediction accuracy (MAE) and BO's ability to reach high quantiles in the sampled candidate pool; practical discovery judged by identifying high-yield experimental procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported: LLM ICL (GPT-4) MAE = 1.854 vs Gaussian process regression MAE = 1.893; fine-tuned LLM MAE = 1.325. The ICL model reached the 99% quantile after 15 samples but failed to find the true maximum in that sample pool; GPR was slightly closer to the maximum.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Gaussian Process Regression with text embeddings; traditional GPR BO baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LLM ICL had similar MAE to GPR but was less effective at finding maxima in the sample pool in the cited experiment; fine-tuning improved LLM predictive accuracy below GPR MAE.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not a clear overall efficiency gain in the cited study—LLM ICL matched but did not outperform GPR in optimization; fine-tuned LLM improved predictive accuracy which can improve allocation efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The review highlights tradeoffs: token/context limits constrain ICL performance; LLMs may hallucinate and lack domain specificity unless fine-tuned; LLM surrogates offer flexible input formats (natural language) but can be less sample-efficient or less reliable for acquisition-guided optimization without careful engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>LLMs can function as flexible surrogates for BO, but practical constraints (prompt size, uncertainty estimation techniques) and domain fine-tuning determine whether LLM-driven allocation outperforms classical probabilistic surrogates; hybrid schemes or fine-tuning are recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2513.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2513.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ulissi cyclic workflow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cyclic workflow combining ML screening, DFT validation, and ML retraining (Ulissi et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative agent-like pipeline where ML models screen a large materials space, selected candidates are validated with DFT, and the resulting DFT data are fed back to retrain and improve the ML model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Active learning across intermetallics to guide discovery of electrocatalysts for CO2 reduction and H2 evolution</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML + DFT cyclic active learning workflow</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Start with a candidate database (e.g., intermetallics), use ML to predict catalytic metrics and rank candidates, perform DFT on selected candidates to obtain high-fidelity labels, retrain ML on augmented data, and iterate to converge on high-performing materials while minimizing costly DFT evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Computational materials discovery for electrocatalysis (CO2RR, HER) and screening of intermetallics and surfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate expensive DFT calculations to ML-selected candidates (those predicted best or most informative), using an active selection criterion to maximize discovery and/or model improvement per DFT run.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of DFT calculations (counts) performed and iterations of ML retraining; wall-time for DFT not quantified uniformly.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicit via selection heuristics (ML-predicted performance with uncertainty or ranking); the DFT validation provides maximal information per selected candidate by reducing surrogate uncertainty in targeted regions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Selection balances screening promising candidates (exploitation) and querying uncertain regions to reduce model error (exploration); exact acquisition heuristics vary by implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>The pipeline naturally yields diversity via ML ranking across chemical space and by selecting candidates from different clusters of the search space; no unique diversity-penalty algorithm is mandated.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Limited DFT budget (compute time, CPU/GPU resources); time-to-solution constraints for high-throughput DFT campaigns.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Select small subsets for DFT based on ML predictions/uncertainties to maximize discovery probability per DFT calculation; iterate until budget consumed or model converged.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>DFT-predicted catalytic activities and subsequent experimental validation; identification of alloy compositions predicted to have optimal ORR or CO2RR activity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Rossmeisl et al. used ~150 BO iterations to find optimal high-entropy alloy compositions, delivering up to 20x speedup versus grid search; Ulissi-style workflows reduced candidate lists from thousands to small sets for DFT validation (paper examples mention narrowing to 10-14 candidates in some searches).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Grid search/exhaustive DFT evaluation, manual DFT selection guided by expert intuition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>ML+active DFT loop found near-optimal compositions with far fewer DFT evaluations than exhaustive approaches (example: matching grid-search optima with ~150 iterations instead of full enumeration).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported acceleration factors up to ~20x in certain alloy composition searches (Rossmeisl et al. example cited in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Tradeoffs include surrogate model bias (interpolation over extrapolation), dependence on descriptor quality, and cost of repeated retraining; however, iterative validation efficiently channels DFT effort toward promising regions.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Target DFT calculations to ML-ranked candidates with explicit uncertainty considerations; incorporate kinetic modeling or domain-informed surrogates to prioritize highest-impact evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2513.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2513.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sargent ML-accelerated DFT framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine-learning-accelerated high-throughput DFT screening (Sargent et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active-learning framework that pairs ML regressors with high-throughput DFT: ML predicts adsorption energies across many sites, the volcano relationship identifies promising sites, and DFT validates top candidates to iteratively build a labeled dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accelerated discovery of CO2 electrocatalysts using active machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML-accelerated DFT active learning pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Encode adsorption sites as numeric arrays, train ML regressors (random forest, boosted trees) on a subset of DFT-labeled sites, use model predictions plus volcano relations to prioritize candidate sites for DFT, perform DFT on prioritized sites, and retrain the model — repeating the cycle to efficiently explore a very large space of surfaces and adsorption sites.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Screening of CO2 reduction electrocatalysts (copper-containing intermetallics and their adsorption sites).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate DFT computations to adsorption sites that ML predicts will have the highest catalytic activity (from predicted adsorption energies and volcano models) or highest model uncertainty/utility.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Count of DFT simulations (e.g., ~4000 DFT calculations on adsorption sites guided by the ML framework vs a much larger full enumeration of sites); number of ML regression cycles (~300 regressions mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Model predictive uncertainty and the expected improvement of candidate sites as informed by ML+volcano scoring; DFT validation yields high information per selected site.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploit by evaluating sites predicted to be near the volcano optimum; explore by retraining and querying additional uncertain or under-explored site clusters as model data accumulates.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Selection across t-SNE clusters of coordination environments to ensure coverage across distinct site types; implicit diversity via cluster-aware analysis and selection of representative sites.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed DFT compute budget and time; practical experimental validation budget for synthesizing candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Prioritize DFT calculations that maximize expected discovery utility (ML-predicted activity combined with volcano relations) within available compute budget; iterate until convergence or budget exhausted.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Predicted and experimentally validated catalytic performance metrics (e.g., adsorption energies, Faradaic efficiency, power conversion efficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Framework ran ~300 regressions guiding DFT at ~4000 adsorption sites out of a search space of 12,229 surfaces and ~228,969 adsorption sites, and identified Cu-Al as a promising material; experimental de-alloyed Cu-Al catalysts achieved >80% Faraday efficiency at 400 mA cm−2 compared with ~66% for pure Cu and a 2.8-fold improvement in cathodic power conversion efficiency at 400 mA cm−2.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Unguided DFT enumeration, naïve ML without active selection, and previous state-of-the-art pure Cu catalysts as experimental baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Substantial reduction in DFT workload to identify high-performing candidates and improved experimental performance vs pure Cu; ML-guided screening found superior catalysts with far fewer DFT runs than exhaustive approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reduced the effective DFT cost by focusing on ~4000 key adsorption sites rather than full enumeration of hundreds of thousands of sites; enabled experimental validation of improved catalyst with fewer computational resources.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The review highlights the balance between ML surrogate accuracy and DFT validation cost: better surrogates reduce DFT needs but require representative labels; descriptor encoding of sites and cluster-based selection help balance coverage vs cost.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Combine ML predictions with domain-specific performance heuristics (volcano relations) to prioritize DFT targets; use clustering/embedding (t-SNE) of site types to ensure diverse sampling while prioritizing high expected utility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2513.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2513.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phoenics & Gryffin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phoenics and Gryffin (BO variants for chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Phoenics is a Bayesian optimizer designed for chemistry tasks; Gryffin is a BO algorithm tailored for categorical variables using expert priors to inform sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Phoenics (Bayesian optimizer) and Gryffin (categorical-aware BO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Phoenics implements BO tailored for chemical experiments (fast, data-efficient). Gryffin extends BO to categorical/discrete variables by incorporating domain-expert knowledge as priors to guide sampling in spaces with non-continuous factors (e.g., reagent choices).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemical experimental optimization where variables include continuous and categorical factors (e.g., reagent selection, catalyst composition categories, synthesis parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Use BO-style acquisition functions adapted to the variable types: Phoenics for general chemistry BO, Gryffin for exploiting expert-informed priors over categorical choices to allocate experiments to high-probability categories and promising continuous settings.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of experiments / iterations; algorithmic overheads (not quantified in FLOPs/dollars in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition-driven expected improvement/utility, with Gryffin including expert-informed priors that bias sampling toward informative categories.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition-function-based trade-off; Gryffin uses priors to temper exploration of categorical choices while still enabling discovery in under-explored categories.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Gryffin's expert priors can be used to encourage or discourage exploration across categories; Phoenics focuses on efficient exploration but does not enforce explicit diversity beyond acquisition-driven sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experimental budget (number of runs) and resource constraints in lab experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Optimize acquisition to maximize expected gain under a fixed number of trials; use priors and surrogate adaptivity to reduce wasted trials on low-value categories.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Task-specific experimental objective improvements; no single metric universally defined.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The review cites Phoenics and Gryffin as tools designed to improve sample efficiency and handle categorical variables; no single numeric performance summary is provided in the review article itself.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard BO with Gaussian processes, random sampling, and uninformed categorical handling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>These algorithms are reported in the literature to improve sample efficiency in chemistry problems, particularly Gryffin on categorical variables, but the review does not provide direct numerical comparisons beyond references to their utility.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Method-specific gains reported in original algorithm papers; review highlights that these tools extend BO applicability to chemistry-relevant variable types, improving allocation efficiency where categorical decisions matter.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Using expert priors can accelerate discovery if expert knowledge is reliable, but incorrect priors risk biasing search away from surprising breakthroughs; Gryffin aims to balance this by retaining probabilistic exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>For problems with categorical variables, integrate expert knowledge as priors (Gryffin) while maintaining exploration via BO acquisition to balance discovery risk and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2513.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2513.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemScope / ChemNavigator / ChemExecutor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemScope, ChemNavigator and ChemExecutor (GPT-4-directed MOF synthesis workflow)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-4-driven multi-component workflow for planning, tasking, and executing MOF synthesis projects with iterative human feedback and experimental record templates to close the loop and optimize protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A GPT-4 Reticular Chemist for Guiding MOF Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reticular MOF synthesis workflow (ChemScope / ChemNavigator / ChemExecutor)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ChemScope: GPT-4 ingests project goals and lab resources to produce a staged project blueprint; ChemNavigator: updates summaries and generates tasks using prior trials and human feedback; ChemExecutor: produces step-by-step experimental instructions and templates for recording outcomes; human experimenters execute experiments and feed structured feedback to the system for iterative optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Synthesis planning and iterative optimization for metal-organic frameworks (MOFs) and multistep synthesis protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>High-level project planning by GPT-4 breaks tasks into stages and assigns subtasks; experimental prioritization arises from stage indicators and iterative feedback (procedural, not purely acquisition-function-based). Decision-making blends literature-derived priors, human constraints (available resources), and empirical outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not specified quantitatively; practical limits include human-experiment execution time and token/context size for GPT-4 prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicit: experimental feedback recorded in templates is used as informative examples for in-context learning to refine subsequent protocol generation; information gain is operationalized via improved experimental outcomes across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploit via refining protocols that yield improved outcomes; explore by proposing alternative tasks/conditions based on literature and prior unsuccessful attempts; the mechanism is heuristic and human-in-the-loop rather than a strict acquisition-function optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity arises from GPT-4 generating multiple tasks/approaches at each stage and from human-in-the-loop guidance; no explicit mathematical diversity objective is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Lab resource availability and human labor constraints; not framed as a fixed numeric experiment budget in the reported study.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Constraints are provided to GPT-4 as part of the project context (available reagents, equipment) and the system plans tasks within those constraints; explicit optimization under a numeric budget is not described.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Successful synthesis and characterization of target MOFs (e.g., discovery and characterization of isomorphic MOF-521s) and progression through stage indicators defined in project blueprints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported success in discovering/characterizing a series of isomorphic MOF-521s via iterative GPT-4-guided human-in-the-loop workflows; quantitative metrics of experimental budgets or speedups are not provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Traditional human-driven literature review and ad hoc protocol development.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>The workflow demonstrated practical utility in planning and iteratively improving synthetic procedures with limited coding skills required; direct numeric comparison to purely human workflows not presented in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative: GPT-4 enabled structured project planning and iterative optimization that reduced the burden on human experts, but no explicit % or x-fold efficiency numbers given.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Strengths: natural-language interaction, rapid task decomposition, iterative learning from feedback; weaknesses: reliance on prompt engineering, potential for hallucination, and need for human verification during execution.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>LLM-driven project decomposition paired with structured feedback templates can focus experimental effort effectively; providing explicit constraints and literature context improves allocation decisions, but integrating automated execution (robotics) would be required to fully realize autonomous cost-sensitive allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2513.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2513.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Chemist (Jun Jiang)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>An all-round AI-Chemist with a scientific mind</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An integrated platform comprising machine-reading, mobile robotics, and computational modules that digitizes literature, standardizes protocols, schedules robotic execution, and incorporates computation to guide autonomous experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An all-round AI-Chemist with a scientific mind</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-Chemist (three-module autonomous laboratory)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three modules: (1) machine-reading mines literature to build a knowledge base and standardize protocols; (2) mobile robot module executes experiments across workstations; (3) computational module manages tasks, customizes workflows, and stores data. The system responds to research questions by generating experiment plans that the robot executes with data fed back to the computational module.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Autonomous experimental synthesis and characterization (e.g., discovery of water-oxidation catalysts from Martian meteorite analogs).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>High-level task planning based on mined literature, available lab resources, and experimental objectives; specifics of formal acquisition functions or explicit information-gain computations are not specified in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Practical lab time (autonomous run durations) and number of experiments executed; formal computational cost metrics are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicit via iterative experimental outcomes and literature-informed task prioritization; no explicit mutual-information or expected improvement metric is described.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Heuristic/task-planning driven: the system proposes and executes experiments informed by literature and prior results, iterating to meet stage indicators; not described as using formal exploration/exploitation acquisition functions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity arises from literature-derived alternative protocols and the platform's ability to execute varied workflows across multiple workstations; no explicit diversity-optimization mechanism reported.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Lab resource availability, reagent availability, and runtime of mobile robots; not formalized as numeric budgets in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>The platform schedules tasks across available workstations and stores/standardizes protocols to reuse resources efficiently; no explicit constrained optimization algorithm described.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Discovery/optimization of catalysts suitable for target objectives (e.g., water-oxidation catalysts from Martian meteorite mixtures).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Demonstrated discovery campaign for water-oxidation catalysts using Martian meteorite samples; the review does not supply detailed numeric efficiency comparisons for AI-Chemist specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Pre-existing robotic systems that lack integrated literature-reading and computational planning modules (e.g., earlier mobile robots driven solely by BO without literature integration).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>AI-Chemist augments prior mobile robotic systems by integrating literature-derived knowledge and planning, enabling richer experiment planning; quantitative gains versus earlier systems are not explicitly enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not specified numerically in the review; qualitative improvement in planning fidelity and autonomy is emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Integration of literature knowledge increases planning sophistication but adds complexity (natural-language interpretation, protocol standardization) and requires robust verification to avoid propagating literature errors.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Embedding literature-derived domain knowledge into planning can improve which experiments are proposed, but the paper highlights the need to combine this with formal decision-theoretic allocation (e.g., BO/AL) to optimize resource-limited campaigns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A mobile robotic chemist <em>(Rating: 2)</em></li>
                <li>An all-round AI-Chemist with a scientific mind <em>(Rating: 2)</em></li>
                <li>A Bayesian experimental autonomous researcher for mechanical design <em>(Rating: 2)</em></li>
                <li>Bayesian Optimization of Catalysts With In-context Learning <em>(Rating: 2)</em></li>
                <li>Active learning across intermetallics to guide discovery of electrocatalysts for CO2 reduction and H2 evolution <em>(Rating: 2)</em></li>
                <li>Accelerated discovery of CO2 electrocatalysts using active machine learning <em>(Rating: 2)</em></li>
                <li>A GPT-4 Reticular Chemist for Guiding MOF Discovery <em>(Rating: 2)</em></li>
                <li>Phoenics: A Bayesian Optimizer for Chemistry <em>(Rating: 1)</em></li>
                <li>Gryffin: An algorithm for Bayesian optimization of categorical variables informed by expert knowledge <em>(Rating: 1)</em></li>
                <li>Is GPT-3 all you need for low-data discovery in chemistry (Leveraging large language models for predictive chemistry) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2513",
    "paper_id": "paper-270786925",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "Bayesian Optimization (BO)",
            "name_full": "Bayesian Optimization",
            "brief_description": "A probabilistic optimization framework that builds a surrogate model of an expensive-to-evaluate objective and uses an acquisition function (e.g., EI, PI, UCB) to select new experiments that trade off exploration and exploitation.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Bayesian optimization (general framework)",
            "system_description": "BO constructs a surrogate (commonly a Gaussian process or other regressor) to approximate a black-box objective (e.g., catalytic activity). An acquisition function scores candidate points according to expected utility; the highest-scoring points are selected for true evaluation (experiment or high-fidelity simulation). Iteratively, observations update the surrogate and the process repeats.",
            "application_domain": "Catalyst discovery, materials optimization, experimental parameter tuning (electrocatalysis, photocatalysis, alloy composition optimization).",
            "resource_allocation_strategy": "Allocate experimental/simulation budget to candidate points maximizing a chosen acquisition function (e.g., Expected Improvement, Probability of Improvement, Upper Confidence Bound), thereby selecting points that balance predicted high performance and model uncertainty.",
            "computational_cost_metric": "Reported/used metrics in examples: number of physical experiments or number of DFT simulations (counts); retrospective comparisons use wall-clock experimental time reductions (e.g., 'experiments per day' or 'days of autonomous running'). FLOPs/dollars not reported in the paper.",
            "information_gain_metric": "Acquisition functions that implicitly capture expected improvement/uncertainty reduction (EI, PI, UCB); Bayesian inference updating the surrogate posterior acts as the information-update mechanism.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicit via acquisition functions: EI/PI/UCB trade off exploitation of high predicted objective values and exploration of high-uncertainty regions; surrogate posterior variance drives exploration component.",
            "diversity_mechanism": "No general, explicit diversity-promoting mechanism is mandated by BO itself in the paper; extensions cited (e.g., Gryffin, Phoenics) address categorical/structured variables but explicit diversity heuristics are not central in baseline BO examples.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed number of experiments or compute-limited (number of DFT runs); time-limited laboratory runs (days/hours).",
            "budget_constraint_handling": "BO inherently prioritizes highest-utility experiments under a limited budget (select top acquisition-scoring points iteratively); practitioners cap iterations or stop when improvement saturates or budget exhausted.",
            "breakthrough_discovery_metric": "Task-dependent performance metrics such as peak current density (electro-oxidation), C2 yield, Faradaic efficiency, activity volcano metrics; breakthroughs identified by surpassing prior best performance thresholds.",
            "performance_metrics": "Examples reported in paper: Yamauchi et al. found optimal PtPdAu composition in 47 experiments (&lt;1% of composition space); Cooper's mobile chemist ran 688 experiments in 8 days (photocatalyst discovery); Rossmeisl et al. BO ran ~150 iterations and reported up to 20x acceleration vs grid search in one study. Metrics are counts of experiments/iterations and task-specific performance (current density, yields).",
            "comparison_baseline": "Grid search, manual experimentation, Gaussian-process baselines, random or uniform sampling.",
            "performance_vs_baseline": "BO substantially reduced required experiments relative to grid or uniform search: e.g., Yamauchi: optimal found in 47 experiments (&lt;1% of space); Rossmeisl: up to ~20x faster than grid search; Cooper: autonomous BO pipeline produced 60x reduction in experimental time compared to manual operations (paper's reported figure).",
            "efficiency_gain": "Reported examples: discovery using &lt;1% of search space (Yamauchi), up to 20x fewer iterations to reach optimum (Rossmeisl), and ~60x reduction in experimental time for autonomous BO-driven runs (Cooper mobile chemist example in the review).",
            "tradeoff_analysis": "The review emphasizes BO's formal balancing of exploration and exploitation via acquisition functions and discusses limits: surrogate models interpolate better than extrapolate, choice of descriptors affects efficiency, and token/representation limits (when using LLM surrogates) can impede performance. It notes that adding domain knowledge (e.g., via LLMs) can improve allocation efficiency beyond pure BO.",
            "optimal_allocation_findings": "Principles: use probabilistic surrogates with acquisition functions to maximize expected utility per experiment; incorporate domain knowledge (descriptors, physics) to improve surrogate quality; restrict search via informed descriptors to reduce wasted budget; stop when acquisition improvement plateaus. For categorical/discrete domains use specialized BO variants (e.g., Gryffin).",
            "uuid": "e2513.0",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Active Learning (AL)",
            "name_full": "Active Learning (uncertainty/information-driven sampling)",
            "brief_description": "Adaptive sampling approach where the learning model selects new data points to label (perform simulation/experiment) that are expected to most improve model performance, typically by targeting high uncertainty or high expected error.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Active learning frameworks for catalyst discovery",
            "system_description": "AL workflows train an initial surrogate on a small set of evaluated points, then iteratively select next candidates using heuristics such as model uncertainty (ensemble variance), predicted error estimators, or information-theoretic criteria; the new labeled points are measured with high-fidelity experiments/DFT and used to retrain the model.",
            "application_domain": "High-throughput DFT screening, electrocatalyst discovery (CO2RR, HER), materials property prediction, reaction yield prediction.",
            "resource_allocation_strategy": "Allocate expensive labels (DFT calculations or experiments) to points of highest expected information gain — e.g., highest prediction variance from an ensemble, highest predicted model error, or that maximize a proxy for uncertainty-reduction per cost.",
            "computational_cost_metric": "Number of expensive labels performed (count of DFT calculations or physical experiments); wall-clock time for experiments/DFT is mentioned qualitatively; no unified FLOPs/dollar metric presented.",
            "information_gain_metric": "Uncertainty reduction (ensemble prediction variance), predicted error from an error model (KRR error predictor), and related information-theoretic heuristics are used as proxies for information gain.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration via selecting high-uncertainty/unlabeled regions; exploitation arises when selecting points with promising predicted performance as part of optimization-oriented AL; choice depends on objective (model-building vs global optimization).",
            "diversity_mechanism": "Some AL implementations implicitly promote diversity by choosing high-uncertainty candidates across the space and by using ensemble-based selection; agent-based cyclic workflows (Ulissi et al.) select across diverse candidate classes, but explicit diversity-penalty mechanisms are not standardized across papers.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of expensive labels (DFT runs / experiments); compute and chemical consumption limits.",
            "budget_constraint_handling": "AL prioritizes samples expected to maximize model improvement per label; ensembles and error predictors estimate where labels are most valuable, and iterations continue until budget is consumed or model convergence criteria met.",
            "breakthrough_discovery_metric": "Identification of candidates with superior computed properties (e.g., favorable adsorption energies) or experimentally validated high activity; breakthrough defined by exceeding performance thresholds or finding previously unanticipated high-performing materials.",
            "performance_metrics": "Examples: Jung et al. reported an ensemble/KRR AL pipeline with best model RMSE ~0.05 eV for *CO binding energy predictions; Ulissi-style cyclic workflows reduced DFT workload by focusing on promising candidates (search over 1499 intermetallics reduced to handfuls); Sargent et al. performed ~300 ML regression cycles and guided DFT for ~4000 adsorption sites out of a much larger search space to identify Cu-Al catalysts with experimentally validated &gt;80% Faraday efficiency (vs ∼66% for pure Cu).",
            "comparison_baseline": "Random sampling, exhaustive DFT screening, uniform sampling, and non-adaptive ML-assisted selection.",
            "performance_vs_baseline": "AL drastically reduced required expensive labels relative to exhaustive search: e.g., instead of evaluating thousands-to-hundreds-of-thousands of sites, guided AL focused DFT on ~4000 sites and still identified high-performance catalysts; reported RMSEs and discovery rates improved relative to unguided sampling.",
            "efficiency_gain": "Examples: Sargent et al. guided ~4000 DFT calculations rather than enumerating all ~228k adsorption sites, enabling practical screening; Jung et al. achieved low RMSE with far fewer ab initio calculations via AL-driven selection.",
            "tradeoff_analysis": "The review notes tradeoffs: AL reduces expensive label cost but depends on surrogate generality (interpolation vs extrapolation limits); descriptor choice affects cost-effectiveness; there is a tension between maximizing immediate discovery (optimization) and improving the global model (information collection).",
            "optimal_allocation_findings": "Effective allocation uses uncertainty- or error-based selection, ensemble methods for uncertainty estimation, and combines cheap descriptors or physics-based surrogates to reduce expensive evaluations; integrating domain knowledge (e.g., theory-based descriptors or LLM output) further improves allocation efficiency.",
            "uuid": "e2513.1",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Mobile Robotic Chemist (Cooper)",
            "name_full": "A mobile robotic chemist (AGV-based autonomous mobile robot system)",
            "brief_description": "An automated mobile-robot laboratory that autonomously executes high-throughput experiments across distributed workstations and uses Bayesian algorithms to select experiments for photocatalyst discovery.",
            "citation_title": "A mobile robotic chemist",
            "mention_or_use": "use",
            "system_name": "AGV-based autonomous mobile robotic chemist",
            "system_description": "A mobile robotic platform with an autonomous guided vehicle (AGV) and multiple workstations (solid/liquid dispensing, sonication, photolysis, GC, storage). It executes automated experiments and uses Bayesian optimization to choose experimental conditions; data are fed back iteratively to the BO loop.",
            "application_domain": "Photocatalyst optimization for water-splitting / hydrogen production.",
            "resource_allocation_strategy": "Bayesian optimization-driven selection of next experiments within a 10-variable design space; acquisition function selects promising compositions/conditions to evaluate experimentally.",
            "computational_cost_metric": "Experimental counts and wall-clock lab time (e.g., 688 experiments over 8 days). Computational costs of BO surrogate not quantified in FLOPs or dollars in the review.",
            "information_gain_metric": "Implicit via the BO acquisition function (which uses surrogate predictive mean and variance to estimate expected improvement/information value).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "BO acquisition function balances exploring uncertain regions vs exploiting high predicted photocatalytic activity.",
            "diversity_mechanism": "No explicit diversity-promoting mechanism described beyond the BO acquisition-driven sampling; the system does not incorporate domain theoretical models in its decision-making (paper notes this limitation).",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Time-limited experimental run and consumables (lab time, reagents); implicit fixed experiment count per campaign.",
            "budget_constraint_handling": "BO selects experiments expected to yield maximal improvement per experiment under the campaign time window; practical constraints handled by scheduling across workstations.",
            "breakthrough_discovery_metric": "Photocatalytic activity metrics such as hydrogen production rate/photocatalytic current density and identification of formulations with substantially higher activity.",
            "performance_metrics": "Example: Autonomous run completed 688 experiments in 8 days and discovered improved photocatalysts; the platform reportedly delivered ~60x reduction in experimental time cost compared to manual operations (as reported in the review).",
            "comparison_baseline": "Manual experimental screening and non-autonomous workflows.",
            "performance_vs_baseline": "Significant speed-up: ~60x reduction in experimental time vs manual operations; discovery of higher-activity formulations that would have been difficult for chemists to predict.",
            "efficiency_gain": "~60x reduction in experimental time; discovered high-performing photocatalysts within an automated campaign.",
            "tradeoff_analysis": "Paper highlights tradeoffs: robotic mobility and flexibility vs throughput; lack of domain-model integration limits chemical insight; BO reduces experiments but may miss opportunities without embedded chemical knowledge.",
            "optimal_allocation_findings": "Empirical finding: autonomous BO-guided experimentation across distributed stations can accelerate discovery dramatically; integrating chemical/domain knowledge into the acquisition or surrogate could further improve allocation efficiency.",
            "uuid": "e2513.2",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "BEAR",
            "name_full": "Bayesian Experiments for Autonomous Researchers (BEAR)",
            "brief_description": "A framework that combines Bayesian optimization with high-throughput automated experimentation to close the loop for self-driving material discovery.",
            "citation_title": "A Bayesian experimental autonomous researcher for mechanical design",
            "mention_or_use": "mention",
            "system_name": "BEAR (Bayesian experimental autonomous researcher)",
            "system_description": "BEAR integrates BO with automated experimental platforms to autonomously propose and execute experiments, collect measurements, and update the surrogate model iteratively, enabling end-to-end autonomous material discovery cycles.",
            "application_domain": "Autonomous experimental design across materials and mechanical design contexts; exemplified in materials discovery.",
            "resource_allocation_strategy": "Use BO acquisition functions to prioritize experiments that maximize expected improvement/information per experiment; execution is automated by high-throughput hardware.",
            "computational_cost_metric": "Count of executed experiments and total autonomous runtime; specific computational cost metrics are not provided in detail in the review.",
            "information_gain_metric": "Acquisition functions grounded in Bayesian posterior (expected improvement or related criteria) which implicitly measure expected utility/information.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition-function-driven trade-off between sampling high-mean and high-uncertainty regions; iteratively updated surrogate directs subsequent experiments.",
            "diversity_mechanism": "Not described as having explicit diversity-promoting sub-algorithms in the review; diversity arises indirectly from the acquisition function and problem geometry.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed experimental budget (number of automated experiments) and physical lab resource constraints.",
            "budget_constraint_handling": "BO-driven prioritization of experiments to maximize gain under the available automation/trial budget.",
            "breakthrough_discovery_metric": "Task-specific improvement in objective (e.g., mechanical performance or materials property); the framework targets largest expected improvement per automated trial.",
            "performance_metrics": "The review cites BEAR as an example of BO+automation enabling self-driven discovery; explicit numeric outcomes for BEAR within this review are not provided.",
            "comparison_baseline": "Manual experiment planning and non-autonomous optimization.",
            "performance_vs_baseline": "Not numerically quantified in this review; BEAR is presented as a paradigm combining BO with automation to accelerate discovery.",
            "efficiency_gain": "Not specified numerically in this review.",
            "tradeoff_analysis": "BEAR reduces human planning overhead and trial counts via BO but inherits the surrogate/model limitations discussed for BO and AL (descriptor dependence, interpolation limits).",
            "optimal_allocation_findings": "Combining BO with automation yields efficient allocation of physical experiments; careful surrogate and acquisition choices are critical to maximize benefits.",
            "uuid": "e2513.3",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLM surrogate BO (White et al.)",
            "name_full": "Bayesian optimization using Large Language Models as surrogate models",
            "brief_description": "An approach using language models (GPT-family) as the surrogate predictive model inside a BO loop, using in-context learning or fine-tuning and special prompting to extract uncertainty estimates for acquisition.",
            "citation_title": "Bayesian Optimization of Catalysts With In-context Learning",
            "mention_or_use": "use",
            "system_name": "LLM-as-surrogate Bayesian optimization",
            "system_description": "LLMs (e.g., GPT-4 or other OpenAI models) accept natural-language descriptions of experimental procedures and output predicted property values; uncertainty is induced via multiple-choice prompts or top-k completions to produce discrete output distributions which are used by BO acquisition functions to select experiments.",
            "application_domain": "Catalyst synthesis/procedure → property prediction tasks (example: oxidative coupling of methane C2 yield prediction from synthesis procedure text).",
            "resource_allocation_strategy": "BO with LLM surrogate: form discrete predictive distributions from LLM outputs (via multiple-choice or repeated top-k completions) and feed these into acquisition function scoring to allocate experiments expected to yield high objective or information.",
            "computational_cost_metric": "Number of samples (experimental trials) and LLM query counts; token size limitations noted as practical constraints; explicit FLOPs/dollars not reported but token budget and prompt length highlighted as limiting factors.",
            "information_gain_metric": "Acquisition functions operate on the surrogate's predictive distribution derived from LLM outputs; implicit expected improvement/uncertainty measures from the discrete output distribution are used.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Same BO mechanics: acquisition function uses surrogate mean/uncertainty (derived from LLM output distributions) to select candidates that balance exploration and exploitation.",
            "diversity_mechanism": "Not explicitly described; LLM sampling diversity (top-k restarts) can produce an ensemble-like spread but no intentional diversity-penalty algorithm is described.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Limited prompt/context size (token limit), limited number of experiment queries; human/robot execution constraints.",
            "budget_constraint_handling": "The surrogate (LLM) is queried sparsely due to token/context limitations; BO terminates when quantiles saturate or when budget exhausted; fine-tuning can reduce per-query costs by improving single-query accuracy.",
            "breakthrough_discovery_metric": "C2 yield prediction accuracy (MAE) and BO's ability to reach high quantiles in the sampled candidate pool; practical discovery judged by identifying high-yield experimental procedures.",
            "performance_metrics": "Reported: LLM ICL (GPT-4) MAE = 1.854 vs Gaussian process regression MAE = 1.893; fine-tuned LLM MAE = 1.325. The ICL model reached the 99% quantile after 15 samples but failed to find the true maximum in that sample pool; GPR was slightly closer to the maximum.",
            "comparison_baseline": "Gaussian Process Regression with text embeddings; traditional GPR BO baseline.",
            "performance_vs_baseline": "LLM ICL had similar MAE to GPR but was less effective at finding maxima in the sample pool in the cited experiment; fine-tuning improved LLM predictive accuracy below GPR MAE.",
            "efficiency_gain": "Not a clear overall efficiency gain in the cited study—LLM ICL matched but did not outperform GPR in optimization; fine-tuned LLM improved predictive accuracy which can improve allocation efficiency.",
            "tradeoff_analysis": "The review highlights tradeoffs: token/context limits constrain ICL performance; LLMs may hallucinate and lack domain specificity unless fine-tuned; LLM surrogates offer flexible input formats (natural language) but can be less sample-efficient or less reliable for acquisition-guided optimization without careful engineering.",
            "optimal_allocation_findings": "LLMs can function as flexible surrogates for BO, but practical constraints (prompt size, uncertainty estimation techniques) and domain fine-tuning determine whether LLM-driven allocation outperforms classical probabilistic surrogates; hybrid schemes or fine-tuning are recommended.",
            "uuid": "e2513.4",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Ulissi cyclic workflow",
            "name_full": "Cyclic workflow combining ML screening, DFT validation, and ML retraining (Ulissi et al.)",
            "brief_description": "An iterative agent-like pipeline where ML models screen a large materials space, selected candidates are validated with DFT, and the resulting DFT data are fed back to retrain and improve the ML model.",
            "citation_title": "Active learning across intermetallics to guide discovery of electrocatalysts for CO2 reduction and H2 evolution",
            "mention_or_use": "use",
            "system_name": "ML + DFT cyclic active learning workflow",
            "system_description": "Start with a candidate database (e.g., intermetallics), use ML to predict catalytic metrics and rank candidates, perform DFT on selected candidates to obtain high-fidelity labels, retrain ML on augmented data, and iterate to converge on high-performing materials while minimizing costly DFT evaluations.",
            "application_domain": "Computational materials discovery for electrocatalysis (CO2RR, HER) and screening of intermetallics and surfaces.",
            "resource_allocation_strategy": "Allocate expensive DFT calculations to ML-selected candidates (those predicted best or most informative), using an active selection criterion to maximize discovery and/or model improvement per DFT run.",
            "computational_cost_metric": "Number of DFT calculations (counts) performed and iterations of ML retraining; wall-time for DFT not quantified uniformly.",
            "information_gain_metric": "Implicit via selection heuristics (ML-predicted performance with uncertainty or ranking); the DFT validation provides maximal information per selected candidate by reducing surrogate uncertainty in targeted regions.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Selection balances screening promising candidates (exploitation) and querying uncertain regions to reduce model error (exploration); exact acquisition heuristics vary by implementation.",
            "diversity_mechanism": "The pipeline naturally yields diversity via ML ranking across chemical space and by selecting candidates from different clusters of the search space; no unique diversity-penalty algorithm is mandated.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Limited DFT budget (compute time, CPU/GPU resources); time-to-solution constraints for high-throughput DFT campaigns.",
            "budget_constraint_handling": "Select small subsets for DFT based on ML predictions/uncertainties to maximize discovery probability per DFT calculation; iterate until budget consumed or model converged.",
            "breakthrough_discovery_metric": "DFT-predicted catalytic activities and subsequent experimental validation; identification of alloy compositions predicted to have optimal ORR or CO2RR activity.",
            "performance_metrics": "Rossmeisl et al. used ~150 BO iterations to find optimal high-entropy alloy compositions, delivering up to 20x speedup versus grid search; Ulissi-style workflows reduced candidate lists from thousands to small sets for DFT validation (paper examples mention narrowing to 10-14 candidates in some searches).",
            "comparison_baseline": "Grid search/exhaustive DFT evaluation, manual DFT selection guided by expert intuition.",
            "performance_vs_baseline": "ML+active DFT loop found near-optimal compositions with far fewer DFT evaluations than exhaustive approaches (example: matching grid-search optima with ~150 iterations instead of full enumeration).",
            "efficiency_gain": "Reported acceleration factors up to ~20x in certain alloy composition searches (Rossmeisl et al. example cited in the review).",
            "tradeoff_analysis": "Tradeoffs include surrogate model bias (interpolation over extrapolation), dependence on descriptor quality, and cost of repeated retraining; however, iterative validation efficiently channels DFT effort toward promising regions.",
            "optimal_allocation_findings": "Target DFT calculations to ML-ranked candidates with explicit uncertainty considerations; incorporate kinetic modeling or domain-informed surrogates to prioritize highest-impact evaluations.",
            "uuid": "e2513.5",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Sargent ML-accelerated DFT framework",
            "name_full": "Machine-learning-accelerated high-throughput DFT screening (Sargent et al.)",
            "brief_description": "An active-learning framework that pairs ML regressors with high-throughput DFT: ML predicts adsorption energies across many sites, the volcano relationship identifies promising sites, and DFT validates top candidates to iteratively build a labeled dataset.",
            "citation_title": "Accelerated discovery of CO2 electrocatalysts using active machine learning",
            "mention_or_use": "use",
            "system_name": "ML-accelerated DFT active learning pipeline",
            "system_description": "Encode adsorption sites as numeric arrays, train ML regressors (random forest, boosted trees) on a subset of DFT-labeled sites, use model predictions plus volcano relations to prioritize candidate sites for DFT, perform DFT on prioritized sites, and retrain the model — repeating the cycle to efficiently explore a very large space of surfaces and adsorption sites.",
            "application_domain": "Screening of CO2 reduction electrocatalysts (copper-containing intermetallics and their adsorption sites).",
            "resource_allocation_strategy": "Allocate DFT computations to adsorption sites that ML predicts will have the highest catalytic activity (from predicted adsorption energies and volcano models) or highest model uncertainty/utility.",
            "computational_cost_metric": "Count of DFT simulations (e.g., ~4000 DFT calculations on adsorption sites guided by the ML framework vs a much larger full enumeration of sites); number of ML regression cycles (~300 regressions mentioned).",
            "information_gain_metric": "Model predictive uncertainty and the expected improvement of candidate sites as informed by ML+volcano scoring; DFT validation yields high information per selected site.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploit by evaluating sites predicted to be near the volcano optimum; explore by retraining and querying additional uncertain or under-explored site clusters as model data accumulates.",
            "diversity_mechanism": "Selection across t-SNE clusters of coordination environments to ensure coverage across distinct site types; implicit diversity via cluster-aware analysis and selection of representative sites.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed DFT compute budget and time; practical experimental validation budget for synthesizing candidates.",
            "budget_constraint_handling": "Prioritize DFT calculations that maximize expected discovery utility (ML-predicted activity combined with volcano relations) within available compute budget; iterate until convergence or budget exhausted.",
            "breakthrough_discovery_metric": "Predicted and experimentally validated catalytic performance metrics (e.g., adsorption energies, Faradaic efficiency, power conversion efficiency).",
            "performance_metrics": "Framework ran ~300 regressions guiding DFT at ~4000 adsorption sites out of a search space of 12,229 surfaces and ~228,969 adsorption sites, and identified Cu-Al as a promising material; experimental de-alloyed Cu-Al catalysts achieved &gt;80% Faraday efficiency at 400 mA cm−2 compared with ~66% for pure Cu and a 2.8-fold improvement in cathodic power conversion efficiency at 400 mA cm−2.",
            "comparison_baseline": "Unguided DFT enumeration, naïve ML without active selection, and previous state-of-the-art pure Cu catalysts as experimental baseline.",
            "performance_vs_baseline": "Substantial reduction in DFT workload to identify high-performing candidates and improved experimental performance vs pure Cu; ML-guided screening found superior catalysts with far fewer DFT runs than exhaustive approaches.",
            "efficiency_gain": "Reduced the effective DFT cost by focusing on ~4000 key adsorption sites rather than full enumeration of hundreds of thousands of sites; enabled experimental validation of improved catalyst with fewer computational resources.",
            "tradeoff_analysis": "The review highlights the balance between ML surrogate accuracy and DFT validation cost: better surrogates reduce DFT needs but require representative labels; descriptor encoding of sites and cluster-based selection help balance coverage vs cost.",
            "optimal_allocation_findings": "Combine ML predictions with domain-specific performance heuristics (volcano relations) to prioritize DFT targets; use clustering/embedding (t-SNE) of site types to ensure diverse sampling while prioritizing high expected utility.",
            "uuid": "e2513.6",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Phoenics & Gryffin",
            "name_full": "Phoenics and Gryffin (BO variants for chemistry)",
            "brief_description": "Phoenics is a Bayesian optimizer designed for chemistry tasks; Gryffin is a BO algorithm tailored for categorical variables using expert priors to inform sampling.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Phoenics (Bayesian optimizer) and Gryffin (categorical-aware BO)",
            "system_description": "Phoenics implements BO tailored for chemical experiments (fast, data-efficient). Gryffin extends BO to categorical/discrete variables by incorporating domain-expert knowledge as priors to guide sampling in spaces with non-continuous factors (e.g., reagent choices).",
            "application_domain": "Chemical experimental optimization where variables include continuous and categorical factors (e.g., reagent selection, catalyst composition categories, synthesis parameters).",
            "resource_allocation_strategy": "Use BO-style acquisition functions adapted to the variable types: Phoenics for general chemistry BO, Gryffin for exploiting expert-informed priors over categorical choices to allocate experiments to high-probability categories and promising continuous settings.",
            "computational_cost_metric": "Number of experiments / iterations; algorithmic overheads (not quantified in FLOPs/dollars in the review).",
            "information_gain_metric": "Acquisition-driven expected improvement/utility, with Gryffin including expert-informed priors that bias sampling toward informative categories.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition-function-based trade-off; Gryffin uses priors to temper exploration of categorical choices while still enabling discovery in under-explored categories.",
            "diversity_mechanism": "Gryffin's expert priors can be used to encourage or discourage exploration across categories; Phoenics focuses on efficient exploration but does not enforce explicit diversity beyond acquisition-driven sampling.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed experimental budget (number of runs) and resource constraints in lab experiments.",
            "budget_constraint_handling": "Optimize acquisition to maximize expected gain under a fixed number of trials; use priors and surrogate adaptivity to reduce wasted trials on low-value categories.",
            "breakthrough_discovery_metric": "Task-specific experimental objective improvements; no single metric universally defined.",
            "performance_metrics": "The review cites Phoenics and Gryffin as tools designed to improve sample efficiency and handle categorical variables; no single numeric performance summary is provided in the review article itself.",
            "comparison_baseline": "Standard BO with Gaussian processes, random sampling, and uninformed categorical handling.",
            "performance_vs_baseline": "These algorithms are reported in the literature to improve sample efficiency in chemistry problems, particularly Gryffin on categorical variables, but the review does not provide direct numerical comparisons beyond references to their utility.",
            "efficiency_gain": "Method-specific gains reported in original algorithm papers; review highlights that these tools extend BO applicability to chemistry-relevant variable types, improving allocation efficiency where categorical decisions matter.",
            "tradeoff_analysis": "Using expert priors can accelerate discovery if expert knowledge is reliable, but incorrect priors risk biasing search away from surprising breakthroughs; Gryffin aims to balance this by retaining probabilistic exploration.",
            "optimal_allocation_findings": "For problems with categorical variables, integrate expert knowledge as priors (Gryffin) while maintaining exploration via BO acquisition to balance discovery risk and efficiency.",
            "uuid": "e2513.7",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ChemScope / ChemNavigator / ChemExecutor",
            "name_full": "ChemScope, ChemNavigator and ChemExecutor (GPT-4-directed MOF synthesis workflow)",
            "brief_description": "A GPT-4-driven multi-component workflow for planning, tasking, and executing MOF synthesis projects with iterative human feedback and experimental record templates to close the loop and optimize protocols.",
            "citation_title": "A GPT-4 Reticular Chemist for Guiding MOF Discovery",
            "mention_or_use": "use",
            "system_name": "Reticular MOF synthesis workflow (ChemScope / ChemNavigator / ChemExecutor)",
            "system_description": "ChemScope: GPT-4 ingests project goals and lab resources to produce a staged project blueprint; ChemNavigator: updates summaries and generates tasks using prior trials and human feedback; ChemExecutor: produces step-by-step experimental instructions and templates for recording outcomes; human experimenters execute experiments and feed structured feedback to the system for iterative optimization.",
            "application_domain": "Synthesis planning and iterative optimization for metal-organic frameworks (MOFs) and multistep synthesis protocols.",
            "resource_allocation_strategy": "High-level project planning by GPT-4 breaks tasks into stages and assigns subtasks; experimental prioritization arises from stage indicators and iterative feedback (procedural, not purely acquisition-function-based). Decision-making blends literature-derived priors, human constraints (available resources), and empirical outcomes.",
            "computational_cost_metric": "Not specified quantitatively; practical limits include human-experiment execution time and token/context size for GPT-4 prompts.",
            "information_gain_metric": "Implicit: experimental feedback recorded in templates is used as informative examples for in-context learning to refine subsequent protocol generation; information gain is operationalized via improved experimental outcomes across iterations.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploit via refining protocols that yield improved outcomes; explore by proposing alternative tasks/conditions based on literature and prior unsuccessful attempts; the mechanism is heuristic and human-in-the-loop rather than a strict acquisition-function optimizer.",
            "diversity_mechanism": "Diversity arises from GPT-4 generating multiple tasks/approaches at each stage and from human-in-the-loop guidance; no explicit mathematical diversity objective is reported.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Lab resource availability and human labor constraints; not framed as a fixed numeric experiment budget in the reported study.",
            "budget_constraint_handling": "Constraints are provided to GPT-4 as part of the project context (available reagents, equipment) and the system plans tasks within those constraints; explicit optimization under a numeric budget is not described.",
            "breakthrough_discovery_metric": "Successful synthesis and characterization of target MOFs (e.g., discovery and characterization of isomorphic MOF-521s) and progression through stage indicators defined in project blueprints.",
            "performance_metrics": "Reported success in discovering/characterizing a series of isomorphic MOF-521s via iterative GPT-4-guided human-in-the-loop workflows; quantitative metrics of experimental budgets or speedups are not provided in the review.",
            "comparison_baseline": "Traditional human-driven literature review and ad hoc protocol development.",
            "performance_vs_baseline": "The workflow demonstrated practical utility in planning and iteratively improving synthetic procedures with limited coding skills required; direct numeric comparison to purely human workflows not presented in the review.",
            "efficiency_gain": "Qualitative: GPT-4 enabled structured project planning and iterative optimization that reduced the burden on human experts, but no explicit % or x-fold efficiency numbers given.",
            "tradeoff_analysis": "Strengths: natural-language interaction, rapid task decomposition, iterative learning from feedback; weaknesses: reliance on prompt engineering, potential for hallucination, and need for human verification during execution.",
            "optimal_allocation_findings": "LLM-driven project decomposition paired with structured feedback templates can focus experimental effort effectively; providing explicit constraints and literature context improves allocation decisions, but integrating automated execution (robotics) would be required to fully realize autonomous cost-sensitive allocation.",
            "uuid": "e2513.8",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "AI-Chemist (Jun Jiang)",
            "name_full": "An all-round AI-Chemist with a scientific mind",
            "brief_description": "An integrated platform comprising machine-reading, mobile robotics, and computational modules that digitizes literature, standardizes protocols, schedules robotic execution, and incorporates computation to guide autonomous experiments.",
            "citation_title": "An all-round AI-Chemist with a scientific mind",
            "mention_or_use": "use",
            "system_name": "AI-Chemist (three-module autonomous laboratory)",
            "system_description": "Three modules: (1) machine-reading mines literature to build a knowledge base and standardize protocols; (2) mobile robot module executes experiments across workstations; (3) computational module manages tasks, customizes workflows, and stores data. The system responds to research questions by generating experiment plans that the robot executes with data fed back to the computational module.",
            "application_domain": "Autonomous experimental synthesis and characterization (e.g., discovery of water-oxidation catalysts from Martian meteorite analogs).",
            "resource_allocation_strategy": "High-level task planning based on mined literature, available lab resources, and experimental objectives; specifics of formal acquisition functions or explicit information-gain computations are not specified in the review.",
            "computational_cost_metric": "Practical lab time (autonomous run durations) and number of experiments executed; formal computational cost metrics are not reported.",
            "information_gain_metric": "Implicit via iterative experimental outcomes and literature-informed task prioritization; no explicit mutual-information or expected improvement metric is described.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Heuristic/task-planning driven: the system proposes and executes experiments informed by literature and prior results, iterating to meet stage indicators; not described as using formal exploration/exploitation acquisition functions.",
            "diversity_mechanism": "Diversity arises from literature-derived alternative protocols and the platform's ability to execute varied workflows across multiple workstations; no explicit diversity-optimization mechanism reported.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Lab resource availability, reagent availability, and runtime of mobile robots; not formalized as numeric budgets in the review.",
            "budget_constraint_handling": "The platform schedules tasks across available workstations and stores/standardizes protocols to reuse resources efficiently; no explicit constrained optimization algorithm described.",
            "breakthrough_discovery_metric": "Discovery/optimization of catalysts suitable for target objectives (e.g., water-oxidation catalysts from Martian meteorite mixtures).",
            "performance_metrics": "Demonstrated discovery campaign for water-oxidation catalysts using Martian meteorite samples; the review does not supply detailed numeric efficiency comparisons for AI-Chemist specifically.",
            "comparison_baseline": "Pre-existing robotic systems that lack integrated literature-reading and computational planning modules (e.g., earlier mobile robots driven solely by BO without literature integration).",
            "performance_vs_baseline": "AI-Chemist augments prior mobile robotic systems by integrating literature-derived knowledge and planning, enabling richer experiment planning; quantitative gains versus earlier systems are not explicitly enumerated.",
            "efficiency_gain": "Not specified numerically in the review; qualitative improvement in planning fidelity and autonomy is emphasized.",
            "tradeoff_analysis": "Integration of literature knowledge increases planning sophistication but adds complexity (natural-language interpretation, protocol standardization) and requires robust verification to avoid propagating literature errors.",
            "optimal_allocation_findings": "Embedding literature-derived domain knowledge into planning can improve which experiments are proposed, but the paper highlights the need to combine this with formal decision-theoretic allocation (e.g., BO/AL) to optimize resource-limited campaigns.",
            "uuid": "e2513.9",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A mobile robotic chemist",
            "rating": 2,
            "sanitized_title": "a_mobile_robotic_chemist"
        },
        {
            "paper_title": "An all-round AI-Chemist with a scientific mind",
            "rating": 2,
            "sanitized_title": "an_allround_aichemist_with_a_scientific_mind"
        },
        {
            "paper_title": "A Bayesian experimental autonomous researcher for mechanical design",
            "rating": 2,
            "sanitized_title": "a_bayesian_experimental_autonomous_researcher_for_mechanical_design"
        },
        {
            "paper_title": "Bayesian Optimization of Catalysts With In-context Learning",
            "rating": 2,
            "sanitized_title": "bayesian_optimization_of_catalysts_with_incontext_learning"
        },
        {
            "paper_title": "Active learning across intermetallics to guide discovery of electrocatalysts for CO2 reduction and H2 evolution",
            "rating": 2,
            "sanitized_title": "active_learning_across_intermetallics_to_guide_discovery_of_electrocatalysts_for_co2_reduction_and_h2_evolution"
        },
        {
            "paper_title": "Accelerated discovery of CO2 electrocatalysts using active machine learning",
            "rating": 2,
            "sanitized_title": "accelerated_discovery_of_co2_electrocatalysts_using_active_machine_learning"
        },
        {
            "paper_title": "A GPT-4 Reticular Chemist for Guiding MOF Discovery",
            "rating": 2,
            "sanitized_title": "a_gpt4_reticular_chemist_for_guiding_mof_discovery"
        },
        {
            "paper_title": "Phoenics: A Bayesian Optimizer for Chemistry",
            "rating": 1,
            "sanitized_title": "phoenics_a_bayesian_optimizer_for_chemistry"
        },
        {
            "paper_title": "Gryffin: An algorithm for Bayesian optimization of categorical variables informed by expert knowledge",
            "rating": 1,
            "sanitized_title": "gryffin_an_algorithm_for_bayesian_optimization_of_categorical_variables_informed_by_expert_knowledge"
        },
        {
            "paper_title": "Is GPT-3 all you need for low-data discovery in chemistry (Leveraging large language models for predictive chemistry)",
            "rating": 1,
            "sanitized_title": "is_gpt3_all_you_need_for_lowdata_discovery_in_chemistry_leveraging_large_language_models_for_predictive_chemistry"
        }
    ],
    "cost": 0.03287425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automation and machine learning augmented by large language models in a catalysis study</p>
<p>Yuming Su 
Chemical</p>
<p>X Wang 0000-0002-7906-8061
Chemical</p>
<p>Yuanxiang Ye 
Chemical</p>
<p>Yibo Xie 
Chemical</p>
<p>Yujing Xu yujingxu@xmu.edu.cn 
Chemical</p>
<p>Yibin Jiang yibin_jiang@outlook.com 
Chemical</p>
<p>Cheng Wang wangchengxmu@xmu.edu.cn 
Chemical</p>
<p>Automation and machine learning augmented by large language models in a catalysis study
40BB4C14FBE0B80D0604DC669301071310.1039/d3sc07012cReceived 31st December 2023 Accepted 21st June 2024
Recent advancements in artificial intelligence and automation are transforming catalyst discovery and design from traditional trial-and-error manual mode into intelligent, high-throughput digital methodologies.This transformation is driven by four key components, including high-throughput information extraction, automated robotic experimentation, real-time feedback for iterative optimization, and interpretable machine learning for generating new knowledge.These innovations have given rise to the development of self-driving labs and significantly accelerated materials research.Over the past two years, the emergence of large language models (LLMs) has added a new dimension to this field, providing unprecedented flexibility in information integration, decision-making, and interacting with human researchers.This review explores how LLMs are reshaping catalyst design, heralding a revolutionary change in the fields.</p>
<p>Introduction</p>
<p>The eld of catalyst design and discovery is undergoing a profound transformation, facilitated by the convergence of articial intelligence (AI) [1][2][3] and automation systems, [4][5][6] as well as utilization of large data.This shi is propelled by advancements in four crucial areas: high-throughput information extraction, [7][8][9][10][11][12][13][14][15][16] automated robotic systems for chemical experimentation, [4][5][6][17][18][19] real-time active machine learning (ML) with on-line data processing and feedback for iterative optimization, 4,[20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35] and interpretable machine learning for generating knowledge, [36][37][38][39] each playing a pivotal role in evolving traditional methodologies. Centrl to this modern era are self-driving labs 40 that are further integrated with theoretical simulations and extensive databases, revolutionizing how catalysts are created and optimized.</p>
<p>Recently, large language models (LLMs) such as GPT-x, ERNIE Bot, Claude-x, and Lamma-x, 41 have begun to dramatically enhance these four technological pillars.By processing natural language, automating code generation and data analysis, optimizing design of experiment (DoE) algorithms, and facilitating human-computer interaction, 16,[42][43][44][45][46][47] LLMs are setting new standards for efficiency and innovation in catalysis research (Fig. 1).These capabilities allow for the extraction and utilization of data from diverse and unstructured sources such as scattered texts, videos, and images, previously inaccessible to more traditional ML technologies that relied on well-organized datasets.</p>
<p>Moreover, automated and intelligent robotic systems, which have seen signicant adoption over the last decade, spanning from ow systems 19,48,49 to desktops 50,51 and humanoid mobile robots, 4,5 now seamlessly integrate with advanced LLMs.This synergy is reshaping decision-making strategies within the eld, transitioning from traditional methods like Bayesian optimization 4 and active learning 32 to more sophisticated, LLM-enhanced approaches, 45,47 towards more talented self-driving labs for closed-loop discovery.This is only the beginning of a shiing paradigm to on-demand catalyst development and in silico performance scanning for catalyst design and optimization.</p>
<p>Despite these technological advances, the role of the human researcher remains indispensable.7][38][39] Articial neural networks (ANNs) 52 used to be regarded as black-box models that are hard to explain, but recent innovations such as SHapley Additive exPlanations (SHAP) 53 for graph neural networks (GNNs) and attention mechanisms in transformer models are enhancing the transparency of articial neural networks, which were previously considered opaque.In addition, LLMs have also showcased their capabilities in extracting data mapping and articulating them in a clear plain language format.</p>
<p>Given the rapid pace of these advancements, it is timely to review the revolutionary shi in AI applications for catalysis research and development.This review will delve into how the integration of LLMs is redening the four foundational ML technologies in catalysis, providing a historical perspective and discussing recent implementations that foreshadow the future of AI-assisted catalyst design.</p>
<p>High-throughput chemical information extraction</p>
<p>Traditionally, data extraction required manual efforts, which has successfully underpinned the establishment of chemical databases like Reaxys 54 and SciFinder. 55With the increasing demand to autonomously gather and standardize chemical information effectively, the development of automated data extraction methods has split into two primary directions: the extraction of chemical information from gures including optical chemical structure recognition (OCSR), [7][8][9][10] and text information extraction.Both avenues benet signicantly from enhancements provided by pre-trained LLMs. 15,16</p>
<p>Information extraction from gures</p>
<p>A considerable amount of chemical information resides in gures, rendering Optical Chemical Structure Recognition (OCSR) essential for converting these complex visual data into accessible and interpretable formats.The primary task of OCSR is to transform visual representations of chemical structures into formats ready for computer processing.We now list and briey discuss these different computer-ready formats.</p>
<p>2.1.1String representations.SMILES (Simplied Molecular Input Line Entry System): known for its human readability, SMILES translates chemical structures into linear text strings.SMARTS (SMILES Arbitrary Target Specication): an extension of SMILES, SMARTS allows for dening substructural patterns within molecules, enhancing search and analysis capabilities.</p>
<p>InChI (International Chemical Identier): provides a structured and layered representation of chemical data, facilitating interoperability across different data systems.SELFIES (Self-referencing Embedded Strings): designed to ensure the validity of molecules represented, enhancing data integrity.</p>
<p>These string representations, integral to systematic chemical naming, have become increasingly valuable with the advent of language models.The seamless integration of these formats into LLMs enhances their utility, making them more than just systematic nomenclature but a dynamic part of molecular data processing.Furthermore, the development of multi-modal large models allows for directly translating structural drawings to the string representations without prior conversion, marking a signicant advancement in the eld. 56g. 1 The workflow of catalyst design and discovery with information extraction, automated chemical experimentation, active machine learning, and interpretable machine learning.</p>
<p>2.1.2Graph-based representations.Transforming chemical drawings into graph-based representations views molecules as nodes (atoms) and connections as edges (bonds), aligning with computational analysis methods in machine learning and network theory.</p>
<p>2.1.3Evolution of OCSR technology.Initially, OCSR technology was predominantly rule-based, with the rst systems developed in the early 1990s. 57Today, state-of-the-art OCSR systems combine rule-based methods with machine learning techniques to improve accuracy and efficiency. 9,58,59,76This hybrid approach addresses the challenges of interpreting complex chemical drawings and converting them into machinereadable formats.We will delve into these technologies in more detail, particularly focusing on recent advancements with multimodal pre-trained large models.</p>
<p>2.1.4Rule-based OCSR.Rule-based OCSR systems are designed to automate the extraction of chemical data by emulating human perceptual abilities.These systems perform a range of tasks including character detection, shape recognition, and the identication of entity connections.][62][63][64] 2.1.4.1 Segmentation challenges.The initial and crucial step in rule-based OCSR is the segmentation of chemical structures from potentially complex images.This task is challenging and critical as it sets the foundation for all subsequent analyses.Early rule-based models such as optical recognition of chemical structures (OROCS), chemical literature data extraction (CLiDE), [65][66][67] the optical structure recognition application (OSRA) and Imago 68,69 faced signicant challenges in accurately segmenting chemical structures.These systems oen struggled with noisy data and the presence of fragmented characters or text lines adjacent to the chemical structures.</p>
<p>In 2014, Simone Marinai et al. 70 made an improvement by introducing a Markov logic-based probabilistic logic inference engine (Fig. 2).This development improved the ability to clean up noisy extractions, although challenges with fragmented elements persisted.More recently, in 2021, Yifei Wang et al. 59 advanced the eld further by employing a Single Shot MultiBox Detector (SSD) neural network combined with a Non-Maximum Area Suppression (NMAS) algorithm.This combination was specically designed to enhance object identication within a single frame, signicantly improving segmentation accuracy to 89.5% on a dataset of 2100 handwritten cyclic compound samples.</p>
<p>2.1.4.2 Inherent limitations.Despite these advancements, rule-based systems are oen limited by two major factors:</p>
<p>(1) Insufficient understanding of embedded rules: the complexity of the embedded rules can lead to misinterpretations and errors in data extraction.</p>
<p>(2) Susceptibility to noise: the intricate rules are prone to interference from noisy data, which can degrade the quality of the output.</p>
<p>MSE-DUDL combines a convolutional neural network (CNN) known for its prowess in visual pattern recognition, and a long short-term memory (LSTM) network equipped with an "attention" mechanism.This attention mechanism allows the model to focus selectively on different parts of the molecular structure, facilitating accurate SMILES prediction.While the method achieved an accuracy of 83% on a specialized test set, it faced limitations in recognizing certain complex chemical structures and stereochemical details, and struggled with images presented in inverted formats.The accuracy and reliability of OCSR continue to improve as newer models are developed and rened.The use of multiple models for cross-validation purposes enhances robustness, offering better performance than what could be achieved by a single model.This progress is vital as it addresses the signicant challenge of extracting organic reaction data on a large scale, a task that is increasingly crucial due to the exponential growth of available chemical data.</p>
<p>Other visual information extraction.</p>
<p>The extraction and analysis of experimental data, particularly data presented in gures, are critical yet challenging tasks in chemical research.Beyond the mere detection of chemical structures, there is a signicant need for advanced capabilities to analyze experimental data comprehensively.This task requires a multimodal approach that can integrate and cross-validate information from both gures and textual descriptions, an area that remains relatively underdeveloped.</p>
<p>2.1.6.1 Advancements in multimodal large models.Recent advancements in AI have introduced multimodal large models, such as GPT-4, Gemini, and Claude, which have demonstrated promising capabilities in summarizing information from diverse sources.These models can be adept at extracting and synthesizing comprehensive experimental data from the scien-tic literature on catalysis.</p>
<p>2.1.6.2Capabilities of multimodal large models in chemical data analysis 2.1.6.2.1 Graphical data analysis.Many of these advanced models are now capable of interpreting trends and patterns directly from graphical representations, although the variability in data presentation styles continues to challenge the accuracy and reliability of the extractions.more intuitive interfaces between researchers and computational systems.</p>
<p>2.1.6.2.3 Integration with OSRA.Efforts are ongoing to integrate systems like the Optical Structure Recognition Application (OSRA) with multimodal LLMs to enhance the extraction of chemical structures from the literature.For instance, DP Technology's introduction of the Uni-Finder module represents a step forward (still at a testing stage on May 5th 2024).This module is designed for the comprehensive reading of scientic documents, including journal papers and patents, which facilitates a deeper understanding and utilization of published research.</p>
<p>The continuous improvement of multimodal LLMs is expected to revolutionize how scientic results are communicated and utilized.As these models become more sophisticated, they will enable the scientic community to integrate vast amounts of data in unprecedented ways.This integration is anticipated to lead to the development of new tools that could dramatically enhance the efficiency and creativity of catalyst design processes.The ability to compile and analyze the extensive data generated globally by researchers represents a transformative shi towards data-driven science, promising signicant advancements in how we discover and develop new materials.</p>
<p>Text information extraction with language models</p>
<p>Before the advent of large language models (LLMs), there was signicant effort in natural language processing (NLP) dedicated to extracting chemical information from texts.This process involved several traditional NLP tasks such as named entity recognition, relation extraction, and the construction of knowledge graphs. 77,78In named entity recognition, entities (which could be single words or phrases) are identied and categorized within the text, facilitating the detection of reagents, products, catalysts, and other chemical entities.Relation extraction focuses on identifying the connections between these entities, while knowledge graphs organize these entities and their relationships into structured representations.This foundation has enabled the creation of catalysis datasets related to topics like hydrogen production, 12 CO 2 reduction, 13,14 and single-atom heterogeneous catalysis. 79.2.1 Evolution of tools and techniques 2.2.1.1ChemDataExtractor.The ChemDataExtractor tool, 80,81 developed as early as 2016, utilizes word tokenization, clustering, and traditional machine-learning models to extract chemical knowledge from the literature.This tool can identify compounds and their properties, setting a precedent for the integration of more sophisticated models.In 2021, Regina Barzilay et al. developed the ChemRxnExtractor, 82 a two-stage deep learning architecture based on transformer models.This system uses product extraction and reaction role labelling to structure chemical data. The tansformer architecture's attention mechanism allows the model to concentrate on relevant parts of the data for different tasks, and its adaptive pre-training on large-scale unlabelled text has signicantly improved its ability to identify and organize chemical information from textual sources (Fig. 4).It achieved notable F1 scores of 76.2% for product extraction and 78.7% for reaction role labelling on a specialized dataset.</p>
<p>2.2.1.2SciBERT.3][14][15][16] These models have effectively turned the extraction of text-based data from scientic papers into a nearly solved challenge.</p>
<p>2.2.1.3LLMs.Omar M. Yaghi et al. 16 utilized OpenAI's GPT-3.5 to extract and format synthesis information of metalorganic frameworks (MOFs) from the literature.They addressed the hallucination issue in LLMs through careful prompt engineering and context provision (Fig. 5).The process involved segmenting the text, creating numeric vectors to represent each segment, comparing vectors to the ones of predened synthesis descriptions, and choosing the segments with high similarity.GPT-3.5 then classied these segments as 'synthesis' or 'nonsynthesis' using in-context learning (ICL), before formatting the synthesis information into tables.This approach, which also led to the development of a chemistry chatbot, demonstrates a promising framework for using LLMs for extracting and organizing scientic information.</p>
<p>Summary</p>
<p>In the domain of chemical information extraction, advancements have been marked by the development and deployment of diverse methods and tools.These technologies are succinctly summarized in Table 1 and are broadly categorized into three primary types based on the underlying technology: rule-based OCSR, machine learning-based (ML-based) OCSR, and language model-based (LM-based) systems.</p>
<p>The rule-based OCSR systems, once dominant, are now increasingly complemented or surpassed by neural networkbased methods due to their exibility and growing accuracy.These machine learning-based systems are not only more adaptable but also continue to improve as they learn from more data.The incorporation of rule-based techniques as a supplementary approach provides a layered methodological depth that enhances the overall robustness and generalizability of these technologies.</p>
<p>Language model-based systems, particularly those utilizing advanced LLMs, represent the frontier of chemical information extraction.Although their full potential is yet to be realized, the rapid evolution into multimodal models suggests that transformative developments could emerge shortly.These models are particularly promising for handling the vast and complex data typical in catalysis research.The transition to open-source methods has also played a critical role in this eld.Beginning with systems like OSRA in the 1990s, the move towards open-source has not only facilitated wider access to advanced tools but has also spurred innovation and customization, enhancing the collective capability of the research community.</p>
<p>This evolving landscape of chemical information extraction methods underscores the importance of continual adaptation and development to harness the ever-increasing volumes of data in catalysis and other elds of chemistry.</p>
<p>Automated and intelligent chemical robotic system</p>
<p>Automation technologies have profoundly transformed modern manufacturing, yet their integration into chemical research remains limited.This is primarily due to the challenges in meeting the diverse and exible synthesis and characterization requirements of various chemical systems.Effective machine learning applications in this context demand a densely populated dataset within the search space to develop reliable models and derive meaningful insights.Consequently, the experimental systems employed must be both high-throughput and dependable.</p>
<p>4][85][86] The origins of chemical automation date back to the 1960s and 1970s with the development of automated devices like automated peptide synthesizers, 87 DNA synthesizers, 88 and organic synthesis modules. 891][92][93][94][95][96] More recently, the introduction of humanoid chemical robots [4][5][6] and autonomous ow-based synthesis platforms [17][18][19] has marked a new era of innovation in intelligent chemical synthesis.</p>
<p>A notable feature of this latest advancement is the interactive "ask and tell" process, such as active learning, where models are continuously trained on current observations and actively request additional data.This interactive approach can signicantly accelerate discovery efficiency compared to traditional screening strategies. 97Therefore, experimental processes must be designed to be not only high-throughput but also sufficiently exible to allow frequent access and modications.This is also the stage where LLMs can contribute, integrating crucial domain knowledge to enhance exploration and decision-making processes.</p>
<p>In this section, we will discuss how advancements in hardware design, coupled with LLMs, enhance operational exibility.Later, we will explore the promising potential of LLMdriven active learning in the subsequent section.</p>
<p>Automated and intelligent chemical experiment platform</p>
<p>To address diverse research tasks, various hardware design principles and methods were employed in building automation systems.This review will cover two categories of the systems:</p>
<p>(1) Humanoid robotic systems: this approach relies on the usage of multi-axis arms that provide a high degree of operation exibility, mimicking the behavior of human operators.</p>
<p>(2) Automated ow chemical systems: these systems are designed on the foundation of uid dynamics and transport pipelines to achieve precise chemical operations, which can be seamlessly interfaced with analytical instruments.</p>
<p>Humanoid robotic system</p>
<p>In a laboratory environment, a robotic arm coupled with automated guided vehicles (AGVs) and advanced computer vision systems 5 can robustly complete tasks such as sample preparation and handling, control of instruments, and integration of data recording, analysis, and experiment design.Key to this scheme is the exibility introduced by AGVs and robotic arms as compared to that of their predecessors.</p>
<p>The AGV-based autonomous mobile robot system launched by Andrew I. Cooper et al. 4 is a remarkable advance in chemical automation.The team found improved photocatalysts for producing hydrogen from water aer autonomous running for 8 days, completing 688 experiments in a design space of 10 variables.The robot (Fig. 6) can handle sample vials among eight workstations distributed around the lab, including a solid reagent dispensing system, a mixed liquid dispensing system and capping module, an ultrasound module, a photolysis module, a gas chromatography (GC) analysis module, and three separate sample storage modules to achieve a variety of experimental tasks.</p>
<p>Despite the great advances, the mobile robotic chemist from Cooper's group is purely driven by Bayesian algorithms and does not capture existing chemical knowledge or include theoretical or physical models.Later, a comprehensive articial intelligence chemistry laboratory (Fig. 7) was developed by Jun Jiang's team. 5This AI-Chemist consists of three modules, including a machine-reading module, a mobile robot module, and a computational module.The AI-Chemist system responds to scientic questions posed by researchers by tapping into vast amounts of literature.It digitizes and standardizes experimental protocols, enriching its knowledge base.The platform manages tasks, monitors the mobile robots, customizes experiment workows, and stores the data for future use.The research team used the platform to nd the best combinations of several Martian meteorite rocks to synthesize efficient water oxidation catalysts for future use in Martian exploration. 98he recent A-lab, developed by Gerbrand Ceder et al., 6 represents a signicant advancement in the eld of solid material synthesis.Despite some controversy on the actual phases of the fabricated materials, the hallmark of the A-lab is its high degree of automation, which encompasses the entire synthesis and characterization process, including critical steps such as powder dosing, sample heating, and X-ray diffraction (XRD) for product characterization.</p>
<p>One critical issue with the robotic arm system in laboratory settings is its moderate capacity to parallelize experimental tasks.While robotic arms bring automation and precision to the table, they still mimic human researchers to conduct multiple operations one by one.This constraint is particularly evident in high-throughput settings where speed and efficiency are paramount.To address this, integrating robotic systems with other automated solutions might be necessary.</p>
<p>Automated ow chemical system</p>
<p>01][102][103][104] The reactors used in the ow system can be categorized into two distinct types: batch reactors connected by pipelines and continuous ow reactors.The major advantage of the ow system comes from low-cost modularity, where the reaction module, product separation module, and detection module can all be connected to the same pipeline in sequence or parallel.</p>
<p>3.3.1 Batch reactors.An example of the batch reactor system connected by pipelines is the Chemputer developed by Leroy Cronin et al. in 2019. 17It is a general automated platform for organic synthesis (Fig. 8) with a uid backbone from a series of syringe pumps and six-way valves.The materials can be transported among modules.The modules support many operations including mixing, ltration, liquid-liquid separation, evaporation, and chromatographic separation.The same research team 18 has also introduced an autonomous workow to read the literature and execute experiments.A chemical description language (cDL) that aims to include all the synthesis operations in a standard format was proposed.Utilizing this system, the authors showcased the automated synthesis of 12 compounds from the literature, encompassing the painkiller lidocaine and several other pivotal molecules.]105 One drawback of many ow systems is the lack of exibility for different experiment tasks.One solution is to use general modules and their combination to support wider experiments.Alternatively, the modules can be reaction-specic as long as they can be designed and fabricated efficiently.Leroy Cronin et al. 49 showcased a portable, suitcase-sized chemical synthesis platform with automated on-demand 3D printing of groups of reactors for different reactions.Researchers demonstrated the broad applicability of this system by synthesizing ve organic small molecules, four oligopeptides, and four oligonucleotides, achieving good yields and purity.</p>
<p>The implementation of batch reactors with increased throughput has accelerated the search for catalysts in more complex systems that involve multiphase reactions.Cheng Wang et al. 106 developed a fast screening platform with a coherent implementation of automated ow cell assembly and GC characterization.It was used for parallel synthesis, electrochemical characterization, and catalytic performance evaluation of electrocatalysts for the reduction of CO 2 to C 2+ products, which led to the discovery of a Mg-Cu bimetallic catalyst with competitive CO 2 to C 2+ performance and good stability compared to the top catalysts from other literature reports (Fig. 9).</p>
<p>3.3.2Continuous ow reactors.Continuous ow reactors 107 provide a scalable solution for organic molecule synthesis, 103,108 inorganic material preparation, 109,110 colloidal nanomaterial synthesis, 111,112 and electrochemical synthesis, 113,114 and have gained wide applications in industry.</p>
<p>The reactants are rst pumped into a mixing device and then ow into temperature-controlled pipes or microstructured reactors until the reaction is complete.][117][118][119][120][121][122] Timothy F. Jamison et al. 115 developed a exible, manually recongurable benchtop ow chemistry platform (Fig. 10), including various reactor modules for heating/cooling, photochemical reaction, and packed bed reaction.In addition, the platform integrates liquid-liquid separation technology and is equipped with inline analysis tools such as high performance liquid chromatography (HPLC), Fourier transform infrared spectroscopy (FTIR), Raman spectroscopy, and mass spectrometry.</p>
<p>One issue of the continuous ow system is its high cost in paralleling and adaptation.To partly address this issue, Kerry Gilmore et al. 116   by Nathan Collins et al. 117 in an advanced automated continuous ow synthesizer called AutoSyn, which can access 3800 unique process combinations and up to seven consecutive reaction steps for efficiently preparing a variety of pharmaceutical small molecule compounds with a scale from milligrams to grams within hours.</p>
<p>To make the uidic system even more adaptive, Klavs F. Jensen et al. 123 combined the robotic arm and the ow system (Fig. 11): the robotic arm is responsible for assembling modular process units, including reactors and separators, into a continuous ow path.Aer the synthesis, the robotic arm can disconnect the reagent lines and move the processing module to the appropriate storage location.Pneumatic grippers are used to ensure tight connections between process chambers.In 2023, the same group introduced a prototype that further incorporates machine learning with robotics to autonomously design, synthesize, and analyze dye-like molecules with minimal human intervention. 124This system successfully synthesized and characterized 303 new dyes, advancing the efficiency of chemical discovery.</p>
<p>Flow chemistry systems, while revolutionizing chemical synthesis and processing, present several limitations in automation.The setup and maintenance of these systems are complex and resource-intensive.Establishing precise control over ow rates, temperature, and pressure requires specialized equipment and expertise.This complexity also extends to scalability issues; while ow systems excel in scaling up certain types of reactions, they may be less adaptable for reactions requiring long residence times or intricate synthesis steps.Additionally, the rigidity in altering reaction conditions can limit their exibility, making them less suitable for laboratories that frequently switch between diverse chemical processes.Material compatibility is another concern, as the construction materials of the ow reactors must withstand a wide range of chemicals and conditions, limiting their use with highly reactive or corrosive substances.Furthermore, while adept at handling large-scale production, ow chemistry systems can be less efficient for small-scale synthesis, oen leading to inefficiencies and wastage when dealing with minute quantities.</p>
<p>Large language models and robots</p>
<p>The introduction of LLMs to robotic systems denes a new frontier in automation.</p>
<p>First, LLMs have facilitated the development of robotics, including log information extraction, assisted robot design, 125 and task generation and planning. 42,43,126,127As pointed out by Francesco Stella et al., 125 LLMs can be the creator for designing the automating system, be the mentor and copilot for domain scientists who do not have the necessary educational background to implement automation in their research, and be an assistant to debugging, troubleshooting, and method selection during the technology implementation phase to accelerate the process.</p>
<p>Second, LLMs, especially the multimodal ones, can help develop next-generation robots with increased exibility.Vemprala and others from the Microso team 126 proposed a strategy that combines prompt engineering and a high-level feature library to enable ChatGPT to handle various robotic tasks and scenarios.An open-source tool called PromptCra was  introduced, which includes a collaboration platform and a ChatGPT-integrated sample robot simulator.However, the LLM-controlled robotic movement is not robust enough for direct use in chemistry experiments where safety and reliability are of primary concern.</p>
<p>Third, LLMs also offer solutions to program robots.Kourosh Darvish et al. introduced the CLAIRIFY method, 42 which combines automatic iterative prompting with program verication to ensure the syntactic accuracy of task plans and their alignment with environmental constraints.The system's objective is to produce a syntactically correct task plan suitable for robotic action as a prompt for LLMs to generate a program.However, the generated plan needs to be veried to detect any compilation error and pass the error messages as subsequent input prompts for iterative interaction with the LLMs.The capability of this method was demonstrated by translating natural language to an abstract and concise high-level chemical description language (cDL), which was originally developed and used in the control of Chemputers. 18ompared to high-level descriptive codes, generating lowlevel operational codes to interface directly with the robotic system can be more complicated.Genki N. Kanda et al. 43 demonstrated that GPT-4 can generate low-level operational Python scripts for automated robots like Opentrons-2 (OT-2) from natural language instructions.They designed a pipeline based on GPT-4 to automatically translate natural language experimental descriptions into Python scripts compatible with OT-2.Leveraging OpenAI, this approach iteratively queries the model, extracts, and validates scripts using a simulator of OT-2, and provides feedback on any errors for correction.This shi towards natural language instruction simplies the automation process, making it accessible to a broader range of researchers and promoting the automation of biological experiments.</p>
<p>Summary</p>
<p>Automated and intelligent chemical robotic systems are promising to signicantly enhance the efficiency, accuracy, and reproducibility of experiments.Table 2 summarizes various types of automated and intelligent chemical robotic systems, detailing their specic functions, supported operations, characterization techniques, and chemical spaces they explored.These systems range from humanoid robotic systems to batch reactors and continuous ow reactors, each with unique capabilities and applications to study different chemical systems.</p>
<p>We expect a much enhanced automation level in chemistry research.However, current automation in chemistry still faces challenges, particularly in the trade-offs between the exibility and throughput of automated systems.For instance, although capable of vast amounts of operations compared to ow systems, humanoid robotic systems are usually slower in operational speed to ensure accuracy and safety.On the other hand, ow chemistry systems can handle hundreds or thousands of experiments per day, but are more task-specic with limited exibility.New developments in these strategies are required to enhance exibility, throughput, and robustness at the same time.</p>
<p>Another challenge lies in the control part of the robotic systems.Although digital twins are very common for humanoid robotics and in industry, the development of digital twins for the whole automated chemistry system is still at its initial stage despite a few efforts. 4,18,128Ensuring the integrity and safety of experimental procedures remains paramount in automation labs.Therefore, greater attention must be directed toward enhancing the capability to simulate experimental procedures and detect any potential physical or chemical issues during the development of various robotic systems.Furthermore, despite the rapid advancements in novel algorithms, such as reinforcement learning, the control of robots in chemistry labs oen relies on hardcoded programming.This limitation restricts their ability to perform complex tasks and adds challenges to the maintenance, transferability, and future development of the systems.LLMs appear promising in introducing exibility to control systems.However, the reliability of LLMgenerated code must be veried either by human experts or through digital twins.It is foreseeable that digital twins and LLMs will soon be more cohesively integrated into the control of chemical robotic systems.</p>
<p>Design and discovery of catalysts with active machine learning</p>
<p>In the discovery of catalysts, the search space is oen vast and grows exponentially with the number of parameters.This inherent complexity makes the traditional trial-and-error approach for catalyst screening both labor and computationally intensive and time-consuming.The emergence of ML and LLMs has provided opportunities to address this problem.By utilizing ML and LLMs to guide experimental design with experimental or theoretical feedback, the search for catalysts can be signicantly accelerated.</p>
<p>Design of catalysts guided by machine learning</p>
<p>The implementation of ML in experimental design can lead to more efficient and cost-effective research.Olsson 129 denes active machine learning as a supervised machine learning technique in which the learner (i.e., the machine learning model) determines the sampling point from which it learns.Bayesian optimization (BO) 130 and active learning (AL) are two important branches of active machine learning that are applied in catalyst design.</p>
<p>BO is an optimization strategy that balances the exploration of uncertain regions and the exploitation of known regions with superior objective values.It is generally used to optimize a black-box function and consists of three key components:</p>
<p>(1) Surrogate model: this is a predictive model designed to approximate the underlying function.][135][136] (2) Acquisition function: an acquisition function is a scoring function used to rank sampling points within the input space based on the surrogate model's predictions.Examples of such</p>
<p>Chemical Science</p>
<p>Review functions include expected improvement (EI), 137,138 probability of improvement (PI), 139 and upper condence boundary (UCB). 140The acquisition function is instrumental in selecting the most promising candidates for further evaluation.</p>
<p>(3) Bayesian inference: 141 this is a foundational technique in Bayesian optimization, utilized for training the surrogate model.It uses Bayes' theorem to update the probability of a hypothesis or event based on observed evidence.</p>
<p>On the other hand, AL is a family of machine learning techniques that aims to minimize the number of labelled data points while obtaining a high-performance model.It can usually be achieved through an adaptive sampling strategy, which prioritizes the labelling of data points with the highest uncertainty and information gain for the model.</p>
<p>Both BO 4,20-28 and AL [29][30][31][32][33][34] have been applied in the design of and search for catalysts.BO can efficiently explore the vast parameter space of catalyst design and select experiments that are likely to yield the desired products.By iteratively updating the ML model and selecting new experiments based on the retrained model, BO can guide the search for optimal catalysts.AL, in the meantime, can assist in selecting the most informative data points for labelling, reducing labelling costs while improving model performance.It has been applied in many elds including materials design, 142,143 retrosynthesis, 144,145 and drug discovery. 146,147Besides the original purpose of AL, its application in catalyst design also demonstrated its capability for global optimization, presenting a remarkable analogy to the BO framework.The applications of BO and AL in the eld of catalysis will be discussed respectively below.</p>
<p>Bayesian optimization</p>
<p>BO effectively balances exploration and exploitation to identify the best candidates within the design space.The method can signicantly reduce the number of experiments required to nd the optimal reaction parameters or formulations.For example, in 2020, Yusuke Yamauchi and coworkers 20 employed BO to efficiently discover ternary PtPdAu alloy catalysts.They exhibited excellent catalytic activity in electrochemical methanol oxidation (Fig. 12).Remarkably, through only 47 experiments, which is less than 1% of the potential composition space, the authors successfully discovered the optimal composition with a high catalytic performance.More interestingly, the sampling scheme using current density as the performance metric yielded a precursor composition with minimal Au content, which would have been challenging for chemists to predict.Thus, the implementation of BO can not only accelerate the search for catalysts but also offer new insights into the design of catalysts.In 2020, Bayesian experiments for autonomous researchers (BEAR) 21 combined BO with high-throughput automated experiment systems to achieve self-driven material discoverya cycle of the design of experiments, automated experiment feedback, and retraining of machine learning models to design new experiments.As discussed before, Andrew I. Cooper et al. 4 developed an AI chemist to improve the catalytic performance for hydrogen production with BO (Fig. 13).It successfully discovered a mixture of photocatalysts that exhibited six times higher activity than the original formulation.Compared to manual operations, the experimental time cost is reduced by approximately 60 times.</p>
<p>In 2021, Jan Rossmeisl et al. 22 developed a computational framework that combines density functional theory (DFT) calculations, ML-driven kinetic modelling, and BO to explore a wide range of composition space to search for multi-component high entropy alloys for the oxygen reduction reaction (ORR).To accelerate catalyst discovery, the authors integrated kinetic modelling with BO, where a Gaussian-process-based surrogate model provided suggestions for alloy compositions.The proposed compositions were evaluated using the kinetic model, and the surrogate model was updated based on the ORR activity predicted by the kinetic model.BO effectively identied optimal compositions through 150 iterations, including Ag 18 Pd 82 , Ir z50 Pt z50 , and Ir z10 Pd z60 Ru z30 (Fig. 14).These compositions closely matched the optimal compositions found through grid search in the same chemical space.Experimental conrmation of the three optimized compositions by highthroughput thin-lm synthesis and ORR testing in the Ag-Pd, Ir-Pt, and Pd-Ru binary alloy spaces, reveals the best-performing compositions of Ag 14 Pd 86 , Ir 35 Pt 65 , and Pd 65 Ru 35 .The experimental results reasonably matched the results of BO, and BO can accelerate the discovery of optimal catalysts by up to 20 times.</p>
<p>Active learning</p>
<p>Active learning is a strategy that explores the design space to establish a precise and reliable mapping from it to an output space (e.g.various properties of compounds) and optimizes toward high-performance solutions.Active learning can be used to reduce the number of expensive DFT simulations for the design and screening of catalysts in a large space.</p>
<p>Yousung Jung et al. 30 proposed an active learning method in the discovery of catalysts for the CO 2 RR driven by uncertainty and prediction error.It utilizes cost-effective non-ab initio input features, i.e., LMTO d-band width and electronegativity, as chemisorption descriptors to predict adsorption energies on alloy surfaces.Screening of large-scale materials is carried out by combining these descriptors with two machine learning models: an ensemble of articial neural networks (ANNs) and kernel ridge regression (KRR).The catalytic performance of a set of 263 alloy systems was studied by predicting <em>CO binding energy using the models.During the active learning process, an ensemble consisting of ve neural networks with the same architecture but varied initial weights was trained on an initial dataset.The ensemble was used to predict the </em>CO binding energy on the rest of the dataset to nd candidates with the highest prediction variance, which will be included in the next training process.As an alternative machine learning model, the performance of KRR 148,149 was also elaborated.It involves the training of a KRR model on the initial dataset with <em>CO binding energy as the output.Then, an additional KRR model was trained on the prediction error from the previously trained model as an error predictor. 148,150Later, the KRR error predictor was used to estimate the error rate for the rest of the dataset, which helps select candidates for the next round of training.Both models (ensemble of ANNs and the KRR model) were used to predict the adsorption energy of CO on (100) crystalline surfaces.The best model gives an RMSE of only 0.05 eV without the d-band center as a descriptor.The authors discovered Cu 3 Y@Cu</em> to be a highly active and cost-effective catalyst for the CO 2 RR.</p>
<p>Besides the original purpose of using active learning to establish an accurate and reliable model, it can also be utilized for global optimization.In 2018, Zachary W. Ulissi et al. 31 proposed a cyclic workow with ideas from agent-based model optimization and active learning for screening electrocatalysts for the CO 2 RR and HER.This workow, illustrated in Fig. 15, involves machine learning screening, DFT validation, and machine learning retraining.To start, the researchers obtained a search space of intermetallic crystals and their corresponding surfaces from the Materials Project. 151They then selected a series of materials as optimal candidates for catalysis using a machine-learning model.DFT calculations for the selected candidates were performed, providing more accurate predictions of the catalytic properties.The DFT results were then used to retrain the machine learning model, creating an iterative process for continuously improving the catalyst database.In their study, the authors considered a total of 31 elements, composed of 50% d-block elements and 33% p-block elements.The search space consists of 1499 intermetallics for potential catalysis applications.131 possible surfaces from 54 alloys and 258 possible surfaces from 102 alloys were identied as valid candidates for the CO 2 RR and HER, respectively.The number of candidate alloy catalysts can be further reduced to 10 and 14 for the CO 2 RR and HER.This comprehensive screening approach allowed for the identication of theoretically promising catalysts for the CO 2 RR and HER.</p>
<p>In 2020, Edward H. Sargent et al. 32 developed a machine learning-accelerated, high-throughput DFT framework for rapid screening of CO 2 RR electrocatalysts (Fig. 16) similar to the one from Zachary W. Ulissi's group 31 described above.The researchers studied a dataset of 244 different copper-containing intermetallics, forming a search space of 12 229 surfaces and 228 969 adsorption sites.DFT simulations were performed on a subset of these sites to calculate the CO adsorption energies.These data were then used to train machine learning models to predict the CO adsorption energy on the adsorption sites.The researchers encoded each adsorption site as a numeric array and used a combination of random forest and boosted trees to enhance prediction performance.The framework combined the machine learning predicted CO adsorption energy with the volcano scaling relationship to identify sites with the highest catalytic activity.These optimal points were then simulated using DFT to provide additional training data for the machinelearning model.Thus, an active learning workow was established, cycling between DFT simulations, machine learning regression, and machine learning prioritization, to continuously query and construct a DFT database.This workow performed over 300 regressions, which guided DFT calculations for</p>
<p>Chemical Science Review</p>
<p>CO binding energies at approximately 4000 different adsorption sites to identify Cu-Al as the most promising material for the CO 2 RR in the search space.Furthermore, the authors synthesized de-alloyed nanoporous Cu-Al catalysts for validation, which achieved over 80% Faraday efficiency (compared to ∼66% for pure Cu) at a current density of 400 mA cm −2 (1.5 V vs. NHE).It showed a 2.8-fold improvement in cathodic power conversion efficiency (PCE) at 400 mA cm −2 compared to previous state-of-the-art results.This work demonstrated an effective method for high-throughput catalyst screening, combining machine learning and DFT calculations.</p>
<p>While BO and AL are initially different approaches, they tend to converge on the catalyst optimization task.BO usually uses a probabilistic model with the goal of optimization, while AL can adopt more diverse models with the goal of efficiently constructing a machine learning model.When AL also used a probabilistic model and assessed uncertainty in making the decision about which point to explore next, it is equivalent to exploration-oriented BO, but the ultimate goal of AL is to improve the model most efficiently, which is beyond the uncertainty strategy.</p>
<p>When all the obtainable information about the system comes from the previous experimental/calculation results, BO and AL are mathematically sound methods to most efficiently explore the space.However, when domain knowledge is available, it is possible to come up with a more efficient strategy by combining the testing information with domain knowledge.The addition of domain knowledge into the process can be achieved by using LLMs.</p>
<p>Design and synthesis of catalysts guided by large language models</p>
<p>The diverse and interdisciplinary knowledge spanning chemistry, materials science, computer science, and data science, which are needed for the data-driven design and discovery of catalysts, can present a formidable challenge for researchers.LLMs 152,153 offer a promising solution to overcome the knowledge gaps from multiple elds efficiently.LLMs have been used by chemists for tasks such as catalytic reaction prediction, 45 property prediction, [154][155][156][157] and synthesis condition design. 156,158n BO and AL, a machine learning model (or a surrogate model) is necessary to approximate a mapping.Traditional machine learning models can take continuous, discrete, or categorical variables as the input.In contrast, LLMs, with their inherent capabilities to process natural language descriptions and generate new content accordingly, can be potentially used as a surrogate model, which can support a versatile input format.To incorporate the training data into the models, incontext learning (ICL), a technique that includes training data as examples in the prompt for LLMs, can be used.Alternatively, ne-tuning the models using the existing dataset represents another viable approach.</p>
<p>Review</p>
<p>Chemical Science</p>
<p>Andrew D. White's group 45 demonstrated the usage of LLMs as the surrogate model in Bayesian optimization.The aim is to use a generative pre-trained transformer (GPT) as a surrogate model to predict the properties of the product according to the experimental procedure.Both ne-tuning and ICL were used for model training.To introduce prediction uncertainty when querying the LLMs, they designed two prompting strategies, a (1) multiple-choice option template and (2) top k completions template for regression.With the multiple-choice template, the LLM will treat the regression problem as a multi-option question to give a predicted value in one of the ve ranges.Furthermore, the probability of selecting each option can be accessed.In the top k completion template, the question will be queried k times to the LLM to generate k answers.Both strategies generated a discrete probability distribution of the output, which can be used in Bayesian optimization.The authors used a series of models from OpenAI (text-curie-001, text-davinci-003, GPT-4, etc.) with in-context learning or ne-tuning to predict the C 2 yield for oxidative coupling of methane based on synthesis procedures.Gaussian process regression was used as a baseline with text embedding to convert the synthesis description to a numeric input.Among the LLMs, GPT-4 is the best model in either ICL or ne-tuning.When GPT-4 and the top-k completion strategies were used, the ICL model showed comparable performance (mean absolute error, which is abbreviated as MAE, of 1.854) to the Gaussian process regression (MAE of 1.893).When the ne-tuning was implemented, the MAE of the model was further decreased to 1.325.Later, the authors implemented Bayesian optimization using the Gaussian process or LLMs with ICL as the surrogate model.The ICL model reached 99% quantile aer 15 samples, aer which the performance did not improve signicantly and failed to nd the maximum value in the sample pool.Although the GPR model also failed to nd the maximum in the sample pool, it was a little closer to the maximum and showed a higher efficiency in the optimization.Due to the token size limitation and the complexity of the C 2 data, the authors only selected the ve most relevant examples during ICL, which can be the reason</p>
<p>Chemical Science</p>
<p>Review why the ICL model did not perform as well as the GPR model in Bayesian optimization.However, as a proof-of-concept, it is enough to demonstrate that LLMs have the potential to guide researchers in decision-making.The in-context learning ability of the LLMs is promising for building an interactive workow where an AI agent iteratively assists and instructs human experts to increase search efficiency through experimental feedback.Recently, Omar M. Yaghi and his coworkers have built such a workow and demonstrated its capability in the synthesis of MOFs with prompt engineering and in-context learning. 47This innovative workow involves three components: ChemScope, ChemNavigator, and ChemExecutor (Fig. 17).With the usage of Chem-Scope, the human researchers offer GPT-4 the project goals and necessary information like the literature of reticular chemistry and availability of lab resources to generate a project blueprint.Here, GPT-4 reads the general concepts of reticular chemistry and constructs a scheme of the project with multiple stages, where each stage contains well-dened objectives and indicators for their completion.Then, ChemNavigator and ChemExecutor were used coherently to go through the stages and achieve the objectives dened by ChemScope.ChemNavigator was used to dene tasks to complete the objectives of the current stage.It takes the project scheme from ChemScope, previous trial-and-error summaries, human feedback, and current situations to update the summaries and generate three tasks accordingly.With the updated summary and tasks from ChemNavigator, the ChemExecutor outputs step-by-step instructions to complete the task.Additionally, ChemExecutor also denes a template to record the experimental feedback from the human researchers, which will be used later in the next iteration.At this point, the human researchers will perform the experiments and ll up the template.The interaction among ChemExecutor, ChemExecutor, and human researchers was iterated several times until the completion of the project.The recording of experimental feedback and consistent updating of the summary enabled GPT-4 to learn from experiment outcomes and optimize protocols to complete the complex tasks.Using this human-computer interactive workow, the researchers successfully discovered and characterized a series of isomorphic MOF-521s.This work highlights the advantages of the large language model in interacting with human experts in natural language without coding skills, making it easy to use for all chemists.Additionally, the in-context learning facilitated by GPT-4 can continuously optimize experimental protocols to complete complicated research tasks.When such a workow is integrated with automated robotic systems, it paves the way for a new paradigm of self-driving labs, where the design and discovery of catalysts go beyond a purely data-driven approach.</p>
<p>Despite the potential applications of LLMs in the design of and search for catalysts, there are still some problems to be addressed.The major problem is the well-known hallucinations in the context generated by LLMs.Although researchers have tried to mitigate this issue through methods such as prompt engineering, in-context learning, and ne-tuning, further improvements are needed to enable the accuracy and reliability of these models.Secondly, LLMs with direct domain expertise are still lacking.Thus, when dealing with domain-specic scientic problems, the models need to be ne-tuned; otherwise they can show low accuracy and misunderstanding.While LLMs hold promise in chemical research, further research and improvements are necessary to overcome the existing limitations and bring the application of articial intelligence in the research of catalysts into a new era.Review Chemical Science</p>
<p>Summary</p>
<p>Traditional trial-and-error methods require a signicant cost of time in screening and testing candidate catalysts, together with inference through expert knowledge and occasionally serendipity.Active machine learning can lower the knowledge barrier and greatly accelerate the discovery process by utilizing experimental data to build surrogate models, avoiding brute-force or uniform search of the entire chemical space.The implementation of active machine learning in the optimization of catalyst search is summarized in Table 3.Several challenges persist in implementing active machine learning, particularly related to surrogate models.These models excel in interpolation rather than extrapolation, making them prone to overtting and necessitating training data of a specic scale.Many efforts are made to improve the surrogate models for higher generality (e.g., Phoenics 135 ) and extend variables from simple continuous variables to discrete or categorical variables (e.g., Gryffin 136 ).Additionally, a crucial challenge lies in selecting relevant catalysis features compatible with surrogate models.Incorporating irrelevant descriptors can impede the effectiveness of active learning algorithms, reducing their performance to that of uniform random search.The difficulty in feature selection connes certain closed-loop searches to mere recipe optimization, treating the process as a black box and adjusting only continuous variables such as reagent ratios or concentrations (Table 3).However, catalytic reaction activity and selectivity are closely linked to explicit factors such as intermediate adsorption energy, d-band center, electronegativity, and steric hindrance, which inherently serve as valid features.These features can be assessed through ab initio theoretical calculations or in situ characterization.While the advent of automated laboratories has alleviated concerns regarding insufficient data acquisition, it remains a costly endeavor, especially considering the challenges in automating certain characterization techniques.Consequently, strategies for evaluating and selecting an appropriate subset from these explicit features require further renement.The subsequent section will delve into the detailed elaboration of chemical descriptors employed in machine learning algorithms.</p>
<p>Interpretable machine learning for catalysis</p>
<p>In catalysis research, the pursuit of knowledge extends beyond mere data collection; true understanding stems from interpretable models that can elucidate observations in ways that are comprehensible to human scientists. 39,159In this section, we explore the potential role of large language models (LLMs) in identifying suitable descriptors for catalysis systems and enhancing model-agnostic methods for interpretability.[38]</p>
<p>Descriptors for traditional machine learning</p>
<p>Understanding catalysis data begins with the identication of the correct descriptors of catalytic systems.Descriptors are crucial in interpretable machine learning because they must not only capture relevant information but also minimize redundancy.The range of available descriptors provides substantial exibility in modelling various aspects of catalytic processes.</p>
<p>5.1.1Experimental descriptors.Experimental descriptors are mainly the reaction conditions, normally including temperature, pH value, pressure, voltage, reactant concentration, and reaction time. 160,161.1.2Topological/structural descriptors.Topological descriptors are derived from molecular connectivity tables using graph theory to specify connectivity, paths, and structural features.A similar concept can be extended to crystalline materials for catalysis such as zeolites. 162,163Other structural descriptors include atomic/covalent radius, atomic number (mass number), atomic position, 164 group number, molar volume, lattice constants, rotational angle, bond length, coordination number, the number of protons and valence electrons, 165 active sites, and surface properties such as defects, microstructure, and facet characteristics. 166,167.1.3Molecular ngerprints.Fingerprints are a variety of molecular descriptors that encode a molecule based on the presence or absence of specic chemical substructures.These substructures range from simple functional groups to more complex molecular motifs.Some of the ngerprints are based on pre-dened fragments, such as Molecular ACCess System (MACCS), 168 the Daylight ngerprints, 169 and a more recent extension the Local Functional Group Fingerprint (LoFFi). 170ther ngerprints delve into the connectivity or topology of a molecule, exemplied by the Extended Connectivity Fingerprint (ECFP) 171 and its more interpretable simplication molecular fragment featurization (MFF). 172.1.4Trans-rotational-invariant 3D representations.While atomic coordinates in Cartesian axes can represent molecules or crystalline materials, these representations are not inherently invariant to translation and rotation-properties that many chemical properties of interest do possess.To address this, several strategies have been developed to make these representations operational-invariant.One approach involves augmenting the data through multiple translations and rotations, a method that is cumbersome but effective in some cases.Another method expands atomic coordinates around a central point using spherical harmonics and radial functions, exem-plied by the Smooth Overlap of Atomic Positions (SOAP) representation.A third strategy involves generating special auto-correlation functions of some function of interest, such as the revised autocorrelation functions (RACs), 173 which correlate atomic properties within a molecule or material for highly efficient encoding.</p>
<p>5.1.5Physicochemical descriptors.Physicochemical descriptors, rooted in organic physical chemistry, systematically describe the electronic and steric properties of molecules and substituent groups.A notable example is the Hammett parameters, which quantify the electronic effects of substituent groups on aromatic rings based on the linear free energy relationship.Various electronic descriptors are attributed to molecular properties at the atomic level, [174][175][176] including the lipid/water distribution coefficient log P, molar refractivity (MR), electronegativity, and atomic charges.8][179] Tools like PaDEL-Descriptor soware 180 and SPOC descriptors 181 package them into comprehensive descriptor suites for broader applications in research.</p>
<p>5.1.6Spectrum-based descriptors.</p>
<p>Spectrum-based descriptors 182,183 form a latent space reecting key physicochemical properties of molecules and materials, which are both measurable and calculable.Certain spectra can directly reveal interactions critical in catalysis, such as the vibrational spectra of CO adsorbed on metal surfaces, which provide insights into adsorption energy, charge transfer degree, bond energy, and the d-band center of the metal. 184.1.7Theory-based descriptors.In heterogeneous catalysis, the adsorption of a reactant on the catalyst's surface typically represents the initial step.Consequently, adsorption energy serves as a critical descriptor.Notably, the adsorption energies of various species are interconnected through the linear free energy relationship, or the scaling law, oen referred to as Brønsted-Evans-Polanyi (BEP) relations. 185A recent study by Lin Zhuang and coworkers applied principal component analysis to the adsorption energies of multiple species, 186 revealing just two independent components which correspond to covalent and ionic interactions, respectively.Beyond adsorption energy, the potential of zero charges on an electrocatalyst's surface adds another vital dimension to electrocatalyst design. 187[190][191][192][193][194] 5.1.8Graph-based representations.Graph-based representations have emerged as a potent tool for delineating the geometry and connectivity of catalytic materials.In these models, atoms are depicted as nodes and bonds as edges within molecular or crystal graphs.Graph convolution techniques allow for embedding these graphs into numeric vectors, making them suitable for analysis via machine learning models. 166Since the application of this approach to inorganic crystalline materials 195 in 2017 and to organic reactions 196 in 2018, graph-based machine learning models for molecules have rapidly developed.This methodology is now a mainstream approach for addressing the complex, high-dimensional, nonlinear relationships characteristic of catalysis.</p>
<p>Descriptor selection and machine learning</p>
<p>Descriptor selection is a crucial step in the machine learning process, involving the elimination of irrelevant and redundant descriptors.This task is particularly challenging in catalysis research, where data sets are oen limited.An overly large set of descriptors can lead to spurious correlations that do not reect underlying chemical phenomena.Traditional machine learning techniques vary in how they select descriptors: 5.2.1 Multivariate linear regression (MLR).][199][200][201] Methods like LASSO promote sparsity (encouraging most of the coefficients to be zero) in the model by penalizing the magnitude of the coefficients, which helps in reducing overtting and enhances interpretability by retaining only the most signicant features.</p>
<p>5.2.2</p>
<p>Tree-based models.Models such as random forests and gradient boosting trees 202 inherently select and rank features based on their importance, which helps in understanding which descriptors are more critical. 172.2.3 Symbolic regression (SR) and sparsifying operator (SISSO).These methods elegantly assemble descriptors into mathematical formulations that provide deeper insights into catalysis mechanisms. 203For example, SR identied a simple geometric parameter m/t to guide the design of oxide perovskite catalysts with enhanced oxygen evolution reaction activities. 204unhai Ouyang who developed SISSO 203 continued to rene the method, which has been widely implemented to nd the numerical relationship, ranging from predicting free energy 205,206 to reaction activity. 207.2.4 Dimensional reduction.Techniques like principal component analysis (PCA) 208 reduce the dimensionality of the data, although PCA's linearity is a limitation.Nonlinear dimension reduction methods, such as kernel PCA and manifold learning or autoencoders, have been developed to overcome these restrictions.</p>
<p>Articial Neural Networks (ANNs): these models automatically extract and continuously rene descriptors through the iterative updating of network weights.</p>
<p>Incorporating chemical knowledge through LLMs</p>
<p>All the above descriptor selection processes have neglected the physical meanings of descriptors, which can lead to models that lack interpretability or generalizability.Large language models (LLMs) have the potential to revolutionize this process by embedding chemical knowledge into the selection process.They can track the physical signicance of descriptors and, when data alone are insufficient, use embedded chemistry knowledge to guide the selection process.Recent advancements have demonstrated the utility of augmenting traditional models with LLMs to leverage the linguistic implications of descriptors, providing a novel perspective on model training and interpretability. 209.3.1 Pre-trained molecular models.Pre-trained molecular models, inspired by the success of pre-trained language models, utilize deep neural networks trained on vast unlabelled molecular databases.These models can be ne-tuned for specic downstream tasks, signicantly enhancing representation capabilities and improving prediction accuracy across a range of applications. 210The pre-training tasks typically involve reconstructing molecules from masked or perturbed structures, whether represented in 3D space, as 2D images or graphs, or as 1D symbolic sequences like SMILES.</p>
<p>One such pre-trained model, Uni-Mol, incorporates 3D information in its self-training reconstruction process and has outperformed state-of-the-art models in molecular property prediction.It demonstrates strong performance in tasks that require spatial information, such as predicting protein-ligand binding poses and generating molecular conformations. 211imilarly, Payel Das et al. showed that a motif-based transformer applied to 3D heterogeneous molecular graphs (MOL-FORMER) excels by utilizing attention mechanisms to capture spatial relationships within molecules. 157Another innovative approach, the Chemical Space Explorer (ChemSpacE), uses pretrained deep generative models for exploring chemical space in an interpretable and interactive manner. 212The ChemSpacE model has exhibited impressive capabilities in molecule optimization and manipulation tasks across both single-property and multi-property scenarios.This process not only enhances the interpretability of deep generative models by navigating through their latent spaces but also facilitates human-in-theloop exploration of chemical spaces and molecule design.</p>
<p>Despite these advancements, caution is necessary when considering the information used during pre-training.Unlike natural languages, which are imbued with rich contextual and cultural knowledge, pure chemical structures typically contain limited information, oen constrained to basic chemical rules such as the octet rule.Pre-training models solely on 2D chemical structures or 1D SMILES strings without incorporating additional chemical knowledge may lead to models that lack substantial chemical understanding.</p>
<p>Pre-trained models, with their capacity for insightful interpretations and enhancements in molecular predictions, hold signicant promise for transforming areas in catalyst design, molecular property prediction, and reaction optimization.</p>
<p>5.3.2Direct use of language models.Before the widespread adoption of ChatGPT, researchers in 2019 began exploring the potential of using extensive text from scientic literature to encode materials science knowledge within word embeddings, aiming to recommend materials for functional applications. 35his approach resembles the Retrieval-Augmented Generation (RAG) agent, which employs a foundational language model that dynamically retrieves and integrates information from external data sources.This method helps reduce hallucination and adapt to specic domains.Fine-tuning large models on domain-specic materials, while more resource-intensive, is also a viable strategy.</p>
<p>Beyond the RAG agent, there are several examples of using language model architectures to train on chemistry data using SMILES or other molecular representations.These molecular pre-training models, discussed in the previous section, are developed from scratch purely with chemistry data and, as such, do not inherit the broader knowledge typically embedded in LLMs.Notable examples include SELFormer, which utilizes a transformer-based chemical language model to learn highquality molecular representations called SELFIES. 213Born and Manica proposed the Regression Transformer (RT), a method that abstracts regression as a conditional sequence modelling problem. 214Alán Aspuru-Guzik and his team investigated the ability of simple language models to learn complex molecular distributions, demonstrating their powerful generative capabilities through the prediction of distributions of the highest scoring penalized log P molecules in ZINC15. 215Francesca Grisoni provided a comprehensive overview of the current state, challenges, and future prospects of chemical language models in drug discovery. 216ecent initiatives have leveraged the capabilities of pretrained language models like GPT-3, ne-tuning them with chemically curated data.In 2023, Berend Smit et al. published</p>
<p>Chemical Science Review</p>
<p>an inuential paper titled "Is GPT-3 all you need for low-data discovery in chemistry" 15 rst on preprint.The title was apparently inspired by the seminal Google paper on transformers.The paper was later published in Nature Machine Intelligence with a modied title "Leveraging large language models for predictive chemistry". 156They experimented with ne-tuning GPT-3 using chemistry data written in a sentence and used it as a general machine learning model for classication and regression.The chemicals are represented by either SMILES or IUPAC names in natural language, which makes no difference in the prediction performance.The ne-tuned GPT-3 model achieved superior performance over traditional models in predicting material properties and reaction yields, especially in data-scarce scenarios.Its ability to accept the IUPAC names of chemicals as inputs facilitates non-specialist use.The authors explored the model's potential in generating molecules based on specic requirements and tested its in-context learning capabilities, which also showed promising results.</p>
<p>It is interesting to discuss what aspect of the LLM's ability is used in the task of learning chemistry data.Most likely, the LLM's abilities to learn new patterns and apply basic chemical logic are critical in these tasks.It is not clear if the LLM's general knowledge about specic molecule or functional groups is used or not.It is important to recognize that these models may not fully "understand" the underlying chemistry and should be used with caution due to their potential for producing misleading results or hallucinations.Despite these limitations, this work introduces a novel paradigm in machine learning that utilizes language models to foster advancements in low-data learning within the eld of chemistry.</p>
<p>Interpreting machine learning results</p>
<p>For data-driven research in catalysis to be fully benecial, it's crucial for models to be understandable so that human scientists can actively participate and apply their ndings.LLMs introduce both new challenges and opportunities for achieving this goal.</p>
<p>5.4.1 Model-agnostic interpretation methods.One commonly employed approach for model interpretation is SHapley Additive exPlanations (SHAP), 202 which utilizes principles from game theory, specically Shapley values, to assign importance to each feature and provide local explanations.This method has been widely used in catalysis studies to quantitatively analyze features responsible for variations in adsorption energy across different species, 217 key process variables inuencing yields, 218 and molecular features determining catalytic activity. 170Similarly, Local Interpretable Model-Agnostic Explanations (LIME), 219 which locally models descriptors' effects via an interpretable linear model, and Partial Dependence Plots (PDPs) that visualize the effect of features on predicted outcomes by marginalizing over the values of all other features, are also extensively used. 220,221.4.2Challenges with interpreting in-context learning.Applying these model-agnostic methods to the in-context learning of LLMs presents difficulties.A fundamental challenge lies in identifying coherent prompts that accurately map input features (X) to their corresponding outputs (Y).For example, consider the prompt: "Given input SMILES of the molecular catalyst is C(CCN)CC(]O)O and output yield of the reaction is 40%, please derive the output from the input".If we only asked the LLM to give the answer, we have no way to know how the model actually works.We need to add some prompt to ask the LLM to explain how the answer is arrived at.It is yet to be tested what prompt can accurately achieve the purpose and eliminate any hallucination.The ideal prompt may also be model specic and fulll two critical criteria:</p>
<ol>
<li>
<p>Interpretability: ideally, prompts should be phrased in natural language to ensure they are easily understood by human users.</p>
</li>
<li>
<p>Accuracy: prompts must accurately map input features to outputs, providing a clear and logical explanation of the data.</p>
</li>
</ol>
<p>3][224] However, as a result of gradient descent, it is not guaranteed that these searched prompts are generally interpretable.Additionally, gradientdescent-based methods are usually computationally expensive.To address these two problems, Jianfeng Gao et al. 44 introduced an interpretable auto-prompting method (iPrompt) using LLMs to directly generate and modify the prompts.There are three steps to search for ideal prompts in this method:</p>
<p>(1) Prompt proposal: in this stage, a prex of data points is fed to the LLMs, requiring them to complete the prompts that map the input features to the output values.It generates a series of candidate prompts that will be evaluated further.</p>
<p>(2) Reranking: the performance of the candidate prompts from (1) is evaluated, and those that maximize the accuracy will be maintained.</p>
<p>(3) Iterate with exploration: the top candidate prompts from (2) will be truncated randomly.Then the truncated prompts will be fed to LLMs to regenerate new prompts while maintaining accuracy.</p>
<p>This iterative process continues until no further improvements are observed.The direct generation and modication of prompts by LLMs in steps 1 and 3 enhance interpretability, while accuracy is optimized in step 2. However, despite their impressive capabilities, LLMs may still lack depth in mathematical rigor, theoretical simulation, or specialized domain knowledge required for some catalysis applications.Incorporating AI agents equipped with a comprehensive toolkit could potentially address these limitations, enhancing both the interpretability and accuracy of machine learning models in catalysis.</p>
<p>Summary</p>
<p>Interpretable machine learning models are becoming indispensable in chemical research for exploring complex chemical processes and catalytic mechanisms.These models allow chemists to extract diverse chemical information from data and elucidate structure-activity relationships with precision and efficiency.The shi towards models that prioritize excellent interpretability and continuity, such as those employing physicochemical and theory-based descriptors, marks a signicant advance over traditional Boolean ngerprints, which oen lack intuitive insights and demonstrate poor extrapolative capabilities.The use of LLMs in the learning process to bring in domain knowledge and consider chemical meaning of different descriptors in the learning process is still limited.</p>
<p>Recent developments in graph-based and latent space descriptors of pre-trained models are attracting increasing attention, despite sometimes not providing direct insights.These descriptors are valued for their potential to bridge sophisticated computational models with practical chemical understanding, a connection that is strengthening due to ongoing algorithmic improvements.</p>
<p>Model-agnostic methods like SHAP, LIME, and PDP provide robust frameworks for interpreting machine learning models.However, the methods need a signicant update to meet the new challenge due to the involvement of LLMs.</p>
<p>As we look to the future, the enhancement of interpretable models and the expansion of model-agnostic methods are set to increase AI's utility beyond mere speed and accuracy.By integrating tailored, interpretable descriptors across different systems, this approach not only deepens chemical insights but also empowers the use of machine learning to quantitatively analyze structure-activity relationships, thus broadening AI's impact on scientic discovery.</p>
<p>Conclusions and perspectives</p>
<p>The design and discovery of optimal catalysts is a complex endeavor due to the inherent complexity of catalytic processes and the vast search space.Traditional trial-and-error approaches are laborious, time-consuming, and oen fail to provide sufficient insights.However, the recent advancements in high-throughput information extraction, automated chemical experimentation, active machine learning, and interpretable machine learning have revolutionized this eld.</p>
<p>Automated extraction of unstructured chemical data, facilitated by optical character recognition and large language models (LLMs), lays the groundwork for robust data-driven approaches.Automated robotic platforms streamline experimentation, enabling real-time decision-making and facilitating closed-loop optimizations.Active learning algorithms optimize experiment selection based on accumulated data to minimize trial numbers.Interpretable machine learning models disclose underlying structure-property relationships, providing critical insights for rational catalyst design.</p>
<p>Despite these advances, challenges persist.Information extraction needs to evolve to handle diverse unstructured data formats reliably.Current technologies like image segmentation tools 225,226 are still advancing towards fully autonomous capabilities for extracting and analyzing raw chemical data from gures.Moreover, the integration of text and gure data demands enhanced anaphora resolution and inference capabilities to support detailed analyses.Future developments in multimodal AI, capable of processing text, images, video, and voice, will be crucial in this aspect.</p>
<p>LLMs have demonstrated potential in comprehending complex data and have been applied successfully in projects like the one-pot synthesis conditions of MOFs.Yet, the full scope of their capabilities, especially in formatting conditions for multistep synthesis procedures, remains underexplored.The cost and operational speed of robotic systems also limits their widespread adoption in chemical laboratories, necessitating innovations in specialized post-synthesis processing and autosampling for diverse catalytic systems.</p>
<p>The variability in control interfaces across different laboratory equipment poses another challenge, limiting hardware transferability among research communities.Standardizing control languages or systems could enhance collaborative efforts.Although natural language is commonly used to instruct experiments, its ambiguity necessitates sophisticated mapping to specic robotic operations, a task where LLMs could play a transformative role if their reliability is proven in more complex scenarios.</p>
<p>Furthermore, the high-dimensional nature of catalysis design and the chemical consumption in high-throughput processes suggest that automated platforms should be capable of managing varied reaction scales, from small-scale synthesis and characterization to larger-scale production.</p>
<p>As machine learning approaches become more integrated into catalyst design, it is anticipated that they will address increasingly complex design problems.Incorporating scientic hypotheses into the discovery process requires an iterative approach, where hypotheses are generated and modied, and data are queried for validation.AI agents, 227 e.g., ChemCrow 228 equipped with tools for automated experimentation, information retrieval, and machine learning, show promise in bridging these capabilities to create a self-evolving, intelligent system.</p>
<p>Although human feedback should ideally not exist in the process, it can be used for safety checks or as alternative solutions if any of the functions (e.g., automated experimentation) are missing in the toolset, as demonstrated by Omar M. Yaghi et al. 47 In the iteration, the AI agents should be instructed to generate or modify hypotheses together with their validation procedures within the toolset.Later the toolset can be utilized to give feedback to the AI agents for further improvement of the hypotheses via LLMs directly or Bayesian inference.</p>
<p>In conclusion, the last decade's advances have shied the paradigm from traditional methods to a more efficient, systematic approach to experimental design in catalyst research.The integration of LLMs and AI agents promises to further enhance the capability, exibility, and efficiency of these systems, paving the way for a future where intelligent systems can autonomously explore vast chemical spaces and contribute to scientic discovery in unprecedented ways.</p>
<p>Chemical Science</p>
<p>Review</p>
<ol>
<li>1
1
Fig. 2 Scheme of the Markov logic OCSR with low-level image information extraction and probabilistic logic inference.Reproduced with permission from ref. 70 Copyright 2014, American Chemical Society.</li>
</ol>
<p>superior accuracy and speed by extracting chemical structures from PDFs and outputting them in standardized formats, showcasing its efficacy over other OCSR systems like MolVec, OSRA, and Imago.2.1.5.1.4MolScribe.Representing the cutting edge, Mol-Scribe is an image-to-graph generation model 76 that merges neural network capabilities with rule-based methods.It predicts atoms and bonds along with their geometric layouts to construct 2D molecular graphs, applying symbolic chemistry constraints to recognize complex chemical patterns, including chirality and abbreviations.Enhanced by data augmentation strategies, MolScribe effectively handles domain shis and various drawing styles found in chemical literature.Its robustness has been conrmed through testing, showing an accuracy of 76-93% on public benchmarks.</p>
<p>Fig. 3
3
Fig. 3 Overview of the integrated DECIMER workflow including image segmentation, classification, and translation to obtain SMILES.Reproduced with permission from ref. 74 under CC BY license.</p>
<p>Fig. 4
4
Fig. 4 Scheme of the automated chemical reaction extraction from scientific literature.Reproduced with permission from ref. 82 Copyright 2019, American Chemical Society.</p>
<p>Fig. 5
5
Fig. 5 Scheme of the ChatGPT chemistry assistant workflow to extract synthesis information of MOFs from the literature.Reproduced with permission from ref. 16 Copyright 2023, American Chemical Society.</p>
<p>Fig. 6
6
Fig. 6 Autonomous mobile robot and experimental stations.The mobile robotic chemist (a), the roadmap of the whole laboratory (b) and several workstations (c-e) are shown.Reproduced with permission from ref. 4 Copyright 2020, Springer Nature.</p>
<p>Fig. 7
7
Fig. 7 Design of the all-round AI-Chemist with a scientific mind.It includes three modules for chemistry knowledge, autonomous experimentation, and theoretical computation and machine learning (A).The workflow of the AI-Chemist to study various systems are shown in (B).Reproduced with permission from ref. 5 Copyright 2022, China Science Publishing &amp; Media Ltd.</p>
<p>reported a "radial synthesizer" based on a series of continuous ow modules arranged radially around a central switching station, which allows selective access to individual reactors and avoids equipment redundancies and reconguration among different reactions.Storing stable intermediates inside uidic pathways enables simultaneous optimization of subsequent steps during route development.Online monitoring via infrared (IR) and 1 H/ 19 F NMR spectroscopy enables fast post-reaction analysis and feedback.The performance of this system has been demonstrated in transition metal-catalyzed C-C and C-N cross-coupling, olenation, reductive amination, nucleophilic aromatic substitution reactions, lightdriven oxidation-reduction catalysis, and continuous multi-step reactions.In addition, ow selection valve technology can be used to create different process combinations, as demonstrated</p>
<p>Fig. 8
8
Fig. 8 Physical implementation of the synthesis platform Chemputer.The scheme (A) and the actual set-up (B) of the Chemputer are shown respectively.Reproduced with permission from ref. 17 Copyright 2019, AAAS.</p>
<p>Fig. 10
10
Fig. 10 Plug-and-play, reconfigurable, continuous-flow chemical synthesis system.The workflow (A), the design of the flow system (B) and its actual setup (C) with interchangeable modules (D) are shown in the figure.Reproduced with permission from ref. 115 Copyright 2018, AAAS.</p>
<p>Fig. 11 A
11
Fig. 11 A robotically reconfigurable flow chemistry platform.Reproduced with permission from ref. 123 Copyright 2019, AAAS.</p>
<p>Fig. 12
12
Fig. 12 Bayesian optimization of the methanol electro-oxidation process.(a) Peak current density of methanol electro-oxidation as a function of the number of BO rounds.(b) A contour plot showing the peak current density and a ternary plot depicting the chemical composition in the electrolyte solution.Reproduced with permission from ref. 20 Copyright 2020, Royal Chemical Society.</p>
<p>Fig. 15
15
Fig. 15 Workflow for automating theoretical materials discovery.(a) and (b) The experimental workflow for catalyst discovery is accelerated by the ab initio DFT workflow.(c) Scientists relied on their expertise and experimental results to screen data for DFT calculations traditionally.(d) This work uses ML to select DFT data automatically and systematically.Reproduced with permission from ref. 31 Copyright 2018, Springer Nature.</p>
<p>Fig. 16
16
Fig. 16 Screening of CO 2 RR electrocatalysts using an active learning algorithm based on the DFT framework.(a) A two-dimensional activity volcano plot of the CO 2 RR.(b) A two-dimensional selectivity volcano plot of the CO 2 RR.(c) DFT calculations were performed on approximately 4000 adsorption sites of Cu-containing alloys identified by t-SNE.On the right, the Cu-Al clusters are labeled numerically.(d) Representative coordination sites for each cluster are labeled in the t-SNE.Reproduced with permission from ref. 32 Copyright 2020, Springer Nature.</p>
<p>Fig. 17
17
Fig. 17Framework diagram for GPT-4-directed MOF synthesis.The workflow consists of three phases: Reticular ChemScope, Reticular ChemNavigator, and Reticular Executor.The ICL capability of GPT-4 is achieved by combining pre-designed prompt systems with continuous human feedback.Reproduced with permission from ref. 47 Copyright 2021, Wiley.</p>
<p>Fig. 17Framework diagram for GPT-4-directed MOF synthesis.The workflow consists of three phases: Reticular ChemScope, Reticular ChemNavigator, and Reticular Executor.The ICL capability of GPT-4 is achieved by combining pre-designed prompt systems with continuous human feedback.Reproduced with permission from ref. 47 Copyright 2021, Wiley.</p>
<p>Table 1 Comparison of methods for information extraction
1MethodTypeExtracted contentSupported modalityOpen sourceReferenceCLiDERule-basedMolecular structures and chargeText &amp; imageYes66OSRARule-basedMolecular structuresText &amp; imageYes68ImagoRule-basedDepicted molecules with up and downText &amp; imageYes69stereo bonds and pseudostemsMSE-DUDLML-basedStructures of natural products andImageNo71peptide sequencesDECIMERML-basedChemical classes, species, organismImageYes72parts, and spectral dataMolMinerML-basedMolecule structuresImageNo75ChemDataExtractorLM-basedIdentiers, spectroscopic attributes, andTextYes80 and 81chemical property attributes (e.g.,melting point, oxidation/reductionpotentials, photoluminescence lifetime,and quantum yield)SciBERTLM-basedIdentiers of chemicalsTextYes11ChemRxnExtractorLM-basedReactants, catalysts, and solvents forTextYes82reactionsGPT-3.5LM-basedMOF synthesisTextNo16GPT-4LM-basedText &amp; imageNo</p>
<p>Table 2
2
The comparison of methods for robotic systems in chemistry.The systems are categorized into three types: humanoid, flow, or a mixture of both.The supported operations, characterization and originally studied chemical systems are listed in the table
TypeDescriptionSynthesis operationsCharacterizationTarget compoundsReferenceHumanoidMobile roboticSolid dispensing, liquidGas chromatographyCatalysts for photolysis of4chemistdispensing, capping/water to produce hydrogenuncapping, heating, andsonicationAn all-round AI-Solid dispensing, liquidUV-vis, uorescence,Materials for5Chemistdispensing, magneticand Ramanelectrocatalysts,stirring, sonication, drying,spectroscopy, and gasphotocatalysts, andcentrifugation, and liquidchromatographyluminescenceextractionA-lab, anPowder dosing and sampleX-ray diffraction (XRD)Primarily oxides and6autonomousheatingphosphates identiedlaboratorythrough extensive ab initiophase-stability dataFlow: BatchModular roboticMixing, ltration, liquid--Organic molecules17reactorssynthesis systemliquid separation,evaporation, andchromatographicseparationA portable suitcase-Liquid transfer,-Organic molecules49sized chemicaltemperature control,synthesis platformevaporation, ltration, andseparationFast screeningLiquid handling, electricMicro-fast gasElectrocatalysts for the106platform for thecell preparation, andchromatographyCO 2 RRCO 2 RRelectrolysisFlow: continuousBenchtop owLiquid handling, heating,High-performanceRecongurable system for115ow reactorschemistry platformcooling, photoreaction,liquid chromatographyautomated optimization ofextraction and purication(HPLC), IRdiverse chemical reactionsspectroscopy, Ramanspectroscopy, and massspectrometryRadial synthesizerLiquid transfer, mixing,IR spectrometry andCross-coupling,116systemand dilutionnuclear magneticolenation, reductiveresonanceamination, nucleophilicaromatic substitutionreactions, light-drivenredox catalysis, andcontinuous multi-stepreactionsAn automatedHeating, liquid-liquidLiquidPharmaceutical small117multistep chemicalseparation, gas-liquidchromatography-massmoleculessynthesizerseparation, andspectrometry (LC-MS)heterogeneous catalysisHumanoid roboticA robotic platformLiquid handling,High-performanceOrganic molecules123system with owfor ow synthesis ofseparation, andliquid chromatographyreactorsorganic compoundstemperature adjustmentand nuclear magneticresonance</p>
<p>Table 3
3
The comparison of active machine learning algorithms in chemistry.The algorithms rely on many surrogate models from the Gaussian process to the recent LLMs.Targets of the surrogate models are listed corresponding to the different research systems
TypeSurrogate modelsVariables (input)Target (output)Research systemsReferenceBayesianRandom forest andRatio of a metal precursorCurrent densityElectrocatalytic oxidation20optimizationGaussian process(continuous)of methanolGaussian processReagent concentration forHydrogen evolutionPhotocatalytic hydrogen4catalyst synthesisrategeneration(continuous)Gaussian processAlloy compositionsCurrent densityElectrocatalytic O 222(continuous)reductionLarge language modelsExperimental procedure asC 2 yieldOxidative coupling of45from open AItextmethaneActiveArticial neuralElectronegativity and d-<em>CO binding energyElectrocatalytic CO 230learningnetworks and kernelband width of alloys(</em>CO refers to adsorbedreductionridge regression(continuous)CO on a solid surface)Extra tree regressor,Fingerprints of the surfaceAdsorption energies ofElectrocatalytic CO 231random forest,and sites of intermetallicsCO and Hreduction and H 2Gaussian process,etc.(discrete)evolutionRandom forest andFingerprints of adsorptionCO adsorption energyElectrocatalytic CO 232boosted treesites from copper-reductioncontaining metals(discrete)GPT-4Synthesis procedure as textSuccess or failure of theMOF synthesis47inputsynthesis
© 2024 The Author(s).Published by the Royal Society of Chemistry Chem.Sci., 2024, 15, 12200-12233 | 12219</p>
<p>© 2024 The Author(s). Published by the Royal Society of Chemistry
© 2024 The Author(s). Published by the Royal Society of Chemistry Chem. Sci., 2024, 15, 12200-12233 | 12217
AcknowledgementsWe acknowledge the nancial support from the National Key R&amp;D Program of China (2021YFA1502500), the National Natural Science Foundation of China (22125502, 22071207, and 22121001), and the Fundamental Research Funds for the Central Universities (No. 20720220011).Notes and referencesData availabilityNo primary research results, soware or code have been included and no new data were generated or analysed as part of this review.Author contributionsAll the authors wrote the review together.Chemical Science ReviewConflicts of interestThe authors declare no conict of interest.
J Gasteiger, Chemistry in times of articial intelligence. 202021</p>
<p>Machine learning in materials science: Recent progress and emerging applications. T Mueller, A G Kusne, R Ramprasad, Rev. Comput. Chem. 292016</p>
<p>Machine-learning-assisted materials discovery using failed experiments. P Raccuglia, K C Elbert, P D Adler, C Falk, M B Wenny, A Mollo, M Zeller, S A Friedler, J Schrier, A J Norquist, Nature. 5332016</p>
<p>B Burger, P M Maffettone, V V Gusev, C M Aitchison, Y Bai, X Wang, X Li, B M Alston, B Li, R Clowes, N Rankin, B Harris, R S Sprick, A I Cooper, A mobile robotic chemist. 2020583</p>
<p>An all-round AI-Chemist with a scientic mind. Q Zhu, F Zhang, Y Huang, H Xiao, L Zhao, X Zhang, T Song, X Tang, X Li, G He, B Chong, J Zhou, Y Zhang, B Zhang, J Cao, M Luo, S Wang, G Ye, W Zhang, X Chen, S Cong, D Zhou, H Li, J Li, G Zou, W Shang, J Jiang, Y Luo, Natl. Sci. Rev. 1902022</p>
<p>An autonomous laboratory for the accelerated synthesis of novel materials. N J Szymanski, B Rendy, Y Fei, R E Kumar, T He, D Milsted, M J Mcdermott, M Gallant, E D Cubuk, A Merchant, H Kim, A Jain, C J Bartel, K Persson, Y Zeng, G Ceder, Nature. 6242023</p>
<p>A review of optical chemical structure recognition tools. K Rajan, H O Brinkhaus, A Zielesny, C Steinbeck, J. Cheminf. 602020</p>
<p>ImageDataExtractor: A Tool To Extract and Quantify Data from Microscopy Images. K T Mukaddem, E J Beard, B Yildirim, J M Cole, J. Chem. Inf. Model. 602020</p>
<p>ChemSchematicResolver: a toolkit to decode 2D chemical diagrams with labels and R-groups into annotated chemical named entities. E J Beard, J M Cole, J. Chem. Inf. Model. 602020</p>
<p>Review of techniques and models used in optical chemical structure recognition in images and scanned documents. F Musazade, N Jamalova, J Hasanov, J. Cheminf. 612022</p>
<p>I Beltagy, K Lo, A Cohan, 10.48550/arXiv.1903.10676arXiv:1903.10676SciBERT: A Pretrained Language Model for Scientic Text, arXiv, 2019, preprint. </p>
<p>A text mining framework for screening catalysts and critical process parameters from scientic literature -A study on Hydrogen production from alcohol. A Kumar, S Ganesh, D Gupta, H Kodamana, Chem. Eng. Res. Des. 1842022</p>
<p>A corpus of CO2 electrocatalytic reduction process extracted from the scientic literature. L Wang, Y Gao, X Chen, W Cui, Y Zhou, X Luo, S Xu, Y Du, B Wang, Sci. Data. 1752023</p>
<p>Revisiting Electrocatalyst Design by a Knowledge Graph of Cu-Based Catalysts for CO2 Reduction. Y Gao, L Wang, X Chen, Y Du, B Wang, ACS Catal. 132023</p>
<p>Is GPT-3 all you need for low-data discovery in chemistry?. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, 10.26434/chemrxiv-2023-fw8n4ChemRxiv. 2023preprint</p>
<p>ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis. Z Zheng, O Zhang, C Borgs, J T Chayes, O M Yaghi, J. Am. Chem. Soc. 1452023</p>
<p>Organic synthesis in a modular robotic system driven by a chemical programming language. S Steiner, J Wolf, S Glatzel, A Andreou, J M Granda, G Keenan, T Hinkley, G Aragon-Camarasa, P J Kitson, D Angelone, L Cronin, Science. 22112019</p>
<p>A universal system for digitization and automatic execution of the chemical synthesis literature. S H M Mehr, M Craven, A I Leonov, G Keenan, L Cronin, Science. 3702020</p>
<p>Digitization and validation of a chemical synthesis literature database in the ChemPU. S Rohrbach, M Šiaučiulis, G Chisholm, P.-A Pirvan, M Saleeb, S H M Mehr, E Trushina, A I Leonov, G Keenan, A Khan, A Hammer, L Cronin, Science. 3772022</p>
<p>Mesoporous trimetallic PtPdAu alloy lms toward enhanced electrocatalytic activity in methanol oxidation: unexpected chemical compositions discovered by Bayesian optimization. A S Nugraha, G Lambard, J Na, M S A Hossain, T Asahi, W Chaikittisilp, Y Yamauchi, J. Mater. Chem. A. 82020</p>
<p>A Bayesian experimental autonomous researcher for mechanical design. A E Gongora, B Xu, W Perry, C Okoye, P Riley, K G Reyes, E F Morgan, K A Brown, Sci. Adv. 2020, 6, eaaz1708</p>
<p>Bayesian Optimization of High-Entropy Alloy Compositions for Electrocatalytic Oxygen Reduction. J K Pedersen, C M Clausen, O A Krysiak, B Xiao, T A A Batchelor, T Löffler, V A Mints, L Banko, M Arenz, A Savan, W Schuhmann, A Ludwig, J Rossmeisl, Angew. Chem., Int. Ed. 602021</p>
<p>Bayesian optimization of single-atom alloys and other bimetallics: © 2024 The Author(s). G O Kayode, A F Hill, M M Montemore, Royal Society of Chemistry Chem. Sci. 152024. 12225Published by the</p>
<p>Review Chemical Science efficient screening for alkane transformations, CO2 reduction, and hydrogen evolution. J. Mater. Chem. A. 112023</p>
<p>Bayesian-optimization-assisted discovery of stereoselective aluminum complexes for ring-opening polymerization of racemic lactide. X Wang, Y Huang, X Xie, Y Liu, Z Huo, M Lin, H Xin, R Tong, Nat. Commun. 36472023</p>
<p>Composition-Designed Multielement Perovskite Oxides for Oxygen Evolution Catalysis. Y Okazaki, Y Fujita, H Murata, N Masuyama, Y Nojima, H Ikeno, S Yagi, I Yamada, Chem. Mater. 342022</p>
<p>Descriptor-Free Design of Multicomponent Catalysts. Y Zhang, T C Peck, G K Reddy, D Banerjee, H Jia, C A Roberts, C Ling, ACS Catal. 122022</p>
<p>Exploring the Composition Space of High-Entropy Alloy Nanoparticles for the Electrocatalytic H2/CO Oxidation with Bayesian Optimization. V A Mints, J K Pedersen, A Bagger, J Quinson, A S Anker, K M Ø Jensen, J Rossmeisl, M Arenz, ACS Catal. 122022</p>
<p>Benchmarking the performance of Bayesian optimization across multiple experimental materials science domains. Q Liang, A E Gongora, Z Ren, A Tiihonen, Z Liu, S Sun, J R Deneault, D Bash, F Mekki-Berrada, S A Khan, K Hippalgaonkar, B Maruyama, K A Brown, J Fisher Iii, T Buonassisi, Comput. Mater. 1882021</p>
<p>Active learning-based exploration of the catalytic pyrolysis of plastic waste. Y Ureel, M R Dobbelaere, O Akin, R J Varghese, C G Pernalete, J W Thybaut, K M Van Geem, Fuel. 1253402022</p>
<p>Active learning with nonab initio input features toward efficient CO2 reduction catalysts. J Noh, S Back, J Kim, Y Jung, Chem. Sci. 92018</p>
<p>Active learning across intermetallics to guide discovery of electrocatalysts for CO2 reduction and H2 evolution. K Tran, Z W Ulissi, Nat. Catal. 12018</p>
<p>Accelerated discovery of CO2 electrocatalysts using active machine learning. M Zhong, K Tran, Y Min, C Wang, Z Wang, C.-T Dinh, P De Luna, Z Yu, A S Rasouli, P Brodersen, S Sun, O Voznyy, C.-S Tan, M Askerka, F Che, M Liu, A Seitokaldani, Y Pang, S.-C Lo, A Ip, Z Ulissi, E H Sargent, Nature. 5812020</p>
<p>Exploring Optimal Water Splitting Bifunctional Alloy Catalyst by Pareto Active Learning. M Kim, Y Kim, M Y Ha, E Shin, S J Kwak, M Park, I.-D Kim, W.-B Jung, W B Lee, Y Kim, H.-T Jung, Adv. Mater. 22114972023</p>
<p>Searching for an Optimal Multi-Metallic Alloy Catalyst by Active Learning Combined with Experiments. M Kim, M Y Ha, W.-B Jung, J Yoon, E Shin, I -D. Kim, W B Lee, Y Kim, H.-T Jung, Adv. Mater. 21089002022</p>
<p>Unsupervised word embeddings capture latent knowledge from materials science literature. V Tshitoyan, J Dagdelen, L Weston, A Dunn, Z Rong, O Kononova, K A Persson, G Ceder, A Jain, Nature. 5712019</p>
<p>Recent advances in knowledge discovery for heterogeneous catalysis using machine learning. M Erdem Günay, R Yıldırım, Catal. Rev. 632021</p>
<p>T Toyao, Z Maeno, S Takakusagi, T Kamachi, I Takigawa, K.-I Shimizu, Machine Learning for Catalysis Informatics: Recent Applications and Prospects. 201910</p>
<p>Bridging the complexity gap in computational heterogeneous catalysis with machine learning. T Mou, H S Pillai, S Wang, M Wan, X Han, N M Schweitzer, F Che, H Xin, Nat. Catal. 62023</p>
<p>Extracting Knowledge from Data through Catalysis Informatics. A J Medford, M R Kunz, S M Ewing, T Borders, R Fushimi, ACS Catal. 82018</p>
<p>The rise of self-driving labs in chemical and materials sciences. M Abolhasani, E Kumacheva, Nat. Synth. 22023</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, P Liu, J.-Y Nie, J.-R Wen, 10.48550/arXiv.2303.18223arXiv:2303.18223A Survey of Large Language Models. 2023preprint</p>
<p>Large language models for chemistry robotics. N Yoshikawa, M Skreta, K Darvish, S Arellano-Rubach, Z Ji, L Bjørn Kristensen, A Z Li, Y Zhao, H Xu, A Kuramshin, A Aspuru-Guzik, F Shkurti, A Garg, Auton. Robots. 472023</p>
<p>LLMs can generate robotic scripts from goal-oriented instructions in biological laboratory automation, arXiv. T Inagaki, A Kato, K Takahashi, H Ozaki, G N Kanda, 10.48550/arXiv.2304.10267arXiv:2304.102672023preprint</p>
<p>C Singh, J X Morris, J Aneja, A M Rush, J Gao, 10.48550/arXiv.2210.01848arXiv:2210.01848Explaining Patterns in Data with Language Models via Interpretable Autoprompting, arXiv, 2022, preprint. </p>
<p>M Caldas Ramos, S S Michtavy, M D Porosoff, A D White, 10.48550/arXiv.2304.05341arXiv:2304.05341Bayesian Optimization of Catalysts With Incontext Learning. arXiv, 2023, preprint</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 6242023</p>
<p>A GPT-4 Reticular Chemist for Guiding MOF Discovery. Z Zheng, Z Rong, N Rampal, C Borgs, J T Chayes, O M Yaghi, Angew. Chem., Int. Ed. e2023119832023</p>
<p>A modular programmable inorganic cluster discovery robot for the discovery and synthesis of polyoxometalates. D S Salley, G A Keenan, D.-L Long, N L Bell, L Cronin, ACS Cent. Sci. 62020</p>
<p>An autonomous portable platform for universal chemical synthesis. J S Manzano, W Hou, S S Zalesskiy, P Frei, H Wang, P J Kitson, L Cronin, Nat. Chem. 142022</p>
<p>Laboratory automation. 1: Syntheses via vinyl sulfones. 14. Robotic orchestration of organic reactions: Yield optimization via an automated system with operator-specied reaction sequences. A R Frisbee, M H Nantz, G W Kramer, P L Fuchs, J. Am. Chem. Soc. 1061984</p>
<p>Selfdriving laboratory for accelerated discovery of thin-lm materials. B P Macleod, F G L Parlane, T D Morrissey, F Hse, L M Roch, K E Dettelbach, R Moreira, L P E Yunker, M B Rooney, J R Deeth, V Lai, G J Ng, H Situ, R H Zhang, M S Elliott, T H Haley, D J Dvorak, A Aspuru-Guzik, J E Hein, C P Berlinguette, Sci. Adv. 2020, 6, eaaz8867</p>
<p>An Introductory Review of Deep Learning for Prediction Models With Big Data, Front. F Emmert-Streib, Z Yang, H Feng, S Tripathi, M Dehmer, 10.3389/frai.2020.00004Artif. Intell. 32020</p>
<p>S Lundberg, S.-I Lee, 10.48550/arXiv.1705.07874A Unied Approach to Interpreting Model Predictions. 2017preprint</p>
<p>Reaxys. May 12, 2021</p>
<p>SciFinder. May 12, 2021</p>
<p>Accurate prediction of molecular properties and drug targets using a self-supervised image representation learning framework. X Zeng, H Xiang, L Yu, J Wang, K Li, R Nussinov, F Cheng, Nat. Mach. Intell. 42022</p>
<p>Automatic processing of graphics for image databases in science. R Rozas, H Fernandez, J. Chem. Inf. Comput. Sci. 301990</p>
<p>Chemical structure recognition (CSR) system: automatic analysis of 2D chemical structures in document images. S S Bukhari, Z Iikhar, A Dengel, 2019 International Conference on Document Analysis and Recognition (ICDAR). 2019presented in part</p>
<p>A component-detection-based approach for interpreting off-line handwritten chemical cyclic compound structures. Y Wang, T Zhang, X Yu, 2021 IEEE International Conference on Engineering. 2021presented in part at the</p>
<p>Kekule: OCR-optical chemical (structure) recognition. J R Mcdaniel, J R Balmuth, J. Chem. Inf. Comput. Sci. 321992</p>
<p>Chemical structure recognition: a rule-based approach presented in part at the Document Recognition and Retrieval XIX. N M Sadawi, A P Sexton, V Sorge, 2012</p>
<p>Robust method of segmentation and recognition of chemical structure images in cheminy presented in part at the Pre-proceedings of the 9th IAPR international workshop on graphics recognition. A Fujiyoshi, K Nakagawa, M Suzuki, 2011GREC</p>
<p>Research on chemical expression images recognition. C Hong, X Du, L Zhang, Joint International Mechanical, Electronic and Information Technology Conference (JIMET-15). 2015. 2015presented in part at the</p>
<p>Automated extraction of chemical structure information from digital raster images. J Park, G R Rosania, K A Shedden, M Nguyen, N Lyu, K Saitou, Chem. Cent. J. 2009, 3, 4</p>
<p>Optical recognition of chemical graphics. R Casey, S Boyer, P Healey, A Miller, B Oudot, K Zilles, Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR'93). 2nd International Conference on Document Analysis and Recognition (ICDAR'93)1993presented in part at the</p>
<p>Chemical literature data extraction: the CLiDE Project. P Ibison, M Jacquot, F Kam, A Neville, R W Simpson, C Tonnelier, T Venczel, A P Johnson, J. Chem. Inf. Comput. Sci. 331993</p>
<p>CLiDE Pro: the latest generation of CLiDE, a tool for optical chemical structure recognition. A T Valko, A P Johnson, J. Chem. Inf. Model. 492009</p>
<p>Optical structure recognition soware to recover chemical information: OSRA, an open source solution. I V Filippov, M C Nicklaus, J. Chem. Inf. Model. 492009</p>
<p>V Smolov, F Zentsev, M Rybalkin, Imago: Open-Source Toolkit for 2D Chemical Structure Image. 2011Recognition presented in part at the TREC</p>
<p>Markov logic networks for optical chemical structure recognition. P Frasconi, F Gabbrielli, M Lippi, S Marinai, J. Chem. Inf. Model. 542014</p>
<p>Molecular Structure Extraction from Documents Using Deep Learning. J Staker, K Marshall, R Abel, C M Mcquaw, J. Chem. Inf. Model. 592019</p>
<p>DECIMER: towards deep learning for chemical image recognition. K Rajan, A Zielesny, C Steinbeck, J. Cheminf. 652020</p>
<p>DECIMER-Segmentation: Automated extraction of chemical structure depictions from scientic literature. K Rajan, H O Brinkhaus, M Sorokina, A Zielesny, C Steinbeck, J. Cheminf. 202021</p>
<p>DECIMER. ai-An open platform for automated optical chemical structure identication, segmentation and recognition in scientic publications. K Rajan, H O Brinkhaus, M I Agea, A Zielesny, C Steinbeck, Nat. Commun. 50452023</p>
<p>MolMiner: You only look once for chemical structure recognition. Y Xu, J Xiao, C.-H Chou, J Zhang, J Zhu, Q Hu, H Li, N Han, B Liu, S Zhang, J Han, Z Zhang, S Zhang, W Zhang, L Lai, J Pei, J. Chem. Inf. Model. 622022</p>
<p>MolScribe: Robust Molecular Structure Recognition with Image-to-Graph Generation. Y Qian, J Guo, Z Tu, Z Li, C W Coley, R Barzilay, J. Chem. Inf. Model. 632023</p>
<p>LSTMVoter: chemical named entity recognition using a conglomerate of sequence labeling tools. W Hemati, A Mehler, J. Cheminf. 32019</p>
<p>Graph Convolutional Networks for Chemical Relation Extraction. D Mahendran, C Tang, B T Mcinnes, Companion Proceedings of the Web Conference 2022, Virtual Event. Lyon, France2022</p>
<p>Language models and protocol standardization guidelines for accelerating synthesis planning in heterogeneous catalysis. M Suvarna, A C Vaucher, S Mitchell, T Laino, J Pérez-Ramírez, Nat. Commun. 79642023</p>
<p>ChemDataExtractor: A Toolkit for Automated Extraction of Chemical Information from the Scientic Literature. M C Swain, J M Cole, J. Chem. Inf. Model. 562016</p>
<p>ChemDataExtractor 2.0: Autopopulated Ontologies for Materials Science. J Mavračić, C J Court, T Isazawa, S R Elliott, J M Cole, J. Chem. Inf. Model. 612021</p>
<p>Automated chemical reaction extraction from scientic literature. J Guo, A S Ibanez-Lopez, H Gao, V Quach, C W Coley, K F Jensen, R Barzilay, J. Chem. Inf. Model. 622021</p>
<p>Automated peptide synthesis. R B Merrield, J M Stewart, Nature. 2071965</p>
<p>Automation and optimization by simplex methods of 6-chlorohexanol synthesis. C Porte, W Debreuille, F Draskovic, A Delacroix, Process Control Qual. 41996</p>
<p>Investigation of cocatalysis conditions using an automated microscale multireactor workstation: Synthesis of m esotetramesitylporphyrin. R W Wagner, F Li, H Du, J S Lindsey, Org. Process Res. Dev. 31999</p>
<p>A retrospective on the automation of laboratory synthetic chemistry. J S Lindsey, Chemom. Intell. Lab. Syst. 171992</p>
<p>Instrument for automated synthesis of peptides. R B Merrield, J M Stewart, N Jernberg, Anal. Chem. 381966</p>
<p>Automated synthesis of gene fragments. G Alvarado-Urbina, G M Sathe, W.-C Liu, M F Gillen, P D Duck, R Bender, K K Ogilvie, Science. 2141981</p>
<p>Automation on the laboratory bench. M Legrand, A Foucard, J. Chem. Educ. 7671978</p>
<p>Integration of virtual and high-throughput screening. J Bajorath, Nat. Rev. Drug Discovery. 12002</p>
<p>In silico design of porous polymer networks: highthroughput screening for methane storage materials. R L Martin, C M Simon, B Smit, M Haranczyk, J. Am. Chem. Soc. 1362014</p>
<p>Impact of high-throughput screening in biomedical research. R Macarron, M N Banks, D Bojanic, D J Burns, D A Cirovic, T Garyantes, D V S Green, R P Hertzberg, W P Janzen, J W Paslay, U Schopfer, G Sitta Sittampalam, Nat. Rev. Drug Discovery. 102011</p>
<p>High-throughput screening of solid-state catalyst libraries. S M Senkan, Nature. 3941998</p>
<p>. X D Xiang, X Sun, G Briceno, Y Lou, K.-A Wang, H Chang, W G Wallace-Freedman, S.-W Chen, P G Schultz, A combinatorial approach to materials discovery. 2681995Science</p>
<p>Combinatorial and highthroughput materials science. W F Maier, K Stoewe, S Sieg, Angew. Chem., Int. Ed. 462007</p>
<p>Accelerating electrolyte discovery for energy storage with high-throughput screening. L Cheng, R S Assary, X Qu, A Jain, S P Ong, N N Rajput, K Persson, L A Curtiss, J. Phys. Chem. Lett. 62015</p>
<p>Less is more: Sampling chemical space with active learning. J S Smith, B Nebgen, N Lubbers, O Isayev, A E Roitberg, 10.1063/1.5023802J. Chem. Phys. 1482018</p>
<p>Automated synthesis of oxygen-producing catalysts from Martian meteorites by a robotic AI chemist. Q Zhu, Y Huang, D Zhou, L Zhao, L Guo, R Yang, Z Sun, M Luo, F Zhang, H Xiao, X Tang, X Zhang, T Song, X Li, B Chong, J Zhou, Y Zhang, B Zhang, J Cao, G Zhang, S Wang, G Ye, W Zhang, H Zhao, S Cong, H Li, L.-L Ling, Z Zhang, W Shang, J Jiang, Y Luo, Nat. Synth. 32024</p>
<p>Ondemand continuous-ow production of pharmaceuticals in a compact, recongurable system. A Adamo, R L Beingessner, M Behnam, J Chen, T F Jamison, K F Jensen, J.-C M Monbaliu, A S Myerson, E M Revalor, D R Snead, T Stelzer, N Weeranoppanant, S Y Wong, P Zhang, Science. 3522016</p>
<p>The synthesis of active pharmaceutical ingredients (APIs) using continuous ow chemistry. M Baumann, I R Baxendale, Beilstein J. Org. Chem. 112015</p>
<p>Flow chemistrya key enabling technology for (multistep) organic synthesis. J Wegner, S Ceylan, A Kirschning, Adv. Synth. Catal. 3542012</p>
<p>Flow "ne" synthesis: high yielding and selective organic synthesis by ow methods. S Kobayashi, Chem.-Asian J. 112016</p>
<p>Dynamic ow synthesis of porous organic cages. M E Briggs, A G Slater, N Lunt, S Jiang, M A Little, R L Greenaway, T Hasell, C Battilocchio, S V Ley, A I Cooper, Chem. Commun. 512015</p>
<p>Continuous ow techniques in organic synthesis. G Jas, A Kirschning, Chem. -Eur. J. 92003</p>
<p>Convergence of multiple synthetic paradigms in a universally programmable chemical synthesis machine. D Angelone, A J S Hammer, S Rohrbach, S Krambeck, J M Granda, J Wolf, S Zalesskiy, G Chisholm, L Cronin, Nat. Chem. 132021</p>
<p>Fast Screening for Copper-Based Bimetallic Electrocatalysts: Efficient Electrocatalytic Reduction of CO 2 to C 2+ Products on Magnesium-Modied Copper. M Xie, Y Shen, W Ma, D Wei, B Zhang, Z Wang, Y Wang, Q Zhang, S Xie, C Wang, Y Wang, Angew. Chem., Int. Ed. e2022134232022</p>
<p>The hitchhiker's guide to ow chemistry. M B Plutschack, B U Pieber, K Gilmore, P H Seeberger, Chem. Rev. 1172017</p>
<p>A high-throughput synthesis of 1, 2, 4-oxadiazole and 1, 2, 4-triazole libraries in a continuous ow reactor. A R Bogdan, Y Wang, RSC Adv. 52015</p>
<p>A simple milliuidic benchtop reactor system for the high-throughput synthesis and functionalization of gold nanoparticles with different sizes and shapes. S E Lohse, J R Eller, S T Sivapalan, M R Plews, C J Murphy, ACS Nano. 72013</p>
<p>High-throughput continuous ow synthesis of nickel nanoparticles for the catalytic hydrodeoxygenation of guaiacol. E J Roberts, S E Habas, L Wang, D A Ruddy, E A White, F G Baddour, M B Griffin, J A Schaidle, N Malmstadt, R L Brutchey, ACS Sustain. Chem. Eng. 52017</p>
<p>High-throughput synthesis of lignin particles (∼30 nm to ∼2 mm) via aerosol ow reactor: Size fractionation and utilization in pickering emulsions. M Ago, S Huan, M Borghei, J Raula, E I Kauppinen, O J Rojas, ACS Appl. Mater. Interfaces. 82016</p>
<p>Reproducible, highthroughput synthesis of colloidal nanocrystals for optimization in multidimensional parameter space. E M Chan, C Xu, A W Mao, G Han, J S Owen, B E Cohen, D J Milliron, Nano Lett. 102010</p>
<p>Electrochemical aromatic C-H hydroxylation in continuous ow. H Long, T.-S Chen, J Song, S Zhu, H.-C Xu, Nat. Commun. 39452022</p>
<p>Flow electrochemistry: a safe tool for uorine chemistry. B Winterson, T Rennigholtz, T Wirth, Chem. Sci. 122021</p>
<p>Recongurable system for automated optimization of diverse chemical reactions. A.-C Bédard, A Adamo, K C Aroh, M G Russell, A A Bedermann, J Torosian, B Yue, K F Jensen, T F Jamison, Science. 3612018</p>
<p>Automated radial synthesis of organic molecules. S Chatterjee, M Guidi, P H Seeberger, K Gilmore, Nature. 5792020</p>
<p>Fully automated chemical synthesis: toward the universal synthesizer. N Collins, D Stout, J.-P Lim, J P Malerich, J D White, P B Madrid, M Latendresse, D Krieger, J Szeto, V.-A Vu, Org. Process Res. Dev. 242020</p>
<p>Integrated plug ow synthesis and crystallisation of pyrazinamide. C D Scott, R Labes, M Depardieu, C Battilocchio, M G Davidson, S V Ley, C C Wilson, K Robertson, React. Chem. Eng. 32018</p>
<p>A droplet-reactor system capable of automation for the continuous and scalable production of noble-metal nanocrystals. G Niu, L Zhang, A Ruditskiy, L Wang, Y Xia, Nano Lett. 182018</p>
<p>Continuous ow droplet-based crystallization platform for producing spherical drug microparticles. E W Yeap, D Z Ng, D Lai, D J Ertl, S Sharpe, S A Khan, Org. Process Res. Dev. 232018</p>
<p>The coupling of in-ow reaction with continuous ow seedless tubular crystallization. B Rimez, J Septavaux, B Scheid, React. Chem. Eng. 42019</p>
<p>O Okafor, K Robertson, R Goodridge, V Sans, Continuous-ow crystallisation in 3D-printed compact devices. 20194</p>
<p>A robotic platform for ow synthesis of organic compounds informed by AI planning. C W Coley, D A Thomas, Iii , J A M Lummiss, J N Jaworski, C P Breen, V Schultz, T Hart, J S Fishman, L Rogers, H Gao, R W Hicklin, P P Plehiers, J Byington, J S Piotti, W H Green, A J Hart, T F Jamison, K F Jensen, Science. 15662019</p>
<p>B A Koscher, R B Canty, M A Mcdonald, K P Greenman, C J Mcgill, C L Bilodeau, W Jin, H Wu, F H Vermeire, B Jin, T Hart, T Kulesza, S.-C Li, T S Jaakkola, R Barzilay, R Gómez-Bombarelli, W H Green, K F Jensen, Autonomous, multiproperty-driven molecular discovery: From predictions to measurements and back. 20231407</p>
<p>How can LLMs transform the robotic design process?. F Stella, C Della Santina, J Hughes, Nat. Mach. Intell. 2023</p>
<p>Chatgpt for robotics: Design principles and model abilities. S Vemprala, R Bonatti, A Bucker, A Kapoor, Microso Auton. Syst. Robot. Res. 2023, 2, 20</p>
<p>L Wang, Y Ling, Z Yuan, M Shridhar, C Bao, Y Qin, B Wang, H Xu, X Wang, 10.48550/arXiv.2310.01361arXiv:2310.01361Generating Robotic Simulation Tasks via Large Language Models, arXiv, 2023, preprint. GenSim</p>
<p>Organic synthesis in a modular robotic system driven by a chemical programming language. S Steiner, J Wolf, S Glatzel, A Andreou, J M Granda, G Keenan, T Hinkley, G Aragon-Camarasa, P J Kitson, D Angelone, L Cronin, Science. 22112019</p>
<p>A literature survey of active machine learning in the context of natural language processing. F Olsson, Report. )110031542009Swedish Institute of Computer Science</p>
<p>Y Ureel, M R Dobbelaere, Y Ouyang, K De Ras, M K Sabbe, G B Marin, K M Van Geem, Active Machine Learning for Chemical Engineers: A Bright Future Lies Ahead, Engineering. 202327</p>
<p>. X Wang, Y Jin, S Schmitt, M Olhofer, ACM Comput. Surv. 2872023Recent Advances in Bayesian Optimization</p>
<p>Optimisation of water demand forecasting by articial intelligence with short data sets. R González Perea, E Camacho Poyato, P Montesinos, J A Rodríguez Díaz, Biosyst. Eng. 1772019</p>
<p>Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks. J M Hernández-Lobato, R P Adams, International Conference on Machine Learning. 2015presented in part</p>
<p>Practical variational inference for neural networks. A Graves, Proceedings of the 24th International Conference on Neural Information Processing Systems. the 24th International Conference on Neural Information Processing SystemsGranada, Spain2011presented in part at the</p>
<p>. F Häse, L M Roch, C Kreisbeck, A Aspuru-Guzik, Phoenics: A Bayesian Optimizer for Chemistry, ACS Cent. Sci. 42018</p>
<p>Gryffin: An algorithm for Bayesian optimization of categorical variables informed by expert knowledge. F Häse, M Aldeghi, R J Hickman, L M Roch, A Aspuru-Guzik, 10.1063/5.0048164Applied Physics Reviews. 82021</p>
<p>J Močkus, 10.1007/978-3-662-38527-2_55Optimization Techniques IFIP Technical Conference: Novosibirsk. G I Marchuk, Berlin Heidelberg, Berlin, HeidelbergSpringerJuly 1-7, 1974. 1975</p>
<p>Optimization of one-dimensional multimodal functions. A Zilinskas, J. R. Stat. Soc., C: Appl. Stat. 271978</p>
<p>A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise. H J Kushner, J. Basic Eng. 861964</p>
<p>Using condence bounds for exploitationexploration trade-offs. P Auer, J. Mach. Learn. Res. 32003</p>
<p>Counting biomolecules with Bayesian inference. J.-B Masson, Nat. Comput. Sci. 22022</p>
<p>Active learning for accelerated design of layered materials. L Bassman Oelie, P Rajak, R K Kalia, A Nakano, F Sha, J Sun, D J Singh, M Aykol, P Huck, K Persson, P Vashishta, Comput. Mater. 742018</p>
<p>Bias free multiobjective active learning for materials design and discovery. K M Jablonka, G M Jothiappan, S Wang, B Smit, B Yoo, Nat. Commun. 23122021</p>
<p>Iterative experimental design based on active machine learning reduces the experimental burden associated with reaction screening. N S Eyke, W H Green, K F Jensen, React. Chem. Eng. 52020</p>
<p>Using Active Learning to Develop Machine Learning Models for Reaction Yield Prediction. S Viet Johansson, H Gummesson Svensson, E Bjerrum, A Schliep, M Haghir, C Chehreghani, O Tyrchan, Engkvist, Mol. Inf. e22000432022</p>
<p>Active learning for computational chemogenomics. D Reker, P Schneider, G Schneider, J B Brown, Future Med. Chem. 92017</p>
<p>Active-learning strategies in computer-assisted drug discovery. D Reker, G Schneider, Drug Discovery Today. 202015</p>
<p>Kernel ridge regression with active learning for wind speed prediction. F Douak, F Melgani, N Benoudjit, Appl. Energy. 1032013</p>
<p>Evidence-based uncertainty sampling for active learning. M Sharma, M Bilgic, Data Min. Knowl. Discov. 312017</p>
<p>A two-stage regression approach for spectroscopic quantitative analysis. F Douak, N Benoudjit, F Melgani, Chemom. Intell. Lab. Syst. 1092011</p>
<p>Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. A Jain, S P Ong, G Hautier, W Chen, W D Richards, S Dacek, S Cholia, D Gunter, D Skinner, G Ceder, K A Persson, APL Mater. 2013, 1, 011002</p>
<p>What can large language models do in chemistry?. T Guo, K Guo, B Nan, Z Liang, Z Guo, N Chawla, O Wiest, X Zhang, 10.48550/arXiv.2305.18365arXiv:2305.18365a comprehensive benchmark on eight tasks, arXiv, 2023, preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Adv. Neural Inf. Process Syst. 332020</p>
<p>Language models for the prediction of SARS-CoV-2 inhibitors. A E Blanchard, J Gounley, D Bhowmik, M Chandra Shekar, I Lyngaas, S Gao, J Yin, A Tsaris, F Wang, J Glaser, Int. J. High Perform. Comput. 362022</p>
<p>TransPolymer: a Transformer-based language model for polymer property predictions. C Xu, Y Wang, A Barati, Farimani, Comput. Mater. 642023</p>
<p>Leveraging large language models for predictive chemistry. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, Nat. Mach. Intell. 62024</p>
<p>Large-scale chemical language representations capture molecular structure and properties. J Ross, B Belgodere, V Chenthamarakshan, I Padhi, Y Mroueh, P Das, Nat. Mach. Intell. 42022</p>
<p>. Z Yang, Y Wang, L Zhang, 10.1101/2023.04.19.537579AI becomes a masterbrain scientist. 2023preprintbioRxiv</p>
<p>J Kim, D Kang, S Kim, H W Jang, Catalyze Materials Science with Machine Learning. 20213</p>
<p>Molecular Machine Learning for Chemical Catalysis: Prospects and Challenges. S Singh, R B Sunoj, Acc. Chem. Res. 562023</p>
<p>Predictive modeling in homogeneous catalysis: a tutorial. A G Maldonado, G Rothenberg, Chem. Soc. Rev. 392010</p>
<p>Inverse design of porous materials using articial neural networks. B Kim, S Lee, J Kim, Sci. Adv. 2020, 6, eaax9324</p>
<p>Machine learning approach for structure-based zeolite classication. D A Carr, M Lach-Hab, S Yang, I I Vaisman, E Blaisten-Barojas, Microporous Mesoporous Mater. 1172009</p>
<p>Atomic-position independent descriptor for machine learning of material properties. A Jain, T Bligaard, Phys. Rev. B. 2141122018</p>
<p>Estimation of Electronegativity Values of Elements in Different Valence States. K Li, D Xue, J. Phys. Chem. A. 1102006</p>
<p>MoleculeNet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, K Leswing, V Pande, Chem. Sci. 92018</p>
<p>. P Schlexer Lamoureux, K T Winther, J A Garrido Torres, V Streibel, M Zhao, M Bajdich, F Abild-Pedersen, T Bligaard, Machine Learning for Computational Heterogeneous Catalysis. 112019ChemCatChem</p>
<p>Reoptimization of MDL Keys for Use in Drug Discovery. J L Durant, B A Leland, D R Henry, J G Nourse, J. Chem. Inf. Comput. Sci. 422002</p>
<p>Unsupervised Data Base Clustering Based on Daylight's Fingerprint and Tanimoto Similarity: A Fast and Automated Way To Cluster Small and Large Data Sets. D Butina, J. Chem. Inf. Comput. Sci. 391999</p>
<p>Uncovering the inuence of the modier redox potential on CO2 reduction through combined datadriven machine learning and hypothesis-driven experimentation. X He, Y Su, J Zhu, N Fang, Y Chen, H Liu, D Zhou, C Wang, J. Mater. Chem. A. 112023</p>
<p>Extended-Connectivity Fingerprints. D Rogers, M Hahn, J. Chem. Inf. Model. 502010</p>
<p>Machine-Learning-Guided Discovery and Optimization of Additives in Preparing Cu Catalysts for CO2 Reduction. Y Guo, X He, Y Su, Y Dai, M Xie, S Yang, J Chen, K Wang, D Zhou, C Wang, J. Am. Chem. Soc. 1432021</p>
<p>Resolving Transition Metal Chemical Space: Feature Selection for Machine Learning and Structure-Property Relationships. J P Janet, H J Kulik, J. Phys. Chem. A. 1212017</p>
<p>Predictive and mechanistic multivariate linear regression models for reaction development. C B Santiago, J.-Y Guo, M S Sigman, Chem. Sci. 92018</p>
<p>Machine-Learning-Augmented Chemisorption Model for CO2 Electroreduction Catalyst Screening. X Ma, Z Li, L E K Achenie, H Xin, J. Phys. Chem. Lett. 62015</p>
<p>. H Mai, T C Le, D Chen, D A Winkler, R A Caruso, Machine Learning for Electrocatalyst and Photocatalyst Design and Discovery. 1222022Chem. Rev.</p>
<p>The Development of Multidimensional Analysis Tools for Asymmetric Catalysis and Beyond. M S Sigman, K C Harper, E N Bess, A Milo, Acc. Chem. Res. 492016</p>
<p>Comparing quantitative prediction methods for the discovery of small-molecule chiral catalysts. J P Reid, M S Sigman, Nat. Rev. Chem. 22018</p>
<p>Three-Dimensional Correlation of Steric and Electronic Free Energy Relationships Guides Asymmetric Propargylation. K C Harper, M S Sigman, Science. 3332011</p>
<p>PaDEL-descriptor: an open source soware to calculate molecular descriptors and ngerprints. C W Yap, J. Comput. Chem. 322011</p>
<p>An Ensemble Structure and Physicochemical (SPOC) Descriptor for Machine-Learning Prediction of Chemical Reaction and Molecular Properties. Q Yang, Y Liu, J Cheng, Y Li, S Liu, Y Duan, L Zhang, S Luo, Chemphyschem. e2022002552022</p>
<p>E L Willighagen, H M G W Denissen, R Wehrens, L M C Buydens, On the Use of 1H and 13C 1D NMR Spectra as QSPR Descriptors. 200646</p>
<p>Machine learning models capable of chemical deduction for identifying reaction products. T Jin, Q Zhao, A B Schoeld, B M Savoie, 10.26434/chemrxiv-2023-l6lzpChemRxiv. 2023preprint</p>
<p>Quantitatively Determining Surface-Adsorbate Properties from Vibrational Spectroscopy with Interpretable Machine Learning. X Wang, S Jiang, W Hu, S Ye, T Wang, F Wu, L Yang, X Li, G Zhang, X Chen, J Jiang, Y Luo, J. Am. Chem. Soc. 1442022</p>
<p>Fast Prediction of Selectivity in Heterogeneous Catalysis from Extended Brønsted-Evans-Polanyi Relations: A Theoretical Insight. D Loffreda, F Delbecq, F Vigné, P Sautet, Angew. Chem., Int. Ed. 482009</p>
<p>Unsupervised machine learning reveals eigen reactivity of metal surfaces. F Wei, L Zhuang, Sci. Bull. 692024</p>
<p>The importance of a charge transfer descriptor for screening potential CO(2) reduction electrocatalysts. S Ringe, Nat. Commun. 25982023</p>
<p>Descriptors of Oxygen-Evolution Activity for Oxides: A Statistical Evaluation. W T Hong, R E Welsch, Y Shao-Horn, J. Phys. Chem. C. 1202015</p>
<p>A Universal Descriptor for Complicated Interfacial Effects on Electrochemical Reduction Reactions. C Ren, S Lu, Y Wu, Y Ouyang, Y Zhang, Q Li, C Ling, J Wang, J. Am. Chem. Soc. 1442022</p>
<p>Scaling-Relation-Based Analysis of Bifunctional Catalysis: The Case for Homogeneous Bimetallic Alloys. M Andersen, A J Medford, J K Nørskov, K Reuter, ACS Catal. 72017</p>
<p>Toward Excellence of Electrocatalyst Design by Emerging Descriptor-Oriented Machine Learning. J Liu, W Luo, L Wang, J Zhang, X.-Z Fu, J.-L Luo, Adv. Funct. Mater. 21107482022</p>
<p>High-Throughput Screening of Electrocatalysts for Nitrogen Reduction Reactions Accelerated by Interpretable Intrinsic Descriptor. X Lin, Y Wang, X Chang, S Zhen, Z J Zhao, J Gong, Angew Chem. Int. Ed. Engl. e2023001222023</p>
<p>Electric Dipole Descriptor for Machine Learning Prediction of Catalyst Surface-Molecular Adsorbate Interactions. X Wang, S Ye, W Hu, E Sharman, R Liu, Y Liu, Y Luo, J Jiang, J. Am. Chem. Soc. 1422020</p>
<p>L H Mou, T Han, P E S Smith, E Sharman, J Jiang, Machine Learning Descriptors for Data-Driven Catalysis Study. 2023, 10, e2301020</p>
<p>Universal fragment descriptors for predicting properties of inorganic crystals. O Isayev, C Oses, C Toher, E Gossett, S Curtarolo, A Tropsha, Nat. Commun. 2017, 8, 15679</p>
<p>A graphconvolutional neural network model for the prediction of chemical reactivity. C W Coley, W Jin, L Rogers, T F Jamison, T S Jaakkola, W H Green, R Barzilay, K F Jensen, Chem. Sci. 102019</p>
<p>Linear free energy relationships in rate and equilibrium phenomena. L P Hammett, Trans. Faraday Soc. 341938</p>
<p>Linear Free Energy Relationships from Rates of Esterication and Hydrolysis of Aliphatic and Orthosubstituted Benzoate Esters. R W TaJr, J. Am. Chem. Soc. 741952</p>
<p>Data Science Meets Physical Organic Chemistry. J M Crawford, C Kingston, F D Toste, M S Sigman, Acc. Chem. Res. 542021</p>
<p>The Evolution of Data-Driven Modeling in Organic Chemistry. W L Williams, L Zeng, T Gensch, M S Sigman, A G Doyle, E V Anslyn, ACS Cent. Sci. 72021</p>
<p>Holistic prediction of enantioselectivity in asymmetric catalysis. J P Reid, M S Sigman, Nature. 5712019</p>
<p>From local explanations to global understanding with explainable AI for trees. S M Lundberg, G Erion, H Chen, A Degrave, J M Prutkin, B Nair, R Katz, J Himmelfarb, N Bansal, S.-I Lee, Nat. Mach. Intell. 22020</p>
<p>SISSO: A compressed-sensing method for identifying the best low-dimensional descriptor in an immensity of offered candidates. R Ouyang, S Curtarolo, E Ahmetcik, M Scheffler, L M Ghiringhelli, Phys. Rev. Mater. 2838022018</p>
<p>Simple descriptor derived from symbolic regression accelerating the discovery of new perovskite catalysts. B Weng, Z Song, R Zhu, Q Yan, Q Sun, C G Grice, Y Yan, W J Yin, Nat. Commun. 35132020</p>
<p>Physical descriptor for the Gibbs energy of inorganic crystalline solids and temperature-dependent materials chemistry. C J Bartel, S L Millican, A M Deml, J R Rumptz, W Tumas, A W Weimer, S Lany, V Stevanović, C B Musgrave, A M Holder, Nat. Commun. 41682018</p>
<p>Data-Driven Descriptor Engineering and Rened Scaling Relations for Predicting Transition Metal Oxide Reactivity. W Xu, M Andersen, K Reuter, ACS Catal. 112020</p>
<p>Single-atom alloy catalysts designed by rst-principles calculations and articial intelligence. Z.-K Han, D Sarker, R Ouyang, A Mazheika, Y Gao, S V Levchenko, Nat. Commun. 18332021</p>
<p>Probabilistic principal component analysis. M E Tipping, C M Bishop, J. R. Stat. Soc., B: Stat. Methodol. 611999</p>
<p>Augmenting interpretable models with large language models during training. C Singh, A Askari, R Caruana, J Gao, Nat. Commun. 79132023</p>
<p>A Systematic Survey of Chemical Pre-trained Models. J Xia, Y Zhu, Y Du, Y Liu, S Z Li, IJCAI. 2023</p>
<p>Uni-Mol: A Universal 3D Molecular Representation Learning Framework. G Zhou, Z Gao, Q Ding, H Zheng, H Xu, Z Wei, L Zhang, G Ke, The Eleventh International Conference on Learning Representations. 2023</p>
<p>ChemSpacE: Interpretable and Interactive Chemical Space Exploration. Y Du, X Liu, N M Shah, S Liu, J Zhang, B Zhou, 10.26434/chemrxiv-2022-x49mh-v3ChemRxiv. 2022preprint</p>
<p>SELFormer: molecular representation learning via SELFIES language models. A Yüksel, E Ulusoy, A Ünlü, T Dogan, Mach. Learn.: Sci. Technol. 2023, 4, 025035</p>
<p>Regression Transformer enables concurrent sequence regression and generation for molecular language modelling. J Born, M Manica, Nat. Mach. Intell. 52023</p>
<p>Language models can learn complex molecular distributions. D Flam-Shepherd, K Zhu, A Aspuru-Guzik, Nat. Commun. 32932022</p>
<p>Chemical language models for de novo drug design: Challenges and opportunities. F Grisoni, Curr. Opin. Struct. Biol. 1025272023</p>
<p>Combined High-Throughput DFT and ML Screening of Transition Metal Nitrides for Electrochemical CO2 Reduction. A G Yohannes, C Lee, P Talebi, D H Mok, M Karamad, S Back, S Siahrostami, ACS Catal. 132023</p>
<p>Machine learning based interpretation of microkinetic data: a Fischer-Tropsch synthesis case study. A Chakkingal, P Janssens, J Poissonnier, A J Barrios, M Virginie, A Y Khodakov, J W Thybaut, React. Chem. Eng. 72022</p>
<p>Why Should I Trust You?": Explaining the Predictions of Any Classier. M T Ribeiro, S Singh, C Guestrin, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningSan Francisco, California, USA2016</p>
<p>Interpretable design of Ir-free trimetallic electrocatalysts for ammonia oxidation with graph neural networks. H S Pillai, Y Li, S H Wang, N Omidvar, Q Mu, L E K Achenie, F Abild-Pedersen, J Yang, G Wu, H Xin, Nat. Commun. 7922023</p>
<p>Impacts of catalyst and process parameters on Ni-catalyzed methane dry reforming via interpretable machine learning. K Vellayappan, Y Yue, K H Lim, K Cao, J Y Tan, S Cheng, T Wang, T Z H Gani, I A Karimi, S Kawi, 10.1016/j.apcatb.2023.122593Appl. Catal., B. 3302023</p>
<p>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts, arXiv. T Shin, Y Razeghi, R L Logan, I V , E Wallace, S Singh, 10.48550/arXiv.2101.00190arXiv:2010.15980DOI: 10.48550/ arXiv.2101.00190Continuous Prompts for Generation, arXiv, 2021, preprint. 2020preprint</p>
<p>WARP: Word-level Adversarial ReProgramming. K Hambardzumyan, H Khachatrian, J May, Online. August, 2021</p>
<p>A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, P Dollár, R Girshick, 10.48550/arXiv.2304.02643arXiv:2304.02643Segment Anything, arXiv, 2023, preprint. </p>
<p>Validity and Reliability Analysis of the PlotDigitizer Soware Program for Data Extraction from Single-Case Graphs. O Aydin, M Y Yassikaya, Perspect. Behav. Sci. 452022</p>
<p>H Yang, S Yue, Y He, 10.48550/arXiv.2306.02224arXiv:2306.02224Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions, arXiv, 2023, preprint. </p>
<p>Augmenting large language models with chemistry tools. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, Nat. Mach. Intell. 652024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4024 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4024</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4024</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-94.html">extraction-schema-94</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-1185efcd2272697efe489c0bcfe7be7601af42e7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1185efcd2272697efe489c0bcfe7be7601af42e7" target="_blank">A Deep Learning Model to Predict a Diagnosis of Alzheimer Disease by Using 18F-FDG PET of the Brain.</a></p>
                <p><strong>Paper Venue:</strong> Radiology</p>
                <p><strong>Paper TL;DR:</strong> A deep learning algorithm developed for early prediction of Alzheimer disease achieved 82% specificity at 100% sensitivity, an average of 75.8 months prior to the final diagnosis, which in ROC space outperformed reader performance.</p>
                <p><strong>Paper Abstract:</strong> Purpose To develop and validate a deep learning algorithm that predicts the final diagnosis of Alzheimer disease (AD), mild cognitive impairment, or neither at fluorine 18 (18F) fluorodeoxyglucose (FDG) PET of the brain and compare its performance to that of radiologic readers. Materials and Methods Prospective 18F-FDG PET brain images from the Alzheimer's Disease Neuroimaging Initiative (ADNI) (2109 imaging studies from 2005 to 2017, 1002 patients) and retrospective independent test set (40 imaging studies from 2006 to 2016, 40 patients) were collected. Final clinical diagnosis at follow-up was recorded. Convolutional neural network of InceptionV3 architecture was trained on 90% of ADNI data set and tested on the remaining 10%, as well as the independent test set, with performance compared to radiologic readers. Model was analyzed with sensitivity, specificity, receiver operating characteristic (ROC), saliency map, and t-distributed stochastic neighbor embedding. Results The algorithm achieved area under the ROC curve of 0.98 (95% confidence interval: 0.94, 1.00) when evaluated on predicting the final clinical diagnosis of AD in the independent test set (82% specificity at 100% sensitivity), an average of 75.8 months prior to the final diagnosis, which in ROC space outperformed reader performance (57% [four of seven] sensitivity, 91% [30 of 33] specificity; P < .05). Saliency map demonstrated attention to known areas of interest but with focus on the entire brain. Conclusion By using fluorine 18 fluorodeoxyglucose PET of the brain, a deep learning algorithm developed for early prediction of Alzheimer disease achieved 82% specificity at 100% sensitivity, an average of 75.8 months prior to the final diagnosis. © RSNA, 2018 Online supplemental material is available for this article. See also the editorial by Larvie in this issue.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4024.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4024.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aβ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Beta-amyloid (amyloid-β)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregated peptide species considered a core molecular marker and hypothesized pathogenic driver of Alzheimer's disease; detectable in cerebrospinal fluid and by PET imaging with radiolabeled Aβ ligands.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td>Accumulation/deposition of beta-amyloid (Aβ) in the brain is discussed as a marker and implicated mechanism of Alzheimer's disease.</td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td>Paper cites existing literature that Aβ is a marker of AD and can be detected in CSF and by PET with radiolabeled Aβ ligands (examples: 18F-florbetapir, flutemetamol, florbetaben). The current study does not provide original experimental evidence for Aβ causation; it only references prior work (refs cited in paper). No quantitative results for Aβ pathology are reported in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>CSF assays and amyloid PET imaging using radiolabeled Aβ ligands (e.g., 18F-florbetapir, flutemetamol, florbetaben) — mentioned as established methods.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Cerebrospinal fluid Aβ levels and PET amyloid ligand retention (amyloid PET signal).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Not reported in this paper; the authors note these are increasingly specific biomarkers but do not provide sensitivity/specificity/AUC values here (they cite external systematic reviews/meta-analyses for diagnostic accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Used for early diagnosis; implied utility across preclinical and symptomatic stages but this paper does not present primary data on staging with Aβ measures.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Mentioned via citation to prior human clinical studies and PET/CSF biomarker literature; no original Aβ experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Paper notes amyloid PET and CSF Aβ testing are costly and may not be universally available or reimbursed; this study emphasizes that 18F-FDG PET is more established and widely available. The authors do not present head-to-head performance data in this manuscript and do not claim amyloid measures as part of their dataset; therefore direct comparisons are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Deep Learning Model to Predict a Diagnosis of Alzheimer Disease by Using 18F-FDG PET of the Brain.', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4024.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4024.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FDG‑DL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>18F‑FDG PET with deep learning (Inception V3) classifier</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A detection approach using fluorine-18 fluorodeoxyglucose PET brain images analyzed by a convolutional neural network (Inception V3) to predict eventual clinical diagnosis of Alzheimer disease, MCI, or neither several years before final diagnosis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td>Not a causal mechanism per se; the study leverages patterns of cerebral hypometabolism (reduced 18F-FDG uptake) — particularly in posterior cingulate, parietotemporal and frontal regions — as an imaging manifestation reflecting underlying neuronal dysfunction associated with AD.</td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td>The paper relies on established associations between regional cortical hypometabolism on 18F-FDG PET and AD (cites prior studies). The present study provides evidence that a deep learning model can detect patterns of hypometabolism (and more complex whole-brain patterns) predictive of a future clinical AD diagnosis: on independent test set the model identified all 7 patients who ultimately had AD (sensitivity 100%) and separated classes in learned feature space (t-SNE). No causal mechanistic experiments are performed—hypometabolism is treated as a biomarker of disease-related neuronal dysfunction rather than proof of causation.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>18F‑FDG PET brain imaging analyzed with a convolutional neural network (Inception V3) trained on ADNI data (1921 studies for training/validation) and tested on ADNI hold-out (188 studies) and an independent local test set (40 studies). Preprocessing included resampling, cropping, a 4×4 grid tiling of axial sections, and standard augmentation; model pretrained on ImageNet and fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Regional cortical hypometabolism patterns on 18F-FDG PET—classically posterior cingulate, parietotemporal cortices, and frontal lobes—and holistic whole-brain pixel/voxel patterns learned by the network (saliency maps showed emphasis in caudal parietotemporal sections but no single human-interpretable biomarker was isolated).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Independent test set (n=40): AUC for AD = 0.98 (95% CI: 0.94–1.00); AD sensitivity = 100% (7/7), specificity = 82% (27/33), precision = 54% (7/13); authors report '82% specificity at 100% sensitivity' and average prediction ~75.8 months prior to final diagnosis. ADNI 10% hold-out (n=188): AUC for AD = 0.92; AD sensitivity = 81% (29/36), specificity = 94% (143/152), precision = 76% (29/38). Model performance for MCI was substantially lower (AUC ~0.52–0.63; low sensitivity/precision).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Effective as an early prediction tool — on average the model's predictions preceded the final clinical diagnosis by ~75.8 months (suggesting utility in prodromal/early stages and long before clinical AD diagnosis in this cohort). The study includes subjects across normal cognition, MCI (prodromal), and AD.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Human clinical imaging study: retrospective analysis of prospectively collected ADNI 18F‑FDG PET data (2109 imaging studies from 1002 patients) for training/holdout and a retrospective independent test set of 40 clinical PET studies from the authors' institution; model evaluation vs radiologist readers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Authors note multiple limitations: small independent external test set (n=40) and selection bias (patients were memory-clinic referrals, excluding many non-AD neurological disorders), lack of non-AD neurodegenerative cases in training data (limits generalizability), black-box nature of deep learning with no single interpretable imaging biomarker found (saliency maps did not yield unified human-interpretable features), weaker performance for identifying MCI (high variability and continuum with AD), and need for larger prospective, multi-institutional validation before clinical deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Deep Learning Model to Predict a Diagnosis of Alzheimer Disease by Using 18F-FDG PET of the Brain.', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Alzheimer's disease <em>(Rating: 2)</em></li>
                <li>Diagnostic accuracy of (18)F amyloid PET tracers for the diagnosis of Alzheimer's disease: a systematic review and meta-analysis <em>(Rating: 2)</em></li>
                <li>Multicenter standardized 18F-FDG PET diagnosis of mild cognitive impairment, Alzheimer's disease, and other dementias <em>(Rating: 2)</em></li>
                <li>Review and meta-analysis of biomarkers and diagnostic imaging in Alzheimer's disease <em>(Rating: 2)</em></li>
                <li>Brain FDG PET and the diagnosis of dementia <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4024",
    "paper_id": "paper-1185efcd2272697efe489c0bcfe7be7601af42e7",
    "extraction_schema_id": "extraction-schema-94",
    "extracted_data": [
        {
            "name_short": "Aβ",
            "name_full": "Beta-amyloid (amyloid-β)",
            "brief_description": "Aggregated peptide species considered a core molecular marker and hypothesized pathogenic driver of Alzheimer's disease; detectable in cerebrospinal fluid and by PET imaging with radiolabeled Aβ ligands.",
            "citation_title": "",
            "mention_or_use": "mention",
            "proposed_cause": "Accumulation/deposition of beta-amyloid (Aβ) in the brain is discussed as a marker and implicated mechanism of Alzheimer's disease.",
            "cause_evidence": "Paper cites existing literature that Aβ is a marker of AD and can be detected in CSF and by PET with radiolabeled Aβ ligands (examples: 18F-florbetapir, flutemetamol, florbetaben). The current study does not provide original experimental evidence for Aβ causation; it only references prior work (refs cited in paper). No quantitative results for Aβ pathology are reported in this study.",
            "detection_method": "CSF assays and amyloid PET imaging using radiolabeled Aβ ligands (e.g., 18F-florbetapir, flutemetamol, florbetaben) — mentioned as established methods.",
            "biomarker_or_finding": "Cerebrospinal fluid Aβ levels and PET amyloid ligand retention (amyloid PET signal).",
            "detection_performance": "Not reported in this paper; the authors note these are increasingly specific biomarkers but do not provide sensitivity/specificity/AUC values here (they cite external systematic reviews/meta-analyses for diagnostic accuracy).",
            "detection_stage": "Used for early diagnosis; implied utility across preclinical and symptomatic stages but this paper does not present primary data on staging with Aβ measures.",
            "study_type": "Mentioned via citation to prior human clinical studies and PET/CSF biomarker literature; no original Aβ experiments in this paper.",
            "limitations_or_counter_evidence": "Paper notes amyloid PET and CSF Aβ testing are costly and may not be universally available or reimbursed; this study emphasizes that 18F-FDG PET is more established and widely available. The authors do not present head-to-head performance data in this manuscript and do not claim amyloid measures as part of their dataset; therefore direct comparisons are not provided here.",
            "uuid": "e4024.0",
            "source_info": {
                "paper_title": "A Deep Learning Model to Predict a Diagnosis of Alzheimer Disease by Using 18F-FDG PET of the Brain.",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "FDG‑DL",
            "name_full": "18F‑FDG PET with deep learning (Inception V3) classifier",
            "brief_description": "A detection approach using fluorine-18 fluorodeoxyglucose PET brain images analyzed by a convolutional neural network (Inception V3) to predict eventual clinical diagnosis of Alzheimer disease, MCI, or neither several years before final diagnosis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proposed_cause": "Not a causal mechanism per se; the study leverages patterns of cerebral hypometabolism (reduced 18F-FDG uptake) — particularly in posterior cingulate, parietotemporal and frontal regions — as an imaging manifestation reflecting underlying neuronal dysfunction associated with AD.",
            "cause_evidence": "The paper relies on established associations between regional cortical hypometabolism on 18F-FDG PET and AD (cites prior studies). The present study provides evidence that a deep learning model can detect patterns of hypometabolism (and more complex whole-brain patterns) predictive of a future clinical AD diagnosis: on independent test set the model identified all 7 patients who ultimately had AD (sensitivity 100%) and separated classes in learned feature space (t-SNE). No causal mechanistic experiments are performed—hypometabolism is treated as a biomarker of disease-related neuronal dysfunction rather than proof of causation.",
            "detection_method": "18F‑FDG PET brain imaging analyzed with a convolutional neural network (Inception V3) trained on ADNI data (1921 studies for training/validation) and tested on ADNI hold-out (188 studies) and an independent local test set (40 studies). Preprocessing included resampling, cropping, a 4×4 grid tiling of axial sections, and standard augmentation; model pretrained on ImageNet and fine-tuned.",
            "biomarker_or_finding": "Regional cortical hypometabolism patterns on 18F-FDG PET—classically posterior cingulate, parietotemporal cortices, and frontal lobes—and holistic whole-brain pixel/voxel patterns learned by the network (saliency maps showed emphasis in caudal parietotemporal sections but no single human-interpretable biomarker was isolated).",
            "detection_performance": "Independent test set (n=40): AUC for AD = 0.98 (95% CI: 0.94–1.00); AD sensitivity = 100% (7/7), specificity = 82% (27/33), precision = 54% (7/13); authors report '82% specificity at 100% sensitivity' and average prediction ~75.8 months prior to final diagnosis. ADNI 10% hold-out (n=188): AUC for AD = 0.92; AD sensitivity = 81% (29/36), specificity = 94% (143/152), precision = 76% (29/38). Model performance for MCI was substantially lower (AUC ~0.52–0.63; low sensitivity/precision).",
            "detection_stage": "Effective as an early prediction tool — on average the model's predictions preceded the final clinical diagnosis by ~75.8 months (suggesting utility in prodromal/early stages and long before clinical AD diagnosis in this cohort). The study includes subjects across normal cognition, MCI (prodromal), and AD.",
            "study_type": "Human clinical imaging study: retrospective analysis of prospectively collected ADNI 18F‑FDG PET data (2109 imaging studies from 1002 patients) for training/holdout and a retrospective independent test set of 40 clinical PET studies from the authors' institution; model evaluation vs radiologist readers.",
            "limitations_or_counter_evidence": "Authors note multiple limitations: small independent external test set (n=40) and selection bias (patients were memory-clinic referrals, excluding many non-AD neurological disorders), lack of non-AD neurodegenerative cases in training data (limits generalizability), black-box nature of deep learning with no single interpretable imaging biomarker found (saliency maps did not yield unified human-interpretable features), weaker performance for identifying MCI (high variability and continuum with AD), and need for larger prospective, multi-institutional validation before clinical deployment.",
            "uuid": "e4024.1",
            "source_info": {
                "paper_title": "A Deep Learning Model to Predict a Diagnosis of Alzheimer Disease by Using 18F-FDG PET of the Brain.",
                "publication_date_yy_mm": "2019-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Alzheimer's disease",
            "rating": 2
        },
        {
            "paper_title": "Diagnostic accuracy of (18)F amyloid PET tracers for the diagnosis of Alzheimer's disease: a systematic review and meta-analysis",
            "rating": 2
        },
        {
            "paper_title": "Multicenter standardized 18F-FDG PET diagnosis of mild cognitive impairment, Alzheimer's disease, and other dementias",
            "rating": 2
        },
        {
            "paper_title": "Review and meta-analysis of biomarkers and diagnostic imaging in Alzheimer's disease",
            "rating": 2
        },
        {
            "paper_title": "Brain FDG PET and the diagnosis of dementia",
            "rating": 2
        }
    ],
    "cost": 0.009751,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Deep Learning Model to Predict a Diagnosis of Alzheimer Disease by Using ${ }^{18} \mathrm{~F}$-FDG PET of the Brain</h1>
<p>Yiming Ding $\cdot$ Jae Ho Sohn, MD, MS ・ Michael G. Kawczynski, MS ・ Hari Trivedi, MD ・ Roy Harnish, MS $\cdot$ Nathaniel W. Jenkins, MS ・ Dmytro Lituiev, PhD ・Timothy P. Copeland, MPP ・ Mariam S. Aboian, MD, PhD $\cdot$ Carina Mari Aparici, MD ・ Spencer C. Behr, MD ・ Robert R. Flavell, MD, PhD ・ Shih-Ying Huang, PhD $\cdot$ Kelly A. Zalocusky, PhD ・ Lorenzo Nardo, PhD ・ Youngho Seo, PhD ・ Randall A. Hawkins, MD, PhD $\cdot$ Miguel Hernandez Pampaloni, MD, PhD $\cdot$ Dexter Hadley, MD, PhD $\cdot$ Benjamin L. Franc, MD, MS</p>
<p>From the Department of Radiology and Biomedical Imaging (Y.D., J.H.S., H.T., R.H., N.W.J., T.P.C., M.S.A., C.M.A., S.C.B., R.R.F., S.Y.H., Y.S., R.A.H., M.H.P., B.L.F.) and Institute for Computational Health Sciences (J.H.S., M.G.K., H.T., D.L., K.A.Z., D.H.), University of California, San Francisco, 550 Parnassus Ave, San Francisco, CA 94143; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, Berkeley, Calif (Y.D.); and Department of Radiology, University of California, Davis, Sacramento, Calif (L.N.). From the 2017 RSNA Annual Meeting. Received April 23, 2018; revision requested July 3; final revision received August 24; accepted September 13. Address correspondence to J.H.S. (e-mail: sohn87@gmail.com).
H.T. supported by Foundation for the National Institutes of Health fellowship (5T32EB001631-10). Data collection and sharing for this project was funded by the Alzheimer's Disease Neuroimaging Initiative (ADNI), National Institutes of Health (U01 AG024904) and U.S. Department of Defense (W81XWH-12-2-0012). J.H.S. supported by University of California, San Francisco (CTSI Resident Research Grant 2017, Radiology \&amp; Biomedical Imaging Seed Grant #17-11). ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California. Data used in the preparation of this article were obtained from the ADNI database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report.</p>
<p>Conflicts of interest are listed at the end of this article.
See also the editorial by Larvie in this issue.
Radiology 2019; 290:456-464 $\cdot$ https://doi.org/10.1148/radiol.2018180958 $\cdot$ Content code: Nn</p>
<p>Purpose: To develop and validate a deep learning algorithm that predicts the final diagnosis of Alzheimer disease (AD), mild cognitive impairment, or neither at fluorine $18\left({ }^{18} \mathrm{~F}\right)$ fluorodeoxyglucose (FDG) PET of the brain and compare its performance to that of radiologic readers.</p>
<p>Materials and Methods: Prospective ${ }^{18} \mathrm{~F}$-FDG PET brain images from the Alzheimer's Disease Neuroimaging Initiative (ADNI) (2109 imaging studies from 2005 to 2017, 1002 patients) and retrospective independent test set ( 40 imaging studies from 2006 to 2016, 40 patients) were collected. Final clinical diagnosis at follow-up was recorded. Convolutional neural network of InceptionV3 architecture was trained on $90 \%$ of ADNI data set and tested on the remaining $10 \%$, as well as the independent test set, with performance compared to radiologic readers. Model was analyzed with sensitivity, specificity, receiver operating characteristic (ROC), saliency map, and $t$-distributed stochastic neighbor embedding.</p>
<p>Results: The algorithm achieved area under the ROC curve of 0.98 ( $95 \%$ confidence interval: $0.94,1.00$ ) when evaluated on predicting the final clinical diagnosis of AD in the independent test set ( $82 \%$ specificity at $100 \%$ sensitivity), an average of 75.8 months prior to the final diagnosis, which in ROC space outperformed reader performance ( $57 \%$ [four of seven] sensitivity, $91 \%$ [30 of 33] specificity; $P&lt;.05$ ). Saliency map demonstrated attention to known areas of interest but with focus on the entire brain.</p>
<p>Conclusion: By using fluorine 18 fluorodeoxyglucose PET of the brain, a deep learning algorithm developed for early prediction of Alzheimer disease achieved $82 \%$ specificity at $100 \%$ sensitivity, an average of 75.8 months prior to the final diagnosis.
© RSNA, 2018
Online supplemental material is available for this article.</p>
<p>Although Alzheimer disease (AD) remains a diagnosis based on clinical grounds $(1,2)$, advancements in diagnostic technology such as PET with fluorine $18\left({ }^{18} \mathrm{~F}\right)$ fluorodeoxyglucose (FDG) allow earlier diagnosis and treatments, when they may be most effective (3). There is a continuous spectrum from normal cognition to AD , including mild cognitive impairment (MCI) as a prodromal stage of AD $(4,5)$. Classically, patients with AD tend to show hypometabolism on ${ }^{18} \mathrm{~F}$-FDG PET scans in regions of the posterior cingulate, parietotemporal cortices, and frontal lobes, while patients with MCI often show posterior cingulate and parietotemporal hypometabolism with variable frontal lobe involvement. However, there is
substantial overlap of findings as both entities lie along a continuum (5). In current practice, ${ }^{18} \mathrm{~F}$-FDG PET requires interpretation by specialists in nuclear medicine and neuroimaging to make pattern recognition decisions mostly using qualitative readings. This is particularly challenging in the setting of a disease that involves a wide continuous spectrum, from normal cognition to MCI to AD , with only a fraction of patients with MCI eventually advancing to AD . At present, there is no definite marker to determine this eventual progress.</p>
<p>There is wide recognition that deep learning may assist in addressing the increasing complexity and volume of imaging data, as well as the varying expertise of trained imaging</p>
<h2>Abbreviations</h2>
<p>AD = Alzheimer disease, ADNI = Alzheimer's Disease Neuroimaging Initiative, AUC = area under the ROC curve, $\mathrm{CI}=$ confidence interval, FDG = fluorodeoxyglucose, $\mathrm{MCI}=$ mild cognitive impairment, $\mathrm{ROC}=$ receiver operating characteristic, $t$-SNE $=t$-distributed stochastic neighbor embedding</p>
<h2>Summary</h2>
<p>By using fluorine 18 fluorodeoxyglucose PET of the brain, a deep learning algorithm developed for early prediction of Alzheimer disease achieved $82 \%$ specificity at $100 \%$ sensitivity, an average of 75.8 months prior to the final diagnosis.</p>
<h2>Implications for Patient Care</h2>
<ul>
<li>A deep learning algorithm can be used to improve the accuracy of predicting the diagnosis of Alzheimer disease from fluorine 18 fluorodeoxyglucose PET of the brain.</li>
<li>A deep learning algorithm can be used as an early prediction tool for Alzheimer disease, especially in conjunction with other biochemical and imaging tests, thereby providing an opportunity for early therapeutic intervention.
physicians (6). There has been substantial effort to apply deep learning in many diseases and imaging types such as breast cancer detection with mammography, pulmonary nodule detection with CT, and hip osteoarthritis classification with radiography, though integration into clinical flow is yet to be developed and validated (7-10). The application of machine learning technology to complex patterns of findings, such as those found at functional PET imaging of the brain, is only beginning to be explored.</li>
</ul>
<p>In this study, we aimed to evaluate whether a deep learning algorithm could be trained to predict the final clinical diagnoses in patients who underwent ${ }^{18} \mathrm{~F}$-FDG PET of the brain and, once trained, how the deep learning algorithm compares with the current standard clinical reading methods in differentiation of patients with final diagnoses of AD, MCI, or no evidence of dementia. We hypothesized that the deep learning algorithm could detect features or patterns that are not evident on standard clinical review of images and thereby improve the final diagnostic classification of individuals.</p>
<h2>Materials and Methods</h2>
<h2>Data Acquisition</h2>
<p>This institutional review board approved, written informed consent waived, and Health Insurance Portability and Accountability Act compliant study involved retrospective analysis of prospectively collected $2109{ }^{18} \mathrm{~F}$-FDG PET imaging studies from 1002 patients, most patients with multiple scans, with dates ranging from May 2005 to January 2017, across Alzheimer's Disease Neuroimaging Initiative (ADNI)-1, ADNI2, and ADNI-GO (Grand Opportunities) studies (Appendix E1 [online]). Data regarding the patient's final diagnoses were downloaded from the ADNI web portal (adni.loni.ucla.edu) (11). Detailed ${ }^{18} \mathrm{~F}$-FDG PET imaging protocols can be found at http://adni.loni.usc.edu/methods/documents/ (12-14). Ninety percent (1921 imaging studies, 899 patients) of this data set was used for model training and internal validation. The re-
maining $10 \%$ (188 imaging studies, 103 patients) was used for model testing, which we call $10 \%$ ADNI hold-out test set, serving as the internal test set from the perspective of the algorithm. An additional test set was obtained from the author's own institution, which we call independent test set, serving as the external test set from the perspective of the algorithm. The independent test set (Fig 1) comprised $40^{18} \mathrm{~F}$-FDG PET imaging studies from 40 patients who were not enrolled in the ADNI, with imaging study dates ranging from 2006 to 2016. Approximately 45 minutes after intravenous administration of $8-10 \mathrm{mCi}{ }^{18} \mathrm{~F}$-FDG, following standard clinical guidelines, images were acquired as dedicated PET emission-only images (ECAT HR+; Siemens, Knoxville, Tenn) or as part of PET-CT (Discovery VCT, General Electric, Wakesha, Wis; or Biograph 16, Siemens). Only PET emission images were utilized in the test set to remain consistent with the training set. Necropsy data were used as the final diagnosis in one patient for which they were available. None of the patients had a diagnosis of a dementia of the non-Alzheimer type. For both data sets, final clinical diagnosis after all follow-up examinations was used as the ground truth label.</p>
<h2>Data Preprocessing</h2>
<p>The imaging data were preprocessed by using a grid method (Fig 2). Images were resampled to $2-\mathrm{mm}$ isotropic voxels and cropped to a $100 \times 100 \times 90$-pixel grid resulting in a $200 \times$ $200 \times 180-\mathrm{mm}^{3}$ volume. An Otsu threshold was utilized to select brain voxels. Connected component analysis was used to derive the relevant imaging volume by selecting the cranialmost and caudal-most sections representing more than $100 \times$ $100 \mathrm{~mm}^{3}$ of brain parenchyma. The total volume was then divided into 16 evenly spaced sections, rounded to the nearest axial location, and distributed into a $4 \times 4$ grid with the cra-nial-most section in the top left and caudal-most section in the bottom right. All preprocessing steps were conducted in Python (Python 2.7; Python Software Foundation, Wilmington, Del; 2009) using package SciPy (http://www.scipy.org).</p>
<h2>Model Training</h2>
<p>After preprocessing, the images were $512 \times 512$ matrix size and were loaded onto a machine with Linux operating system (Ubuntu 14.04; Canonical, London, England). The machine has six-core Intel i7 5930k 3.5-gHz processor (Intel, Santa Clara, Calif), 64 GB of DDR4 SDRAM, and a NVIDIA Pascal Titan X graphical processing unit (Nvidia Corporation, Santa Clara, Calif) with CUDA 8.0 and CuDNN 6.0 (Nvidia). Convolutional neural network architecture Inception V3 was used in the study (15). The network was pretrained on ImageNet, an everyday image data set containing 14 million images of 1000 classes, before being fine-tuned using $90 \%$ of the ADNI data set (1921 imaging studies). Data augmentation, including random width and height shift (range, $0 \%-10 \%$ ) and zooming (range, $0 \%-8 \%$ ), was performed on the training set. Dropout layer with a rate of 0.6 was added before the fully connected layers at the end of the network as means of regulation. The neural network architecture is shown in Figure 3 and Appendix E1 (online).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Inclusion and exclusion criteria for the independent test set. Patient must have had at least one follow-up with a neurologist at our local institution. ADNI = Alzheimer's Disease Neuroimaging Initiative.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example of fluorine 18 fluorodeoxyglucose PET images from Alzheimer's Disease Neuroimaging Initiative set preprocessed with the grid method for patients with Alzheimer disease (AD). One representative zoomed-in section was provided for each of three example patients: A, 76-year-old man with AD, B, 83-year-old woman with mild cognitive impairment (MCI), and, C, 80-year-old man with non-AD/MCI. In this example, the patient with AD presented slightly less gray matter than did the patient with non-AD/MCI. The difference between the patient with MCI and the patient with non-AD/MCI appeared minimal to the naked eye.</p>
<p>Adam, a first-order gradient-based stochastic optimization algorithm, with a learning rate of 0.0001 , categorical cross entropy loss function, and batch size of 8 was used for model training (16). The trained algorithm was tested by the accuracy on the held-out ADNI data set $(n=188)$ and the independent test set ( $n=40$ ). Keras 2.0 (2017; Google, Mountain View, Calif) with Tensorflow 1.3 (2015; Google) backend was used for designing neural networks and loading pretrained weights. All programs were run in Python 2.7.</p>
<h2>Model Interpretation and Data Visualization</h2>
<p>To gain further intuition into how the network derived its decisions, one average saliency map taken across $10 \%$ ADNI test set and one across independent test set were shown. Saliency map plots the gradient of AD class score regarding each input pixel and thereby visualizes areas on the images that were deemed important for the classification result (17). To illustrate the connection between the saliency map and anatomy, an additional example individual saliency map was presented with anatomy overlay. All saliency maps were produced by using Keras 2.0.
$t$-Distributed stochastic neighbor embedding ( $t$-SNE) (18), a dimension reduction method that preserves relative closeness of data points, was then performed on features extracted by the deep learning network on training data. By using package scikitlearn (19), the 1024 features were first reduced to dimension 30 with principal component analysis before $t$-SNE was applied with learning rate 200 and 1000 iterations to reduce the dimension further to 2 .</p>
<h2>Clinical Interpretation of ${ }^{18} \mathrm{~F}$-FDG PET</h2>
<p>To obtain reader performance on the independent test set, three board-certified nuclear medicine physicians (R.A.H., nuclear medicine; B.L.F., nuclear medicine; S.C.B., abdominal imaging and nuclear medicine) with 36, 14, and 5 years of experience, respectively, performed independent interpretations of the $40^{18} \mathrm{~F}$-FDG PET imaging studies from the independent test set. Interpretations consisted of two components: qualitative interpretation of the PET emission images in axial, sagittal, and coronal planes, followed by a semiquantitative regional metabolic analysis using a commercially available clinical neuro-analysis software package (MIM Software, Cleveland, Ohio). Only ${ }^{18} \mathrm{~F}$-FDG PET imaging data, name, age, and date of scan were visible to the readers. Qualitative and quantitative interpretations for one patient were performed consecutively before moving on to the next patient. If any of the three qualitative interpretations disagreed, the imaging study was interpreted by two additional radiology readers (L.N, nuclear medicine; C.M.A., nuclear medicine) with 1 year and 13 years of experience, respectively. The diagnosis of the majority of the five radiology readers was taken as the final clinical imaging diagnosis.</p>
<h2>Model Testing and Statistical Analysis</h2>
<p>The trained deep learning model was tested on two test data sets: $10 \%$ ADNI set as internal hold-out test set and independent test set from local institution as external test set. Probability that an image belongs to class AD, MCI, and non-AD/ MCI was output by the model, and the class with the highest probability was chosen as the classification result.</p>
<p>Receiver operating characteristic (ROC) curves of the model on $10 \%$ ADNI set were plotted and the area under the ROC curve (AUC) was calculated. To compare the performance of deep learning model to reader performance, the ROC curves of deep learning model on independent test set were plotted with $95 \%$ confidence interval (CI), calculated by using package pROC 1.12 .1 in R 3.5.1 with 200 iterations</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p><strong>Figure 3:</strong> Convolutional neural network architecture, Inception v3, used in this study. Inception v3 network stacks 11 inception modules where each module consists of pooling layers and convolutional filters with rectified linear units as activation function. The input of the model is two-dimensional images of 16 horizontal sections of the brain placed on 4 × 4 grids as produced by the preprocessing step. Three fully connected layers of size 1024, 512, and 3 are added to the final concatenation layer. A dropout with rate of 0.6 is applied before the fully connected layers as means of regularization. The model is pretrained on ImageNet dataset and further fine-tuned with a batch size of 8 and learning rate of 0.0001.</p>
<p>of bootstrapping (20,21). The sensitivity and specificity of reader performance were plotted in the same ROC space. If clinical reader's sensitivity and specificity point lies outside of the CI space of the ROC curves, then the result was deemed as statistically significant. Sensitivity, specificity, precision, and F1 score were reported for both deep learning model and radiology readers. Model training, model testing, and model visualization were performed by Y.D., J.H.S., and M.K. Statistical analysis was performed by Y.D. and J.H.S.</p>
<h1>Results</h1>
<h2>Demographics</h2>
<p>As shown in Table 1, the ADNI set was composed of 2109 imaging studies from 1002 patients. The average age of the patients was 76 years (range, 55–93 years) for men and 75 years (range, 55–96 years) for women (P &lt; .001), with an average age of 77 years (range, 56–92 years) for men and 75 years (range, 55–93 years) for women in the AD group (P = .04), 76 years (range, 55–93 years) for men and 74 years (range, 57–91 years) for women in the MCI group (P = .01), and 76 years (range, 60–90 years) for men and 75 years (range, 60–96 years) for women in the non-AD/MCI group (P = .14). The overall percentage of men was 54% (547 of 1002) by patient and 58% (1225 of 2109) by imaging study. The average follow-up period of the patients was 54 months by patient and 62 months by imaging study.</p>
<p>The independent test set was composed of 40 patients, with seven clinically diagnosed as having AD, seven as having MCI, and 26 as having non-AD/MCI at the end of the follow-up period. The average age of the 40 test patients was 66 years (range, 48–84 years) for men and 71 years (range, 41–84 years) for women, with an average age of 69 years (range, 56–79 years) for men and 73 years (range, 73–73 years) for women in the AD group, 63 years (range, 48–83 years) for men and 68 years (range, 68–68 years) for women in the MCI group, and 66 years (range, 55–84 years) for men and 71 years (range, 41–84 years) for women in the non-AD/MCI group. The overall percentage of men was 58% (23 of 40), while the percentage in the AD, MCI, and non-AD/MCI group was 86% (six of seven), 86% (six of seven), and 42% (11 of 26), respectively. The average follow-up period of the patients was 76 months, with an average of 82 months in the AD group, 75 months in the MCI group, and 74 months in the non-AD/MCI group.</p>
<h2>Results of Model Training</h2>
<p>The ROC curves of the inception V3 network trained on 90% of ADNI data and tested on the remaining 10% are shown in Figure 4a. The AUC for prediction of AD, MCI, and non-AD/MCI was 0.92, 0.63, and 0.73 respectively. The above AUCs indicate that the deep learning network had reasonable ability to distinguish patients who finally progressed to AD at the time of imaging from those who stayed to have MCI or non-AD/MCI, but was weaker at discriminating patients with MCI from the others. As shown in Table 2, in the prediction of AD, MCI, and non-AD/MCI, the respective sensitivity was 81% (29 of 36), 54% (43 of 79), and 59% (43 of 73), specificity was 94% (143 of 152), 68% (74 of 109), and 75% (86 of 115), and precision was 76% (29 of 38), 55% (43 of 78), and 60% (43 of 72).</p>
<p>The ROC curves of the inception V3 network trained on 90% ADNI data and tested on independent test set with 95% CI are shown in Figure 4b. The AUC for the prediction of AD, MCI, and non-AD/MCI was 0.98 (95% CI: 0.94, 1.00), 0.52 (95% CI: 0.34, 0.71), and 0.84 (95% CI: 0.70, 0.99), respectively. Choosing the class with the highest probability as the classification result, in the prediction of AD, MCI, and non-AD/MCI, respectively, the sensitivity was 100% (seven of seven), 43% (three of seven), and 35% (nine of 26), the specificity was 82% (27 of 33), 58% (19 of 33), and 93% (13 of 14), and the precision was 54% (seven of 13), 18% (three of 17), and 90% (nine of 10). With a perfect sensitivity rate and reasonable specificity on AD, the model preserves a strong ability to predict the final diagnoses prior to the full follow-up period that, on average, concluded 76 months later.</p>
<h1>Table 1: Demographics of Datasets</h1>
<p>A: ADNI Set</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Average Age (y)*</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Male Sex ${ }^{\dagger}$</th>
<th style="text-align: center;">Average Follow-up (mo)*</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Clinical Diagnosis</td>
<td style="text-align: center;">No. of <br> Patients</td>
<td style="text-align: center;">No. of Imaging Studies</td>
<td style="text-align: center;">Male</td>
<td style="text-align: center;">Female</td>
<td style="text-align: center;">$P$ Value</td>
<td style="text-align: center;">Per <br> Patient</td>
<td style="text-align: center;">Per Imaging Study</td>
<td style="text-align: center;">Per Patient</td>
<td style="text-align: center;">Per Imaging Study</td>
</tr>
<tr>
<td style="text-align: center;">AD</td>
<td style="text-align: center;">236</td>
<td style="text-align: center;">484</td>
<td style="text-align: center;">$\begin{gathered} 76.8 \pm 7.4 \ (56-92) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 75.3 \pm 7.6 \ (55-93) \end{gathered}$</td>
<td style="text-align: center;">.04</td>
<td style="text-align: center;">$\begin{aligned} &amp; 140 / 236 \ &amp; (59) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 285 / 484 \ &amp; (59) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 34.0 \pm \ &amp; 19.0 \end{aligned}$</td>
<td style="text-align: center;">$36 \pm 20.6$</td>
</tr>
<tr>
<td style="text-align: center;">MCI</td>
<td style="text-align: center;">406</td>
<td style="text-align: center;">861</td>
<td style="text-align: center;">$\begin{gathered} 75.5 \pm 7.7 \ (55-93) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 74.2 \pm 8.0 \ (57-91) \end{gathered}$</td>
<td style="text-align: center;">.01</td>
<td style="text-align: center;">$\begin{aligned} &amp; 240 / 406 \ &amp; (59) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 535 / 861 \ &amp; (62) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 57.5 \pm \ &amp; 27.3 \end{aligned}$</td>
<td style="text-align: center;">$67.3 \pm 31.8$</td>
</tr>
<tr>
<td style="text-align: center;">Non-AD/MCI</td>
<td style="text-align: center;">360</td>
<td style="text-align: center;">764</td>
<td style="text-align: center;">$\begin{gathered} 75.9 \pm 5.8 \ 60-90 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 75.3 \pm 6.2 \ (60-96) \end{gathered}$</td>
<td style="text-align: center;">.14</td>
<td style="text-align: center;">$\begin{aligned} &amp; 165 / 360 \ &amp; (46) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 405 / 764 \ &amp; (53) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 61.7 \pm \ &amp; 32.6 \end{aligned}$</td>
<td style="text-align: center;">$73.9 \pm 37.2$</td>
</tr>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">1002</td>
<td style="text-align: center;">2109</td>
<td style="text-align: center;">$\begin{gathered} 75.9 \pm 7.1 \ (55-93) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 74.9 \pm 7.29 \ 55-96) \end{gathered}$</td>
<td style="text-align: center;">.001</td>
<td style="text-align: center;">$\begin{aligned} &amp; 547 / 1002 \ &amp; (54) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 1225 / 2109 \ &amp; (58) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 53.5 \pm \ &amp; 29.8 \end{aligned}$</td>
<td style="text-align: center;">$62.5 \pm 35.1$</td>
</tr>
<tr>
<td style="text-align: center;">B: Independent Set</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Average Age (y)*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Clinical Diagnosis</td>
<td style="text-align: center;">No. of Patients</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Male</td>
<td style="text-align: center;">Female</td>
<td style="text-align: center;">$P$ Value</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Male Sex ${ }^{\dagger}$</td>
<td style="text-align: center;">Average Follow-up (mo)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">AD</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{gathered} 68.7 \pm 9.4 \ (56-79) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 73.0 \pm 0.0 \ (73-73) \end{gathered}$</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6/7 (86)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">82.1</td>
</tr>
<tr>
<td style="text-align: center;">MCI</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{gathered} 63.3 \pm 15.7 \ (48-83) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 68.0 \pm 0.0 \ (68-68) \end{gathered}$</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6/7 (86)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">75.1</td>
</tr>
<tr>
<td style="text-align: center;">Non-AD/MCI</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{gathered} 65.5 \pm 8.9 \ (55-84) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 70.8 \pm 1.3 \ (41-84) \end{gathered}$</td>
<td style="text-align: center;">.21</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">11/26 (42)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">73.5</td>
</tr>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{gathered} 65.8 \pm 10.8 \ (48-84) \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 70.8 \pm 10.7 \ (41-84) \end{gathered}$</td>
<td style="text-align: center;">.15</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23/40 (58)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">75.8</td>
</tr>
</tbody>
</table>
<p>Note.-Unless otherwise indicated, data are averages $\pm$ standard deviation. ADNI = Alzheimer's Disease Neuroimaging Initiative, AD = Alzheimer disease, $\mathrm{MCI}=$ mild cognitive impairment, Non-AD/MCI = neither Alzheimer disease nor mild cognitive impairment. NA = not applicable.</p>
<ul>
<li>Data in parentheses are the range.
${ }^{\dagger}$ Data in parentheses are the percentage of male patients.</li>
</ul>
<h2>Model Interpretation: Saliency Map and $t$-SNE Plot</h2>
<p>As shown in the saliency map in Figure 5b and 5c, the second and third sections in the third row demonstrate the most intense signals among the scattered areas of signal. The result indicates their importance in the decision of classifying a patient with AD , which is in line with the clinical implication that more caudal sections in the parietotemporal regions are informative of AD. However, the patterns are not specific enough to extract a unified human-interpretable imaging biomarker, and overall, the saliency map suggests that the deep learning model considered the whole brain when making the prediction, as presented in Figure 5a.</p>
<p>As shown in Figure 6, after dimension reduction with $t$-SNE, the features extracted by Inception V3 network separated the three classes into approximately three clusters. While the cluster of non-AD/MCI was almost pure, the cluster of MCI was mixed with patients with non-AD/MCI and patients with AD, and the cluster of AD was mixed with the other two classes. This gives insight to the behavior of the model at test time: We obtained a high sensitivity rate on AD class because nearly all patients with AD were located in the AD cluster; we obtained a relatively high precision rate on non-AD/MCI class because the non-AD/MCI cluster was almost pure.</p>
<h2>Comparison to Clinical Interpretations</h2>
<p>As reported in Table 2, the sensitivity, specificity, and precision for reader performance were $57 \%$ (four of seven), $91 \%$ (30 of 33),
and $57 \%$ (four of seven) for class AD; $14 \%$ (one of seven), $76 \%$ (25 of 33), and $11 \%$ (one of nine) for class MCI; and $77 \%$ (20 of 26), $71 \%$ (10 of 14), and $83 \%$ (20 of 24) for class non-AD/ MCI. By plotting reader performance and ROC curves for model performance in the same ROC space as in Figure 4b for class AD in independent test set, reader performance lies below the model ROC curve and outside its $95 \%$ CI. While for class MCI and non-AD/MCI, reader performance lies above and below the model ROC curves, respectively, but both within the $95 \%$ CI of the ROC curve. Therefore, compared with radiology readers, the deep learning model performed better, with statistical significance, at recognizing patients who would go on to have a clinical diagnosis of AD. On the independent test set, it also performs better at recognizing patient with neither AD nor MCI, while worse at recognizing patients who would develop MCI but would not advance to AD , though without statistical significance.</p>
<h2>Discussion</h2>
<p>There is a growing number of patients living with AD , and it has been forecasted that more than $2 \%$ of the U.S. population and $1 \%$ of the world's population will have AD by 2050 (22,23). Unfortunately, early identification of those patients who will have a final diagnosis of AD is challenging. The deep learning algorithm developed and tested in our study was shown to be robust across ADNI hold-out test set and independent test set, with $100 \%$ sensitivity ( $95 \%$ CI: $65 \%, 100 \%$ ) for AD</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Receiver operating characteristic (ROC) curves of deep learning model Inception V3 trained on 90% of Alzheimer's Disease Neuroimaging Initiative (ADNI) data and tested on the remaining 10% of ADNI set and independent test set. (a) ROC curves of trained deep learning model tested on the remaining 10% of ADNI set. ROC curve labeled AD (Alzheimer disease) represents the core model performance for distinguishing AD versus all other cases. ROC curves for mild cognitive impairment (MCI) and non-AD/MCI are also reported for technical completeness. (b) ROC curves including the 95% confidence interval of trained deep learning model tested on the independent test set together with reader performance plotted on ROC space. The deep learning algorithm performs statistically significantly better at recognizing patients with AD on the independent test set. The algorithm is also better at recognizing patient with non-AD/ MCI and worse at recognizing patients with MCI, but did not reach statistical significance.</p>
<p>Table 2: Performance Comparison of Deep Learning Algorithm and Radiology Readers</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Sensitivity (%)*</th>
<th>Specificity (%)*</th>
<th>Precision (%)*</th>
<th>F1 Score (%)</th>
<th>No. of Imaging Studies</th>
</tr>
</thead>
<tbody>
<tr>
<td>Deep learning model on 10% ADNI set</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>AD</td>
<td>81 (29/36)</td>
<td>94 (143/152)</td>
<td>76 (29/38)</td>
<td>78</td>
<td>36</td>
</tr>
<tr>
<td>MCI</td>
<td>54 (43/79)</td>
<td>68 (74/109)</td>
<td>55 (43/78)</td>
<td>55</td>
<td>79</td>
</tr>
<tr>
<td>Non-AD/MCI</td>
<td>59 (43/73)</td>
<td>75 (86/115)</td>
<td>60 (43/72)</td>
<td>59</td>
<td>73</td>
</tr>
<tr>
<td>Deep learning model on independent test set</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>AD</td>
<td>100 (7/7)†</td>
<td>82 (27/33)</td>
<td>54 (7/13)</td>
<td>70†</td>
<td>7</td>
</tr>
<tr>
<td>MCI</td>
<td>43 (3/7)†</td>
<td>58 (19/33)</td>
<td>18 (3/17)†</td>
<td>25†</td>
<td>7</td>
</tr>
<tr>
<td>Non-AD/MCI</td>
<td>35 (9/26)</td>
<td>93 (13/14)†</td>
<td>90 (9/10)†</td>
<td>50</td>
<td>26</td>
</tr>
<tr>
<td>Radiology readers on independent test set</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>AD</td>
<td>57 (4/7)</td>
<td>91 (30/33)</td>
<td>57 (4/7)</td>
<td>57</td>
<td>7</td>
</tr>
<tr>
<td>MCI</td>
<td>14 (1/7)</td>
<td>76 (25/33)</td>
<td>11 (1/9)</td>
<td>13</td>
<td>7</td>
</tr>
<tr>
<td>Non-AD/MCI</td>
<td>77 (20/26)</td>
<td>71 (10/14)</td>
<td>83 (20/24)</td>
<td>80</td>
<td>26</td>
</tr>
</tbody>
</table>
<p>Note.—Unless otherwise indicated, data are averages ± standard deviation. ADNI = Alzheimer's Disease Neuroimaging Initiative, AD = Alzheimer disease, MCI = mild cognitive impairment, Non-AD/MCI = neither Alzheimer disease nor mild cognitive impairment. * Numbers in parentheses are raw data used to calculate the percentage. † Numbers indicate higher performance from deep learning algorithm compared with reader performance on independent test set.</p>
<p>prediction on the independent test set. Furthermore, in predicting the final diagnosis of AD on the independent test set, it outperformed three radiology readers in ROC space, with statistical significance. With further validation with larger and more diverse datasets, this algorithm may be able to augment radiologist reader performance and improve the prediction of AD diagnosis, providing an opportunity for early intervention.</p>
<p>Multiple previous studies have shown that the distinctive distribution of areas of cortical hypometabolism on 18F-FDG PET images has implications in differentiating AD or MCI from a normal brain; however, 18F-FDG itself is not a definitive imaging biomarker for AD or MCI. The past decade has produced several tools for the early diagnosis of AD, including increasingly specific biomarkers of the disease (24,25). For example, β-amyloid (Aβ), a marker of AD, can be detected in the cerebral spinal fluid or at imaging with PET by using radiolabeled Aβ ligands, such as 18F-florbetapir, flutemetamol, and florbetaben (3,26,27). However, these innovations are</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p><strong>Figure 5:</strong> Saliency map of deep learning model Inception V3 on the classification of Alzheimer disease. <strong>(a)</strong> A representative saliency map with anatomic overlay in 77-year-old man. <strong>(b)</strong> Average saliency map over 10% of Alzheimer's Disease Neuroimaging Initiative set. <strong>(c)</strong> Average saliency map over independent test set. The closer a pixel color is to the "High" end of the color bar in the image, the more influence it has on the prediction of Alzheimer disease class.</p>
<p>associated with a high cost that may not be reimbursed by a patient's health insurance or may not be universally available; hence, enhancement of the diagnostic and predictive ability of a long-established imaging technique, such as 18F-FDG PET, using a deep learning algorithm offers the opportunity to provide clinically relevant molecular imaging data across a multitude of populations and settings worldwide.</p>
<p>Substantial work in the area of computer-aided diagnosis and risk classification has been performed by using structural imaging of the brain (28,29). But less work has been devoted to applying deep learning methods to functional imaging alone to classify patients with symptoms of dementia. To our knowledge, the method in our present study has not previously been emphasized in the literature. After training the deep learning model on 90% of the ADNI dataset, validation of the model using the remaining 10% of the ADNI 10% hold-out dataset yielded discrimination of AD of more than 90% as shown by the AUC. Notably, the pooled sensitivity and specificity of 18F-FDG PET imaging in identifying mild AD as the cause of a patient's symptoms across several studies are reported as 90% and 89%, respectively (30–32).</p>
<p>Application of the model to standard clinical 18F-FDG PET imaging studies performed on a cohort of patients for the indication of memory loss (referred to as independent test set) yielded high predictive ability for those patients who were ultimately diagnosed with AD (92% in ADNI test set and 98% in the independent test set) and those who were non-AD/MCI (73% in ADNI test set and 84% in the independent test set). Arguably, these two groups are the most important to classify correctly. However, the model's predictive ability for those patients who were ultimately diagnosed with MCI was lower (63% in ADNI test set and 52% in the independent test set). This is not unexpected given the high degree of variability in the diagnosis of MCI and its existence on a continuum with AD. The lower diagnostic power can also be caused by the fact that patients who carried final diagnosis of MCI may have been at a state too early to show clinical signs of AD or may be those who will not progress to AD.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p><strong>Figure 6:</strong> Visualization of training set after dimension reduction with <em>t</em>-distributed stochastic neighbor embedding (<em>t</em>SNE). Each dot represents the 1024 features output by the final fully connected layer of the Inception V3 network. Red dots represent samples from Alzheimer disease (AD), green dots represent samples from mild cognitive impairment (MCI), and blue dots represent samples from neither classes (non-AD/MCI).</p>
<p>It is noteworthy that model visualization with saliency map did not reveal a distinctly human interpretable imaging biomarker that appears influential for AD prediction. Instead, the deep learning algorithm apparently utilized the whole brain with varying degrees of influence from various anatomic areas to make its final decision. This highlights the strength of the deep learning algorithm that considers the brain as a pixel-by-pixel volume in its classification, implying</p>
<p>that the deep learning algorithm arrives at the diagnosis distinct from how humans interpret the imaging studies.</p>
<p>Our study had several limitations. First, our independent test data were relatively small $(n=40)$ and were not collected as part of a clinical trial. Most notably, this was a highly selected cohort in that all patients must have been referred to the memory clinic and neurologist must have decided that a PET study of the brain would be useful in clinical management. This effectively excluded most non-AD neurodegenerative cases and other neurologic disorders such as stroke that could affect memory function. Arguably, such cohort of patients would be the most relevant group to test the deep learning algorithm, but the algorithm's performance on a more general patient population remains untested and unproven, hence the pilot nature of this study.</p>
<p>Second, the deep learning algorithm's robustness is inherently limited by the clinical distribution of the training set from ADNI. The algorithm achieved strong performance on a small independent test set, where the population substantially differed from the ADNI test set; however, its performance and robustness cannot yet be guaranteed on prospective, unselected, and real-life scenario patient cohorts. Further validation with larger and prospective external test set must be performed before actual clinical use. Furthermore, this training set from ADNI did not include non-AD neurodegenerative cases, limiting the utility of the algorithm in such patient population. Third, the deep learning algorithm did not yield a human interpretable imaging biomarker despite visualization with saliency map, which highlights the inherent black-box limitation of deep learning algorithms. The algorithm instead made predictions based on holistic features of the imaging study, distinct from the human expert approaches. Fourth, MCI and non-AD/MCI were inherently unstable diagnoses in that their accuracy is dependent on the length of follow-up. For example, some of the MCI patients, if followed up for long enough time, may have eventually progressed to AD.</p>
<p>Overall, our study demonstrates that a deep learning algorithm can predict the final diagnosis of AD from ${ }^{18} \mathrm{~F}$ FDG PET imaging studies of the brain with high accuracy and robustness across external test data. Furthermore, this study proposes a working deep learning approaches and a set of convolutional neural network hyperparameters, validated on a public dataset, that can be the groundwork for further model improvement. With further large-scale external validation on multi-institutional data and model calibration, the algorithm may be integrated into clinical workflow and serve as an important decision support tool to aid radiology readers and clinicians with early prediction of AD from ${ }^{18} \mathrm{~F}$ FDG PET imaging studies.</p>
<p>Author contributions: Guarantors of integrity of entire study, J.H.S., Y.S., M.H.P., D.H., B.L.F.: study concepts/study design or data acquisition or data analysis/interpretation, all authors; manuscript drafting or manuscript revision for important intellectual content, all authors; approval of final version of submitted manuscript, all authors; agrees to ensure any questions related to the work are appropriately resolved, all authors; literature research, J.H.S., R.H., R.R.F., K.A.Z., L.N., D.H., B.L.F.: clinical studies, J.H.S., H.T., N.W.J., M.S.A., C.M.A., S.C.B., R.R.F., R.A.H., M.H.P., D.H., B.L.F.: statistical analysis, Y.D., J.H.S., R.H., N.W.J., D.L.,
T.P.C., K.A.Z., R.A.H., D.H., and manuscript editing, Y.D., J.H.S., H.T., N.W.J., S.C.B., K.A.Z., L.N., Y.S., M.H.P., D.H., B.L.F.</p>
<p>Disclosures of Conflicts of Interest: Y.D. disclosed no relevant relationships. J.H.S. Activities related to the present article: received grants from UCSF. Activities not related to the present article: disclosed no relevant relationships. Other relationships: disclosed no relevant relationships. M.G.K. Activities related to the present article: disclosed no relevant relationships. Activities not related to the present article: employed by Genentech and has stock options from Roche. Other relationships: disclosed no relevant relationships. H.T. disclosed no relevant relationships. R.H. Activities related to the present article: disclosed no relevant relationships. Activities not related to the present article: employed by UCSF Radiology department as a programmer, analyst, algorithm developer. Other relationships: disclosed no relevant relationships. N.W.J. disclosed no relevant relationships. D.L. Activities related to the present article: received payment from NVIDIA for travel to meetings. Activities not related to the present article: received payment from HUST Science Forum UC Berkeley for lectures including service on speakers bureaus. Other relationships: disclosed no relevant relationships. T.P.C. disclosed no relevant relationships. M.S.A. disclosed no relevant relationships. C.M.A. disclosed no relevant relationships. S.C.B. Activities related to the present article: disclosed no relevant relationships. Activities not related to the present article: institution received research grant from GE Healthcare. Other relationships: disclosed no relevant relationships. R.R.F. disclosed no relevant relationships. S.Y. H. disclosed no relevant relationships. K.A.Z. disclosed no relevant relationships. L.N. disclosed no relevant relationships. Y.S. disclosed no relevant relationships. R.A.H. disclosed no relevant relationships. M.H.P. disclosed no relevant relationships. D.H. disclosed no relevant relationships. B.L.F. disclosed no relevant relationships.</p>
<h2>References</h2>
<ol>
<li>Jack CR Jr, Albert MS, Knopman DS, et al. Introduction to the recommendations from the National Institute on Aging-Alzheimer's Association workgroups on diagnostic guidelines for Alzheimer's disease. Alzheimers Dement 2011;7(3): $257-262$.</li>
<li>McKhann GM, Knopman DS, Chertkow H, et al. The diagnosis of dementia due to Alzheimer's disease: recommendations from the National Institute on AgingAlzheimer's Association workgroups on diagnostic guidelines for Alzheimer's disease. Alzheimers Dement 2011;7(3):263-269.</li>
<li>Scheltens P, Blennow K, Bretzler MMB, et al. Alzheimer's disease. Lancet 2016; 388(10043):505-517.</li>
<li>Matsuda H. The role of neuroimaging in mild cognitive impairment. Neuropathology 2007;27(6):570-577.</li>
<li>Mosconi L, Tsui WH, Herholz K, et al. Multicenter standardized 18F-FDG PET diagnosis of mild cognitive impairment, Alzheimer's disease, and other dementias. J Nucl Med 2008;49(3):390-398.</li>
<li>Wang S, Summers RM. Machine learning and radiology. Med Image Anal 2012; 16(5):933-951.</li>
<li>Dhungel N, Carneiro G, Bradley AP. A deep learning approach for the analysis of masses in mammograms with minimal user intervention. Med Image Anal 2017;37: $114-128$.</li>
<li>Setio AAA, Ciompi F, Litjens G, et al. Pulmonary nodule detection in CT images: false positive reduction using multi-view convolutional networks. IEEE Trans Med Imaging 2016;35(5):1160-1169.</li>
<li>Xu L, Wu X, Chen K, Yao L. Multi-modality sparse representation-based classification for Alzheimer's disease and mild cognitive impairment. Comput Methods Programs Biomed 2015;122(2):182-190.</li>
<li>Xue Y, Zhang R, Deng Y, Chen K, Jiang T. A preliminary examination of the diagnostic value of deep learning in hip osteoarthritis. PLoS One 2017;12(6): e0178992.</li>
<li>Mueller SG, Weiner MW, Thal LJ, et al. The Alzheimer's disease neuroimaging initiative. Neuroimaging Clin N Am 2005;15(4):869-877, xi-xii.</li>
<li>Alzheimer's Disease Neuroimaging Initiative PET technical procedures manual. http://adni.loni.usc.edu/wp-content/uploads/2010/09/PET-Tech_Procedures_ Manual_v9.5.pdf. Published 2006. Accessed July 18, 2018.</li>
<li>ADNI 2 PET technical procedures manual for FDG and AV-45 ADNI 2 PET technical procedures manual AV-45 (Florbetapir F 18) \&amp; FDG. http://adni.loni. usc.edu/wp-content/uploads/2010/05/ADNI2_PET_Tech_Manual_0142011.pdf. Published 2011. Accessed July 18, 2018.</li>
<li>ADNI-GO PET technical procedures manual for FDG and AV-45 ADNI-GO PET Technical Procedures Manual. http://adni.loni.usc.edu/wp-content/uploads/ 2010/05/ADNIGO_PET_Tech_Manual_01142011.pdf. Published 2011. Accessed July 18, 2018.</li>
<li>Szegedy C, Vazhoucke V, Ioffe S, Shlens J, Wojna Z. Rethinking the inception architecture for computer vision. IEEE Conference on Computer Vision and Pattern Recognition, 2015; 2818-2826.</li>
<li>Kingma DP, Ba J. Adam: A method for stochastic optimization. https://arxiv.org/ abs/1412.6980. Published December 22, 2014. Accessed February 20, 2018.</li>
<li>
<p>Simonyan K, Vedaldi A, Zisserman A. Deep inside convolutional networks: visualis ing image classification models and saliency maps. https://arxiv.org/abs/1312.6034. Published December 20, 2013. Accessed March 18, 2018.</p>
</li>
<li>
<p>van der Maaten L, Hinton G. Visualizing data using t-SNE. J Mach Learn Res 2008;9(Nov):2579-2605.</p>
</li>
<li>Pedregosa F, Varoquaux G, Gramfort A, et al. Scikit-learn: machine learning in Python. J Mach Learn Res 2011;12(Oct):2825-2830.</li>
<li>Robin X, Turck N, Hainard A, et al. pROC: an open-source package for R and S+ to analyze and compare ROC curves. BMC Bioinformatics 2011;12(1):77.</li>
<li>The R Core Team. R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing, 2018.</li>
<li>Brookmeyer R, Gray S, Kawas C. Projections of Alzheimer's disease in the United States and the public health impact of delaying disease onset. Am J Public Health 1998;88(9):1337-1342.</li>
<li>Brookmeyer R, Johnson E, Ziegler-Graham K, Arrighi HM. Forecasting the global burden of Alzheimer's disease. Alzheimers Dement 2007;3(3):186-191.</li>
<li>Nordberg A. Dementia in 2014. Towards early diagnosis in Alzheimer disease. Nat Rev Neurol 2015;11(2):69-70.</li>
<li>Wexman E, Muehlbrock J-S, Simmons A. Combining MRI and CSF measures for classification of Alzheimer's disease and prediction of mild cognitive impairment conversion. Neuroimage 2012;62(1):229-238.</li>
<li>McKhann G, Drachman D, Folstein M, Katzman R, Price D, Stadlan EM. Clinical diagnosis of Alzheimer's disease: report of the NINCDS-ADRDA Work Group
under the auspices of Department of Health and Human Services Task Force on Alzheimer's Disease. Neurology 1984;34(7):939-944.</li>
<li>Morris E, Chalkidou A, Hammers A, Peacock J, Summers J, Keevil S. Diagnostic accuracy of (18)F amyloid PET tracers for the diagnosis of Alzheimer's disease: a systematic review and meta-analysis. Eur J Nucl Med Mol Imaging 2016;43(2):374-385.</li>
<li>Nie D, Zhang H, Adeli E, Liu L, Shen D. 3D deep learning for multi-modal im-aging-guided survival time prediction of brain tumor patients. Med Image Comput Comput Assin Interv 2016;9901:212-220.</li>
<li>Shen W, Zhou M, Yang F, Yang C, Tian J. Multi-scale convolutional neural networks for lung nodule classification. Inf Process Med Imaging 2015;24:588-599.</li>
<li>Bloudek LM, Spackman DE, Blankenburg M, Sullivan SD. Review and metaanalysis of biomarkers and diagnostic imaging in Alzheimer's disease. J Alzheimers Dis 2011;26(4):627-645.</li>
<li>Marcus C, Mena E, Subramaniam RM. Brain PET in the diagnosis of Alzheimer's disease. Clin Nucl Med 2014;39(10):e413-e422; quiz e423-e426.</li>
<li>Shivamurthy VK, Tahari AK, Marcus C, Subramaniam RM. Brain FDG PET and the diagnosis of dementia. AJR Am J Roentgenol 2015;204(1):W76-W85.</li>
</ol>            </div>
        </div>

    </div>
</body>
</html>
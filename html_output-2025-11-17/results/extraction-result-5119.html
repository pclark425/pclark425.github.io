<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5119 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5119</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5119</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-5021fd710fd17dee53bc7bc7bf334b148ef3d8b6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5021fd710fd17dee53bc7bc7bf334b148ef3d8b6" target="_blank">LogicInference: A New Dataset for Teaching Logical Inference to seq2seq Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> LogicInference is presented, a new dataset to evaluate the ability of models to perform logical inference using propositional logic and a small subset of first-order logic, represented both in semi-formal logical notation, as well as in natural language.</p>
                <p><strong>Paper Abstract:</strong> Machine learning models such as Transformers or LSTMs struggle with tasks that are compositional in nature such as those involving reasoning/inference. Although many datasets exist to evaluate compositional generalization, when it comes to evaluating inference abilities, options are more limited. This paper presents LogicInference, a new dataset to evaluate the ability of models to perform logical inference. The dataset focuses on inference using propositional logic and a small subset of first-order logic, represented both in semi-formal logical notation, as well as in natural language. We also report initial results using a collection of machine learning models to establish an initial baseline in this dataset.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5119.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5119.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5.1.1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5.1.1 sequence-to-sequence Transformer (small/base/large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequence-to-sequence Transformer family (T5.1.1) evaluated by fine-tuning and training-from-scratch on the new LOGICINFERENCE dataset to measure strict logical inference and multi-step reasoning (including explicit reasoning chain generation). Experiments compare three sizes and pre-trained vs from-scratch regimes across IID, OOD, and length generalization splits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the limits of transfer learning with a unified text-to-text transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5.1.1 (small, base, large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Seq2seq Transformer (T5.1.1) variants used: small (77M params), base (248M), large (783M). Models were either fine-tuned from the public pre-trained T5.1.1 checkpoints (pre-trained for 1M steps, fine-tuned for 20k steps) or trained-from-scratch for 50k steps. Training used AdaFactor, batch size 128, fixed LR 0.001, input/target sequence lengths 256/512.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>77M, 248M, 783M</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LOGICINFERENCE (new dataset introduced in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Synthetic seq2seq dataset for propositional logic + a small subset of first-order logic. Tasks include: (1) language->logic translation, (2a/2b) single-step inferences (formal and natural-language), (3a/3b) multi-step inference chains (formal and natural-language). Models must output final answer and the step-by-step inference chain (optionally the inference rule names). Dataset provides IID, OOD (systematicity), and length (productivity) splits and two output formats: answer-at-beginning (b) or answer-at-end (e).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning pre-trained T5.1.1 checkpoints vs training-from-scratch; seq2seq generation of explicit inference chains and final answer; dataset design choices to discourage spurious cues (variable renaming, premise shuffling, canonical naming for type-1 examples); controlled splits (IID, OOD, length) to probe compositional generalization; two dataset modes (answer at start vs end) to test single-pass vs chain-as-scratchpad behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Sequence-level exact-match accuracy reported (Table 2) on LOGICINFERENCE variants. Representative results (sequence accuracy): For LOGICINFERENCE_b (answer at beginning) trained-from-scratch (50k): IID: small=0.890, base=0.872, large=0.793; OOD: 0.788/0.750/0.640; length: 0.335/0.257/0.187. For LOGICINFERENCE_b pre-trained (fine-tuned 20k): IID: 0.853/0.821/0.886; OOD: 0.774/0.763/0.789; length: 0.384/0.370/0.459. For LOGICINFERENCE_e (answer at end) similar trends: pre-trained large achieves up to 0.905 IID (see Table 2). Detailed breakdown (Table 3) for base model on LOGICINFERENCE_e: per-problem-type sequence accuracy (From-scratch | Pre-trained) across splits: Type1 IID 0.987 | 0.975, OOD 0.973 | 0.980, length 0.075 | 0.393; Type2a IID 0.933 | 0.888, Type2b IID 0.885 | 0.842, Type3a IID 0.839 | 0.722, Type3b IID 0.678 | 0.639; overall base: from-scratch IID 0.859 / OOD 0.779 / length 0.252; pre-trained IID 0.802 / OOD 0.777 / length 0.389. Qualitative findings: pre-training substantially helps on the length (productivity) split and helps the large model across splits; OOD (systematicity) is harder than IID; length split is the hardest for all models; answer-position (beginning vs end) produces only small differences overall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Models struggle on the length (productivity) split (large drop in accuracy); OOD (systematic) generalization is also substantially harder than IID. Training-from-scratch large models can overfit and perform worse than smaller models. Errors concentrate on longer inputs; models sometimes latch onto dataset artifacts (mitigated by renaming/shuffling). Dataset limitations: only a small subset of first-order logic covered, synthetic (automatically generated) natural language lacking human variety, so transfer to natural-text reasoning is not proven. Some variance across runs observed, especially for larger sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Pre-trained vs from-scratch: pre-training improves compositional generalization, particularly on length split and helps the large model; small/base don't uniformly benefit from pre-training on IID/OOD. Model-size: when training from scratch, small/base outperform large (likely overfitting), but with pre-training the large model achieves best performance on several splits. Answer position (b vs e): no clear consistent advantage; mixed results with only small effects. Compared to other inference/nli benchmarks (discussed qualitatively), LOGICINFERENCE is focused on chain generation and longer multi-step symbolic inference—complements SNLI, CLUTRR, EntailmentBank, and math-reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Analyses include: effect of pre-training (improves length split strongly; helps large model), effect of model size (large from-scratch underperforms, likely overfitting), answer-position ablation (b vs e shows minor influence), per-problem-type error analysis (task 3b hardest; type 1 especially hard when training from scratch on length split), accuracy vs input length curve (errors concentrate on longer inputs; Figure 3), and observed run-to-run variance (not averaged across multiple seeds; authors recommend reporting averages in future work). Dataset generation ablations: variable renaming and premise shuffling were necessary to remove spurious cues that earlier versions leaked to models.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>The paper is primarily an evaluation/benchmark paper introducing LOGICINFERENCE and establishing T5.1.1 baselines; it does not propose new architectural modifications or prompting schemes beyond training/fine-tuning and dataset design choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogicInference: A New Dataset for Teaching Logical Inference to seq2seq Models', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Exploring the limits of transfer learning with a unified text-to-text transformer <em>(Rating: 2)</em></li>
                <li>Attention is all you need <em>(Rating: 2)</em></li>
                <li>Compositional generalization in semantic parsing: Pre-training vs. specialized architectures <em>(Rating: 2)</em></li>
                <li>A large annotated corpus for learning natural language inference <em>(Rating: 2)</em></li>
                <li>Explaining answers with entailment trees <em>(Rating: 2)</em></li>
                <li>CLUTRR: A diagnostic benchmark for inductive reasoning from text <em>(Rating: 2)</em></li>
                <li>Analysing mathematical reasoning abilities of neural models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks <em>(Rating: 1)</em></li>
                <li>Compositionality decomposed: How do neural networks generalise? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5119",
    "paper_id": "paper-5021fd710fd17dee53bc7bc7bf334b148ef3d8b6",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "T5.1.1",
            "name_full": "T5.1.1 sequence-to-sequence Transformer (small/base/large)",
            "brief_description": "Sequence-to-sequence Transformer family (T5.1.1) evaluated by fine-tuning and training-from-scratch on the new LOGICINFERENCE dataset to measure strict logical inference and multi-step reasoning (including explicit reasoning chain generation). Experiments compare three sizes and pre-trained vs from-scratch regimes across IID, OOD, and length generalization splits.",
            "citation_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "mention_or_use": "use",
            "model_name": "T5.1.1 (small, base, large)",
            "model_description": "Seq2seq Transformer (T5.1.1) variants used: small (77M params), base (248M), large (783M). Models were either fine-tuned from the public pre-trained T5.1.1 checkpoints (pre-trained for 1M steps, fine-tuned for 20k steps) or trained-from-scratch for 50k steps. Training used AdaFactor, batch size 128, fixed LR 0.001, input/target sequence lengths 256/512.",
            "model_size": "77M, 248M, 783M",
            "logical_reasoning_task": "LOGICINFERENCE (new dataset introduced in this paper)",
            "task_description": "Synthetic seq2seq dataset for propositional logic + a small subset of first-order logic. Tasks include: (1) language-&gt;logic translation, (2a/2b) single-step inferences (formal and natural-language), (3a/3b) multi-step inference chains (formal and natural-language). Models must output final answer and the step-by-step inference chain (optionally the inference rule names). Dataset provides IID, OOD (systematicity), and length (productivity) splits and two output formats: answer-at-beginning (b) or answer-at-end (e).",
            "method_or_approach": "Fine-tuning pre-trained T5.1.1 checkpoints vs training-from-scratch; seq2seq generation of explicit inference chains and final answer; dataset design choices to discourage spurious cues (variable renaming, premise shuffling, canonical naming for type-1 examples); controlled splits (IID, OOD, length) to probe compositional generalization; two dataset modes (answer at start vs end) to test single-pass vs chain-as-scratchpad behavior.",
            "performance": "Sequence-level exact-match accuracy reported (Table 2) on LOGICINFERENCE variants. Representative results (sequence accuracy): For LOGICINFERENCE_b (answer at beginning) trained-from-scratch (50k): IID: small=0.890, base=0.872, large=0.793; OOD: 0.788/0.750/0.640; length: 0.335/0.257/0.187. For LOGICINFERENCE_b pre-trained (fine-tuned 20k): IID: 0.853/0.821/0.886; OOD: 0.774/0.763/0.789; length: 0.384/0.370/0.459. For LOGICINFERENCE_e (answer at end) similar trends: pre-trained large achieves up to 0.905 IID (see Table 2). Detailed breakdown (Table 3) for base model on LOGICINFERENCE_e: per-problem-type sequence accuracy (From-scratch | Pre-trained) across splits: Type1 IID 0.987 | 0.975, OOD 0.973 | 0.980, length 0.075 | 0.393; Type2a IID 0.933 | 0.888, Type2b IID 0.885 | 0.842, Type3a IID 0.839 | 0.722, Type3b IID 0.678 | 0.639; overall base: from-scratch IID 0.859 / OOD 0.779 / length 0.252; pre-trained IID 0.802 / OOD 0.777 / length 0.389. Qualitative findings: pre-training substantially helps on the length (productivity) split and helps the large model across splits; OOD (systematicity) is harder than IID; length split is the hardest for all models; answer-position (beginning vs end) produces only small differences overall.",
            "limitations_or_failure_cases": "Models struggle on the length (productivity) split (large drop in accuracy); OOD (systematic) generalization is also substantially harder than IID. Training-from-scratch large models can overfit and perform worse than smaller models. Errors concentrate on longer inputs; models sometimes latch onto dataset artifacts (mitigated by renaming/shuffling). Dataset limitations: only a small subset of first-order logic covered, synthetic (automatically generated) natural language lacking human variety, so transfer to natural-text reasoning is not proven. Some variance across runs observed, especially for larger sizes.",
            "comparison": "Pre-trained vs from-scratch: pre-training improves compositional generalization, particularly on length split and helps the large model; small/base don't uniformly benefit from pre-training on IID/OOD. Model-size: when training from scratch, small/base outperform large (likely overfitting), but with pre-training the large model achieves best performance on several splits. Answer position (b vs e): no clear consistent advantage; mixed results with only small effects. Compared to other inference/nli benchmarks (discussed qualitatively), LOGICINFERENCE is focused on chain generation and longer multi-step symbolic inference—complements SNLI, CLUTRR, EntailmentBank, and math-reasoning datasets.",
            "ablation_or_analysis_results": "Analyses include: effect of pre-training (improves length split strongly; helps large model), effect of model size (large from-scratch underperforms, likely overfitting), answer-position ablation (b vs e shows minor influence), per-problem-type error analysis (task 3b hardest; type 1 especially hard when training from scratch on length split), accuracy vs input length curve (errors concentrate on longer inputs; Figure 3), and observed run-to-run variance (not averaged across multiple seeds; authors recommend reporting averages in future work). Dataset generation ablations: variable renaming and premise shuffling were necessary to remove spurious cues that earlier versions leaked to models.",
            "additional_notes": "The paper is primarily an evaluation/benchmark paper introducing LOGICINFERENCE and establishing T5.1.1 baselines; it does not propose new architectural modifications or prompting schemes beyond training/fine-tuning and dataset design choices.",
            "uuid": "e5119.0",
            "source_info": {
                "paper_title": "LogicInference: A New Dataset for Teaching Logical Inference to seq2seq Models",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "rating": 2
        },
        {
            "paper_title": "Attention is all you need",
            "rating": 2
        },
        {
            "paper_title": "Compositional generalization in semantic parsing: Pre-training vs. specialized architectures",
            "rating": 2
        },
        {
            "paper_title": "A large annotated corpus for learning natural language inference",
            "rating": 2
        },
        {
            "paper_title": "Explaining answers with entailment trees",
            "rating": 2
        },
        {
            "paper_title": "CLUTRR: A diagnostic benchmark for inductive reasoning from text",
            "rating": 2
        },
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
            "rating": 1
        },
        {
            "paper_title": "Compositionality decomposed: How do neural networks generalise?",
            "rating": 1
        }
    ],
    "cost": 0.01206675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LogicINferENCE: A NEW DATASET FOR TEACHING LOGICAL INFERENCE TO SEQ2SEQ MODELS</h1>
<p>Santiago Ontañón, Joshua Ainslie, Vaclav Cvicek \&amp; Zachary Fisher<br>Google Research<br>Mountain View, CA 94043, USA<br>{santiontanon, jainslie, vcvicek, zachfisher}@google.com</p>
<h4>Abstract</h4>
<p>Machine learning models such as Transformers or LSTMs struggle with tasks that are compositional in nature such as those involving reasoning/inference. Although many datasets exist to evaluate compositional generalization, when it comes to evaluating inference abilities, options are more limited. This paper presents LogicINFERENCE, a new dataset to evaluate the ability of models to perform logical inference. The dataset focuses on inference using propositional logic and a small subset of first-order logic, represented both in semi-formal logical notation, as well as in natural language. We also report initial results using a collection of machine learning models to establish an initial baseline in this dataset.</p>
<h2>1 INTRODUCTION</h2>
<p>It is well known that machine learning models such as Transformers (Vaswani et al., 2017) or LSTMs (Hochreiter \&amp; Schmidhuber, 1997) struggle with tasks that are compositional in nature (Liška et al., 2018; Hupkes et al., 2020; Keysers et al., 2019; Ontanón et al., 2021) such as those involving reasoning/inference. Many datasets have been proposed to show this effect such as SCAN (Lake \&amp; Baroni, 2018) or PCFG (Hupkes et al., 2020) among many others. However, when it comes to evaluating inference, options are more limited. This paper presents LOGICINFERENCE ${ }^{1}$, a new dataset to evaluate the ability of models to perform logical inference. The dataset focuses on inference using propositional logic and a small subset of first-order logic, represented both in semiformal logical notation, and in natural language. LOGICINFERENCE has two main long-term goals: (1) to evaluate the ability of models to perform logical inference, and the degree to which inference chains are real or hallucinated, and (2) to assess whether learning logical inference abilities in the abstract (e.g., getting better in this dataset) would then transfer to other real-world tasks.</p>
<p>Some datasets already exist to assess inference in machine learning models. LOGICINFERENCE is designed to complement those. For example, natural language entailment datasets, such as SNLI (Bowman et al., 2015), involve only single step inferences, as the model only needs to establish the relation between two pieces of text (entails/contradicts/unrelated), but it is not asked to present the reasoning chain (although an extension with simple explanations was recently proposed in Camburu et al. (2018)). CLUTRR (Sinha et al., 2019) asks models to reason about family relation (given a story in natural language with a collection of characters, where some of their family relations are mentioned, the model needs to infer a family relation that was not mentioned). The main drawback of this dataset is that the types of reasoning it covers are limited to reason about family trees/graphs. Another dataset is EntailmentBank (Dalvi et al., 2021), which asks models to construct an entailment tree that supports a hypothesis given some text/corpus. While this dataset is a great resource for evaluation, its limited size ( 1840 instances in total) limits the types of experiments that can be performed on it. Also related are datasets designed to evaluate mathematical reasoning abilities. For example, the mathematics (Saxton et al., 2019) dataset is a relatively large dataset ( 2 m examples) concerning math problems. One difference with our dataset is that the mathematics dataset only contains inputs and final answers, but no reasoning chain that the model has to produce. The GSM8k dataset (Cobbe et al., 2021) is a similar dataset, but that contains reasoning</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>chains for about 8 k simpler math word problems, and the Math32k dataset Wang et al., 2017) is a middle ground, where there is no reasoning chain, but the equation used to produce the final answer is provided as part of the dataset.</p>
<p>The main features of LOGICINFERENCE are:</p>
<ul>
<li>It covers propositional logic and a subset of first-order logic ${ }^{2}$;</li>
<li>The model needs to learn a variety of tasks such as translate language to logic, one-step or multi-step inferences, and doing so in semi-formal logic notation or natural language;</li>
<li>The model is asked for the step-by-step inference chains used to reach the conclusions;</li>
<li>The dataset includes corner cases, such as contradictory premises, questions that do not follow from the premises, or that are obvious (answer already stated in the premise), so that the model learns to handle these cases and learns not to hallucinate inference chains when asked to infer something that cannot be inferred or that is obvious;</li>
<li>The dataset contains several data splits and configurations for better evaluation.</li>
</ul>
<p>Moreover, we acknowledge the dataset has also several important limitations, which we plan to address in the future. First, only a subset of first-order logic is covered, and only symbolic reasoning is involved (no numerical or mathematical reasoning). But second, and most important, the main limitation of the dataset is that the natural language it contains is automatically generated, and hence it does not contain the expected variety and complexity found in natural language written by humans.</p>
<p>In the remainder of this paper, we present the dataset in Section 2, a brief description of how it is generated in Section 3, and finally Section 4 presents the performance of a collection of machine learning models to establish baseline performance for future reference.</p>
<h1>2 THE LOGICINFERENCE DATASET</h1>
<p>LogicInference is a sequence-to-sequence dataset, where both the input and output are strings of text. The input is a question, and the model needs to provide the answer, including any reasoning chain used to generate it. The model may be asked to provide the name of the inference rule used in each step. Examples in the dataset can belong to one of 5 different types: 1 (language to logic), 2a (one step inference), 2b (one step inference in natural language), 3a (inference chains) and 3b (inference chains in natural language) (examples in Table 1). The main motivation for having both $2 \mathrm{a} / 3 \mathrm{a}$ and $2 \mathrm{~b} / 3 \mathrm{~b}$ is that hopefully the model can learn the general principles of inference from both forms of examples, and hence generalize better when confronted with real world tasks after training in this dataset. Appendix B shows more examples of the dataset, illustrating interesting cases.</p>
<p>Splits. The dataset is synthetically generated and the generating script can generate a dataset of arbitrary size. Moreover, we provide support for three different data splits: IID, OOD, and length split. In the IID split, data is randomly split between training and test (we expect models to do well in this split). In the OOD split, data is split such that the model sees different reasoning chains during training and during testing, and hence, it should be a harder generalization challenge. Specifically, it targets the ability of the model to exhibit systematicity (Hupkes et al., 2020). Finally, the length split has the model training on shorter examples, and then test on longer examples. This type of length split targets the ability of the model to exhibit productivity (Hupkes et al., 2020), and is known to be hard for machine learning models (Lake \&amp; Baroni, 2018).</p>
<p>Answer Position. Finally, notice that in the examples in Table 1 the model is asked to provide the final answer of problem types $3 \mathrm{a} / 3 \mathrm{~b}$ in the very first token of the output. This requires the model to assess whether or not an inference chain exists to prove the target clause, with a single forward pass in the case of seq2seq models. One hypothesis we wanted to evaluate is whether giving the model the chance of performing more computation for harder problems (by asking it to generate the inference chain first, and only once the chain is generated, answer yes or not) would allow the model to generalize better. Hence, two versions of the dataset exist, LOGICINFERENCE ${ }<em e="e">{b}$ (which has the answer at the beginning) and LOGICINFERENCE ${ }</em>$ (which has the answer at the end).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Input: Translate the following inference to logic notation: If James were rich, then Susan is playing squash. James is rich. Therefore Susan is playing squash. <br> Output: p -&gt; q. p. Therefore q.</td>
</tr>
<tr>
<td>2a</td>
<td>Input: What can be inferred from the following premises in a single inference step (ignoring inferences that add new predicates or constants)? Name the inference rule being used: forall x_2: Q(x_2). exists x_2: P_2(x_2) -&gt; Q_2(x_2). forall x_2: P_2(x_2). <br> Output: exists x_2: Q_2(x_2) can be inferred via the existential modus ponens rule. forall x_2: Q(x_2) and P_2(x_2) can be inferred via the universal conjunction rule. forall x_2: P_2(x_2) and Q(x_2) can be inferred via the universal conjunction rule.</td>
</tr>
<tr>
<td>2b</td>
<td>Input: What can be inferred from the following premises in a single inference step (ignoring inferences that add new predicates or constants)? David is working if and only if it is cloudy. David is not working. <br> Output: If David works, then it is cloudy. If it is cloudy, then David is working. It is not cloudy.</td>
</tr>
<tr>
<td>3a</td>
<td>Input: Consider the following premises. p &lt;-&gt; p_2. p_2 -&gt; ~q_2. q_2. Can we infer p from them? If possible, name the inference rules being used at each step. <br> Output: Yes, via the following inference chain. From p_2 -&gt; ~q_2, q_2 we can infer ~p_2 via modus tollens. Finally, from p &lt;-&gt; p_2, ~p_2 we can infer ~p via biconditional elimination.</td>
</tr>
<tr>
<td>3b</td>
<td>Input: Consider the following premises. There is at least one x for which if x were an astronaut, then x is climbing a mountain. For all x, if x climbs a mountain, then x is an astronaut. Can we infer the following from them? If we can, name the inference rule being used: There is at least one x for which if x were an astronaut, then x is climbing a mountain. <br> Output: Yes, that is one of the premises.</td>
</tr>
</tbody>
</table>
<p>Table 1: Sample training examples from the LOGICINFERENCE dataset.</p>
<h1>3 DATASET GENERATION</h1>
<p>In order to generate LOGICINFERENCE, the following procedure was used:</p>
<ol>
<li>Let $\mathbb{R}$ be a set of inference rules $\mathbb{R}$, such as modus ponens, modus tollens, etc. Rules include propositional logic ( 17 rules) and rules with universal and existential quantifiers ( 49 rules) (see Appendix A for examples and details).</li>
<li>Given the rules in $\mathbb{R}$, we generate a set of inference problems $\mathbb{P}$, where an inference problem $p=(P, I, C, U)$ is defined by a set of premises $p . P$, a set of potential inference chains $p . I$ and $p . C$ ( $p . I$ containing inference chains to prove some statements, and $p . C$ to disprove some statements), a set of unrelated clauses $p . U$ (clauses that cannot be proven or disproven from the premises). This is done by starting from an inference rule, and randomly choosing other rules that could be used to infer any of the current premises, creating arbitrarily long inference chains/trees. We control for contradictions introduced by this process, but allow some in the dataset to test the model's ability to detect them.</li>
<li>For each inference problem $p$, we generate a set of renaming variations by renaming the propositions, constants and variables appearing in them (to prevent models memorizing patterns associated with variable names). For example, $p \rightarrow q$, could be renamed to $r \rightarrow$ $p_{2}$. This results in an enlarged set of problems $\mathbb{P}_{v}$.</li>
<li>To generate a training example, we pick a variation $p \in \mathbb{P}_{v}$ at random, and then we stochastically pick one of 5 the different problem types to generate an example:
Type 1: language to logic: given a natural language representation of the premises and a potential inference, the model is asked to translate it to a more formal logical notation.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">From Scratch (50k)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Pre-trained (20k)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LOGICINFERENCE $_{\mathrm{b}}$</td>
<td style="text-align: center;">small</td>
<td style="text-align: center;">base</td>
<td style="text-align: center;">large</td>
<td style="text-align: center;">small</td>
<td style="text-align: center;">base</td>
<td style="text-align: center;">large</td>
</tr>
<tr>
<td style="text-align: left;">IID</td>
<td style="text-align: center;">$\mathbf{0 . 8 9 0}$</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.793</td>
<td style="text-align: center;">0.853</td>
<td style="text-align: center;">0.821</td>
<td style="text-align: center;">0.886</td>
</tr>
<tr>
<td style="text-align: left;">OOD</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">$\mathbf{0 . 7 8 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Length</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.257</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.384</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">$\mathbf{0 . 4 5 9}$</td>
</tr>
<tr>
<td style="text-align: left;">LOGICINFERENCE $_{\mathrm{e}}$</td>
<td style="text-align: center;">small</td>
<td style="text-align: center;">base</td>
<td style="text-align: center;">large</td>
<td style="text-align: center;">small</td>
<td style="text-align: center;">base</td>
<td style="text-align: center;">large</td>
</tr>
<tr>
<td style="text-align: left;">IID</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.859</td>
<td style="text-align: center;">0.799</td>
<td style="text-align: center;">0.819</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">$\mathbf{0 . 9 0 5}$</td>
</tr>
<tr>
<td style="text-align: left;">OOD</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.779</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">0.777</td>
<td style="text-align: center;">$\mathbf{0 . 8 1 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Length</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.389</td>
<td style="text-align: center;">$\mathbf{0 . 4 5 7}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Sequence-level accuracy of T5.1.1 models on the different dataset splits.</p>
<p>Type 2a: one step inference: given a set of premises, the model is asked to predict all the possible one step inferences that can be done from them.
Type 2b: one step inference in natural language: same as 2 a , but the input and output are in natural language, rather than in logical notation.
Type 3a: inference chains: given a set of premises and a potential inference, the model is asked whether we can prove the inference or not from the premises, and to provide the inference chain in either case.
Type 3b: inference chains in natural language: same as 3a, but using natural language, rather than formal notation.
5. The IID split dataset is generated by trying to generate up to $n$ ( $\mathrm{n}=200 \mathrm{k}$ in our experiments) examples in this way and filtering for duplicates, and then splitting them randomly $90 \%$ / $10 \%$ between train / test.
6. The $O O D$ split is generated by randomly splitting the set $\mathbb{P}$ into two subsets (one for training, one for testing), and then continuing the process of generation separately for each of those subsets. In this way, we can ensure that, given inference problem, all the examples derived from it are either in the training set or they are all in the test set. This creates a harder generalization challenge, as problems in the test set will have reasoning patterns (sequence of rule applications) not seen during training.
7. Finally, the length split is generated by splitting the examples based on the number of premises their corresponding inference problems have: less or equal to $k$ ( $=4$ in our experiments) premises for training, and more than $k$ for testing. With the default settings, the dataset has 118931 instances for training and 13215 for testing in the IID split, 118822 / 13266 in the OOD split, and 114790 / 17356 in the length split (See Appendix D for more detailed statistics).</p>
<p>Considerations, In problems of type 3a and 3b, the potential inference may be selected from the unrelated set (e.g., Consider the following premises. p. p $\rightarrow$ q. Can we infer r?). This is to prevent the model from making up answers when users ask questions that cannot be answered from the premises. With some probability, problems contain contradictions in the premises (either direct contradictions, or their inferences reach a contradiction). This can happen in problems of types 2a, 2b, 3a and 3b. In this case, the model is supposed to identify this, and respond accordingly (anything can be inferred from a contradiction). Some inference rules can produce an infinite number of inferences, e.g.: if we know $p$, we also know $p \vee q, p \vee r, p \vee s$, etc. In problems of types 2a and 2 b , the model is explicitly told not to introduce new variables or predicates, to prevent this. Finally, sometimes, the potential inference is directly present in the premises. Again these problems are introduced to force the model to detect this and respond accordingly.</p>
<h1>4 BASELINES</h1>
<p>In order to establish some baseline performance in this dataset, we evaluated the performance of a collection of T5 (Raffel et al., 2019) models. Specifically, we evaluated the small, base and large configurations of T5.1.1, which have $77 \mathrm{~m}, 248 \mathrm{~m}$ and 783 m parameters respectively. Moreover, as previous work has shown that pre-training significantly helps in compositional generalization (Furrer et al., 2020), we evaluated both finetuning the public pre-trained checkpoints (for 20k steps), as well as training from scratch (for 50k steps). Although performance was still increasing at the end of</p>
<p>training, it was starting to taper off, as shown in the training curves in Appendix E. We used batch size 128, constant learning rate of 0.001 , the AdaFactor (Shazeer \&amp; Stern, 2018) optimizer, and sequence lengths of 256 tokens for inputs, and 512 for targets. Table 2 show the performance of these models evaluated using sequence level accuracy (percentage of times they predicted the exact ground truth output sequence).
The results show that pre-training (which was known to help compositional generalization) significantly helps in the length split, where the pre-trained models significantly outperforming the non-pre-trained versions. Pre-training also seems to help the large model in all three splits. Interestingly, the small and base models do not seem to benefit from pre-training for the IID or OOD splits. The OOD split (measuring systematicity) was harder than the IID set as expected, but the length split (measuring productivity) was even harder for models. Surprisingly, we did not see any clear difference in between LOGICINFERENCE ${ }<em b="b">{e}$ and LOGICINFERENCE ${ }</em>$ setup, but the other results are mixed). Moreover, detailed analysis shows that trained-from-scratch models struggle in problems of type 1 in the length split, while this is not as pronounced in the pre-trained models (more detailed results in Appendix E). Finally, when training from scratch, we saw the large models perform worse than the smaller models, which might be due to overfitting.}$ (the large model achieves higher performance in IID/OOD with the LOGICINFERENCE ${ }_{e</p>
<h1>5 Conclusions and Future Work</h1>
<p>In this paper we presented LOGICINFERENCE, a new dataset designed to evaluate the inference abilities of seq2seq models, and to investigate if models trained in this dataset transfer inference abilities to real world natural language tasks (which is part of our future work). We evaluated a collection of T5.1.1 models on our dataset in order to establish an initial baseline performance as a reference for future work. As part of our future work, we would like to improve the dataset by including a larger subset of first order logic, and improve the natural language in the dataset by having humans rephrase a subset of the automatically generated inputs/output pairs in the dataset.</p>
<h2>REFERENCES</h2>
<p>Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015.</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language inference with natural language explanations. Advances in Neural Information Processing Systems, 31, 2018.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. The devil is in the detail: Simple tricks improve systematic generalization of transformers. arXiv preprint arXiv:2108.12284, 2021.</p>
<p>Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. Explaining answers with entailment trees. arXiv preprint arXiv:2104.08661, 2021.</p>
<p>Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Schärli. Compositional generalization in semantic parsing: Pre-training vs. specialized architectures. arXiv preprint arXiv:2007.08970, 2020.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$.</p>
<p>Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed: How do neural networks generalise? Journal of Artificial Intelligence Research, 67:757-795, 2020.</p>
<p>Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, et al. Measuring compositional generalization: A comprehensive method on realistic data. In International Conference on Learning Representations, 2019.</p>
<p>Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International Conference on Machine Learning, pp. 2873-2882. PMLR, 2018.</p>
<p>Adam Liška, Germán Kruszewski, and Marco Baroni. Memorize or generalize? searching for a compositional rnn in a haystack. arXiv preprint arXiv:1802.06467, 2018.</p>
<p>Santiago Ontanón, Joshua Ainslie, Vaclav Cvicek, and Zachary Fisher. Making transformers solve compositional tasks. arXiv preprint arXiv:2108.04378, 2021.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019.</p>
<p>Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pp. 4596-4604. PMLR, 2018.</p>
<p>Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L Hamilton. Clutrr: A diagnostic benchmark for inductive reasoning from text. arXiv preprint arXiv:1908.06177, 2019.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.</p>
<p>Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 845-854, 2017.</p>
<h1>A APPENDIX: INFERENCE RULE DEFINITION</h1>
<p>There are 17 propositional rules and 2 base quantified rules defined in the dataset generation script. Each rule is defined as a 5-tuple: $(P, I, C, U$, name $)$, where $P, I, C$, and $U$ have the same meaning as for the inference problem definition above (premises, inferences, contradictions, unrelated), and name is the name of the inference rule. For example, modus ponens is defined as follows:</p>
<p>$$
\left(\begin{array}{cccc}
P &amp; = &amp; {p \rightarrow q, p} \
I &amp; = &amp; {q} \
C &amp; = &amp; {\neg q} \
U &amp; = &amp; {r, \neg r} \
\text { name } &amp; = &amp; \text { "modus ponens" }
\end{array}\right)
$$</p>
<p>which, in the Python definition looks as follows:</p>
<div class="codehilite"><pre><span></span><code>([[&quot;=&gt;&quot;, [&quot;P&quot;], [&quot;q&quot;]], [&quot;P&quot;]],
    [[&quot;q&quot;]],
    [[&quot;^&quot;, [&quot;q&quot;]]],
    [[&quot;r&quot;], [&quot;^&quot;, [&quot;r&quot;]]],
    [&quot;P&quot;, &quot;q&quot;, &quot;r&quot;],
    &quot;modus ponens&quot;)
</code></pre></div>

<p>Notice that in the Python version there is an additional field (right before the name) that contains a list of all the atomic propositions involved in the rules. This is included for convenience, to prevent having to search them every time.</p>
<p>In addition to the 17 propositional rules, there are 2 quantified rules (universal instantiation, and existential generalization). Universal instantiation looks as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="p">([[</span><span class="s2">&quot;forall&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;var x&quot;</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;P&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;var x&quot;</span><span class="p">]]],</span>
<span class="p">[[</span><span class="s2">&quot;P&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;a&quot;</span><span class="p">]],</span>
<span class="p">[[</span><span class="s2">&quot;^&quot;</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;P&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;a&quot;</span><span class="p">]]],</span>
<span class="p">[[</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;a&quot;</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;^&quot;</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;a&quot;</span><span class="p">]]],</span>
<span class="p">[[</span><span class="s2">&quot;P&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;var x&quot;</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;P&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;a&quot;</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;a&quot;</span><span class="p">]],</span>
<span class="s2">&quot;universal instantiation&quot;</span><span class="p">)</span>
</code></pre></div>

<p>Finally, from each of the 17 propositional rules, we generate one universally quantified rule automatically by turning each predicate $p$ into $P(x)$ and adding a universal quantifier. We also generate several existentially quantified rules by universally quantifying all premises but one, and existentially quantifying one of them. Then the conclusions are existentially quantified. This generates 47 additional rules, resulting in $17+2+47=66$ total rules used for generation of inference problems, which form the set $\mathbb{R}$.</p>
<h2>B APPENDIX: MORE DATASET EXAMPLES</h2>
<p>We include here a few more examples from the dataset, to illustrate some of its features.</p>
<h2>B. 1 LOGICINFERENCE $<em e="e">{5}$ VS LOGICINFERENCE $</em>$</h2>
<p>Here is the same training example for both versions of the dataset. This is how it looks in LOGICINFERENCE $_{5}$ (with the answer at the beginning):</p>
<p>Input: Consider the following premises. forall x_3: Q_1(x_3). forall x_3:
$\sim$ P_3(x_3). forall x_3: P_3(x_3) or Q_3(x_3). Can we infer exists x_3: Q_3(x_3) from them?</p>
<p>Output: No, we can see why via the following inference chain. From forall x_3: P_3(x_3) or Q_3(x_3), forall x_3: P_3(x_3) we can infer forall x_3: Q_3(x_3). From forall x_3: Q_3(x_3), forall x_3: Q_1(x_3) we can infer forall x_3: Q_3(x_3) and</p>
<p>Q_1(x_3). Finally, from forall x_3: Q_3(x_3) and Q_1(x_3) we can infer forall x_3: Q_3(x_3), which contradicts exists x_3: "Q_3(x_3).</p>
<p>This is how it looks in LOGICINFERENCE ${ }_{c}$ (with the answer at the end):
Input: Consider the following premises. forall x_3: Q_1(x_3). forall x_3: "P_3(x_3). forall x_3: P_3(x_3) or Q_3(x_3). Can we infer exists x_3: "Q_3(x_3) from them?</p>
<p>Output: From forall x_3: P_3(x_3) or Q_3(x_3), forall x_3: "P_3(x_3) we can infer forall x_3: Q_3(x_3). From forall x_3: Q_3(x_3), forall x_3: Q_1(x_3) we can infer forall x_3: Q_3(x_3) and Q_1(x_3). Finally, from forall x_3: Q_3(x_3) and Q_1(x_3) we can infer forall x_3: Q_3(x_3), which contradicts exists x_3: "Q_3(x_3). Therefore, the answer is no.</p>
<p>Notice that the input is the same, and the only change in the output is that the answer ("No") appears either in the first token, or in the last token of the output. The main goal of doing this variation is as follows: if the answer is in the first token, the model is forced to already predict (with a single forward pass of the Transformer encoder in our experiments) the final answer of the problem, regardless of how long is the inference chain required to assess this answer. If a human were to solve this problem, she would probably use some scrap paper to write down some inferences before reaching the final answer. We hoped to use the inference chain as this "scrap paper", before reaching the final decision (although our experiments show that this had only a small effect in performance).</p>
<h1>B. 2 Contradictions, Unrelated and Obvious Answers</h1>
<p>As mentioned above, some examples do not require any inference chain, but contain unrelated or obvious questions, or even contradictory premises. We added these examples (in a small proportion) to teach the model that it is not always the case that it has to look for an inference chain, and some times the question is either obvious or does not make sense. Hopefully, this contributes to minimize hallucination when training on this dataset and trying to transfer logical inference skills to another task.</p>
<p>This is an example where the answer is obvious, as it is stated as one of the premises:
Input: Consider the following premises. There is at least one x_1 for which x_1 is a lawyer. For all x_1, x_1 will go running. Can we infer the following from them? If we can, name the inference rule being used: There is at least one x_1 for which x_1 is a lawyer.</p>
<p>Output: Yes, that is one of the premises.
This is an example, where the question is unrelated to the premises:
Input: Consider the following premises. It is not overcast. It is overcast if and only if Joseph is happy. Can we infer the following from them? If we can, name the inference rule being used: Karen is not an electrician.</p>
<p>Output: No, we cannot infer that from the premises.
And finally, this is an example where the premises contain a contradiction:
Input: Consider the following premises. For all x_4, x_4 is an electrician. Joseph is not an electrician. Can we infer the following from them? If we can, name the inference rule being used: Joseph is not making tea.</p>
<p>Output: Yes, the premises are contradictory, so we can infer anything from them.</p>
<h2>C APPENDIX: DATASET GENERATION</h2>
<p>This section presents some additional details on dataset generation. Specifically, we provide some additional details on how inference problems are created, and how clauses are translated to natural language.</p>
<h1>C. 1 Inference Problem Generation</h1>
<p>As mentioned in the paper, an inference problem is defined as a tuple $p=(P, I, C, U)$, with a set of premises, inferences, contradictions and unrelated clauses. Inference problems are the starting point from which training examples are generated.</p>
<p>To generate an inference problem, we basically create random inference chains by chaining some of the defined inference rules above. Specifically, we use the following procedure:</p>
<ol>
<li>One inference rule $r_{0}$ is selected at random.</li>
<li>From $r_{0}$, we can form an inference problem $p$ as follows: the premises of the problem are the same as those in the rule: $p . P=r_{0} . P$. For each inference in the rule, we construct an inference chain of length one in the problem: $p . I=\left{\left(c,\left[\left(r_{0} \cdot P, c, r_{0} \cdot\right.\right.\right.\right.$ name $\left.\left.)\right]\right) \mid c \in r_{0} . I}$ (each inference chain consists of the final conclusion, and a list of triples: (premises, conclusion, inference rule name) with the inference steps). We do the same with the contradictions: $p . C=\left{\left(c,\left[\left(r_{0} \cdot P, c, r_{0}\right)\right]\right) \mid c \in r_{0} . C\right}$. The set of unrelated clauses are also taken directly from the rule: $p . U=r_{0} . U$.</li>
<li>At this point, we can now take any of the premises $c \in p . P$, and choose a random rule $r_{1} \in \mathbb{R}$ that could be used to infer $c$. Then, we remove $c$ from $p . P$, and add the premises of $r_{1}$ to $p . P$. We then extend all the inference chains in $p . I$ and $p . C$ by adding one initial step using $r_{1}$ All variables in $r_{1}$ are renamed to unique names before using it to update the inference problem, to prevent any name clashes. This step 3 can be iterated as many times as desired to create arbitrarily long inference chains.</li>
<li>Finally, as the previous process just creates arbitrary inference chains, it is possible that the generated chains are redundant (inferring things that are already known), or even introducing contradictions. Thus, a final step to simplify inference chains (any inference step that infers something that was already known is removed, and if the statement that wants to be proven or contradicted is inferred earlier, then the inference chain is cut at that point). As mentioned, inference problems might be created in a way that their premises lead to contradictions. If any such problem is generated, it is stored on a separate buffer, and at the end of inference problem generation, some of those problems with contradictions (up to a user specified proportion) are added back to the set of inference problems, so that we have, up to the desired fraction of problems with contradictions (by default, we limit to at most $10 \%$ of inference problems containing contradictions, but in reality many less than that are generated.</li>
</ol>
<p>The process that generates an inference problem receives a hand-defined probability distribution over the possible problem lengths (number of times we iterate step 3 above) that we want, and this is used for generating inference problems. By default, we use the following distribution: [0.425, 0.3, $0.2,0.05,0.025]$ (for $0,1,2,3,4$ and 5 applications of step 3 respectively).</p>
<h2>C. 2 Translation to Natural Language</h2>
<p>Translation of a clause to natural language follows a set of patterns:</p>
<ul>
<li>Atoms of the form p, q, etc. get translated to one of these forms:</li>
<li>"subject verb-action" (e.g., Mary plays tennis),</li>
<li>"Subject predicate" (e.g., Mary is happy),</li>
<li>"Impersonal-action" (e.g., It is raining)</li>
<li>There is a set of predefined subjects, verb-actions, predicates, and impersonal actions and they are sampled randomly (but without replacement within the same training example, to prevent repetitions).</li>
<li>When an atom is of the form $\mathrm{P}(\mathrm{e}), \mathrm{Q}(\mathrm{e})$, etc. then only the patterns with subjects above are used and e is mapped to the subject, and P/Q to the verb-action/predicate.</li>
<li>
<p>When an atom is of the form $\mathrm{P}(\mathrm{x}), \mathrm{Q}(\mathrm{x})$, etc. then only the patterns with subjects above are used and the subject is just rendered as $x$ (since $x$ is a variable here). We acknowledge that this step could use better translation to natural language.</p>
</li>
<li>
<p>Each atom can be rendered in several modes (present, past, negated, etc.) to be used in the patterns below.</p>
</li>
<li>Or, and, implication and biconditional, have patterns like X or Y , if X then Y , etc.</li>
<li>Quantified clauses also have patterns: For all x, X and There is at least one x for which X.</li>
<li>Finally, there is a special case for existentially quantified rules of the form exists $x, P(x)$ and $Q(x)$ that renders it as some Xs are Y (where X and Y are the predicates associated with P and Q respectively).</li>
</ul>
<p>In the generation script there are currently 20 possible subjects (the 10 most common male and 10 most common female names in English), 30 possible predicates, 15 possible actions, and 8 possible impersonal-actions. For example, p $-&gt;$ q, could be translated to If John plays Tennis then it will snow.</p>
<h1>C. 3 Additional Considerations</h1>
<p>During the development of LOGICINFERENCE, we observed that models obtained much better performance in the first versions of the dataset, which was surprising to us. We realized that the generation pipeline had certain biases which models were latching onto for predictions. For example:</p>
<ul>
<li>Due to the way problems are generated, variable names were indicative of the particular inference chain that had used. Each time a rule is applied for generation, there is an internal counter (that starts at 1), and all variables in a rule are then renamed based on that counter (p becomes p_1, etc.). We noticed that models were able to identify patterns based on these numbers, and that's why we introduced the variable renaming variations. This made the problem harder, as removed these types of clues.</li>
<li>In addition to variable renaming variations, we had to randomly shuffle the premises of a problem each time a new example was being generated, as the order in which the premises appear in a problem also contained clues that models were latching onto.</li>
<li>For problems of type 1, there are many ways in which a problem can be translated to semiformal notation. For example, if a natural language statement, can be translated to p -&gt; q, then it could also be translated to $\mathrm{r}-&gt;\mathrm{s}$, as it's just a matter of which names do we decide to use to assign to each predicate. This was making evaluation complex, as models were proposing translations that were correct, but different from that present in the dataset. To solve this problem, we defined a canonical naming convention, where the first atomic proposition that appears is always called p , the second always called q , etc. This was used only for problems of type 1.</li>
</ul>
<h2>D APPENDix: DATASET STATISTICS</h2>
<p>The dataset generation script can generate datsets or arbitrary sizes. In this section, we report statistics using the default configuration, which is:</p>
<ul>
<li>Inference chain distribution: $[0.425,0.3,0.2,0.05,0.025]$.</li>
<li>Try to generate up to 5000 inference problems.</li>
<li>Try to generate 25 renaming variations.</li>
<li>Try to generate 200000 training examples.</li>
<li>For IID/OOD split train/test with a 0.9/0.1 ratio.</li>
<li>For the length split, use problems with 4 premises or less as train, and 5 or more as test.</li>
</ul>
<p>With these parameters, the resulting dataset (using random seed 0), has the following statistics (notice that all the splits have less than 200000 training examples, due to duplication removal):</p>
<ul>
<li>IID: 177543 / 19728 instances for train / test.</li>
<li>
<p>OOD: 177578 / 19743 instances for train / test</p>
</li>
<li>
<p>length: 175179 / 22092 instances for train / test.</p>
</li>
</ul>
<p>More detailed statistics for one of the splits (IID):</p>
<ul>
<li>The script generated 4814 different inference problems generated (168 with contradictions).</li>
<li>In these 4814 inference problems, we find (note that these do not add up to 4814 , since each inference problem might have more than one inference chain, and also, we are counting chains in both the inferences and contradictions set):</li>
<li>1079 inference chains of length 0 (appear in premises).</li>
<li>607 inference chains of length 1 (application of a single inference rule).</li>
<li>1347 inference chains of length 2 .</li>
<li>4227 inference chains of length 3 .</li>
<li>2127 inference chains of length 4 .</li>
<li>852 inference chains of length 5 .</li>
<li>77199 renaming variations were generated.</li>
<li>The example distribution among the different problem types was (notice that the low count of problems of type 2 a is because, as they do not depend on inference chains, but only on premises, many were removed due to duplication removal):</li>
<li>1: 24019</li>
<li>2a: 48422</li>
<li>2b: 37794</li>
<li>3a: 49713</li>
<li>3b: 37323</li>
<li>Once tokenized using the T5 default vocabulary, we see the following token length distribution in the training set (min / median / $90 \%$ percentile max):</li>
<li>Input: 18 / 97 / 147 / 244</li>
<li>Output: 2 / 70 / 269 / 582</li>
</ul>
<p>Notice that we use input/output sequence lengths of 256 and 512 respectively for our experiments, so, the longest examples will get cropped. This only happens for less than 100 examples in the IID set, so we decided not to increase the sequence length beyond 256/512 for efficiency.</p>
<p>Moreover, concerning the tokenization, we also would like to note that the characters "&lt;" and "-" are not part of the T5 vocab, and hence are parsed as "unknown". Moreover, since the " $&lt;$ " always appears before a "-", and "-" can never appear before a "-", we can perform a simple string replacement of unknown tokens by these two tokens easily. However, we note that this might cause some small problems for T5, and hence, an extended vocab with these two tokens might obtain slightly better results.</p>
<h1>E APPENDIX: DETAILED RESULTS</h1>
<p>Learning Curves. Figures 1 and 2 show the learning curves corresponding to the results in Table 2. As the curves show, evaluation sequence accuracy still continued to grow at the end of training (except for the length split when training from scratch, which had stabilized). Hence, training for longer would result in better results. Also, although we do not show it, training set accuracy reaches above 0.99 very quickly (even when training from scratch), usually after 20 k iterations when training from scratch. However, as noted by Csordás et al. (2021), test accuracy continues to grow in compositional generalization tasks even after training set accuracy (or even IID validation set accuracy) has saturated.</p>
<p>Accuracy by Problem Type. It is also interesting to see in which types of problems do errors occur. Table 3, shows the performance divided by problem type in the LOGICINFERENCE ${ }_{\mathrm{e}}$ version. We report results for the base model, which generally achieved good results. Table 3 reveals an interesting effect: when training from scratch, models significantly struggle in task 1 ( 0.075 accuracy). Moreover, task 3 b seems to be the harder task overall in the IID and OOD splits.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Training curves for the LOGICINFERENCE ${ }_{e}$ (answer at the beginning) version of the dataset. Vertical axis is sequence level accuracy on the test set, and horizontal axis is training step. left) training from scratch for 50k steps. right) fine-tuning for 20k steps starting from the public T5.1.1 pre-trained checkpoints (pre-trained for 1 m steps).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Training curves for the LOGICINFERENCE ${ }_{e}$ (answer at the end) version of the dataset. Vertical axis is sequence level accuracy on the test set, and horizontal axis is training step. left) training from scratch for 50k steps. right) fine-tuning for 20k steps starting from the public T5.1.1 pre-trained checkpoints (pre-trained for 1 m steps).</p>
<p>Accuracy by Problem Length. Figure 3 plots the accuracy as a function of the problem input length (in tokens). As the figure shows, examples with shorter inputs are easier to predict (higher accuracy), and errors are concentrated in the longer examples. This shows that models struggle to generalize via productivity. Moreover, notice that in the length split, since examples are split by the</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">From Scratch (50k)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Pre-trained (20k)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Problem Type</td>
<td style="text-align: center;">IID</td>
<td style="text-align: center;">OOD</td>
<td style="text-align: center;">length</td>
<td style="text-align: center;">IID</td>
<td style="text-align: center;">OOD</td>
<td style="text-align: center;">length</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.075</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">0.980</td>
<td style="text-align: center;">0.393</td>
</tr>
<tr>
<td style="text-align: center;">2a</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.416</td>
</tr>
<tr>
<td style="text-align: center;">2b</td>
<td style="text-align: center;">0.885</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.842</td>
<td style="text-align: center;">0.854</td>
<td style="text-align: center;">0.395</td>
</tr>
<tr>
<td style="text-align: center;">3a</td>
<td style="text-align: center;">0.839</td>
<td style="text-align: center;">0.673</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.396</td>
</tr>
<tr>
<td style="text-align: center;">3b</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.564</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.639</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">0.335</td>
</tr>
<tr>
<td style="text-align: center;">overall</td>
<td style="text-align: center;">0.859</td>
<td style="text-align: center;">0.779</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.777</td>
<td style="text-align: center;">0.389</td>
</tr>
</tbody>
</table>
<p>Table 3: Sequence-level accuracy by problem type for the base model on the LOGICINFERENCE ${ }_{e}$ dataset version.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Sequence-level accuracy (vertical axis) as a function of the input length of the problem (in tokens, horizontal axis) for the base models in the LOGICINFERENCE ${ }_{\mathrm{e}}$ version of the dataset.
number of input premises, there are no problems with short inputs in testing, and hence, the curves start more to the right.
Results Variance. As we were only trying to present example baseline performance, we only report accuracy of a single run per model configuration. However, we observed a some degree of variance in the results in our experiments. In some preliminary runs with larger size models ( $x l$ size), we observed that while the best runs achieved better results than those in the table ( 0.940 accuracy in the IID split), the variance problem was more pronounced (small and base seemed to be more stable), and we decided not to include $x l$ results, as that would require multiple runs and reporting averages, increasing the computation cost of the experiments significantly. Previous work on compositional generalization tasks has already reported on this type of variance in some tasks, and hence, this is not particularly surprising. However, the consequence is that future work using this dataset should probably report the average of several runs to prevent this issue.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ A larger subset of first-order logic (FOL) will be included in future revisions of the dataset. Only a small subset was added in this dataset, as the inference mechanisms used to generate it were designed for propositional logic, and we added the small subet of FOL that our code could handle without problems.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1881 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1881</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1881</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-37.html">extraction-schema-37</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <p><strong>Paper ID:</strong> paper-280016234</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.17851v2.pdf" target="_blank">Triadic Novelty: A Typology and Measurement Framework for Recognizing Novel Contributions in Science</a></p>
                <p><strong>Paper Abstract:</strong> Scientific progress depends on novel ideas, but current reward systems often fail to recognize them. Many existing metrics conflate novelty with popularity, privileging ideas that fit existing paradigms over those that challenge them. This study develops a theory-driven framework to better understand how different types of novelty emerge, take hold, and receive recognition. Drawing on network science and theories of discovery, we introduce a triadic typology: Pioneers, who introduce entirely new topics; Mavericks, who recombine distant concepts; and Vanguards, who reinforce weak but promising connections. We apply this typology to a dataset of 41,623 articles in the interdisciplinary field of philanthropy and nonprofit studies, linking novelty types to five-year citation counts using mixed-effects negative binomial regression. Results show that novelty is not uniformly rewarded. Pioneer efforts are foundational but often overlooked. Maverick novelty shows consistent citation benefits, particularly rewarded when it displaces prior focus. Vanguard novelty is more likely to gain recognition when it strengthens weakly connected topics, but its citation advantage diminishes as those reinforced nodes become more central. To enable fair comparison across time and domains, we introduce a simulated baseline model. These findings improve the evaluation of innovations, affecting science policy, funding, and institutional assessment practices.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1881.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1881.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>5yr_citations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Five-year forward citation counts (Web of Science)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Short-term citation counts within a five-year forward window used as the primary dependent measure of recognition in the paper's regression models; analyzed relative to structural novelty scores (Pioneer, Maverick, Vanguard).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>citation metrics (short-term citations)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Triadic novelty types (Pioneer: new topics; Maverick: new distant ties; Vanguard: reinforcement of weak ties) using initial novelty scores and novelty impact scores</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Pioneer: regression coefficient = -0.014 (p = 0.84) — no significant positive effect (i.e., effectively no reward within 5 years); Maverick: main effect coefficient = 0.546 (p < 0.001) indicating substantial positive association with 5-year citations (exp(0.546) ≈ 1.73, ~73% higher expected counts); Vanguard: main effect coefficient = 0.109 (p < 0.001) indicating modest positive association (exp(0.109) ≈ 1.12, ~12% higher expected counts); Vanguard × impact interaction = 0.026 (p < 0.001); Vanguard quadratic interaction = -0.005 (p < 0.001) showing diminishing returns.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>conditional / mixed: Maverick ~ positive approximately linear (log-linear NB coefficient); Vanguard ~ bell-shaped (positive then diminishing returns; significant positive interaction with impact and a negative quadratic term); Pioneer ~ no relationship in 5-year window unless combined with high novelty impact (threshold/conditional effect).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Evaluation measured on 5-year horizon; Pioneer contributions often show little or negative short-term citation effect and require longer, less predictable trajectories for recognition (paper notes Pioneer may gain recognition only when subsequent uptake is high).</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Philantropic and Nonprofit Studies (PNPS) — interdisciplinary spanning Social Sciences, Life Sciences & Biomedicine, Technology, Arts & Humanities, Physical Sciences</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Journal-level heterogeneity is large (random intercept variance for journal J = 1.035) and year variance smaller (J = 0.382); specific journals differ (e.g., VOLUNTAS high on Pioneer novelty but low Pioneer impact; NVSQ low Pioneer novelty but high Pioneer impact), indicating venue-dependent recognition patterns rather than uniform field effect.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>Five-year forward citation counts (WoS-indexed citations)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Novelty impact scores (structural uptake measures) and the authors' interpretation of 'recognition' via five-year citations — no independent external ground truth like Nobel prizes; novelty impact score is treated as descriptive uptake measure.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Pioneer: initial novelty correlates weakly with impact (Spearman ρ = 0.087) and shows no significant citation reward (coef = -0.014, p = 0.84) — signaling under-recognition by short-term citation proxy; Maverick diminishment correlation with impact ρ = 0.29; correlations between triadic novelty and disruption index are small (standardized Pioneer vs disruption ρ = -0.041; raw Maverick vs disruption ρ = 0.399) indicating mismatch between different proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Yes — explicit comparison: transformational (Pioneer) is often undervalued in five-year citations (no significant positive effect), recombinatory/transformative recombination (Maverick) is rewarded strongly (coef = 0.546, p < 0.001), and early reinforcement (Vanguard) yields moderate reward with diminishing marginal returns (coef = 0.109 with significant quadratic negative term).</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>The paper compares multiple proxies (triadic novelty scores vs the disruption index vs citations) and finds limited overlap (low Spearman correlations), i.e., different proxies fail in different ways but do not report compounded multiplicative failure; disruption index is noted as biased by citation inflation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Not applicable — no ML/AI automated decision system tested for paper acceptance; only algorithmic baseline simulation and network-based novelty measures were used.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Not applicable to ML training; however, the authors note citation inflation and preferential attachment in historical data produce structural bias that they adjust for via simulation (see simulated baseline, alpha = 0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Standardization via a simulated baseline model (z-score normalization) that adjusts for citation inflation and preferential attachment; selected attractiveness parameter α = 0.05 produced high agreement (Spearman > 0.7) between simulated and observed temporal novelty trends, enabling cross-year/field standardized novelty comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Examples noted where transformational work was later recognized after long lags (Einstein's gravitational waves, proposed 1916, validated ~100 years later; Higgs boson hypothesis gained prominence only after LHC empirical validation). Empirical journal-level counterexample: NVSQ produced few pioneering articles but those had high Pioneer impact (selective amplification).</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Novelty impact (uptake) strongly moderates citation reward (e.g., Pioneer yields citations only when Pioneer impact high); journal venue (large random effect variance) moderates recognition; radial domain granularity and WoS coverage limit detection (field/coverage bias); five-year window limits detection of slow-burning Pioneer effects.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Analysis of 41,623 articles (published 1960–2017) citing 313,172 unique WoS references; weighted co-occurrence citation network (WCCN) of WoS subject categories; mixed-effects negative binomial regression with random intercepts for journal and year; five-year forward citation window; simulated baseline model (per-year synthetic papers) with preferential attachment and linear attractiveness parameter α = 0.05; standardization via z-scores.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1881.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1881.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Disruption_index</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Disruption index (normalized disruption score)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used bibliometric proxy (disruption index) compared against triadic novelty measures for robustness; the paper reports low correlations between disruption and triadic novelty types and notes disruption's sensitivity to citation inflation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>citation-based disruption metric (bibliometric proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Disruption index (log-transformed, normalized disruption scores) used as an external comparator to triadic novelty measures</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Correlations with triadic measures: standardized Pioneer vs disruption ρ = -0.041; raw Pioneer vs disruption ρ = -0.244; Maverick (raw) vs disruption ρ = 0.399; Vanguard vs disruption small (standardized ρ = 0.034); Vanguard Impact vs disruption negative (standardized ρ = -0.198). The paper cites literature noting disruption index bias due to citation inflation.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>low/weak correlation — disruption does not strongly co-vary with triadic novelty types (i.e., different axes of novelty are captured).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Disruption index is sensitive to citation inflation across time (cited literature and authors note this), producing time-dependent bias unless adjusted; authors standardize triadic novelty against a simulated baseline to avoid this inflation bias.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Used for comparison within PNPS dataset (interdisciplinary field) — results likely generalizable to other fields given prior literature on disruption index biases.</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not quantified by field in this paper beyond correlations; authors note disruption's sensitivity to citation inflation which may differentially affect fields with differing publication growth.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>Disruption index</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Not provided — disruption used as an alternative proxy rather than ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Low concordance with triadic novelty (see above correlations); the disruption index fails to capture Pioneer and Vanguard novelty dimensions strongly (small or negative correlations).</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Indirect: raw Maverick shows some overlap with disruption (ρ = 0.399) suggesting disruption may partially reflect recombinatory novelty, but it poorly represents Pioneer and Vanguard forms.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Paper demonstrates that disruption and triadic measures diverge, and disruption's time-sensitivity (citation inflation) can bias conclusions — indicating multiple proxies can fail in distinct ways rather than uniformly.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>No intervention to alter disruption index; used only as comparator and critique (authors used simulated baseline to standardize triadic novelty instead).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Not specific; general observation that disruption can misrepresent Pioneer novelty (which often shows low short-term citation/disruption scores).</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Citation inflation and publication growth over time (temporal bias) moderate disruption index outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Comparison across the same PNPS article sample (41,623 papers), computing Spearman correlations between triadic novelty measures (raw and standardized) and normalized/log-transformed disruption scores.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1881.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1881.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Journal_venue_effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Journal venue / editorial selection effect (random intercept for journal)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study models journal-level heterogeneity as a random effect and evaluates journal-level standardized novelty (aggregated z-scores) to show how venues differ in publishing and amplifying novel work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>journal decisions / editorial selection and venue visibility</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Average standardized triadic novelty scores per journal (z-scores from simulated baseline) and journal random intercept variance from mixed-effects model</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Random-effect variance for journal = 1.035 (substantial), indicating venue accounts for large heterogeneity in citation performance; journal rankings show VOLUNTAS high in Pioneer novelty but low Pioneer impact, while NVSQ low Pioneer novelty but high Pioneer impact (qualitative contrast).</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>venue moderates recognition — multiplicative/heterogeneous effects modeled via random intercepts; aggregation of standardized novelty highlights journals' differing receptivity (no single functional form across journals).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Journals differ in long-term amplification of novel ideas; standardized scores control for year-to-year citation inflation enabling cross-year comparisons; patterns differ by journal editorial strategy (e.g., exploratory vs selective).</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Journals publishing within PNPS; includes interdisciplinary and field-specific outlets (Table 3 lists top journals for each novelty type).</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Quantitative journal differences reported via standardized average novelty — top-ten lists for each novelty type show that both interdisciplinary and field-specific journals publish novel work; specific cross-field patterns illustrated but not reduced to single numeric ratios across fields.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>Aggregated standardized novelty scores (journal averages) and 5-year citations as outcome controlled by journal random effect</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>None external — journal effect inferred from modeled citation counts and standardized novelty deviations from simulated baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Not expressed as single number; large journal variance (J = 1.035) indicates venue choice can create substantial gap between an article's novelty and observed recognition via citations.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Journals differ in how they publish and amplify novelty types: e.g., VOLUNTAS publishes many Pioneer pieces (exploratory) but these show low Pioneer impact in PNPS; NVSQ selectively publishes fewer Pioneer works which tend to attain higher impact — qualitative contrast evidencing editorial strategy effects.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>By aggregating standardized novelty across journals, the authors show that raw citation metrics alone obscure venue-level receptivity; standardization reduces temporal and structural biases but differences across journals remain large.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Standardization (simulated baseline + z-scores) applied at article level and aggregated to journals to correct structural biases; enabled identification of journals relatively more receptive to different novelty types.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>VOLUNTAS: high Pioneer novelty but low Pioneer impact (exploratory outlet with limited uptake); NVSQ: low Pioneer novelty but high Pioneer impact (selective amplification) — examples of divergent editorial outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Editorial strategy (exploratory vs selective), journal visibility/disciplinary reach, and subject-category composition (authors controlled for WoS domains).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Aggregated standardized novelty across journals with at least 30 PNPS articles; mixed-effects NB regression with (1|Journal) random intercept and journal-level variance reported; full dataset 41,623 articles.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1881.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1881.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simulated_baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulated baseline model for standardization (adjusting for citation inflation and preferential attachment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A per-year simulation that generates synthetic papers mirroring observed counts and subject-category multiplicities using a preferential attachment plus linear attractiveness parameter (α) to create baseline novelty distributions used to z-standardize real novelty scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>algorithmic baseline adjustment for bibliometric evaluation (not an evaluator itself but an adjustment mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Simulated distributions of Pioneer, Maverick, Vanguard novelty scores under random assignment with preferential attachment; real novelty scores standardized (z-scores) against these simulated distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Chosen attractiveness parameter α = 0.05 produced highest agreement; Spearman correlation between simulated and observed temporal novelty trends exceeded 0.7 for most novelty types (authors' selection criterion), indicating that baseline captures structural growth patterns; standardization reveals deviations from expected novelty (z-scores).</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>Standardization converts raw novelty into deviations from expected (z-score), removing predictable growth/attachment effects; relationship is normalization rather than a functional effect mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Model explicitly corrects for temporal bias (citation inflation and publication growth); used per-year simulation (1960–2017) so standardized scores are comparable across time.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Applied to PNPS dataset but designed to be generalizable across fields by preserving per-paper subject-category counts and using preferential attachment based on node strength.</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Standardization adjusts for field/subject-category size differences because each simulated paper is assigned the same number of subject categories as its real counterpart; authors used this to compare journals and fields independent of raw publication volume.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>Raw triadic novelty scores and five-year citation counts are adjusted/standardized using the simulated baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Simulated expectation under structural (random) model used as baseline/gauge; deviations (z-scores) treated as indicators of unusually high/low novelty relative to structural expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Not directly a gap measure but standardization quantified deviations; selection of α = 0.05 yielded Spearman > 0.7 indicating baseline well-matched to trends — enabling detection of anomalies (articles/journals > ~1–2 SD above baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Standardized scores allow fairer cross-time/field comparison of Pioneer versus Maverick versus Vanguard by removing structural growth biases that otherwise favor some types (e.g., recombinations increasing with publications).</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Model addresses some proxy failures (time and preferential attachment) but does not correct for non-WoS citations (books, policy reports) or semantic granularity limits — authors note residual biases remain (coverage, granularity).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Model is an automated simulation (not machine learning); performance judged by Spearman similarity to observed trends (> 0.7) and used to compute per-article z-scores.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Model parameterizes historical preferential attachment (node strengths) — authors tuned attractiveness α to best replicate observed history; this acknowledges and models historical-data-driven bias rather than ML training bias.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Standardization (z-score) is the tested intervention; it enabled cross-year and cross-journal comparisons and identification of journals that publish unusually novel work (Table 3; Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Not direct counterexamples; model selection process (tuning α) highlights that raw novelty trends can be replicated well by structural models, implying many apparent novelty increases may be expected under preferential-attachment dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Choice of α (linear attractiveness) moderates baseline fit; WoS subject-category granularity and coverage moderate what deviations are detectable; temporal publication growth pattern matters for baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Simulated synthetic papers per year matching observed counts for 1960–2017; each simulated paper assigned same number of subject categories as real counterpart; selection probability for a category ∝ node strength(previous year) + α; tested multiple α values and selected α = 0.05 based on Spearman correlations with observed trends; standardized real scores via z = (real - mean_sim)/sd_sim.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1881.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1881.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Peer_review_and_funding_bias_mentions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Literature mentions of bias in peer review and funding against novelty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited literature and conceptual discussion noting that novel ideas often lack legitimacy and are less likely to receive funding or recognition; used to motivate the study rather than empirically tested within this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>peer review / funding decisions (literature mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Qualitative statements and previous studies referenced (e.g., 'Bias Against Novelty in Science' and other literature), not operationalized in this paper's dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Qualitative claim: novel ideas are 'often ignored, undervalued, or even punished' — no numeric acceptance-rate or funding-rate estimate is provided in this paper (references cited contain empirical estimates but are not re-analyzed here).</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>Descriptive/qualitative — novelty faces skepticism and lower legitimacy in peer review and funding according to cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Not quantified here; paper notes that novelty recognition can be delayed and path-dependent, with some ideas taking decades to gain acceptance (examples cited).</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>General across science; cited references include work on interdisciplinarity and funding (e.g., Bromham et al. on interdisciplinary research funding), but PNPS is primary empirical field of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Paper cites that social sciences and humanities may be especially underrepresented in WoS and that funding/recognition patterns vary by discipline (no numerical cross-field bias estimates provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>Not applicable (conceptual mention).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Not quantified here; authors point to literature that documents bias but do not re-estimate acceptance or funding probabilities in this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Mentioned conceptually: novelty (transformational) faces more barriers than incremental research according to the literature; no direct quantitative comparison performed in this paper for peer review or grant decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Not applicable to this mention-only entity.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Not in this paper — authors call for novelty-sensitive indicators and policy interventions to mitigate bias but do not test peer-review or funding interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Not provided empirically here beyond literature examples of delayed recognition (Einstein gravitational waves, Higgs).</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Cited literature suggests legitimacy, disciplinary norms, institutional expectations, and readiness of the knowledge system moderate acceptance of novel ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Conceptual/literature-cited statements; primary empirical analysis of the paper focuses on citations rather than direct peer-review or funding outcomes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bias Against Novelty in Science: A Cautionary Tale for Users of Bibliometric Indicators <em>(Rating: 2)</em></li>
                <li>Interdisciplinary Research Has Consistently Lower Funding Success <em>(Rating: 2)</em></li>
                <li>Breakthrough recognition: Bias Against Novelty and Competition for Attention <em>(Rating: 2)</em></li>
                <li>Atypical Combinations and Scientific Impact <em>(Rating: 2)</em></li>
                <li>The Memory of Science: Inflation, Myopia, and the Knowledge Network <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1881",
    "paper_id": "paper-280016234",
    "extraction_schema_id": "extraction-schema-37",
    "extracted_data": [
        {
            "name_short": "5yr_citations",
            "name_full": "Five-year forward citation counts (Web of Science)",
            "brief_description": "Short-term citation counts within a five-year forward window used as the primary dependent measure of recognition in the paper's regression models; analyzed relative to structural novelty scores (Pioneer, Maverick, Vanguard).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "citation metrics (short-term citations)",
            "novelty_measure": "Triadic novelty types (Pioneer: new topics; Maverick: new distant ties; Vanguard: reinforcement of weak ties) using initial novelty scores and novelty impact scores",
            "bias_magnitude": "Pioneer: regression coefficient = -0.014 (p = 0.84) — no significant positive effect (i.e., effectively no reward within 5 years); Maverick: main effect coefficient = 0.546 (p &lt; 0.001) indicating substantial positive association with 5-year citations (exp(0.546) ≈ 1.73, ~73% higher expected counts); Vanguard: main effect coefficient = 0.109 (p &lt; 0.001) indicating modest positive association (exp(0.109) ≈ 1.12, ~12% higher expected counts); Vanguard × impact interaction = 0.026 (p &lt; 0.001); Vanguard quadratic interaction = -0.005 (p &lt; 0.001) showing diminishing returns.",
            "relationship_type": "conditional / mixed: Maverick ~ positive approximately linear (log-linear NB coefficient); Vanguard ~ bell-shaped (positive then diminishing returns; significant positive interaction with impact and a negative quadratic term); Pioneer ~ no relationship in 5-year window unless combined with high novelty impact (threshold/conditional effect).",
            "temporal_pattern": "Evaluation measured on 5-year horizon; Pioneer contributions often show little or negative short-term citation effect and require longer, less predictable trajectories for recognition (paper notes Pioneer may gain recognition only when subsequent uptake is high).",
            "field_studied": "Philantropic and Nonprofit Studies (PNPS) — interdisciplinary spanning Social Sciences, Life Sciences & Biomedicine, Technology, Arts & Humanities, Physical Sciences",
            "field_differences": "Journal-level heterogeneity is large (random intercept variance for journal J = 1.035) and year variance smaller (J = 0.382); specific journals differ (e.g., VOLUNTAS high on Pioneer novelty but low Pioneer impact; NVSQ low Pioneer novelty but high Pioneer impact), indicating venue-dependent recognition patterns rather than uniform field effect.",
            "proxy_metric_studied": "Five-year forward citation counts (WoS-indexed citations)",
            "ground_truth_measure": "Novelty impact scores (structural uptake measures) and the authors' interpretation of 'recognition' via five-year citations — no independent external ground truth like Nobel prizes; novelty impact score is treated as descriptive uptake measure.",
            "proxy_truth_gap": "Pioneer: initial novelty correlates weakly with impact (Spearman ρ = 0.087) and shows no significant citation reward (coef = -0.014, p = 0.84) — signaling under-recognition by short-term citation proxy; Maverick diminishment correlation with impact ρ = 0.29; correlations between triadic novelty and disruption index are small (standardized Pioneer vs disruption ρ = -0.041; raw Maverick vs disruption ρ = 0.399) indicating mismatch between different proxies.",
            "incremental_vs_transformational": "Yes — explicit comparison: transformational (Pioneer) is often undervalued in five-year citations (no significant positive effect), recombinatory/transformative recombination (Maverick) is rewarded strongly (coef = 0.546, p &lt; 0.001), and early reinforcement (Vanguard) yields moderate reward with diminishing marginal returns (coef = 0.109 with significant quadratic negative term).",
            "multiple_proxy_failures": "The paper compares multiple proxies (triadic novelty scores vs the disruption index vs citations) and finds limited overlap (low Spearman correlations), i.e., different proxies fail in different ways but do not report compounded multiplicative failure; disruption index is noted as biased by citation inflation.",
            "automated_system_performance": "Not applicable — no ML/AI automated decision system tested for paper acceptance; only algorithmic baseline simulation and network-based novelty measures were used.",
            "training_data_bias": "Not applicable to ML training; however, the authors note citation inflation and preferential attachment in historical data produce structural bias that they adjust for via simulation (see simulated baseline, alpha = 0.05).",
            "intervention_tested": "Standardization via a simulated baseline model (z-score normalization) that adjusts for citation inflation and preferential attachment; selected attractiveness parameter α = 0.05 produced high agreement (Spearman &gt; 0.7) between simulated and observed temporal novelty trends, enabling cross-year/field standardized novelty comparisons.",
            "counter_examples": "Examples noted where transformational work was later recognized after long lags (Einstein's gravitational waves, proposed 1916, validated ~100 years later; Higgs boson hypothesis gained prominence only after LHC empirical validation). Empirical journal-level counterexample: NVSQ produced few pioneering articles but those had high Pioneer impact (selective amplification).",
            "moderating_factors": "Novelty impact (uptake) strongly moderates citation reward (e.g., Pioneer yields citations only when Pioneer impact high); journal venue (large random effect variance) moderates recognition; radial domain granularity and WoS coverage limit detection (field/coverage bias); five-year window limits detection of slow-burning Pioneer effects.",
            "sample_size_and_methods": "Analysis of 41,623 articles (published 1960–2017) citing 313,172 unique WoS references; weighted co-occurrence citation network (WCCN) of WoS subject categories; mixed-effects negative binomial regression with random intercepts for journal and year; five-year forward citation window; simulated baseline model (per-year synthetic papers) with preferential attachment and linear attractiveness parameter α = 0.05; standardization via z-scores.",
            "uuid": "e1881.0"
        },
        {
            "name_short": "Disruption_index",
            "name_full": "Disruption index (normalized disruption score)",
            "brief_description": "A widely used bibliometric proxy (disruption index) compared against triadic novelty measures for robustness; the paper reports low correlations between disruption and triadic novelty types and notes disruption's sensitivity to citation inflation.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_system_type": "citation-based disruption metric (bibliometric proxy)",
            "novelty_measure": "Disruption index (log-transformed, normalized disruption scores) used as an external comparator to triadic novelty measures",
            "bias_magnitude": "Correlations with triadic measures: standardized Pioneer vs disruption ρ = -0.041; raw Pioneer vs disruption ρ = -0.244; Maverick (raw) vs disruption ρ = 0.399; Vanguard vs disruption small (standardized ρ = 0.034); Vanguard Impact vs disruption negative (standardized ρ = -0.198). The paper cites literature noting disruption index bias due to citation inflation.",
            "relationship_type": "low/weak correlation — disruption does not strongly co-vary with triadic novelty types (i.e., different axes of novelty are captured).",
            "temporal_pattern": "Disruption index is sensitive to citation inflation across time (cited literature and authors note this), producing time-dependent bias unless adjusted; authors standardize triadic novelty against a simulated baseline to avoid this inflation bias.",
            "field_studied": "Used for comparison within PNPS dataset (interdisciplinary field) — results likely generalizable to other fields given prior literature on disruption index biases.",
            "field_differences": "Not quantified by field in this paper beyond correlations; authors note disruption's sensitivity to citation inflation which may differentially affect fields with differing publication growth.",
            "proxy_metric_studied": "Disruption index",
            "ground_truth_measure": "Not provided — disruption used as an alternative proxy rather than ground truth.",
            "proxy_truth_gap": "Low concordance with triadic novelty (see above correlations); the disruption index fails to capture Pioneer and Vanguard novelty dimensions strongly (small or negative correlations).",
            "incremental_vs_transformational": "Indirect: raw Maverick shows some overlap with disruption (ρ = 0.399) suggesting disruption may partially reflect recombinatory novelty, but it poorly represents Pioneer and Vanguard forms.",
            "multiple_proxy_failures": "Paper demonstrates that disruption and triadic measures diverge, and disruption's time-sensitivity (citation inflation) can bias conclusions — indicating multiple proxies can fail in distinct ways rather than uniformly.",
            "automated_system_performance": "Not applicable.",
            "training_data_bias": "Not applicable.",
            "intervention_tested": "No intervention to alter disruption index; used only as comparator and critique (authors used simulated baseline to standardize triadic novelty instead).",
            "counter_examples": "Not specific; general observation that disruption can misrepresent Pioneer novelty (which often shows low short-term citation/disruption scores).",
            "moderating_factors": "Citation inflation and publication growth over time (temporal bias) moderate disruption index outputs.",
            "sample_size_and_methods": "Comparison across the same PNPS article sample (41,623 papers), computing Spearman correlations between triadic novelty measures (raw and standardized) and normalized/log-transformed disruption scores.",
            "uuid": "e1881.1"
        },
        {
            "name_short": "Journal_venue_effect",
            "name_full": "Journal venue / editorial selection effect (random intercept for journal)",
            "brief_description": "The study models journal-level heterogeneity as a random effect and evaluates journal-level standardized novelty (aggregated z-scores) to show how venues differ in publishing and amplifying novel work.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "journal decisions / editorial selection and venue visibility",
            "novelty_measure": "Average standardized triadic novelty scores per journal (z-scores from simulated baseline) and journal random intercept variance from mixed-effects model",
            "bias_magnitude": "Random-effect variance for journal = 1.035 (substantial), indicating venue accounts for large heterogeneity in citation performance; journal rankings show VOLUNTAS high in Pioneer novelty but low Pioneer impact, while NVSQ low Pioneer novelty but high Pioneer impact (qualitative contrast).",
            "relationship_type": "venue moderates recognition — multiplicative/heterogeneous effects modeled via random intercepts; aggregation of standardized novelty highlights journals' differing receptivity (no single functional form across journals).",
            "temporal_pattern": "Journals differ in long-term amplification of novel ideas; standardized scores control for year-to-year citation inflation enabling cross-year comparisons; patterns differ by journal editorial strategy (e.g., exploratory vs selective).",
            "field_studied": "Journals publishing within PNPS; includes interdisciplinary and field-specific outlets (Table 3 lists top journals for each novelty type).",
            "field_differences": "Quantitative journal differences reported via standardized average novelty — top-ten lists for each novelty type show that both interdisciplinary and field-specific journals publish novel work; specific cross-field patterns illustrated but not reduced to single numeric ratios across fields.",
            "proxy_metric_studied": "Aggregated standardized novelty scores (journal averages) and 5-year citations as outcome controlled by journal random effect",
            "ground_truth_measure": "None external — journal effect inferred from modeled citation counts and standardized novelty deviations from simulated baseline.",
            "proxy_truth_gap": "Not expressed as single number; large journal variance (J = 1.035) indicates venue choice can create substantial gap between an article's novelty and observed recognition via citations.",
            "incremental_vs_transformational": "Journals differ in how they publish and amplify novelty types: e.g., VOLUNTAS publishes many Pioneer pieces (exploratory) but these show low Pioneer impact in PNPS; NVSQ selectively publishes fewer Pioneer works which tend to attain higher impact — qualitative contrast evidencing editorial strategy effects.",
            "multiple_proxy_failures": "By aggregating standardized novelty across journals, the authors show that raw citation metrics alone obscure venue-level receptivity; standardization reduces temporal and structural biases but differences across journals remain large.",
            "automated_system_performance": "Not applicable.",
            "training_data_bias": "Not applicable.",
            "intervention_tested": "Standardization (simulated baseline + z-scores) applied at article level and aggregated to journals to correct structural biases; enabled identification of journals relatively more receptive to different novelty types.",
            "counter_examples": "VOLUNTAS: high Pioneer novelty but low Pioneer impact (exploratory outlet with limited uptake); NVSQ: low Pioneer novelty but high Pioneer impact (selective amplification) — examples of divergent editorial outcomes.",
            "moderating_factors": "Editorial strategy (exploratory vs selective), journal visibility/disciplinary reach, and subject-category composition (authors controlled for WoS domains).",
            "sample_size_and_methods": "Aggregated standardized novelty across journals with at least 30 PNPS articles; mixed-effects NB regression with (1|Journal) random intercept and journal-level variance reported; full dataset 41,623 articles.",
            "uuid": "e1881.2"
        },
        {
            "name_short": "Simulated_baseline",
            "name_full": "Simulated baseline model for standardization (adjusting for citation inflation and preferential attachment)",
            "brief_description": "A per-year simulation that generates synthetic papers mirroring observed counts and subject-category multiplicities using a preferential attachment plus linear attractiveness parameter (α) to create baseline novelty distributions used to z-standardize real novelty scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "algorithmic baseline adjustment for bibliometric evaluation (not an evaluator itself but an adjustment mechanism)",
            "novelty_measure": "Simulated distributions of Pioneer, Maverick, Vanguard novelty scores under random assignment with preferential attachment; real novelty scores standardized (z-scores) against these simulated distributions.",
            "bias_magnitude": "Chosen attractiveness parameter α = 0.05 produced highest agreement; Spearman correlation between simulated and observed temporal novelty trends exceeded 0.7 for most novelty types (authors' selection criterion), indicating that baseline captures structural growth patterns; standardization reveals deviations from expected novelty (z-scores).",
            "relationship_type": "Standardization converts raw novelty into deviations from expected (z-score), removing predictable growth/attachment effects; relationship is normalization rather than a functional effect mapping.",
            "temporal_pattern": "Model explicitly corrects for temporal bias (citation inflation and publication growth); used per-year simulation (1960–2017) so standardized scores are comparable across time.",
            "field_studied": "Applied to PNPS dataset but designed to be generalizable across fields by preserving per-paper subject-category counts and using preferential attachment based on node strength.",
            "field_differences": "Standardization adjusts for field/subject-category size differences because each simulated paper is assigned the same number of subject categories as its real counterpart; authors used this to compare journals and fields independent of raw publication volume.",
            "proxy_metric_studied": "Raw triadic novelty scores and five-year citation counts are adjusted/standardized using the simulated baseline.",
            "ground_truth_measure": "Simulated expectation under structural (random) model used as baseline/gauge; deviations (z-scores) treated as indicators of unusually high/low novelty relative to structural expectations.",
            "proxy_truth_gap": "Not directly a gap measure but standardization quantified deviations; selection of α = 0.05 yielded Spearman &gt; 0.7 indicating baseline well-matched to trends — enabling detection of anomalies (articles/journals &gt; ~1–2 SD above baseline).",
            "incremental_vs_transformational": "Standardized scores allow fairer cross-time/field comparison of Pioneer versus Maverick versus Vanguard by removing structural growth biases that otherwise favor some types (e.g., recombinations increasing with publications).",
            "multiple_proxy_failures": "Model addresses some proxy failures (time and preferential attachment) but does not correct for non-WoS citations (books, policy reports) or semantic granularity limits — authors note residual biases remain (coverage, granularity).",
            "automated_system_performance": "Model is an automated simulation (not machine learning); performance judged by Spearman similarity to observed trends (&gt; 0.7) and used to compute per-article z-scores.",
            "training_data_bias": "Model parameterizes historical preferential attachment (node strengths) — authors tuned attractiveness α to best replicate observed history; this acknowledges and models historical-data-driven bias rather than ML training bias.",
            "intervention_tested": "Standardization (z-score) is the tested intervention; it enabled cross-year and cross-journal comparisons and identification of journals that publish unusually novel work (Table 3; Figure 3).",
            "counter_examples": "Not direct counterexamples; model selection process (tuning α) highlights that raw novelty trends can be replicated well by structural models, implying many apparent novelty increases may be expected under preferential-attachment dynamics.",
            "moderating_factors": "Choice of α (linear attractiveness) moderates baseline fit; WoS subject-category granularity and coverage moderate what deviations are detectable; temporal publication growth pattern matters for baseline.",
            "sample_size_and_methods": "Simulated synthetic papers per year matching observed counts for 1960–2017; each simulated paper assigned same number of subject categories as real counterpart; selection probability for a category ∝ node strength(previous year) + α; tested multiple α values and selected α = 0.05 based on Spearman correlations with observed trends; standardized real scores via z = (real - mean_sim)/sd_sim.",
            "uuid": "e1881.3"
        },
        {
            "name_short": "Peer_review_and_funding_bias_mentions",
            "name_full": "Literature mentions of bias in peer review and funding against novelty",
            "brief_description": "Cited literature and conceptual discussion noting that novel ideas often lack legitimacy and are less likely to receive funding or recognition; used to motivate the study rather than empirically tested within this dataset.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_system_type": "peer review / funding decisions (literature mentions)",
            "novelty_measure": "Qualitative statements and previous studies referenced (e.g., 'Bias Against Novelty in Science' and other literature), not operationalized in this paper's dataset.",
            "bias_magnitude": "Qualitative claim: novel ideas are 'often ignored, undervalued, or even punished' — no numeric acceptance-rate or funding-rate estimate is provided in this paper (references cited contain empirical estimates but are not re-analyzed here).",
            "relationship_type": "Descriptive/qualitative — novelty faces skepticism and lower legitimacy in peer review and funding according to cited literature.",
            "temporal_pattern": "Not quantified here; paper notes that novelty recognition can be delayed and path-dependent, with some ideas taking decades to gain acceptance (examples cited).",
            "field_studied": "General across science; cited references include work on interdisciplinarity and funding (e.g., Bromham et al. on interdisciplinary research funding), but PNPS is primary empirical field of the paper.",
            "field_differences": "Paper cites that social sciences and humanities may be especially underrepresented in WoS and that funding/recognition patterns vary by discipline (no numerical cross-field bias estimates provided in this paper).",
            "proxy_metric_studied": "Not applicable (conceptual mention).",
            "ground_truth_measure": "Not applicable.",
            "proxy_truth_gap": "Not quantified here; authors point to literature that documents bias but do not re-estimate acceptance or funding probabilities in this dataset.",
            "incremental_vs_transformational": "Mentioned conceptually: novelty (transformational) faces more barriers than incremental research according to the literature; no direct quantitative comparison performed in this paper for peer review or grant decisions.",
            "multiple_proxy_failures": "Not applicable to this mention-only entity.",
            "automated_system_performance": "Not applicable.",
            "training_data_bias": "Not applicable.",
            "intervention_tested": "Not in this paper — authors call for novelty-sensitive indicators and policy interventions to mitigate bias but do not test peer-review or funding interventions.",
            "counter_examples": "Not provided empirically here beyond literature examples of delayed recognition (Einstein gravitational waves, Higgs).",
            "moderating_factors": "Cited literature suggests legitimacy, disciplinary norms, institutional expectations, and readiness of the knowledge system moderate acceptance of novel ideas.",
            "sample_size_and_methods": "Conceptual/literature-cited statements; primary empirical analysis of the paper focuses on citations rather than direct peer-review or funding outcomes.",
            "uuid": "e1881.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bias Against Novelty in Science: A Cautionary Tale for Users of Bibliometric Indicators",
            "rating": 2
        },
        {
            "paper_title": "Interdisciplinary Research Has Consistently Lower Funding Success",
            "rating": 2
        },
        {
            "paper_title": "Breakthrough recognition: Bias Against Novelty and Competition for Attention",
            "rating": 2
        },
        {
            "paper_title": "Atypical Combinations and Scientific Impact",
            "rating": 2
        },
        {
            "paper_title": "The Memory of Science: Inflation, Myopia, and the Knowledge Network",
            "rating": 1
        }
    ],
    "cost": 0.01716425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Triadic Novelty: A Typology and Measurement Framework for Recognizing Novel Contributions in Science</p>
<p>Jin Ai 
School of Social Policy and Practice
University of Pennsylvania
PhiladelphiaPAUSA</p>
<p>Richard S Steinberg 
Department of Economics
School of Liberal Arts
Indiana University
IndianapolisINUSA</p>
<p>Chao Guo 
School of Social Policy and Practice
University of Pennsylvania
PhiladelphiaPAUSA</p>
<p>Nascimento Silva 
Observatory on Social Media (OSoMe)
Luddy School of Informatics, Computing, and Engineering
Indiana University
BloomingtonINUSA</p>
<p>Triadic Novelty: A Typology and Measurement Framework for Recognizing Novel Contributions in Science
936C31941C926344F672F8F2ECEEB25BScientific NoveltyEvaluation MetricsKnowledge NetworksScience of ScienceComputational Social Science
Scientific progress depends on novel ideas, but current reward systems often fail to recognize them.Many existing metrics conflate novelty with popularity, privileging ideas that fit existing paradigms over those that challenge them.This study developed a theory-driven framework to better understand how different types of novelty emerge, take hold, and receive recognition.Drawing on network science and theories of discovery, we introduced a triadic typology: Pioneers, who introduce entirely new topics; Mavericks, who recombine distant concepts; and Vanguards, who reinforce weak connections.We constructed measurements for each novelty and apply this typology to a dataset of 41,623 articles in an interdisciplinary field called philanthropy and nonprofit studies, linking novelty types to five-year citation counts using mixed-effects negative binomial regression.Results showed that novelty is not uniformly rewarded.Pioneer efforts are foundational but often overlooked.Maverick novelty shows consistent citation benefits, particularly rewarded when it displaces prior focus.Vanguard novelty is more likely to gain more recognition, but its citation advantage diminishes as those reinforced nodes become more central.To enable fair comparison of novel contributions, we also introduced a simulated baseline model to adjust for citation inflations and preferential attachment across different time and domains.Our findings improve the evaluation of novel research, affecting science and innovation policy, funding, and institutional assessment practices.</p>
<p>Introduction</p>
<p>Despite their potential to reshape entire fields, novel ideas are often ignored, undervalued, or even punished (1) because they break from the norm.Novel research tends to lack legitimacy, making it less likely to receive funding or recognition (2)(3)(4).Thomas Kuhn described this dilemma as the "essential tension" between divergent innovation and convergent tradition (5).New ideas challenge established ways of thinking and often encounter skepticism from scholars and institutions invested in disciplinary conventions.This challenge is more pressing today, as technological change blurs boundaries between natural science, engineering, social sciences, and humanities (6).It is thus increasingly needed to develop evaluation frameworks that can effectively capture novelty underwriting this evolving scientific landscape.</p>
<p>Current novelty measures fall into two categories: general citation impact and direct novelty metrics.Citation-based proxies conflate popularity with novelty, overlooking the many important ideas that do not gain immediate recognition (4,7,8).Highly cited work is not always the most novel (9), and citation patterns reflect social dynamics as much as intellectual breakthroughs (10).Direct novelty metrics include: frequency-based models that identify atypical concept pairings (4,8,11), typological approaches that classify innovation strategies (3,12), disruptiveness-based metrics that assess shifts in citation patterns (13,14), and high-dimensional models that analyze semantic abnormality (11,15).While promising, most rely on a single score (4,8,(13)(14)(15), focus only on high-impact work (3,11,12), or neglect the contributions of early adopters or fast followers (16,17).</p>
<p>Fundamentally, most existing novelty metrics are designed to predict which ideas are likely to gain influence, rather than to assess novelty as a multidimensional and intrinsic feature of knowledge production.This emphasis on eventual success sidelines valuable contributions, not because they lack substance, but because they depart from mainstream norms, institutional expectations, or incentive structures (18).Some insights arise not through goal-oriented optimization but through open-ended exploration.For instance, Lehman and Stanley (19) showed that novelty-driven search, valuing uniqueness without fixed objectives, can outperform traditional, outcome-based strategies in tasks such as maze navigation and biped walking.Further, the popularitybased measures are censored at the last year of the data, omitting works that eventually become quite influential.A more effective approach must therefore recognize novelty for what it is, not for what it may later become, and account for the diverse forms it can take at the moment of its emergence.</p>
<p>In response to these limitations, this paper introduces a new typology of novelty grounded in theories of scientific discovery.Drawing from a review of foundational work in the philosophy and sociology of science, we identify four dimensions that characterize how novelty emerges and is expressed in research.Based on these dimensions, we developed a three-part typology: Pioneers, Mavericks, and Vanguards, each representing a distinct pathway through which scholars generate new knowledge.This framework offers a more direct and versatile approach to identifying novelty, one that does not rely on retrospective impact but instead captures the intrinsic uniqueness of ideas at the point of contribution.</p>
<p>Conceptual Dimensions of Novelty</p>
<p>We identified four key dimensions that characterize the nature of novelty in scientific research.They are relational, multifaceted, contextual, and dependent on both the initial and first subsequent connections.</p>
<p>Relational.Novelty is not simply the product of isolated ideas, but rather emerges through the relationships between them.Building on Bacon's emphasis on the structure of scientific discovery, Merton (20) argued that novelty arises from sustained interactions between scientists and existing knowledge, not individual brilliance alone.This relational view was further illustrated by Swanson's (21) theory of undiscovered public knowledge, which shows how significant discoveries stem from connecting disconnected literatures.In bioinformatics, the integration of biological data with computational methods transformed genetic research, as seen in the Human Genome Project (22).In econophysics, tools from physics have been used to model complex economic systems, offering new insights into market behavior (23).These examples reflect how linking previously unconnected ideas can generate transformative knowledge.</p>
<p>Multifaceted.Novelty takes multiple forms, shaped by the varied roles that scientists play in generating, organizing, and advancing knowledge.Drawing from Merton's (20) discussion of scientific labor and Znaniecki's (24) typology of intellectual roles, novelty can emerge from initiating new lines of inquiry, synthesizing disparate knowledge, or deepening existing paradigms.These diverse roles reflect the multifaceted nature of novelty.Burt's (25) brokerage theory similarly highlights how bridging structural holes in knowledge networks leads to different types of informational advantages.These distinctions are evident in the varying logics of interdisciplinary, multidisciplinary, and transdisciplinary research, which represent additive, interactive, and integrative approaches to connecting knowledge across fields (26).Thus, novelty cannot be captured by a single mechanism, rather, it varies depending on how ideas are combined and by whom.</p>
<p>Contextual.</p>
<p>Novelty is inherently contextual, shaped by the specific conditions under which it emerges and is recognized.Merton (20) noted that multiple discoveries often occur independently and simultaneously, suggesting that certain breakthroughs are conditioned by the readiness of the knowledge system, much like a ripe apple falling from a tree (27).Truly singular discoveries are rare and require explanation beyond individual genius.The perceived novelty of an idea depends on factors such as timing, location, and disciplinary context.For example, artificial intelligence (AI) has existed for decades, but only gained widespread recognition with advances in hardware (e.g., GPUs) and algorithmic design (28).Even today, innovations in AI may be seen as novel in one domain or region while appearing routine in another (29,30).These examples highlight that novelty cannot be separated from the conditions that frame its reception.</p>
<p>First Subsequent Connections are Essential.Breakthroughs rarely standalone, rather, they often depend on first subsequent connections that validate, reinforce, and extend the initial insight.Merton's (20) concept of "multiples" points to the redundancy in scientific discovery, which is sometimes viewed as wasteful duplication.However, Burt (25) argues that such redundancy can play a generative role by reinforcing emerging ideas and filling structural gaps.In many cases, a novel contribution may not gain recognition until it is supported by additional efforts.For example, Einstein's theory of gravitational waves was proposed in 1916 but remained speculative until confirmed by the LIGO and Virgo collaborations a century later (31).Similarly, the Higgs boson hypothesis gained prominence only after empirical validation at the Large Hadron Collider (32).These cases illustrate how first follow-up efforts, whether independent or collaborative, are critical to realizing the potential of novel ideas.</p>
<p>A Triad of Novelty Typology: Pioneer, Maverick, and Vanguard</p>
<p>These four dimensions provide the conceptual foundation for our approach to novelty.We constructed a weighted co-occurrence citation network (WCCN) in which nodes represent research topics and edges indicate the co-occurrence of topics within a single article.The relational dimension is embedded in the structure of the network, capturing how novelty arises from linking previously unconnected or rarely connected ideas.To reflect the multifaceted nature of novelty, we distinguished between three types of contributions: the introduction of new nodes (entirely novel topics), new ties (connections between previously separate topics), and first repeated ties (reinforcements of fragile or emerging connections).These structural features correspond to different forms of novelty that shape the subsequent knowledge landscape.We incorporated the contextual nature of novelty by focusing on specific fields within disciplinary boundaries.Finally, we identified articles that provide first subsequent connections, capturing how repeated engagement can solidify an emerging idea.</p>
<p>Three types of novelty are proposed (Figure 1).Pioneers introduce new topics, Mavericks link distant domains, and Vanguards strengthen weak links.These categories have distinct effects on the evolution of an intellectual field.</p>
<p>Measuring Novelty.For each novelty type, we developed two complementary measures that capture different stages of the influence of a novel idea on the evolution of the field.The first measure, the initial novelty score, quantifies the immediate structural change an article introduces to the topic network.This measure captures how the article reshapes or expands the knowledge structure at the moment of its publication.The second measure, the novelty impact score, reflects whether and to what extent those initial contributions become embedded in the evolving structure of the field.This is a descriptive measure, not a causal one.It captures the persistence and diffusion of a novel idea over time, and results from a combination of the novelty itself and is shaped by multiple exogenous factors (e.g., institutional changes, shifts in funding priorities, or broader trends in the field).</p>
<p>Pioneer Novelty: Introducing New Topics</p>
<p>Pioneer novelty captures contributions that introduce entirely new topics into the knowledge structure of a field.These pioneering articles expand the boundaries of scientific inquiry by adding new conceptual areas, represented as previously unseen nodes in the topic network.For example, in Figure 1: Panel I, node F (orange) illustrates a new topic introduced into an existing network (depicted by blue nodes and gray ties).Initial Pioneer Novelty Score.Let  !denote the set of topics associated with the article , and let  # be the set of all articles published up to time .We define Pioneer novelty  $%&amp;'((!(r) as the number of new nodes introduced by the article .That is, topics represented in  !but absent from the set  ) ! comprising all the topics introduced before the time it was published.Formally:
𝑁 $%&amp;'((! (𝑟) = |𝐼 ! \𝐼 ) ! |
This score reflects the breadth of conceptual novelty contributed by the article, capturing the extent to which it expands the topical space of the field.</p>
<p>Pioneer Novelty Impact Score.Not all new topics gain traction (33).Some pioneering ideas become central to future research (e.g., green node F in Figure 1: Panel I), while others remain peripheral (e.g., purple node F).To capture this variation in influence, we developed a second score to estimate the uptake of each newly introduced topic.We define the Pioneer Impact Score  $%&amp;'((!() of a given paper  as the maximum relative frequency with which any of its associated topics reappears in future works.Specifically:
𝑆 $%&amp;'((! (𝑟) = 𝑚𝑎𝑥 % ∈ + " |) !,!$ &amp;! (%)| |) !,!$ &amp;! !'!() |
Where  #,#1 2# () denotes the set of articles published within the future window (,  + ) that are associated with the topic  , and | #,#1 2# #&#34; | represents the total number of articles published during that same period.</p>
<p>This score captures the maximum 1 uptake of any new topic introduced by article , indicating how strongly that conceptual addition becomes embedded in subsequent research.</p>
<p>Maverick Novelty: Reconfiguring Existing Knowledge</p>
<p>Maverick novelty captures contributions that challenge the status quo by connecting previously unrelated or indirectly linked topics.Rather than introducing entirely new areas, Mavericks reshape the internal structure of the field, often by forging conceptual bridges across topical boundaries.These contributions embody unconventional thinking and frequently confront prevailing assumptions or knowledge silos.For instance, in Figure 1: Panel II, the orange edges between nodes A-B and B-D represent new ties that shorten the conceptual distance between those topic pairs.Initial Maverick Novelty Score.This novel contribution is computed as the average fractional change in distance caused by the newly introduced ties.Let  !denote the set of new ties introduced by the article , and let  5 represent the initial shortest distance path between the two nodes connected by the tie , computed using the Dijkstra algorithm (34).If two nodes are disconnected, their distance is treated as infinite.The distance change for each new tie is calculated as 1 − ) 1 The Pioneer novelty impact score can be measured as the average, median, maximum, or minimum of these changes;</p>
<p>we choose the maximum for this study.</p>
<p>A higher score indicates that the article introduces more conceptually distant connections, reflecting greater structural redirection.</p>
<p>Maverick Novelty Impact Score.New ties introduced by Mavericks can shift attention within the network, either amplifying nearby research or diverting focus away from it.We capture this with two measures: the Enhancement Maverick score and the Diminishment Maverick score.These reflect changes2 in the proportional weights of neighbor ties.That is, ties connected to the nodes involved in the new link, before and after the publication of the Maverick article.For example, in Figure 1: Panel II, the new ties connect nodes A, B, and D. The relevant neighbor ties include A-F, A-C, B-C, and D-E.If the average proportional weight of these ties increases after the new tie is introduced, the article exhibits enhancement; if it decreases, it exhibits diminishment.Assuming that each new tie is introduced by a different new article and  = 1, in this scenario, the proportional weight change from  − 1 to  of the four neighbor ties was 1/2 (the only weight change A-C divided by the two new articles).From  to  +  this increases to 1 in the enhancement scenario and decreases to 1/8 in the diminishment scenario.Thus, the enhancement score is calculated as 1/8 = 1/4(1 − 1/2), and the diminishment score is
−3/32 = 1/4(1/8 − 1/2).
Let  !denote the set of neighbor ties to the article .Let  &gt;,# denote the weight gained by the neighbor tie ℎ (∈  ! ) between time  and  + 1.Let | # | denote the total number of articles at time .
Let 𝑔 &gt; ?(@&amp;!( = 6 2# ∑ #A6 B C #A2# 2D +,, |) , | and 𝑔 &gt; 3@#(! = 6 2# ∑ #12# B C #16 2D +,, |) , |
be the proportional weights of neighbor ties h before and after the new ties.Let  &gt; =  &gt; ?(@&amp;!( −  &gt; 3@#(! .Enhancement occurs when  &gt; &gt; 0, while diminishment occurs when  &gt; &lt; 0.</p>
<p>Hence, the function describing the measure is as follows:
𝑆 E'&gt;3';(F('# 93:(!%;&lt; (𝑟) = 6 |G " | ∑ &gt; ∈ G " 𝑚𝑎𝑥 (𝛥𝑔 &gt; , 0) 𝑆 H%F%'%I&gt;F('# 93:(!%;&lt; (𝑟) = 6 |G " | ∑ &gt; ∈ G " 𝑚𝑖𝑛 (𝛥𝑔 &gt; , 0)
The enhancement score reflects whether a Maverick connection increases engagement with existing related ties, while the diminishment score captures a decline in attention to previously adjacent topics.</p>
<p>Vanguard Novelty: Reinforcing Emerging Ideas</p>
<p>Vanguard novelty captures the early adopters or fast followers of emerging ideas.It is defined as reinforcing ties between previously weakly but directly connected topics.Since one article can strengthen multiple ties, the measure captures how an article strengthens ties based on prior connections between topics, with higher novelty assigned to articles reinforcing ties with fewer prior occurrences.The process involves comparing the number of prior occurrences of each connection and ranking articles accordingly, up to a predefined limit , beyond which ties are no longer considered novel.Vanguard novelty occurs when an article strengthens previously weakly connected nodes, such as the connection between nodes M and P or N and P in a hypothetical article  6 , whose ranking vector is (1,1,0).Now, consider a hypothetical article  J , within which two ties are strengthened.As A-C appeared twice before and B-C appeared three times, its ranking vector is (0,1,1).If two articles strengthen links appearing both once and twice before, we differentiate them by checking if one strengthens a link that has appeared three times.We set an upper limit  on the number of times a link can appear before it no longer qualifies as novel.For example, if  = 5, articles with identical records for twice-, three-, and four-time links are ranked equally.In this case, the scenario  6 ranks higher.</p>
<p>Initial Vanguard Novelty Score.Let  != ( 6 , …, &lt; , …, K ) denote the ranking vector for the article , where  6 is the number of ties strengthened by article  appeared once before time t,  J the number that appeared twice, and so on.The ranking vector allows for lexicographic ordering (35) of Vanguard articles.If two articles differ in  6 , the one with the higher value is ranked higher.If they have the same  6 , we compare  J , and so on.Articles with identical ranking vectors have the same Vanguard novelty.This lexicographic process is similar to alphabetizing words; the first "letter" ( 6 ) takes priority, and subsequent "letters" ( J ,  L , etc.) are considered only if needed.Once articles are ranked lexicographically, ordinal numbers are assigned with "1" given to the top-ranked article(s), "2" to the next, and so forth.</p>
<p>Vanguard Novelty Impact Score.Because Vanguard ties increase node connectivity, they could also increase the influence of the reinforced links around them.Its significance is thus measured by the average3 change in the weighted degree centrality of the nodes involved.Let  !represent the set of topics in the article  published at the time  connected by Vanguard ties.Let  (,# denote the total count of articles citing a topic  (∈  ! ) and |  # | the total number of articles at the time .The function describing the significance measure is as follows:
𝑆 M3'NO3!7 (𝑟) = 6 |E " | ∑ ( ∈ E " ( P -,!$ &amp;! |) !$ &amp;! | − P -,!./ |) !./ | )
This measure reflects whether the connections reinforced by Vanguards become more structurally central over time, capturing their increasing integration into the evolving network of the field.</p>
<p>Research Context.The Triadic Novelty framework was applied to philanthropic and nonprofit studies (PNPS), an interdisciplinary and emerging field spanning domains of Social Sciences, Life Sciences &amp; Biomedicine, Technology, Arts &amp; Humanities, and Physical Sciences (36).The intellectual diversity, along with the evolving boundaries and uneven patterns of recognition across different disciplinary fields (36), makes it wellsuited for examining how different types of novelty emerge and are rewarded over time.</p>
<p>Results</p>
<p>We examined the Triadic Novelty framework to a dataset of 41,623 peer-reviewed articles published between 1960 and 2017 in the PNPS.These articles cite 313,172 unique references indexed in the Web of Science (WoS).The dataset was drawn from a broader corpus of 60,684 articles spanning 1899 -2022, with pre-1960 publications used to construct the knowledge base of the field.Because we assessed citation impact using a five-year forward citation window, the regression analysis included only articles published no later than 2017.Citation counts reflect citations from other WoS-indexed publications.</p>
<p>Correlation between Initial and Impact Scores</p>
<p>To assess the alignment between initial and impact measures for each novelty type, we computed Spearman correlations using only non-zero values.For Maverick novelty, we used absolute values of diminishment impact scores, as smaller raw diminishment scores indicate higher impact.As Table 1 shows, Maverick had the strongest correlations, particularly for diminishment impact (ρ = 0.29), suggesting that articles introducing distant conceptual connections tend to align in their initial introduction and longer-term integration.Pioneer novelty exhibited weak correlations (ρ = 0.087), while Vanguard novelty shows a modest negative relationship (ρ = -0.131),indicating that early reinforcement of weak ties does not necessarily translate into broader structural uptake.</p>
<p>Citation Impact of Novelty Types</p>
<p>To assess how different types of novelty are recognized under current academic reward systems, we modeled their association with five-year citations using a mixed-effects negative binomial regression.While our novelty scores are structurally defined and independent of citation behavior, both citation and impact measures were evaluated using a consistent five-year forward window.Full regression results appear in Table 2, and descriptive statistics are reported in Figure S1.</p>
<p>Pioneer Novelty: New Topics Often Went Unrewarded.Pioneer novelty, which captures the introduction of entirely new topics into the field network, showed no significant relationship with five-year citation impact.As shown in Table 2, neither the direct effect of introducing new topics (regression coefficient  = −0.014, −  = 0.84) nor the interaction effect with Pioneer novelty impact ( = 0.104,  = 0.17) was significant.However, the predicted citation (Figure 2A) and marginal effect plots (Figure 2B) revealed a more nuanced picture.Figure 2A shows that pioneer novelty boosted citations only when paired with high levels of novelty impact.When the uptake is low, novelty has little effect or even reduces citations.Figure 2B further shows that the marginal effect of pioneer novelty stayed near zero at low impact levels but became more positive as impact increased.This result supports the core concern motivating this framework: foundational contributions that expand the conceptual boundaries of a field often go unrecognized unless others actively adopt and build upon the new topics introduced.</p>
<p>Maverick Novelty: Recombination Was Rewarded, Especially When It Redirected.</p>
<p>Maverick novelty was defined as research that forged new ties between previously disconnected or distantly linked topics.The main effect was strongly positive ( = 0.546,  &lt; 0.001) indicating that these novel connections tended to be well-received.</p>
<p>Although its interaction with enhancement ( = 0.053,  = 0.79) and diminishment ( = 0.279,  = 0.47) impact scores were not significant, the predicted citation plots (Figures 2C and 2E) revealed that higher Maverick novelty scores consistently led to more citations, regardless of whether the new connections amplified or diverted attention to/from neighbor ties.The marginal effect plots (Figures 2D and 2F) provide additional nuance.For enhancement impact (Figure 2D), the marginal effect of Maverick novelty remained relatively stable across the range of enhancement scores, with most of the probability distribution concentrated above zero.This suggested that recombination is reliably beneficial, even if it does not noticeably reinforce the surrounding structure.By contrast, the marginal effect became more positive as the diminishment impact increased (Figure 2F), indicating that Maverick novelty tended to receive more recognition when it displaced local knowledge.Together, these results highlight that recombination was consistently rewarded, and may gain added traction when it redirects, rather than reinforces, existing topic structures.</p>
<p>Vanguard Novelty: Reinforcement Paid off, but Only to a Point.Vanguard novelty, which captures research that reinforces weakly connected but adjacent topics, was significantly associated with increased five-year citations ( = 0.109,  &lt; 0.001).Its interaction with Vanguard impact was also significantly positive ( = 0.026,  &lt; 0.001), indicating that such reinforcement attracted more citations when the strengthened topic pairs gain prominence.As depicted in Figure 2G, predicted citations rose with increasing vanguard impact, with greater gains when the reinforced topic pairs became more central in the later field network.However, the reward was only up one point.The significant negative interaction with the quadratic term ( = −0.005, &lt; 0.001) revealed diminishing returns.Figure 2H shows a bell-shaped curve: the marginal effect of Vanguard novelty peaks at moderate impact levels and then tapers off.While early reinforcement was rewarded, further reinforcement in already-integrated areas yielded less added value.These results suggest that Vanguard novelty pays off most when solidifying emerging connections, but its advantage declines in well-established knowledge areas.</p>
<p>These results confirm our core proposition: novelty is not a single trait, but a multidimensional process shaped by how ideas enter a field and how they are built upon.Maverick novelty is consistently rewarded, especially when it redirects attention within the network.Vanguard novelty gains recognition when applied to emerging areas, though its marginal benefits decline as the structural centrality of the reinforced nodes increases beyond a certain point.In contrast, Pioneer novelty often goes underrecognized unless these topics subsequently become central within the field network.</p>
<p>Simulated Baseline Model to Compare Novelty Performance across Journals</p>
<p>Later publications tend to have higher citations simply because the number of publications has grown.Novelty levels also vary because of differences in the number of articles in disciplines and fields.To account for this structural bias, we simulated a baseline model to make novelty levels and impacts comparable across articles in our database.Our model adjusted for citation inflation (37), preferential attachment (38)(39)(40), and the limited number of WoS subject categories.For each year, the model generated the same number of papers as in the observed data.Each simulated paper was assigned the same number of subject categories as its real counterpart, ensuring structural consistency.Like Price (40), the selection of subject categories followed a preferential attachment process.The distributions of the novelty scores generated by this baseline allowed us to standardize (using z-scores) our data at the article level.</p>
<p>We then applied the simulated model to evaluate the novel performance of journals, aggregating scores from individual papers included.Using standardized novelty, we identified which journals tend to publish highly novel work.We ranked the journals that contain at least 30 PNPS articles across three types of novelty based on initial novelty scores.Table 3 presents the top ten journals for each novelty type based on average standardized novelty scores.These lists include both interdisciplinary and field-specific outlets, demonstrating that high novelty is not confined to traditional PNPS journals.Notably, several journals (e.g., Public Relations Review, Social Science &amp; Medicine, etc.) consistently appear across different novelty types, showing cross-field capacity to support novel efforts.</p>
<p>To complement the table, we visualized the average standardized novelty scores of the three core journals of PNPS (36) using a radar plot (Figure 3).Voluntas exhibited the highest Pioneer novelty, suggesting it frequently introduced new topics, which likely reflected its comparative and international focus.However, these contributions showed relatively low Pioneer impact, indicating limited uptake of these pioneering ideas.In contrast, Nonprofit and Voluntary Sector Quarterly (NVSQ) ranked low on Pioneer novelty but achieved the highest Pioneer impact, meaning its few pioneering articles tended to become highly influential.This pattern suggests that NVSQ was selective in novel approaches but effective in amplifying it, while VOLUNTAS played a broader exploratory role.These contrasting patterns illustrate how different underlying editorial strategies contribute to field development through evaluating novelty.</p>
<p>Validating Against Existing Novelty Measure</p>
<p>As a robustness check, we compared our standardized and raw novelty scores to a widely used novelty proxy, the disruption index (13,14).Table 4 shows the Spearman correlation between our novelty metrics and normalized disruption scores (logtransformed).The results revealed minimal overlap between the two (e.g.,  = −0.041for Pioneer novelty), confirming that our typology captures distinct dimensions of novelty.Interestingly, correlation was somewhat stronger when using raw scores, particularly for Maverick novelty ( = 0.399), suggesting that the disruption index may partially reflect structural recombination, but not other novelty forms like field expansion (Pioneer) or reinforcement (Vanguard).It is important to note that the disruption scores are not standardized against any simulated model or time-sensitive baseline, and prior research shows it is sensitive to citation inflation (41), which can bias its interpretation.In contrast, our standardized novelty scores explicitly adjusted for time and reference count, which may explain the lower correlations observed.Still, even when using normalized disruption scores, we found a range of correlations with our raw novelty scores, from moderately negative to positive, further supporting the idea that our framework captures distinct dimensions of novelty.</p>
<p>Discussion</p>
<p>A Triadic Novelty Framework for Understanding Research Novelty.We propose a theory-based framework consisting of three kinds of novelty.Pioneer novelty captures the introduction of entirely new topics; Maverick novelty captures the creation of new connections between distant topics; and Vanguard novelty captures the early reinforcement of weak but existing ties.Each kind of novelty has different effects on the evolution of the field.To capture these differences, we develop four impact measures.Pioneer impact measures future uptake of the newly introduced topics.Maverick Enhancement and Diminishment measure the extent to which the new recombinatory connections increase or decrease attention to the surrounding structure.Vanguard impact measures the increase in centralization of reinforced topic pairs.All of these measures are computed from WCCN.</p>
<p>Our novelty metrics differ from traditional citation-count measures, as evidenced by our negative binomial regression results.Additionally, our novelty scores are empirically distinct from existing novelty measures, featured by the disruption index, especially for Pioneer and Vanguard types.Further, novelty impact is important for assessing researchers and for evaluating departments and journal.To support these comparisons, we also develop a simulated baseline model that allows us to calculate standardized novelty scores that are fully comparable across publication years and disciplines.</p>
<p>Integrating Novelty and Popularity in Research Evaluation.Our findings underscore that the relationship between novelty and popularity is conditional rather than uniform.While our results demonstrated clear variation in how each type of novelty aligned with citation outcomes, the broader implication is that integration into the existing knowledge structure is a critical mediator of whether novel contributions are recognized.The stronger alignment for Maverick novelty, particularly in its diminishment form, suggests that novelty is more likely to be rewarded when it redirects rather than simply adds to existing topic configurations.In contrast, the limited citation benefits of Pioneer novelty reflect persistence challenges for contributions that lie outside established frameworks, even when structurally important.Vanguard novelty, those fast followers or early adopters, while more predictable in its uptake, illustrates that once a contribution becomes common, its marginal value diminishes.These findings indicate that evaluation systems relying on citation-based metrics tend to favor novelty that builds upon or reconfigures familiar ground, rather than ideas that originate entirely outside it.</p>
<p>At a collective level, our standardized novelty scores clarify how field-and journal-level features shape recognition.Our results do not just reflect individual novelty performance; they also make visible the institutional selectivity through which novelty is evaluated.Because our standardized novelty scores are calibrated against a simulated baseline, they allow us to observe how different publication venues and disciplines differ in their receptivity to novel efforts, independent of raw citation counts.This reinforces the need for evaluation frameworks that are sensitive to temporal constraints, disciplinary contexts, and field norms, especially when novelty metrics are applied in settings such as tenure review, grant allocation, or journal benchmarking.</p>
<p>Limitations and Suggestions for Future Research.While our framework captures key structural and temporal dynamics of novelty, it does not fully reflect the cultural or interpretive dimensions embedded in topic relationships (42,43).The reliance on WoS data imposes additional limitations: WoS omits citations from books, policy reports, and many interdisciplinary outlets (44), potentially underrepresenting the impact of certain novel contributions, particularly in the social sciences and humanities.Moreover, our use of WoS subject categories as proxies for topics introduces a constraint on granularity.With only 254 predefined categories, the novelty detection process may overlook fine-grained thematic distinctions, especially for identifying truly pioneering work.As a result, the limited variance in Pioneer novelty scores may partially explain the lack of significant association with short-term citation impact observed in our analysis.With the current granularity level for topics, our methodology works best for emerging fields in which not all subject categories have been explored yet.Such a limitation, however, can be addressed by using a more granular topic distribution, such as keywords or fine-grained categorization systems (45).</p>
<p>Additionally, although the five-year window effectively captures early recognition for most articles, some novel ideas, especially those associated with Pioneer novelty, may follow longer and less predictable trajectories of influence.This reflects a broader tradeoff, that is longer citation windows offer more accurate assessments of novelty influence but reduce the timeliness and practical utility of such measures for evaluating authors, journals, departments, or fields.Future research should explore how novelty trajectories differ across institutional contexts (e.g., general purpose vs. specialty journals; open-vs.closed-access journals) and how integration occurs through qualitative mechanisms such as discourse framing or paradigm negotiation.Applying this typology in experimental or evaluative settings, such as funding decisions, early-career recognition, or portfolio diversity assessments, could yield new insights into the strategic risks and rewards of novelty.</p>
<p>Policy Implications for Science and Innovation.The triadic novelty framework offers actionable implications for science and innovation policy.As research evaluation increasingly relies on quantitative metrics, our findings uncover a potential structural bias against contributions that do not immediately align with established knowledge structures.Such bias may unintentionally discourage risk-taking and long-term impact research, especially in areas where conceptual breakthroughs require longer time to diffuse or challenge dominant paradigms.To foster a more balanced and inclusive research ecosystem, policymakers should complement traditional impact measures with novelty-sensitive indicators to help identify undervalued intellectual frontiers and support a more diverse and forward-looking portfolio of scientific innovation.</p>
<p>Materials and Methods</p>
<p>Analysis Dataset.We applied the Triadic Novelty framework to the PNPS, an emerging interdisciplinary field that investigates topics such as altruism, charity, volunteerism, nonprofits, civil society, and social economy (46).The full dataset comprised 60,684 articles published from 1899 to 2022, with 513,406 references retrieved from the WoS Core Collection.We selected WoS due to its comprehensive coverage and standardized classification system (47).Only documents labeled as "articles", including journal papers and conference proceedings with cited references, were included (44).The dataset was assembled using a curated list of journals and keywords relevant to the field (48,49), detailed in the Supplementary Information (SI).Given the developmental history of the field, articles published before 1960 were treated as the foundational knowledge base.Novelty in later publications was measured relative to this base using the field citation network.Our primary analysis focused on 41,623 articles published between 1960 and 2017, citing 313,172 unique references indexed in WoS.The five-year forward citation window was used to calculate novelty impact scores, enabling consistent measurement of early-stage scholarly recognition.</p>
<p>Network Construction.The WCCN was operationalized by the co-cited WoS subject categories (50).For instance, if an article cites a reference to the subject category 'Public Economics' and another to 'Applied Physics,' nodes 'Public Economics' and 'Applied Physics' were connected in the WCCN of WoS subject categories.The weight of each tie was determined by the total number of articles that co-cited the two subject categories.Specifically, in our dataset, one reference could have up to nine different subject categories.When a reference was linked to multiple categories, the categories were weighted equally.Novelty scores then calculated within this network.Although the 254 subject categories span all disciplines and may only loosely align with topical nuances of individual references, they still meaningfully reflect the broader orientations and disciplinary approaches (51,52), thereby serving as a useful indicator of the topical evolution of the field.</p>
<p>Regression Estimator.To examine how different types of novelty are recognized within current academic reward systems, we used a mixed-effects negative binomial regression model to estimate their association with five-year citation counts.This modeling approach is appropriate given the nature of our outcome variable: citation counts are non-negative integers with a distribution characterized by overdispersion (i.e., variance exceeds the mean), which violates assumptions of standard linear and Poisson models (53).The negative binomial model accounts for this overdispersion by introducing an additional dispersion parameter, resulting in more accurate standard errors and valid statistical inference.</p>
<p>Given the hierarchical structure of our data, with articles nested within journals () and publication years (), we included random intercepts for both journal and year to capture unobserved heterogeneity (54).The random effect (shown in Table 2) for journals exhibited substantial variance ( J = 1.035) indicating that journal venue played a significant role in citation performance, likely reflecting differences in visibility, disciplinary culture, and audience reach (55).The variance for publication year was smaller but still meaningful ( J = 0.382), suggesting modest temporal shifts in citation behavior due to evolving research priorities or external conditions.Including these random effects improved the precision of fixed-effect estimates by isolating the influence of novelty from contextual variation.</p>
<p>The dependent variable was the citation counts received within five years of publication, as recorded in the WoS, limited to citations from other WoS-indexed publications.Independent variables included initial novelty scores and novelty impact scores for each of the three novelty types, along with their interaction terms.To control for disciplinary variation, we included dummy variables based on the WoS domains (52).The estimation is summarized as follows:
5 − 𝑦𝑒𝑎𝑟 𝐶𝑖𝑡𝑎𝑡𝑖𝑜𝑛 𝐶𝑜𝑢𝑛𝑡 !" ~ 𝛽 $ + 𝛽 % 𝑁 !" &amp;!'())<em> + 𝛽 + 𝑁 !" &amp;!'())</em> (𝑁 !" &amp;!'())<em> × 𝑆 !" &amp;!'())</em> ) + 𝛽 , 𝑁 !" &amp;!'())<em> 𝑁 !" -./)</em>!01 + 𝛽 2 𝑁 !" &amp;!'())<em> (𝑁 !" -./)</em>!01 × 𝑆 !" 3(4 -./)<em>!01 ) +
 5  !" -./)</em>!01 +  6 ( !" -./)<em>!01 ×  !" 7!8 -./)</em>!01 ) +  9  !" :.(;&lt;.<em>=+  &gt; ( !" :.(;&lt;.</em>=×  !" :.(;&lt;.<em>= ) + ?( !" :.(;&lt;.</em>=×  !" :.(;&lt;.*=!</p>
<p>)
𝛽 %$ 𝐹𝑖𝑒𝑙𝑑 @'0@0! + 𝛽 %% 𝐹𝑖𝑒𝑙𝑑 .&amp;4 + 𝛽 %+ 𝐹𝑖𝑒𝑙𝑑 B!'8)= + 𝛽 %, 𝐹𝑖𝑒𝑙𝑑 C)04 + (1 | 𝐽𝑜𝑢𝑛𝑟𝑎𝑙 " ) + (1 | 𝑌𝑒𝑎𝑟 ! ) + 𝜀 !"
To improve interpretability and stabilize the scale of the regression coefficients, several transformations are applied.The Pioneer novelty impact score was multiplied by 100.Both Maverick Enhancement and Diminishment were rescaled by multiplying their absolute values by 100.For Vanguard novelty, the initial novelty score required additional processing.Articles with a raw Vanguard score vectors equal to (0,0,0) were excluded.The remaining articles were ranked based on their score vectors, then the ranks were inverted so that higher-ranked articles corresponded to higher numerical values, and then the values were log-transformed to standardize the distribution.The Vanguard impact score was also rescaled by multiplying 100.</p>
<p>Baseline Model Design.To enable meaningful comparisons of novelty scores across time and domains, we simulated a baseline model that captures expected levels of novelty under random assignment.This model generated synthetic papers for each year between 1960 and 2017 that mirrored the observed distribution in the real data.Each synthetic paper retained the same number of subject categories as its real counterpart.These subject categories are assigned stochastically, based on a preferential attachment mechanism: the probability of selecting a subject category  is proportional to its cumulative prior citations, formally defined as node strength () from the previous year.To ensure that rarely cited categories still have a non-zero selection probability, we introduced a linear attractiveness term .The resulting selection probability is modeled as () ~ () + .</p>
<p>We tested several values of  and selected the one that best replicated the temporal trend of novelty scores in the real data.Table 5 shows the average Spearman correlations between the synthetic and real data across five different values of .The highest agreement between observed and simulated novelty trajectories was achieved at  = 0.05, which we adopted as the optimal setting.At this value, the Spearman correlation between model and real trends exceeded 0.7 for most novelty types, confirming the ability of the model to capture structural patterns while allowing us to detect deviations.</p>
<p>Standardization Procedure.We then standardized the real novelty scores using zscores based on the simulated baseline distribution.For each real paper , we computed a standardized score as  != ( !−  !)/ !, where  ! is the raw novelty score of the real paper, and  ! and  ! are the mean and standard deviation of the simulated novelty scores for synthetic papers with similar properties.This process highlighted deviation from expected novelty, thereby emphasizing contributions that exceeded structural randomness.To validate this standardization, we compared the aggregate distributions of real and simulated novelty scores.As shown in Figure S2, the baseline model closely approximates the empirical distribution of real data.</p>
<p>Python Package.The Triadic Novelty measures and their normalized versions are available through an open-source Python package named "triadic-novelty."The package can be installed using pip or accessed directly from the GitHub repository at http://github.com/philanthrophysics/triadic-novelty.("norms", "reciprocity"), ("cooperation", "reciprocity"), ("generalized reciprocity", "evolution"), ("cooperative behavio<em>", "experiment</em>"), ("cooperative behavio<em>", "group</em>"), ("cooperative behavio<em>", "human"), ("cooperative behavio</em>", "animal<em>"), ("cooperative behavio</em>", "social"), ("cooperation", "social"), ("cooperation", "human"), ("cooperation", "animal"), "prosocial<em>", "pro-social</em>", "empathic behavio<em>", "empathetic behavio</em>", "other regarding", "other-regarding", ("giving", "religio<em>"), ("donation", "religio</em>"), ("giving", "faith"), ("donation", "faith"), ("giving", "spiritual<em>"), ("donation", "spiritual</em>"), ("giving", "christian"), ("muslim", "giving"), ("islam", "giving"), ("buddhist", "giving"), ("jewish", "giving"), "zakat", "tzedakah", "tithe", "waqf"</p>
<p>List of Keywords to Remove False Positives</p>
<p>("altruistic", "suicide"), ("civil societ<em>", "revolution</em>"),("warm<em>glow", "familiarity"), ("charity", "mount"),("charity", "salmonella"),("charit</em>", "hospital<em>"), ("altruism", "autonomous vehicle"),("altruism", "autonomous driving"), ("charit</em>", "chronic")</p>
<p>List of Journals</p>
<p>6 |
6
The initial Maverick novelty score for a paper  is defined as: 93:(!%;&lt; () =</p>
<p>Figures and TablesFigure 1 .r 2 article r 1 Figure 2 .
112
Figures and Tables</p>
<p>Figure 3 .
3
Figure 3. Novelty Performances Across Core Journals in Philanthropic and Nonprofit Studies.Each polar axis corresponds to a standardized novelty score or respective impact.</p>
<p>Table 1 .
1
Correlation Between Initial and Impact Novelty Scores by Type
Novelty TypeSpearman Correlation (ρ)Pioneer0.087Maverick (Enhancement)0.216Maverick (Diminishment)0.290Vanguard-0.131</p>
<p>Table 3 .
3
Journal Rankings by Standardized Novelty in Philanthropic and Nonprofit Studies
PioneerMaverickVanguard1PUBLIC RELATIONS REVIEWSOCIAL SCIENCE &amp; MEDICINEAMERICAN NATURALIST2JOURNAL OF ECONOMIC PSYCHOLOGYREVESCO-REVISTA DE ESTUDIOS COOPERATIVOSINTERNATIONAL JOURNAL OF BEHAVIORAL DEVELOPMENT3SOCIETY &amp; NATURAL RESOURCESSOCIETY &amp; NATURAL RESOURCESEVOLUTION4 SOCIAL DEVELOPMENTSOCIAL WORKFORBES5PHYSICA A-STATISTICAL MECHANICS AND ITS APPLICATIONSZYGONHARVARD BUSINESS REVIEW6 BEHAVIORAL ECOLOGYCOMPUTERS IN HUMAN BEHAVIORANIMAL BEHAVIOUR7AMERICAN SOCIOLOGICAL REVIEWINTERNATIONAL ECONOMICS JOURNAL OF SOCIALJOURNAL OF TAXATION8SOCIAL SCIENCE &amp; MEDICINESOCIAL MOVEMENT STUDIESBEHAVIORAL ECOLOGY AND SOCIOBIOLOGY9BEHAVIORAL ECOLOGY AND SOCIOBIOLOGYPUBLIC RELATIONS REVIEWBEHAVIORAL ECOLOGY10 WORLD DEVELOPMENTCOMMUNITY DEVELOPMENTJOURNAL OF EVOLUTIONARY BIOLOGY</p>
<p>Table 4 .
4
Correlation with Disruption Index Only strictly positive values of novelty and disruption scores are included in the analysis (strictly negative for Maverick Diminishment).
Standardized ScoresRaw ScoresPioneer-0.041-0.244Pioneer Impact0.323-0.055Maverick-0.0360.399Maverick Enhancement-0.0310.159Maverick Diminishment-0.0080.242Vanguard0.0340.043Vanguard Impact-0.198-0.127Note:</p>
<p>•</p>
<p>Nonprofit and Voluntary Sector Quarterly (NVSQ, previously named Journal of Voluntary Action Research) • Nonprofit Management and Leadership (NML) • VOLUNTAS: International Journal of Voluntary and Nonprofit Organizations</p>
<p>The Maverick novelty impact score can be measured as the average, median, maximum, or minimum of these changes; we choose the average for this study.
The Vanguard novelty impact score can be measured as the average, median, maximum, or minimum of these changes; we choose the average for this study.
AcknowledgmentsThis work used JetStream2 at Indiana University through allocation CIS230183 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services &amp; Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296.The fourth author thanks the support from the Air Force Office of Scientific Research under Award #FA9550-19-1-0391.(The funders had no role in study design, data collection and analysis, the decision to publish, or preparation of the manuscript.)The first author thanks Dr. Kathi Badertscher for her support in the initial stage of the manuscript.Data Availability StatementThe Web of Science Raw Data (2022 Version) employed in this paper was obtained from the Web of Science under a specific institutional agreement between Clarivate Analytics and Indiana University, which forbids the authors from sharing data derivatives.We, however, share the calculated novelty scores associated with each Web of Science ID, available at https://doi.org/10.5281/zenodo.15741054.Database Assembling ProcessWe combined journal-based and keyword-based searches to build the analysis dataset.All articles appearing in the three core journals are included.Articles published elsewhere were included based on a curated list of keywords.A detailed justification for the approach can be found in reference(36).Lists of keywords and journals are summarized below.List of Curated Keywords"philanthropy", "philanthropic", "philanthropist", "misanthropy", "misanthropic", "charity", "charities", "charitable", "benevolence", "benevolent societ<em>", "altruism", "altruistic", "almsgiving", "gospel of wealth", "gift exchange</em>", "gift-exchange<em>", "gifting", "gift giving", "gift-giving", "collective good</em>", "public good<em>", "the commons</em>", "social economy", "social economies", "gift economy", "gift economies", "grant econom<em>", "grant economies", "generosity", ("generous", "social"), ("generous", "people"), ("generous", "behavio</em>"), ("generous", "act<em>"), ("generous", "giving"), ("gratitude", "giving"), ("gratitude", "help</em>"), ("gratitude", "donate"), ("gratitude", "donation"), ("gratitude", "donating"), ("social capital", "civil societ<em>"), ("trust", "civil societ</em>"), ("communit<em>", "civil societ</em>"), "the third sector<em>", "voluntary sector</em>", "non-distribution constraint<em>", "nondistribution constraint</em>", "501c3", "501(c)(3)", "501c4", "501(c)(4)", "eleemosynary institution<em>", "eleemosynary corporate</em>", "eleemosynary corporation<em>", "eleemosynary organi</em>ation<em>", "civic participation</em>", "civic engagement<em>", "social movement</em>", ("collective action<em>", "social"),("collective action</em>", "event<em>"), ("collective action</em>", "group"),("collective action<em>", "global"), ("collective action</em>", "civil societ<em>"), "grassroots movement</em>", "grassroots group<em>", "grassroots organi</em>ation<em>", "grassroots association</em>", "nonprofit<em>", "non-profit</em>", "not for profit<em>", "not-for-profit</em>", "nongovernmental organi<em>ation</em>", "nongovernmental agenc<em>", "non-governmental organi</em>ation<em>", "non-governmental agenc</em>", "nongovernmental institution<em>", "non-governmental institution</em>", "grantmaking", "grant-making", "grantseeking", "grant-seeking", "endowment foundation<em>", "endowment fund</em>", "community foundation<em>", "family foundation</em>", "private foundation<em>", "corporate foundation</em>", "social enterprise<em>", "social entrepreneur</em>", "low-profit limited liability compan<em>", "b corp", "benefit corporation</em>", "flexible purpose corporation<em>", "community interest compan</em>", "community organizing", "community engagement", "community based organi<em>tion</em>", "community-based organi<em>tion</em>", "voluntary association<em>", "voluntary organi</em>ation<em>", "selfhelp group</em>", "self-help group<em>", "selfhelp association</em>", "self-help association<em>", "selfhelp organi</em>ation<em>", "self-help organi</em>ation<em>", "membership association</em>", "membership organi<em>ation</em>", "giving circle<em>", ("giving", "pledge"), "donor advised fund</em>", "donoradvised fund<em>", "corporation giving", "corporate giving", "workplace giving", "impact invest</em>", "program<em>related investment</em>", "volunteering behavio<em>", "volunteerism", "voluntarism", "voluntaristics", "benefit</em> of volunteering", "helping other<em>", "helping behavio</em>", "giving behavio<em>", "individual giving", "giving money", "giving time", ("donate", "money"), ("donate", "time", "volunteer</em>"), ("donate", "motive<em>"), ("donate", "motivation</em>"), ("donate", "behavio<em>"), ("donate", "decision</em>"), ("donation", "money"), ("donation", "time", "volunteer<em>"), ("donation", "motive</em>"), ("donation", "motivation<em>"), ("donation", "behavio</em>"), ("donation", "decision<em>"), "donor fatigue", "anonymous giving", ("giving", "diaspora"), "planned giving", ("giving", "in-kind"), ("gift", "in-kind"), ("giving", "wealthy"), ("benefit</em> of giving"), ("indian", "giving"), ("indigenous", "giving"), ("native american", "giving"),("native-american", "giving"), "mutual aid*", ("social", "reciprocity"), "serial reciprocity", ("norm", "reciprocity"),
The Pivot Penalty in Research. R Hill, 10.1038/s41586-025-09048-1Nature. 182025</p>
<p>Interdisciplinary Research Has Consistently Lower Funding Success. L Bromham, R Dinnage, X Hua, Nature. 5342016</p>
<p>Tradition and Innovation in Scientists' Research Strategies. J G Foster, A Rzhetsky, J A Evans, Am. Sociol. Rev. 802015</p>
<p>Bias Against Novelty in Science: A Cautionary Tale for Users of Bibliometric Indicators. J Wang, R Veugelers, P Stephan, Res. Policy. 462017</p>
<p>The Essential Tension: Tradition and Innovation in Scientific Research. T S Kuhn, The Essential Tension: Selected Studies in Scientific Tradition and Change. University of Chicago Press1977</p>
<p>Blurring Disciplinary Boundaries. G Mcbean, A Martinelli, Science. 3582017</p>
<p>Citation Metrics Covary with Researchers' Assessments of the Quality of Their Works. D W Aksnes, F N Piro, L W Fossum, Quant. Sci. Stud. 42023</p>
<p>. B Uzzi, S Mukherjee, M Stringer, B Jones, Atypical Combinations and Scientific Impact. Science. 3422013</p>
<p>A Network-based Normalized Impact Measure Reveals Successful Periods of Scientific Discovery Across Discipline. Q Ke, A J Gates, A.-L Barabási, Proc. Natl. Acad. Sci. Natl. Acad. Sci2023120e2309378120</p>
<p>Measuring the Influence of Non-scientific Features on Citations. S Mammola, E Piano, A Doretto, E Caprio, D Chamberlain, Scientometrics. 1272022</p>
<p>Beyond Citations: Measuring Novel Scientific Ideas and their Impact in Publication Text. S Arts, N Melluso, R Veugelers, 10.1162/rest_a_01561Rev. Econ. Stat. 1. 332025</p>
<p>What Types of Novelty Are Most Disruptive?. E Leahey, J Lee, R J Funk, Am. Sociol. Rev. 882023</p>
<p>. R J Funk, J Owen-Smith, A Dynamic Network Measure of Technological Change. Manag. Sci. 632017</p>
<p>Large Teams Develop and Small Teams Disrupt Science and Technology. L Wu, D Wang, J A Evans, Nature. 5662019</p>
<p>Surprising Combinations of Research Contents and Contexts are Related to Impact and Emerge with Scientific Outsiders from Distant Disciplines. F Shi, J Evans, Nat. Commun. 1416412023</p>
<p>What Is Disruptive Innovation?. C Christensen, M Raynor, R Mcdonald, Harv. Bus. Rev. 2015</p>
<p>C C Markides, P A Geroski, Fast Second: How Smart Companies Bypass Radical Innovation to Enter and Dominate New Markets. John Wiley &amp; Sons2004</p>
<p>Breakthrough recognition: Bias Against Novelty and Competition for Attention. S Chai, A Menon, Res. Policy. 482019</p>
<p>Abandoning Objectives: Evolution Through the Search for Novelty Alone. J Lehman, K O Stanley, Evol. Comput. 192011</p>
<p>R K Merton, The Sociology of Science: Theoretical and Empirical Investigations. N W Storer, University of Chicago Press1979</p>
<p>Migraine and Magnesium: Eleven Neglected Connections. D R Swanson, Perspect. Biol. Med. 311988</p>
<p>A Vision for the Future of Genomics Research. F S Collins, E D Green, A E Guttmacher, M S Guyer, Nature. 4222003</p>
<p>The Story of Econophysics. K C Dash, 2019Cambridge Scholars Publishing1st edition</p>
<p>F Znaniecki, The Social Role of the Man of Knowledge. Columbia University Press1940</p>
<p>Structural Holes and Good Ideas. R S Burt, Am. J. Sociol. 1102004</p>
<p>Multidisciplinarity, Interdisciplinarity and Transdisciplinarity in Health Research, Services, Education and Policy: 1. Definitions, Objectives, and Evidence of Effectiveness. B C K Choi, A W P Pak, Clin. Investig. Med. Med. Clin. Exp. 292006</p>
<p>D Wang, A.-L Barabási, The Science of Science. Cambridge University Press20211st edition</p>
<p>How Cutting-Edge Computer Chips Are Speeding Up the AI Revolution. D Garisto, Nature. 6302024</p>
<p>Artificial intelligence in innovation Research: A Systematic Review, Conceptual Framework, and Future Research Directions. M M Mariani, I Machado, V Magrelli, Y K Dwivedi, Technovation. 1221026232023</p>
<p>The Global Geography of Artificial Intelligence in Life Science Research. L Schmallenbach, T W Bärnighausen, M J Lerchenmueller, Nat. Commun. 1575272024</p>
<p>Make First Detection of Gravitational Waves Produced by Colliding Neutron Stars. Virgo Ligo, MIT News Mass. Inst. Technol. 2017</p>
<p>Particle Physics Isn't Going to Die -Even if the LHC Finds No New Particles. Nature. 6072022</p>
<p>Data-driven Predictions in the Science of Science. A Clauset, D B Larremore, R Sinatra, 10.1126/science.aal4217Science. 2017</p>
<p>Chapter 10. K Mehlhorn, P Sanders, Shortest Paths" in Algorithms and Data Structures: The Basic Toolbox. Springer2008</p>
<p>Measuring Dispositions for Lexicographic Preferences of Environmental Goods: Integrating Economics. R S Rosenberger, G L Peterson, A Clarke, T C Brown, Psychology and Ethics. Ecol. Econ. 442003</p>
<p>Intellectual Structure and Dynamics of Novelty Within Philanthropic and Nonprofit Studies: A Computational and Structural Analysis. J Ai, 2024Indiana University</p>
<p>The Memory of Science: Inflation, Myopia, and the Knowledge Network. R K Pan, A M Petersen, F Pammolli, S Fortunato, J. Informetr. 122018</p>
<p>Emergence of Scaling in Random Networks. A.-L Barabási, R Albert, Science. 2861999</p>
<p>Competition and Multiscaling in Evolving Networks. G Bianconi, A.-L Barabási, Europhys. Lett. 544362001</p>
<p>A General Theory of Bibliometric and Other Cumulative Advantage Processes. D D S Price, J. Am. Soc. Inf. Sci. 271976</p>
<p>The disruption index is biased by citation inflation. A M Petersen, F Arroyave, F Pammolli, Quant. Sci. Stud. 52024</p>
<p>Analyzing Affiliation Networks. S P Borgatti, D S Halgin, The SAGE Handbook of Social Network Analysis. SAGE Publications Ltd2011</p>
<p>J Scott, Social Network Analysis. 198822</p>
<p>. Clarivate, 27 September 2024Document types. Web of Science. Available at</p>
<p>The Nonprofit Sector: A Research Handbook. W. W. Powell, R. Steinberg2006Yale University Press2nd Edition</p>
<p>Large-scale Comparison of Bibliographic Data Sources: Scopus, Web of Science. M Visser, N J Van Eck, L Waltman, Dimensions, Crossref, and Microsoft Academic. Quant. Sci. Stud. 22021</p>
<p>D F Burlingame, Philanthropy in America: A Comprehensive Historical Encyclopedia. Bloomsbury Academic20043 volumes</p>
<p>D H Smith, R A Stebbins, M A Dover, A Dictionary of Nonprofit Terms and Concepts. Indiana University Press2006</p>
<p>. C Birkle, D A Pendlebury, J Schnell, J Adams, Web Of, Science As a Data Source for Research on Scientific and Scholarly Activity. Quant. Sci. Stud. 12020</p>
<p>Indicators of the Interdisciplinarity of Journals: Diversity, Centrality, and Citations. L Leydesdorff, I Rafols, J. Informetr. 52011</p>
<p>Practical Method to Reclassify Web of Science Articles into Unique Subject Categories and Broad Disciplines. S Milojević, Quant. Sci. Stud. 12020</p>
<p>Regression analyses of counts and rates: Poisson, overdispersed Poisson, and negative binomial models. W Gardner, E P Mulvey, E C Shaw, Psychol. Bull. 1181995</p>
<p>Multilevel Analysis: An Introduction To Basic And Advanced Multilevel Modeling. T A B Snijders, 2011SAGE Publications LtdSecond edition</p>
<p>A comparison among citation-based journal indicators and their relative changes with time. X Z Liu, H Fang, J. Informetr. 141010072020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5918 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5918</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5918</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-270391318</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.07835v4.pdf" target="_blank">SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific Literature</a></p>
                <p><strong>Paper Abstract:</strong> We present SciRIFF (Scientific Resource for Instruction-Following and Finetuning), a dataset of 137K instruction-following instances for training and evaluation, covering 54 tasks. These tasks span five core scientific literature understanding capabilities: information extraction, summarization, question answering, claim verification, and classification. SciRIFF is unique in being entirely expert-written, high-quality instruction-following dataset for extracting and synthesizing information from research literature across diverse scientific fields. It features complex instructions with long input contexts, detailed task descriptions, and structured outputs. To demonstrate its utility, we finetune a series of large language models (LLMs) using a mix of general-domain and SciRIFF instructions. On nine out-of-distribution held-out tasks (referred to as SciRIFF-Eval), LLMs finetuned on SciRIFF achieve 70.6% average improvement over baselines trained only on general-domain instructions. SciRIFF facilitates the development and evaluation of LLMs to help researchers navigate the rapidly growing body of scientific literature.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5918",
    "paper_id": "paper-270391318",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0085355,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific Literature
29 Sep 2025</p>
<p>David Wadden 
Kejian Shi kejian.shi@yale.edu 
Yale University</p>
<p>Jacob Morrison 
Allen Institute for AI</p>
<p>University of Washington</p>
<p>Alan Li 
Yale University</p>
<p>Aakanksha Naik 
Allen Institute for AI</p>
<p>Shruti Singh 
Yale University</p>
<p>Nitzan Barzilay 
Allen Institute for AI</p>
<p>Hebrew University
5 MIT</p>
<p>Kyle Lo 
Allen Institute for AI</p>
<p>University of Washington</p>
<p>Tom Hope 
Allen Institute for AI</p>
<p>Hebrew University
5 MIT</p>
<p>Luca Soldaini 
Allen Institute for AI</p>
<p>Shannon Zejiang Shen 
Doug Downey 
Allen Institute for AI</p>
<p>Northwestern University</p>
<p>Hannaneh Hajishirzi 
Allen Institute for AI</p>
<p>University of Washington</p>
<p>Arman Cohan arman.cohan@yale.edu 
Allen Institute for AI</p>
<p>Yale University</p>
<p>Guanting Dong 
Hongyi Yuan 
Keming Lu 
Chengpeng Li 
Mingfeng Xue 
Dayiheng Liu 
Wei Wang 
Zheng Yuan 
Chang Zhou 
Jingren 2024 Zhou 
How 
Abhimanyu Dubey 
Abhinav Jauhri 
Abhinav Pandey 
Abhishek Kadian 
Ahmad Al-Dahle 
Aiesha Letman 
Akhil Mathur 
Alan Schelten 
Amy Yang 
Angela Fan 
Anirudh Goyal 
Anthony Hartshorn 
Aobo Yang 
Jason Fries 
Leon Weber 
Natasha Seelam 
Gabriel Al- Tay 
Debajyoti Datta 
Samuele Garda 
Sunny Kang 
Rosaline Su 
Wojciech Kusa 
Samuel Cahyawijaya 
Fabio Barth 
Simon Ott 
Matthias Samwald 
Stephen Bach 
Stella Biderman 
Mario Sänger 
Bo Wang 
Alison Callahan 
Daniel León Periñán 
Théo Gi- Gant 
Patrick Haller 
Jenny Chim 
Jose Posada 
John Giorgi 
Rangasai Karthik 
Marc Sivaraman 
Marianna Pàmies 
Robert Nezhurina 
Michael Martin 
Moritz Cul- Lan 
Nathan Freidank 
Shubhan- Shu Dahlberg 
Shamik Mishra 
Nicholas Bose 
Yanis Broad 
Shlok Labrak 
Sid Deshmukh 
Ayush Kiblawi 
Minh Singh 
Trishala Chien Vu 
Jonas Neeraj 
Albert Golde 
Yu Gu 
Robert Tinn 
Hao Cheng 
Michael Lucas 
Naoto Usuyama 
Xiaodong Liu 
Tristan Naumann 
Jianfeng Gao 
Hoifung 2021 Poon 
Tanishq Gupta 
Mohd Zaki 
Devanshi Khatsuriya 
Kausik Hira 
Anoop Krishnan 
Tianyu Han 
Lisa C Adams 
Jens-Michalis Papaioan- Nou 
Paul Grundmann 
Tom Oberhauser 
Alexander Löser 
Daniel Truhn 
Keno Kyrill 
Dan Hendrycks 
Collin Burns 
Steven Basart 
Andy Zou 
Rezarta Islamaj 
Robert Leaman 
Sun Kim 
Dongseop Kwon 
Chih-Hsuan Wei 
Donald C Comeau 
Yifan Peng 
David Cissel 
Cathleen Coss 
Carol Fisher 
Yizhong Wang 
Swaroop Mishra 
Pegah Alipoormo- Labashi 
Allen Institute for AI</p>
<p>Yeganeh Kordi 
Amirreza Mirzaei 
Atharva Naik 
ArutArjun Ashok 
Selvan Dhanasekaran 
Anjana Arunkumar 
David Stap 
Eshaan Pathak 
Giannis Karamanolakis 
Haizhi Lai 
Ishan Puro- Hit 
Ishani Mondal 
Jacob Anderson 
Kirby Kuznia 
Krima Doshi 
Kuntal Kumar Pal 
Maitreya Patel 
Mehrad Moradshahi 
Mihir Parmar 
Mirali Purohit 
PhaniNeeraj Varshney 
Rohitha Kaza 
RavsehajPulkit Verma 
Singh Puri 
Rushang Karia 
Savan Doshi 
Shailaja Keyur Sampat 
Siddhartha Mishra 
Sujan 
Jason Wei 
Maarten Bosma 
Vincent Zhao 
Kelvin Guu 
Adams Wei Yu 
Brian Lester 
Nan Du 
Andrew M Dai 
Quoc V Le 
Finetuned 
Shufa Wei 
Xiaolong Xu 
Xianbiao Qi 
Xi Yin 
Jun Xia 
Jingyi Ren 
Peijun Tang 
Yuxiang Zhong 
Yihao Chen 
Xiaoqin Ren 
Yuxin Liang 
Liankai Huang 
Kai Xie 
Weikang Gui 
Wei Tan 
Shuanglong Sun 
Yongquan Hu 
Qinxian Liu 
Nanjin Li 
Chihao Dai 
Tong Xie 
Yuwei Wan 
Wei Huang 
Zhenyu Yin 
Yixuan Liu 
Shaozhou Wang 
Qingyuan Linghu 
Chunyu Kit 
An Yang 
Baosong Yang 
Binyuan Hui 
Bo Zheng 
Bowen Yu 
Chengpeng Li 
Chengyuan Li 
Fei Huang 
Haoran Wei 
Huan Lin 
Da Yin 
Fan Yin 
Ming Zhong 
Hritik Bansal 
Jiawei Han 
Kai-Wei Chang 
Dynosaur </p>
<p>Kehua Feng
Keyan Ding, Weijie Wang, Xiang Zhuang, Ming Qin, Yu ZhaoZeyuan Wang, Jianhua Yao</p>
<p>Association for Computational Linguistics
TorontoCanada</p>
<p>Dawn Song, and Jacob Steinhardt
Mantas Mazeika</p>
<p>Preeti Gokal Kochar
Stella Koppel, Keiko Sekiya, Janice Ward, Susan Schmidt, and Zhiyong Lu. 2021aRob Guzman, Dorothy Trinh, Deborah Whitman</p>
<p>SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific Literature
29 Sep 2025DD220F08E10EA65F864B9CE245CADF84arXiv:2406.07835v4[cs.CL]
We present SciRIFF (Scientific Resource for Instruction-Following and Finetuning), a dataset of 137K instruction-following instances for training and evaluation, covering 54 tasks.These tasks span five core scientific literature understanding capabilities: information extraction, summarization, question answering, claim verification, and classification.SciRIFF is unique in being entirely expert-written, highquality instruction-following dataset for extracting and synthesizing information from research literature across diverse scientific fields.It features complex instructions with long input contexts, detailed task descriptions, and structured outputs.To demonstrate its utility, we finetune a series of large language models (LLMs) using a mix of general-domain and SciRIFF instructions.On nine out-of-distribution heldout tasks (referred to as SciRIFF-Eval), LLMs finetuned on SciRIFF achieve 70.6% average improvement over baselines trained only on general-domain instructions.SciRIFF facilitates the development and evaluation of LLMs to help researchers navigate the rapidly growing body of scientific literature.* Equal contribution.Full author contributions here.</p>
<p>Introduction</p>
<p>LLMs have the potential to advance scientific progress by helping researchers navigate and draw insights from the scientific literature.To accomplish these tasks, LLMs must be able to reliably follow a range of instructions-e.g. to extract information, summarize content, or answer questions-when given research articles as input.These instructions are often grounded in entire scientific articles, featuring longer inputs than other typical instruction-following resources in the science domain.In addition, the model's responses may need to be structured according to a specific format or schema that supports aggregation for literature review (Marshall and Wallace, 2019), or is consumable by software components like augmented reading interfaces (Lo et al., 2023;Palani et al., 2023).For example, when analyzing clinical trials, responses should follow a PICO framework (Population, Intervention, Comparison, Outcome), or when examining methodology papers, follow a standardized format capturing study design, sample size, statistical methods, and key findings, or when performing question answering or fact checking, accompany appropriate evidence for attribution and verification.Such outputs can be represented as json to ensure structured, consistent formatting that enhance both human readability and seamless machine processing (e.g., for claim verification and the input claim "Coffee consumption reduces diabetes risk", the response could be { "verdict": "support", "evidence": ["Study A shows 23% risk reduction", "Meta-analysis B confirms protective effect"], "confidence": "moderate"}).</p>
<p>While bespoke models are available for specific scientific literature understanding tasks, models that can flexibly follow instructions in domainspecific settings of science are preferable both for their ease of use (offering a unified input / output interface) and for their ability to generalize to novel applications and settings within that domain.</p>
<p>The general instruction-following capabilities of LLMs have advanced rapidly in recent years, largely due to the availability of general-purpose instruction datasets (Zhang et al., 2023a).In addition, some instruction-following resources are available for specific scientific and medical tasks, such as describing the properties of a molecule (Fang et al., 2024;Yu et al., 2024) or answering medical exam questions (Toma et al., 2023;Han et al., 2023) (see §5 for a review).However, few resources are available for supporting instruction-following for flexible scientific literature understanding capabilities across a range of domains.</p>
<p>In this work, we present SciRIFF (Scientific Resource for Instruction-Following and Finetuning), a comprehensive dataset to enable progress on instruction-following over scientific literature.SciRIFF includes 137K demonstrations for 54 tasks A.1 spanning five broad scientific literature understanding task categories: information extraction, summarization, question answering, claim verification, and classification.</p>
<p>SciRIFF covers five scientific domains, ranging from AI to clinical medicine (Figure 2).</p>
<p>Unlike synthetic or LLM-distilled instructionfollowing data (e.g., Lambert et al., 2024a), we prioritize human-annotated data to better capture nuanced domain expertise, complex structures, and reasoning required for scientific tasks.Additionally, existing datasets undergo individualized, manually written processes for data conversion to diverse instructions and undergo expert verification, ensuring accuracy and reliability ( §2.1).</p>
<p>Our resource is a unique and specialized instruction-following meta-dataset.As illustrated in Figure 1 and with sample prompt templates provided in Appendix F, it is characterized by: (1) grounding every instance in scientific articles or texts, (2) requiring structured and complex responses, such as answers paired with attributions (i.e., tracing the source of the answer), and (3) featuring longer input contexts compared to most existing resources in the science domain (see Figure 5 and Table 7 in Appendix A).</p>
<p>All instruction templates are created by experts (authors of the paper) to ensure quality.Our experiments ( §4) show that simple templates-similar to those used in prior work such as FlanV2 (Chung et al., 2024) or generated by an LLM (GPT-4o)-do not capture the complexity of our tasks.As a result, models finetuned on these instructions perform substantially worse than those using our expert-crafted instructions.</p>
<p>We also present a new benchmark dataset SciRIFF-Eval ( §3.1) for evaluating instructionfollowing capabilities of LLMs in the science domain.Specifically, we hold out nine datasets from SciRIFF as an unseen evaluation benchmark which covers a representative range of skills and tasks.To demonstrate the utility of SciRIFF in improving scientific literature instruction following, we perform supervised finetuning experiments on several LLMs ranging different sizes. 1 When finetuned on a mix of SciRIFF and general open-source instruction-following data (i.e., Tülu v2 (Ivison et al., 2023a)), our models show consistent improvements on SciRIFF-Eval compared to training on general-domain instructions alone.Our evaluation tasks test true out-of-distribution generalization with formats and templates entirely excluded from training.</p>
<p>In summary, our contributions are as follows: • We introduce SciRIFF, a high-quality and comprehensive resource for instruction-following in the science domain, containing 137K instances covering a wide range of tasks.</p>
<p>• We present SciRIFF-Eval, a diverse evaluation suite in scientific literature understanding (4.1K selected instances from unseen tasks).</p>
<p>• We release a range of LLMs finetuned on SciR-IFF, achieving substantial improvements in scientific literature instruction-following, and conduct experiments showing insights on training strategy and instruction data scaling.</p>
<p>• We release SciRIFF dataset, evaluation suite SciRIFF-Eval, model checkpoints, and code to enable the community to reproduce our results and contribute to task sourcing for broader coverage.</p>
<p>SciRIFF</p>
<p>SciRIFF is a comprehensive instruction-following resource for real-world scientific literature understanding, with 137K instructions for training and indomain validation.In addition, the test set SciRIFF-Eval includes 4.1K instances from held-out tasks.Our resource spans five task categories and subjects, (Figure 1), with particular emphasis on attribution and evidence in scientific tasks.Many tasks require models not only to provide answers but also to support them with evidence from the source paper to ensure verifiable outputs.</p>
<p>Our focus is on document-grounded scientific literature understanding tasks, rather than tasks that evaluate scientific knowledge recall (Feng et al., 2024), or general mathematical, problemsolving abilities without reference to scientific literature (e.g., SciInstruct (Zhang et al., 2024a), MMLU (Hendrycks et al., 2021a)).In addition</p>
<p>Dataset construction</p>
<p>We construct SciRIFF through a rigorous pipeline that transforms existing scientific literature datasets into high-quality instruction-following instances, which involves template engineering, output schema design, and quality control steps that go well beyond simple dataset reformatting.See § A.1 for full task list.We perform all template writing and annotation with domain experts.Domain experts are the paper authors with extensive experience in NLP.We chose this approach rather than using synthetic data (e.g., (Köksal et al., 2023;Li et al., 2023)).We believe it is sensible to exhaust available humanannotated resources for this emerging area before turning to potentially noisy synthetic data generation (see Appendix F for sample templates, which show the complexity of the tasks.)Further, in §4 we show that using templates from prior work, or using LLMs to generate templates results in significant decline in performance.In addition, we need high-quality evaluation data, which we construct by holding out nine SciRIFF tasks as an evaluation benchmark ( §3.1).</p>
<p>We adopt json as the output format for structured tasks (34 of 54 tasks).3json is a convenient format for tasks requiring attribution, such as information extraction and question answering, where outputs must explicitly pair answers with supporting evidence in a human-and-machine-readable format.Our training set spans multiple scientific domains (Figure 2).We create instruction mixes of varying context lengths. 4We refer readers to Appendix A.3 for details and statistics.</p>
<p>Dataset selection criteria We focus on scientific literature understanding tasks in which the model is given a portion of scientific text as input, and is instructed to produce a response derived directly from the text.The task families include summarization, reading comprehension, entailment, classification, and information extraction, which are relevant for real-world use cases (e.g., meta-analysis of literature, clinical decision-making, augmented reading).We provide detailed information and citations of all source datasets in Appendix A. We exclude datasets that require retrieval from document collections (e.g., open-domain QA), since it is unclear how to build instruction-response pairs from them.We also exclude datasets that assess general reasoning and mathematical problem-solving skills without grounding on scientific literature, such as ScienceQA (Lu et al., 2022), SciBench (Wang et al., 2023b), andMATH (Hendrycks et al., 2021b) since such resources already exist.Additionally, we only keep datasets that are publicly available, have a permissive license, and are well-documented and actively maintained.See Appendix A.1 for the complete task list.</p>
<p>Quality Verification.Each template was verified by an additional author for clarity and correctness.We will include guidelines and best practices for prompt-writing in the release and aim to promote community contributions to expand SciRIFF through our open-sourced data collection process.</p>
<p>Experiment setup</p>
<p>We conduct supervised finetuning experiments to evaluate the effectiveness of SciRIFF in improv-ing LLM performance on scientific instructionfollowing tasks across various model families and sizes.Our experiments explore different data configurations and their impact on scientific instruction-following as measured through SciRIFF-Eval described in §3.1.</p>
<p>Evaluation</p>
<p>We selected a set of nine tasks from SciRIFF for evaluation, designed to cover a diverse range of task categories and scientific domains.SciRIFF tests true out-of-distribution generalization with instructions entirely excluded from training.The inputs, outputs, and evaluation metrics for each task are summarized in Table 1.Additional details of evaluation tasks are included in Appendix D.</p>
<p>Scientific Instruction Finetuning</p>
<p>Our goal is to adapt pretrained LLMs to the scientific literature domain.We conduct full finetuning experiments using a range of models and data configurations to assess the effectiveness of SciRIFF.In §4.3, we present an additional analysis examining the potential of using SciRIFF for continual finetuning of instruction-tuned models, exploring a compute-efficient strategy for adaptation.</p>
<p>Data sources</p>
<p>We finetune using two primary datasets: (1) SciRIFF,5 and (2) Tülu V2 Mix (Ivison et al., 2023b), an open-source high-quality general-domain instruction-following dataset that includes demonstrations from various sources, both human-written (e.g., Flan (Wei et al., 2022)) and distilled from proprietary LLMs (e.g., ShareGPT6 , Open Assistant7 ).The original Tülu V2 Mix contains 326,154 examples, including 7.5K scientific literature understanding demonstrations which overlap (i.e.contaminated) with our evaluation set SciRIFF-Eval.We remove those 7.5K examples for clean experiments and to avoid contamination with SciRIFF-Eval.For all experiments, we consistently use this filtered version and refer to this as Tülu V2 Mix to maintain controlled finetuning and unbiased evaluations.</p>
<p>Base models We use following base LLMs as starting points: Llama 3.1-8B (Touvron et al., 2023b) Qwen 2.5-1.5B(Yang et al., 2024).8While our primary focus is on improving base models, we also experiment with models that have undergone proprietary instruction tuning and preference optimization ("-instruct" versions) (Ouyang et al., 2022).</p>
<p>Although direct comparisons with "-instruct" models are complicated by unknown training details, we show that SciRIFF can provide additional value even in these cases.We note, however, that our main results and analyses focus on the controlled experiments with base models where we can fully account for all training conditions.</p>
<p>Finetuning data configurations For each model, we explore three data configurations: (1) Tülu V2 Mix only, to establish a baseline for general instruction-following;</p>
<p>(2) SciRIFF only, to assess the impact of scientific instruction data in isolation; and (3) SciRIFF+Tülu, combining the general and scientific instruction data.</p>
<p>Results</p>
<p>This section discusses our key results and findings.</p>
<p>Main Results</p>
<p>We report our main experimental results in Table 2.</p>
<p>For fair comparison, all models are finetuned on the same data mixes.We show that training on SciRIFF instances results in the best average performance in each model family.Six frontier models, such as GPT-5, Gemini-2.5-Pro(Gemini-2.5,2025) and Kimi-K2 (Kimi, 2025), serve as strong baselines.Additionally, we evaluate selected domain-expert models for comprehensiveness, including SciL-itLLM 7B (Li et al., 2024), BioMedical-Llama3 8B (Bolton et al., 2024), BioMistral 7B (Labrak et al., 2024), CodeLlama 7B (Rozière et al., 2023), and a weak baseline Llama 2 7B (Touvron et al., 2023a).Furthermore, to demonstrate the necessity of expert-written templates for our tasks, we conduct an ablation study comparing our templates against alternatives in §4.2, with details in Appendix B.</p>
<p>Our key findings are below:</p>
<p>SciRIFF enhances scientific literature understanding Table 2 shows that finetuning on SciR-IFF consistently enhances the overall performance on SciRIFF-Eval.Compared to the corresponding base models finetuned on Tülu, SciRIFF-trained models achieve, on average, 70.6% performance gain.Furthermore, without exception, SciRIFF also adds values when finetuning on "-instruct" models (44.6% on average).Across all model groups, the "-instruct" variants trained exclusively on SciRIFF achieve the highest average scores within their respective groups.Finally, while the new frontier models are very strong, with GPT-5 achieving the top baseline score of 61.summarization task (MuP) generally decreases after SciRIFF finetuning.This suggests that while SciRIFF is particularly effective for enhancing IE capabilities, it may not provide additional benefits for summarization tasks that are likely wellcovered in general instruction-following training.</p>
<p>The fact that frontier models our strong finetuned models achieve only an average score of around 60 highlights the difficulty of SciRIFF-Eval.Model performance remains relatively low on tasks like EI; This is due to a combination of task difficulty and evaluation challenges, which we discuss in §6.</p>
<p>Balancing scientific and general data As shown in Table 2, combining SciRIFF and Tülu V2 Mix training data (SciRIFF+Tülu) yields the best performance on SciRIFF-Eval for base models.This suggests that incorporating general instructionfollowing data may provide some broader capability transfer, which base models particularly benefit from, though the impact remains limited (within 2.2%).On the other hand, training "-instruct" models exclusively on SciRIFF data proves to be slightly more effective (within 1% on average).</p>
<p>Comparing with domain-specialized baselines Models trained on in-domain scientific corpora in continual pretraining, followed by instructiontuning for science literature tasks, can be very com-petitive (e.g., SciLitLLM at 60.3 on SciRIFF-Eval).</p>
<p>In contrast, models specialized for biomedical and general science tasks (e.g., BioMedical, BioMistral) consistently underperform in literature understanding.Llama 2 7B achieves an average score of only 19.5, with near-zero performance on IE tasks (BioRED and SciERC) partly due to its inability to follow JSON output requirements.We also observe that CodeLlama, likely benefiting from exposure to JSON and code-based reasoning improvements, outperforms Llama 2 and BioMistral.None of the specialized models match the performance of our approach, which uniquely leverages SciRIFF training to enhance scientific literature understanding.</p>
<p>Grounded Attribution vs. General Reasoning</p>
<p>While highly capable at strong general reasoning, DeepSeek-V3.1 and Kimi-K2 show lower performance on tasks requiring grounded attribution.Specifically, Table 2 shows their evidence-finding scores on Qasper and SciFact are lower than other frontier models, as is their performance on Multi-Cite.This suggests a distinction between general problem-solving and the specific skill of finding and attributing evidence from a given text.This finding, also discussed in concurrent work (Li et al., 2025), indicates that strong abstract reasoning does not guarantee proficiency in document-grounded tasks.This reinforces the value of SciRIFF-Eval as a specialized benchmark for measuring this crucial, evidence-based capability in scientific literature understanding.</p>
<p>Template Ablation</p>
<p>We compare our standard expert-written templates with (1) simple templates that mirror FlanV2 (Chung et al., 2024) and ( 2) templates generated by GPT-4o.We conduct the analysis on selected (due to compute constraints) Base models with SciRIFF only training data, to exclude confounding factors (see templating details in Appendix B.) While prompt ablations are more meaningful for general-purpose language models rather than supervised-finetuned models (Voronov et al., 2024;Kung and Peng, 2023), we present the experiments to validate our design decisions to rely on expert human-written templates for the emerging and complex domain of instruction-following for scientific literature understanding and synthesis.Table 3 shows that expert-written templates, which carefully specify task requirements and output structures, outperform the alternatives.We argue, along with detailed descriptions in §2 and prompt examples at Appendix §F, that expert-written template is preferred.These ablations, while not central to our main contributions and objectives, provide signals on the importance of careful template design for scientific literature understanding tasks.</p>
<p>Continual Finetuning Analysis</p>
<p>In early phase of our study, we explore strategies for efficient adaptation.Specifically, we examined whether starting from an existing instructiontuned checkpoint (on general domain instructions) could provide compute advantages over training from scratch, without hurting SciRIFF-Eval performance.For this controlled experiment, we selected two starting points: (1) Llama 2 base and (2) the same model already finetuned on sciencedecontaminated Tülu V2 Mix (referred as Tülu V2).We explored different training approaches:For Llama 2 base, we train on all available Tülu V2 Mix demonstrations, combined with 1000 instances per SciRIFF task, given the empirical findings in § 4.4.For the Tülu V2 starting point, we perform continual finetuning using 1000 instances per SciR-IFF task, together with a matching number (1000) of instances sampled from Tülu V2 Mix.</p>
<p>Table 4 reports average SciRIFF-Eval performance for our two starting checkpoints using three data configurations.Starting from Tülu V2 performs comparably to Llama 2 base while requiring only 20% of the compute (Table 5).When trained on SciRIFF+Tülu data, models from both starting points achieve similar performance: Tülu V2 is slightly better on science at 7B and nearly identical at 70B.Given that finetuning Tülu V2 requires only 20% of the data, this highlights a computeefficient adaptation for scientific domains, aligning with prior findings (Dong et al., 2024;Shi et al., 2023).While our main experiments ( §3.2) use newer architectures,9 this analysis, along with the</p>
<p>Instruction Data Scale</p>
<p>We define n sci as the number of instances per SciR-IFF task. Figure 3 shows that performance on SciRIFF-Eval increases sharply as n sci rises from 100 to 500 and levels off subsequently.We found that 1,000 instances per science task are sufficient for peak performance for Llama 2 models.Therefore, we set n sci = 1000 across our experiments in the continual finetuning analysis ( §4.3).et al., 2023;Li et al., 2023).In this work, we create instructions using human-written templates ( §2.1) for quality assurance.We refer the readers to see template examples in Appendix F for evidence.</p>
<p>Related Work</p>
<p>Instruction-following resources for scientific literature.Despite many instruction-following collections, few resources focus on scientific literature, which are crucial for assisting researchers and accelerating discovery (Taylor et al., 2022;Xie et al., 2023).Recent work has taken steps in this direction with the development of instruction-following datasets for specific domains such as mathematics (Yue et al., 2024a,b;Shao et al., 2024;Luo et al., 2023;Tang et al., 2024;Toshniwal et al., 2024), medicine (Parmar et al., 2022;Wu et al., 2024;Rohanian et al., 2023), chemistry (Yu et al., 2024;Zhang et al., 2024b), molecular biology (Fang et al., 2024;Tran et al., 2023), materials science (Song et al., 2023), and college-level foundational science (Zhang et al., 2024a).In contrast, SciRIFF both covers a broader set of scientific domains and focuses on document-grounded scientific literature understanding tasks that can power real-world scientific use cases.While recent work such as Li et al. (2024) explores improving language models' scientific understanding through continuous pretraining and SFT, our work specifically contributes a diverse, high-quality instruction dataset for this domain.Some instruction-tuning resources have explored structured output formats (Zhang et al., 2023b;Wang et al., 2023a;Jiao et al., 2023;Gao et al., 2023), but not with a focus on science.Finally, most datasets in SciRIFF have longer instruction contexts than prior works (see Appendix  2025) introduce SciReas, a meta-benchmark for scientific problem-solving that includes a subset of SciRIFF tasks.Their analysis characterizes SciRIFF as focusing on grounded literature comprehension, distinguishing it from abstract reasoning benchmarks.This distinction is supported by their findings that performance on SciRIFF has low correlation with reasoning-focused benchmarks like GPQA (Rein et al., 2023), validating the unique contribution of our resource for measuring essential skills in evidence-based literature understanding.</p>
<p>Conclusion and Future Work</p>
<p>In this work, we introduced SciRIFF, a resource to facilitate progress on LLM instruction-following over scientific literature.We demonstrated that training on SciRIFF leads to significant improvement in model performance on held-out scientific tasks (on average 70.6% over baselines).The large improvements we observe, especially on tasks requiring structured extraction and evidence-finding, underscore the value of targeted data for building practical tools for researchers.</p>
<p>As observed in §4.1, neither our best finetuned models nor the proprietary frontier models are sufficiently strong on SciRIFF-Eval (around 60%), which demonstrates the difficulty of our tasks.Utilizing LLMs to perform more flexible and finegrained evaluations (Kim et al., 2024) represents a promising direction.Future work could focus on reliably generating multiple templates for such complex tasks in a more controlled and principled manner to help models improve their generalization to unseen tasks.Incorporating reliable synthetic data generation techniques and preference data (Lambert et al., 2024b) for scientific literature understanding tasks is also a promising avenue.</p>
<p>In conclusion, we are optimistic that the SciRIFF data and evaluations SciRIFF-Eval, as well as the model checkpoints, will serve as valuable resources to build systems for scientific researchers.</p>
<p>Limitation</p>
<p>While we demonstrated the effectiveness of SciR-IFF and the value of SciRIFF-Eval, we note the following limitations about our work: Although we included a wide range of datasets, this still could limit the open-ended tasks that could involve literature understanding.For example, more sophisticated iterative or chat-style interactions mimicking interactions with a research assistant are not captured with SciRIFF.Finally, computational constraints prevented us to experiment with largest open-source models; we suspect that training larger open-source models (such as Llama 3.1 405B) can provide even further improvements over state-ofthe-art commercial models.</p>
<p>Ethics Statement</p>
<p>The ethical risks associated with this work are minimal.As we source the data from existing datasets and we work in the science domain, we do not suspect major risks are involved in the curation of our dataset.However, potential biases might still exist in some datasets.For example, one of the source datasets is paper summarization which is sourced from OpenReview.net peer reviews by the original authors.And peer reviews might inherently occasionally include biases or unhelpful languages.As with all LLMs, our trained models are still prone to issues such as hallucinations, so users should exercise caution when interpreting model outputs, particularly in downstream applications in science, and verify any generated content for accuracy and relevance.</p>
<p>A SciRIFF Provenance</p>
<p>In this section, we provide additional details for SciRIFF introduced in the main body of our paper ( §1, §2).</p>
<p>A.1 SciRIFF Task and Schema</p>
<p>We provide detailed information on all tasksincluding citations, URLs to source websites, and licensing information where available-in Table 6.SciRIFF task taxonomy is visualized in Figure 4.Where convenient, we use datasets as preprocessed by the BigBio resource (https://huggingface.co/ bigbio); details will also be provided in the dataset card upon release.</p>
<p>A.2 Task Length Distribution</p>
<p>Figure 5 shows the distribution of input and output lengths for demonstrations in SciRIFF.</p>
<p>Table 7 compares SciRIFF with selected instruction-following datasets, including canonical collections commonly used for general fine-tuning and selected datasets specialized in scientific domains.Our dataset features longer input contexts than existing resources.</p>
<p>A.3 Instruction Mix Statistics</p>
<p>We further describe our data mixture following the main discussion in §2. Figure 2 presents an overview of the SciRIFF training set distribution over task categories and domains.The domain distribution reflects the current landscape of available high-quality scientific datasets (e.g., Reid et al., 2022), with a notable representation from the biomedicine and AI domain.This aligns with our dataset selection criteria, which prioritize welldocumented resources with permissive licenses.</p>
<p>Given the significant presence of information extraction tasks, a large percentage of datasets in SciRIFF (34 datasets; 63%) require structured outputs.</p>
<p>We construct three instruction mixes from this dataset collection, with maximum context lengths (input + output tokens) of 4, 096, 8,192 and 16,382 per instance (longer instances are truncated where possible and discarded otherwise; see Appendix A.4). Due to model and hardware limitations, we conduct experiments in this work using the SciR-IFF-4096 mixture, and make the longer mixtures available to enable future research.In what follows, we refer to SciRIFF-4096 simply as SciRIFF.</p>
<p>A.4 Truncation Strategy</p>
<p>In §A.3, we mention that when an instance exceeds the maximum context length for a given version of SciRIFF, we truncate where possible and discard otherwise.In particular, we truncate for tasks (like question answering) where the task output can be localized to particular passages in the input document by randomly removing irrelevant passages until the document fits in the desired context.For tasks like summarization, where the task output cannot easily be localized, we simply discard examples that are longer than the context window.</p>
<p>B Template Ablation</p>
<p>We created two variants of templates for comparison: (1) simple templates adapted from previous work FlanV2, a collection of datasets, templates, and methods for general-purpose instruction tuning (Chung et al., 2024), and (2) LLM-generated templates with GPT-4o.</p>
<p>B.1 Evaluation under Alternative Template</p>
<p>For fair evaluation, we develop corresponding variants of our evaluation templates (for SciRIFF-Eval tasks; §3.1) to ensure that models trained on alternative templates are evaluated on prompts of matching distribution.</p>
<p>B.2 Simple Template</p>
<p>We adapted the style of FlanV2's basic instruction format while maintaining essential task requirements.For example, we transformed complex templates into basic input-output patterns (e.g., Summarize:text\n \n Summary: \n) while preserving necessary variable substitutions using "variable" syntax in Jinja.To ensure valid comparison and prevent complete task failure, we maintained minimal but crucial specifications such as  Table 7: Comparison with selected instruction-following datasets.We use the following abbreviations: PH -Physics; FP -Formal Proof; MatSci -Materials Science.We report average token counts for input/output using Llama 2 tokenizer using up to 200k subsamples from each dataset.* BoX dataset is not readily available.output format requirements (e.g., JSON structure) and output constraints where necessary.</p>
<p>The simple templates still require careful adaptation since previous work typically handles simpler scenarios -their templates rarely need to ground instructions in scientific papers or specify structured output formats.We made deliberate choices to preserve these critical requirements while simplifying the instruction language and reducing template complexity.Figures 6 and 7 show examples of simple templates.</p>
<p>For evaluation, we created SciRIFF-Eval-Simple, a variant of our evaluation suite using simple prompts.This ensures that models trained on simple templates are not unfairly evaluated on complex instructions, while still testing the core capabilities required for scientific literature understanding tasks.</p>
<p>B.3 Synthetic Template</p>
<p>We also explored using GPT-4o to generate instruction templates 11 .</p>
<p>For each task category, we provided GPT-4o with a canonical example template and detailed specifications including task requirements, inputoutput structures, and available variables ({{ an-chors}}) from our prior post-processing steps (See §2).Generating templates for diverse scientific literature understanding tasks proved challenging.The complexity of our tasks-ranging from evidence-based question answering to structured information extraction-makes it difficult to create 11 Initial attempts at naive prompting failed to produce usable templates.a universal prompting strategy.</p>
<p>We provide our prompt template for synthetic template generation in Figure 8.</p>
<p>For evaluation, similar to the approach in §B.2, we created SciRIFF-Eval-Synthetic, following the same principle of matching training and evaluation distribution.</p>
<p>B.4 Results and Discussion</p>
<p>Table 8 shows that expert-written templates, which carefully specify task requirements and output structures, outperform the alternatives.Additionally, we observe that the (in-distribution) evaluation for Synthetic variants show zero performance on QASPER and SciERC tasks (See Figure 9 and Figure 10-our expert-crafted template-for reference).Upon inspection, we found that GPT-4o12 failed to specify the required output format correctly, thus the evaluation fails.Nevertheless, when we drop the two tasks, we still see that expert-written templates perform much stronger than the alternatives.</p>
<p>C Training Details</p>
<p>For instruction-tuning, our training hyperparameters were as follows:</p>
<p>• Precision: BFloat16  3. Sci.Selected represents the average score dropping QASPER and SciERC tasks (representative of complex output in SciRIFF), where synthetic templates failed to specify the required complex output formats.We show that our templates show stronger performances under either comparison scheme.</p>
<p>• Effective batch size: 128 For context, each training run of 7B-sized models requires approximately 40 GPU hours on H100 GPUs, making comprehensive ablation studies (on e.g.task mixing ratios) prohibitively expensive for most research labs.We have prioritized our computational resources for experiments that directly address core research questions while maintaining reproducibility for typical computing budgets.</p>
<p>D Evaluation Details</p>
<p>The following pages show full input / output examples for all SciRIFF-Eval tasks, along with details on metric calculations.This information will be available on our project GitHub page.We use gpt-4o-2024-08-06 model for tasks using an LLM judge as evaluation.</p>
<p>Prompt Generation</p>
<p>Today, you will write instruction templates (in Jinja) to format an instruction-following task that a researcher might reasonably ask about scientific literature.</p>
<p>You will be writing templates in Jinja formats.Input field and output field are separated by "|||".Since our Jinja template will likely be a multiline string, please use a block scalar "|" to indicate a multiline string in Jinja.For example: """ jinja: | <input part: most of your instructions will be in this part> ||| <output part> """ Here is the task that you are about to create template for: {{TASK_DESCRIPTION}} {{RELEVANT_CONTEXT}} &lt;-start_of_author_notes-NOT IN ACTUAL PROMPT&gt; Author notes: We optionally provide "relevant context" in this general format.In actual generation, we drop fields that do not apply.</p>
<p>-task_family: The category to which this task belongs.Options include summarization, ie, qa, entailment, and classification.</p>
<p>-domain: Scientific field(s) that the task covers like "artificial intelligence" -input_context: Whether the input is full paper text, a table, etc.</p>
<p>-source_type: Indicates whether the input comes from a single paper or multiple sources.</p>
<p>-output_context: Clear text descriptions for output requirements like "Yes or No", json, jsonlines.</p>
<p>&lt;-end_of_author_notes-NOT IN ACTUAL PROMPT&gt;</p>
<p>You should clearly and concisely specify task requirements and any special output structures (if applicable).For tasks that require JSON (or JSON array) outputs, explicitly mention the output requirement in your template.</p>
<p>Here is the list of anchor variables for this task, which are prepared for you: {{VARIABLES_POSTPROCESSED_BY_EXPERT_ANNOTATORS}} Important: the content enclosed by "{{" and "}}": should NOT change.You should re-use the verbatim texts for anchor variables.</p>
<p>Here is a template example belonging to the same task category.You should only study the overall structure and the style, but do not copy the content: {{DEMONSTRATION_FROM_STANDARD_SCIRIFF_INSTRUCTION}} Make sure your generated template prompt is clear and not verbose.</p>
<p>Figure 8: Template generation prompt for GPT-4o for synthetic templates §B.3.We adapt the prompt for individual tasks.We note that GPT-4o often generate vague and under-specified instructions for our use case.</p>
<p>Evaluation tasks</p>
<p>This doc has a list of all evaluation tasks, including input / output examples and evaluation metrics.Context: sensitization, behavioral changes, and low body mass index (BMI).One possible cellular target that may mediate some of these findings is the hypocretin/orexin neurons.This neuronal system plays a role in regulating wakefulness/sleep cycles, pain perception, and appetite.Food intake, in contrast, receives circadian modulation through hormones such as leptin, ghrelin, insulin and orexin.A low level of hypocretin-1/orexin-A in the cerebrospinal fluid is sufficient to diagnose narcolepsy type 1, being a highly specific and sensitive biomarker, and the irreversible loss of hypocretin neurons is responsible for the main symptoms of the disease: Orexins, or hypocretins, are excitatory neuropeptides involved in the regulation of feeding behavior and the sleep and wakefulness states.</p>
<p>Tabl e of contents</p>
<p>[Lines omitted for space]</p>
<p>Orexin A (OXA) and orexin B (OXB) are recently discovered neuropeptides that appear to play a role in various distinct functions such as arousal and the sleep-wake cycle as well as on appetite and regulation of feeding and energy homeostasis.Orexins were first described as neuropeptides expressed by a sp Orexin/hypocretin neurons located in the lateral hypothalamus play a critical role in the maintenance of arousal and contribute to the regulation of multiple homeostatic and behavioral processes.</p>
<p>Question: What processes do orexin/hypocretin neurons regulate?You will be shown an abstract from a biomedical research paper.Given this abstract, your task is to extract all unique entities of the following types: ["Chemical", "Variant", "Gene", "CellLine", "Disease", "Species"].</p>
<p>Please return the output as a JSON object of the format: {"CellLine": ["hRPTEC", ...], "Chemical":</p>
<p>["Glucose", ...], "Disease": ["Diabetes", ...], "Gene": ["HNF-6", ...], "Species": ["Patients", ...], "Variant": ["Pro75Ala", ...]}.The keys should be entity types and values should be lists of extracted entities belonging to the corresponding type.If you cannot find entities belonging to a specific type, the value should be [].</p>
<p>Only output the JSON object and do not include any additional text.</p>
<p>Abstract:</p>
<p>Fatal carbamazepine induced fulminant eosinophilic (hypersensitivity) myocarditis: emphasis on anatomical and histological characteristics, mechanisms and genetics of drug hypersensitivity and differential diagnosis.The most severe adverse reactions to carbamazepine have been observed in the haemopoietic system, the liver and the cardiovascular system.A frequently fatal, although exceptionally rare side effect of carbamazepine is necrotizing eosinophilic (hypersensitivity) myocarditis.We report a case of hypersensitivity myocarditis secondary to administration of carbamazepine.Acute hypersensitivity myocarditis was not suspected clinically, and the diagnosis was made post-mortem.Histology revealed diffuse infiltration of the myocardium by eosinophils and lymphocytes with myocyte damage.Clinically, death was due to cardiogenic shock.To best of our knowledge this is the second case of fatal carbamazepine induced myocarditis reported in English literature.Task output: The table, with each cell as a json line.Metrics: BLEU score between predicted and gold reference.Manual inspection showed that BLEU was pretty reliable for this task.Input You are provided with a table from a material science paper.Here are JSON templates for two types of numeric cells: "Other" and "Glass_Compound_Amount": {"value": "xx", "type": "Other"} {"value":</p>
<p>Output</p>
<p>"xx", "type": "Glass_Compound_Amount", "constituent": "xx", "unit": "xx", "material": "xx"} Please describe all numeric cells in the above table following the JSON templates (proceeding by row in a left-right, top-down direction).For each cell, output one JSON description per line.For any unanswerable attributes in the templates, set their value to the placeholder "xx".</p>
<p>Cell Description:</p>
<p>Output {"value": "0", "type": "Other"} {"value": "137", "type": "Other"} {"value": "24.3", "type": "Other"} {"value": "17", "type": "Other"} {"value": "129", "type": "Other"} {"value": "23.8", "type": "Other"} {"value": "22", "type": "Other"} {"value": "126", "type": "Other"} {"value": "24.7", "type": "Other"} {"value": "27", "type": "Other"} {"value": "117", "type": "Other"} {"value": "25.0", "type": "Other"}</p>
<p>Evi dence Inference</p>
<p>Task input: Abstract of a clinical trial report.Task output: List of all ICO (intervention / comparator / outcome) tuples, together with the effect of the intervention on the outcome and the textual evidence of this effect.Metrics: "Fuzzy" F1.Given a prediction and a reference tuple, compute the token overlap for each tuple item.If token overlaps for all fields exceed 0.3, the predicted tuple is judged as a match to the reference.Input</p>
<p>You will be shown the abstract of a medical clinical trial report.Your task is to extract all the findings from this report into a JSON array.Each finding should contain the following five elements:</p>
<p>-Intervention: The medical intervention being tested.This should be a text span copied from the input passage.</p>
<p>-Comparator: The baseline against which the intervention is being evaluated.This should be a text span copied from the input passage.If no comparator is reported, set to <code>null</code>.</p>
<p>-Outcome: The medical outcome whose effect is being measured.This should be a text span copied from the input passage.</p>
<p>-Effect: The effect of the intervention on the outcome, relative to the comparator.The effect should be one of the following three values: ("significantly increased", "significantly decreased", "no significant difference").</p>
<p>-Evidence: The evidence for the effect.This should be a text span copied from the input passage.</p>
<p>Please format your output as a JSON array.Each entry in the output should be an array containing the 5 elements listed above, in the following order: [<intervention>, <comparator>, <outcome>, <effect>, <evidence>].</p>
<p>For example, an output with two findings might read: [["aspirin", "placebo", "headache severity", "significantly decreased", "Mean headache severity was significantly decreased in the aspirin group compared to the placebo group (p &lt; 0.05)."],["aspirin", "placebo", "weight loss", "no significant difference", "We did not observe any difference in weight loss between the group given aspirin relative to the control group"]] There are 3 finding(s) in the abstract below.Please extract them.Output only the JSON array with these 3 findings.Do not include any additional text.Abstract: ABSTRACT.OBJECTIVES: To compare the efficacy and safety of SB4 (an etanercept biosimilar) with reference product etanercept (ETN) in patients with moderate to severe rheumatoid arthritis (RA) despite methotrexate (MTX) therapy.</p>
<p>ABSTRACT.METHODS: This is a phase III, randomised, double-blind, parallel-group, multicentre study with a 24-week primary endpoint.Patients with moderate to severe RA despite MTX treatment were randomised to receive weekly dose of 50 mg of subcutaneous SB4 or ETN.The primary endpoint was the American College of Rheumatology 20% (ACR20) response at week 24.Other efficacy endpoints as well as safety, immunogenicity and pharmacokinetic parameters were also measured.</p>
<p>ABSTRACT.RESULTS: 596 patients were randomised to either SB4 (N=299) or ETN (N=297).The ACR20 response rate at week 24 in the per-protocol set was 78.1% for SB4 and 80.3% for ETN.The 95% CI of the adjusted treatment difference was -9.41% to 4.98%, which is completely contained within the predefined equivalence margin of -15% to 15%, indicating therapeutic equivalence between SB4 and ETN.Other efficacy endpoints and pharmacokinetic endpoints were comparable.The incidence of treatment-emergent adverse events was comparable (55.2% vs 58.2%), and the incidence of antidrug antibody development up to week 24 was lower in SB4 compared with ETN (0.7% vs 13.1%).</p>
<p>ABSTRACT.CONCLUSIONS: SB4 was shown to be equivalent with ETN in terms of efficacy at week 24.SB4 was well tolerated with a lower immunogenicity profile.The safety profile of SB4 was comparable with that of ETN.</p>
<p>ABSTRACT.TRIAL REGISTRATION NUMBERS: NCT01895309, EudraCT 2012-005026-30.</p>
<p>Findings:</p>
<p>Output [ [ "SB4 (an etanercept biosimilar)", "etanercept (ETN)", "therapeutic equivalence -Patients with moderate to severe rheumatoid arthritis (RA) despite methotrexate (MTX) treatment", "no significant difference", "The 95% CI of the adjusted treatment difference was \u22129.41% to 4.98%, which is completely contained within the predefined equivalence margin of \u221215% to 15%, indicating therapeutic equivalence between SB4 and ETN."</p>
<p>],</p>
<p>[ "SB4 (an etanercept biosimilar)", "etanercept (ETN)", "incidence of antidrug antibody development up to week 24 -Patients with moderate to severe rheumatoid arthritis (RA) despite methotrexate (MTX) treatment", "significantly decreased", "the incidence of antidrug antibody development up to week 24 was lower in SB4 compared with ETN (0.7% vs 13.1%)."</p>
<p>],</p>
<p>[ "SB4 (an etanercept biosimilar)", "etanercept (ETN)", "incidence of treatment-emergent adverse events -Patients with moderate to severe rheumatoid arthritis (RA) despite methotrexate (MTX) treatment", "no significant difference", "The incidence of treatment-emergent adverse events was comparable (55.2% vs 58.2%)" ] ]</p>
<p>Mul ti ci te</p>
<p>Task Input: A citation sentence from a research paper.Task output: A list of intents for the citation sentence.Metrics: Compare predicted vs. reference intents using exact-match F1.Input Your task is to classify the citation intent within the following provided text from a computational linguistics research paper.The cited work is demarcated by "<cite>" and "</cite>".Determine the purpose of the cited work by selecting from the listed categories:</p>
<p>-Background: The cited paper underpins the subject matter.</p>
<p>-Motivation: The cited paper inspires or provides a rationale for the current research.</p>
<p>-Uses: The current work utilizes concepts or tools from the cited paper.</p>
<p>-Extends: The current work advances ideas or methods from the cited paper.</p>
<p>-Similarities: The current work identifies commonalities with the cited paper.</p>
<p>-Differences: The current work delineates its distinction from the cited paper.</p>
<p>-FutureWork: The cited paper is acknowledged as groundwork for prospective research.</p>
<p>Indicate the intents by listing them in a <code>json</code> array, e.g.["Background", "Uses"].More than one intent may be applicable.Do not include any extraneous text in your response.</p>
<p>Output</p>
<p>["Similarities", "Uses"] MUP Task input: Full text of a machine learning paper.Task output: Short paper summary that a reviewer might write as part of a paper review.Metrics: Use GPT-3.5 to judge similarity of generated summary to human reference on 1-5 scale.Based on manual inspection, this was higher-quality than automated metrics like ROUGE.Input</p>
<p>You will be presented with the title and body text of a computer science research paper.Please write a summary of the work that would be informative for a peer reviewer assessing its quality.Mathe et al. , 2016 ;Das et al. , 2017) , robotics control ( Finn et al. , 2016 ;Zhang et al. , 2018) , medical diagnosis ( Ling et al. , 2017 ;Peng et al. , 2018 ) and computer games ( Mnih et al. , 2015 ;Silver et al. , 2016 ) .One fundamental reason that drives the success of such models and enables them to outperform classical algorithms is the availability of large amounts of training data .Typically such training data is either fully observed or the features stem from an action-independent observation model ( which clearly can depend on the state of the system ) .</p>
<p>However , the fundamental assumption that the same features are always readily available during deployment could not hold in many real-world applications .For instance , consider a medical support system for monitoring and treating patients during their stay at hospital which was trained on rich historical medical data .To provide the best possible treatment , the system might need to perform several measurements of the patient over time , while some of them could be costly or even pose a health risk .Therefore , during deployment , it is more ideal that the system could function with minimal features while during training more features might have been available .In such cases , we are interested in decision making models that actively take the measurement process , i.e. , feature acquisition , into account and only acquire the information relevant for making a decision .</p>
<p>In this paper , we consider the challenging problem of learning effective policies when the cost of information acquisition can not be neglected .To be successful , we need to learn policies which acquires the information required for solving a task in the cheapest way possible .[Truncated for space].</p>
<p>3-sentence paper summary:</p>
<p>Output</p>
<p>In this paper the authors propose an approach for simultaneously learning how to explore more efficiently in POMDPs via targeted feature acquisition, and learning a reward-maximizing control policy, balancing the cost of feature acquisition with the expected reward.Learning is done via a VAE framework which combines a belief inference model and an observation decoder, with a key innovation being that inference is done as a sequential process.Results comparing this approach to other variational inference approaches show the proposed framework reaches better performance with lower cost (particularly, number of acquired features).</p>
<p>Qasper</p>
<p>Task input: The full text of an NLP research paper, and a question answerable from the paper body (but not the abstract).Task output: An answer to the question, accompanied by the extracts from the paper body supplying the answer.Metrics: We compute metrics for both the answer and the evidence.</p>
<p>Answer: GPT-3.5 judge of similarity of model answer to human reference (1-5 scale).Evidence: Token F1 overlap with gold evidence.Input  --------------------------------------- Wikilinks can be seen as a large-scale, naturally-occurring, crowd-sourced dataset where thousands of human annotators provide ground truths for mentions of interest.This means that the dataset contains various kinds of noise, especially due to incoherent contexts.The contextual noise presents an interesting test-case that supplements existing datasets that are sourced from mostly coherent and well-formed text.</p>
<p>[Truncated for space] ----------------------------------------Question: How was a quality control performed so that the text is noisy but the annotations are accurate?</p>
<p>To answer the question, format your response as a <code>json</code> object with two fields:</p>
<p>"answer": A string providing a succinct answer to the question, in your own words."evidence": An array of strings.Each entry should be a full paragraph from the paper.Together, the evidence should serve as a justification for the answer.</p>
<p>For instance, for the question "What baselines did the authors compare against?", a sample response might be:</p>
<p>{ "answer": "BERT and RoBERTa.""evidence": ["We compare our approach against two baselines.In Table 1, we compare against BERT.In Table 2, we compare against RoBERTa.Our findings indicate that our approach improves over both baeslines..."] }</p>
<p>The "answer" field should be roughly 190 characters in length.</p>
<p>Do not include any text in your response other than the json.If the question is unanswerable given the provided excerpts, respond with the single word "null".</p>
<p>To repeat, the question is: How was a quality control performed so that the text is noisy but the annotations are accurate?</p>
<p>Answer JSON object:</p>
<p>Output { "answer": "Profile pictures from the Twitter users' profiles.You will be shown an abstract from a computer science research paper.Given this abstract, your task is to extract all unique entities with the following types:</p>
<p>-"Task": Applications, problems to solve, systems to construct.Examples include "information extraction", "machine reading system", "image segmentation".</p>
<p>-"Method": : Methods, models, systems to use, or tools, components of a system, frameworks.</p>
<p>Examples include "language model", "CORENLP", "POS parser".</p>
<p>-"Metric": Metrics, measures, or entities that can express quality of a system / method.Examples include "F1", "BLEU", "Precision", "time complexity".</p>
<p>-"Material": Data, datasets, resources, Corpus, Knowledge base.Examples include "image data", "speech data", "stereo images", "CoNLL", "Wikipedia".</p>
<p>-"OtherScientificTerm": Phrases that are a scientific terms but do not fall into any of the above classes.Examples include "physical or geometric constraints", "qualitative prior knowledge", "tree kernel", "noise".</p>
<p>-"Generic": General terms or pronouns that may refer to a entity but are not themselves informative, often used as connection words.Examples include "model", "approach", "them".</p>
<p>Please return the output as a JSON object of the format: {"type1" : ["example_entity", ...], "type2" : ["example_entity", ...]}.The keys should be entity types and values should be lists of extracted entities belonging to the corresponding type.Entity types with no matching entities should be assigned an empty array [].</p>
<p>For instance, the output might look like: {"Task": ["speech recognition", ...], "Method":</p>
<p>["Conditional random field"], "Material": [], ...}.</p>
<p>Only output the JSON object and do not include any additional text.</p>
<p>Abstract:</p>
<p>We present a syntax-based constraint for word alignment, known as the cohesion constraint.It requires disjoint English phrases to be mapped to non-overlapping intervals in the French sentence.</p>
<p>We evaluate the utility of this constraint in two different algorithms.The results show that it can provide a significant improvement in alignment quality.</p>
<p>Output</p>
<p>{ "Generic": ["algorithms"], "Material": ["English phrases", "French sentence"], "Method": [], "Metric": ["alignment quality"], "OtherScientificTerm": ["cohesion constraint", "syntax-based constraint"], "Task": ["word alignment"] }</p>
<p>Sci Fact</p>
<p>Task input: An abstract from a biomedical research article, and a scientific claim.Task output:</p>
<p>A fact-checking verdict indicating whether the abstract supports or refutes the claim, or has no relevant information.The evidence --i.e.sentences from the abstract justifying the verdict.Metrics: We compute metrics for both the answer and the evidence.</p>
<p>Verdict: Label F1.Evidence: Token F1 overlap with gold evidence.Input</p>
<p>You will be shown a scientific claim, and the abstract of a biomedical research paper.Each sentence from the abstract will be on a separate line.Your task is to return a JSON object with two fields:</p>
<p>-"verdict": The fact-checking verdict.If the information in the abstract supports the claim, write "SUPPORT".If the abstract contradicts the claim, write "CONTRADICT".If the abstract does not provide enough information to arrive at a verdict, write "NEI" (for "not enough information").</p>
<p>-"evidence": An array of sentences providing evidence for the verdict.Please copy all relevant sentences verbatim from the abstract.If the verdict was "NEI", then return an empty array.</p>
<p>For instance, if the model were given the claim "smoking causes cancer", the output might be { "verdict": "SUPPORT", "evidence": ["The results of our meta-analysis provide overwhelming support that cigarette smoking is a risk cause for lung cancer."]} Your response should not include any text other than the json.</p>
<p>Claim: Therapeutics receiving accelerated approval encounter a lower frequency of post-marketing safety events Abstract: Importance Postmarket safety events of novel pharmaceuticals and biologics occur when new safety risks are identified after initial regulatory approval of these therapeutics.These safety events can change how novel therapeutics are used in clinical practice and inform patient and clinician decision making.Objectives To characterize the frequency of postmarket safety events among novel therapeutics approved by the US Food and Drug Administration (FDA), and to examine whether any novel therapeutic characteristics known at the time of FDA approval were associated with increased risk.[Truncated for space] Biologics, psychiatric therapeutics, and accelerated and near-regulatory deadline approval were statistically significantly associated with higher rates of events, highlighting the need for continuous monitoring of the safety of novel therapeutics throughout their life cycle.</p>
<p>Output</p>
<p>{ "verdict": "CONTRADICT", "evidence": [ "In multivariable analysis, postmarket safety events were statistically significantly more frequent among biologics (incidence rate ratio [IRR] = 1.93; 95% CI, 1.06-3.52;P = .03),therapeutics indicated for the treatment of psychiatric disease (IRR = 3.78; 95% CI, 1.77-8.06;P &lt; .001),those receiving accelerated approval (IRR = 2.20; 95% CI, 1.15-4.21;P = .02),and those with near\u2013regulatory deadline approval (IRR = 1.90; 95% CI, 1.19-3.05;P = .008);events were statistically significantly less frequent among those with regulatory review times less than 200 days (IRR = 0.46; 95% CI, 0.24-0.87;P = .02)." ] } QASPER You will be shown sections from a scientific research paper, together with a question about the paper.This is an extractive question-answering task, where you must find and extract relevant text spans directly from the paper to answer the question.Your response should strictly be a json object with two fields:</p>
<p>-"answer": An array of strings extracted directly from the paper which, collectively, answer the question.</p>
<p>-"evidence": An array of strings.Each should be an excerpt from the paper, in which one or more of the extracted answers can be found.</p>
<p>For example, for the question "What baselines did the authors compare against?", a sample response might be:</p>
<p>SciERC</p>
<p>You will be shown an abstract from a computer science research paper.Given this abstract, your task is to extract all unique entities with the following types:</p>
<p>-"Task": Applications, problems to solve, systems to construct.Examples include "information extraction", "machine reading system", "image segmentation".</p>
<p>-"Method": : Methods, models, systems to use, or tools, components of a system, frameworks.Examples include "language model", "CORENLP", "POS parser".</p>
<p>-"Metric": Metrics, measures, or entities that can express quality of a system / method.Examples include "F1", "BLEU", "Precision", "time complexity".</p>
<p>-"Material": Data, datasets, resources, Corpus, Knowledge base.Examples include "image data", "speech data", "stereo images", "CoNLL", "Wikipedia".</p>
<p>-"OtherScientificTerm": Phrases that are a scientific terms but do not fall into any of the above classes.Examples include "physical or geometric constraints", "qualitative prior knowledge", "tree kernel", "noise".</p>
<p>-"Generic": General terms or pronouns that may refer to a entity but are not themselves informative, often used as connection words.Examples include "model", "approach", "them".</p>
<p>Please return the output as a JSON object of the format: {"type1" : ["example_entity", ...], "type2" : ["ex-ample_entity", ...]}.The keys should be entity types and values should be lists of extracted entities belonging to the corresponding type.Entity types with no matching entities should be assigned an empty array "[]".</p>
<p>For instance, the output might look like: {"Task": ["speech recognition", ...], "Method": ["Conditional random field"], "Material": [], ...}.</p>
<p>Only output the JSON object and do not include any additional text.</p>
<p>HealthVer</p>
<p>You will be shown a claim about public health and the abstract of a biomedical research paper.Each sentence from the abstract will be on a separate line.Your task is to return a JSON object with two fields:</p>
<p>-"verdict": The fact-checking verdict.If the information in the abstract supports the claim, write "SUPPORT".If the abstract contradicts the claim, write "CONTRADICT".If the abstract does not provide enough information to arrive at a verdict, write "NEI" (for "not enough information").</p>
<p>-"evidence": An array of sentences providing evidence for the verdict.Please copy all relevant sentences verbatim from the abstract.If the verdict was "NEI", then return an empty array.</p>
<p>For instance, if the model were given the claim "wearing masks can prevent the spread of COVID", the output might be: You are provided with the table above from a materials science paper.Here are JSON templates for two types of numeric cells: "Other" and "Glass_Compound_Amount": {"value": "xx", "type": "Other"} {"value": "xx", "type": "Glass_Compound_Amount", "constituent": "xx", "unit": "xx", "material": "xx"} Please describe all numeric cells in the above table following the JSON templates (proceeding by row in a left-right, top-down direction).For each cell, output one JSON description per line.For any unanswerable attributes in the templates, set their value to the placeholder "xx".</p>
<p>Figure 1 :
1
Figure 1: Example SciRIFF tasks.Given an input context from a research paper, the text prompt instructs an LLM to perform an operation on the input-e.g.determine whether the abstract entails a scientific claim, extract information over the full_text, answer a question, etc.The model's output must conform to a task-specific, user-specified structure.SciRIFF unifies 54 scientific literature understanding tasks under a common input / output format, enabling the development of LLMs that can flexibly generalize to novel scientific use cases.</p>
<p>Figure 3 :
3
Figure 3: Performance on SciRIFF-Eval vs. n sci (instances/task).Gains saturate at n sci = 1000 (see §3.2)</p>
<p>Figure 5 :
5
Figure 5: Distribution of input (left) and output (right) token lengths over SciRIFF training instances.</p>
<p>input: A collection of biomedical research excerpts and a question answerable from the excerpts.Task output: A list of answers to the question.Metrics: Compare predicted vs. reference answers using exact-match F1.Input Below are a collection of excerpts from biomedical research articles.Excerpts are separated by newlines.Your task is to answer a question based these excerpts.Your response should be formatted as a <code>json</code> array.For instance, given excerpts from articles studying breast cancer, and the question "what are some common genes associated with breast cancer?", an answer might be formatted like: ["BRCA1", "BRCA2", "TP53", ...].Only include answers that are mentioned in the provided exerpts.The array should always have at least one answer; do not respond with an empty array [].Do not include any text in your response other than the answer array.</p>
<p>Abstract of a biomedical research article.Task output: All entities in the article of the following types: predicted vs. reference entities using exact-match F1.Input</p>
<p>A passage from a research paper including a table.</p>
<p>|</p>
<p>Sample no.| Ph, volume percent of crystals | Activation energy (kJ/mol) of the scale factor for normalised frequency | ln(t 0, s) of the scale factor | G unrelaxed shear modulus (GPa) | energies of shear stress relaxation and unrelaxed shear modulus of disilicate lithium glasses</p>
<p>Context with Citation: In addition to that, we implemented semi-supervised classification by training in the positive samples of the <cite>[9]</cite> dataset and training in only the lexicon as negative samples.</p>
<p>Your summary should be 3 sentences long.In your response, include only the summary and no additional text.Paper title: Reinforcement Learning with Efficient Active Feature Acquisition Paper body: 1 INTRODUCTION .Recently , machine learning models for automated sequential decision making have shown remarkable success across many application areas , such as visual recognition (</p>
<p>You will be shown sections from a scientific research paper, together with a question about the paper.Paragraphs in the paper are separated by newlines.Your task is to answer the question based on the contents of the paper.Paper:</p>
<p>-Named Entity Disambiguation for Noisy Text We address the task of Named Entity Disambiguation (NED) for noisy text.We present WikilinksNED, a large-scale NED dataset of text fragments from the web, which is significantly noisier and more challenging than existing news-based datasets.To capture the limited and noisy local context surrounding each mention, we design a neural model and train it with a novel method for sampling informative negative examples.We also describe a new way of initializing word and entity embeddings that significantly improves performance.Our model significantly outperforms existing state-of-the-art methods on WikilinksNED while achieving comparable performance on a smaller newswire dataset.The WikilinksNED Dataset: Entity Mentions in the Web We introduce WikilinksNED, a large-scale NED dataset based on text fragments from the web.Our dataset is derived from the Wikilinks corpus BIBREF14 , which was constructed by crawling the web and collecting hyperlinks (mentions) linking to Wikipedia concepts (entities) and their surrounding text (context).Wikilinks contains 40 million mentions covering 3 million entities, collected from over 10 million web pages.</p>
<p>Figure 9: Canonical template for QASPER task in Figure 4. See §F for description.</p>
<p>Figure 10: Canonical template for SciERC task in Figure 4. See §F for description.</p>
<p>{Figure 11 :
11
Figure 11: Canonical template for HealthVer task in Figure 4. See §F for description.</p>
<p>Figure 12 :
12
Figure 12: Canonical template for DiSCoMaT task in Figure 4. See §F for description.</p>
<p>, Llama 3.2-3B (Dubey et al., 2024), and
NameTypeInputOutputMetricsBioASQ List QAQAQuestion, paper excerpts Answer entitiesExact match F1BioREDIE(NER)Biomedical abstract6 entity typesExact match F1DiSCoMaTIE(Table)LaTex table excerptTable entriesBLEU scoreEvidence Inference (EI) IE(Rel)Clinical trial abstractPICOString overlap F1MultiCite (MC)Classification Citation contextCitation intentsExact match F1MuPSummarization ML paper full textPeer review summary LLM judge similarityQasperQANLP paper questionAnswer / Attribution LLM judge similarity / Token F1SciERCIE(Rel)CS abstract6 entity typesExact match F1SciFactEntailmentClaim, abstractVerdict / EvidenceLabel F1 / Token F1Table 1: Evaluation tasks included in SciRIFF-Eval. "/" separators indicate two separate subtasks. We use GPT-4oas our LLM judge and evaluate similarity on a 1-5 scale; see Appendix D for details.</p>
<p>Table 2 :
2
Performance on SciRIFF-Eval tasks across model families and training configurations ( §3.2).Best performance per model group is bolded.Columns with a "/" indicate two evaluation metrics as described in §3.1.
1, out of the</p>
<p>Table 3 :
3
Average SciRIFF-Eval scores across selected configurations.Columns use their matching evaluations, SciRIFF-Eval, SciRIFF-Eval-Simple, and SciRIFF-Eval-Synthetic for fair comparisons.See Appendix B and Table8for details.</p>
<p>Table 4 :
4
Comparison of SciRIFF-Eval (Sci.)performance for models finetuned from Llama 2 base and Tülu V2 (science-decontaminated).
ModelData7B 70BLlama 2 Tülu36.7 47.5SciRIFF48.0 51.1SciRIFF+Tülu 46.0 50.8Tülu V2SciRIFF SciRIFF+Tülu 47.0 50.7 47.0 48.8</p>
<p>). Gains saturate at n sci = 1000 (see §3.2) results in § 4.4, illustrates how practitioners can optimize training for SciRIFF under fixed model architectures.
CheckpointSciRIFF Tülu-V2TotalLlama 2 base35,357318,686 354,043Tülu V235,35735,35770,714</p>
<p>Table 5 :
5
SciRIFF and Tülu V2 Mix instances used for finetuning described in §4.3, with n sci = 1000.</p>
<p>Table 7 for a comparison).
our static benchmark, SciArena (Zhao et al., 2025)provides a dynamic platform that evaluates modelsvia ongoing expert preference voting. In contrast,SciRIFF provides fully expert-written instructions,serving both as a benchmark and training resource.Concurrent with our work, Li et al. (Other scientific literature benchmarks. Priorworks have developed benchmarks to improveand assess scientific literature understanding. No-table efforts in the biomedical domain includeBLUE (Peng et al., 2019), BLURB (Gu et al.,2021), InBoXBART (Parmar et al., 2022), and Big-Bio (Fries et al., 2022); SciRIFF covers a broaderset of domains than these resources. Other ef-forts such as (Singh et al., 2023; Taylor et al.,2022; Wei et al., 2023) cover domains beyondbiomedicine, but are not targeted for traininginstruction-following models. SciASSESS (Caiet al., 2024) evaluates LLMs' proficiency in sci-entific literature analysis, focusing on memoriza-tion and reasoning abilities. Complementary to</p>
<p>Table 6 :
6Name# InstancesDomainAvg. LengthGeneral DomainFlan V2 (Chung et al., 2024)15MGeneral355.6 / 31.2SuperNI (Wang et al., 2022)97KGeneral291.1 / 38.7Tülu V2 Mix (Ivison et al., 2023b)326KGeneral353.3 / 696.9Scientific DomainBoX (Parmar et al., 2022)141KBiomedX  *SciInstruct (Zhang et al., 2024a)254KMath, PH, Chem, FP88.4 / 265.6Mol-Instructions (Fang et al., 2024)2.04MBiomolecular126.3 / 112.9MathInstruct (Yue et al., 2024a)262KMath82.5 / 174.0MedInstruct-52K (Zhang et al., 2023c)52KMedical148.2 / 96.9LlaSMol (Yu et al., 2024)3.29MChem81.9 / 53.0SciRIFF (Our work)137KAI, Biomed, Clinical, Chem, MatSci 1242.9 / 139.6
Overview of source datasets repurposed for SciRIFF ( §2).SciRIFF is licensed under ODC-By and is derived from existing scientific literature understanding datasets.{} indicates subsets belonging to the same source.Figure 4: Overview of SciRIFF dataset.Dashed black lines indicate that a task is included in SciRIFF-Eval and held out during model training.Scientific domains are colored as follows: ■Biomedicine;■AI;■Clinical Medicine;■Chemistry;■Materials Science;■Miscellaneous.</p>
<p>Output a JSON object where keys are entity types and values are lists of extracted entities.
SciERC-simpleExtract all unique entities from the paper abstract.Abstract:{{paper}}|||{{ ner_dict | tojson }}Figure 6: simple template for SciERC task.QASPER-simpleRead the following paper excerpts and answer the question. Output a JSON object with "answer" and "evidence" fields.Paper: {{paper}}Question: {{question}}|||{{output}}Figure 7: simple template for QASPER task.ModelDataBioASQ BioR DiscMT EI MC MuP Qasper SciERC SciFact Sci. Sci.SelectedLlama-3.1-8B Ours46.2 84.2 83.9 23.5 71.0 68.5 49.8 52.2 56.2 83.3 / 71.9 62.865.0Simple57.3 64.4 19.6 4.1 9.4 42.3 49.8 / 65.0 33.7 65.8 / 52.6 42.236.6Synthetic 41.0 58.1 38.7 0.3 9.1 57.50.00.063.4 / 39.7 28.036.7Qwen2.5-1.5B Ours43.6 81.8 45.6 18.9 71.2 67.8 47.0 / 51.4 52.7 78.8 / 70.5 57.257.7Simple36.2 43.1 38.5 0.8 2.9 42.5 32.5 / 48.3 28.8 47.9 / 42.2 33.029.9Synthetic 33.9 39.7 40.9 0.3 7.7 68.50.00.07.2 / 11.8 19.128.6Table 8:Performance on evaluation tasks (SciRIFF-Eval,SciRIFF-Eval-Simple,andSciRIFF-Eval-Synthetic respectively, across ablations for  §4.2 and Appendix B. This table accompa-nies Table• Epochs: 5• Weight decay: 0• Warmup ratio: 0.03• Learning rate: 2e-5 (1e-5 for 70B)• Max. seq. length: 4,096
Other types of post-training such as preference optimization are outside our scope.
The remaining four tasks are naturally formatted for instruction-following and ready for Jinja templating which don't require any special treatment.
Paper authors transform raw dataset annotations into standardized json schemas before templating.
  4  We conduct our experiments using SciRIFF-4096 (hereafter SciRIFF) due to computational constraints.
In our study, we use 70.5K instances for training.
https://sharegpt.com/
https://github.com/LAION-AI/Open-Assistant
We do not train larger models due to compute constraint. However, as shown in §4.1 improvements are consistent across sizes/families.
Due to compute constraints, we do not extend this analysis
Note that the effort to prompt GPT4o to generate template for diverse and different scientific literature understanding tasks is non-trivial in itself; See Figure8.
AcknowledgementsWe thank Minyi Chen, Yicheng Gao, Kaiyuan Guan, and Yujie Qiao for their data sourcing contributions during early phase of the project.We are grateful to Google's TRC program for compute support.:A dynamic growth paradigm for instruction-tuning data curation.In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4031-4047, Singapore.Association for Computational Linguistics.Author contributionsDavid Wadden and Kejian Shi contributed equally and led the project.Jacob Morrison, Alan Li, and Aakanksha Naik were among the core contributors and substantially contributed to the experiments and data collection.Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, and Luca Soldaini contributed ideas and provided additional support with experiments.Shannon Shen, Doug Downey, Hanna Hajishirzi, and Arman Cohan provided core mentorship and advising.E Instruction Template CreationInstruction templates are written in (Pallets, 2024), Guidelines and best practices" for prompt-writing will be available at our GitHub repository.Each prompt was double-checked by an additional paper author for clarity and correctness.F Sample templateIn this section, we provide examples of our expertwritten templates that demonstrate the complexity and precision required for scientific literature understanding tasks, described in §1 and §2.1.These templates are carefully designed to elicit structured outputs while requiring sophisticated capabilities such as information extraction with attribution, multistep reasoning, and adherence to specific output schemas.The templates shown -QASPER (QA, Figure9), SciERC (IE, Figure10), HealthVer (Factchecking, Figure11), DiSCoMaT (IE over tabular data, Figure12), and DataFinder Reco MC (Multiple Choice QA, Figure13) -demonstrates how our instruction format guides models to perform challenging tasks like answering questions with evidence attribution, extracting nested entity relationships, and verifying scientific claims with supporting rationales. 13G Information About Use of AI AssistantsWe use OpenAI ChatGPT and Anthropic Claude for grammar checking in manuscript preparation. 13Our preliminary experiments showed that even strong proprietary models like GPT-4o struggled to reliably generate such structured outputs without explicit templates.This observation motivated our decision to use expert-written templates.
What are the desired characteristics of calibration sets? identifying correlates on long form scientific summarization. Griffin Adams, Bichlien Nguyen, Jake Smith, Yingce Xia, Shufang Xie, Anna Ostropolets, Budhaditya Deb, Yuan-Jyue Chen, Tristan Naumann, Noémie Elhadad, 10.18653/v1/2023.acl-long.587Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Dayne Freitag, Mark Dredze, and Alan Ritter. 2024. Schemadriven information extraction from heterogeneous tables. Fan Bai, Junmo Kang, Gabriel Stanovsky, arXiv:2305.14336Preprint</p>
<p>The ACL Anthology reference corpus: A reference dataset for bibliographic research in computational linguistics. Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson, Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett Powley, Dragomir Radev, Yee Fan Tan, Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC'08). the Sixth International Conference on Language Resources and Evaluation (LREC'08)Marrakech, Morocco2008European Language Resources Association (ELRA</p>
<p>Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, Christopher D Manning, arXiv:2403.18421Biomedlm: A 2.7b parameter language model trained on biomedical text. 2024arXiv preprint</p>
<p>TLDR: Extreme summarization of scientific documents. Isabel Cachola, Kyle Lo, Arman Cohan, Daniel Weld, 10.18653/v1/2020.findings-emnlp.428Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin Wang, Zhifeng Gao, Hongshuai Wang, Yongge Li, Mujie Lin, Shuwen Yang, Jiankun Wang, Mingjun Xu, Jin Huang, Fang Xi, Jiaxi Zhuang, Yuqi Yin, Yaqi Li, Changhong Chen, Zheng Cheng, Zifeng Zhao, arXiv:2403.01976Linfeng Zhang, and Guolin Ke. 2024. Sciassess: Benchmarking llm proficiency in scientific literature analysis. arXiv preprint</p>
<p>Multi-label classification for biomedical literature: an overview of the biocreative vii litcovid track for covid-19 literature topic annotations. Qingyu Chen, Alexis Allot, Robert Leaman, Rezarta Islamaj, Jingcheng Du, Li Fang, Kai Wang, Shuo Xu, Yuefu Zhang, Parsa Bagherzadeh, 10.1093/database/baac069Database. 692022. 2022</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Yu, Journal of Machine Learning Research. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei25702024</p>
<p>Structural scaffolds for citation intent classification in scientific publications. Arman Cohan, Waleed Ammar, Madeleine Van Zuylen, Field Cady, 10.18653/v1/N19-1361Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>A discourse-aware attention model for abstractive summarization of long documents. Arman Cohan, Franck Dernoncourt, Soon Doo, Trung Kim, Seokhwan Bui, Walter Kim, Nazli Chang, Goharian, 10.18653/v1/N18-2097Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Short Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLouisianaNew Orleans20182Association for Computational Linguistics</p>
<p>Overview of the first shared task on multi perspective scientific document summarization (MuP). Arman Cohan, Guy Feigenblat, Tirthankar Ghosal, Michal Shmueli-Scheuer, Proceedings of the Third Workshop on Scholarly Document Processing. the Third Workshop on Scholarly Document ProcessingGyeongju, Republic of KoreaAssociation for Computational Linguistics2022</p>
<p>The colorado richly annotated full text (craft) corpus: Multi-model annotation in the biomedical domain. Kevin Bretonnel Cohen, Karin Verspoor, Karën Fort, Christopher Funk, Michael Bada, Martha Palmer, Lawrence E Hunter, 10.1007/978-94-024-0881-2_53Handbook of Linguistic Annotation. Springer2017</p>
<p>Introduction to the bio-entity recognition task at JNLPBA. Nigel Collier, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, Jin-Dong Kim, Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP). the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP)Geneva, Switzerland. COLING2004</p>
<p>A dataset of information-seeking questions and answers anchored in research papers. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, Matt Gardner, 10.18653/v1/2021.naacl-main.365Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2021</p>
<p>Free dolly: Introducing the world's first truly open instruction-tuned llm. Databricks, 2023Blog post</p>
<p>Nlm-gene, a richly annotated gold standard dataset for gene entities that addresses ambiguity and multispecies gene recognition. Rezarta Islamaj, Chih-Hsuan Wei, David Cissel, Nicholas Miliaras, Olga Printseva, Oleg Rodionov, Keiko Sekiya, Janice Ward, Zhiyong Lu, 10.1016/j.jbi.2021.103779Journal of Biomedical Informatics. 1181037792021b</p>
<p>Ncbi disease corpus: a resource for disease name recognition and concept normalization. Rezarta Islamaj Dogan, Robert Leaman, Zhiyong Lu, 10.1016/j.jbi.2013.12.006Journal of Biomedical Informatics. 472014</p>
<p>Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, Hannaneh Hajishirzi, arXiv:2311.10702Camels in a changing climate: Enhancing lm adaptation with tulu 2. 2023aarXiv preprint</p>
<p>Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, Hannaneh Hajishirzi, arXiv:2311.10702Camels in a changing climate: Enhancing lm adaptation with tulu 2. 2023barXiv preprint</p>
<p>Instruct and extract: Instruction tuning for on-demand information extraction. Yizhu Jiao, Ming Zhong, Sha Li, Ruining Zhao, Siru Ouyang, Ji Heng, Jiawei Han, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>PubMedQA: A dataset for biomedical research question answering. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, Xinghua Lu, 10.18653/v1/D19-1259Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>AxCell: Automatic extraction of results from machine learning papers. Marcin Kardas, Piotr Czapla, Pontus Stenetorp, Sebastian Ruder, Sebastian Riedel, Ross Taylor, Robert Stojnic, 10.18653/v1/2020.emnlp-main.692Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>SciReviewGen: A large-scale dataset for automatic literature review generation. Tetsu Kasanishi, Masaru Isonuma, Junichiro Mori, Ichiro Sakata, 10.18653/v1/2023.findings-acl.418Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Prometheus: Inducing finegrained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, Minjoon Seo, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Team Kimi, arXiv:2507.20534Kimi k2: Open agentic intelligence. 2025arXiv preprint</p>
<p>. Martin Krallinger, Obdulia Rabal, Ahmad Saber, Martín Akhondi, Jesus Pérez Pérez, Gael Pérez Santamaría, Georgios Rodríguez, Ander Tsatsaronis, José Intxaurrondo, Antonio Baso, Umesh K López, Erin M Nandal, Ambika Van Buel, Marleen Chandrasekhar, Astrid Rodenburg, Marius A Laegreid, Julen Doornenbal, Anália Oyarzábal, Alfonso Lourenço, Valencia, 2017Overview of the biocreative vi chemical-protein interaction track</p>
<p>Martin Krallinger, Obdulia Rabal, Florian Leitner, Miguel Vazquez, David Salgado, Zhiyong Lu, Robert Leaman, Yanan Lu, Donghong Ji, Daniel M Lowe, Roger A Sayle, Theresa Riza, Rafal Batista-Navarro, Torsten Rak, Tim Huber, Sérgio Rocktäschel, David Matos, Buzhou Campos, Hua Tang, Tsendsuren Xu, Keun Munkhdalai, S V Ho Ryu, Senthil Ramanan, Slavko Nathan, Marko Žitnik, Lutz Bajec, Matthias Weber, Irmer, A Saber, Jan A Akhondi, Shuo Kors, Xin Xu, Utpal An, Asif Kumar Sikdar, Masaharu Ekbal, Yoshioka, M Thaer, Miji Dieb, Karin Choi, Madian Verspoor, C Lee Khabsa, Hongfang Giles, Liu, 10.1186/1758-2946-7-S1-S2The chemdner corpus of chemicals and drugs and its annotation principles. Komandur Elayavilli Ravikumar, Andre Lamurias, Francisco M Couto, Hong-Jie Dai, Richard Tzong-Han, Tsai, Caglar Ata, Tolga Can, Anabel Usié, Rui Alves, Isabel Segura-Bedmar, Paloma MartínezJulen Oyarzabal, and Alfonso Valencia20157S2</p>
<p>Do models really learn to follow instructions? an empirical study of instruction tuning. Po-Nien Kung, Nanyun Peng, 10.18653/v1/2023.acl-short.113Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20232Short Papers)</p>
<p>. Fabrício Kury, Alex Butler, Chi Yuan, Li-Heng Fu, Yingcheng Sun, Hao Liu, Ida Sim, Simona Carini, Chunhua Weng, 10.1038/s41597-020-00620-020207281</p>
<p>Abdullatif Köksal, Timo Schick, Anna Korhonen, Hinrich Schütze, arXiv:2304.08460Longform: Effective instruction tuning with reverse instructions. 2023arXiv preprint</p>
<p>Biomistral: A collection of open-source pretrained large language models for medical domains. Yanis Labrak, Adrien Bazoge, Emmanuel Morin, P Gourraud, Mickael Rouvier, Richard Dufour, 10.48550/arXiv.2402.10373Annual Meeting of the Association for Computational Linguistics. 2024</p>
<p>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James, V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A Smith, Yizhong Wang, arXiv:2411.15124Pradeep Dasigi, and Hannaneh Hajishirzi. 2024a. Tulu 3: Pushing frontiers in open language model post-training. Preprint</p>
<p>Nathan Lambert, Valentina Pyatkin, Jacob Morrison, Bill Miranda, Khyathi Yuchen Lin, Nouha Chandu, Sachin Dziri, Tom Kumar, Yejin Zick, Noah A Choi, Hannaneh Smith, Hajishirzi, arXiv:2403.13787Re-wardBench: Evaluating reward models for language modeling. 2024barXiv preprint</p>
<p>MultiCite: Modeling realistic citations requires moving beyond the single-sentence singlelabel setting. Anne Lauscher, Brandon Ko, Bailey Kuehl, Sophie Johnson, Arman Cohan, David Jurgens, Kyle Lo, 10.18653/v1/2022.naacl-main.137Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>QASA: Advanced question answering on scientific articles. Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-In Lee, Moontae Lee, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningPMLR2023202</p>
<p>Demystifying scientific problem-solving in llms by probing knowledge and reasoning. Alan Li, Yixin Liu, Arpan Sarkar, Doug Downey, Arman Cohan, arXiv:2508.192022025arXiv preprint</p>
<p>Biocreative v cdr task corpus: a resource for chemical disease relation extraction. Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Thomas C Wiegers, Zhiyong Lu, 10.1093/database/baw068Database. 682016. 2016</p>
<p>Sihang Li, Jian Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun Xu, Xiang Wang, Linfeng Zhang, Guolin Ke, Hengxing Cai, arXiv:2408.15545Scilitllm: How to adapt llms for scientific literature understanding. 2024arXiv preprint</p>
<p>Self-alignment with instruction backtranslation. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, Mike Lewis, arXiv:2308.062592023arXiv preprint</p>
<p>Kyle Lo, Joseph Chee Chang, Andrew Head, Jonathan Bragg, Amy X Zhang, Cassidy Trier, Chloe Anastasiades, Tal August, Russell Authur, Danielle Bragg, Erin Bransom, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Yen-Sung Chen, Evie ( Yu-Yen, ) Cheng, Yvonne Chou, Doug Downey, Rob Evans, Raymond Fok, F Q Hu, Regan Huff, Dongyeop Kang, Tae Soo Kim, Rodney Michael Kinney, Aniket Kittur, B Hyeonsu, Egor Kang, Bailey Klevak, Michael Kuehl, Matt Langan, Jaron Latzke, Kelsey Lochner, Eric Stuart Macmillan, Tyler C Marsh, Aakanksha Murray, Ngoc-Uyen Naik, Srishti Nguyen, Soya Palani, Caroline Park, Napol Paulic, Rachatasumrit, Paul Smita R Rao, Zejiang Sayre, Pao Shen, Luca Siangliulue, Huy Soldaini, Madeleine Tran, Lucy Lu Van Zuylen, Christopher Wang, Caroline M Wilhelm, Jiangjiang Wu, Angele Yang, Marti A Zamarron, Daniel S Hearst, Weld, ArXiv, abs/2303.14334The semantic reader project: Augmenting scholarly documents through ai-powered interactive reading interfaces. 2023</p>
<p>MUFFIN: Curating multi-faceted instructions for improving instruction following. Renze Lou, Kai Zhang, Jian Xie, Yuxuan Sun, Janice Ahn, Hanzi Xu, Yu Su, Wenpeng Yin, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, The 36th Conference on Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Multi-XScience: A large-scale dataset for extreme multidocument summarization of scientific articles. Yao Lu, Yue Dong, Laurent Charlin, 10.18653/v1/2020.emnlp-main.648Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. Yi Luan, Luheng He, Mari Ostendorf, Hannaneh Hajishirzi, 10.18653/v1/D18-1360Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, arXiv:2308.09583Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 2023arXiv preprint</p>
<p>Biored: a rich biomedical relation extraction dataset. Ling Luo, Po-Ting Lai, Chih-Hsuan Wei, Cecilia N Arighi, Zhiyong Lu, 10.1093/bib/bbac282Briefings in Bioinformatics. 2352022</p>
<p>Toward systematic review automation: a practical guide to using machine learning tools in research synthesis. Iain , James Marshall, Byron C Wallace, Systematic Reviews. 82019</p>
<p>Cross-Task Generalization via Natural Language Crowdsourcing Instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, arXiv2021</p>
<p>Medmentions: A large biomedical corpus annotated with umls concepts. Sunil Mohan, Donghui Li, arXiv:1902.094762019Preprint</p>
<p>COVID-QA: A question answering dataset for COVID-19. Timo Möller, Anthony Reina, Raghavan Jayakumar, Malte Pietsch, Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020. the 1st Workshop on NLP for COVID-19 at ACL 2020Online. Association for Computational Linguistics2020</p>
<p>The materials science procedural text corpus: Annotating materials synthesis procedures with shallow semantic structures. Sheshera Mysore, Zachary Jensen, Edward Kim, Kevin Huang, Haw-Shiuan Chang, Emma Strubell, Jeffrey Flanigan, Andrew Mccallum, Elsa Olivetti, Proceedings of the 13th Linguistic Annotation Workshop. the 13th Linguistic Annotation WorkshopFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Learning to generate instruction tuning datasets for zero-shot task adaptation. V Nihal, Yiyang Nayak, Avi Nan, Stephen H Trost, Bach, arXiv:2402.183342024arXiv preprint</p>
<p>A corpus with multi-level annotations of patients, interventions and outcomes to support language processing for medical literature. Benjamin Nye, Jessy Junyi, Roma Li, Yinfei Patel, Iain Yang, Ani Marshall, Byron Nenkova, Wallace, 10.18653/v1/P18-1019Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, arXiv:2203.02155Jan Leike, and Ryan Lowe. 2022Preprint</p>
<p>Relatedly: Scaffolding literature reviews with existing related work sections. Srishti Palani, Aakanksha Naik, Doug Downey, Amy X Zhang, Jonathan Bragg, Joseph Chee, Chang , Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>In-BoXBART: Get instructions into biomedical multitask learning. Mihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, Murad Mohammad, Chitta Baral, Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets. Yifan Peng, Shankai Yan, Zhiyong Lu, Proceedings of the 2019 Workshop on Biomedical Natural Language Processing. the 2019 Workshop on Biomedical Natural Language Processing2019. 2019</p>
<p>Anatomical entity mention recognition at literature scale. Sampo Pyysalo, Sophia Ananiadou, Bioinformatics. 3062014</p>
<p>M2d2: A massively multidomain language modeling dataset. Machel Reid, Victor Zhong, Suchin Gururangan, Luke Zettlemoyer, 2022</p>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, arXiv:2311.12022Gpqa: A graduate-level google-proof q&amp;a benchmark. 2023arXiv preprint</p>
<p>Exploring the effectiveness of instruction tuning in biomedical language processing. Mohammadmahdi Omid Rohanian, David A Nouriborji, Clifton, arXiv:2401.005792023arXiv preprint</p>
<p>Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Romain Liu, Tal Sauvestre, Jérémy Remez, Artyom Rapin, Ivan Kozhevnikov, Joanna Evtimov, Manish Bitton, Cristian Canton Bhatt, Aaron Ferrer, Wenhan Grattafiori, Alexandre Xiong, Jade Défossez, Faisal Copet, Hugo Azhar, Touvron, arXiv:2308.12950Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code. arXiv preprint</p>
<p>COVID-fact: Fact extraction and verification of real-world claims on COVID-19 pandemic. Arkadiy Saakyan, Tuhin Chakrabarty, Smaranda Muresan, 10.18653/v1/2021.acl-long.165Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Evidence-based fact-checking of health-related claims. Mourad Sarrouti, Asma Ben Abacha, Yassine Mrabet, Dina Demner-Fushman, 10.18653/v1/2021.findings-emnlp.297Findings of the Association for Computational Linguistics: EMNLP 2021. Punta CanaDominican Republic. Association for Computational Linguistics2021</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, CoRR, abs/2402.033002024</p>
<p>Specialist or generalist? instruction tuning for specific NLP tasks. Chufan Shi, Yixuan Su, Cheng Yang, Yujiu Yang, Deng Cai, 10.18653/v1/2023.emnlp-main.947Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>SciRepEval: A multi-format benchmark for scientific document representations. Amanpreet Singh, Mike D' Arcy, Arman Cohan, Doug Downey, Sergey Feldman, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>HoneyBee: Progressive instruction finetuning of large language models for materials science. Yu Song, Santiago Miret, Huan Zhang, Bang Liu, Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Mathscale: Scaling instruction tuning for mathematical reasoning. Zhengyang Tang, Xingxing Zhang, Benyou Wang, Furu Wei, arXiv:2403.028842024arXiv preprint</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>A dataset for n-ary relation extraction of drug combinations. Aryeh Tiktinsky, Vijay Viswanathan, Danna Niezni, Yosi Dana Meron Azagury, Hillel Shamay, Tom Taub-Tabib, Yoav Hope, Goldberg, 10.18653/v1/2022.naacl-main.233Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Clinical camel: An open-source expert-level medical language model with dialogue-based knowledge encoding. Augustin Toma, Patrick R Lawler, Jimmy Ba, G Rahul, Barry Krishnan, Bo Rubin, Wang, ArXiv, abs/2305.120312023</p>
<p>Openmathinstruct-1: A 1.8 million math instruction tuning dataset. Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, Igor Gitman, arXiv: Arxiv- 2402.101762024arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, arXiv:2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien RodriguezPreprintand Thomas Scialom. 2023a. Llama 2: Open foundation and finetuned chat models</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023barXiv preprint</p>
<p>Bioinstruct: Instruction tuning of large language models for biomedical natural language processing. Hieu Tran, Zhichao Yang, Zonghai Yao, Hong Yu, JAMIA Journal of the American Medical Informatics Association. 2023</p>
<p>Sergios Petridis, Dimitris Polychronopoulos, et al. 2015. An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Dirk Michael R Alvers, Anastasia Weissenborn, Krithara, 10.1186/s12859-015-0564-6BMC Bioinformatics. 16138</p>
<p>DataFinder: Scientific dataset recommendation from natural language descriptions. Vijay Viswanathan, Luyu Gao, Tongshuang Wu, Pengfei Liu, Graham Neubig, 10.18653/v1/2023.acl-long.573Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada20231Association for Computational Linguistics</p>
<p>Mind your format: Towards consistent evaluation of in-context learning improvements. Anton Voronov, Lena Wolf, Max Ryabinin, 10.48550/arXiv.2401.06766Annual Meeting of the Association for Computational Linguistics. 2024</p>
<p>Fact or fiction: Verifying scientific claims. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine Van Zuylen, Arman Cohan, Hannaneh Hajishirzi, 10.18653/v1/2020.emnlp-main.609Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Generating (factual?) narrative summaries of rcts: Experiments with neural multi-document summarization. Byron C Wallace, Sayantan Saha, Frank Soboczenski, Iain J Marshall, AMIA Joint Summits on Translational Science Proceedings. 2021</p>
<p>Xiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze Chen, Yuansen Zhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao Gui, Jihua Kang, Jingsheng Yang, Siyuan Li, Chunsai Du, arXiv:2304.08085Instructuie: Multi-task instruction tuning for unified information extraction. 2023aarXiv preprint</p>
<p>SCIBENCH: Evaluating college-level scientific problem-solving abilities of large language models. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun Loomba, Shichang Zhang, Yizhou Sun, Wei Wang, The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23. 2023b</p>
<p>Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. Botao Yu, Frazier N Baker, Ziqi Chen, Xia Ning, Huan Sun, arXiv:2402.093912024arXiv preprint</p>
<p>MAmmoTH: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, The Twelfth International Conference on Learning Representations. 2024a</p>
<p>Xiang Yue, Tuney Zheng, Ge Zhang, Wenhu Chen, arXiv:2405.03548Mammoth2: Scaling instructions from the web. 2024barXiv preprint</p>
<p>Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, Jie Tang, arXiv:2401.079502024aarXiv preprint</p>
<p>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Wanli Ouyang, Dongzhan Zhou, Shufei Zhang, Mao Su, Han-Sen Zhong, Yuqiang Li, arXiv:2402.06852Chemllm: A chemical large language model. 2024barXiv preprint</p>
<p>Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang, arXiv:2308.10792struction tuning for large language models: A survey. 2023aarXiv preprint</p>
<p>Tianshu Zhang, Xiang Yue, Yifei Li, Huan Sun, arXiv:2311.09206Tablellama: Towards open large generalist models for tables. 2023barXiv preprint</p>
<p>Alpacare:instruction-tuned large language models for medical application. Xinlu Zhang, Chenxin Tian, Xianjun Yang, Lichang Chen, Zekun Li, Linda Ruth Petzold, arXiv:2310.145582023carXiv preprint</p>
<p>Sciarena: An open evaluation platform for foundation models in scientific literature tasks. Yilun Zhao, Kaiyan Zhang, Tiansheng Hu, Sihong Wu, Ronan Le Bras, Taira Anderson, Jonathan Bragg, Joseph Chee Chang, Jesse Dodge, Matt Latzke, Yixin Liu, Charles Mcgrady, Xiangru Tang, Zihang Wang, Chen Zhao, Hannaneh Hajishirzi, Doug Downey, Arman Cohan, arXiv:2507.010012025arXiv preprint</p>
<p>Lima: Less is more for alignment. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, L Yu, Susan Zhang, Gargi Ghosh, M Lewis, Luke Zettlemoyer, Omer Levy, Advances in Neural Information Processing Systems (NeurIPS). 2023</p>            </div>
        </div>

    </div>
</body>
</html>
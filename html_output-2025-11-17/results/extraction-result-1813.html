<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1813 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1813</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1813</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-ec9a1de6cfeb06198377359709c7049001ad38f4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ec9a1de6cfeb06198377359709c7049001ad38f4" target="_blank">VR-Goggles for Robots: Real-to-Sim Domain Adaptation for Visual Control</a></p>
                <p><strong>Paper Venue:</strong> IEEE Robotics and Automation Letters</p>
                <p><strong>Paper TL;DR:</strong> This letter proposes a simple yet effective shift loss that is agnostic to the downstream task, to constrain the consistency between subsequent frames which is important for consistent policy outputs, and validate the shift loss for artistic style transfer for videos and domain adaptation.</p>
                <p><strong>Paper Abstract:</strong> In this letter, we deal with the reality gap from a novel perspective, targeting transferring deep reinforcement learning (DRL) policies learned in simulated environments to the real-world domain for visual control tasks. Instead of adopting the common solutions to the problem by increasing the visual fidelity of synthetic images output from simulators during the training phase, we seek to tackle the problem by translating the real-world image streams back to the synthetic domain during the deployment phase, to make the robot feel at home. We propose this as a lightweight, flexible, and efficient solution for visual control, as first, no extra transfer steps are required during the expensive training of DRL agents in simulation; second, the trained DRL agents will not be constrained to being deployable in only one specific real-world environment; and third, the policy training and the transfer operations are decoupled, and can be conducted in parallel. Besides this, we propose a simple yet effective shift loss that is agnostic to the downstream task, to constrain the consistency between subsequent frames which is important for consistent policy outputs. We validate the shift loss for artistic style transfer for videos and domain adaptation, and validate our visual control approach in indoor and outdoor robotics experiments.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1813.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1813.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VR-Goggles</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VR-Goggles (Real-to-Sim Domain Adaptation with Shift Loss)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deployment-time real-to-sim image translation pipeline that maps real camera streams to the simulated visual domain used for policy training; includes a novel 'shift loss' to enforce temporal consistency across sequential frames without requiring optical flow.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Visual-control agents (sim-trained navigation and driving policies) with VR-Goggles adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Learning-based visual-control agents (A3C-trained navigation policy for indoor mobile robot; imitation/conditional-imitation driving policies for autonomous driving) that take first-person RGB images as input and output control commands (discrete navigation commands or steering).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics (visual navigation and autonomous driving)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Gazebo; CARLA</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Gazebo: indoor robot simulator used to render first-person RGB images for navigation tasks (provides physics/robot kinematics but not semantic ground-truth in this study). CARLA: urban driving simulator that produces photorealistic daytime RGB renderings and provides ground-truth semantic labels and varied weather conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Photorealistic rendering for CARLA (visual/weather variation); moderate/standard indoor rendering for Gazebo (not photorealistic); physics/dynamics modeled at simulator defaults but visual realism is the primary fidelity focus.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Visual rendering (textures, scene geometry, lighting & weather in CARLA), camera viewpoint, semantic labels (in CARLA), vehicle kinematics/robot motion as provided by the simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Real camera noise characteristics, nighttime lighting conditions (when testing nighttime RobotCar data vs CARLA daytime), sensor noise distributions, detailed real-world photometric/color shifts, and some real-world appearance artifacts were not modeled; domain mismatch in texture/lighting and image-level noise remained and were handled by translation rather than simulator changes.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Indoor: real office with RealSense R200 on a Turtlebot3 Waffle; Outdoor driving: real nighttime town street imagery from Oxford RobotCar dataset and tests on a Bulldog vehicle with a PointGrey Blackfly camera.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Goal-directed visual navigation (indoor) and conditional imitation driving / lane-following (outdoor driving).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Navigation: reinforcement learning (A3C); Driving: imitation learning / conditional imitation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Navigation/driving success rate (task completion), average percentage of distance-to-goal traveled, average distance between infractions (e.g., collisions, opposite-lane, sidewalk), collision rates; temporal loss (for video-style-shift-consistency experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Simulation evaluation (examples from the paper): Single-Domain training: Straight success 81.3%, One-turn 64.0%, Navigation 60.0%, Nav.dynamic 58.7%; Multi-Domain training: Straight 97.3%, One-turn 85.3%, Navigation 84.0%, Nav.dynamic 74.7% (these are reported in-CARLA simulated evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Representative reported results: Indoor real deployment success rates: No-Goggles 0%, CycleGAN 60%, VR-Goggles 100%; CARLA->real/weather-testing (benchmark style): for Navigation task (Single-Domain policy) No-Goggles 0.0%, CycleGAN 21.3%, VR-Goggles 45.3%; for Navigation (Multi-Domain) No-Goggles 0.0%, CycleGAN 42.7%, VR-Goggles 61.3%. (Various other metrics in Table II show consistent improvements using VR-Goggles.)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Not used in the study; authors note domain randomization as a related approach but argue it was not directly applicable given dataset/recoding constraints. A 'Multi-Domain' training (combining several weather-condition datasets) was used as a limited analogue to domain randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Differences in texture, lighting and weather (day vs. night), visual appearance (color/contrast), lack of simulator-modeled camera noise and real-world visual artifacts, semantic/statistical distribution shift between sim and real images, and temporal inconsistency of frame-to-frame translations (leading to unstable policy outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Applying real-to-sim image translation at deployment (so no training-time image adaptation needed), training a per-real-environment adaptation model using a modest set of collected real images (~2000 images per environment as suggested), using semantic-consistency loss when simulator semantic labels are available (CyCADA-style) and the proposed shift loss to ensure temporal consistency across frames (avoiding optical-flow dependency). Decoupling adaptation from policy training enabled parallel development and ease of deployment across different real environments without retraining policies.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>The paper emphasizes that perfect photorealism in simulation is not required if a robust real-to-sim translator is available; instead, important requirements are (1) adequate semantic alignment (helped by simulator semantic labels) and (2) temporal consistency in translated streams (enforced by shift loss). Quantitative fidelity bounds (e.g., percent errors) are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A deployment-time real-to-sim adaptation (VR-Goggles) can effectively bridge the reality gap for visual-control policies trained in simulation: it avoids adding overhead during policy training, allows reusing the same policy across visually different real environments, and—when combined with a shift loss—produces temporally consistent translated image streams that substantially improve task success in both simulated benchmarks (CARLA testing-weather) and real indoor/outdoor deployments. Semantic constraints (when available) and the shift loss are important enablers; collecting a modest dataset of real images per target environment and training an adaptation model suffices instead of retraining policies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VR-Goggles for Robots: Real-to-Sim Domain Adaptation for Visual Control', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1813.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1813.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CycleGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cycle-Consistent Generative Adversarial Network (CycleGAN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised unpaired image-to-image translation method that learns mappings between two visual domains using adversarial and cycle-consistency losses; used here as a baseline adaptation technique to translate real images to the simulated domain at deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unpaired image-to-image translation using cycle-consistent adversarial networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Visual-control agents (navigation and driving) with CycleGAN-based input adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Same learning-based visual navigation and driving agents as tested with VR-Goggles; CycleGAN is used to translate real sensory inputs into the simulator visual domain before feeding to the trained policy.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics (visual navigation and autonomous driving)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Gazebo; CARLA (as the target simulated domains used by policies)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Same as for VR-Goggles: Gazebo for indoor navigation rendering; CARLA for urban driving scenes with varied weather and semantic labels.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Same simulator visual fidelity as above (CARLA photorealistic; Gazebo less so); CycleGAN focuses on image-level translation rather than altering simulator fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Visual appearance: maps distribution of real images to simulator image distribution; can exploit semantic constraints when combined with segmentation models (CyCADA extension).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Does not explicitly enforce temporal consistency (unless extended), and without semantic constraints may permute semantics in complex scenes; does not model dynamics or sensor temporal noise patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Same real-world testbeds: indoor office with RealSense on Turtlebot3; outdoor nighttime RobotCar and Bulldog vehicle tests.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Same tasks: visual navigation and driving via image translation prior to policy inference.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>CycleGAN trained unsupervised on unpaired sets of simulated and real images to learn mapping real->sim or sim->real (here used sim<-real).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Same policy-level metrics: success rate, % distance to goal, infractions metrics; temporal-loss metrics used in stylization experiments to measure frame-to-frame consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>N/A (CycleGAN is an adaptation model rather than a policy trainer); simulation performance of policies unchanged by CycleGAN training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Improved relative to no adaptation but typically worse than VR-Goggles: example indoor deployment: CycleGAN success 60% (versus 0% for No-Goggles and 100% for VR-Goggles); CARLA benchmark testing: CycleGAN often improves success vs direct deployment (e.g., Navigation Single-Domain 21.3% with CycleGAN vs 0.0% No-Goggles) but is outperformed by VR-Goggles in most reported metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>CycleGAN struggles when semantic permutations occur or when temporal consistency is required (single-frame translation can introduce frame-to-frame artifacts), and it may not fully correct for lighting/weather mismatches and camera noise differences.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Sufficient unpaired training images from both domains; addition of semantic-consistency loss (CyCADA-style) when simulator semantic labels are available can help preserve semantics; however, temporal-consistency mechanisms (e.g., shift loss or optical-flow constraints) are needed for stable control.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>CycleGAN requires the domain mapping to be learnable from available unpaired images; it benefits from semantic supervision to avoid semantic misalignment, but paper does not provide quantitative fidelity thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CycleGAN-based real-to-sim translation improves sim-to-real transfer versus no adaptation, but lacks temporal-consistency guarantees and can produce artifacts that impair control; augmenting CycleGAN with the paper's shift loss and semantic constraints (VR-Goggles) yields better downstream policy performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VR-Goggles for Robots: Real-to-Sim Domain Adaptation for Visual Control', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1813.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1813.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain Randomization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Randomization (visual randomization in simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sim-to-real strategy that randomizes visual properties (textures, lighting, camera pose) during simulator training so policies generalize to real-world variability without explicit image translation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Domain randomization for transferring deep neural networks from simulation to the real world</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>General sim-trained agents (conceptual technique, not specifically instantiated in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Technique intended to make policies robust to visual variability by training on highly randomized simulated renderings so they generalize to real images.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics (general visual sim-to-real transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Various simulators (conceptual; referenced works use custom randomized renderers and simulators)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulators augmented to randomize texture, lighting, camera parameters and other visual properties across training episodes to span a wide appearance distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Approach focuses on extensive visual diversity rather than high photorealistic fidelity; can be implemented with low to moderate visual fidelity but high randomized variability.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Randomized textures, lighting, camera viewpoints; possibly other visual factors to cover target real-world appearance space.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Physics/dynamics not necessarily varied/accurate; real camera noise and specific scene semantics may not be covered unless explicitly randomized.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Not directly applied in this paper's experiments due to dataset and setup constraints; discussed as a related sim-to-real technique.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>General: robust visual policies that transfer to varied real-world appearances without per-environment adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Training-time augmentation of simulator renderings with randomized visual parameters prior to policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Not reported in this paper's experiments; conceptually measured via downstream policy performance on real tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Mentioned as randomizing textures, lighting conditions, and camera positions during training; authors note practical limitations in some simulators and no guarantee of covering arbitrary real scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>May fail if the randomized distribution does not cover the real environment's visual modes; computational cost or simulator capability may limit effective randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Requires the simulator to support broad, inexpensive randomization of visual properties; large-scale randomized datasets during training.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper notes domain randomization can help but may not be feasible with some simulators and cannot guarantee coverage of arbitrary real-world visual domains.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Discussed as a competing sim-to-real approach; authors highlight limitations (simulator capabilities and coverage) and motivate their real-to-sim alternative that avoids expensive training-time augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VR-Goggles for Robots: Real-to-Sim Domain Adaptation for Visual Control', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unpaired image-to-image translation using cycle-consistent adversarial networks <em>(Rating: 2)</em></li>
                <li>CyCADA: Cycle-consistent adversarial domain adaptation <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Using simulation and domain adaptation to improve efficiency of deep robotic grasping <em>(Rating: 2)</em></li>
                <li>Sim-to-real robot learning from pixels with progressive nets <em>(Rating: 2)</em></li>
                <li>Transfer learning from synthetic to real images using variational autoencoders for precise position detection <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1813",
    "paper_id": "paper-ec9a1de6cfeb06198377359709c7049001ad38f4",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "VR-Goggles",
            "name_full": "VR-Goggles (Real-to-Sim Domain Adaptation with Shift Loss)",
            "brief_description": "A deployment-time real-to-sim image translation pipeline that maps real camera streams to the simulated visual domain used for policy training; includes a novel 'shift loss' to enforce temporal consistency across sequential frames without requiring optical flow.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Visual-control agents (sim-trained navigation and driving policies) with VR-Goggles adaptation",
            "agent_system_description": "Learning-based visual-control agents (A3C-trained navigation policy for indoor mobile robot; imitation/conditional-imitation driving policies for autonomous driving) that take first-person RGB images as input and output control commands (discrete navigation commands or steering).",
            "domain": "robotics (visual navigation and autonomous driving)",
            "virtual_environment_name": "Gazebo; CARLA",
            "virtual_environment_description": "Gazebo: indoor robot simulator used to render first-person RGB images for navigation tasks (provides physics/robot kinematics but not semantic ground-truth in this study). CARLA: urban driving simulator that produces photorealistic daytime RGB renderings and provides ground-truth semantic labels and varied weather conditions.",
            "simulation_fidelity_level": "Photorealistic rendering for CARLA (visual/weather variation); moderate/standard indoor rendering for Gazebo (not photorealistic); physics/dynamics modeled at simulator defaults but visual realism is the primary fidelity focus.",
            "fidelity_aspects_modeled": "Visual rendering (textures, scene geometry, lighting & weather in CARLA), camera viewpoint, semantic labels (in CARLA), vehicle kinematics/robot motion as provided by the simulators.",
            "fidelity_aspects_simplified": "Real camera noise characteristics, nighttime lighting conditions (when testing nighttime RobotCar data vs CARLA daytime), sensor noise distributions, detailed real-world photometric/color shifts, and some real-world appearance artifacts were not modeled; domain mismatch in texture/lighting and image-level noise remained and were handled by translation rather than simulator changes.",
            "real_environment_description": "Indoor: real office with RealSense R200 on a Turtlebot3 Waffle; Outdoor driving: real nighttime town street imagery from Oxford RobotCar dataset and tests on a Bulldog vehicle with a PointGrey Blackfly camera.",
            "task_or_skill_transferred": "Goal-directed visual navigation (indoor) and conditional imitation driving / lane-following (outdoor driving).",
            "training_method": "Navigation: reinforcement learning (A3C); Driving: imitation learning / conditional imitation learning.",
            "transfer_success_metric": "Navigation/driving success rate (task completion), average percentage of distance-to-goal traveled, average distance between infractions (e.g., collisions, opposite-lane, sidewalk), collision rates; temporal loss (for video-style-shift-consistency experiments).",
            "transfer_performance_sim": "Simulation evaluation (examples from the paper): Single-Domain training: Straight success 81.3%, One-turn 64.0%, Navigation 60.0%, Nav.dynamic 58.7%; Multi-Domain training: Straight 97.3%, One-turn 85.3%, Navigation 84.0%, Nav.dynamic 74.7% (these are reported in-CARLA simulated evaluations).",
            "transfer_performance_real": "Representative reported results: Indoor real deployment success rates: No-Goggles 0%, CycleGAN 60%, VR-Goggles 100%; CARLA-&gt;real/weather-testing (benchmark style): for Navigation task (Single-Domain policy) No-Goggles 0.0%, CycleGAN 21.3%, VR-Goggles 45.3%; for Navigation (Multi-Domain) No-Goggles 0.0%, CycleGAN 42.7%, VR-Goggles 61.3%. (Various other metrics in Table II show consistent improvements using VR-Goggles.)",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": "Not used in the study; authors note domain randomization as a related approach but argue it was not directly applicable given dataset/recoding constraints. A 'Multi-Domain' training (combining several weather-condition datasets) was used as a limited analogue to domain randomization.",
            "sim_to_real_gap_factors": "Differences in texture, lighting and weather (day vs. night), visual appearance (color/contrast), lack of simulator-modeled camera noise and real-world visual artifacts, semantic/statistical distribution shift between sim and real images, and temporal inconsistency of frame-to-frame translations (leading to unstable policy outputs).",
            "transfer_enabling_conditions": "Applying real-to-sim image translation at deployment (so no training-time image adaptation needed), training a per-real-environment adaptation model using a modest set of collected real images (~2000 images per environment as suggested), using semantic-consistency loss when simulator semantic labels are available (CyCADA-style) and the proposed shift loss to ensure temporal consistency across frames (avoiding optical-flow dependency). Decoupling adaptation from policy training enabled parallel development and ease of deployment across different real environments without retraining policies.",
            "fidelity_requirements_identified": "The paper emphasizes that perfect photorealism in simulation is not required if a robust real-to-sim translator is available; instead, important requirements are (1) adequate semantic alignment (helped by simulator semantic labels) and (2) temporal consistency in translated streams (enforced by shift loss). Quantitative fidelity bounds (e.g., percent errors) are not provided.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "A deployment-time real-to-sim adaptation (VR-Goggles) can effectively bridge the reality gap for visual-control policies trained in simulation: it avoids adding overhead during policy training, allows reusing the same policy across visually different real environments, and—when combined with a shift loss—produces temporally consistent translated image streams that substantially improve task success in both simulated benchmarks (CARLA testing-weather) and real indoor/outdoor deployments. Semantic constraints (when available) and the shift loss are important enablers; collecting a modest dataset of real images per target environment and training an adaptation model suffices instead of retraining policies.",
            "uuid": "e1813.0",
            "source_info": {
                "paper_title": "VR-Goggles for Robots: Real-to-Sim Domain Adaptation for Visual Control",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "CycleGAN",
            "name_full": "Cycle-Consistent Generative Adversarial Network (CycleGAN)",
            "brief_description": "An unsupervised unpaired image-to-image translation method that learns mappings between two visual domains using adversarial and cycle-consistency losses; used here as a baseline adaptation technique to translate real images to the simulated domain at deployment.",
            "citation_title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
            "mention_or_use": "use",
            "agent_system_name": "Visual-control agents (navigation and driving) with CycleGAN-based input adaptation",
            "agent_system_description": "Same learning-based visual navigation and driving agents as tested with VR-Goggles; CycleGAN is used to translate real sensory inputs into the simulator visual domain before feeding to the trained policy.",
            "domain": "robotics (visual navigation and autonomous driving)",
            "virtual_environment_name": "Gazebo; CARLA (as the target simulated domains used by policies)",
            "virtual_environment_description": "Same as for VR-Goggles: Gazebo for indoor navigation rendering; CARLA for urban driving scenes with varied weather and semantic labels.",
            "simulation_fidelity_level": "Same simulator visual fidelity as above (CARLA photorealistic; Gazebo less so); CycleGAN focuses on image-level translation rather than altering simulator fidelity.",
            "fidelity_aspects_modeled": "Visual appearance: maps distribution of real images to simulator image distribution; can exploit semantic constraints when combined with segmentation models (CyCADA extension).",
            "fidelity_aspects_simplified": "Does not explicitly enforce temporal consistency (unless extended), and without semantic constraints may permute semantics in complex scenes; does not model dynamics or sensor temporal noise patterns.",
            "real_environment_description": "Same real-world testbeds: indoor office with RealSense on Turtlebot3; outdoor nighttime RobotCar and Bulldog vehicle tests.",
            "task_or_skill_transferred": "Same tasks: visual navigation and driving via image translation prior to policy inference.",
            "training_method": "CycleGAN trained unsupervised on unpaired sets of simulated and real images to learn mapping real-&gt;sim or sim-&gt;real (here used sim&lt;-real).",
            "transfer_success_metric": "Same policy-level metrics: success rate, % distance to goal, infractions metrics; temporal-loss metrics used in stylization experiments to measure frame-to-frame consistency.",
            "transfer_performance_sim": "N/A (CycleGAN is an adaptation model rather than a policy trainer); simulation performance of policies unchanged by CycleGAN training.",
            "transfer_performance_real": "Improved relative to no adaptation but typically worse than VR-Goggles: example indoor deployment: CycleGAN success 60% (versus 0% for No-Goggles and 100% for VR-Goggles); CARLA benchmark testing: CycleGAN often improves success vs direct deployment (e.g., Navigation Single-Domain 21.3% with CycleGAN vs 0.0% No-Goggles) but is outperformed by VR-Goggles in most reported metrics.",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "CycleGAN struggles when semantic permutations occur or when temporal consistency is required (single-frame translation can introduce frame-to-frame artifacts), and it may not fully correct for lighting/weather mismatches and camera noise differences.",
            "transfer_enabling_conditions": "Sufficient unpaired training images from both domains; addition of semantic-consistency loss (CyCADA-style) when simulator semantic labels are available can help preserve semantics; however, temporal-consistency mechanisms (e.g., shift loss or optical-flow constraints) are needed for stable control.",
            "fidelity_requirements_identified": "CycleGAN requires the domain mapping to be learnable from available unpaired images; it benefits from semantic supervision to avoid semantic misalignment, but paper does not provide quantitative fidelity thresholds.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "CycleGAN-based real-to-sim translation improves sim-to-real transfer versus no adaptation, but lacks temporal-consistency guarantees and can produce artifacts that impair control; augmenting CycleGAN with the paper's shift loss and semantic constraints (VR-Goggles) yields better downstream policy performance.",
            "uuid": "e1813.1",
            "source_info": {
                "paper_title": "VR-Goggles for Robots: Real-to-Sim Domain Adaptation for Visual Control",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "Domain Randomization",
            "name_full": "Domain Randomization (visual randomization in simulation)",
            "brief_description": "A sim-to-real strategy that randomizes visual properties (textures, lighting, camera pose) during simulator training so policies generalize to real-world variability without explicit image translation.",
            "citation_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "mention_or_use": "mention",
            "agent_system_name": "General sim-trained agents (conceptual technique, not specifically instantiated in experiments)",
            "agent_system_description": "Technique intended to make policies robust to visual variability by training on highly randomized simulated renderings so they generalize to real images.",
            "domain": "robotics (general visual sim-to-real transfer)",
            "virtual_environment_name": "Various simulators (conceptual; referenced works use custom randomized renderers and simulators)",
            "virtual_environment_description": "Simulators augmented to randomize texture, lighting, camera parameters and other visual properties across training episodes to span a wide appearance distribution.",
            "simulation_fidelity_level": "Approach focuses on extensive visual diversity rather than high photorealistic fidelity; can be implemented with low to moderate visual fidelity but high randomized variability.",
            "fidelity_aspects_modeled": "Randomized textures, lighting, camera viewpoints; possibly other visual factors to cover target real-world appearance space.",
            "fidelity_aspects_simplified": "Physics/dynamics not necessarily varied/accurate; real camera noise and specific scene semantics may not be covered unless explicitly randomized.",
            "real_environment_description": "Not directly applied in this paper's experiments due to dataset and setup constraints; discussed as a related sim-to-real technique.",
            "task_or_skill_transferred": "General: robust visual policies that transfer to varied real-world appearances without per-environment adaptation.",
            "training_method": "Training-time augmentation of simulator renderings with randomized visual parameters prior to policy learning.",
            "transfer_success_metric": "Not reported in this paper's experiments; conceptually measured via downstream policy performance on real tasks.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": "Mentioned as randomizing textures, lighting conditions, and camera positions during training; authors note practical limitations in some simulators and no guarantee of covering arbitrary real scenes.",
            "sim_to_real_gap_factors": "May fail if the randomized distribution does not cover the real environment's visual modes; computational cost or simulator capability may limit effective randomization.",
            "transfer_enabling_conditions": "Requires the simulator to support broad, inexpensive randomization of visual properties; large-scale randomized datasets during training.",
            "fidelity_requirements_identified": "Paper notes domain randomization can help but may not be feasible with some simulators and cannot guarantee coverage of arbitrary real-world visual domains.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Discussed as a competing sim-to-real approach; authors highlight limitations (simulator capabilities and coverage) and motivate their real-to-sim alternative that avoids expensive training-time augmentation.",
            "uuid": "e1813.2",
            "source_info": {
                "paper_title": "VR-Goggles for Robots: Real-to-Sim Domain Adaptation for Visual Control",
                "publication_date_yy_mm": "2018-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
            "rating": 2
        },
        {
            "paper_title": "CyCADA: Cycle-consistent adversarial domain adaptation",
            "rating": 2
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2
        },
        {
            "paper_title": "Using simulation and domain adaptation to improve efficiency of deep robotic grasping",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real robot learning from pixels with progressive nets",
            "rating": 2
        },
        {
            "paper_title": "Transfer learning from synthetic to real images using variational autoencoders for precise position detection",
            "rating": 1
        }
    ],
    "cost": 0.014665000000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>VR-Goggles for Robots: Real-to-sim Domain Adaptation for Visual Control</h1>
<p>Jingwei Zhang ${ }^{<em> 1}$ Lei Tai</em>2 Peng Yun ${ }^{2}$ Yufeng Xiong ${ }^{1}$ Ming Liu ${ }^{2}$ Joschka Boedecker ${ }^{1}$ Wolfram Burgard ${ }^{1}$</p>
<h4>Abstract</h4>
<p>In this paper, we deal with the reality gap from a novel perspective, targeting transferring Deep Reinforcement Learning (DRL) policies learned in simulated environments to the real-world domain for visual control tasks. Instead of adopting the common solutions to the problem by increasing the visual fidelity of synthetic images output from simulators during the training phase, we seek to tackle the problem by translating the real-world image streams back to the synthetic domain during the deployment phase, to make the robot feel at home. We propose this as a lightweight, flexible, and efficient solution for visual control, as 1) no extra transfer steps are required during the expensive training of DRL agents in simulation; 2) the trained DRL agents will not be constrained to being deployable in only one specific real-world environment; 3) the policy training and the transfer operations are decoupled, and can be conducted in parallel. Besides this, we propose a simple yet effective shift loss that is agnostic to the downstream task, to constrain the consistency between subsequent frames which is important for consistent policy outputs. We validate the shift loss for artistic style transfer for videos and domain adaptation, and validate our visual control approach in indoor and outdoor robotics experiments.</p>
<p>Index Terms-Deep Learning in Robotics and Automation, Visual-Based Navigation, Model Learning for Control.</p>
<h2>I. INTRODUCTION</h2>
<p>DIONEERED by the Deep Q-network [1] and followed up by various extensions and advancements [2]-[5], Deep Reinforcement Learning (DRL) algorithms show great potential in solving high-dimensional real-world robotics sensory control tasks. However, DRL methods typically require several millions of training samples, making them infeasible to train directly on real robotic systems. As a result, DRL algorithms are generally trained in simulated environments, then transferred to and deployed in real scenes. However, the reality gap, namely the noise pattern, texture, lighting condition discrepancies, etc., between synthetic renderings and real sensory readings, imposes major challenges for generalising the sensory control policies trained in simulation to reality.</p>
<p>This work was supported by: the Shenzhen Science, Technology and Innovation Commission (SZSTI) JCYJ20160428154842603; the BrainLinksBrainTools cluster of excellence funded by the DFG (German Research Foundation), grant number EXC 1086; the Research Grant Council of Hong Kong SAR Government, China, under Project No. 11210017, No. 16212815 and No. 21202816, the National Natural Science Foundation of China (Grant No. U1713211) awarded to Prof. Ming Liu. (Corresponding authors: Jingwei Zhang and Lei Tai.)
*The first two authors contributed equally to this work.
${ }^{1}$ Jingwei Zhang, Yufeng Xiong, Joschka Boedecker and Wolfram Burgard are with the Department of Computer Science, University of Freiburg. Breisgau 79110, Germany (e-mail: {zhang, xiongy, jboedeck, burgard}@informatik.uni-freiburg.de)
${ }^{2}$ Lei Tai, Peng Yun and Ming Liu are with the Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong (e-mail: {ltai, pyun, eelium}@ust.hk)</p>
<p>In this paper, we focus on visual control tasks, where autonomous agents perceive the environment with their onboard cameras, and execute commands based on the colour image reading streams. A natural way and also the typical choice in the recent literature on dealing with the reality gap for visual control, is by increasing the visual fidelity of the simulated images [6], [7], by matching the distribution of synthetic images to that of the real ones [8], [9], and by gradually adapting the learned features and representations from the simulated domain to the real-world domain [10]. These sim-to-real methods, however, inevitably have to add preprocessing steps for each individual training frame to the already expensive learning pipeline of DRL policies; or a policy training or finetuning phase has to be conducted for each visually different real-world scene.</p>
<p>This paper attempts to tackle the reality gap in the visual control domain from a novel perspective, with the aim of adding minimal extra computational burden to the learning pipeline. We cope with the reality gap only during the actual deployment phase of agents in real-world scenarios, by adapting the real camera streams to the synthetic modality, so as to translate the unfamiliar or unseen features of real images back into the simulated style, which the agents have already learned how to deal with during training in the simulation.</p>
<p>Compared to the sim-to-real methods bridging the reality gap, our proposed real-to-sim approach, which we refer to as the VR-Goggles, has several appealing properties: (1) Our proposed method is highly lightweight: It does not add any extra processing burden to the training phase of DRL policies; and (2) Our approach is highly flexible and efficient: Since we decouple the policy training and the adaptation operations, the preparations for transferring the polices from simulation to the real world can be conducted in parallel with the training of the control policies. From each visually different realworld environment that we expect to deploy the agent in, we just need to collect several (typically on the order of 2000) images, and train a VR-Goggles model for each of them. More importantly, we do not need to retrain or finetune the visual control policy for new environments.</p>
<p>As an additional contribution, we propose a new shift loss, which enables generating consistent synthetic image streams without imposing temporal constraints, and does not require sequential training data. We show that shift loss is a promising and cheap alternative to the constraints imposed by optical flow, and demonstrate its effectiveness in artistic style transfer for videos and domain adaptation.</p>
<h2>II. Related Works</h2>
<h2>A. Domain Adaptation</h2>
<p>Visual domain adaptation, or image-to-image translation, targets translating images from a source domain into a target domain. We here focus on the most general unsupervised methods that require minimal manual effort and are applicable in robotics control tasks.</p>
<p>CycleGAN [11] introduced a cycle-consistent loss to enforce an inverse mapping from the target domain to the source domain on top of the source to target mapping. It does not require paired data from the two domains of interest and shows convincing results for relatively simple data distributions containing few semantic types. However, in terms of translating between more complex data distributions containing many more semantic types, its results are not as satisfactory, in that permutations of semantics often occur. Several works investigate imposing semantic constraints [12], [13], e.g., CyCADA [12] enforces a matching between the semantic map of the translated image and that of the input.</p>
<h2>B. Domain Adaptation for Learning based Visual Control</h2>
<p>Learning-based methods such as DRL and imitation learning have been applied to robotics control tasks including manipulation and navigation. Below we review the recent literature mainly considering the visual reality gap.</p>
<p>Bousmalis et al. [6] bridged the reality gap for manipulation by adapting synthetic images to the realistic domain during training, with a combination of image-level and feature-level adaptation. Also following the sim-to-real direction, Stein et al. [7] utilized CycleGAN to translate every synthetic frame to the realistic style during training navigation policies. Although effective, these approaches still add an adaptation step before each training iteration, which can slow down the whole learning pipeline.</p>
<p>The method of domain randomization [8], [9], [14] is proposed to randomize the texture of objects, lighting conditions, and camera positions during training, such that the learned model could generalize naturally to real-world scenarios. However, such randomizing might not be efficiently realized by some robotic simulators at a relatively low cost. Moreover, there is no guarantee that these randomized simulations can cover the visual modality of an arbitrary real-world scene.</p>
<p>Rusu et al. [10] deals with the reality gap by progressively adapting the features and representations learned in simulation to that of the realistic domain. This method, however, still needs to go through a policy finetuning phase for each visually different real-world scenario.</p>
<p>Apart from the approaches mentioned above, some works chose special setups to circumvent the reality gap. For example, $2 D$ Lidar [15]-[17] and depth images [18], [19] are sometimes chosen as the sensor modality, since the discrepancies between the simulated domain and the real-world domain for them can be smaller than those for colour images. Zhu et al. [20] conducted real-world experiments with visual inputs. However, in their setups, the real-world scene is highly visually similar to the simulation, a condition that can be relatively difficult to meet in practice.</p>
<p>Very related to our method is the work of Inoue et al. which also adopts a real-to-sim direction [21]. They train VAEs to perform the adaptation during deployment of the trained object detection model in the real world. However, their method relies on paired data between two domains and focuses on supervised perception tasks.</p>
<p>In this paper, we mainly consider domain adaptation for learning-based visual navigation. In terms of visual aspects, the adaptation for navigation is quite challenging, since navigation agents usually work in environments at relatively larger scales compared to the relatively confined workspaces for manipulators. We believe our proposed real-to-sim method could be potentially adopted in other control domains.</p>
<p>An essential aspect of domain adaptation, within the context of dealing with the reality gap is the consistency between subsequent frames, which has not been considered in any of the adaptation methods mentioned above. As an approach for solving sequential decision making, the consistency between the subsequent inputs for DRL agents can be critical for the successful fulfilment of their final goals. Apart from solutions for solving the reality gap, the general domain adaptation literature also lacks works considering sequential frames instead of single frames. Therefore, we look to borrow techniques from other fields that successfully extend single-frame algorithms to the video domain, among which the most applicable methods are from the artistic style transfer literature.</p>
<h2>C. Artistic Style Transfer for Videos</h2>
<p>Artistic style transfer is a technique for transferring the artistic style of artworks to photographs [22]. Artistic style transfer for videos works on video sequences instead of individual frames, targeting generating temporally consistent stylizations for sequential inputs. Ruder et al. [23] provides a key observation that: a trained stylization network with a total downsampling factor of $K$ (e.g., $K=4$ for a network with 2 convolutional layers of stride 2), is shift invariant to shifts equal to the multiples of $K$ pixels, but can output substantially different stylizations otherwise. This undesired property (of not being shift invariant) causes the output of the trained network to change substantially for even very tiny changes in the input, which leads to temporal inconsistency (under the assumption that only relatively limited changes would appear in subsequent input frames). However, their solution of adding temporal constraints between generated subsequent frames, is rather expensive, as it requires optical flow as input during deployment. Huang et al. [24] offers a relatively cheap solution, requiring the temporal constraint only during training single-frame artistic style transfer. However, we suspect that constraining optical flow on single frames is not well-defined. We suspect that their improved temporal consistency is actually due to the inexplicitly imposed consistency constraints for regional shifts by optical flow. We validate this suspicion in our experiments (Sec. IV-A).</p>
<p>We propose that the fundamental problem causing the inconsistency can be solved by an additional constraint of shift loss, which we introduce in Sec. III-D. We show that the shift loss constrains the consistency between generated subsequent</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: The VR-Goggles pipeline. We depict the computation of the losses $\mathcal{L}<em _mathcal_S="\mathcal{S">{\text{GAN}</em>}}}$, $\mathcal{L<em _mathcal_R="\mathcal{R">{\text{cyc}</em>}}}$, $\mathcal{L<em _mathcal_R="\mathcal{R">{\text{sem}</em>}}}$ and $\mathcal{L<em _mathcal_S="\mathcal{S">{\text{shift}</em>}}}$. We present both outdoor and indoor scenarios, where the adaptation for the outdoor scene is trained with the semantic loss $\mathcal{L<em _mathcal_S="\mathcal{S">{\text{sem}}$ (since its simulated domain CARLA has ground truth semantic labels to train a segmentation network $f</em>}}$), and the indoor one without (since its simulated domain Gazebo does not provide semantic ground truth). The components marked in red are those involved in the final deployment: a real sensor reading is captured ($r \sim p_{\text{real}}$), then passed through the generator $G_{\mathcal{S}}$ to be translated into the simulated domain $\mathcal{S}$, where the DRL agents were originally trained; the translated image $\hat{s}$ is then fed to the DRL policy, which outputs control commands. For clarity, we skip the counterpart losses $\mathcal{L<em _mathcal_R="\mathcal{R">{\text{GAN}</em>}}}$, $\mathcal{L<em _mathcal_S="\mathcal{S">{\text{cyc}</em>}}}$, $\mathcal{L<em _mathcal_S="\mathcal{S">{\text{sem}</em>}}}$ and $\mathcal{L<em _mathcal_R="\mathcal{R">{\text{shift}</em>$.}}</p>
<p>frames, without the need for the relatively expensive optical flow constraint. We argue that for a network that has been properly trained to learn a smooth function approximation, small changes in the input should also result in small changes in the output.</p>
<h2>III. METHODS</h2>
<h3>A. Problem formulation</h3>
<p>We consider visual data sources from two domains: $\mathcal{S}$, containing sequential frames ${s_0, s_1, s_2, \ldots}$ (e.g., synthetic images output from a simulator; $s \sim p_{\text{sim}}$, where $p_{\text{sim}}$ denotes the simulated data distribution), and $\mathcal{R}$, containing sequential frames ${r_0, r_1, r_2, \ldots}$ (e.g., real camera readings from the onboard camera of a mobile robot; $r \sim p_{\text{real}}$, where $p_{\text{real}}$ denotes the distribution of the real sensory readings). We emphasize that, although we require our method to generate consistent outputs for sequential inputs, we do not need the training data to be sequential; we formalize it in this way only because some of our baseline methods have this requirement.</p>
<p>DRL agents are typically trained in the simulated domain $\mathcal{S}$, and expected to execute in the real-world domain $\mathcal{R}$. As we have discussed, we choose to tackle this problem by translating the images from $\mathcal{R}$ to $\mathcal{S}$ during deployment. In the following, we introduce our approach for performing domain adaptation. Also to cope with the sequential nature of the incoming data streams, we introduce a shift loss technique for constraining the consistency of the translated subsequent frames.</p>
<h3>B. CycleGAN Loss</h3>
<p>We first build on top of CycleGAN [11], which learns two generative models to map between domains: $G_{\mathcal{R}}: \mathcal{S} \to \mathcal{R}$, with its discriminator $D_{\mathcal{R}}$, and $G_{\mathcal{S}}: \mathcal{R} \to \mathcal{S}$, with its discriminator $D_{\mathcal{S}}$, via training two GANs simultaneously:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _mathcal_R="\mathcal{R">{\text{GAN}</em>}}}(G_{\mathcal{R}}, D_{\mathcal{R}}; \mathcal{S}, \mathcal{R})= &amp; \mathbb{E<em _text_real="\text{real">{p</em>(r)] + \
&amp; \mathbb{E}}}} [\log D_{\mathcal{R}<em _text_sim="\text{sim">{p</em>(s)))] \
\mathcal{L}}}} [\log(1 - D_{\mathcal{R}}(G_{\mathcal{R}<em _mathcal_S="\mathcal{S">{\text{GAN}</em>}}}(G_{\mathcal{S}}, D_{\mathcal{S}}; \mathcal{R}, \mathcal{S})= &amp; \mathbb{E<em _text_sim="\text{sim">{p</em>(s)] + \
&amp; \mathbb{E}}}} [\log D_{\mathcal{S}<em _text_real="\text{real">{p</em>(r)))],
\end{aligned}
$$}}} [\log(1 - D_{\mathcal{S}}(G_{\mathcal{S}</p>
<p>in which $G_{\mathcal{R}}$ learns to generate images $G_{\mathcal{R}}(s)$ matching those from domain $\mathcal{R}$, while $G_{\mathcal{S}}$ translates $r$ to domain $\mathcal{S}$. We also constrain mappings with the cycle consistency loss [11]:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _mathcal_R="\mathcal{R">{\text{cyc}</em>}}}(G_{\mathcal{S}}, G_{\mathcal{R}}; \mathcal{R}) &amp;= \mathbb{E<em _text_real="\text{real">{p</em>(r)) - r\right|}}} [\left|G_{\mathcal{R}}(G_{\mathcal{S}<em _text_cyc="\text{cyc">{1}] \
\mathcal{L}</em><em _mathcal_R="\mathcal{R">{\mathcal{S}}}(G</em>}}, G_{\mathcal{S}}; \mathcal{S}) &amp;= \mathbb{E<em _text_sim="\text{sim">{p</em>]
\end{aligned}
$$}}} [\left|G_{\mathcal{S}}(G_{\mathcal{R}}(s)) - s\right|_{1</p>
<h3>C. Semantic Loss</h3>
<p>Since our translation domains of interest are between synthetic images and real-world sensor images, we take advantage of the fact that many recent robotic simulators provide ground truth semantic labels and add a semantic constraint inspired by CyCADA [12]. (For simplicity in the following we use CyCADA to refer to CycleGAN plus this semantic loss instead of the full CyCADA approach [12]).</p>
<p>Assuming that for images from domain $\mathcal{S}$, the ground truth semantic labels $\mathbf{Y}$ are available, a semantic segmentation network $f_{\mathcal{S}}$ can be obtained by minimizing the cross-entropy loss $\mathbb{E}<em _mathcal_S="\mathcal{S">{s \sim \mathcal{S}}[\text{CrossEnt}(\mathbf{Y}_s, f</em>$. Then semantically consistent image translation can be achieved by minimizing the following losses, which imposes consistency between the semantic maps of the input and that of the generated output:}}(s))]$. We further assume that the ground truth semantic for domain $\mathcal{R}$ is lacking (which is the case for most real scenarios), meaning that $f_{\mathcal{R}}$ is not easily obtainable. In this case, we use $f_{\mathcal{S}}$ to generate "semi" semantic labels for domain $\mathcal{R</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _mathcal_R="\mathcal{R">{\text{sem}</em>}}}(G_{\mathcal{S}}; \mathcal{R}, f_{\mathcal{S}}) &amp;= \mathbb{E<em _text_real="\text{real">{p</em>(r)))] \
\mathcal{L}}}} [\text{CrossEnt}(f_{\mathcal{S}}(r), f_{\mathcal{S}}(G_{\mathcal{S}<em _mathcal_S="\mathcal{S">{\text{sem}</em>}}}(G_{\mathcal{R}}; \mathcal{S}, f_{\mathcal{S}}) &amp;= \mathbb{E<em _text_sim="\text{sim">{p</em>(s)))]
\end{aligned}
$$}}} [\text{CrossEnt}(f_{\mathcal{S}}(s), f_{\mathcal{S}}(G_{\mathcal{R}</p>
<h3>D. Shift Loss for Consistent Generation</h3>
<p>Different from the current literature of domain adaptation, our model is additionally expected to output consistent images for sequential inputs. Although with $\mathcal{L}_{\text{sem}}$, the semantics of the consecutive outputs are constrained, inconsistencies and artifacts still occur quite often. Moreover, in cases where ground truth semantics are unavailable from either domain, the sequential outputs are even less constrained, which could potentially lead to inconsistent policy outputs. Following the</p>
<p>discussions in Sec. II-C, we introduce the shift loss to constrain the consistency even in these situations.</p>
<p>For an input image $s$, we use $s_{[x \rightarrow i, y \rightarrow j]}$ to denote the result of a shift operation: shifting $s$ along the $X$ axis by $i$ pixels, and $j$ pixels along the $Y$ axis. We sometimes omit $y \rightarrow 0$ or $x \rightarrow 0$ in the subscript if the image is only shifted along the $X$ or $Y$ axis. According to [23], a trained stylization network is shift invariant to shifts of multiples of $K$ pixels ( $K$ represents the total downsampling factor of the network), but can output significantly different stylizations otherwise. This causes the output of the trained network to change greatly for even very small changes in the input. We thus propose to add a simple yet direct and effective shift loss ( $u$ denotes uniform distribution):</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _mathcal_R="\mathcal{R">{\text {shift } \mathcal{R}}\left(G</em>}} ; \mathcal{S}\right) &amp; =\mathbb{E<em _sim="{sim" _text="\text">{p</em> \
&amp; \left[\left|G_{\mathcal{R}}(s)}}}, i, j \sim u(1, K-1)<em _mathcal_R="\mathcal{R">{[x \rightarrow i, y \rightarrow j]}-G</em>\right)\right|}}\left(s_{[x \rightarrow i, y \rightarrow j]<em _shift="{shift" _text="\text">{2}^{2}\right] \
\mathcal{L}</em><em _mathcal_S="\mathcal{S">{\mathcal{S}}}\left(G</em>}} ; \mathcal{R}\right) &amp; =\mathbb{E<em _stal="{stal" _text="\text">{p</em> \
&amp; \left[\left|G_{\mathcal{S}}(r)}}}, i, j \sim u(1, K-1)<em _mathcal_S="\mathcal{S">{[x \rightarrow i, y \rightarrow j]}-G</em>\right]
\end{aligned}
$$}}\left(r_{[x \rightarrow i, y \rightarrow j]}\right)\right|_{2}^{2</p>
<p>Shift loss constrains the shifted output to match the output of the shifted input, regarding the shifts as image-scale movements. Assuming that only limited regional movement would appear in subsequent input frames, shift loss effectively smoothes the mapping function for small regional movements, restricting the changes in its outputs for subsequent inputs. This can be regarded as a cheap alternative for imposing consistency constraints on small movements, eliminating the need for the optical flow information, which is crucial for meeting the requirements of real-time robotics control.</p>
<h2>E. Full Objective</h2>
<p>Our full objective for learning VR-Goggles (Fig. 1) is ( $\lambda_{\text {cyc }}$, $\lambda_{\text {sem }}$ and $\lambda_{\text {shift }}$ are the loss weightings):</p>
<p>$$
\begin{aligned}
&amp; \mathcal{L}\left(G_{\mathcal{R}}, G_{\mathcal{S}}, D_{\mathcal{R}}, D_{\mathcal{S}} ; \mathcal{S}, \mathcal{R}, f_{\mathcal{S}}\right) \
&amp; \quad=\mathcal{L}<em _mathcal_R="\mathcal{R">{\mathrm{GAN}</em>}}}\left(G_{\mathcal{R}}, D_{\mathcal{R}} ; \mathcal{S}, \mathcal{R}\right)+\mathcal{L<em _mathcal_S="\mathcal{S">{\mathrm{GAN}</em>\right) \
&amp; \quad+\lambda_{\mathrm{cyc}}\left(\mathcal{L}}}}\left(G_{\mathcal{S}}, D_{\mathcal{S}} ; \mathcal{R}, \mathcal{S<em _mathcal_R="\mathcal{R">{\mathrm{cyc}</em>}}}\left(G_{\mathcal{S}}, G_{\mathcal{R}} ; \mathcal{R}\right)+\mathcal{L<em _mathcal_S="\mathcal{S">{\mathrm{cyc}</em>\right)\right) \
&amp; \quad+\lambda_{\mathrm{sem}}\left(\mathcal{L}}}}\left(G_{\mathcal{R}}, G_{\mathcal{S}} ; \mathcal{S<em _mathcal_R="\mathcal{R">{\mathrm{sem}</em>}}}\left(G_{\mathcal{S}} ; \mathcal{R}, f_{\mathcal{R}}\right)+\mathcal{L<em _mathcal_S="\mathcal{S">{\mathrm{sem}</em>\right)\right) \
&amp; \quad+\lambda_{\text {shift }}\left(\mathcal{L}}}}\left(G_{\mathcal{R}} ; \mathcal{S}, f_{\mathcal{S}<em _mathcal_R="\mathcal{R">{\text {shift }</em>}}}\left(G_{\mathcal{R}} ; \mathcal{S}\right)+\mathcal{L<em _mathcal_S="\mathcal{S">{\text {shift }</em>\right)\right)
\end{aligned}
$$}}}\left(G_{\mathcal{S}} ; \mathcal{R</p>
<p>This corresponds to solving the following optimization:</p>
<p>$$
G_{\mathcal{R}}^{<em>}, G_{\mathcal{S}}^{</em>}=\arg \min <em _mathcal_R="\mathcal{R">{G</em> \max }}, G_{\mathcal{S}}<em _mathcal_R="\mathcal{R">{D</em>\right)
$$}}, D_{\mathcal{S}}} \mathcal{L}\left(G_{\mathcal{R}}, G_{\mathcal{S}}, D_{\mathcal{R}}, D_{\mathcal{S}</p>
<h2>IV. EXPERIMENTS</h2>
<h2>A. Validating Shift Loss: Artistic Style Transfer for Videos</h2>
<p>To evaluate our method, we firstly conduct experiments for artistic style transfer for videos, to validate the effectiveness of shift loss on constraining consistency for sequential frames. We collect a training dataset of 98 HD video footage sequences (from VIDEVO ${ }^{1}$ containing 2450 frames in total); the Sintel [25] sequences are used for testing, as their groundtruth optical flow is available. We compare the performance of the models trained under the following setups: (1) $\boldsymbol{F F}$</p>
<p>[22]: Canonical feed forward style transfer trained on single frames; (2) $\boldsymbol{F F + f l o w}$ [24]: FF trained on sequential images, with optical flow added for imposing temporal constraints on subsequent frames; (3) Ours: FF trained on single frames, with an additional shift loss as discussed in Sec. III-D.</p>
<p>As a proof of concept, we begin our evaluation by comparing the three setups on their ability to generate shift invariant stylizations for shifted single frames. In particular, for each image $s$ in the testing dataset, we generate 4 more test images by shifting the original image along the $X$ axis by $1,2,3,4$ pixels respectively, and pass all 5 frames $\left(s, s_{[x \rightarrow 1]}, s_{[x \rightarrow 2]}\right.$, $\left.s_{[x \rightarrow 3]}, s_{[x \rightarrow 4]}\right)$ through the trained network to examine the consistency of the generated images. The results shown in Fig. 2 validate the discussion from [23], since the stylizations for $s$ and $s_{[x \rightarrow 4]}$ from $F F$ are almost identical ( $K=4$ for the trained network), but differ substantially otherwise. FF-flow improves the invariance by a limited amount; Ours is capable of generating consistent stylizations for shifted inputs, with the shift loss directly reducing the shift variance.</p>
<p>We then evaluate the consistency of stylized sequential frames, computing the temporal loss [24] using the ground truth optical flow for the Sintel sequences (Table I). Although the temporal loss is part of the optimization objective of $F F$ flow, and our method does not have access to any optical flow information, Ours is still able to achieve lower temporal loss with the shift loss constraint.</p>
<p>We further visualize the consistency comparison in Fig. 3, where we show the temporal error maps, the same metric as in [24], of two stylized consecutive frames for each method. The error increases linearly as shown from black to white in grayscale. Ours (bottom row) achieves the highest temporal consistency. Further details about style transfer training and the calculation of temporal error map are available in the supplement file [26].</p>
<h2>B. Quantitative Evaluation: Carla Benchmark</h2>
<p>Secondly, we conduct a quantitative evaluation of our proposed real-to-sim policy transfer pipeline. Since there are no publicly available common benchmarks for real-world autonomous driving evaluation, we test our pipeline in the Carla simulator following its benchmark setup [27], [28]. We choose the imitation learning pipeline because the reinforcement learning policy in [27] performs substantially worse. In [27], the expert datasets for Carla benchmark are collected under 4 different weather conditions (daytime, daytime after rain, daytime hard rain and clear sunset), and the policy is tested on benchmark tasks under cloudy daytime and soft rain at sunset. Since the datasets under the testing benchmark conditions are not available ${ }^{2}$ for us to conduct domain adaptation, we split the provided training datasets into three training conditions (daytime, daytime after rain, clear sunset) and one testing condition (daytime hard rain) as shown in Fig. 4.</p>
<p>We present comparisons for both phases in the policy transfer pipeline: policy training and domain adaptation.</p>
<p>For the policy training phase, we adopt the following training regimes: (1) Single-Domain: We train one policy</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Shift-invariance evaluation, comparing between <em>FF</em>, <em>FF+flow</em> and <em>Ours</em>. We shift an input image <em>s</em> along the <em>X</em> axis by 1, 2, 3, 4 pixels respectively and feed all 5 frames through the networks trained via <em>FF</em>, <em>FF+flow</em> and <em>Ours</em> and show the generated stylizations. We mark the most visible differences with small circles and dim the rest of the generated images. As is discussed in [23], <em>FF</em> generates almost identical stylizations for <em>s</em> and <em>s</em>[<em>x→4</em>] (because 4 is a multiple of the total downsampling factor of the trained network), but those for <em>s</em>[<em>x→1</em>], <em>s</em>[<em>x→2</em>], <em>s</em>[<em>x→3</em>] differ significantly. <em>FF+flow</em> improves the shift-invariance, but we suspect the improvement is due to the inexplicit consistency constraint on regional shifts imposed by optical flow. <em>Ours</em> is able to generate shift-invariant stylizations with the proposed <em>shift loss</em>.</p>
<table>
<thead>
<tr>
<th></th>
<th>FF</th>
<th>FF+flow</th>
<th>Ours</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td><em>mosaic</em></td>
<td></td>
</tr>
<tr>
<td>ambush5</td>
<td>0.152</td>
<td>0.130</td>
<td>0.127</td>
</tr>
<tr>
<td>bamboo1</td>
<td>0.119</td>
<td>0.093</td>
<td>0.086</td>
</tr>
<tr>
<td>market6</td>
<td>0.132</td>
<td>0.110</td>
<td>0.108</td>
</tr>
<tr>
<td>temple2</td>
<td>0.127</td>
<td>0.104</td>
<td>0.098</td>
</tr>
<tr>
<td>sleeping2</td>
<td>0.115</td>
<td>0.089</td>
<td>0.083</td>
</tr>
<tr>
<td>shaman3</td>
<td>0.124</td>
<td>0.095</td>
<td>0.087</td>
</tr>
<tr>
<td>alley2</td>
<td>0.122</td>
<td>0.096</td>
<td>0.090</td>
</tr>
<tr>
<td>bamboo2</td>
<td>0.113</td>
<td>0.091</td>
<td>0.089</td>
</tr>
<tr>
<td>alley1</td>
<td>0.113</td>
<td>0.085</td>
<td>0.078</td>
</tr>
<tr>
<td>sleeping1</td>
<td>0.125</td>
<td>0.099</td>
<td>0.092</td>
</tr>
<tr>
<td></td>
<td></td>
<td><em>lamuse</em></td>
<td></td>
</tr>
<tr>
<td>ambush5</td>
<td>0.154</td>
<td>0.131</td>
<td>0.130</td>
</tr>
<tr>
<td>bamboo1</td>
<td>0.123</td>
<td>0.097</td>
<td>0.090</td>
</tr>
<tr>
<td>market6</td>
<td>0.135</td>
<td>0.112</td>
<td>0.112</td>
</tr>
<tr>
<td>temple2</td>
<td>0.138</td>
<td>0.108</td>
<td>0.107</td>
</tr>
<tr>
<td>sleeping2</td>
<td>0.121</td>
<td>0.100</td>
<td>0.092</td>
</tr>
<tr>
<td>shaman3</td>
<td>0.132</td>
<td>0.106</td>
<td>0.094</td>
</tr>
<tr>
<td>alley2</td>
<td>0.129</td>
<td>0.104</td>
<td>0.094</td>
</tr>
<tr>
<td>bamboo2</td>
<td>0.114</td>
<td>0.090</td>
<td>0.091</td>
</tr>
<tr>
<td>alley1</td>
<td>0.127</td>
<td>0.096</td>
<td>0.083</td>
</tr>
<tr>
<td>sleeping1</td>
<td>0.132</td>
<td>0.102</td>
<td>0.101</td>
</tr>
</tbody>
</table>
<p>Table I: Comparing temporal loss between <em>FF</em>, <em>FF+flow</em> and <em>Ours</em>. <em>FF+flow</em> directly optimizes on this metric, while optical flow is never provided to <em>Ours</em>; yet <em>Ours</em> achieves lower temporal loss on the evaluated <em>Sintel</em> sequences.</p>
<p>under each of the three training weather conditions; (2) <em>Multi-Domain</em>: A policy is trained under a combined dataset containing all three training weather conditions. We note that since the imitation policy is trained with datasets instead of interacting with the simulation environment, the full approach of <em>Domain Randomization</em> [8], [9] could not be directly applied, as it requires to randomize the textures of each object, lighting conditions and viewing angles of the rendered scenes. Thus the <em>Multi-Domain</em> can be considered as a relatively limited realization of the <em>Domain Randomization</em> approach in the <em>Carla</em> benchmark dataset setup. As for the progressive nets approach [10], it requires a finetuning phase of the policy in the real world, which for autonomous driving means that we need to deploy the trained policy onto a real car and finetune it through rather expensive real-world interactions. Thus we do not consider this approach in this evaluation. (An additional comparison experiment with the progressive nets can be found in the supplementary materials [26].)</p>
<p>For the <em>domain adaptation</em> phase, we compare the following adaptation methods: (1) <em>No-Goggles</em>: Feed the testing data directly to the trained policy; (2) <em>CycleGAN</em> [11]: Use <em>CycleGAN</em> to translate the test data to the training domain before feeding to policy nets and (3) <em>Ours</em>: Add <em>shift loss</em> on top of (2) as <em>VR-Goggles</em> to translate the inputs. For both <em>CycleGAN</em> and <em>VR-Goggles</em>, we train an adaptation network from the testing weather condition to each of the three training conditions. (For more details about the training of the policy and adaptation models, please refer to the supplementary materials [26].)</p>
<p>The four benchmark tasks (<em>Straight</em>, <em>One Turn</em>, <em>Navigation</em> and <em>Nav. dynamic</em>) are in order of increasing difficulty and each of them consists of 25 different preset trajectories. Since the <em>Multi-Domain</em> policy is trained with three weather conditions instead of four as in the original setup [27] due to the reason discussed earlier, directly deploying the <em>Multi-Domain</em> policy fail to finish any of the two harder tasks under the relatively extreme testing weather condition. For the different adaptation strategies, our <em>VR-Goggles</em> outperforms <em>CycleGAN</em> on almost all of the metrics, especially the two harder tasks (<em>Navigation</em> and <em>Nav. dynamic</em>) in terms of both the success rate and the average percentage of distance to goal traveled. The average distance traveled between two infractions is reported only for the hardest task [27]: navigating in the presence of dynamic objects (<em>Nav. dynamic</em>). The adaptation models of <em>Ours</em> enable the agents to drive safely with mostly lower infraction frequencies compared with <em>CycleGAN</em>. <em>CycleGAN</em> collides with pedestrians less often with the <em>Multi-Domain</em>.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Temporal error maps between generated stylizations for subsequent input frames. The error increases linearly as shown from black to white in grayscale. 1st row: input frames; 2nd ~ 4th row: temporal error maps (with the corresponding stylizations shown on top) of outputs from FF, FF+flow, and Ours. We here choose a very challenging style (mosaic) for temporal consistency, as it contains many fine details, with tiny tiles laid over the original image in the final stylizations. Yet, Ours achieves very high consistency.</p>
<p>policy. A probable explanation is that most episodes under this setup end due to collision with cars and static obstacles, so there does not occur too many challenging pedestrian conditions. For example, for direct deployment without adaptation (No-Goggles), the average distance between collisions with pedestrians is higher than 4.6 km, because the total navigation distance for all 25 episodes in this task is only 4.6 km which is too short to encounter pedestrians.</p>
<p>We note that the transfer pipelines of Single-Domain policies behave much better than directly deploying the Multi-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Carla weather conditions used in benchmarking: three training conditions (a), (b), (c) and one testing condition (d).</p>
<p>Domain policy, and the training time of the former policy is also much shorter than that of the latter [26].</p>
<h3>C. Real-world Indoor &amp; Outdoor Navigation</h3>
<p>Finally, we conduct real-world robotics experiments for both indoor and outdoor visual navigation tasks. We begin by training learning-based visual navigation policies, taking simulated first-person-view images as inputs, outputting moving commands for specific navigation targets. Then, we deploy the trained policy onto real robots, comparing the following domain adaptation approaches: (1) No-Goggles: Feed the sensor readings directly to the trained policy; (2) CycleGAN/CyCADA [11], [12]: Use CycleGAN (when semantic ground truth is not available) / CyCADA (when ground truth semantic maps are provided by the simulator) to translate the real sensory inputs to the synthetic domain before feeding to the policy nets; (3) Ours: Add shift loss on top of (2) as the VR-Goggles.</p>
<p>For indoor office experiments, we build an office environment in Gazebo [29] and render s ~ p_{sim} from this simulation environment (Fig. 6a). We capture r ~ p_{real} from a real office (Fig. 6b) using a RealSense R200 camera mounted on a Turtlebot3 Waffle. For conducting the domain adaptation,</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>Training</th>
<th></th>
<th>Testing</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Single-Domain</td>
<td>Multi-Domain</td>
<td>Single-Domain</td>
<td></td>
<td></td>
<td>Multi-Domain</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>No-Gog.</td>
<td>CycleGAN</td>
<td>VR-Gog.</td>
<td>No-Gog.</td>
<td>CycleGAN</td>
<td>VR-Gog.</td>
</tr>
<tr>
<td></td>
<td>Straight</td>
<td>81.3</td>
<td>97.3</td>
<td>13.3</td>
<td>93.3</td>
<td>90.7</td>
<td>64.0</td>
<td>96.0</td>
<td>100</td>
</tr>
<tr>
<td>Success rate</td>
<td>One turn</td>
<td>64.0</td>
<td>85.3</td>
<td>1.3</td>
<td>54.7</td>
<td>54.7</td>
<td>36.0</td>
<td>61.3</td>
<td>76.0</td>
</tr>
<tr>
<td>(%)</td>
<td>Navigation</td>
<td>60.0</td>
<td>84.0</td>
<td>0.0</td>
<td>21.3</td>
<td>45.3</td>
<td>0.0</td>
<td>42.7</td>
<td>61.3</td>
</tr>
<tr>
<td></td>
<td>Nav. dynamic</td>
<td>58.7</td>
<td>74.7</td>
<td>0.0</td>
<td>21.3</td>
<td>32.0</td>
<td>0.0</td>
<td>34.7</td>
<td>56.0</td>
</tr>
<tr>
<td>Ave. distance</td>
<td>Straight</td>
<td>89.7</td>
<td>96.5</td>
<td>37.8</td>
<td>95.8</td>
<td>94.7</td>
<td>83.6</td>
<td>95.9</td>
<td>98.3</td>
</tr>
<tr>
<td>to goal</td>
<td>One turn</td>
<td>73.6</td>
<td>71.1</td>
<td>18.4</td>
<td>36.3</td>
<td>48.4</td>
<td>24.7</td>
<td>52.0</td>
<td>71.3</td>
</tr>
<tr>
<td>travelled</td>
<td>Navigation</td>
<td>68.6</td>
<td>88.8</td>
<td>7.3</td>
<td>36.7</td>
<td>51.0</td>
<td>7.4</td>
<td>60.8</td>
<td>73.1</td>
</tr>
<tr>
<td>(%)</td>
<td>Nav. dynamic</td>
<td>68.2</td>
<td>80.7</td>
<td>5.0</td>
<td>32.6</td>
<td>51.3</td>
<td>5.2</td>
<td>55.7</td>
<td>72.2</td>
</tr>
<tr>
<td>Ave. distance</td>
<td>Opposite lane</td>
<td>2.83</td>
<td>2.55</td>
<td>0.23</td>
<td>0.77</td>
<td>0.72</td>
<td>0.26</td>
<td>0.83</td>
<td>2.22</td>
</tr>
<tr>
<td>travelled between</td>
<td>Sidewalk</td>
<td>6.47</td>
<td>9.70</td>
<td>0.21</td>
<td>1.15</td>
<td>2.62</td>
<td>0.38</td>
<td>1.29</td>
<td>2.46</td>
</tr>
<tr>
<td>two infractions</td>
<td>Collision-static</td>
<td>2.38</td>
<td>3.03</td>
<td>0.14</td>
<td>0.52</td>
<td>0.87</td>
<td>0.16</td>
<td>0.77</td>
<td>1.26</td>
</tr>
<tr>
<td>in Nav. dynamic</td>
<td>Collision-car</td>
<td>2.06</td>
<td>1.03</td>
<td>0.29</td>
<td>1.01</td>
<td>1.40</td>
<td>0.27</td>
<td>0.59</td>
<td>0.77</td>
</tr>
<tr>
<td>(km)</td>
<td>Collision-pedestrian</td>
<td>15.10</td>
<td>16.17</td>
<td>2.17</td>
<td>4.03</td>
<td>6.98</td>
<td>&gt;4.60</td>
<td>7.29</td>
<td>4.67</td>
</tr>
</tbody>
</table>
<p>TABLE II: Quantitative evaluation of goal-directed Carla navigation benchmark tasks [27]. We train imitation policies under single weather condition (Single-Domain) and three training weather conditions (Multi-Domain). Policies are evaluated in testing weather condition through direct deploying (No-Goggles), translating the input image through CycleGAN and through VR-Goggles (for transferring the Multi-Domain policy with CycleGAN and VR-Goggles we train one adaptation network from the testing weather condition to each of the three training weather conditions and report the average results under those three adaptations). Higher is better.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: Real-world visual control experiments. <em>Indoor</em> (yellow):. A navigation policy is firstly trained in a simulated environment (Fig. 6a) that is able to navigate to chairs based on visual inputs. Without retraining or finetuning, our proposed <em>VR-Goggles</em> enables the mobile robot to directly deploy this policy in a real office environment (Fig. 6b), achieving 100% success rate in a set of real-world experiments. Here <em>Miss</em> refers to test runs where the agent stays put or rotate in place and simply ignores the chair even when they are in sight as the policy trained in the simulation could not cope with the drastically visually different inputs (<em>No-Goggles</em>), or due to the inconsistency of the translated subsequent outputs which hinders the successful fulfilment of the goal-reaching task (<em>CycleGAN</em>). <em>Hit</em> refers to frames where the agent captures the chair in sight and outputs commands to move towards it. <em>Outdoor</em> (cyan): An autonomous driving policy (via conditional imitation learning [28]) is trained in <em>Carla</em> daytime (Fig. 6c), a <em>VR-Goggles</em> model is trained to translate between <em>Carla</em> daytime and <em>Robotcar</em> nighttime (Fig. 6d), which enables the real-world nighttime deployment of the trained policy.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: Samples from the simulated environment (left) and the real world (right) used in our indoor (top) and outdoor (bottom) navigation experiments.</p>
<p>as the simulator (<em>Gazebo</em>) does not provide ground truth semantics, we drop the semantic constraint $\mathcal{L}_{\text{sem}}$. The input images are of size 640 × 360 and the adaptation network is trained with 256 × 256 crops. We use the same network architecture as in <em>CycleGAN</em>, and train for 50 epochs with a learning rate of 2e − 4 as we observe no performance gain training for longer iterations.</p>
<p>We train the navigation policy using Canonical A3C with 8 parallel workers [2] in <em>Gazebo</em>, and deploy the trained policy onto <em>Turtlebot3 Waffle</em> and compare the three <em>domain adaptation</em> approaches (Fig. 5). Without <em>domain adaptation</em>, <em>No-Goggles</em> fails completely in the real-world tasks; our proposed <em>VR-Goggles</em> achieves the highest success rate (0%, 60% and 100% for <em>No-Goggles</em>, <em>CycleGAN</em> and <em>Ours</em> respectively) due to the quality and consistency of the translated streams. The control cycle runs in real-time at 13Hz on a <em>Nvidia TX2</em>.</p>
<p>Finally, we conduct <em>outdoor</em> autonomous driving experiments (we sample s ∼ <em>p</em><sub>sim</sub> from the <em>Carla</em> daytime [27] environment Fig. 6c and sample r ∼ <em>p</em><sub>real</sub> from a nighttime dataset of <em>Robotcar</em> [30] Fig. 6d) with input images of size 640 × 400. Considering that <em>VR-Goggles</em> outperforms <em>CycleGAN</em> in <em>indoor</em> experiments, and since outdoor robotics experiments are relatively expensive, we only compare <em>No-Goggles</em> and <em>VR-Goggles</em> in the <em>outdoor</em> autonomous driving scenario. We take the driving policy trained through conditional imitation learning [28] as in Section IV-B. This policy takes as inputs the first person view RGB image and a high-level command, which falls in a discrete action space and is generated through a global planner (<em>straight, left, right, follow, none</em>). In our real-world experiments, this high-level direction command is set as <em>straight</em>, indicating the vehicle (a <em>Bulldog</em> with a <em>PointGrey Blackfly</em> camera mounted on it) to always go along the road. The control policy outputs the steering angle.</p>
<p>The control policy is trained purely in <em>Carla</em> simulated daytime, while it is tested in a nighttime town street scene (Fig. 5). It is non-trivial to quantitatively evaluate the control policy in the real world, so we show two representative sequences marked with the output steering commands. The top row of each sequence shows the continuous outputs of <em>No-Goggles</em>. Due to the huge difference between the real nighttime and the simulated daytime, the vehicle failed to move along the road. Our <em>VR-Goggles</em>, however, successfully guides the vehicle along the road as instructed by the global planner (the policy prefers to turn right since it is trained in a right-driving environment) <sup>3</sup>.</p>
<p><sup>3</sup>A video demonstrating our approach and much more experimental results are available at https://sites.google.com/view/zhang-tai-19ral-vrg/home, where we also show that the <em>VR-Goggles</em> can easily train a new model for a new type of chair without finetuning the indoor control policy.</p>
<h2>V. CONCLUSIONS</h2>
<p>In this paper, we tackle the reality gap occurring when deploying learning-based visual control policies trained in simulation to the real world, by translating the real images back to the synthetic domain during deployment. Due to the sequential nature of the incoming sensor streams for control tasks, we propose shift loss to increase the consistency of the translated subsequent frames, and validate it both in artistic style transfer for videos and domain adaptation. We verify our proposed VR-Goggles pipeline as a lightweight, flexible and efficient solution for visual control through Carla benchmark as well as a set of real-world robotics experiments. It would be interesting to apply our method to manipulation, as this paper has been mainly focused on navigation. Also, evaluating our method in more challenging environments on more sophisticated control tasks could be another future direction.</p>
<h2>ACKNOWLEDGMENT</h2>
<p>The authors would like to thank Christian Dornhege and Daniel Büscher for the discussion of the initial idea.</p>
<h2>REFERENCES</h2>
<p>[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., "Human-level control through deep reinforcement learning," Nature, vol. 518, no. 7540, p. 529, 2015.
[2] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, "Asynchronous methods for deep reinforcement learning," in Proceedings of The 33rd International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, M. F. Balcan and K. Q. Weinberger, Eds., vol. 48. New York, New York, USA: PMLR, 20-22 Jun 2016, pp. 1928-1937.
[3] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, "Continuous control with deep reinforcement learning," arXiv preprint arXiv:1509.02971, 2015.
[4] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, "Trust region policy optimization," in Proceedings of the 32nd International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, F. Bach and D. Blei, Eds., vol. 37. Lille, France: PMLR, 07-09 Jul 2015, pp. 1889-1897.
[5] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," arXiv preprint arXiv:1707.06347, 2017.
[6] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, et al., "Using simulation and domain adaptation to improve efficiency of deep robotic grasping," in 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. 4243-4250.
[7] G. J. Stein and N. Roy, "Genesis-rt: Generating synthetic images for training secondary real-world tasks," in 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. $7151-7158$.
[8] F. Sadeghi and S. Levine, "Cad2rl: Real single-image flight without a single real image," in Proceedings of Robotics: Science and Systems, Cambridge, Massachusetts, July 2017.
[9] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain randomization for transferring deep neural networks from simulation to the real world," in Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on. IEEE, 2017, pp. 23-30.
[10] A. A. Rusu, M. Večerík, T. Rothörl, N. Heess, R. Pascanu, and R. Hadsell, "Sim-to-real robot learning from pixels with progressive nets," in Proceedings of the 1st Annual Conference on Robot Learning, ser. Proceedings of Machine Learning Research, S. Levine, V. Vanhoucke, and K. Goldberg, Eds., vol. 78. PMLR, 13-15 Nov 2017, pp. 262-270.
[11] J. Zhu, T. Park, P. Isola, and A. A. Efros, "Unpaired image-to-image translation using cycle-consistent adversarial networks," in 2017 IEEE International Conference on Computer Vision (ICCV), Oct 2017, pp. $2242-2251$.
[12] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros, and T. Darrell, "CyCADA: Cycle-consistent adversarial domain adaptation," in Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, J. Dy and A. Krause, Eds., vol. 80. Stockholmsmässan, Stockholm Sweden: PMLR, 10-15 Jul 2018, pp. 1989-1998.
[13] A. Cherian and A. Sullivan, "Sem-gan: Semantically-consistent image-to-image translation," arXiv preprint arXiv:1807.04409, 2018.
[14] L. Pinto, M. Andrychowicz, P. Welinder, W. Zaremba, and P. Abbeel, "Asymmetric actor critic for image-based robot learning," in Proceedings of Robotics: Science and Systems, Pittsburgh, Pennsylvania, June 2018.
[15] L. Tai, G. Paolo, and M. Liu, "Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mipless navigation," in 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Sept 2017, pp. 31-36.
[16] J. Zhang, L. Tai, J. Boedecker, W. Burgard, and M. Liu, "Neural SLAM: Learning to explore with external memory," arXiv preprint arXiv:1706.09520, 2017.
[17] O. Zhelo, J. Zhang, L. Tai, M. Liu, and W. Burgard, "Curiosity-driven exploration for mipless navigation with deep reinforcement learning," arXiv preprint arXiv:1804.00456, 2018.
[18] J. Zhang, J. T. Springenberg, J. Boedecker, and W. Burgard, "Deep reinforcement learning with successor features for navigation across similar environments," in 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Sept 2017, pp. 2371-2378.
[19] L. Tai, J. Zhang, M. Liu, and W. Burgard, "Socially-compliant navigation through raw depth inputs with generative adversarial imitation learning," in 2018 IEEE International Conference on Robotics and Automation (ICRA), May 2018, pp. 1111-1117.
[20] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi, "Target-driven visual navigation in indoor scenes using deep reinforcement learning," in 2017 IEEE International Conference on Robotics and Automation (ICRA), May 2017, pp. 3357-3364.
[21] T. Inoue, S. Choudhury, G. De Magistris, and S. Dasgupta, "Transfer learning from synthetic to real images using variational autoencoders for precise position detection," in 2018 25th IEEE International Conference on Image Processing (ICIP), 2018, pp. 2725-2729.
[22] J. Johnson, A. Alahi, and L. Fei-Fei, "Perceptual losses for realtime style transfer and super-resolution," in European Conference on Computer Vision. Springer, 2016, pp. 694-711.
[23] M. Ruder, A. Dosovitskiy, and T. Brox, "Artistic style transfer for videos and spherical images," International Journal of Computer Vision, vol. 126, no. 11, pp. 1199-1219, Nov 2018. [Online]. Available: https://doi.org/10.1007/s11263-018-1089-z
[24] H. Huang, H. Wang, W. Luo, L. Ma, W. Jiang, X. Zhu, Z. Li, and W. Liu, "Real-time neural style transfer for videos," in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017, pp. 7044-7052.
[25] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black, "A naturalistic open source movie for optical flow evaluation," in European Conference on Computer Vision. Springer, 2012, pp. 611-625.
[26] J. Zhang, L. Tai, Y. Xiong, M. Liu, J. Boedecker, and W. Burgard, "Supplement file of VR-Goggles for robots: Real-to-sim domain adaptation for visual control," Tech. Rep., 2018. [Online]. Available: https://ram-lab.com/file/tailei/vr_goggles/supplement.pdf
[27] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, "CARLA: An open urban driving simulator," in Proceedings of the 1st Annual Conference on Robot Learning, ser. Proceedings of Machine Learning Research, S. Levine, V. Vanhoucke, and K. Goldberg, Eds., vol. 78. PMLR, 13-15 Nov 2017, pp. 1-16.
[28] F. Codevilla, M. Miiller, A. López, V. Koltun, and A. Dosovitskiy, "End-to-end driving via conditional imitation learning," in 2018 IEEE International Conference on Robotics and Automation (ICRA), May 2018, pp. 1-9.
[29] N. Koenig and A. Howard, "Design and use paradigms for gazebo, an open-source multi-robot simulator," in 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), vol. 3, Sep. 2004, pp. 2149-2154 vol.3.
[30] W. Maddern, G. Pascoe, C. Linegar, and P. Newman, "1 Year, 1000km: The Oxford RobotCar Dataset," The International Journal of Robotics Research (IJRR), vol. 36, no. 1, pp. 3-15, 2017.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/carla-simulator/imitation-learning&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
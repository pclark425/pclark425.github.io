<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2226 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2226</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2226</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-60.html">extraction-schema-60</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <p><strong>Paper ID:</strong> paper-281675235</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.23874v1.pdf" target="_blank">Multi-Value-Product Retrieval-Augmented Generation for Industrial Product Attribute Value Identification</a></p>
                <p><strong>Paper Abstract:</strong> Identifying attribute values from product profiles is a key task for improving product search, recommendation, and business analytics on e-commerce platforms, which we called Product Attribute Value Identification (PAVI) . However, existing PAVI methods face critical challenges, such as cascading errors, inability to handle out-of-distribution (OOD) attribute values, and lack of generalization capability. To address these limitations, we introduce Multi-Value-Product Retrieval-Augmented Generation (MVP-RAG), combining the strengths of retrieval, generation, and classification paradigms. MVP-RAG defines PAVI as a retrieval-generation task, where the product title description serves as the query, and products and attribute values act as the corpus. It first retrieves similar products of the same category and candidate attribute values, and then generates the standardized attribute values. The key advantages of this work are: (1) the proposal of a multi-level retrieval scheme, with products and attribute values as distinct hierarchical levels in PAVI domain (2) attribute value generation of large language model to significantly alleviate the OOD problem and (3) its successful deployment in a real-world industrial environment. Extensive experimental results demonstrate that MVP-RAG performs better than the state-of-the-art baselines.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2226.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2226.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MVP-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Value-Product Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid retrieval + generation + classification system for product attribute value identification that retrieves both similar products (product-level few-shot examples) and candidate attribute values, then uses an LLM to generate standardized attribute values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MVP-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines two retrieval levels (attribute-value retrieval via TACLR-style scoring and product retrieval via BGE embeddings/cosine similarity) to produce candidate context, then conditions a Qwen2.5 LLM (next-token objective) on that context to generate standardized attribute values for products. Also injects an out-of-distribution attribute-value batch during training to improve OOD discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>e-commerce / natural language processing (product attribute value identification)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Retrieval similarity scores (TACLR prediction scores for attribute values; cosine similarity using BGE embeddings for product retrieval) and Coverage (overlap of retrieved candidate set with ground-truth set)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Attribute candidates are scored using TACLR-style retrieval scores (contrastive-learnt similarity between product text and attribute-value prompts); similar products are retrieved via BGE embeddings and cosine similarity. Coverage is computed as whether the retrieved candidate set intersects the ground-truth value set (binary per instance) and reported as a percent across the test set.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>hybrid (data-driven ML prediction: embedding similarity / contrastive retrieval scores; empirical surrogate: coverage as intersection-overlap proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Human-verified attribute labels (Xianyu-PAVI dataset) evaluated with micro-averaged precision@1, recall@1 and F1@1</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Ground truth originates from a three-step pipeline: model prediction, seller feedback, and manual review; the human-reviewed labels in the Xianyu-PAVI dataset are treated as the gold set. Evaluation aggregates TP/FP/FN across attributes and computes micro precision@1, recall@1 and F1@1. For non-empty ground-truth sets an exact match of the top-1 predicted value counts as TP.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Several empirical gaps reported: (1) When the true attribute value appears among retrieved candidate attribute values, F1 reaches 92.6% vs 86.3% when it does not appear (gap = 6.3 percentage points). (2) Compared to TACLR, precision improved from 85.4% to 93.2% (reported as ~+7.8 percentage points). (3) MVP-RAG F1 = 89.5% on Xianyu-PAVI; improvement vs Qwen2.5(Product-RAG) reported as +26.3 percentage points on F1 and vs TACLR reported as +3.3 percentage points on F1.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Coverage and retrieval proxies: product-recall coverage increases from 56.3% to 78.5% as number of retrieved same-category products increases; attribute-candidate coverage rises from 94.9% to 99.6% as number of retrieved attribute values increases. When true value is in candidate set, generation F1 = 92.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Precision ≈ 93.8% (reported elsewhere as 93.2% when compared to TACLR), Recall = 85.3%, F1 = 89.5% (micro-averaged @1 on Xianyu-PAVI).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Designed to handle out-of-distribution (OOD) attribute values and the long-tail of product attribute spaces; authors explicitly inject OOD attribute values into training, so discoveries include in-distribution and OOD cases (extrapolative elements).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Paper notes OOD is a key challenge and that generation-based components alleviate OOD problems qualitatively, but it does not provide a quantitative breakdown of proxy-to-ground-truth gap specifically stratified by OOD distance.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Multi-level retrieval (attribute-level TACLR candidates + product-level few-shot examples) to supply context, LLM conditional generation to produce values beyond retrieval lists, and training with an explicit batch of OOD attribute values to improve discovery of unknown values.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Reported empirical improvements attributable to these methods include: overall F1 = 89.5% (state-of-the-art on Xianyu-PAVI), +26.3 percentage points F1 vs Qwen2.5(Product-RAG) baseline, +3.3 percentage points F1 vs TACLR baseline, and precision improvement vs TACLR of nearly 8 percentage points (85.4% -> 93.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Not numerically quantified. Qualitatively: retrieval and automated generation provide fast, large-scale predictions (deployed to process millions of listings daily), while ground-truth requires seller feedback and manual human review which is implied to be orders of magnitude more costly/time-consuming.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Applied industrial NLP for product attribute extraction is a mature applied domain with many existing methods (classification, retrieval, generation), but handling large dynamic taxonomies and OOD remains an active research area.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td>Not explicitly analyzed quantitatively; attribute-level and product-level retrieval proxies are combined and likely partially correlated (both derive from textual similarity signals), but the paper reports that combining them reduces hallucination and improves robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Yes — retrieval (attribute-value candidate retrieval via TACLR and product retrieval via BGE embeddings) → LLM generation conditioned on retrieved context → evaluation against human-reviewed ground-truth labels. The paper discusses how absence/presence of true value in retrieved candidates influences final generation F1 (error propagation from retrieval to generation).</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Identified challenges include cascading errors in two-stage pipelines, OOD attribute values, hallucination from pure generation, sensitivity to retrieval quality (if retrieved product attribute values are highly incorrect the model may still succeed up to ~75% error for clear attributes but will fail beyond that), lack of multimodal (image/video) inputs, and inference latency of LLM reasoning outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Large, long-tailed attribute vocabularies (average ~716 attribute values per category), noisy seller-generated descriptions, dynamic and evolving taxonomies, necessity to restrict retrieval within same category to ensure attribute consistency, and real-world deployment constraints (latency, scale).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2226.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2226.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TACLR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TACLR (contrastive retrieval for attribute values)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-based PAVI method that encodes product descriptions and attribute-value prompts into vectors (contrastive learning) and selects candidate attribute values by similarity and adaptive thresholding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Taclr: A scalable and efficient retrieval-based method for industrial product attribute value identification.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TACLR</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Contrastive retrieval model that constructs prompts from attribute-tree information, encodes product queries and attribute-value entries, and retrieves top-k attribute values per category based on similarity and adaptive thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>e-commerce / NLP (product attribute value retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>TACLR prediction score / retrieval similarity (contrastive similarity) and top-k candidate set coverage</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>A learned retrieval score (from contrastive training) used to rank attribute-value candidates for each product; top-k candidates form the surrogate candidate set used in downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction (contrastive retrieval / embedding similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Human-verified attribute labels evaluated with micro precision@1, recall@1, F1@1 on Xianyu-PAVI</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Same human-verified dataset (seller feedback + manual review) and evaluation rules as used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Reported reference points: TACLR precision cited as 85.4% (used as comparison), TACLR F1 reported in comparisons (paper implies TACLR F1 ≈ 86.2% though exact table alignment is noisy). MVP-RAG reports improvements over TACLR of +3.3 percentage points in F1 and ~+7.8 points in precision.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Attribute-candidate coverage from TACLR is used as baseline; explicit coverage numbers for TACLR alone are not separated in the paper beyond the precision/statistics referenced (precision ≈ 85.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Reported baseline metrics in table and comparisons: precision ≈ 85.4%, F1 ≈ mid-80s (paper reports TACLR as previous SOTA and gives comparative deltas: MVP-RAG improves F1 by 3.3 percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Operates primarily in-distribution as a retrieval system limited by the attribute taxonomy; less capable for novel/OOD values compared with generation-based methods unless the taxonomy contains them.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Not explicitly quantified; paper positions TACLR as strong for retrieval but limited in handling novel attribute values and thresholding sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>TACLR itself uses contrastive learning and adaptive thresholding as internal methods to calibrate retrieval scores; the MVP-RAG paper uses TACLR outputs as candidates and augments with generation to reduce gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>When TACLR's candidate list is augmented by MVP-RAG's multi-level retrieval + generation, the paper reports precision improvements of roughly +7.8 percentage points and F1 improvements of +3.3 percentage points vs TACLR alone.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Not discussed numerically in the TACLR reference within this paper; retrieval is implied cheaper than human review but provides only candidate lists that still require downstream mapping/generation to match ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Represents an established retrieval-based approach in industrial PAVI settings (presented as prior SOTA in retrieval paradigm).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Used as the attribute-level retrieval stage in a cascade: TACLR retrieval → generation/alignment → evaluation. Errors in retrieval reduce chance that generation can produce correct output (paper quantifies generation F1 is higher when true value is present in retrieved candidates).</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Limited ability to identify OOD attribute values, sensitivity to threshold selection, and limited generalization to evolving taxonomies; provides candidates but requires downstream selection/alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Effectiveness depends on completeness of attribute taxonomy and quality of training pairs; long-tail attributes and dynamic taxonomies reduce coverage of retrieval-only approaches.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2226.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2226.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Product-RAG (Qwen2.5 Product-RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Product-Retrieval-Augmented Generation using Qwen2.5 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that conditions an LLM (Qwen2.5) on retrieved similar products (product-level retrieval only) to generate attribute values; used as a comparison to MVP-RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Qwen2.5 (Product-RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM (Qwen2.5) augmented with product-level retrieval (BGE embeddings + cosine similarity) to provide context/examples for generation, but without the attribute-value-level retrieval augmentation that MVP-RAG includes.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>e-commerce / NLP (generation-based PAVI)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Product retrieval similarity (BGE cosine similarity) used as a surrogate for relevant context; generation likelihood (next-token probability) as implicit surrogate objective during training.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Similar-product retrieval based on embedding similarity supplies few-shot examples; next-token prediction (language modeling loss) is used to train the generator. Retrieval similarity is used as a proxy for the relevance of examples to the target product.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>hybrid (data-driven ML prediction for retrieval; surrogate ML objective: next-token prediction loss for generation)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Human-verified attribute labels (Xianyu-PAVI) measured with micro precision@1, recall@1, F1@1.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Same human-reviewed dataset as used across experiments in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Paper reports MVP-RAG outperforms Qwen2.5(Product-RAG) by 26.3 percentage points on F1; Qwen2.5(Product-RAG) baseline F1 appears substantially lower (table indicates product-RAG precision ~58.3%, recall ~50.1%, F1 ~56.3% though table formatting is noisy).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Product-retrieval coverage (when increasing number of retrieved same-category products) increases coverage of test attribute values from 56.3% to 78.5%, but Product-RAG's downstream F1 remains much lower than MVP-RAG's when only product-level retrieval is used.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Reported baseline F1 for Qwen2.5(Product-RAG) is substantially lower than MVP-RAG (MVP-RAG reports +26.3 percentage points F1 improvement over this baseline). Exact baseline F1 is presented in the paper's table but table alignment is noisy; relative delta is explicitly stated.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Primarily benefits from in-distribution examples via retrieval, less effective alone at generating correct labels for OOD attribute values compared with systems that also retrieve attribute-value candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Not quantified; paper argues that relying solely on product retrieval can introduce hallucination and insufficient constraint for output-constrained tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>MVP-RAG extends Product-RAG by adding attribute-value-level retrieval (TACLR candidates) and combining both retrieval levels to reduce hallucination and increase candidate coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>MVP-RAG achieves +26.3 percentage points F1 improvement over Qwen2.5(Product-RAG) baseline, indicating substantial reduction of proxy-to-ground-truth gap by adding attribute-value retrieval and improved conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Not explicitly provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Generation-augmented retrieval approaches are actively developed; pure product-level RAG is effective for some contexts but insufficient for output-constrained attribute generation without attribute candidate augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td>Not analyzed; product-level retrieval and generation objective are coupled in failure modes (irrelevant retrieved examples can cause hallucination).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Yes — product retrieval → LLM generation → evaluation vs human labels. MVP-RAG adds attribute-value retrieval into this cascade to improve success.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Product-only retrieval may retrieve examples with inconsistent attribute labels and therefore insufficiently constrain generation, leading to hallucinations and poor F1 compared to hybrid retrieval+generation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Quality and category-restriction of retrieved products strongly affect final generation; noisy or mislabeled retrieved examples can mislead the generator (authors report failure modes where many retrieved examples with wrong brand can still produce correct result up to a point).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Taclr: A scalable and efficient retrieval-based method for industrial product attribute value identification. <em>(Rating: 2)</em></li>
                <li>A unified generative approach to product attribute-value identification. <em>(Rating: 1)</em></li>
                <li>Sage: Structured attribute value generation for billion-scale product catalogs. <em>(Rating: 1)</em></li>
                <li>An empirical comparison of generative approaches for product attribute-value identification. <em>(Rating: 1)</em></li>
                <li>Toward optimal search and retrieval for rag. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2226",
    "paper_id": "paper-281675235",
    "extraction_schema_id": "extraction-schema-60",
    "extracted_data": [
        {
            "name_short": "MVP-RAG",
            "name_full": "Multi-Value-Product Retrieval-Augmented Generation",
            "brief_description": "A hybrid retrieval + generation + classification system for product attribute value identification that retrieves both similar products (product-level few-shot examples) and candidate attribute values, then uses an LLM to generate standardized attribute values.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MVP-RAG",
            "system_description": "Combines two retrieval levels (attribute-value retrieval via TACLR-style scoring and product retrieval via BGE embeddings/cosine similarity) to produce candidate context, then conditions a Qwen2.5 LLM (next-token objective) on that context to generate standardized attribute values for products. Also injects an out-of-distribution attribute-value batch during training to improve OOD discovery.",
            "domain": "e-commerce / natural language processing (product attribute value identification)",
            "proxy_metric_name": "Retrieval similarity scores (TACLR prediction scores for attribute values; cosine similarity using BGE embeddings for product retrieval) and Coverage (overlap of retrieved candidate set with ground-truth set)",
            "proxy_metric_description": "Attribute candidates are scored using TACLR-style retrieval scores (contrastive-learnt similarity between product text and attribute-value prompts); similar products are retrieved via BGE embeddings and cosine similarity. Coverage is computed as whether the retrieved candidate set intersects the ground-truth value set (binary per instance) and reported as a percent across the test set.",
            "proxy_metric_type": "hybrid (data-driven ML prediction: embedding similarity / contrastive retrieval scores; empirical surrogate: coverage as intersection-overlap proxy)",
            "ground_truth_metric": "Human-verified attribute labels (Xianyu-PAVI dataset) evaluated with micro-averaged precision@1, recall@1 and F1@1",
            "ground_truth_description": "Ground truth originates from a three-step pipeline: model prediction, seller feedback, and manual review; the human-reviewed labels in the Xianyu-PAVI dataset are treated as the gold set. Evaluation aggregates TP/FP/FN across attributes and computes micro precision@1, recall@1 and F1@1. For non-empty ground-truth sets an exact match of the top-1 predicted value counts as TP.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Several empirical gaps reported: (1) When the true attribute value appears among retrieved candidate attribute values, F1 reaches 92.6% vs 86.3% when it does not appear (gap = 6.3 percentage points). (2) Compared to TACLR, precision improved from 85.4% to 93.2% (reported as ~+7.8 percentage points). (3) MVP-RAG F1 = 89.5% on Xianyu-PAVI; improvement vs Qwen2.5(Product-RAG) reported as +26.3 percentage points on F1 and vs TACLR reported as +3.3 percentage points on F1.",
            "proxy_performance": "Coverage and retrieval proxies: product-recall coverage increases from 56.3% to 78.5% as number of retrieved same-category products increases; attribute-candidate coverage rises from 94.9% to 99.6% as number of retrieved attribute values increases. When true value is in candidate set, generation F1 = 92.6%.",
            "ground_truth_performance": "Precision ≈ 93.8% (reported elsewhere as 93.2% when compared to TACLR), Recall = 85.3%, F1 = 89.5% (micro-averaged @1 on Xianyu-PAVI).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "Designed to handle out-of-distribution (OOD) attribute values and the long-tail of product attribute spaces; authors explicitly inject OOD attribute values into training, so discoveries include in-distribution and OOD cases (extrapolative elements).",
            "gap_varies_with_novelty": null,
            "gap_variation_details": "Paper notes OOD is a key challenge and that generation-based components alleviate OOD problems qualitatively, but it does not provide a quantitative breakdown of proxy-to-ground-truth gap specifically stratified by OOD distance.",
            "gap_reduction_method": "Multi-level retrieval (attribute-level TACLR candidates + product-level few-shot examples) to supply context, LLM conditional generation to produce values beyond retrieval lists, and training with an explicit batch of OOD attribute values to improve discovery of unknown values.",
            "gap_reduction_effectiveness": "Reported empirical improvements attributable to these methods include: overall F1 = 89.5% (state-of-the-art on Xianyu-PAVI), +26.3 percentage points F1 vs Qwen2.5(Product-RAG) baseline, +3.3 percentage points F1 vs TACLR baseline, and precision improvement vs TACLR of nearly 8 percentage points (85.4% -&gt; 93.2%).",
            "validation_cost_comparison": "Not numerically quantified. Qualitatively: retrieval and automated generation provide fast, large-scale predictions (deployed to process millions of listings daily), while ground-truth requires seller feedback and manual human review which is implied to be orders of magnitude more costly/time-consuming.",
            "temporal_validation": null,
            "domain_maturity": "Applied industrial NLP for product attribute extraction is a mature applied domain with many existing methods (classification, retrieval, generation), but handling large dynamic taxonomies and OOD remains an active research area.",
            "uncertainty_quantification": false,
            "uncertainty_calibration": "",
            "multiple_proxies": true,
            "proxy_correlation": "Not explicitly analyzed quantitatively; attribute-level and product-level retrieval proxies are combined and likely partially correlated (both derive from textual similarity signals), but the paper reports that combining them reduces hallucination and improves robustness.",
            "validation_cascade": "Yes — retrieval (attribute-value candidate retrieval via TACLR and product retrieval via BGE embeddings) → LLM generation conditioned on retrieved context → evaluation against human-reviewed ground-truth labels. The paper discusses how absence/presence of true value in retrieved candidates influences final generation F1 (error propagation from retrieval to generation).",
            "publication_bias_discussion": false,
            "limitations_challenges": "Identified challenges include cascading errors in two-stage pipelines, OOD attribute values, hallucination from pure generation, sensitivity to retrieval quality (if retrieved product attribute values are highly incorrect the model may still succeed up to ~75% error for clear attributes but will fail beyond that), lack of multimodal (image/video) inputs, and inference latency of LLM reasoning outputs.",
            "domain_specific_factors": "Large, long-tailed attribute vocabularies (average ~716 attribute values per category), noisy seller-generated descriptions, dynamic and evolving taxonomies, necessity to restrict retrieval within same category to ensure attribute consistency, and real-world deployment constraints (latency, scale).",
            "uuid": "e2226.0"
        },
        {
            "name_short": "TACLR",
            "name_full": "TACLR (contrastive retrieval for attribute values)",
            "brief_description": "A retrieval-based PAVI method that encodes product descriptions and attribute-value prompts into vectors (contrastive learning) and selects candidate attribute values by similarity and adaptive thresholding.",
            "citation_title": "Taclr: A scalable and efficient retrieval-based method for industrial product attribute value identification.",
            "mention_or_use": "use",
            "system_name": "TACLR",
            "system_description": "Contrastive retrieval model that constructs prompts from attribute-tree information, encodes product queries and attribute-value entries, and retrieves top-k attribute values per category based on similarity and adaptive thresholds.",
            "domain": "e-commerce / NLP (product attribute value retrieval)",
            "proxy_metric_name": "TACLR prediction score / retrieval similarity (contrastive similarity) and top-k candidate set coverage",
            "proxy_metric_description": "A learned retrieval score (from contrastive training) used to rank attribute-value candidates for each product; top-k candidates form the surrogate candidate set used in downstream tasks.",
            "proxy_metric_type": "data-driven ML prediction (contrastive retrieval / embedding similarity)",
            "ground_truth_metric": "Human-verified attribute labels evaluated with micro precision@1, recall@1, F1@1 on Xianyu-PAVI",
            "ground_truth_description": "Same human-verified dataset (seller feedback + manual review) and evaluation rules as used in this paper.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Reported reference points: TACLR precision cited as 85.4% (used as comparison), TACLR F1 reported in comparisons (paper implies TACLR F1 ≈ 86.2% though exact table alignment is noisy). MVP-RAG reports improvements over TACLR of +3.3 percentage points in F1 and ~+7.8 points in precision.",
            "proxy_performance": "Attribute-candidate coverage from TACLR is used as baseline; explicit coverage numbers for TACLR alone are not separated in the paper beyond the precision/statistics referenced (precision ≈ 85.4%).",
            "ground_truth_performance": "Reported baseline metrics in table and comparisons: precision ≈ 85.4%, F1 ≈ mid-80s (paper reports TACLR as previous SOTA and gives comparative deltas: MVP-RAG improves F1 by 3.3 percentage points).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "Operates primarily in-distribution as a retrieval system limited by the attribute taxonomy; less capable for novel/OOD values compared with generation-based methods unless the taxonomy contains them.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": "Not explicitly quantified; paper positions TACLR as strong for retrieval but limited in handling novel attribute values and thresholding sensitivity.",
            "gap_reduction_method": "TACLR itself uses contrastive learning and adaptive thresholding as internal methods to calibrate retrieval scores; the MVP-RAG paper uses TACLR outputs as candidates and augments with generation to reduce gaps.",
            "gap_reduction_effectiveness": "When TACLR's candidate list is augmented by MVP-RAG's multi-level retrieval + generation, the paper reports precision improvements of roughly +7.8 percentage points and F1 improvements of +3.3 percentage points vs TACLR alone.",
            "validation_cost_comparison": "Not discussed numerically in the TACLR reference within this paper; retrieval is implied cheaper than human review but provides only candidate lists that still require downstream mapping/generation to match ground truth.",
            "temporal_validation": null,
            "domain_maturity": "Represents an established retrieval-based approach in industrial PAVI settings (presented as prior SOTA in retrieval paradigm).",
            "uncertainty_quantification": false,
            "uncertainty_calibration": "",
            "multiple_proxies": false,
            "proxy_correlation": "",
            "validation_cascade": "Used as the attribute-level retrieval stage in a cascade: TACLR retrieval → generation/alignment → evaluation. Errors in retrieval reduce chance that generation can produce correct output (paper quantifies generation F1 is higher when true value is present in retrieved candidates).",
            "publication_bias_discussion": false,
            "limitations_challenges": "Limited ability to identify OOD attribute values, sensitivity to threshold selection, and limited generalization to evolving taxonomies; provides candidates but requires downstream selection/alignment.",
            "domain_specific_factors": "Effectiveness depends on completeness of attribute taxonomy and quality of training pairs; long-tail attributes and dynamic taxonomies reduce coverage of retrieval-only approaches.",
            "uuid": "e2226.1"
        },
        {
            "name_short": "Product-RAG (Qwen2.5 Product-RAG)",
            "name_full": "Product-Retrieval-Augmented Generation using Qwen2.5 (baseline)",
            "brief_description": "A baseline that conditions an LLM (Qwen2.5) on retrieved similar products (product-level retrieval only) to generate attribute values; used as a comparison to MVP-RAG.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Qwen2.5 (Product-RAG)",
            "system_description": "An LLM (Qwen2.5) augmented with product-level retrieval (BGE embeddings + cosine similarity) to provide context/examples for generation, but without the attribute-value-level retrieval augmentation that MVP-RAG includes.",
            "domain": "e-commerce / NLP (generation-based PAVI)",
            "proxy_metric_name": "Product retrieval similarity (BGE cosine similarity) used as a surrogate for relevant context; generation likelihood (next-token probability) as implicit surrogate objective during training.",
            "proxy_metric_description": "Similar-product retrieval based on embedding similarity supplies few-shot examples; next-token prediction (language modeling loss) is used to train the generator. Retrieval similarity is used as a proxy for the relevance of examples to the target product.",
            "proxy_metric_type": "hybrid (data-driven ML prediction for retrieval; surrogate ML objective: next-token prediction loss for generation)",
            "ground_truth_metric": "Human-verified attribute labels (Xianyu-PAVI) measured with micro precision@1, recall@1, F1@1.",
            "ground_truth_description": "Same human-reviewed dataset as used across experiments in the paper.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Paper reports MVP-RAG outperforms Qwen2.5(Product-RAG) by 26.3 percentage points on F1; Qwen2.5(Product-RAG) baseline F1 appears substantially lower (table indicates product-RAG precision ~58.3%, recall ~50.1%, F1 ~56.3% though table formatting is noisy).",
            "proxy_performance": "Product-retrieval coverage (when increasing number of retrieved same-category products) increases coverage of test attribute values from 56.3% to 78.5%, but Product-RAG's downstream F1 remains much lower than MVP-RAG's when only product-level retrieval is used.",
            "ground_truth_performance": "Reported baseline F1 for Qwen2.5(Product-RAG) is substantially lower than MVP-RAG (MVP-RAG reports +26.3 percentage points F1 improvement over this baseline). Exact baseline F1 is presented in the paper's table but table alignment is noisy; relative delta is explicitly stated.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "Primarily benefits from in-distribution examples via retrieval, less effective alone at generating correct labels for OOD attribute values compared with systems that also retrieve attribute-value candidates.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": "Not quantified; paper argues that relying solely on product retrieval can introduce hallucination and insufficient constraint for output-constrained tasks.",
            "gap_reduction_method": "MVP-RAG extends Product-RAG by adding attribute-value-level retrieval (TACLR candidates) and combining both retrieval levels to reduce hallucination and increase candidate coverage.",
            "gap_reduction_effectiveness": "MVP-RAG achieves +26.3 percentage points F1 improvement over Qwen2.5(Product-RAG) baseline, indicating substantial reduction of proxy-to-ground-truth gap by adding attribute-value retrieval and improved conditioning.",
            "validation_cost_comparison": "Not explicitly provided.",
            "temporal_validation": null,
            "domain_maturity": "Generation-augmented retrieval approaches are actively developed; pure product-level RAG is effective for some contexts but insufficient for output-constrained attribute generation without attribute candidate augmentation.",
            "uncertainty_quantification": false,
            "uncertainty_calibration": "",
            "multiple_proxies": true,
            "proxy_correlation": "Not analyzed; product-level retrieval and generation objective are coupled in failure modes (irrelevant retrieved examples can cause hallucination).",
            "validation_cascade": "Yes — product retrieval → LLM generation → evaluation vs human labels. MVP-RAG adds attribute-value retrieval into this cascade to improve success.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Product-only retrieval may retrieve examples with inconsistent attribute labels and therefore insufficiently constrain generation, leading to hallucinations and poor F1 compared to hybrid retrieval+generation methods.",
            "domain_specific_factors": "Quality and category-restriction of retrieved products strongly affect final generation; noisy or mislabeled retrieved examples can mislead the generator (authors report failure modes where many retrieved examples with wrong brand can still produce correct result up to a point).",
            "uuid": "e2226.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Taclr: A scalable and efficient retrieval-based method for industrial product attribute value identification.",
            "rating": 2
        },
        {
            "paper_title": "A unified generative approach to product attribute-value identification.",
            "rating": 1
        },
        {
            "paper_title": "Sage: Structured attribute value generation for billion-scale product catalogs.",
            "rating": 1
        },
        {
            "paper_title": "An empirical comparison of generative approaches for product attribute-value identification.",
            "rating": 1
        },
        {
            "paper_title": "Toward optimal search and retrieval for rag.",
            "rating": 2
        }
    ],
    "cost": 0.01619325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multi-Value-Product Retrieval-Augmented Generation for Industrial Product Attribute Value Identification
28 Sep 2025</p>
<p>Huike Zou zouhuike.zhk@alibaba-inc.com 
Xianyu of Alibaba</p>
<p>Haiyang Yang 
Xianyu of Alibaba</p>
<p>Yindu Su 
Xiaohongshu Inc</p>
<p>Liyu Chen 
Xianyu of Alibaba</p>
<p>Chengbao Lian 
Xianyu of Alibaba</p>
<p>Qingheng Zhang qingheng.zqh@alibaba-inc.com 
Xianyu of Alibaba</p>
<p>Shuguang Han shuguang.sh@alibaba-inc.com 
Xianyu of Alibaba</p>
<p>Jufeng Chen 
Xianyu of Alibaba</p>
<p>Multi-Value-Product Retrieval-Augmented Generation for Industrial Product Attribute Value Identification
28 Sep 20256C7EBB53DF166C9EE663A8CAFC3852B7arXiv:2509.23874v1[cs.IR]
Identifying attribute values from product profiles is a key task for improving product search, recommendation, and business analytics on ecommerce platforms, which we called Product Attribute Value Identification (PAVI) .However, existing PAVI methods face critical challenges, such as cascading errors, inability to handle outof-distribution (OOD) attribute values, and lack of generalization capability.To address these limitations, we introduce Multi-Value-Product Retrieval-Augmented Generation (MVP-RAG), combining the strengths of retrieval, generation, and classification paradigms.MVP-RAG defines PAVI as a retrieval-generation task, where the product title description serves as the query, and products and attribute values act as the corpus.It first retrieves similar products of the same category and candidate attribute values, and then generates the standardized attribute values.The key advantages of this work are: (1) the proposal of a multi-level retrieval scheme, with products and attribute values as distinct hierarchical levels in PAVI domain (2) attribute value generation of large language model to significantly alleviate the OOD problem and (3) its successful deployment in a real-world industrial environment.Extensive experimental results demonstrate that MVP-RAG performs better than the state-of-the-art baselines.</p>
<p>Introduction</p>
<p>Product attribute values are fundamental components in the e-commerce sector, serving as critical elements within the online retail ecosystem.From the perspective of a seller's business strategy, meticulously curated product attribute values can significantly enhance traffic acquisition, increase product visibility, and ultimately drive transaction conversion.From the operational standpoint of e-commerce platforms, these attribute values provide fundamental data support for core function- † Corresponding author.</p>
<p>attributevalue pairs</p>
<p>Figure 1: (a) shows the previous LLM and RAG methods, which either retrieve products from the product pool or directly generate attribute values.In contrast, (b) depicts our approach, which retrieves from both product and attribute value perspectives.This not only reduces the hallucination error caused by retrieving irrelevant products, but also ensures the stability of the results.Compared to previous studies, the core of our approach lies in the change in Retrieval.</p>
<p>alities, such as optimizing product display algorithms (Chen et al., 2024), constructing personalized recommendation systems (Truong et al., 2022), and enabling intelligent question-answering services (Gao et al., 2019), which are essential for improving user experience (Vashishth et al., 2024) and platform operational efficiency.</p>
<p>Professional sellers are usually able to standardize the selection of product attribute values.However, on second-hand trading platforms like Xianyu * , individual sellers often struggle to provide complete and accurate product attributes, which severely impacts the circulation efficiency of products.Therefore, achieving automated and precise product attribute value identification (PAVI) (Shinzato et al., 2023) is of great significance.Currently, PAVI models can be broadly categorized into two-stage (Putthividhya and Hu, 2011;Zhang et al., 2021) and one-stage paradigms.</p>
<p>The two-stage methods involve two steps: product attribute value extraction (PAVE) and alignment.They first extract non-standard attribute values from text using entity recognition (Zheng et al., 2018;Xu et al., 2019;Yan et al., 2021) or question-answering (Wang et al., 2020;Yang et al., 2023) techniques, then map them to standard attribute value.The one-stage methods can be further divided into three strategies: classificationbased, generation-based, and retrieval-based.The classification-based approach (Chen et al., 2022) treats each attribute value as an independent class for multi-label classification.The generation-based approach (Sabeh et al., 2024;Nikolakopoulos et al., 2023;Shinzato et al., 2023) directly generates standard attribute values based on product descriptions.The retrieval-based approach (Su et al., 2025) encodes product descriptions and attribute values into vector representations, and then selects the relevant attribute values through similarity calculations.</p>
<p>The existing PAVI methods have their own strengths and weaknesses, and face various technical challenges.The two-stage PAVI methods first extract non-standard attribute values and then align them to the standard attribute value.This approach suffers from cascading errors, where attributes that cannot be identified in the first stage will be completely lost in the second stage.The one-stage methods can address this problem but introduce other issues.The classification-based methods treat attribute values as independent classes, which is a simple approach but is limited in its ability to identify OOD attribute values, making it unsuitable for evolving e-commerce platforms.The generationbased methods view PAVI as an end-to-end task, which can solve the OOD problem, but suffer from the inability to control the output.The retrievalbased methods consider PAVI as a matching task between product and attribute values, but face issues with undefined thresholds and insensitivity to new attribute values.In summary, the existing methods face various problems, including implicit value identification, OOD recognition, and generalization capability.</p>
<p>In response to the limitations of existing methods, this paper proposes a novel Multi-Value-Product Retrieval Augmented Generation (MVP-RAG) approach, which innovatively combines the strengths of retrieval, generation, and classification paradigms, as shown in Fig 1 .Our method defines PAVI as a retrieval-generation task: the product title description serves as the query, while the product pool and attribute taxonomy act as the corpus.We first retrieve similar products with identical category and attribute values, and then generate the standardized attribute values.For attribute value retrieval, we leverage TACLR (Su et al., 2025) to select the top-K attribute values with the highest prediction scores as the candidate set.For product retrieval, we use the BGE model (Xiao et al., 2024) to generate vector representations of the product corpus, and then retrieve similar products based on similarity.Based on the retrieval results, we use a large language model (LLM) (Zhao et al., 2025) to generate the attribute values.Additionally, we construct a separate batch of out-of-distribution attribute values and incorporate them into the model training, to enhance the model's ability to discover and predict unknown attribute values.</p>
<p>Our contributions are threefold: (1) We introduce a Multi-Value-Product Retrieval-Augmented Generation (MVP-RAG) method for the PAVI task.This approach addresses the limitations of existing PAVI methods, which often suffer from cascading errors, inability to handle out-of-distribution (OOD) attribute values, and lack of generalization capability.(2) We incorporate multi-retrieval techniques into PAVI framework, using TACLR for attribute value retrieval and universal representation models for product retrieval.The retrieved attribute values and product information provide valuable contextual cues to guide the subsequent attribute value generation.(3) We validate the effectiveness of MVP-RAG through extensive experiments on proprietary datasets.In addition, MVP-RAG has been successfully deployed in a real-world industrial environment.</p>
<p>Approach</p>
<p>As shown in Fig3, the overall workflow of MVP-RAG can be divided into two main components: multi-level attribute value-product retrieval ($2.1) and attribute value generation ($2.2).</p>
<p>Multi-level Attribute Value-Product Retrieval</p>
<p>The main purpose of the multi-level attribute valueproduct retrieval in MVP-RAG is to provide candidate attribute values for the subsequent attribute value generation, as well as to offer product-level few-shot information to guide the selection and generation process, as shown in Fig3</p>
<p>Attribute Value Retrieval</p>
<p>In the standard retrieval task (Schütze et al., 2008), given a query, the goal is to retrieve a set of relevant documents from the corpus.Correspondingly, we treat product information as the query and the standard attribute values as the corpus, and retrieve the most relevant attribute values based on the product information.TACLR (Su et al., 2025) is the current state-of-the-art approach for the PAVI task based on retrieval, and we follow its setup.For each product, we concatenate the product title t and description d into a sentence in the format title description as the input.For each attribute value, we fully utilize the attribute tree information and construct a prompt in the format a category with attribute being value as the corpus input.</p>
<p>After constructing the product queries, we leverage the TACLR encoder to obtain the vector representations.Then, for each product query, we calculate the similarity with the attribute values under its corresponding category, and select the top-k candidates as the results.</p>
<p>Product Retrieval</p>
<p>Attribute value retrieval can provide a candidate set for attribute value generation, but accurately selecting the most appropriate attribute value from these candidates remains a challenge.Product-level few-shot examples can provide reference cases to guide the model (OpenAI et al., 2024) in selecting the answers.</p>
<p>In a straightforward approach, we employ vector retrieval for similar product search.Specifically, we utilize the general-purpose representation model BGE (Xiao et al., 2024), to encode products into vector embeddings.We then compute the cosine similarity between the query product and a candidate product pool to select similar items.Furthermore, to ensure the consistency of attribute items, we restrict product retrieval within the same category.</p>
<p>Attribute Value Generation</p>
<p>After completing attribute value retrieval and product retrieval, we integrate them into a unified template as training data.The combined prompt consists of five parts: task definition, note, similar products in the same category, product information, and candidate attribute values.The specific details are shown in the appendixA.</p>
<p>We choose Qwen2.5 (Qwen et al., 2025) as the base model, leveraging their strong performance and robust open-source ecosystem.The training objective of the model is the classic next-word prediction task.Specifically, we exclude the loss computation of the prompt prefix, and instead focus on the special tokens and model output tokens.
L = − T t=1 log P (y t |y &lt;t , X)
Here, X represents the input to the model, y represents the generated token, t represents the position of the generation, and T represents the final generated sequence.</p>
<p>Experiments</p>
<p>Data</p>
<p>We evaluate our model on the large-scale product attribute recognition dataset Xianyu-PAVI.This dataset is sourced from the second-hand ecommerce platform Xianyu, where each product goes through a three-step process of model prediction, seller feedback, and manual review for attribute value assignment.The dataset contains 8,803 product categories, 26,645 category-attribute pairs, and 6.3 million category-attribute-attribute value triples.On average, each category has 3 attributes and 716 attribute values.The statistics of the dataset are shown in Tab 1.</p>
<p>Metrics</p>
<p>Following TACLR (Su et al., 2025), we use microaveraged precision@1, recall@1 and F1 score@1 to evaluate PAVI methods.For each attribute, the ground truth is a set of values V from the taxonomy.If the ground truth set is empty, a correct prediction (True Negative, TN) occurs when the model also predicts an empty set; otherwise, it is a False Positive (FP).When the ground truth set is not empty, the model's top-1 output is a True Positive (TP) if it matches any ground truth value.Predicting an empty set in this case results in a False Negative (FN), while mismatched predictions are both False Positives (FP) and False Negatives (FN) † , as it simultaneously introduces an error and misses the correct value.Table 3 summarizes these outcomes.Final precision, recall, and F1 scores are computed by aggregating TP, FP, and FN counts across the dataset, providing a comprehensive performance evaluation.</p>
<p>In addition, for nonempty set of ground-truth attribute values, we introduce the Coverage metric to quantify the overlap between the retrieved attribute values and the ground-truth set; coverage is deemed achieved when the intersection of the two sets is nonempty.</p>
<p>Baselines</p>
<p>We compare our model with the one-stage PAVI methods, including classification, generation, and the current state-of-the-art retrieval-based methods:</p>
<p>• BERT-CLS This model (Chen et al., 2022) treats PAVI as a multi-label classification task, where each attribute value is considered an independent label.It also performs masking on irrelevant attribute values based on the product category.</p>
<p>• LLM The basic LLM model treats PAVI as a generative task, learning the product attribute vocabulary during training.It then uses the product title description as input and the attribute-attribute value pairs as output.</p>
<p>• Product-RAG Building upon the LLM model, RAG recalls relevant products to provide more contextual information for attribute value identification.</p>
<p>• TACLR The current state-of-the-art PAVI model, which treats PAVI as a retrieval task.It combines contrastive learning and adaptive thresholding techniques to select attribute values by computing the similarity between the product description and the attribute values.</p>
<p>Implementation Details</p>
<p>Retrieve Module.For retrieving similar products, we employ BGE-base (Xiao et al., 2024)</p>
<p>Results</p>
<p>Main Result</p>
<p>Tab 2. shows the comparative results of our model on the Xianyu-PAVI dataset.And MVP-RAG outperforms all the baselines, achieving the state-ofthe-art F1-score on Xianyu-PAVI.We attribute the excellent performance of MVP-RAG to two tages: One is the retrieval-based candidate value generation which ensures the comprehensiveness of the attribute values, the other is incorporating similar products with corresponding attribute values from the same category as supplementary inputs to further improve the accuracy of the identification.The combination of these two strategies ensure both the completeness and accuracy of the attribute value recognition.Specifically, MVP-RAG outperforms the product-retrieval-based methods such as Qwen2.5(Product-RAG)by 26.3% on F1score and outperforms the attribute-retrieval-based methods like previous SOTA TACLR by 3.3%.</p>
<p>Analysis</p>
<p>Impact of Product Counts</p>
<p>Fig 4 .compares the impact on results as the number of retrieved products from the same category is varied.As the number of retrieved same-category products increases, the coverage of the test set's attribute values rises from 56.3% to 78.5% and gradually stabilizes.In contrast, the model's F1 score exhibits relatively minor fluctuations.This is because as the coverage increases with more similar product retrieval, attribute value generation becomes easier, leading to improved recall.</p>
<p>Impact of Attribute Value Counts</p>
<p>Fig 5 .compares the impact on results as the number of retrieved attribute values is varied.As the number of retrieved attribute values increases, the coverage of the test set's true attribute values gradually improves (94.9% -&gt; 99.6%), approaching 100%.The model's recall first increases and then decreases with the number of candidate attribute values, while precision declines overall.F1 and recall exhibit a similar pattern of initial increase followed by decrease, with the F1 score peaking at 89.2% when the number of candidate values is 6.Clearly, as the number of candidate values increases, more information is introduced, with the initial information supplementation eventually leading to misinformation.Additionally, it is worth noting that compared to the TACLR retrieval model that provides candidate values, the model's precision improved by nearly 8% (85.4% -&gt; 93.2%), indicating that MVP-RAG is effective and leads to a significant enhancement in the online user experience.</p>
<p>Accuracy Of Attribute Values And Products</p>
<p>As the number of recalled attribute values increases, the coverage of true attribute values gradually increases.When the true result appears in the candidate attribute value, f1 can reach 92.6%, which is 6.3% higher than when it does not appear.At the same time, we control the number of recalled products to be 4, and analyze whether the accuracy of the attribute value of the recalled product has an impact on the results.For simple attributes with clear distinguishing meanings, such as brand and model, when the attribute value of the recalled product is 75% wrong, MVP-RAG can still return the correct attribute value; when the error is higher than 75%, MVP-RAG has a probability of returning an incorrect attribute value.At the same time, thinking models such as DeepSeek will first analyze whether the attribute value of the given retrieved product is wrong, and then make corrections to predict the attribute value.For attributes that do not have clear meanings, such as color function, MVP-RAG focuses on the product's own information and can still return the correct result when the attribute value of the recalled product is wrong.</p>
<p>DeepSeek focuses on analyzing the information of the recalled product and is more inclined to be consistent with the attributes of the recalled product.</p>
<p>The specific details are shown in the appendixB.Retrieval-Augmented Generation Retrievalbased approaches (Leto et al., 2024) have been widely demonstrated to be highly effective for knowledge-intensive tasks, particularly in output-constrained settings, where the introduction of relevant knowledge can significantly mitigate the issue of hallucination.(Sabeh et al., 2024) incorporated Retrieval-Augmented Generation (RAG) technologies in the PAVI task, utilizing a pre-trained T5 (Raffel et al., 2023) to retrieve similar products and leverage the information to enhance the generation, outperforming direct attribute value generation methods by a substantial margin.In contrast, the unique contribution of current research lies in its utilization of both product-level and attribute-value-level information for the retrieval-augmentation process.By successfully applying this approach in the e-commerce domain, the proposed model is able to address the hallucination problem while also improving performance on the specific task at hand.</p>
<p>Conclusion</p>
<p>In this work, we present multi-value-product retrieval-augmented generation (MVP-RAG) method for the PAVI task.This approach addresses the limitations of existing PAVI methods, which often suffer from cascading errors, inability to handle out-of-distribution (OOD) attribute values, and lack of generalization capability.</p>
<p>Comprehensive experiments on proprietary and public datasets demonstrated MVP-RAG's superiority over classification-and generation-based baselines.Notably, method achieved an F1 score of 89.5% on the large-scale Xianyu-PAVI dataset.Beyond these experimental results, MVP-RAG has been successfully deployed on the real-world ecommerce platform Xianyu, processing millions of product listings daily and seamlessly adapting to dynamic attribute taxonomies, making it a practical solution for large-scale industrial applications.</p>
<p>Limitations</p>
<p>MVP-RAG can accurately identify product attribute values, but there are still many limitations.First, only text information is used, and image or video information of the product has not been used.Multimodal input can provide information for attributes that are difficult to infer from text alone (such as color, material, or shape).Second, the reasoning output based on LLM takes a long time, and the model still needs to be accelerated and optimized in the future.</p>
<p>Figure 2 :
2
Figure 2: An illustration of a portion of the attribute taxonomy.Each category, such as Bag, is linked to multiple attributes, including Brand, Serie, and Size, with standardized values enumerated for each attribute (e.g., LV, Dior, and Channel for Brand).</p>
<p>Figure 3 :
3
Figure 3: Pipeline of MVP-RAG</p>
<p>Figure 5 :
5
Figure 5: Performance analysis across Rag Value Counts</p>
<p>Table 1 :
1
Statistics of the Xianyu-PAVI.
StatisticTrainValidTest# Products809,52881,69985,024# PA Pairs3,584,462 358,582 458,954# Null Pairs 2,345,577 228,534 272,285</p>
<p>Table 2 :
2
Performance comparison of classification, generation, and retrieval methods on Xianyu-PAVI Methods BERT-CLS Qwen2.5 (Product-RAG) Qwen2.5 (fine-tune) TACLR MVP-RAG(ours)
pre50.958.384.585.493.8recall69.150.179.187.185.3f150.563.281.786.289.5
†  In previous work(Shinzato et al., 2023), evaluation metrics did not explicitly consider the false positive (FP) case, and instances involving both false positives (FP) and false negatives (FN) were categorized solely as false positives. In contrast, we employ more rigorous evaluation metrics that independently account for both FP and FN cases.
A Example AppendixTask Description Given the product description, category, similar same category product, attribute set, and candidate attribute values for each attribute, generate the attribute values of the product.Unrecognizable attributes can be returned as unknown.Note 1.The attribute value of the product does not necessarily appear in the reference product attribute value and candidate attribute value 2. If the attribute value of the product exists but does not appear in the reference product attribute value and candidate attribute value, it can be generated, but it is best not to do so 3.If the attribute value of the product does not exist, return None 4. The probability of the given candidate attribute value decreases from the front to the back Reference product information Product description: 2 Sony E-mountReadyStockChanel 23a black denim cc jacket size 36 Brand new full set, direct style can be sent for free inspection within 3-5 working days, direct style delivery, serious buyers only.Explicit Attribute-Brand MVP-RAG When the brand of the given three products is changed to Dior, the result returned is always Chanel.However, when the brand of all four products is changed to Dior, the result returned is Dior.DeepSeek rethink First, there are four examples in the reference product information provided by the user.Although "Chanel" is mentioned in the product description, the brand in the attribute value is marked as "Dior", which is obviously inconsistent.This may be an error or a test case.However, according to the task description, I need to generate attribute values based on the given information and return unknown when it cannot be recognized.The description of the target product is "Chanel New Sale S Engraved 19 Black Gold Small", which clearly men-tions "Chanel", so the brand should be "chanel".But the brands in the reference examples are all Dior.Here we need to judge whether to follow the error in the reference example or fill in the brand correctly according to the product description.According to the second point of the task description, if the attribute value exists but does not appear in the reference, it can be generated but it is better not to.But here the brand is clearly in the description, so it should be filled in correctly instead of following the wrong example.AnswerNo matter how many wrong results are given, it can always return to Chanel.Explicit Attribute-Quality MVP-RAG Always returns a new attribute value that is consistent with the correct result.DeepSeek rethink Let's first look at the information of the reference products.The description of the first product mentions "basically new", and the condition in the attributes is "almost new".The description of the second product says "99% new", and the condition is also "almost new".The descriptions of the third and fourth products mention "out" and "brand new complete set" respectively, and the condition is also "almost new".It seems that no matter whether the description is "basically new", "99% new" or "brand new", the condition is indicated by "almost new" in the attributes.Is this because in the secondhand or resale market, these descriptions are all classified as almost new, rather than brand new?Or it may be a unified processing method in the task, so even if the description says "brand new", the attributes still use "almost new".Answer When almost new appears, DeepSeek will tend to give almost new results.
Kang Chen, Qingheng Zhang, Chengbao Lian, Yixin Ji, Xuwei Liu, Shuguang Han, Guoqiang Wu, Fei Huang, Jufeng Chen, arXiv:2410.16977Ipl: Leveraging multimodal large language models for intelligent product listing. 2024arXiv preprint</p>
<p>Extreme multi-label classification with label masking for product attribute value extraction. Wei-Te Chen, Yandi Xia, Keiji Shinzato, 10.18653/v1/2022.ecnlp-1.16Proceedings of the Fifth Workshop on e-Commerce and NLP (EC-NLP 5). the Fifth Workshop on e-Commerce and NLP (EC-NLP 5)Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Product titles-toattributes as a text-to-text task. Gilad Fuchs, Yoni Acriche, 10.18653/v1/2022.ecnlp-1.12Proceedings of the Fifth Workshop on e-Commerce and NLP (ECNLP 5). the Fifth Workshop on e-Commerce and NLP (ECNLP 5)Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Product-aware answer generation in e-commerce question-answering. Shen Gao, Zhaochun Ren, Yihong Zhao, Dongyan Zhao, Dawei Yin, Rui Yan, Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining. the Twelfth ACM International Conference on Web Search and Data Mining2019</p>
<p>The importance of circular attributes for consumer choice of fashion and textile products in australia. Sustainable Production and Consumption. Celine Klemm, Stefan Kaufman, 202445</p>
<p>Toward optimal search and retrieval for rag. Alexandria Leto, Cecilia Aguerrebere, Ishwar Bhati, Ted Willke, Mariano Tepper, Ai Vy, Vo, arXiv:2411.073962024Preprint</p>
<p>Sage: Structured attribute value generation for billion-scale product catalogs. N Athanasios, Swati Nikolakopoulos, Kaul, Karthik Siva, Bella Gade, Umit Dubrov, Suleiman Batur, Ali Khan, arXiv:2309.059202023Preprint</p>
<p>Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Red Anadkat, Igor Avila, Suchir Babuschkin, Valerie Balaji, Paul Balcom, Baltescu, arXiv:2303.08774Haiming Bao, Mohammad Bavarian, Jeff Belgum, and 262 others. 2024. Gpt-4 technical report. Preprint</p>
<dl>
<dt>Bootstrapped named entity recognition for product attribute extraction. Duangmanee Putthividhya, Junling Hu, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. the 2011 Conference on Empirical Methods in Natural Language ProcessingEdinburgh, ScotlandUK. Association for Computational Linguistics2011</dt>
<dd>
<p>Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, arXiv:2412.15115Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint</p>
</dd>
</dl>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.106832023Preprint</p>
<p>Barbara Plank, and Johann Gamper. 2024. An empirical comparison of generative approaches for product attribute-value identification. Kassem Sabeh, Robert Litschko, Mouna Kacimi, arXiv:2407.01137Preprint</p>
<p>Introduction to information retrieval. Hinrich Schütze, Christopher D Manning, Prabhakar Raghavan, 2008Cambridge University Press Cambridge39</p>
<p>A unified generative approach to product attribute-value identification. Keiji Shinzato, Naoki Yoshinaga, Yandi Xia, Wei-Te Chen, 10.18653/v1/2023.findings-acl.413Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Taclr: A scalable and efficient retrieval-based method for industrial product attribute value identification. Yindu Su, Huike Zou, Lin Sun, Ting Zhang, Haiyang Yang, Liyu Chen, David Lo, Qingheng Zhang, Shuguang Han, Jufeng Chen, arXiv:2501.038352025arXiv preprint</p>
<p>Ampsum: Adaptive multiple-product summarization towards improving recommendation captions. Quoc-Tuan Truong, Tong Zhao, Changhe Yuan, Jin Li, Jim Chan, Soo-Min Pantel, Hady W Lauw, Proceedings of the ACM Web Conference 2022. the ACM Web Conference 20222022</p>
<p>Enhancing customer experience through ai-enabled content personalization in e-commerce marketing. Tarun Kumar Vashishth, Krishan Kewal, Bhupendra Sharma, Sachin Kumar, Chaudhary, Advances in digital marketing in the era of artificial intelligence. 2024Rajneesh Panwar, and 1 others</p>
<p>Learning to extract attribute value from product via question answering: A multi-task approach. Qifan Wang, Li Yang, Bhargav Kanagal, Sumit Sanghai, D Sivakumar, Bin Shu, Zac Yu, Jon Elsas, 10.1145/3394486.3403047KDD '20. New York, NY, USAAssociation for Computing Machinery2020</p>
<p>C-pack: Packed resources for general chinese embeddings. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, Jian-Yun Nie, Proceedings of the 47th international ACM SIGIR conference on research and development in information retrieval. the 47th international ACM SIGIR conference on research and development in information retrieval2024</p>
<p>Scaling up open tagging from tens to thousands: Comprehension empowered attribute value extraction from product title. Huimin Xu, Wenting Wang, Xin Mao, Xinyu Jiang, Man Lan, 10.18653/v1/P19-1514Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>AdaTag: Multi-attribute value extraction from product profiles with adaptive decoding. Jun Yan, Nasser Zalmout, Yan Liang, Christan Grant, Xiang Ren, Xin Luna, Dong , 10.18653/v1/2021.acl-long.362Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>MixPAVE: Mix-prompt tuning for few-shot product attribute value extraction. Li Yang, Qifan Wang, Jingang Wang, Xiaojun Quan, Fuli Feng, Yu Chen, Madian Khabsa, Sinong Wang, Zenglin Xu, Dongfang Liu, 10.18653/v1/2023.findings-acl.633Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Mave: A product dataset for multi-source attribute value extraction. Li Yang, Qifan Wang, Zac Yu, Anand Kulkarni, Sumit Sanghai, Bin Shu, Jon Elsas, Bhargav Kanagal, Proceedings of the fifteenth ACM international conference on web search and data mining. the fifteenth ACM international conference on web search and data mining2022</p>
<p>Queaco: Borrowing treasures from weakly-labeled behavior data for query attribute value extraction. Danqing Zhang, Zheng Li, Tianyu Cao, Chen Luo, Tony Wu, Hanqing Lu, Yiwei Song, Bing Yin, Tuo Zhao, Qiang Yang, 10.1145/3459637.3481946Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management, CIKM '21. the 30th ACM International Conference on Information &amp; Knowledge Management, CIKM '21ACM2021</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Li, arXiv:2303.18223Zikang Liu, and 3 others. 2025. A survey of large language models. Xinyu TangPreprint</p>
<p>Opentag: Open attribute value extraction from product profiles. Guineng Zheng, Subhabrata Mukherjee, Xin Luna Dong, Feifei Li, 10.1145/3219819.3219839Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD '18. the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD '18New York, NY, USAAssociation for Computing Machinery2018</p>            </div>
        </div>

    </div>
</body>
</html>
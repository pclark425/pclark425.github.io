<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1239 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1239</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1239</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-247187721</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2203.00494v1.pdf" target="_blank">DreamingV2: Reinforcement Learning with Discrete World Models without Reconstruction</a></p>
                <p><strong>Paper Abstract:</strong> The present paper proposes a novel reinforcement learning method with world models, DreamingV2, a collaborative extension of DreamerV2 and Dreaming. DreamerV2 is a cutting-edge model-based reinforcement learning from pixels that uses discrete world models to represent latent states with categorical variables. Dreaming is also a form of reinforcement learning from pixels that attempts to avoid the autoencoding process in general world model training by involving a reconstruction-free contrastive learning objective. The proposed DreamingV2 is a novel approach of adopting both the discrete representation of DreamingV2 and the reconstruction-free objective of Dreaming. Compared to DreamerV2 and other recent model-based methods without reconstruction, DreamingV2 achieves the best scores on five simulated challenging 3D robot arm tasks. We believe that DreamingV2 will be a reliable solution for robot learning since its discrete representation is suitable to describe discontinuous environments, and the reconstruction-free fashion well manages complex vision observations.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1239.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1239.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamingV2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamingV2: Reinforcement Learning with Discrete World Models without Reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL system that combines discrete categorical latent state representations (as in DreamerV2) with reconstruction-free contrastive training (as in Dreaming), trained using an InfoNCE objective plus auxiliary contrastive loss and RSSM dynamics; targeted at pixel-based, contact-rich 3D robotic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamingV2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RSSM-based latent world model where the stochastic latent z is a set of 32 categorical variables each with 32 categories (i.e., a 32x32 PMF / 1024-dim one-hot space). The inference model q(z_t|h_t,x_t) and generative p(z_t|h_t) are learned; contrastive critic p(z|x) is parameterized as linear logits (W e_t) with softmax to produce PMFs. Trained without pixel reconstruction using multi-step InfoNCE (J_NCE_k) plus KL overshooting and an auxiliary contrastive objective J_NCE_aux; uses straight-through gradients for discrete backprop, momentum encoder, and random-crop augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (discrete categorical RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>3D robotic manipulation and control from pixels (robot-arm tasks, DeepMind Control Suite, RoboSuite with iGibson rendering)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Contrastive InfoNCE classification accuracy / softmax cross-entropy on predicted discrete latents vs PMFs; KL divergence overshooting term (ELBO's KL component) used as regularizer; no pixel reconstruction loss (no MSE/reconstruction log-likelihood). Task performance (episode reward) used as downstream fidelity-to-task metric.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No explicit MSE/reconstruction metrics reported (reconstruction-free). Downstream task scores at 500K steps: UR5-reach 776 ± 194, Reach-duplo 199 ± 43, Lift 327 ± 150, Door 383 ± 143, PegInHole 436 ± 26 (episode return units). Optimization horizon K set to 3; auxiliary contrastive objective required to make J_NCE_k trainable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable: discrete categorical latents show interpretable structure; visualization (Fig.5) highlights at least three categorical units whose activations align with subtask phases (approach, grasp, lift) during Lift task, indicating some latent dimensions map to semantically meaningful phases.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of latent trajectories and time-series of selected categorical variables; inspection of which categorical units activate during task phases (manual selection/plotting).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Implemented to run on a single NVIDIA V100; discrete representation increases latent dimensionality (1024-dim one-hot) and slows training/inference ~1.7x relative to DreamerV2; momentum encoder and stop-gradient used to reduce memory from O((B×K)^2) to O(B×K). Hyperparameters reported: K=3, momentum coefficient η=0.05.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to DreamerV2: DreamingV2 is ~1.7× slower due to higher latent dimensionality but yields better task performance on complex 3D/contact-rich tasks. Momentum encoder reduces memory requirements vs naive contrastive implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Outperforms baselines on five simulated 3D robot-arm tasks (scores at 500K steps listed): UR5-reach 776±194, Reach-duplo 199±43, Lift 327±150, Door 383±143, PegInHole 436±26; only DreamingV2 solved several tasks within 500K steps where others did not.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High task utility on contact-rich, discontinuous dynamics: discrete categorical latents better capture multimodal and discontinuous transitions and subtasks (phases), enabling superior policy learning on 3D manipulation; reconstruction-free contrastive training reduces object-vanishing and distractor sensitivity, improving robustness for complex visual observations. However, discrete latents hurt performance on certain smooth 2D manipulation tasks (e.g., Finger-turn).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Discrete + reconstruction-free: improved task utility and robustness in complex, contact-rich 3D tasks at the cost of higher latent dimensionality and 1.7× slower computation; reconstruction-free avoids object-vanishing but may lose some information (e.g., velocity) relevant in robot-centric 2D tasks; continuous latents with reconstruction can be better for tasks that are smooth and low-dimensional.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>32 categorical variables × 32 categories (1024-dim), straight-through gradients for discrete sampling, RSSM auxiliary dynamics for multi-step predictions, InfoNCE multi-step contrastive loss J_NCE_k with K up to 3, auxiliary contrastive loss J_NCE_aux (MoCo-like) to stabilize encoder & critic training, momentum encoder (η=0.05), random-crop augmentation, linear critic W e_t producing PMFs via softmax.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against DreamerV2 (discrete with reconstruction), Dreaming (continuous + reconstruction-free), Dreamer (continuous + reconstruction); DreamingV2 outperforms these on 3D contact-rich tasks by combining discrete latents and reconstruction-free training. Dreaming (continuous + contrastive) can outperform on some 2D manipulation tasks due to better continuous representations for polar/angular spaces. DreamerV2 and other reconstruction-based methods suffer from object-vanishing and distractor sensitivity on visually complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Authors recommend multi-step prediction horizon K in {2,3} (performance saturates), momentum encoder with small η (used 0.05), and the auxiliary contrastive objective J_NCE_aux to bootstrap training. They suggest discrete+reconstruction-free as a good configuration for contact-rich 3D tasks but acknowledge hybrid continuous+discrete representations and using negative-sample-free objectives (BYOL/Barlow Twins) as promising directions to reduce compute and preserve task modalities.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1239.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1239.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV2 (Mastering Atari with Discrete World Models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A leading RSSM-based model-based RL method that represents stochastic latent states as categorical (discrete) variables and trains using a variational ELBO with reconstruction loss; effective on Atari and other benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari with discrete world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RSSM with discrete stochastic latent variables (categorical), deterministic hidden state via GRU; trained with ELBO objective including a reconstruction log-likelihood p(x_t|h_t,z_t) and KL regularization; used for latent imagination to train policies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (discrete categorical RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari benchmark, general model-based RL from pixels; used as baseline on robotic manipulation tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction likelihood / ELBO (pixel log-likelihood) and KL divergence; downstream task return.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>On the paper's robot-arm benchmarks at 500K steps: UR5-reach 704 ± 222, Reach-duplo 149 ± 62, Lift 165 ± 126, Door 190 ± 126, PegInHole 376 ± 59 (episode return units). Note: DreamerV2 was outperformed by DreamingV2 on complex 3D tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Discrete latents intended to capture multimodal uncertainty; explicit interpretability not demonstrated in this paper (object vanishing and reconstruction artifacts are noted as limitations of reconstruction-based training).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None specific in this paper; standard inspection of learned latents possible but not emphasized here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>More computationally efficient than DreamingV2 in experiments (~1/1.7 the time) due to lower latent dimensionality (discrete representation in DreamerV2 uses less expanded one-hot encoding in this implementation).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Faster than DreamingV2; tends to struggle on complex visual observations due to reconstruction burden and object-vanishing, reducing task efficiency in visually complex domains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Strong on Atari and many benchmarks historically; on 3D robot-arm tasks here it performs worse than DreamingV2 on several tasks due to reconstruction sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Discrete latents help represent multimodal transitions but reconstruction objective leads to sensitivity to small, task-relevant objects and distractors, reducing downstream policy performance in some visually complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Uses discrete latents to capture multimodality but retains reconstruction costs and associated brittleness; computationally efficient but less robust to visual complexity compared to reconstruction-free variants.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Discrete categorical z, ELBO reconstruction-based training, KL balancing and policy entropy regularization (mentioned as components inherited by DreamingV2 implementation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to reconstruction-free methods (Dreaming, DreamingV2), DreamerV2 often lags on visually complex tasks because of reconstruction limitations but can perform well in other domains (e.g., Atari, some locomotion tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not newly proposed here; DreamerV2 hyperparameters mostly inherited. Authors used DreamerV2 baseline with original policy and hyperparameters for fair comparison.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1239.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1239.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreaming</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreaming: Model-based reinforcement learning by latent imagination without reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reconstruction-free RSSM training approach that replaces pixel reconstruction with contrastive InfoNCE objectives (plus KL overshooting), training latent dynamics by matching predicted latents to PMFs estimated from observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dreaming: Model-based reinforcement learning by latent imagination without reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreaming (contrastive RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RSSM with continuous stochastic latent variables (distributed on hypersphere via von Mises-Fisher-like critic in original Dreaming) trained with multi-step InfoNCE contrastive objectives J_NCE_k and KL overshooting, avoiding pixel reconstruction entirely.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (continuous latent, reconstruction-free)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pixel-based RL across robot tasks and DMC; compared on robot-arm and 2D/3D control tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>InfoNCE softmax cross-entropy (contrastive accuracy) between predicted latents and critic-produced logits; KL overshooting regularizer; downstream episode return.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>On robot-arm benchmarks at 500K steps: UR5-reach 752 ± 1178, Reach-duplo 145 ± 61, Lift 174 ± 107, Door 319 ± 173, PegInHole 353 ± 50 (episode return units). Ranked second in some tasks; performed worse than DreamingV2 on contact-rich tasks requiring discontinuous dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Continuous latents are distributed on hypersphere (von Mises-Fisher analogy); interpretability not emphasized beyond geometric interpretations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Use of cosine similarity logits (z_t W e_t) gives an implicit geometric/hyperspherical structure; no explicit latent-phase visualizations reported here for Dreaming (those shown for DreamingV2).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Generally efficient; contrastive learning with multi-step comparisons suffers O((B×K)^2) memory without momentum encoder; original Dreaming used a linear auxiliary dynamics (less compute for auxiliary prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Faster than DreamingV2 in some settings due to lower latent dimensionality (continuous representation), but fails to capture discontinuous/contact-rich dynamics which harms sample efficiency for those tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Strong on simple 2D manipulation tasks (e.g., Finger-turn, Reacher), ranking best on several 2D tasks; weaker on contact-rich 3D tasks where discrete representation helps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Reconstruction-free training improves handling of complex visual inputs (avoids object-vanishing and distractors), but continuous latents are ill-suited for representing discontinuous/contact-rich dynamics and subtasks, limiting performance on certain robot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Contrastive (reconstruction-free) learning increases robustness to visual complexity but combined with continuous latents may fail on discontinuous dynamics; linear auxiliary dynamics help constrain temporally nearby latents but may be less expressive than RSSM-based auxiliary dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Continuous latent (hypersphere-like), InfoNCE multi-step objective, linear auxiliary dynamics for short-horizon predictions in original Dreaming, random-crop augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms reconstruction-based methods on some complex-visual tasks but is outperformed by DreamingV2 on contact-rich tasks due to inability of continuous latents to represent discontinuous subtasks well.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Original Dreaming used linear auxiliary dynamics to stabilize contrastive learning; paper notes DreamingV2 needed an auxiliary contrastive objective (J_NCE_aux) when using discrete latents and RSSM auxiliary dynamics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1239.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1239.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer (Dream to Control: Learning behaviors by latent imagination)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier RSSM-based model-based RL that uses Gaussian stochastic latents and trains with an ELBO reconstruction objective; imagines latent rollouts for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer (original)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RSSM using Gaussian (continuous) stochastic latents plus deterministic GRU hidden state; trained by ELBO with pixel reconstruction; uses latent imagination to optimize policies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (continuous VAE-style RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based RL from pixels across various simulated tasks; used as a baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO reconstruction log-likelihood and KL divergence; downstream episode return.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In the paper's benchmarks at 500K steps: UR5-reach 701 ± 223, Reach-duplo 5 ± 11, Lift 134 ± 46, Door 154 ± 32, PegInHole 354 ± 47 (episode return units). Generally underperforms on several complex 3D/contact-rich tasks compared to DreamingV2.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Standard continuous latent VAE-style representations; no special interpretability claimed here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned in this paper beyond standard latent inspections.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Comparable to DreamerV2 in implementations; reconstruction requires decoder and pixel-loss computation.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Reconstruction objective can hamper robustness to visual distractions and small objects (object vanishing), reducing sample efficiency in visually complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Solid baselines in many domains but performs worse in visually complex and contact-rich tasks due to reconstruction limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Good where reconstruction is feasible and observations are easier to model; struggles when pixel reconstruction is hard or when critical small objects vanish in reconstructions.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Reconstruction provides dense training signal but suffers from object-vanishing and distractor sensitivity; continuous latent representation can be insufficient for discontinuous dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Gaussian stochastic latents, ELBO-based training with reconstruction loss, latent imagination for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperformed by DreamingV2 on complex 3D manipulation tasks; in some tasks Dreaming (reconstruction-free continuous) or DreamerV2 (discrete+reconstruction) may perform differently depending on task properties.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1239.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1239.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RSSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent State Space Model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent time-series generative model combining deterministic recurrent state (GRU) and stochastic latent variables (either Gaussian or categorical), used to represent world dynamics for model-based RL by imagining latent rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning latent dynamics for planning from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RSSM (Recurrent State Space Model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent tuple (z_t, h_t) with deterministic hidden state h_t = f_GRU(h_{t-1}, z_{t-1}, a_{t-1}) and stochastic z_t ~ p(z_t | h_t); observation model p(x_t | h_t, z_t) and inference q(z_t | h_t, x_t). Supports either Gaussian continuous z or categorical discrete z as implemented in various instantiations (Dreamer, DreamerV2, DreamingV2).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recurrent latent dynamics / generative model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General model-based RL from pixels (Atari, DeepMind Control, robotic manipulation).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO components: reconstruction likelihood (when used) and KL divergence between posterior and prior; in reconstruction-free variants, contrastive InfoNCE replaces reconstruction term and KL overshooting retained.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Performance depends on instantiation (Dreamer/DreamerV2/Dreaming/DreamingV2). Used as core dynamics model enabling multi-step predictions for policy learning; in DreamingV2 RSSM auxiliary dynamics improved performance over linear auxiliary dynamics on several tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Interpretability depends on latent choice: categorical z can yield phase-like interpretable units (observed in DreamingV2), continuous z may map to geometric properties but less discrete phase structure.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent trajectory visualization, inspection of categorical unit activations over time (used in DreamingV2).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Compute depends on latent representation and auxiliary objectives; RSSM with categorical 1024-dim PMFs is more expensive than continuous Gaussian variants; multi-step contrastive comparisons increase memory unless mitigated (momentum encoder & stop-grad).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>RSSM + discrete latents yields stronger performance on discontinuous tasks at higher computational cost; using RSSM as auxiliary dynamics (vs linear) provided better results with discrete latents in ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Enables state-of-the-art or competitive policy learning depending on representation and training objective; DreamingV2 (RSSM+categorical+contrastive) achieved best results on 3D robot-arm tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>RSSM provides a flexible backbone; its utility is highly influenced by (a) whether z is discrete vs continuous and (b) whether training uses reconstruction or contrastive objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Choice of RSSM instantiation trades off fidelity, robustness, and compute: discrete latents + contrastive objectives increase robustness to visual complexity and multimodality but are more compute- and memory-intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of GRU for deterministic state, choice of stochastic latent type (categorical vs Gaussian), choice of training objective (ELBO reconstruction vs InfoNCE contrastive), and auxiliary dynamics (linear vs RSSM multi-step).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>RSSM is the common backbone across Dreamer-family methods; alternatives include non-recurrent forward models and explicit physics simulators. RSSM offers compact latent predictive dynamics suitable for latent imagination-based policy training.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1239.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1239.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerPro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerPro: Reconstruction-free model-based reinforcement learning with prototypical representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reconstruction-free world-model approach that uses prototypical/cluster-based representations to avoid pairwise contrastive comparisons; referenced and compared as a baseline in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DreamerPro: Reconstruction-free model-based reinforcement learning with prototypical representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerPro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reconstruction-free world model using prototypical (cluster) representations to remove the need for pairwise contrastive comparisons; used as a baseline in experiments comparing reconstruction-free approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (prototypical/reconstruction-free)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pixel-based RL, compared on robot-arm and control tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Prototypical assignment losses / classification-like objectives instead of pairwise contrastive InfoNCE; downstream episode return.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported in the comparison table: performs worse than DreamingV2 on several robot-arm tasks (e.g., UR5-reach 668 ± 252 at 500K steps).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Cluster/prototype representations may offer some interpretability (discrete prototypes), but no detailed interpretability analysis presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not detailed in this paper (refer to original DreamerPro reference).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Prototypical approach reduces pairwise negative comparisons, potentially decreasing compute compared to pairwise contrastive methods.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Designed to be more efficient than heavy contrastive pairwise methods; nonetheless, in experiments DreamingV2 outperforms it on many complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Moderate performance on 3D robot-arm tasks; lower than DreamingV2 on most reported benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Prototype-based representation reduces computational burden of contrastive learning but may lose fine-grained task discriminative power in some contact-rich scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Prototype methods trade off pairwise contrastive richness for computational efficiency; this can decrease performance on nuanced tasks requiring fine-grained latent distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Prototypical quantization / clustering of representations to avoid pairwise negatives; reconstruction-free training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared unfavorably to DreamingV2 in several complex 3D tasks in this paper, though it may be more efficient computationally.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1239.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1239.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TPC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal Predictive Coding for model-based planning in latent space</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reconstruction-free world-model method that uses temporal predictive coding (contrastive) objectives for learning latent dynamics; included as a baseline comparison here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Temporal predictive coding for model-based planning in latent space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TPC</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastive predictive coding style world model focused on temporal predictions in latent space; uses q(z_t|h_t,x_t) as critic in some variants; reconstruction-free.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (contrastive / reconstruction-free)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based RL from pixels; evaluated here on robot-arm and control tasks as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Contrastive InfoNCE / temporal predictive coding losses; downstream returns.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In table: e.g., UR5-reach 555 ± 200 at 500K steps (episode return units); generally lower than DreamingV2 on 3D/contact-rich tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not emphasized in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Contrastive objectives with temporal predictions can be memory-intensive without momentum/stop-grad; implementation details determine practical cost.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less performant than DreamingV2 on complex 3D tasks in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Lower returns on several robot-arm tasks compared to DreamingV2.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Temporal predictive contrastive learning aids learning without reconstruction, but may need appropriate latent parameterization (e.g., discrete vs continuous) to handle discontinuous dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Contrastive temporal learning improves robustness to visual complexity but can fail to capture certain dynamics or be less sample efficient in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Temporal contrastive objectives, critic choices (q vs p) vary by implementation; original TPC paper details alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Underperforms DreamingV2 on contact-rich 3D tasks; differs from DreamingV2 by latent parameterization and critic choice.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1239.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1239.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CVRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Variational model-based Reinforcement Learning (CVRL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastive, reconstruction-free world model approach designed for complex observations; mentioned as related work but not deeply evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Contrastive variational model-based reinforcement learning for complex observations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CVRL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastive variational world-model that uses contrastive objectives to learn latent representation without reconstructing pixels; tailored for complex visual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (contrastive / reconstruction-free)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pixel-based RL with complex observations; referenced as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Contrastive InfoNCE-like objectives and downstream returns (details in original CVRL paper).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported in this paper (only cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not detailed here; CVRL introduces auxiliary objectives for policy/world-model control.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not directly compared numerically in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Cited as an approach that addresses reconstruction problems for complex visual observations; specifics in original reference.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Contrastive objective for encoder/world-model representation instead of pixel reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Mentioned as a continuous-latent reconstruction-free alternative; DreamingV2 differs by using discrete latents and multi-step RSSM auxiliary dynamics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dreaming: Model-based reinforcement learning by latent imagination without reconstruction <em>(Rating: 2)</em></li>
                <li>Mastering atari with discrete world models <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>DreamerPro: Reconstruction-free model-based reinforcement learning with prototypical representations <em>(Rating: 2)</em></li>
                <li>Temporal predictive coding for model-based planning in latent space <em>(Rating: 2)</em></li>
                <li>Contrastive variational model-based reinforcement learning for complex observations <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1239",
    "paper_id": "paper-247187721",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "DreamingV2",
            "name_full": "DreamingV2: Reinforcement Learning with Discrete World Models without Reconstruction",
            "brief_description": "A model-based RL system that combines discrete categorical latent state representations (as in DreamerV2) with reconstruction-free contrastive training (as in Dreaming), trained using an InfoNCE objective plus auxiliary contrastive loss and RSSM dynamics; targeted at pixel-based, contact-rich 3D robotic tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DreamingV2",
            "model_description": "RSSM-based latent world model where the stochastic latent z is a set of 32 categorical variables each with 32 categories (i.e., a 32x32 PMF / 1024-dim one-hot space). The inference model q(z_t|h_t,x_t) and generative p(z_t|h_t) are learned; contrastive critic p(z|x) is parameterized as linear logits (W e_t) with softmax to produce PMFs. Trained without pixel reconstruction using multi-step InfoNCE (J_NCE_k) plus KL overshooting and an auxiliary contrastive objective J_NCE_aux; uses straight-through gradients for discrete backprop, momentum encoder, and random-crop augmentation.",
            "model_type": "latent world model (discrete categorical RSSM)",
            "task_domain": "3D robotic manipulation and control from pixels (robot-arm tasks, DeepMind Control Suite, RoboSuite with iGibson rendering)",
            "fidelity_metric": "Contrastive InfoNCE classification accuracy / softmax cross-entropy on predicted discrete latents vs PMFs; KL divergence overshooting term (ELBO's KL component) used as regularizer; no pixel reconstruction loss (no MSE/reconstruction log-likelihood). Task performance (episode reward) used as downstream fidelity-to-task metric.",
            "fidelity_performance": "No explicit MSE/reconstruction metrics reported (reconstruction-free). Downstream task scores at 500K steps: UR5-reach 776 ± 194, Reach-duplo 199 ± 43, Lift 327 ± 150, Door 383 ± 143, PegInHole 436 ± 26 (episode return units). Optimization horizon K set to 3; auxiliary contrastive objective required to make J_NCE_k trainable.",
            "interpretability_assessment": "Partially interpretable: discrete categorical latents show interpretable structure; visualization (Fig.5) highlights at least three categorical units whose activations align with subtask phases (approach, grasp, lift) during Lift task, indicating some latent dimensions map to semantically meaningful phases.",
            "interpretability_method": "Visualization of latent trajectories and time-series of selected categorical variables; inspection of which categorical units activate during task phases (manual selection/plotting).",
            "computational_cost": "Implemented to run on a single NVIDIA V100; discrete representation increases latent dimensionality (1024-dim one-hot) and slows training/inference ~1.7x relative to DreamerV2; momentum encoder and stop-gradient used to reduce memory from O((B×K)^2) to O(B×K). Hyperparameters reported: K=3, momentum coefficient η=0.05.",
            "efficiency_comparison": "Compared to DreamerV2: DreamingV2 is ~1.7× slower due to higher latent dimensionality but yields better task performance on complex 3D/contact-rich tasks. Momentum encoder reduces memory requirements vs naive contrastive implementation.",
            "task_performance": "Outperforms baselines on five simulated 3D robot-arm tasks (scores at 500K steps listed): UR5-reach 776±194, Reach-duplo 199±43, Lift 327±150, Door 383±143, PegInHole 436±26; only DreamingV2 solved several tasks within 500K steps where others did not.",
            "task_utility_analysis": "High task utility on contact-rich, discontinuous dynamics: discrete categorical latents better capture multimodal and discontinuous transitions and subtasks (phases), enabling superior policy learning on 3D manipulation; reconstruction-free contrastive training reduces object-vanishing and distractor sensitivity, improving robustness for complex visual observations. However, discrete latents hurt performance on certain smooth 2D manipulation tasks (e.g., Finger-turn).",
            "tradeoffs_observed": "Discrete + reconstruction-free: improved task utility and robustness in complex, contact-rich 3D tasks at the cost of higher latent dimensionality and 1.7× slower computation; reconstruction-free avoids object-vanishing but may lose some information (e.g., velocity) relevant in robot-centric 2D tasks; continuous latents with reconstruction can be better for tasks that are smooth and low-dimensional.",
            "design_choices": "32 categorical variables × 32 categories (1024-dim), straight-through gradients for discrete sampling, RSSM auxiliary dynamics for multi-step predictions, InfoNCE multi-step contrastive loss J_NCE_k with K up to 3, auxiliary contrastive loss J_NCE_aux (MoCo-like) to stabilize encoder & critic training, momentum encoder (η=0.05), random-crop augmentation, linear critic W e_t producing PMFs via softmax.",
            "comparison_to_alternatives": "Compared against DreamerV2 (discrete with reconstruction), Dreaming (continuous + reconstruction-free), Dreamer (continuous + reconstruction); DreamingV2 outperforms these on 3D contact-rich tasks by combining discrete latents and reconstruction-free training. Dreaming (continuous + contrastive) can outperform on some 2D manipulation tasks due to better continuous representations for polar/angular spaces. DreamerV2 and other reconstruction-based methods suffer from object-vanishing and distractor sensitivity on visually complex tasks.",
            "optimal_configuration": "Authors recommend multi-step prediction horizon K in {2,3} (performance saturates), momentum encoder with small η (used 0.05), and the auxiliary contrastive objective J_NCE_aux to bootstrap training. They suggest discrete+reconstruction-free as a good configuration for contact-rich 3D tasks but acknowledge hybrid continuous+discrete representations and using negative-sample-free objectives (BYOL/Barlow Twins) as promising directions to reduce compute and preserve task modalities.",
            "uuid": "e1239.0"
        },
        {
            "name_short": "DreamerV2",
            "name_full": "DreamerV2 (Mastering Atari with Discrete World Models)",
            "brief_description": "A leading RSSM-based model-based RL method that represents stochastic latent states as categorical (discrete) variables and trains using a variational ELBO with reconstruction loss; effective on Atari and other benchmarks.",
            "citation_title": "Mastering atari with discrete world models",
            "mention_or_use": "use",
            "model_name": "DreamerV2",
            "model_description": "RSSM with discrete stochastic latent variables (categorical), deterministic hidden state via GRU; trained with ELBO objective including a reconstruction log-likelihood p(x_t|h_t,z_t) and KL regularization; used for latent imagination to train policies.",
            "model_type": "latent world model (discrete categorical RSSM)",
            "task_domain": "Atari benchmark, general model-based RL from pixels; used as baseline on robotic manipulation tasks in this paper.",
            "fidelity_metric": "Reconstruction likelihood / ELBO (pixel log-likelihood) and KL divergence; downstream task return.",
            "fidelity_performance": "On the paper's robot-arm benchmarks at 500K steps: UR5-reach 704 ± 222, Reach-duplo 149 ± 62, Lift 165 ± 126, Door 190 ± 126, PegInHole 376 ± 59 (episode return units). Note: DreamerV2 was outperformed by DreamingV2 on complex 3D tasks.",
            "interpretability_assessment": "Discrete latents intended to capture multimodal uncertainty; explicit interpretability not demonstrated in this paper (object vanishing and reconstruction artifacts are noted as limitations of reconstruction-based training).",
            "interpretability_method": "None specific in this paper; standard inspection of learned latents possible but not emphasized here.",
            "computational_cost": "More computationally efficient than DreamingV2 in experiments (~1/1.7 the time) due to lower latent dimensionality (discrete representation in DreamerV2 uses less expanded one-hot encoding in this implementation).",
            "efficiency_comparison": "Faster than DreamingV2; tends to struggle on complex visual observations due to reconstruction burden and object-vanishing, reducing task efficiency in visually complex domains.",
            "task_performance": "Strong on Atari and many benchmarks historically; on 3D robot-arm tasks here it performs worse than DreamingV2 on several tasks due to reconstruction sensitivity.",
            "task_utility_analysis": "Discrete latents help represent multimodal transitions but reconstruction objective leads to sensitivity to small, task-relevant objects and distractors, reducing downstream policy performance in some visually complex tasks.",
            "tradeoffs_observed": "Uses discrete latents to capture multimodality but retains reconstruction costs and associated brittleness; computationally efficient but less robust to visual complexity compared to reconstruction-free variants.",
            "design_choices": "Discrete categorical z, ELBO reconstruction-based training, KL balancing and policy entropy regularization (mentioned as components inherited by DreamingV2 implementation).",
            "comparison_to_alternatives": "Compared to reconstruction-free methods (Dreaming, DreamingV2), DreamerV2 often lags on visually complex tasks because of reconstruction limitations but can perform well in other domains (e.g., Atari, some locomotion tasks).",
            "optimal_configuration": "Not newly proposed here; DreamerV2 hyperparameters mostly inherited. Authors used DreamerV2 baseline with original policy and hyperparameters for fair comparison.",
            "uuid": "e1239.1"
        },
        {
            "name_short": "Dreaming",
            "name_full": "Dreaming: Model-based reinforcement learning by latent imagination without reconstruction",
            "brief_description": "A reconstruction-free RSSM training approach that replaces pixel reconstruction with contrastive InfoNCE objectives (plus KL overshooting), training latent dynamics by matching predicted latents to PMFs estimated from observations.",
            "citation_title": "Dreaming: Model-based reinforcement learning by latent imagination without reconstruction",
            "mention_or_use": "use",
            "model_name": "Dreaming (contrastive RSSM)",
            "model_description": "RSSM with continuous stochastic latent variables (distributed on hypersphere via von Mises-Fisher-like critic in original Dreaming) trained with multi-step InfoNCE contrastive objectives J_NCE_k and KL overshooting, avoiding pixel reconstruction entirely.",
            "model_type": "latent world model (continuous latent, reconstruction-free)",
            "task_domain": "Pixel-based RL across robot tasks and DMC; compared on robot-arm and 2D/3D control tasks in this paper.",
            "fidelity_metric": "InfoNCE softmax cross-entropy (contrastive accuracy) between predicted latents and critic-produced logits; KL overshooting regularizer; downstream episode return.",
            "fidelity_performance": "On robot-arm benchmarks at 500K steps: UR5-reach 752 ± 1178, Reach-duplo 145 ± 61, Lift 174 ± 107, Door 319 ± 173, PegInHole 353 ± 50 (episode return units). Ranked second in some tasks; performed worse than DreamingV2 on contact-rich tasks requiring discontinuous dynamics.",
            "interpretability_assessment": "Continuous latents are distributed on hypersphere (von Mises-Fisher analogy); interpretability not emphasized beyond geometric interpretations.",
            "interpretability_method": "Use of cosine similarity logits (z_t W e_t) gives an implicit geometric/hyperspherical structure; no explicit latent-phase visualizations reported here for Dreaming (those shown for DreamingV2).",
            "computational_cost": "Generally efficient; contrastive learning with multi-step comparisons suffers O((B×K)^2) memory without momentum encoder; original Dreaming used a linear auxiliary dynamics (less compute for auxiliary prediction).",
            "efficiency_comparison": "Faster than DreamingV2 in some settings due to lower latent dimensionality (continuous representation), but fails to capture discontinuous/contact-rich dynamics which harms sample efficiency for those tasks.",
            "task_performance": "Strong on simple 2D manipulation tasks (e.g., Finger-turn, Reacher), ranking best on several 2D tasks; weaker on contact-rich 3D tasks where discrete representation helps.",
            "task_utility_analysis": "Reconstruction-free training improves handling of complex visual inputs (avoids object-vanishing and distractors), but continuous latents are ill-suited for representing discontinuous/contact-rich dynamics and subtasks, limiting performance on certain robot tasks.",
            "tradeoffs_observed": "Contrastive (reconstruction-free) learning increases robustness to visual complexity but combined with continuous latents may fail on discontinuous dynamics; linear auxiliary dynamics help constrain temporally nearby latents but may be less expressive than RSSM-based auxiliary dynamics.",
            "design_choices": "Continuous latent (hypersphere-like), InfoNCE multi-step objective, linear auxiliary dynamics for short-horizon predictions in original Dreaming, random-crop augmentation.",
            "comparison_to_alternatives": "Outperforms reconstruction-based methods on some complex-visual tasks but is outperformed by DreamingV2 on contact-rich tasks due to inability of continuous latents to represent discontinuous subtasks well.",
            "optimal_configuration": "Original Dreaming used linear auxiliary dynamics to stabilize contrastive learning; paper notes DreamingV2 needed an auxiliary contrastive objective (J_NCE_aux) when using discrete latents and RSSM auxiliary dynamics.",
            "uuid": "e1239.2"
        },
        {
            "name_short": "Dreamer",
            "name_full": "Dreamer (Dream to Control: Learning behaviors by latent imagination)",
            "brief_description": "An earlier RSSM-based model-based RL that uses Gaussian stochastic latents and trains with an ELBO reconstruction objective; imagines latent rollouts for policy learning.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination",
            "mention_or_use": "use",
            "model_name": "Dreamer (original)",
            "model_description": "RSSM using Gaussian (continuous) stochastic latents plus deterministic GRU hidden state; trained by ELBO with pixel reconstruction; uses latent imagination to optimize policies.",
            "model_type": "latent world model (continuous VAE-style RSSM)",
            "task_domain": "Model-based RL from pixels across various simulated tasks; used as a baseline in this paper.",
            "fidelity_metric": "ELBO reconstruction log-likelihood and KL divergence; downstream episode return.",
            "fidelity_performance": "In the paper's benchmarks at 500K steps: UR5-reach 701 ± 223, Reach-duplo 5 ± 11, Lift 134 ± 46, Door 154 ± 32, PegInHole 354 ± 47 (episode return units). Generally underperforms on several complex 3D/contact-rich tasks compared to DreamingV2.",
            "interpretability_assessment": "Standard continuous latent VAE-style representations; no special interpretability claimed here.",
            "interpretability_method": "None mentioned in this paper beyond standard latent inspections.",
            "computational_cost": "Comparable to DreamerV2 in implementations; reconstruction requires decoder and pixel-loss computation.",
            "efficiency_comparison": "Reconstruction objective can hamper robustness to visual distractions and small objects (object vanishing), reducing sample efficiency in visually complex tasks.",
            "task_performance": "Solid baselines in many domains but performs worse in visually complex and contact-rich tasks due to reconstruction limitations.",
            "task_utility_analysis": "Good where reconstruction is feasible and observations are easier to model; struggles when pixel reconstruction is hard or when critical small objects vanish in reconstructions.",
            "tradeoffs_observed": "Reconstruction provides dense training signal but suffers from object-vanishing and distractor sensitivity; continuous latent representation can be insufficient for discontinuous dynamics.",
            "design_choices": "Gaussian stochastic latents, ELBO-based training with reconstruction loss, latent imagination for policy learning.",
            "comparison_to_alternatives": "Outperformed by DreamingV2 on complex 3D manipulation tasks; in some tasks Dreaming (reconstruction-free continuous) or DreamerV2 (discrete+reconstruction) may perform differently depending on task properties.",
            "uuid": "e1239.3"
        },
        {
            "name_short": "RSSM",
            "name_full": "Recurrent State Space Model (RSSM)",
            "brief_description": "A latent time-series generative model combining deterministic recurrent state (GRU) and stochastic latent variables (either Gaussian or categorical), used to represent world dynamics for model-based RL by imagining latent rollouts.",
            "citation_title": "Learning latent dynamics for planning from pixels",
            "mention_or_use": "use",
            "model_name": "RSSM (Recurrent State Space Model)",
            "model_description": "Latent tuple (z_t, h_t) with deterministic hidden state h_t = f_GRU(h_{t-1}, z_{t-1}, a_{t-1}) and stochastic z_t ~ p(z_t | h_t); observation model p(x_t | h_t, z_t) and inference q(z_t | h_t, x_t). Supports either Gaussian continuous z or categorical discrete z as implemented in various instantiations (Dreamer, DreamerV2, DreamingV2).",
            "model_type": "latent world model (recurrent latent dynamics / generative model)",
            "task_domain": "General model-based RL from pixels (Atari, DeepMind Control, robotic manipulation).",
            "fidelity_metric": "ELBO components: reconstruction likelihood (when used) and KL divergence between posterior and prior; in reconstruction-free variants, contrastive InfoNCE replaces reconstruction term and KL overshooting retained.",
            "fidelity_performance": "Performance depends on instantiation (Dreamer/DreamerV2/Dreaming/DreamingV2). Used as core dynamics model enabling multi-step predictions for policy learning; in DreamingV2 RSSM auxiliary dynamics improved performance over linear auxiliary dynamics on several tasks.",
            "interpretability_assessment": "Interpretability depends on latent choice: categorical z can yield phase-like interpretable units (observed in DreamingV2), continuous z may map to geometric properties but less discrete phase structure.",
            "interpretability_method": "Latent trajectory visualization, inspection of categorical unit activations over time (used in DreamingV2).",
            "computational_cost": "Compute depends on latent representation and auxiliary objectives; RSSM with categorical 1024-dim PMFs is more expensive than continuous Gaussian variants; multi-step contrastive comparisons increase memory unless mitigated (momentum encoder & stop-grad).",
            "efficiency_comparison": "RSSM + discrete latents yields stronger performance on discontinuous tasks at higher computational cost; using RSSM as auxiliary dynamics (vs linear) provided better results with discrete latents in ablation.",
            "task_performance": "Enables state-of-the-art or competitive policy learning depending on representation and training objective; DreamingV2 (RSSM+categorical+contrastive) achieved best results on 3D robot-arm tasks in this paper.",
            "task_utility_analysis": "RSSM provides a flexible backbone; its utility is highly influenced by (a) whether z is discrete vs continuous and (b) whether training uses reconstruction or contrastive objectives.",
            "tradeoffs_observed": "Choice of RSSM instantiation trades off fidelity, robustness, and compute: discrete latents + contrastive objectives increase robustness to visual complexity and multimodality but are more compute- and memory-intensive.",
            "design_choices": "Use of GRU for deterministic state, choice of stochastic latent type (categorical vs Gaussian), choice of training objective (ELBO reconstruction vs InfoNCE contrastive), and auxiliary dynamics (linear vs RSSM multi-step).",
            "comparison_to_alternatives": "RSSM is the common backbone across Dreamer-family methods; alternatives include non-recurrent forward models and explicit physics simulators. RSSM offers compact latent predictive dynamics suitable for latent imagination-based policy training.",
            "uuid": "e1239.4"
        },
        {
            "name_short": "DreamerPro",
            "name_full": "DreamerPro: Reconstruction-free model-based reinforcement learning with prototypical representations",
            "brief_description": "A reconstruction-free world-model approach that uses prototypical/cluster-based representations to avoid pairwise contrastive comparisons; referenced and compared as a baseline in the experiments.",
            "citation_title": "DreamerPro: Reconstruction-free model-based reinforcement learning with prototypical representations",
            "mention_or_use": "use",
            "model_name": "DreamerPro",
            "model_description": "Reconstruction-free world model using prototypical (cluster) representations to remove the need for pairwise contrastive comparisons; used as a baseline in experiments comparing reconstruction-free approaches.",
            "model_type": "latent world model (prototypical/reconstruction-free)",
            "task_domain": "Pixel-based RL, compared on robot-arm and control tasks in this paper.",
            "fidelity_metric": "Prototypical assignment losses / classification-like objectives instead of pairwise contrastive InfoNCE; downstream episode return.",
            "fidelity_performance": "Reported in the comparison table: performs worse than DreamingV2 on several robot-arm tasks (e.g., UR5-reach 668 ± 252 at 500K steps).",
            "interpretability_assessment": "Cluster/prototype representations may offer some interpretability (discrete prototypes), but no detailed interpretability analysis presented in this paper.",
            "interpretability_method": "Not detailed in this paper (refer to original DreamerPro reference).",
            "computational_cost": "Prototypical approach reduces pairwise negative comparisons, potentially decreasing compute compared to pairwise contrastive methods.",
            "efficiency_comparison": "Designed to be more efficient than heavy contrastive pairwise methods; nonetheless, in experiments DreamingV2 outperforms it on many complex tasks.",
            "task_performance": "Moderate performance on 3D robot-arm tasks; lower than DreamingV2 on most reported benchmarks.",
            "task_utility_analysis": "Prototype-based representation reduces computational burden of contrastive learning but may lose fine-grained task discriminative power in some contact-rich scenarios.",
            "tradeoffs_observed": "Prototype methods trade off pairwise contrastive richness for computational efficiency; this can decrease performance on nuanced tasks requiring fine-grained latent distinctions.",
            "design_choices": "Prototypical quantization / clustering of representations to avoid pairwise negatives; reconstruction-free training.",
            "comparison_to_alternatives": "Compared unfavorably to DreamingV2 in several complex 3D tasks in this paper, though it may be more efficient computationally.",
            "uuid": "e1239.5"
        },
        {
            "name_short": "TPC",
            "name_full": "Temporal Predictive Coding for model-based planning in latent space",
            "brief_description": "A reconstruction-free world-model method that uses temporal predictive coding (contrastive) objectives for learning latent dynamics; included as a baseline comparison here.",
            "citation_title": "Temporal predictive coding for model-based planning in latent space",
            "mention_or_use": "use",
            "model_name": "TPC",
            "model_description": "Contrastive predictive coding style world model focused on temporal predictions in latent space; uses q(z_t|h_t,x_t) as critic in some variants; reconstruction-free.",
            "model_type": "latent world model (contrastive / reconstruction-free)",
            "task_domain": "Model-based RL from pixels; evaluated here on robot-arm and control tasks as a baseline.",
            "fidelity_metric": "Contrastive InfoNCE / temporal predictive coding losses; downstream returns.",
            "fidelity_performance": "In table: e.g., UR5-reach 555 ± 200 at 500K steps (episode return units); generally lower than DreamingV2 on 3D/contact-rich tasks.",
            "interpretability_assessment": "Not emphasized in this paper.",
            "interpretability_method": "Not described here.",
            "computational_cost": "Contrastive objectives with temporal predictions can be memory-intensive without momentum/stop-grad; implementation details determine practical cost.",
            "efficiency_comparison": "Less performant than DreamingV2 on complex 3D tasks in these experiments.",
            "task_performance": "Lower returns on several robot-arm tasks compared to DreamingV2.",
            "task_utility_analysis": "Temporal predictive contrastive learning aids learning without reconstruction, but may need appropriate latent parameterization (e.g., discrete vs continuous) to handle discontinuous dynamics.",
            "tradeoffs_observed": "Contrastive temporal learning improves robustness to visual complexity but can fail to capture certain dynamics or be less sample efficient in some settings.",
            "design_choices": "Temporal contrastive objectives, critic choices (q vs p) vary by implementation; original TPC paper details alternatives.",
            "comparison_to_alternatives": "Underperforms DreamingV2 on contact-rich 3D tasks; differs from DreamingV2 by latent parameterization and critic choice.",
            "uuid": "e1239.6"
        },
        {
            "name_short": "CVRL",
            "name_full": "Contrastive Variational model-based Reinforcement Learning (CVRL)",
            "brief_description": "A contrastive, reconstruction-free world model approach designed for complex observations; mentioned as related work but not deeply evaluated here.",
            "citation_title": "Contrastive variational model-based reinforcement learning for complex observations",
            "mention_or_use": "mention",
            "model_name": "CVRL",
            "model_description": "Contrastive variational world-model that uses contrastive objectives to learn latent representation without reconstructing pixels; tailored for complex visual inputs.",
            "model_type": "latent world model (contrastive / reconstruction-free)",
            "task_domain": "Pixel-based RL with complex observations; referenced as related work.",
            "fidelity_metric": "Contrastive InfoNCE-like objectives and downstream returns (details in original CVRL paper).",
            "fidelity_performance": "Not reported in this paper (only cited as related work).",
            "interpretability_assessment": "Not discussed in this paper.",
            "interpretability_method": "Not discussed here.",
            "computational_cost": "Not detailed here; CVRL introduces auxiliary objectives for policy/world-model control.",
            "efficiency_comparison": "Not directly compared numerically in this paper.",
            "task_performance": "Not provided here.",
            "task_utility_analysis": "Cited as an approach that addresses reconstruction problems for complex visual observations; specifics in original reference.",
            "tradeoffs_observed": "Not discussed here.",
            "design_choices": "Contrastive objective for encoder/world-model representation instead of pixel reconstruction.",
            "comparison_to_alternatives": "Mentioned as a continuous-latent reconstruction-free alternative; DreamingV2 differs by using discrete latents and multi-step RSSM auxiliary dynamics.",
            "uuid": "e1239.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dreaming: Model-based reinforcement learning by latent imagination without reconstruction",
            "rating": 2,
            "sanitized_title": "dreaming_modelbased_reinforcement_learning_by_latent_imagination_without_reconstruction"
        },
        {
            "paper_title": "Mastering atari with discrete world models",
            "rating": 2,
            "sanitized_title": "mastering_atari_with_discrete_world_models"
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "DreamerPro: Reconstruction-free model-based reinforcement learning with prototypical representations",
            "rating": 2,
            "sanitized_title": "dreamerpro_reconstructionfree_modelbased_reinforcement_learning_with_prototypical_representations"
        },
        {
            "paper_title": "Temporal predictive coding for model-based planning in latent space",
            "rating": 2,
            "sanitized_title": "temporal_predictive_coding_for_modelbased_planning_in_latent_space"
        },
        {
            "paper_title": "Contrastive variational model-based reinforcement learning for complex observations",
            "rating": 2,
            "sanitized_title": "contrastive_variational_modelbased_reinforcement_learning_for_complex_observations"
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2,
            "sanitized_title": "learning_latent_dynamics_for_planning_from_pixels"
        }
    ],
    "cost": 0.02002375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DreamingV2: Reinforcement Learning with Discrete World Models without Reconstruction</p>
<p>Masashi Okada 
Tadahiro Taniguchi 
DreamingV2: Reinforcement Learning with Discrete World Models without Reconstruction</p>
<p>The present paper proposes a novel reinforcement learning method with world models, DreamingV2, a collaborative extension of DreamerV2 and Dreaming. Dream-erV2 is a cutting-edge model-based reinforcement learning from pixels that uses discrete world models to represent latent states with categorical variables. Dreaming is also a form of reinforcement learning from pixels that attempts to avoid the autoencoding process in general world model training by involving a reconstruction-free contrastive learning objective. The proposed DreamingV2 is a novel approach of adopting both the discrete representation of DreamingV2 and the reconstruction-free objective of Dreaming. Compared to DreamerV2 and other recent model-based methods without reconstruction, DreamingV2 achieves the best scores on five simulated challenging 3D robot arm tasks. We believe that DreamingV2 will be a reliable solution for robot learning since its discrete representation is suitable to describe discontinuous environments, and the reconstruction-free fashion well manages complex vision observations.</p>
<p>I. INTRODUCTION</p>
<p>World models [1] are a potential approach to achieving the visual servoing of robots in the industry. The world models, which are equipped with compact latent representation models and latent forward dynamics, efficiently predict future trajectories and rewards, allowing us to acquire model predictive controllers [2]- [4] and policies learned by model-based reinforcement learning [5]- [7]. In addition, world modes have various valuable properties for industrial applications, such as transferability to new tasks [8], unsupervised exploration [9], generalization from offline datasets [10], and explainability [11]. World model based agents have demonstrated state-of-the-art results on a wide range of simulated tasks from pixels, such as DeepMind Control Tasks [12] and the Atari benchmark [13], but there is still a scarcity of research on real-world applications.</p>
<p>DreamerV2 [6] is a leading type of world model based reinforcement learning that achieved human-level performance on the Atari benchmark. Unlike previous world models [2], [3], [7], including Dreamer [5] (the earlier version of Dream-erV2), this method uses discrete world models in which discrete random variables represent latent states. A motivation to introduce discrete representation is that categorical distributions can naturally capture multimodal uncertainty of stochastic state transitions. In contrast, earlier world models that use Gaussian distribution cannot manage multimodal † Masashi Okada and Tadahiro Taniguchi are with Digital &amp; AI Technology Center, Technology Division, Panasonic Corporation, Japan.</p>
<ul>
<li>Tadahiro Taniguchi is also with Ritsumeikan University, College of Information Science and Engineering, Japan.</li>
</ul>
<p>okada.masashi001@jp.panasonic.com DreamingV2's contrastive learning to train discrete world models without reconstruction. The trained discrete world models can successfully represent simulated 3D robot arm environments. Given an observation xt, an inference model q(zt|ht, xt) and generative model p(zt|ht) predict trajectories of discrete latent states zt. While, other inference model predicts probability mass functions p(z|xt) from time series of observations x t:t+K . The predicted latents and probability mass functions are paired to compute logits, and then contrastive learning is conducted. As in Dreaming [7], random crop image augmentation is exploited. uncertainty well [14]. The discrete state representation would be useful for real-world robot tasks for the following reasons. First, practical robot tasks demand contact-rich manipulation and making the dynamics highly discontinuous. Second, several robot tasks consist of subtasks. Let us consider a door opening task by robot arms, which includes the subtasks of moving the end-effector closer to a door handle, grabbing the handle, and then pulling it to open the door. To represent such unsmooth state space and discrete phase changes, the use of discrete variables is quite reasonable.</p>
<p>However, DreamerV2 learns the world models using a general time-series variational auto-encoding objective [3], and reconstruction process of complex visual observations will produce various issues in practical circumstances. For example, as pointed out in [7], the autoencoder often fails to recognize small task-relevant objects, which is referred to as object vanishing. In addition, the autoencoder-based world models are sensitive to visual distraction such as unseen taskirrelevant objects and noises like shadows [15]- [17].</p>
<p>Dreaming [7], CVRL (Contrastive Variational Reinforcement Learning) [15], TPC (Temporal Predictive Coding) [16], and DreamerPro [17] are another type of world model methods that involves reconstruction-free objectives to solve the above issues produced by the autoencoding. For instance, Dreaming is a reconstruction-free modification of Dreamer [5] that uses self-supervised contrastive learning [18], [19]. However, these methods require a continuous state variable and do not benefit from the discrete representation described above.</p>
<p>Motivated by the above, we propose a DreamingV2 by exploiting and merging the main principles of DreamerV2 and Dreaming. The top-level concept is summarized in Fig. 1. Our primary contributions are summarized as follows:</p>
<p>• We devise a contrastive learning approach for training discrete world models without reconstruction. To the best of our knowledge, it is the first application of contrastive learning for categorical discrete representations. • We demonstrate that DreamingV2, reinforcement learning using the discrete world models, can achieve stateof-the-art results on five simulated robot-arm tasks. The remainder of this paper is organized as follows. In Sec. II, we present an overview of Dreamer, DreamerV2, and Dreaming. In Sec. III, our proposed method DreamingV2 is specified. In Sec. IV, the effectiveness of DreamingV2 is demonstrated via simulated evaluations. In Sec. V, related work are summarized. Finally, Sec. VI concludes this paper.</p>
<p>II. PRELIMINARY</p>
<p>This section briefly reviews the Recurrent State Space Model (RSSM), and RSSM based reinforcement learning methods: Dreamer, DreamerV2, and Dreaming.</p>
<p>A. Recurrent State Space Model</p>
<p>RSSM is a principal model for representing world models, and is used as a major component of various methods [3]- [7], [9], [17], [20]. RSSM defines the latent variable as a tuple of (z t , h t ) where z t , h t are the stochastic and deterministic variables, respectively. RSSM's generative and inference models are defined as the following:</p>
<p>Generative models :
     h t = f GRU (h t−1 , z t−1 , a t−1 ) z t ∼ p(z t |h t ) x t ∼ p(x t |h t , z t ) , (1) Inference model : z t ∼ q(z t |h t , x t ),
where x and a denote pixel observations and actions, respectively. The deterministic h t is considered to be the hidden state of the gated recurrent unit (GRU) f GRU (·) [21]. Generally, these models are trained by optimizing the variational evidence lower bound objective (ELBO):
J ELBO = t   Eq(z t|ht,xt) [log p(x t |h t , z t )] :=J likelihood (2) −E q(zt|·) [KL [q(z t+1 |h t+1 , x t+1 )||p(z t+1 |h t+1 )]] :=J KL    .
To conduct model predictive control or policy optimization, the reward predictor p(r t |h t , z t ) is also necessary, but it is simply realized by regarding the rewards as observations and learn the reward predictor along with the generative model p(x t |h t , z t ). For readability, the following discussion omits the specifications of the reward function.</p>
<p>B. Dreamer and DreamerV2</p>
<p>Dreamer and DreamerV2 [5], [6] use RSSM to imagine latent trajectories and future rewards to train their policies. The training procedure of Dreamer(V2) is simply summarized as follows: (i) train RSSM with a given replay buffer by optimizing Eq. (2), (ii) train a policy within the trained RSSM-based world model by latent imagination, and (iii) execute the trained policy in a real environment and store the observed results with the replay buffer. The steps above are iteratively performed until some termination condition is satisfied. The major difference between Dreamer and DreamerV2 is the specification of the stochastic latent z t ; Gaussian random variables vs. categorical random variables. DreamerV2 adopted other minor modifications (e.g., KLbalancing and policy entropy regularization), but they are not detailed in this paper since the proposed method adopts the components as-is.</p>
<p>C. Dreaming</p>
<p>Dreaming [7] introduces a reconsruction-free objective derived from the ELBO objective:
J Dreaming := K k=0 J NCE k + J KL k ,(3)
where K is the prediction horizon, and J NCE k and J KL k are referred to as Info-NCE (noise contrastive estimator) objective and overshooting objective, respectively. Since J NCE k mainly characterizes Dreaming, enabling to train RSSM without reconstruction, this section only describes this objective. J NCE k is defined as:
J NCE k := Ep (zt|z t−k ,a&lt;t)q(z t−k |·) log p(z t |x t ) x ∈D p(z t |x )
, wherep(z t |·) and p(z t |x t ) are respectively referred to as auxiliary dynamics and critic in this paper, and D indicates a mini-batch. The auxiliary dynamics is responsible for predicting z t from z t−k , and critic is to evaluate z t predicted by the auxiliary dynamics. The definitions of the auxiliary dynamics and critic are introduced later in Sec. III to highlight the difference from DreamingV2. Let |D| be the size of D, J NCE k is considered as a |D|-class categorical cross entropy objective to discriminate the positive pair (z t , x t ) among the other negative pairs (z t , x ( = x t )). Representation learning with this type of objective is known as contrastive learning [18], [19]. This objective can be easily computed by softmax cross entropy functions, generally predefined in deep learning frameworks 1 , by inputting log p(z|x) as logit. """ Args: zt: discrete latent (32x32) pmf: point mass function (32x32) """ return sum(zt * log(pmf)) (c) pseudo code Simulated robot-arm tasks (top) and training curves (bottom). The UR5-reach is originally introduced in [7], the Reach-duplo task is from DeepMind Control Suite [12], and the remaining tasks are from RoboSuite [22] with iGibsonRenderer [23] for photorealistic visualization. In the bottom figures, horizontal and vertical axes indicate time steps and episode reward, respectively. The lines represent the medians, and the shaded areas depict the percentiles 5% to 95% on 8 random seeds. The horizontal dashed lines indicate the upper limit return of the tasks.
GRU CNN FC1 ∼ ( | ) GRU ∼ ( | ⋅) FC2</p>
<p>III. DREAMINGV2</p>
<p>This section introduces a new model-based reinforcement learning that uses discrete world models trained by contrastive learning, which we call DreamingV2. Figure 2 illustrates the detailed architecture and PyTorch-like psuedo code of some essential operations. DreamingV2's objective is almost similar to Dreaming as:
J DreamingV2 := J Dreaming + J NCE aux .(4)
A difference between DreamingV2 with Dreaming is an auxiliary objective J NCE aux (defined later in Sec. III-D). The other significant difference of DreamingV2 from Dreamer is that continuous latent variables and their distributions have been replaced with discrete variables and distributions. In the following sections, the detail of DreamingV2 is specified, with focus on the changes from Dreaming.</p>
<p>A. Latent state representation</p>
<p>DreamingV2 replaces continuous latent variables with discrete variables in a similar format to DreamerV2. The stochastic latent state z comprises 32 categorical variables, and each variable is a 32-dimensional one-hot vector. With this specification, all distributions of z are point mass functions (PMFs) modeled by categorical distributions. The PMF is parameterized with a 32 × 32 probability matrix. To backpropagate through the discrete latent sampled from the categorical distributions, straight-through gradients [24] are involved as in DreamerV2.</p>
<p>B. Definition of auxiliary dynamics</p>
<p>Dreaming defined the auxiliary dynamicsp(z t |·) as a trainable linear dynamics. An intuitive motivation to introduce such a simple model was that linear dynamics successfully constrain temporally consecutive latent states (namely negative pairs in contrastive learning) are not distributed too further. Instead, DreamingV2 defines the auxiliary dynamics using the RSSM generative models p(z t |h t = f GRU (·)) in Eq. (1). In Fig. 2(a), multi-step prediction is conducted using RSSM models. Without the linear modeling, the discrete representation itself successfully regularizes repulsions between the consecutive negative pairs. This is experimentally supported in the ablation study in Sec. IV-C.2.</p>
<p>C. Definition of Critic</p>
<p>Dreaming defined the critic p(z t |x t ) as:
p(z t |x t ) ∝ exp(z t W e t ),(5)
where W is a trainable matrix, e t := f CNN (x t ), and f CNN (·) denotes feature extraction by a CNN unit. Since log p(z t |x t ) is fed into the softmax cross entropy function, cosine similarity metrics z t W e t are used as logits for contrastive learning. It is interesting to note that Eq. (5) is the von-Mises-Fisher distribution (a hyperspherical probability distribution) [25] so that continuous latent states are distributed on the hypersphere. DreamingV2 defines the critic as categorical PFMs and employs simple parameterization like Eq. (5). calc pmf() in Fig. 2(c) describes the pseudo-code for estimating the PMFs' parameters, where the probability matrix is estimated with simple linear transformation W e t and softmax operation. The logits for contrastive learning can be simply computed like in calc logit() in Fig. 2(c).</p>
<p>D. Auxiliary contrastive objective</p>
<p>We encountered the issue that just optimizing J Dreaming did not work (optimization of J NCE k did not progress at all). To solve this, we heuristically added the auxiliary objective: Fig. 2(b) illustrates the architecture to calculate this objective, which is similar to Momentum Contrast (MoCo) for unsupervised visual representation learning [26]. This objective contributes to the training the CNN encoder and critic parameter W . We found that this simpler objective than J NCE k successfully triggers the overall optimization.
J NCE aux = E p(zt|xt) log p(z t |x t ) x p(z t |x t ) .(6)</p>
<p>E. Momentum encoder</p>
<p>We used a momentum encoder, which is a general scheme introduced in many representation learning publications [26]- [29]. As shown in Fig. 2(a/b), the stop-gradient operator is inserted after the CNN', and its parameterθ is updated using the exponential moving average of the CNN's parameter θ; i.e.,θ ← (1 − η)θ + ηθ, where η ∈ (0, 1] is a momentum coefficient. We adopt the momentum encoder since the stop-gradient operation can drastically reduce memory usage. Without the operation, the memory complexity of contrastive learning is O((B ×K) 2 ) due to the pairwise logit computations, however, it can be reduced to O(B × K) by the stop-gradient.</p>
<p>F. Random image augmentation</p>
<p>As in Dreaming, we apply random image augmentation to observations. A recently introduced random crop method in [30] is adopted to DreamingV2 since the literature has reported performance improvements on model-free reinforcement learning in pixels. This augmentation firstly pads each side of a 64×64 observation by 4 pixels (by repeating boundary pixels), secondly selects a random 64 × 64 boundary box, and then finally outputs the original image shifted by 4 pixels.</p>
<p>G. Implementation</p>
<p>We implemented DreamingV2 in TensorFlow based on the official source code of DreamerV2 2 . We kept the original policy optimization method, hyperparameters and experimental conditions similar to the original ones. DreamingV2 specific hyperparameters K (prediction horizon) and η (momentum coefficient) were set to be K = 3 and η = 0.05, respectively. The effect of K is tested in the ablation study in Sec. II. We observed no significant effect of η as long as the momentum encoder was slowly updated.</p>
<p>IV. EXPERIMENTS A. Comparison to state-of-the-art methods</p>
<p>The main objective of this experiment is to demonstrate that DreamingV2 has advantages over the baseline method DreamerV2 [6] on five robot-arm tasks shown in the top part of Fig. 3. In contrast to the commonly used 2D robot tasks in the DeepMind Control Suite (e.g., Cheetah, Walker) [12], these tasks must be solved by understanding complex vision observations from 3D spaces. In addition, iGibson-Renderer [23] is introduced for high-quality visualization of RoboSuite tasks (i.e., Lift, Door, PegInHole). Although these tasks are in virtual environments, we assume that they well simulate real-world scenarios. This experiment includes the recent world model based reinforcement learning without reconstruction, DreamerPro [17] and TPC [16] 3 . The bottom part of Fig. 3 shows the training curves of the experiments. In these results, DreamingV2 consistently outperformed other baselines. In addition, on the tasks of Reach-duplo, Lift, Door, and PegInHole, DreamingV2 significantly showed better performances. Moreover, only DreamingV2 could solve these tasks within 500K(= 500 × 10 3 ) steps, which is about seven hours of interactions with the environments 4 . DreamingV2 (ours) DreamerV2 [6] Dreaming [7] Dreamer [5] DreamerPro [17] TPC [16] Discrete latent Reconstruction free In contrast to the previous literature, DreamingV2 suppports both discrete latent and reconstruction-free learning. This ablation study reveals how these factors contributed to the above results. For this purpose, we compared Dream-ingV2 with DreamerV2, Dreamer, and Dreaming. Dreamer and Dreaming are implemented based on the original Dream-erV2 and our DreamingV2 code. Hence, several components introduced in DreamerV2 and DreamingV2 (i.e., KLbalancing, policy entropy regularization, momentum encoder, and auxiliary contrastive loss), are inherited by Dreamer and Dreaming. These changes from the original implementation of Dreamer and Dreaming resulted in several performance changes from [7], but we left them untouched to evaluate the effect of discrete latent and reconstruction-free learning. In addition to the previous robot arms tasks, another variety of 10 DMC tasks are evaluated, which are categorized into four classes namely; 2D manipulation, pole-swingup, 3D locomotion, and 2D locomotion. Table I summarizes the training results benchmarked at 500K steps. The results show the mean and standard deviation averaged eight random seed experiments. (A) On the robot arm tasks, DreamingV2 consistently outperformed other Dreamer-variants. We observed reconstruction-based method suffered from reconstructing complex vision observations as shown in Fig. 4. Dreaming also performed well and ranked second on UR5-reach, Lift, and Door tasks. This also shows the effectiveness of reconstruction-free learning on complex observation tasks. However, Dreaming performed significantly worse than DreamingV2 on tasks other than UR5-reach. Because these tasks require contact-rich manipulations, the resulting dynamics are highly discontinuous and difficult to represent with continuous latent variables. (B) On the 2D manipulation tasks, Dreaming demonstrated the best performances. Since these simple environments can be almost described by angular information (joint and rotater angles), we assume that Dreaming with continuous latent on the hypersphere (see Sec. III-C) took advantages to represent these polar coordinates worlds. In contrast, DreamingV2 struggled represent these environments with discrete latent, showing worse performance especially in Finger tasks. (C) On the pole swingup tasks, DreamingV2 achieved the best performance. Despite their low degrees of freedom, these  p: linear as in [7]p: RSSM
(A) 3D Rotot-arm tasks UR5-reach 776±194 704±222 752±1178 701±223 668±252 555±200 Reach-duplo 199±43 149±62 145±61 5±11 87±76 5±16 Lift 327±150 165±126 174±107 134±46 138±64 24±35 Door 383±143 190±126 319±173 154±32 111±110 118±129 PegInHole 436±26 376±59 353±50 354±47 327±43 369±67 (B) 2D Manipulation tasks Reacher-hard 598±447 175±340 743±346 247±392 - - Finger-turn-hard 484±434 600±417 858±210 533±426 - - Reacher-easy 924±210 923±215 947±100 658±429 - - Finger-turn-easy 434±469 498±469 842±286 665±430 - - (C) Difficult pole-swingup tasks Acrobot-swingup 470±129 309±131 359±111 382±147 - - Cartpole-two-poles 308±55 248±103 273±53 256±65 - - (D) 3D locomotion robot tasks Quadruped-walk 492±127 350±89 379±189 242±120 - - Quadruped-run 385±91 352±68 339±128 269±114 - - (E) 2D locomotion tasks Cheetah-run 768±24 811±75 542±132 776±120 - - Walker-walk 857±115 951±28 518±76 906±70 - - Ground Truth Reconstruction Reach Lift Door PegInHoleK = 0 K = 1 K = 2 K = 3 UR5-reach 783±190 686±229 706±232 776±194 Reach-duplo 157±61 169±52 187±54 199±43 Lift 197±83 273±162 353±131 327±150 Door 323±172 369±154 424±81 383±143 PegInHole 398±53 421±46 434±52 436±26UR5-reach 788±154 834±194 Reach-duplo 210±66 219±43 Lift 319±151 396±150 Door 462±141 450±143 PegInHole 437±50 430±26
tasks take a wide range of states and provide a diverse of observations, making reconstruction challenging. The higher performance of DreamingV2 over Dreaming indicates the effectiveness of discrete representation to such chaotic systems [31]. (D) On 3D locomotion tasks, DreamingV2 was ranked best. As similar with the robot arm tasks, 3D observations are complex to reconstruct, limiting the performance of Dreamer and DreamerV2. Since these tasks need contactrich controls, DreamingV2 performed better than Dreaming.</p>
<p>(E) On 2D locomotion tasks, DreamerV2 outperformed other methods. As reported in [7], contrastive learning fails to capture the velocity information from robot-centric observations. However, by capturing discontinuous dynamics with discrete representation, DreamingV2 significantly improved the performance than Dreaming.</p>
<p>C. Ablation study: hyper parameters</p>
<p>1) The effect of multi-step prediction horizon K: Table II analyzes the effect of prediction horizon K on the robot arm tasks. In almost cases, the longer prediction showed better performance, but it saturated at values of K = 2 and K = 3.</p>
<p>2) Modeling of the auxiliary dynamicsp: Table III probes the effect of the auxiliary dynamics modeling (linear as in Dreaming [7] vs. RSSM). RSSM modling performed better on 3 of 5 tasks, which are different results from [7] because we employed discrete representation. Linear modeling demonstrated better on 2 of 5 tasks, however, the differences seemed not significant.  reconstruction-free model-based learning have been developed. CFM (Contrastive Forward Model) [32] utilizes a similar reconstruction-free objective like Dreaming(V2), however, it is dedicated to continuous representation. CVRL [15], TPC [16], and Multi-State Space Model (MSSM) [33] also introduce contrastive learning for decoder-free world modeling. The major differences between them and DreamingV2 are continuous latent representation and the lack of multi-step prediction for contrastive learning (i.e., K = 0). The specific differences that characterize these methods are; CVRL introduces an auxiliary objective for policy optimization and world-model predictive control at test time, TPC replaces the critic p(z t |x t ) with q(z t |h t , x t ), and MSSM computes logits by comparing a pair of latent variables originating from different modals (e.g., different camera views). Dream-erPro [17] introduces the clustering-based or prototype-based method [34], in which a set of clusters represented by finite quantized vectors are trained, thereby eliminating pair-wise comparisons of contrastive learning. Bootstrapped LAtents for Simulating Trajectories (BLAST) [29] uses heuristics from the recent self-supervised method for vision, Bootstrap Your Own Latent (BYOL) [27], to achieve representation learning without reconstruction and negative samples.</p>
<p>D. Analysis of latent states</p>
<p>VI. CONCLUSION</p>
<p>In the present paper, we proposed DreamingV2, a synergistic extention of DreamerV2 and Dreaming. Dream-ingV2 is equipped with discrete latent representation and reconstruction-free objective, which differentiate our method from other existing literature of world models. To the best of our knowledge, applying contrastive learning to discrete latent is the world's first challenge, and we successfully demonstrated that our method could facilitate policy evolution to solve the 3D, visually complex, and contact-rich robot-arm tasks from pixel observations. This result strongly suggests the applicability of industrial use of this method. We believe that this method is also applicable for other complex observation tasks, such as point cloud [35], and multimodal inputs [33].</p>
<p>Although we demonstrated the effectiveness of discrete representation in the robot-arm tasks, DreamingV2 performed worse scores than other methods based on continuous representation on several tasks like Finger-turn. The task dependency of representation needs to be considered in detail in the future study. Hybridization of continuous and discrete representations appears to be attractive for future direction. In addition, another challenge of the proposed method is the increase in latent dimensionality due to the discrete representation (Dreaming and DreamingV2 respectively require 32and 1024(= 32 × 32)-dimensional vectors for z). Our method runs on a single NVIDIA V100 GPU, but the high dimensionality limits the computation speed (approximately 1.7x slower than DreamerV2). A solution to this is to use negative sample free self-supervised learning such as BYOL [27] and Barlow Twins [36].</p>
<p>Fig. 1. DreamingV2's contrastive learning to train discrete world models without reconstruction. The trained discrete world models can successfully represent simulated 3D robot arm environments. Given an observation xt, an inference model q(zt|ht, xt) and generative model p(zt|ht) predict trajectories of discrete latent states zt. While, other inference model predicts probability mass functions p(z|xt) from time series of observations x t:t+K . The predicted latents and probability mass functions are paired to compute logits, and then contrastive learning is conducted. As in Dreaming [7], random crop image augmentation is exploited.</p>
<p>Fig. 2 .
2The RSSM-based architecture and PyTorch-like pseudocode to compute J NCE k and J NCE aux . CNN, GRU, and FC represent a convolutional neural network, a GRU-cell, and a fully-connected layer, respectively. s.g. and EMA are short for stop-gradient and exponential moving average. (a) To compute J NCE k , z t:t+K are inferred and predicted using q(zt) and p(h t+1 |zt) from an observation xt, where K is the prediction horizon. The predicted latent are compared with the point mass functions estimated from observations x t:t+K and then B × (K + 1) logits are computed, where B is the number of episodes in the mini-batch. The notations of xt and x t indicate the different random augmentation in Sec. III-F is applied. For readability, we only illustrate positive logits, however, negative logits are also computed by pairwise comparison of the latent from different time-steps and episodes, yielding (B × (K + 1)) 2 logits. (b) To compute J NCE aux , only the encoders and critic p(z|xt) are taken into account. Also in this figure, we only illustrate positive logits.Deng, et. al., 2021] TPC[Nguyen et. al., 2021] Fig. 3.</p>
<p>Fig. 4 .
4Reconstructed images by DreamerV2's world models. The dashedline ovals highlight the object vanishing. B. Ablation study: continuous latent vs. discrete latent, Reconstruction-based vs. -free learning</p>
<p>Fig. 5
5visualizes latent trajectories on Lift task generated by DreamingV2, in which we can observe three categorical latent variables that represent tasks phases, i.e., (a) approach, (b) grasp, and (c) lift. V. RELATED WORK Recently, world models with self-supervised learning have received a lot of attention, and several methods for</p>
<p>Fig. 5 .
5Latent and reward trajectories of DreamingV2 during a test trial on Lift task. Only 3 of 32 categorical variables, which were supposed to represent subtask phases, were picked up.</p>
<p>= W @ et # linear trans. (1536 =&gt; 1024) x = reshape(x, (-1, 32, 32)) # 1024 =&gt; 32x32 pmf = softmax(x, dim=-1) # make them probs. return pmf def calc_logit(zt, pmf):logit </p>
<p>s.g. </p>
<p>CNN' </p>
<p>calc. PMF </p>
<p>( | ) </p>
<p>CNN' </p>
<p>calc. PMF </p>
<p>( | 
) </p>
<p>CNN' </p>
<p>calc. PMF </p>
<p>( | 
) </p>
<p>logit 
logit </p>
<p>EMA </p>
<p>s.g. 
s.g. </p>
<p>s.g. </p>
<p>CNN' </p>
<p>calc. PMF </p>
<p>( | ) </p>
<p>CNN </p>
<p>calc. PMF </p>
<p>∼ ( | ) </p>
<p>logit </p>
<p>EMA </p>
<p>= 
log exp( ) 
log ∑ exp( ) 
= 
log exp( ) 
log ∑ exp( ) </p>
<p>(a) 
(b) </p>
<p>def calc_pmf(et, W): 
""" 
Calculate point mass functions (PMFs) 
modeled by categorical distributions. 
Args: 
et: feature vectors from CNN (1536x1) 
W: trainable matrix (1024x1536) 
""" 
x </p>
<p>TABLE I ABLATION
ISTUDY: THE EFFECT OF DISCRETE LATENT AND RECONSTRUCTION-FREE WORLD MODELING. BOLDFACE AND UNDERLINES INDICATE THE BEST RESULTS. UNDERLINES MEAN THE SECOND-BEST.</p>
<p>TABLE II ABLATION
IISTUDY: THE EFFECT OF THE OVERSHOOTING DISTANCE K.</p>
<p>TABLE III ABLATION
IIISTUDY: THE EFFECTS OF THE AUXILIARY DYNAMICS.
i.e., softmax cross entropy with logits() in TensorFlow, and cross entropy() in PyTorch.
https://github.com/danijar/dreamerv2 3 Code was taken from https://github.com/fdeng18/ dreamer-pro4 In the Robosuite tasks, control frequency was 20 Hz. Hence, 500K steps are approximately equivalent to seven hours of robot operation.
ACKNOWLEDGMENTMost of the experiments were conducted in ABCI (AI Bridging Cloud Infrastructure), built by the National Institute of Advanced Industrial Science and Technology, Japan.
World models. D Ha, J Schmidhuber, arXiv:1803.10122D. Ha and J. Schmidhuber, "World models," arXiv:1803.10122, 2018.</p>
<p>Embed to control: A locally linear latent dynamics model for control from raw images. M Watter, J Springenberg, J Boedecker, M Riedmiller, NeurIPS. M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller, "Embed to control: A locally linear latent dynamics model for control from raw images," NeurIPS, 2015.</p>
<p>Learning latent dynamics for planning from pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, ICML. D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson, "Learning latent dynamics for planning from pixels," in ICML, 2019.</p>
<p>PlaNet of the Bayesians: Reconsidering and improving deep planning network by incorporating Bayesian inference. M Okada, N Kosaka, T Taniguchi, IROS. M. Okada, N. Kosaka, and T. Taniguchi, "PlaNet of the Bayesians: Reconsidering and improving deep planning network by incorporating Bayesian inference," in IROS, 2020.</p>
<p>Dream to control: Learning behaviors by latent imagination. D Hafner, T Lillicrap, J Ba, M Norouzi, ICLRD. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, "Dream to control: Learning behaviors by latent imagination," ICLR, 2020.</p>
<p>Mastering atari with discrete world models. D Hafner, T Lillicrap, M Norouzi, J Ba, ICLR. D. Hafner, T. Lillicrap, M. Norouzi, and J. Ba, "Mastering atari with discrete world models," in ICLR, 2021.</p>
<p>Dreaming: Model-based reinforcement learning by latent imagination without reconstruction. M Okada, T Taniguchi, ICRAM. Okada and T. Taniguchi, "Dreaming: Model-based reinforcement learning by latent imagination without reconstruction," in ICRA, 2021.</p>
<p>Imagined value gradients: Model-based policy optimization with tranferable latent dynamics models. A Byravan, J T Springenberg, A Abdolmaleki, R Hafner, CoRLA. Byravan, J. T. Springenberg, A. Abdolmaleki, R. Hafner, et al., "Imagined value gradients: Model-based policy optimization with tranferable latent dynamics models," in CoRL, 2020.</p>
<p>Planning to explore via self-supervised world models. R Sekar, O Rybkin, K Daniilidis, P Abbeel, D Hafner, D Pathak, arXiv:2005.05960R. Sekar, O. Rybkin, K. Daniilidis, P. Abbeel, D. Hafner, and D. Pathak, "Planning to explore via self-supervised world models," arXiv:2005.05960, 2020.</p>
<p>MOPO: Model-based offline policy optimization. T Yu, G Thomas, L Yu, S Ermon, J Y Zou, S Levine, C Finn, T Ma, NeurIPS. T. Yu, G. Thomas, L. Yu, S. Ermon, J. Y. Zou, S. Levine, C. Finn, and T. Ma, "MOPO: Model-based offline policy optimization," NeurIPS, 2020.</p>
<p>The arcade learning environment: An evaluation platform for general agents. M G Bellemare, Y Naddaf, J Veness, M Bowling, Journal of Artificial Intelligence Research. 47M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, "The arcade learning environment: An evaluation platform for general agents," Journal of Artificial Intelligence Research, vol. 47, pp. 253-279, 2013.</p>
<p>Explainable autonomous robots: a survey and perspective. T Sakai, T Nagai, Advanced Robotics. T. Sakai and T. Nagai, "Explainable autonomous robots: a survey and perspective," Advanced Robotics, pp. 1-20, 2022.</p>
<p>DeepMind control suite. Y Tassa, Y Doron, A Muldal, T Erez, Y Li, arXiv:1801.00690Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, et al., "DeepMind control suite," arXiv:1801.00690, 2018.</p>
<p>Variational inference MPC for bayesian model-based reinforcement learning. M Okada, T Taniguchi, CoRLM. Okada and T. Taniguchi, "Variational inference MPC for bayesian model-based reinforcement learning," in CoRL, 2019.</p>
<p>Contrastive variational model-based reinforcement learning for complex observations. X Ma, S Chen, D Hsu, W S Lee, CoRL. X. Ma, S. Chen, D. Hsu, and W. S. Lee, "Contrastive variational model-based reinforcement learning for complex observations," CoRL, 2020.</p>
<p>Temporal predictive coding for model-based planning in latent space. T D Nguyen, R Shu, T Pham, H Bui, S Ermon, ICML. T. D. Nguyen, R. Shu, T. Pham, H. Bui, and S. Ermon, "Temporal predictive coding for model-based planning in latent space," in ICML, 2021.</p>
<p>DreamerPro: Reconstruction-free model-based reinforcement learning with prototypical representations. F Deng, I Jang, S Ahn, arXiv:2110.14565F. Deng, I. Jang, and S. Ahn, "DreamerPro: Reconstruction-free model-based reinforcement learning with prototypical representa- tions," arXiv:2110.14565, 2021.</p>
<p>A V Oord, Y Li, O Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. A. v. d. Oord, Y. Li, and O. Vinyals, "Representation learning with contrastive predictive coding," arXiv:1807.03748, 2018.</p>
<p>A simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G Hinton, ICLR. T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, "A simple frame- work for contrastive learning of visual representations," in ICLR, 2020.</p>
<p>Variational recurrent models for solving partially observable control tasks. D Han, K Doya, J Tani, ICLR. D. Han, K. Doya, and J. Tani, "Variational recurrent models for solving partially observable control tasks," in ICLR, 2020.</p>
<p>Learning phrase representations using RNN encoder-decoder for statistical machine translation. K Cho, B Van Merriënboer, C Gulcehre, D Bahdanau, arXiv:1406.1078K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, et al., "Learning phrase representations using RNN encoder-decoder for statistical machine translation," arXiv:1406.1078, 2014.</p>
<p>robosuite: A modular simulation framework and benchmark for robot learning. Y Zhu, J Wong, A Mandlekar, R Martín-Martín, arXiv:2009.12293Y. Zhu, J. Wong, A. Mandlekar, and R. Martín-Martín, "robosuite: A modular simulation framework and benchmark for robot learning," arXiv:2009.12293, 2020.</p>
<p>iGibson 2.0: Object-centric simulation for robot learning of everyday household tasks. C Li, F Xia, R Martín-Martín, M Lingelbach, S Srivastava, arXiv:2108.03272C. Li, F. Xia, R. Martín-Martín, M. Lingelbach, S. Srivastava, et al., "iGibson 2.0: Object-centric simulation for robot learning of everyday household tasks," arXiv:2108.03272, 2021.</p>
<p>Estimating or propagating gradients through stochastic neurons for conditional computation. Y Bengio, N Léonard, A Courville, arXiv:1308.3432Y. Bengio, N. Léonard, and A. Courville, "Estimating or propagating gradients through stochastic neurons for conditional computation," arXiv:1308.3432, 2013.</p>
<p>Understanding contrastive representation learning through alignment and uniformity on the hypersphere. T Wang, P Isola, ICLR. T. Wang and P. Isola, "Understanding contrastive representation learn- ing through alignment and uniformity on the hypersphere," in ICLR, 2020.</p>
<p>Momentum contrast for unsupervised visual representation learning. K He, H Fan, Y Wu, S Xie, R Girshick, CVPR. K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, "Momentum contrast for unsupervised visual representation learning," in CVPR, 2020.</p>
<p>Bootstrap your own latent-a new approach to self-supervised learning. J.-B Grill, F Strub, F Altché, C Tallec, NeurIPS. J.-B. Grill, F. Strub, F. Altché, C. Tallec, et al., "Bootstrap your own latent-a new approach to self-supervised learning," NeurIPS, 2020.</p>
<p>CURL: Contrastive unsupervised representations for reinforcement learning. A Srinivas, M Laskin, P Abbeel, ICML. A. Srinivas, M. Laskin, and P. Abbeel, "CURL: Contrastive unsuper- vised representations for reinforcement learning," in ICML, 2020.</p>
<p>BLAST: Latent dynamics models from bootstrapping. K Paster, L E Mckinney, S A Mcilraith, J Ba, Deep RL Workshop NeurIPS 2021. K. Paster, L. E. McKinney, S. A. McIlraith, and J. Ba, "BLAST: Latent dynamics models from bootstrapping," in Deep RL Workshop NeurIPS 2021, 2021.</p>
<p>Mastering visual continuous control: Improved data-augmented reinforcement learning. D Yarats, R Fergus, A Lazaric, L Pinto, ICLR. D. Yarats, R. Fergus, A. Lazaric, and L. Pinto, "Mastering visual continuous control: Improved data-augmented reinforcement learning," in ICLR, 2022.</p>
<p>Dynamic programming for global control of the acrobot and its chaotic aspect. R Ueda, T Arai, ICRAR. Ueda and T. Arai, "Dynamic programming for global control of the acrobot and its chaotic aspect," in ICRA, 2008.</p>
<p>Learning predictive representations for deformable objects using contrastive estimation. W Yan, A Vangipuram, P Abbeel, L Pinto, arXiv:2003.05436W. Yan, A. Vangipuram, P. Abbeel, and L. Pinto, "Learning predictive representations for deformable objects using contrastive estimation," arXiv:2003.05436, 2020.</p>
<p>Multi-modal mutual information (MUMMI) training for robust self-supervised deep reinforcement learning. K Chen, Y Lee, H Soh, ICRAK. Chen, Y. Lee, and H. Soh, "Multi-modal mutual information (MUMMI) training for robust self-supervised deep reinforcement learning," in ICRA, 2021.</p>
<p>Unsupervised learning of visual features by contrasting cluster assignments. M Caron, I Misra, J Mairal, P Goyal, P Bojanowski, A Joulin, NeurIPS. M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin, "Unsupervised learning of visual features by contrasting cluster as- signments," NeurIPS, 2020.</p>
<p>3DCNN-DQN-RNN: A deep reinforcement learning framework for semantic parsing of large-scale 3d point clouds. F Liu, S Li, L Zhang, C Zhou, R Ye, Y Wang, J Lu, CVPR. F. Liu, S. Li, L. Zhang, C. Zhou, R. Ye, Y. Wang, and J. Lu, "3DCNN- DQN-RNN: A deep reinforcement learning framework for semantic parsing of large-scale 3d point clouds," in CVPR, 2017.</p>
<p>Barlow Twins: self-supervised learning via redundancy reduction. J Zbontar, L Jing, I Misra, Y Lecun, S Deny, ICML. J. Zbontar, L. Jing, I. Misra, Y. LeCun, and S. Deny, "Barlow Twins: self-supervised learning via redundancy reduction," in ICML, 2021.</p>            </div>
        </div>

    </div>
</body>
</html>
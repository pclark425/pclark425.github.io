<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5229 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5229</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5229</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-264288909</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.11829v4.pdf" target="_blank">Graph Foundation Models: Concepts, Opportunities and Challenges</a></p>
                <p><strong>Paper Abstract:</strong> Foundation models have emerged as critical components in a variety of artificial intelligence applications, and showcase significant success in natural language processing and several other domains. Meanwhile, the field of graph machine learning is witnessing a paradigm transition from shallow methods to more sophisticated deep learning approaches. The capabilities of foundation models in generalization and adaptation motivate graph machine learning researchers to discuss the potential of developing a new graph learning paradigm. This paradigm envisions models that are pre-trained on extensive graph data and can be adapted for various graph tasks. Despite this burgeoning interest, there is a noticeable lack of clear definitions and systematic analyses pertaining to this neuicew domain. To this end, this article introduces the concept of Graph Foundation Models (GFMs), and offers an exhaustive explanation of their key characteristics and underlying technologies. We proceed to classify the existing work related to GFMs into three distinct categories, based on their dependence on graph neural networks and large language models. In addition to providing a thorough review of the current state of GFMs, this article also outlooks potential avenues for future research in this rapidly evolving domain.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5229.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5229.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>edge-list serialization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edge-list serialization (graph-to-text via edge lists)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple graph-to-text conversion that linearizes a graph as a sequence of edge tuples (e.g., (nodeA, nodeB, edge_type)), usually appended with node/edge attributes in natural language to form a prompt for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>edge-list serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent the graph as a textual list of edges (each edge as a pair/triple) optionally augmented with node/edge attributes; examples and queries are appended in natural language to form prompts (used as manual prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs / text-attributed graphs (edge lists used for node/edge/graph reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Direct and explicit (faithful to pairwise connectivity); very interpretable; extremely verbose for medium/large graphs (poor compactness); can hit LLM input-length limits; preserves pairwise relations but loses global/structural summaries unless explicitly encoded.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph reasoning and standard graph tasks when expressed in natural language (e.g., cycle detection, subgraph matching, graph reasoning benchmarks assessed in NLGraph/GPT4Graph experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Survey notes edge-list descriptions are a common baseline for graph-to-text prompting but are often outperformed by more compact/local-summary strategies (e.g., neighbor summarization, self-prompting) on complex graph reasoning; they perform worse on large/complex graphs due to verbosity and LLM input-length constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Scales poorly (prompt length explosion), fails to convey higher-order/topological structure succinctly, LLMs still struggle to perform multi-hop reasoning from raw edge lists; not effective for large graphs without truncation or summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Foundation Models: Concepts, Opportunities and Challenges', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5229.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5229.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>adj-list/GML/GraphML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adjacency-list / GML / GraphML serialization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Structured textual encodings of graphs (adjacency lists or graph description languages like GML/GraphML) used as prompt input to LLMs to describe graph topology in machine-readable forms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>adjacency-list / GML / GraphML serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode the graph using adjacency lists or standard graph-markup languages (GML/GraphML) and feed that textual description as part of the prompt; this keeps structure in a somewhat canonical machine-readable layout instead of free-form sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs (used in empirical evaluations comparing description formats)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>More machine-structured than plain edge lists (less ambiguous); more compact for certain graphs; still possibly long; easier for deterministic parsing but LLMs may still lose implicit structural cues unless summarized or transformed.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Empirical graph-understanding/graph-reasoning tasks used in GPT4Graph-style benchmarking (e.g., structural reasoning tasks such as path/cycle detection, subgraph matching).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>GPT4Graph compared multiple description languages (edge list, adjacency list, GML/GraphML) and concluded that representation choice matters but that interactive/self-prompting methods often outperform purely manual description formats; no single serialized format consistently solved complex graph problems for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Still constrained by LLM input length; structural information may not be utilized efficiently by standard LLMs; formal formats can be long and require LLMs to parse syntax precisely which may not guarantee correct multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Foundation Models: Concepts, Opportunities and Challenges', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5229.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5229.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>full_vs_reduced_prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full-text vs. reduced-text prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two prompt styles where 'full text' encodes exhaustive graph information in natural language, and 'reduced text' compresses/abstracts the same information to shorten prompts for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>full-text / reduced-text prompting</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Full-text prompts include detailed natural-language descriptions of nodes/edges/attributes; reduced-text prompts compress or summarize information (e.g., abbreviating repeated patterns, omitting distant nodes) to reduce prompt length while attempting to retain salient information.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-attributed graphs and general graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Trade-off between completeness (full-text higher faithfulness) and compactness/feasibility (reduced-text better for LLM input limits); reduced prompts risk information loss but enable larger graphs to be represented within token limits.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Prompt-based graph tasks in TextForGraph-style studies where compression strategies were evaluated for prompt length vs. effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Reduced-text prompts reduce token usage and can maintain performance if important structure is preserved, but concrete comparative numbers are not reported in this survey; effectiveness depends on how information is compressed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Designing reduced prompts that preserve essential structural cues is nontrivial; compression can remove critical multi-hop information needed for reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Foundation Models: Concepts, Opportunities and Challenges', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5229.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5229.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>neighbor summarization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neighbor summarization (local neighborhood textualization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convert a node's neighborhood (neighbors, local subgraph, attributes) into a concise natural-language summary per node or per query, which is then provided to the LLM as context for node-level reasoning or classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>neighbor summarization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For a target node, generate a textual summary of its local neighborhood (e.g., neighbor identities, summary of neighbor attributes, local motifs) or include only summarized neighbor attributes in prompts; can be manual or automatically generated (automatic neighbor summaries found most effective in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>node-centric graphs / text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compact and focused on the local structure critical for many node-level tasks; preserves local multi-hop signals when summaries include aggregated neighborhood info; more scalable than full edge-list serialization; higher signal-to-noise for local queries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node-level tasks and graph reasoning benchmarks evaluated in Graph-LLM and GPT4Graph; used for neighbor-based reasoning and QA over graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Graph-LLM and GPT4Graph reported neighbor summarization as the most effective prompt engineering method among existing prompt strategies, outperforming plain edge-list descriptions for many tasks (survey reports this conclusion qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Summaries may omit structural context beyond the local neighborhood; automatic summarization quality is critical; choice of how many hops to summarize is a trade-off between informativeness and prompt length.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Foundation Models: Concepts, Opportunities and Challenges', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5229.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5229.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>self-prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-prompting / automatic prompting (graph summarization, exploration, completion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Techniques where the LLM itself generates intermediate prompts or summaries (e.g., graph summarization, neighborhood exploration, completion of partial graphs) to iteratively understand or expand graph information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>self-prompting (graph summarization / exploration / completion)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Automatic generation of prompts by the LLM to summarize the input graph, to explore neighborhoods or to complete missing parts; used as an active process where the model produces intermediate textual artifacts that help it reason about graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs (used as an adaptive prompt strategy for multiple graph tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Interactive and adaptive; can focus LLM attention on salient substructures; reduces dependence on hand-crafted prompts; potentially more compact and informative than static full-graph descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph reasoning tasks and benchmarking in GPT4Graph-style experiments; compared against manual prompting methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>GPT4Graph reports that self-prompting (automatic prompting) is more effective than manual prompting strategies in many settings; self-prompting methods (summarization/exploration/completion) improved LLM performance on graph tasks relative to static descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality depends on LLM's own ability to generate useful probes; can be computationally more expensive (multiple LLM calls); risk of compounding errors if generated summaries are inaccurate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Foundation Models: Concepts, Opportunities and Challenges', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5229.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5229.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>graph-to-token</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-token (node-as-token tokenization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation that treats nodes (or node embeddings) as discrete tokens fed into transformer/LLM backbones (possibly by expanding the vocabulary), enabling node-level tokenization rather than pure natural-language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gimlet: A unified graphtext model for instruction-based molecule zero-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-to-token (node-as-token)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Tokenize graph elements by mapping nodes or node embeddings to tokens (e.g., expand LLM vocabulary or use an encoder to produce token embeddings), and feed sequences of node tokens (plus text tokens) into a transformer/LLM; node positional or structural information is added via position embeddings or auxiliary encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>molecular graphs, multimodal graphs, large-scale graphs where node-level tokens are preferred</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Can represent large graphs more compactly (one token per node), enables joint modeling with natural language tokens, preserves node-level detail; typically requires trainable transformer backbones or access to model internals (not feasible with black-box LLM APIs); higher computational cost and engineering complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Molecule zero-shot learning (GIMLET), other multi-modal graph+text tasks; evaluated where node-level granularity is required for joint graph-text modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Graph-to-token approaches enable stronger joint modeling than pure graph-to-text prompting in tasks requiring tight coupling of graph and language representations (e.g., GIMLET for molecules), but are more costly and require model access; survey notes graph-to-token can process larger graphs but encoding graph structure for the LLM remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires access to/modify LLM internals or trainable transformer backbones; encoding explicit graph topology into token sequences remains an open challenge; higher computational and memory costs compared to text-only prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Foundation Models: Concepts, Opportunities and Challenges', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking <em>(Rating: 2)</em></li>
                <li>Gimlet: A unified graphtext model for instruction-based molecule zero-shot learning <em>(Rating: 2)</em></li>
                <li>GraphWiz: An instructionfollowing language model for graph problems <em>(Rating: 2)</em></li>
                <li>Can language models solve graph problems in natural language? <em>(Rating: 1)</em></li>
                <li>TextForGraph <em>(Rating: 1)</em></li>
                <li>Graph-LLM <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5229",
    "paper_id": "paper-264288909",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "edge-list serialization",
            "name_full": "Edge-list serialization (graph-to-text via edge lists)",
            "brief_description": "A simple graph-to-text conversion that linearizes a graph as a sequence of edge tuples (e.g., (nodeA, nodeB, edge_type)), usually appended with node/edge attributes in natural language to form a prompt for LLMs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "edge-list serialization",
            "representation_description": "Represent the graph as a textual list of edges (each edge as a pair/triple) optionally augmented with node/edge attributes; examples and queries are appended in natural language to form prompts (used as manual prompts).",
            "graph_type": "general graphs / text-attributed graphs (edge lists used for node/edge/graph reasoning tasks)",
            "representation_properties": "Direct and explicit (faithful to pairwise connectivity); very interpretable; extremely verbose for medium/large graphs (poor compactness); can hit LLM input-length limits; preserves pairwise relations but loses global/structural summaries unless explicitly encoded.",
            "evaluation_task": "Graph reasoning and standard graph tasks when expressed in natural language (e.g., cycle detection, subgraph matching, graph reasoning benchmarks assessed in NLGraph/GPT4Graph experiments).",
            "performance_metrics": null,
            "comparison_to_other_representations": "Survey notes edge-list descriptions are a common baseline for graph-to-text prompting but are often outperformed by more compact/local-summary strategies (e.g., neighbor summarization, self-prompting) on complex graph reasoning; they perform worse on large/complex graphs due to verbosity and LLM input-length constraints.",
            "limitations_or_challenges": "Scales poorly (prompt length explosion), fails to convey higher-order/topological structure succinctly, LLMs still struggle to perform multi-hop reasoning from raw edge lists; not effective for large graphs without truncation or summarization.",
            "uuid": "e5229.0",
            "source_info": {
                "paper_title": "Graph Foundation Models: Concepts, Opportunities and Challenges",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "adj-list/GML/GraphML",
            "name_full": "Adjacency-list / GML / GraphML serialization",
            "brief_description": "Structured textual encodings of graphs (adjacency lists or graph description languages like GML/GraphML) used as prompt input to LLMs to describe graph topology in machine-readable forms.",
            "citation_title": "GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking",
            "mention_or_use": "mention",
            "representation_name": "adjacency-list / GML / GraphML serialization",
            "representation_description": "Encode the graph using adjacency lists or standard graph-markup languages (GML/GraphML) and feed that textual description as part of the prompt; this keeps structure in a somewhat canonical machine-readable layout instead of free-form sentences.",
            "graph_type": "general graphs (used in empirical evaluations comparing description formats)",
            "representation_properties": "More machine-structured than plain edge lists (less ambiguous); more compact for certain graphs; still possibly long; easier for deterministic parsing but LLMs may still lose implicit structural cues unless summarized or transformed.",
            "evaluation_task": "Empirical graph-understanding/graph-reasoning tasks used in GPT4Graph-style benchmarking (e.g., structural reasoning tasks such as path/cycle detection, subgraph matching).",
            "performance_metrics": null,
            "comparison_to_other_representations": "GPT4Graph compared multiple description languages (edge list, adjacency list, GML/GraphML) and concluded that representation choice matters but that interactive/self-prompting methods often outperform purely manual description formats; no single serialized format consistently solved complex graph problems for LLMs.",
            "limitations_or_challenges": "Still constrained by LLM input length; structural information may not be utilized efficiently by standard LLMs; formal formats can be long and require LLMs to parse syntax precisely which may not guarantee correct multi-hop reasoning.",
            "uuid": "e5229.1",
            "source_info": {
                "paper_title": "Graph Foundation Models: Concepts, Opportunities and Challenges",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "full_vs_reduced_prompts",
            "name_full": "Full-text vs. reduced-text prompting",
            "brief_description": "Two prompt styles where 'full text' encodes exhaustive graph information in natural language, and 'reduced text' compresses/abstracts the same information to shorten prompts for LLMs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "full-text / reduced-text prompting",
            "representation_description": "Full-text prompts include detailed natural-language descriptions of nodes/edges/attributes; reduced-text prompts compress or summarize information (e.g., abbreviating repeated patterns, omitting distant nodes) to reduce prompt length while attempting to retain salient information.",
            "graph_type": "text-attributed graphs and general graphs",
            "representation_properties": "Trade-off between completeness (full-text higher faithfulness) and compactness/feasibility (reduced-text better for LLM input limits); reduced prompts risk information loss but enable larger graphs to be represented within token limits.",
            "evaluation_task": "Prompt-based graph tasks in TextForGraph-style studies where compression strategies were evaluated for prompt length vs. effectiveness.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Reduced-text prompts reduce token usage and can maintain performance if important structure is preserved, but concrete comparative numbers are not reported in this survey; effectiveness depends on how information is compressed.",
            "limitations_or_challenges": "Designing reduced prompts that preserve essential structural cues is nontrivial; compression can remove critical multi-hop information needed for reasoning.",
            "uuid": "e5229.2",
            "source_info": {
                "paper_title": "Graph Foundation Models: Concepts, Opportunities and Challenges",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "neighbor summarization",
            "name_full": "Neighbor summarization (local neighborhood textualization)",
            "brief_description": "Convert a node's neighborhood (neighbors, local subgraph, attributes) into a concise natural-language summary per node or per query, which is then provided to the LLM as context for node-level reasoning or classification.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "neighbor summarization",
            "representation_description": "For a target node, generate a textual summary of its local neighborhood (e.g., neighbor identities, summary of neighbor attributes, local motifs) or include only summarized neighbor attributes in prompts; can be manual or automatically generated (automatic neighbor summaries found most effective in experiments).",
            "graph_type": "node-centric graphs / text-attributed graphs",
            "representation_properties": "Compact and focused on the local structure critical for many node-level tasks; preserves local multi-hop signals when summaries include aggregated neighborhood info; more scalable than full edge-list serialization; higher signal-to-noise for local queries.",
            "evaluation_task": "Node-level tasks and graph reasoning benchmarks evaluated in Graph-LLM and GPT4Graph; used for neighbor-based reasoning and QA over graphs.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Graph-LLM and GPT4Graph reported neighbor summarization as the most effective prompt engineering method among existing prompt strategies, outperforming plain edge-list descriptions for many tasks (survey reports this conclusion qualitatively).",
            "limitations_or_challenges": "Summaries may omit structural context beyond the local neighborhood; automatic summarization quality is critical; choice of how many hops to summarize is a trade-off between informativeness and prompt length.",
            "uuid": "e5229.3",
            "source_info": {
                "paper_title": "Graph Foundation Models: Concepts, Opportunities and Challenges",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "self-prompting",
            "name_full": "Self-prompting / automatic prompting (graph summarization, exploration, completion)",
            "brief_description": "Techniques where the LLM itself generates intermediate prompts or summaries (e.g., graph summarization, neighborhood exploration, completion of partial graphs) to iteratively understand or expand graph information.",
            "citation_title": "GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking",
            "mention_or_use": "mention",
            "representation_name": "self-prompting (graph summarization / exploration / completion)",
            "representation_description": "Automatic generation of prompts by the LLM to summarize the input graph, to explore neighborhoods or to complete missing parts; used as an active process where the model produces intermediate textual artifacts that help it reason about graph structure.",
            "graph_type": "general graphs (used as an adaptive prompt strategy for multiple graph tasks)",
            "representation_properties": "Interactive and adaptive; can focus LLM attention on salient substructures; reduces dependence on hand-crafted prompts; potentially more compact and informative than static full-graph descriptions.",
            "evaluation_task": "Graph reasoning tasks and benchmarking in GPT4Graph-style experiments; compared against manual prompting methods.",
            "performance_metrics": null,
            "comparison_to_other_representations": "GPT4Graph reports that self-prompting (automatic prompting) is more effective than manual prompting strategies in many settings; self-prompting methods (summarization/exploration/completion) improved LLM performance on graph tasks relative to static descriptions.",
            "limitations_or_challenges": "Quality depends on LLM's own ability to generate useful probes; can be computationally more expensive (multiple LLM calls); risk of compounding errors if generated summaries are inaccurate.",
            "uuid": "e5229.4",
            "source_info": {
                "paper_title": "Graph Foundation Models: Concepts, Opportunities and Challenges",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "graph-to-token",
            "name_full": "Graph-to-token (node-as-token tokenization)",
            "brief_description": "A representation that treats nodes (or node embeddings) as discrete tokens fed into transformer/LLM backbones (possibly by expanding the vocabulary), enabling node-level tokenization rather than pure natural-language prompts.",
            "citation_title": "Gimlet: A unified graphtext model for instruction-based molecule zero-shot learning",
            "mention_or_use": "mention",
            "representation_name": "graph-to-token (node-as-token)",
            "representation_description": "Tokenize graph elements by mapping nodes or node embeddings to tokens (e.g., expand LLM vocabulary or use an encoder to produce token embeddings), and feed sequences of node tokens (plus text tokens) into a transformer/LLM; node positional or structural information is added via position embeddings or auxiliary encoders.",
            "graph_type": "molecular graphs, multimodal graphs, large-scale graphs where node-level tokens are preferred",
            "representation_properties": "Can represent large graphs more compactly (one token per node), enables joint modeling with natural language tokens, preserves node-level detail; typically requires trainable transformer backbones or access to model internals (not feasible with black-box LLM APIs); higher computational cost and engineering complexity.",
            "evaluation_task": "Molecule zero-shot learning (GIMLET), other multi-modal graph+text tasks; evaluated where node-level granularity is required for joint graph-text modeling.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Graph-to-token approaches enable stronger joint modeling than pure graph-to-text prompting in tasks requiring tight coupling of graph and language representations (e.g., GIMLET for molecules), but are more costly and require model access; survey notes graph-to-token can process larger graphs but encoding graph structure for the LLM remains challenging.",
            "limitations_or_challenges": "Requires access to/modify LLM internals or trainable transformer backbones; encoding explicit graph topology into token sequences remains an open challenge; higher computational and memory costs compared to text-only prompting.",
            "uuid": "e5229.5",
            "source_info": {
                "paper_title": "Graph Foundation Models: Concepts, Opportunities and Challenges",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking",
            "rating": 2,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        },
        {
            "paper_title": "Gimlet: A unified graphtext model for instruction-based molecule zero-shot learning",
            "rating": 2,
            "sanitized_title": "gimlet_a_unified_graphtext_model_for_instructionbased_molecule_zeroshot_learning"
        },
        {
            "paper_title": "GraphWiz: An instructionfollowing language model for graph problems",
            "rating": 2,
            "sanitized_title": "graphwiz_an_instructionfollowing_language_model_for_graph_problems"
        },
        {
            "paper_title": "Can language models solve graph problems in natural language?",
            "rating": 1,
            "sanitized_title": "can_language_models_solve_graph_problems_in_natural_language"
        },
        {
            "paper_title": "TextForGraph",
            "rating": 1,
            "sanitized_title": "textforgraph"
        },
        {
            "paper_title": "Graph-LLM",
            "rating": 2
        }
    ],
    "cost": 0.01828875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Graph Foundation Models: Concepts, Opportunities and Challenges
10 Mar 2025</p>
<p>Jiawei Liu 
Cheng Yang 
Zhiyuan Lu 
Junze Chen 
Yibo Li 
Mengmei Zhang 
Ting Bai 
Yuan Fang 
Lichao Sun 
Philip S Yu 
Chuan Shi 
Graph Foundation Models: Concepts, Opportunities and Challenges
10 Mar 2025FC3A0C57AC9E626486B5D7F79A6C94F9arXiv:2310.11829v4[cs.LG]Graph Foundation Models, Large Language Models
Foundation models have emerged as critical components in a variety of artificial intelligence applications, and showcase significant success in natural language processing and several other domains.Meanwhile, the field of graph machine learning is witnessing a paradigm transition from shallow methods to more sophisticated deep learning approaches.The capabilities of foundation models in generalization and adaptation motivate graph machine learning researchers to discuss the potential of developing a new graph learning paradigm.This paradigm envisions models that are pre-trained on extensive graph data and can be adapted for various graph tasks.Despite this burgeoning interest, there is a noticeable lack of clear definitions and systematic analyses pertaining to this new domain.To this end, this article introduces the concept of Graph Foundation Models (GFMs), and offers an exhaustive explanation of their key characteristics and underlying technologies.We proceed to classify the existing work related to GFMs into three distinct categories, based on their dependence on graph neural networks and large language models.In addition to providing a thorough review of the current state of GFMs, this article also outlooks potential avenues for future research in this rapidly evolving domain.</p>
<p>INTRODUCTION</p>
<p>W ITH the rise in computational power and breakthroughs in deep learning techniques, the artificial intelligence (AI) community has introduced the notion of "foundation models": A foundation model is any model that is trained on broad data and can be adapted to a wide range of downstream tasks [1].Foundation models enjoy unique attributes like emergence and homogenization, empowering them to serve as the primary building blocks for a myriad of downstream AI applications [1].Emergence suggests that as a foundation model scales up, it may spontaneously manifest novel capabilities [2].Meanwhile, homogenization alludes to the model's versatility, enabling its deployment across diverse applications [1].Thanks to the development of large language models (LLMs), the concept of foundation models first became a reality in natural language processing (NLP).Since then, foundation models have demonstrated impressive versatility, processing not just text but also image data, video data, audio data and multi-modal inputs.This versatility empowers them to excel in tasks ranging from computer vision [3] and audio signal processing [4] to recommender systems [5].</p>
<p>Much like the evolution witnessed in NLP, graph machine learning is also undergoing a paradigm transition.In its early</p>
<p> Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Ting Bai and Chuan Shi are with School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China.E-mail: {liu jiawei, yangcheng, luzy, junze, yiboL, baiting, shichuan}@bupt.edu.cn Mengmei Zhang is with China Telecom Bestpay, Beijing, China.E-mail: zhangmengmei@bestpay.com.cn Yuan Fang is with School of Computing and Information Systems, Singapore Management University, Singapore.E-mail: yfang@smu.edu.sg Lichao Sun is with Lehigh University, Bethlehem, Pennsylvania, USA.Email: lis221@lehigh.edu Philip S. Yu is with University of Illinois Chicago, Chicago, USA.E-mail: psyu@uic.edu Jiawei Liu and Cheng Yang contributed equally to this research.</p>
<p> Corresponding author: Chuan Shi stages, graph tasks predominantly employed shallow methods, such as random walk [6,7] and matrix factorization [8,9,10,11,12].These methods, however, were typically limited to transductive learning [13].The more recent shift towards deep learning methods has catalyzed the rise of graph neural networks (GNNs).GNNs have revolutionized the landscape by introducing the message-passing mechanism, where nodes iteratively aggregate information from their neighbors.By harnessing GNNs in fully supervised, semi-supervised, or unsupervised settings, researchers have pioneered a variety of customized graph models.These advancements have yielded substantial improvements in tasks like node classification [14], link prediction [15], graph classification [16], and graph clustering [17].However, certain challenges of GNN models still persist.For example, GNNs are restricted with issues related to expressive power [18] and generalizability [19], especially given the ever-expanding datasets and the widening spectrum of tasks.</p>
<p>The remarkable success of foundation models in varied domains is increasingly garnering the interest of graph machine learning researchers.This naturally evokes the question: Could graph foundation models represent the next frontier in graph machine learning?Such models, if realized, would boast enhanced expressive power, improved transferability, and applicability to more intricate graph data and tasks.As illustrated in Figure 1, a graph foundation model (GFM) is envisioned as a model pretrained on extensive graph data, primed for adaptation across diverse downstream graph tasks.Drawing parallels with traditional foundation models, a GFM is also anticipated to embody two principal characteristics: emergence and homogenization.Specifically, emergence refers to novel capabilities shown exclusively in largescale graph models, while homogenization denotes the model's adaptability across different types of graph tasks.Existing deep graph learning methods struggle to encompass these features: their inherent architectures and learning paradigms focus on specific tasks, which restrict the utilization of extensive unlabeled data, Fig. 1: The distinction between deep graph learning and graph foundation models.Deep graph learning tackles specific tasks on specific datasets through end-to-end training.In contrast, graph foundation models (GFMs) are pre-trained on broad graph data and can be adapted to a wide range of downstream graph tasks, expected to demonstrate emergence and homogenization capabilities.</p>
<p>subsequently limiting their expressive and generalization abilities.</p>
<p>Inspired by the success of LLMs as foundation models in NLP, researchers have explored the possibilities of graph foundation models towards the emergence and homogenization capabilities.These explorations primarily focus on the design of backbone architectures for GFMs, and different learning paradigms including pre-training and adaptation, as they are the key strategies of LLMs to acheive the aforementioned capabilities.First and foremost, the emergent abilities of foundation models typically exist only in backbones with a large number of parameters, whereas the parameter count of GNNs is significantly smaller than that of the language backbones.This implies that the backbone of GFMs may need to be redesigned to achieve more substantial knowledge storage towards emergence.As graph data is typically associated with rich text information, an alternative approach is to use LLMs as GFMs.Nonetheless, it remains uncertain whether LLMs can effectively handle graph data and associated tasks, and it is crucial to determine how to model graph structures in LLMs.Additionally, the homogenization of foundation models necessitates the handling of diverse tasks in a uniform manner.Devising effective pre-training tasks (also called pretext tasks) and downstream task adaptation methods are challenging for graph data, due to the complexity in interconnected nodes and various forms of attributes, as well as the diversity in tasks across node-, edge-and graph-levels.Therefore, there is also a need to design suitable pre-training tasks and adaptation mechanisms.</p>
<p>While there is no definitive solution for designing and implementing GFMs, this paper surveys some related researches and categorizes them into three distinct approaches based on their reliance on GNNs and LLMs: (1) GNN-based Models: They aim to enhance existing graph learning paradigms through innovations in the backbone, pre-training, and adaptation aspects; (2) LLMbased Models: They explore the feasibility of using an LLM as a GFM by converting graphs into text or tokens; (3) GNN+LLMbased Models: They explore various forms of synergy between GNNs and LLMs to empower them with enhanced capabilities.</p>
<p>To the best of our knowledge, this is the first survey towards graph foundation models.Existing surveys of foundation models typically explore modalities such as language and vision [1,20], rather than graphs.Additionally, there are two surveys [21,22] dedicated to knowledge graphs and large language models, but knowledge graphs, due to their distinct nature in construction and application, fall outside the scope of this article.We have also noticed a very recent article that mentions the concept of large graph models [23], but it emphasizes opinion statements and lacks a systematic taxonomy.Therefore, the contributions of this article can be summarized as follows:</p>
<p> This article defines the concept of graph foundation models for the first time, and examines the core issues and characteristics of their capabilities.</p>
<p> This article introduces a novel taxonomy and discusses the strengths and limitations of each approach towards graph foundation models.</p>
<p> This article provides promising future directions towards graph foundation models.</p>
<p>The subsequent sections are organized as follows.In Section 2, we introduce the background related to GFMs.Section 3 defines GFMs and highlights their similarities and differences with language foundation models.Sections 4 -6 delve into the relevant works that consider GNN-based models, LLM-based models and GNN+LLM-based models as GFMs, separately.Section 7 engages in a discussion on the future directions of GFMs.In Section 8, we summarize the key points of this paper.</p>
<p>BACKGROUND</p>
<p>Before introducing GFMs, we review background knowledge on deep graph learning and language foundation models.</p>
<p>Deep Graph Learning</p>
<p>This section provides a concise overview from three key aspects: graph data, backbone architectures, and learning paradigms.</p>
<p>Graph Data</p>
<p>Graphs capture intricate relationships among entities and possess several key characteristics that make them challenging for machine learning tasks.The primary challenge stems from their (1) Non-Euclidean Nature: Graph data is inherently non-Euclidean [24], lacking the rigid geometric structure of traditional data formats such as 1D text, 2D images or tabular data.This means that graph data cannot be adequately described within a simple flat space because its intrinsic structure does not conform to the principles of Euclidean geometry [25].Unlike Euclidean data, which often comes in predefined shapes (e.g., images of a specific resolution), non-Euclidean data can vary greatly in size and shape.This variability complicates the design of algorithms that often navigate complex topologies, significantly increasing computational cost compared to operations on simpler Euclidean spaces.</p>
<p>Beyond this fundamental structural complexity, two additional challenges are posed by the nature of graph data.(2) Various Domains: Graph data appears in domains such as social networks [26], biology [27], and transportation [28].It is also used in tasks like 3D human skeleton recognition [29], semantic segmentation [30], and video classification [31].Domain-specific variability with different node types and edge semantics makes creating a universal model challenging [32].(3) Various Types: graph data includes homogeneous, heterogeneous [33], hyper- [34], and dynamic ones [35].Such diversity also brings challenges to deep graph learning.</p>
<p>Backbone Architectures</p>
<p>GNNs are the current mainstream backbone architecture for deep graph learning.Most GNNs follow the message-passing framework [36], enabling nodes to exchange information with neighbors.For example, GCN [14] introduces graph convolution layers, GraphSAGE [37] generates node embeddings using inductive learning, and GAT [38] adds an attention mechanism to weigh neighbor importance, enhancing expressive power.These contributions make GNNs versatile tools for deep graph learning.</p>
<p>However, deepening GNNs is challenging.Increasing layers leads to over-smoothing, where node representations become similar [39], and over-squashing, where information is overly compressed [18].Efforts like DropEdge [40], which randomly removes edges, improve GCN performance and scalability.Graph transformers [41,42,43], with their fully connected attention and long-range relationship modeling, help alleviate over-smoothing and over-squashing [44].</p>
<p>Learning Paradigms</p>
<p>The learning paradigms for deep graph learning encompass three primary categories: Supervised learning.In supervised learning, algorithms use a training dataset with input data and output labels.This is used in tasks like graph classification [45] and graph regression [46].For instance, in molecular property prediction [47], GNNs predict chemical properties using labeled data, aiding drug development and materials research.</p>
<p>Semi-supervised learning.Semi-supervised learning, as discussed in [48], is a primary focus in deep graph learning.It uses both labeled and unlabeled data to improve model performance, with node classification [14] being a key application.The message-passing mechanism [36] allows GNNs to exchange information among nodes, incorporating both data types for predictions.GNNs can also combine with methods like label propagation for better performance [49].</p>
<p>Unsupervised learning.Unsupervised learning discovers patterns and structures without manual labels.Graph clustering identifies structures based on node relationships, while link prediction estimates missing connections.A subclass, self-supervised learning, generates labels from the data itself [50], allowing GNNs to be trained end-to-end for tasks like graph clustering [17] and link prediction [15].</p>
<p>Language Foundation Models</p>
<p>AI is currently undergoing a transformative shift marked by the emergence of some specific language models (such as GPT-3) that are trained on extensive and diverse datasets using self-supervised learning.These models, known as foundation models, are able to produce a wide array of outputs, enabling them to tackle a broad spectrum of downstream tasks.In contrast to the deep graph learning pipeline, the foundation model's approach embraces a pre-training and adaptation framework, enabling the model to achieve several significant advancements, including the emergence [2] and homogenization [1].Foundation models have primarily established themselves in the field of NLP [1], so our discussion will focus on language foundation models in this section.</p>
<p>Language Data</p>
<p>Language data refers to text or spoken content in a human language, encompassing the grammar rules of the natural language and the associated semantics of the words.The quality and quantity of language data play a crucial role in the performance of NLP systems, impacting their accuracy, robustness, and overall effectiveness in various language tasks.In contrast to graph data, language data as Euclidean data is easier to model, and its rich semantic information significantly enhances the knowledge transferability of language models.</p>
<p>Backbone Architectures</p>
<p>An early breakthrough in foundation models is pre-trained language models (PLMs), designed to capture context-aware word representations, which proved remarkably effective as versatile semantic features.Furthermore, researchers have observed that increasing the scale of PLMs, whether by augmenting model size or training data, frequently results in increased model capacity for downstream tasks.These larger PLMs, collectively referred to as LLMs, exhibit emergent abilities [2] compared to their smaller counterparts (e.g., the 1.5B-parameter GPT-2 and 175B-parameter GPT-3).LLMs primarily utilize the Transformer architecture, because highly parallelizable Transformer-based architectures accelerate the pre-training stage and enable the utilization of massive datasets.In Transformer models, tokens serve as the input and represent units at the word level in natural language texts.</p>
<p>Learning Paradigms</p>
<p>As the number of model parameters has rapidly increased, the demand for significantly larger datasets has grown to effectively train these parameters and avoid overfitting.Given the extremely expensive costs associated with building large-scale labeled datasets, the importance of utilizing extensive unlabeled text data has been underscored.Leveraging these unlabeled datasets involves a twostep approach: first, achieving universal representations through self-supervised learning, and subsequently employing these representations for various tasks [51].Based on different adaptation approaches, learning paradigms can be categorized into two types: pre-train and fine-tune and pre-train, prompt, and predict [52].</p>
<p>Pre-train and Fine-tune.In this paradigm, a model is initially pre-trained as a language model (LM), where it predicts the probability of observed textual data.Following the pre-training phase, we need to tune the model for specific tasks, which is known as fine-tuning.Building upon the success of models like ULMFit [53] and BERT [54], fine-tuning has emerged as the predominant method for adapting pre-trained models.In this framework, the primary emphasis lies in objective engineering, encompassing the design of training objectives for both pre-training and finetuning phases.The advantage of fine-tuning is that it can transfer knowledge between source and target tasks (or domains) and benefit the model's performance.For the small size of fine-tuning dataset compared to pre-training dataset, this process can enable adaptation effectively without losing much pre-trained knowledge.</p>
<p>Pre-train, Prompt and Predict.In this paradigm, rather than adjusting PLMs to suit specific downstream tasks, the approach involves reshaping the downstream tasks to align more closely with those tackled during the original LM training, accomplished by providing textual prompts.From the aspect of prompt engineering, the approaches to create a proper prompts can be classified to manual methods and automated methods.Manual methods involve creating intuitive templates based on human insight, which is the most straightforward approach to crafting prompts.Manual methods face challenges in terms of high cost and precision.To address these issues, some approaches have started to experiment with automated prompt generation.When LLMs have billions of parameters, it is more effective if we can just adapt the downstream tasks without adjusting model parameters.This makes prompttuning a promising approach for adapting LLMs.</p>
<p>GRAPH FOUNDATION MODELS</p>
<p>In this section, we will first formally define the concepts of graph foundation models, including the definition, key characteristics and key technologies.Then, we will discuss the similarities and differences between graph and language foundation models.</p>
<p>Definition and Key Characteristics</p>
<p>We define a graph foundation model as follows:</p>
<p>Definition A graph foundation model (GFM) is a model that is expected to benefit from the pre-training of broad graph data, and can be adapted to a wide range of downstream graph tasks.</p>
<p>Compared to deep graph learning that employs end-to-end training, GFMs use pre-training to obtain the knowledge from a substantial amount of unlabeled graph data, and then use adaptation techniques to tailor to various downstream tasks.Some studies [55,56] have already demonstrated that the paradigm of pre-training and adaptation outperform deep graph learning in certain scenarios, e.g., few-shot learning [55], showcasing their superior expressive power and generalization ability.Unlike deep graph learning that aims to achieve better performance on a single task, a GFM is expected to have two key characteristics: emergence and homogenization.</p>
<p>Emergence.Emergence means that the graph foundation model will exhibit some new abilities when having a large parameters or trained on more data, which are also referred to as emergent abilities.Inspired by the various emergent abilities [57,58] possessed by foundation models, we expect GFMs to have similar abilities, including in-context learning, graph reasoning, and zeroshot graph generation, etc. In-context learning allows predictions for various downstream tasks with few examples [59].Graph reasoning decomposes a complex problem into multiple subproblems based on the graph structure and addresses them step by step, such as solving graph algorithm problems [60].Zeroshot graph generation requires the model to generate graphs based on the desired conditions without the need for any examples [61].Note that although language foundation models have demonstrated various emergent abilities, only a few works [59,60,61] have explored emergent abilities of GFMs so far.</p>
<p>Homogenization.Homogenization means that the graph foundation model can be applied to different formats of tasks, such as node classification, link prediction and graph classification.Note that due to the distinct characteristics of tasks on graphs compared to NLP tasks, achieving homogenization is not straightforward.The fundamental question in achieving homogenization is to decide which form to unify different types of graph tasks.</p>
<p>Existing works have attempted homogenization through link prediction [55] or graph-level tasks [56], but there is no consensus on which approach is superior.</p>
<p>Key Technologies</p>
<p>Graph foundation models primarily comprise two key techniques: pre-training and adaptation.This section will provide a brief overview of these two techniques.</p>
<p>Pre-training.Pre-training is a pivotal concept in the development of graph foundation models, akin to its role in language models.It involves pre-training a neural network on a large graph dataset in a self-supervised manner.During pre-training, the model learns to capture the structural information, relationships, and patterns within the graph.There are several pre-training strategies for graph foundation models.Contrastive self-supervised learning [62,63] leverages the idea of learning representations by contrasting positive samples (e.g., similar node pairs) against negative samples (e.g., dissimilar node pairs).Generative self-supervised learning [64,65] encourages the model to reconstruct the structure or predict the features of original graph data.If using LLM as a part of the graph foundation model, we can also employ the pre-training methods introduced in Section 2.2.3.These diverse pre-training approaches enable graph foundation models to learn meaningful representations from raw graph data, enhancing their generalization and adaptability across various graph tasks.</p>
<p>Adaptation.The adaptation of graph foundation models involves tailoring these models to specific downstream tasks or domains to enhance their performance.This process includes several techniques, i.e., vanilla fine-tuning, parameter-efficient fine-tuning and prompt-tuning.Vanilla fine-tuning (Vanilla FT) entails training the entire pre-trained model on task-specific data, allowing for the highest level of customization but often requiring substantial data and computational resources.Parameter-efficient fine-tuning (Parameter-efficient FT) [66,67], on the other hand, adjusts only a subset of the model's parameters, striking a balance between taskspecific adaptation and resource efficiency.Prompt-tuning [56,68] is a versatile approach that relies on external prompts to guide the model's behavior, making it more adaptable and effective.These adaptation techniques enable graph foundation models to excel in a wide range of applications by leveraging their pretrained knowledge while tailoring their capabilities to specific tasks or domains, making them valuable for diverse downstream applications.Note that although LLMs have developed various types of prompt-tuning methods [52] and some other efficient tuning methods, such as Prefix Tuning [69], there are relatively few prompt tuning methods for graph foundation models.</p>
<p>Comparison between GFMs and LLMs</p>
<p>Through conceptual comparison, we can observe similarities in the goals and learning paradigms between graph foundation models (GFMs) and language foundation models (commonly referred to as large language models, LLMs).However, the uniqueness of graph data and graph tasks creates fundamental differences between them, which we refer to as their intrinsic differences.Furthermore, due to the relatively limited research on GFMs at present, many issues that have been extensively explored in LLMs remain unresolved, which we refer to as their extrinsic differences.We summarize the similarities and differences between GFMs and LLMs in Table 1, and will delve into them in detail in this section.</p>
<p>Similarities</p>
<p>As shown in Table 1, both language foundation models and graph foundation models share the common goal of enhancing a model's expressive power and improving its ability to generalize across a wide range of tasks.They aim to create versatile, pre-trained models that can be adapted for specific applications.In addition, both follow the pre-training and adaptation paradigm.They begin by pre-training a model on a large, diverse dataset and then adapt it to task-specific data.</p>
<p>Intrinsic Differences</p>
<p>The intrinsic differences between GFM and LLM primarily manifest in two aspects: data and tasks.As for input data, language foundation models are primarily designed for processing Euclidean data, i.e., text.They are trained on vast text corpora, which are inherently sequential and follow a linear order of words or tokens.On the other hand, GFMs are designed to handle non-Euclidean data (represented as graph structures) or a mixture of Euclidean data (like graph attributes) and non-Euclidean data.</p>
<p>Compared to text data, graph data can capture complex data relationships and is typically more sparse.Moreover, different graphs may exhibit significant differences in type or structure/geometry, all of which pose challenges in the design of GFMs.Furthermore, language data, even when sourced from texts in different domains, still share a common vocabulary.On the other hand, different graph data may lack this common foundation.For instance, nodes represent atoms in a molecular graph, while nodes represent users in a social network, which are entirely different.Furthermore, graphs from different domains can have different structures.Some graphs have a more hierarchical structure, while others may have a more cyclical structure [70].Moreover, for a single graph, the different regions can exhibit different structures.For example, In recommender systems, the user-user subgraph and item-item subgraph generally exhibit very distinct structures [71].</p>
<p>In addition, LLMs are typically designed to handle dozens of tasks [72], but these tasks can all be unified under the format of masked language modeling.The reason is that these tasks all involve processing textual data and using the syntax and semantic information within the text.In contrast, GFMs focus a narrower set of tasks but with diverse formats.They excel at graph tasks such as node classification, link prediction and graph classification.The differences in tasks imply that GFMs cannot be learned using methods similar to those in LLMs, significantly increasing the adaptability challenges of GFMs in downstream tasks.</p>
<p>Extrinsic Differences</p>
<p>In addition to the intrinsic differences in data and tasks, there are also some extrinsic differences between GFMs and LLMs, which are due to the lag in technological advancements in GFMs.This section summarizes these differences as follows:</p>
<p>Backbone Architectures.LLMs, such as GPT-3 [73] and LLaMA [74], are mostly based on the Transformer architecture.The advantages of Transformer in terms of expressive power, scalability, parallelizability, and its excellent performance in handling various NLP tasks have made it the mainstream backbone architecture for LLMs.However, for GFMs, using mainstream GNNs as the backbone architecture may not necessarily be suitable.This is mainly because the expressive power and generalization of GNNs have limitations, and their parameter sizes are often too small to exhibit emergent abilities.To enhance the expressiveness of capturing the graph structure, recent works [70,71,75] efforts to extend GNN to mixed curvature Riemannian space, or to design graph transformers [63] or models that incorporate LLMs [76].However, there is still no unified backbone architecture for GFMs.</p>
<p>Homogenization.LLMs are relatively easy to homogenize.This means that various NLP tasks can be formulated as the same task [77], making it possible to use a single model with unified training paradigm for a wide range of tasks.However, due to the poor transferability of graph structural knowledge, homogenization is more challenging for GFMs.Existing works attempt to achieve homogenization by unifying various tasks as link prediction [55] or graph-level tasks [56].Additionally, constructing a data-task heterogeneous graph [59] may establish connections between tasks, but it is a more complex process.</p>
<p>Domain Generalization.LLMs have demonstrated strong domain generalization capabilities.They can often perform well on tasks and datasets that were not seen during training, showcasing their ability to generalize across various language-related domains.However, due to the diversity and lack of unified vocabulary of graph data, GFMs generally exhibit weaker generalization across datasets, especially when moving to cross-domain graph data [78].Their performance may degrade significantly when faced with graph structures or characteristics that differ substantially from their training data.Achieving strong domain generalization remains a challenging research problem for GFMs.</p>
<p>Emergence.LLMs have shown emergent abilities, where they can generate coherent and contextually relevant text based on few examples or instructions.Representative emergent abilities include in-context learning [57], chain of thought reasoning [58] and zero-shot generation [79].However, GFMs have not demon-strated obvious emergent abilities to the same extent as language foundation models.Only a few recent studies discuss the incontext learning [59], graph reasoning [60] and zero-shot graph generation [61] abilities of GFMs.</p>
<p>Summary</p>
<p>In this section, we define the concept of graph foundation models and related technologies, and compares graph foundation models with language foundation models.If readers wish to have a more comprehensive understanding of the concept of GFMs, they can refer to our supplementary materials A and B, where we introduce the impact of graph data and graph tasks on GFMs.In the following sections, we will introduce three categories of methods for implementing graph foundation models, along with representative works for each method.Specifically, GNN-based models use GNN as the backbone architecture, while LLMbased models transform the graph into the input format of LLM and use LLM as the backbone architecture.GNN+LLM-based models, on the other hand, utilize both GNN and LLM as the backbone architecture simultaneously.The distinction in backbone architecture also impacts the methods for pre-training and adaptation.Therefore, in the following sections, we will introduce the backbone architectures, pre-training, and adaptation strategies for each category of methods, seperately.</p>
<p>GNN-BASED MODELS</p>
<p>Thanks to effective model architectures and training paradigms, language models have achieved remarkable performance in natural language processing tasks.The backbone, pre-training and adaptation techniques employed in language models have inspired a series of corresponding efforts in the field of graph-based tasks.In this section, we will delve into GNN-based models, which draw inspiration from the model architectures or training paradigms used in NLP to apply them to graph tasks.Importantly, unlike the LLMbased models and GNN+LLM-based models to be introduced in the following sections, GNN-based models do not explicitly model text data in their pipeline.We have summarized and categorized the works mentioned in this section in supplemental material C.</p>
<p>Backbone Architectures</p>
<p>Numerous GNNs have been proposed and widely used in various graph-related downstream tasks.These networks are leveraged for feature extraction, often serving as the foundational components of graph models, commonly referred to as "backbones".In this subsection, we introduce two advanced GNN backbones: messagepassing-based and transformer-based methods.</p>
<p>Message Passing-Based Methods</p>
<p>Message Passing Neural Networks (MPNNs) [36] represent a broad category of GNN architectures that operate based on the concept of message passing between nodes in a graph.In the message passing mechanism, each node aggregates information from its neighboring nodes, processes the information, and then sends messages to its neighbors in a series of iterative steps.A typical message passing process can be formulated as :
h k+1 v = U k (h k v , M k uN (v) (h k v , h k u , X e (u,v) )),(1)
where h k v and h k+1 v denote the embedding of node v at layer k and layer k + 1, X e (u,v) denotes the edge attribute of edge (u, v),  Many existing GNN-based models utilize message passingbased models as their backbone.Due to the simplicity and effectiveness, several studies [56,59,62,80,81,82] adopt GCN [14] as their backbone architecture, where GCN [14] employs a localized first-order approximation of spectral graph convolutions for the dual purpose of capturing graph structure and encoding node features.Several studies [56,59,64,65] adopt GAT [38] as their backbone architecture, where GAT [38] replaces the average aggregation operation in GCN with a weighted aggregation approach, facilitated by an attention mechanism.Additionally, a multi-head attention technique can be further used in GAT to enhance its performance.GPPT [68] and VPGNN [83] uses Graph-SAGE [37] as their backbone, which operates by sampling a fixedsize subset of neighboring nodes for each target node and then learns embeddings by aggregating and processing these sampled neighbors' embeddings.Unlike global attention, HGT employs type-specific parameters to define heterogeneous attention over each edge within the graph.To improve the expressive power, a proportion of studies [55,66,84,85,86,87] rely on GIN [16] as their primary architecture, where GIN is a message passing-based model with expressive power theoretically equivalent to a 1-WL test [88].Due to the expressive power of GIN, it is frequently chosen as the backbone for many GNN-based graph models.For an in-depth exploration of message passing-based GNNs, we recommend referring to [50,89,90].</p>
<p>Graph Transformer-Based Methods</p>
<p>While GNNs have demonstrated significant success in learning from graph data, they still confront fundamental limitations, including issues related to limited expressive power [16], oversmoothing [39] and over-squashing [18].In parallel, the transformer architecture [91], which has revolutionized tasks in natural language processing [54,73] and computer vision [92,93], achieving the state-of-the-art performance.It has inspired the development of transformer-based models tailored for graph data [41,42,43,94].Graph transformers have exhibited promising results, particularly in molecular prediction tasks [41], owing to their fully-connected self-attention mechanism.This mechanism enables them to address the shortcomings of traditional messagepassing GNNs thanks to its long-range modeling capability and strong expressive power.</p>
<p>The principal distinction between the backbone architectures with message passing mechanism and the graph transformer lies in their treatment of the underlying graph structure.In the case of the graph transformer, it treats the graph as if it were fully connected, meaning it considers and measures the similarity between every pair of nodes in the graph.Conversely, the message passing mechanism operates under the constraint of the adjacency matrix of the graph.It only propagates information between nodes that are explicitly connected in the graph structure.We illustrate the difference between message passing-based models and graph transformers in Figure 2.</p>
<p>Currently, there are many research efforts focusing on graph transformers.Here we will present part of these studies that employ a pre-training and fine-tuning learning paradigm: Graph-BERT [95] uses intimacy-based and hop-based relative positional embeddings to encode node positions in a subgraph.The intimacybased relative positional embeddings capture the relative positions of nodes in a subgraph based on their connectivity patterns.The hop-based relative distance embeddings capture the relative positions of nodes in a subgraph based on their hop distances.GROVER [96] uses a variant of MPNN called Directed Message Passing Networks (DyMPNs), which can capture the directed nature of molecular graphs and distinguish different types of edges.The DyMPNs in GROVER are used to compute the node and edge embeddings in the Transformer-style architecture.Graphormer [41] uses spatial encoding to represent node relationships, adding shortest path distance as a bias in softmax attention for better spatial dependency capture.Building upon this foundation, G-Adapter [67] introduces a parameter-efficient finetuning approach for graph transformers, utilizing Graphormer as its backbone model.For a more comprehensive exploration, please refer to other literature on graph transformers [97].</p>
<p>Pre-training</p>
<p>Pre-training in the field of NLP involves exposing a model to a vast amount of unlabeled text data, allowing it to learn general language semantic knowledge in a self-supervised manner.This pre-training step equips the model with a foundational understanding of language, enabling it to transfer this knowledge to downstream tasks.Similarly, the graph domain typically includes many unlabeled nodes and graphs, providing opportunities for pre-training on graphs.Graph pre-training enables the graph models to understand graph structure and semantic information, thus encoding meaningful node or graph embeddings [89,90].Recently, some graph pre-training methods have been proposed to learn representations in a self-supervised manner.Based on selfsupervised tasks, graph pre-training methods can be categorized into two types: contrastive methods and generative methods.</p>
<p>Contrastive Methods</p>
<p>Specifically, the contrastive graph pre-training methods aim to maximize mutual information between different views [89], which forces the model to capture invariant semantic information across various views.The graph view can vary in scale, encompassing local, contextual or global perspectives.These perspectives correspond to node-level, subgraph-level, or graph-level information within the graph, leading to two distinct categories: (1) Samescale contrastive learning and (2) Cross-scale contrastive learning.Same-scale contrastive learning compares two graph views at the same level.For example, GCC [84] uses a node's subgraph embedding as its representation, treating subgraphs of the same node as positives and different nodes as negatives.It applies NCE loss to align positives and separate negatives, capturing general patterns.GraphCL [85] and GRACE [62] generate two views by graph augmentation and then employ the InfoNCE loss to contrast node-level embeddings, pushing the graph model to acquire the invariant representations.MA-GCL [82] focuses on manipulating the neural architectures of view encoders instead of perturbing graph inputs or model parameters.GCOPE [98] unifies cross-domain graph pre-training using virtual coordinators and contrastive learning, reducing negative transfer and boosting downstream performance.FUG [99] ensures near-lossless adaptation to diverse graph features with PCA-inspired dimensional encoding and contrastive learning, enabling universal use without preprocessing or model changes.Cross-scale contrastive learning compares two graph views at different levels.For example, DGI [80] utilizes a discriminator to maximize the mutual information between the node embeddings and the graph embedding and minimize the information between node and corrupted graph embedding.Such a contrastive process encourages the encoder to capture information of the whole graph.Although DGI enables the model to capture semantic information about nodes and graphs, it ignores the discrepancies between different nodes.</p>
<p>Generative Methods</p>
<p>In addition to contrastive methods, some generative graph pretraining approaches have been proposed.The aim of generative pre-training methods is to enable GNNs to understand the general structural and attribute semantics of graphs.Thus, the GNNs can be adapted to downstream tasks with universal information.Generative learning frameworks for graphs can be classified into two categories based on how they acquire generative targets [90]: graph reconstruction and property prediction.</p>
<p>Graph reconstruction aims to reconstruct specific parts of given graphs, emphasizing fidelity in reproducing the original graph structure or node attributes.For example, VGAE [81] extends the VAE to the graph domain, where it first employs GCN as an encoder to generate node embeddings and then reconstructs the adjacency matrix by the inner product of node embeddings.Furthermore, GPT-GNN [100] proposes the self-supervised edge and attribute generation tasks to push the model to understand the inherent dependencies of attribute and structure.As a result, the model can learn high-order structure and semantic information.GraphMAEs [64,65] consider that previous generative methods overemphasize structure information, instead, they employ the reconstruction of features and a re-mask decoding strategy in a self-supervised manner.In the property prediction category, models focus on learning and predicting non-trivial properties of provided graphs.For instance, GROVER [96] introduces tasks for nodes and edges, predicting context-aware properties within local subgraphs.The graph-level self-supervision task aims to predict motifs, framing it as a multi-label classification problem with each motif as a label.</p>
<p>Although generative methods are capable of generating novel content, the quality and interpretability of the content are hard to guarantee.In the future, it remains to be explored how to enhance the accuracy and rationality of the generative methods.</p>
<p>Adaptation</p>
<p>Typically, the objectives of pre-training tasks are different from the downstream ones, which hinders the transferability of pretraining models.To this end, fine-tuning is a common adaptation approach based on subtle adjustments of model parameters.In addition, the "pre-train, prompt and predict" paradigm has attracted considerable attention in recent years.By using prompts, the format of downstream tasks is aligned with that of pre-training tasks, enabling pre-training models to handle downstream tasks in a more effective manner.</p>
<p>Fine-Tuning</p>
<p>For the situation where the model conducts the pre-training and downstream tasks in the same domain, we can utilize a pre-training model to generate node embeddings or graph embeddings, and subsequently fine-tune an external task-specific layer to generalize the pre-training model to downstream tasks.DGI [80] and GRACE [62] utilize the pre-trained encoder to obtain node embeddings, and then fine-tune a logistic regression classifier with labeled data to handle the node classification task.Additionally, there is a more practical scenario where pre-training is performed on the known graphs while tested on unseen graphs.Pre-training models cannot encode unknown graphs appropriately, thus we need to fine-tune the model in this situation.GPT-GNN [100] employs the labeled data to fine-tune a downstream task-specific decoder, which guides the pre-training model to adapt to downstream tasks.Moreover, some parameter-efficient fine-tuning methods have been proposed recently.AdapterGNN [66] employs two parallel adapters before and after the message passing stage to modify the input graph.Such addition-based methods only need to fine-tune the introduced parameters.G-Adapter [67] proposes a parameter-efficient fine-tuning method for graph transformer, which introduces graph structure into the fine-tuning by message passing.G-TUNING [101] is a fine-tuning strategy for GNNs that utilizes graphon reconstruction to preserve generative patterns and address structural divergence between pre-training and downstream datasets</p>
<p>Although the fine-tuning methods have achieved significant success, they typically require sufficient labeled data to tune the model parameters.Moreover, conventional fine-tuning methods necessitate repetitive fine-tuning for each task, incurring significant computational costs.Therefore, more advanced fine-tuning techniques for graph foundation models are still to be explored.</p>
<p>Prompt Tuning</p>
<p>Prompt tuning has recently emerged as a strategy to circumvent the need for full-parameter tuning, facilitating both multi-task adaptation and zero-shot learning [52,102,103].This innovative approach has found significant applications in graph data, where recent studies have concentrated on using prompt tuning to enhance the performance and adaptability of GNN-based models.Following the framework proposed by Guo et al. [86], these methods can be categorized into two distinct groups: pre-prompt methods and post-prompt methods, based on whether the taskspecific prompts operate before or after the backbone module.</p>
<p>Pre-prompt methods modify the input graph's topology or node features before message passing to aid downstream tasks or construct a prompt graph to enhance model adaptation.For example, AAGOD [86] proposes to implement the data-centric manipulation by superimposing an amplifier on the adjacency matrix of the original input graph as learnable instance-specific prompts.All In One [56] converts the node-level and graph-level tasks to graph-level tasks.It treats an extra subgraph as a prompt and merges it with the node subgraph.The model subsequently utilizes the combined graph to generate predictions.GPF [87] introduces a uniform feature modification vector to each node in the graph, which can be optimized to adapt pre-training GNN models under any pre-training strategy.Additionally, it features verbalizer-free prompting function, thus aligning the downstream task with the pre-training method's format.PRODIGY [59] is a framework for pre-training an in-context learner over prompt graphs.The goal is to enable a pretrained model to adapt to diverse downstream tasks without optimizing any parameters.IGAP [104] bridges the gap between graph pre-training and inductive finetuning by addressing the graph signal and structure gaps using learnable prompts in the spectral space.TPP [105] achieves a replay-free and forget-free GCIL system by storing task-specific knowledge in compact learnable tokens using graph prompts with a frozen pre-trained GNN, avoiding model updates or data replay.</p>
<p>Post-prompt methods use task-specific prompts on representations after message passing to aid downstream task adaptation.For example, GPPT [68] employs a prompting function to generate a token pair for each class, transforming all node classification tasks into link prediction tasks.GraphPrompt [55] unifies pretraining and downstream tasks into a consistent task format based on subgraph similarity, and utilizes labeled data to learn a taskspecific prompt vector for each task, which modifies the model's Readout operation and narrows the gap between link prediction and downstream tasks.GraphPrompt+ [106] further enhances GraphPrompt by generalizing pre-training tasks and employing layer-wise prompts to capture hierarchical knowledge across the graph encoder, improving task-specific knowledge extraction for both node and graph classification.ProNoG [107] uses conditional prompting for non-homophilic graphs, leveraging a condition-net to generate node-specific prompts that refine embeddings for finegrained task adaptation.</p>
<p>Although these methods have improved the performance in few-shot scenarios, further exploration is needed to understand the semantics and interpretability of the graph prompts.</p>
<p>Discussion</p>
<p>GNN-based models offer several advantages, particularly their ingenious inductive bias and compact parameter size.These models naturally possess essential properties like permutation invariance, enabling them to handle graph-structured data effectively.Additionally, GNN-based models offer the advantage of low-cost training and efficient resource usage, which reduces computational requirements and makes them accessible for deployment even in resource-constrained environments.Moreover, they can generalize well from small amounts of labeled data.By propagating label information through the graph, they can enhance prediction accuracy even when labeled data is sparse.</p>
<p>Furthermore, for more complex graph data such as heterogeneous graphs, hypergraphs, and temporal graphs, preliminary research have investigated GNN-based graph foundation models.For example, CPT-HG [108] and PT-HGNN [109] have designed sophisticated pre-training methods tailored for high-order semantic information in heterogeneous graphs.MultiGPrompt [110] and HGPROMPT [111] both use prompt-based learning to link pre-training and downstream tasks on heterogeneous graphs.PhyGCN [112] and IHP [113] use self-supervised hyperedge prediction and instruction-based prompts respectively to improve node representations in hypergraphs.GraphST [114] and GPT-ST [115] use pre-training to improve spatio-temporal representations for temporal graphs.</p>
<p>Despite their many advantages, GNN-based models have several notable disadvantages.One primary limitation is their lack of explicit text modeling.They often underutilize the rich semantic information embedded in textual attributes associated with nodes or edges, leading to suboptimal exploitation of textual data.Another significant drawback is the limited capacity of GNN-based models to incorporate and utilize general knowledge effectively.Unlike LLMs, which are pre-trained on vast corpora of text and can leverage extensive world knowledge, GNN-based models typically lack such pre-training on diverse and comprehensive datasets.This gap restricts their ability to generalize across different domains and limits their performance in tasks requiring broad contextual understanding or common-sense reasoning.</p>
<p>One promising direction is the integration of LLMs with GNN-based models.LLMs can provide a robust framework for incorporating extensive textual knowledge, enhancing the models' ability to understand and utilize semantic information embedded in text.In the following sections, we will explore graph learning models that incorporate LLMs.</p>
<p>LLM-BASED MODELS</p>
<p>Researchers are actively exploring ways to leverage LLMs as core and sole backbone for graph learning [60,116,117], for the following advantages that can not be underestimated.Firstly, transformer-based models show a remarkable ability to seamlessly integrate textual information in graph data [116].Additionally, employing a LLM-liked backbone empowers models to unify diverse graph learning tasks, as these tasks can be described using natural language.Furthermore, recent advancements, such as NLGraph [60], GPT4Graph [117], showcase the LLMs' prowess in preliminary graph reasoning.These advantages mark a highly promising direction for the development of such models.To discover the potential of engaging LLMs into graph learning, these works involve both graph-based properties and textual information as input for the backbone networks.Following some surveys [21,118], our characterization of the backbone is not confined solely to the narrow definition of LLMs (like GPT-4); it also encompasses certain transformer-based models that leverage textual information.We have summarized and categorized the works mentioned in this section in supplemental material D.</p>
<p>Backbone Architectures</p>
<p>A central question in employing LLMs for graph data is how to align graph data with natural language so that LLMs can understand them.Given that LLMs initially accept tokens as their inputs and rely on self-attention layers to process input sequences for producing hidden representations, it can be a difficult task to attain a fine-grained modeling of graph structure information [116].As illustrated in Figure 3, we categorize existing LLM-based methods into two types, graph-to-token and graph-to-text.The key distinction between these two approaches lies in the use of an additional encoder.Graph-to-token methods rely on an additional encoder (e.g., BERT) to generate embedding-level representations for each node, while graph-to-text method directly translates graph representations into natural language input for LLMs without the need for an additional encoder.</p>
<p>Graph-to-token</p>
<p>One approach entails the tokenization of graph information and imitates the standardized input format of transformer-based models.This methodology necessitates not only the serialization of graph data into tokens but also the solutions for encoding the graph's structural information.Since this method uses node representations as unique tokens for the input of backbone models, the backbone need to be either trainable transformers or open source LLMs.For instance, InstructGLM [116] uses LLaMA [74] and T5 [77] to be their backbones for further tuning.</p>
<p>The concept of graph-to-token initially surfaces in GIMLET [119] that treats node representations as tokens and aims to integrate graph data with textual data.Specifically, GIMLET expands the capabilities of LLMs to accommodate both graph and text data by using the transformer architecture, incorporating generalized position embedding and instruction-based pre-training.Furthermore, efforts have been made to integrate graph data with other modalities beyond just text data.For instance, Meta-Transformer [120] introduces a transformer-based architecture designed to incorporate various forms of multimodal data, including graphs, text, and images.However, despite the promising trend indicated by developing unified multimodal intelligence using a transformer backbone, their approaches cannot be considered as graph foundation models because they do not involve any pre-training and adaptation learning paradigm.</p>
<p>InstructGLM [116] on the other hand, adopts a pre-training and adaptation framework and introduces LLMs to further enhance the model's text processing capabilities, making it a strong contender for the position of a graph foundation model.In this framework, the vocabulary of the LLMs is expanded by incorporating the inherent node feature vectors from the graph as distinct and unique tokens for each node.Leveraging LLMs and the transferability of natural language, InstructGLM makes a valuable contribution to the ongoing movement towards graph foundation model architectures and pipelines that span multiple modalities.</p>
<p>These efforts tokenize graph data to align it with natural language, enabling joint understanding with data from other modalities.Their conclusions showcase promising results for integrating graph data with natural language.However, despite these promising results, how to inform LLMs of underlying graph structures remains an important challenge in this approach.</p>
<p>Graph-to-text</p>
<p>To align graph data with natural language, another approach involves describing graph information using natural language.Several approaches have been developed along this line of thoughts, utilizing prompts to integrate the capabilities of LLMs into classical tasks on graphs.For this method, which exclusively relies on natural language prompts, the associated backbone model can be any LLM, even if it is not open-sourced.For instance, Graph-LLM [118] utilizes multiple language models of different sizes, including BERT [54], DeBERTa [121], Sentence-BERT [122], GPT-4 [123] and LLaMA [74].</p>
<p>Initial attempts mainly use edge list to describe graph structures in natural language and make assessment on various graph tasks.NLGraph [60] also conducts a comprehensive assessment of LLMs across eight graph reasoning tasks as well as popular GNN tasks in natural language.Similarly, utilizing edge lists to describe graph structure, the results once again underscores the limitations of this approach when dealing with complex graph problems.TextForGraph [124] designed two types of prompts, full text and reduced text, to describe information on the graph, effectively compressing the prompt length.When&amp;Why [125] designs several styles of prompts and offers key insights into the use of LLMs for processing structured data.GraphWiz [126] designs different Fig. 3: An illustration of two existing approaches to align graph data with natural language.One approach tokenizes graph data and use node representations (depicted as red tokens) as well as text tokens (depicted as green tokens) to be the input of LLMs.Another approach represents graph data with prompts in natural language and uses text tokens only (depicted as green tokens) as the input of LLMs.</p>
<p>prompts for various tasks on graphs, including cycle detection, subgraph matching, and more.Moreover, GPT4Graph [117] introduces a novel approach to prompt engineering that combines manually crafted prompts with prompts generated by the language model itself, referred to as handcrafted prompts and automatic prompts.Specifically, for manual prompting, it utilizes description languages such as edge lists, adjacency lists, Graph Modeling Language (GML), and Graph Markup Language (GraphML) to represent graph structures and compare their effectiveness.For automatic prompting, it employs techniques like graph summarization, neighborhood summarization, graph exploration, and graph completion to actively engage LLMs in understanding and manipulating graphs, facilitating graph-based reasoning and learning.The findings indicate that self-prompting is a more effective method for informing LLMs about graph structures.Graph-LLM [118] further supports this conclusion, emphasizing that neighbor summarization is the most effective technique in existing prompt engineering methods.</p>
<p>These studies highlight significant potential for using natural language to describe graph data and using textual tokens as the input of LLMs for graph learning.Nevertheless, a key takeaway from their conclusions is that, at the present moment, the way we use these prompts may not be an effective approach for mining underlying graph structures.</p>
<p>Pre-Training</p>
<p>The methods discussed in this section solely employ LLMs as the backbone.Hence, the pre-training phase for these methods corresponds to the pre-training phase of LLMs.There are mainly two tasks used in LLM-based models for graph learning, we will provide a concise overview of these two pre-training tasks.</p>
<p>Language Modeling (LM)</p>
<p>Language Modeling (LM) is one of the most common selfsupervised task in NLP, and is widely adopted by many stateof-the-art LLMs, such as LLaMA [74] and GPT-3 [127].LM task can be essentially addressed to the challenge of estimating probability distributions of the next word.While LM represents a broad concept, it frequently pertains specifically to auto-regressive LM or unidirectional LM in practical applications [51].Many methods involve LM as their pre-training method, namely In-structGLM [116], NLGraph [60], GPT4Graph [117], Graph-LLM [118], TextForGraph [124], When&amp;Why [125], GraphWiz [126] and CGForLLM [128].</p>
<p>In the context of a text sequence represented as
s 1:L = [s 1 , s 2 ,    , s L ]
, its overall joint probability, denoted as p (s 1:L ), can be expressed as a product of conditional probabilities, as shown in equation:
p (s 1:L ) = L l=1
p (s l |s 0:l1 ) .</p>
<p>(
)2
Here, s 0 represents a distinct token signifying the commencement of the sequence.The conditional probability p (s l |s 0:l1 ) is essentially a probability distribution over the vocabulary based on the linguistic context s 0:l1 .To model the context s 0:l1 , a neural encoder f nenc () is employed, and the conditional probability is calculated as follows:</p>
<p>p (s l |s 0:l1 ) = f lm (f nenc (s 0:l1 )).</p>
<p>In this equation, f lm represents the prediction layer.By training the network using maximum likelihood estimation (MLE) with a large corpus, we can effectively learn these probabilities.Nevertheless, a drawback of unidirectional language models is their encoding of contextual information for each token, which is solely based on the preceding leftward context tokens and the token itself.However, for more robust contextual representations of text, it is preferable to capture contextual information from both the forward and backward directions.</p>
<p>Masked Language Modeling (MLM)</p>
<p>Masked language modeling (MLM) is introduced to address the limitation of the traditional unidirectional language model, frequently denoted as a Cloze task [51].In MLM, specific tokens within the input sentences are randomly masked, and the model is then tasked with predicting these masked tokens by analyzing the contextual information in the surrounding text.As an effective pretraining task, MLM is adopted in BERT [54] and T5 [77].Additionally, MLM can be categorized into two subtypes: Sequence-to-Sequence MLM (Seq2Seq MLM) and Enhanced MLM (E-MLM).Seq2Seq MLM involves utilizing the masked sequences as input for a neural encoder, and the resulting output embeddings are used to predict the masked token through a softmax classifier.On the other hand, E-MLM extends the mask prediction task to various types of language modeling tasks or enhances MLM by integrating external knowledge.MLM also faces some drawbacks as this pretraining method may result in a disconnection between the pretraining and fine-tuning stages since the mask token is absent during fine-tuning.InstructGLM [116] and Graph-LLM [118] use T5/BERT as backbones, and adopt MLM pre-training strategy.</p>
<p>There are also many pre-training tasks like Permuted Language Modeling (PLM) [129], Denoising Autoencoder (DAE) [130], Text-Text Contrastive Learning (TTCL), [131] and others.These pre-training tasks are currently not adopted by existing LLM-based models in graph learning, and thus not within the scope of our discussion in this section.However, we believe that in the future, more research will be developed on graph tasks involving these pre-training tasks, offering additional possibilities for the establishment and refinement of graph foundation models.</p>
<p>Adaptation</p>
<p>The adaptation phase plays a pivotal role in enhancing the performance of LLM-based models in graph learning.LLMs are primarily trained on textual corpora, which results in a significant gap between the pre-training phase and their deployment on graph tasks.Both the graph-to-token and graph-to-text methods are accompanied by specific adaptation techniques designed to enhance the LLM's ability to understand graph data effectively.As these methods share a fundamentally similar training procedure that utilizes prompts, we classify these adaptation strategies in the aspect of prompt engineering: manual and automatic.</p>
<p>Manual Prompting</p>
<p>Methods mentioned here use manually created prefix style prompts.For instance, LLMtoGraph [132] and NLGraph [60] employ node and edge lists incorporating other graph properties described in natural language to form a comprehensive prompt.GPT4Graph [117] goes a step further by utilizing additional description languages to represent graph data, such as edge list, adjacency list, GML and GraphML, providing a more extensible framework for manual graph prompts.Furthermore, InstructGLM [116] employs instruction prompting to involve the design of a set of graph descriptions centered around a central node, coupled with task-specific descriptions.Graph-LLM [118], TextForGraph [124], When&amp;Why [125], GraphWiz [126] and CGForLLM [128] also use natural language instructions and subsequently conduct a series of comprehensive experiments.</p>
<p>Automatic Prompting</p>
<p>Creating prompts manually is a time-consuming task, and these prompts can sometimes be sub-optimal [133].To address these drawbacks, automatically generated prompts have been introduced for further adaptation.GPT4Graph [117] firstly tries to employ three different types of prompts generated by LLM itself, namely graph summarization, graph exploration and graph completion, in graph tasks.Specifically, graph summarization generates a summary of the given graph by extracting key features or a summary of the neighborhood information of target nodes.Graph exploration means generating a sequence of queries or actions to retrieve information from the given graph.Graph completion generates partial graphs and prompt itself to complete the missing parts.By leveraging these self-prompting strategies, LLMs can actively engage in the understanding and manipulation of graphs, facilitating graph-based reasoning and learning.Graph-LLM [118] uses automatic prompts as well in the form of neighbor summary, and their experimental results once again emphasize the efficiency of automatic prompting.</p>
<p>Additionally, there are various adaptation approaches based on fine-tuning, including Vanilla Fine-Tuning [54], Intermediate Fine-Tuning (IFT) [134], Multi-task Fine-Tuning (MTFT) [135], and Parameter Efficient Fine-Tuning [136].These methods offer efficient ways to adapt pre-trained models to downstream tasks, although they have not been applied to graph tasks at this time.However, we anticipate that future research will explore the integration of these adaptation approaches into graph tasks, further advancing the development of the graph foundation model.</p>
<p>Discussion</p>
<p>Efforts of aligning graph data with natural language and using sole LLMs as graph learners has paved the way for exciting developments.The integration of graph data, text, and other modalities into transformer-based models presents a promising way, with the potential to connect techniques from the GNN field with advancements in the LLM domain.Moreover, leveraging LLMs allows for the unification of various graph tasks, as these tasks can all be described in natural language.This makes LLMbased backbones a more competitive selection for building GFMs.</p>
<p>Nonetheless, it is essential to acknowledge that the current ways of utilizing LLMs as backbones for graph learning also presents inherent limitations.These limitations encompass the inability of LLMs to effectively process the lengthy textual information required to describe graph structures, their incapacity to engage in multi-hop logical reasoning through graph links, the challenge they face in capturing the topological structures prevalent in highly connected graphs, and their struggle in handling the dynamic nature of graphs that evolve over time.Furthermore, graph-to-text methods are constrained by the LLM's input length, limiting the size of the graph data they can handle.In contrast, graph-to-token methods incur higher computational costs but can process large-scale graph data encountered in real-world scenarios, as each node can usually be represented by just a single token [116].These shortcomings underscore the need for further research in using LLM-based models for graph learning.</p>
<p>Future research directions for LLM-based approaches include enhancing the ability of LLMs to more effectively and efficiently understand critical information in graphs, including node features and topological structures.Considering that LLMs cannot directly comprehend graphs, and flattened natural language description of graphs are likely to result in information loss, efficient and structured modeling techniques for graphs need to be developed.These methods are expected to help bridge the gap between natural language prompts and the comprehensive information present in graph data.Moreover, existing research, such as LLM4DYG [137], has explored the application of LLMs to complex graph data, specifically temporal graphs.However, more diverse types of graph data, such as hypergraphs and heterogeneous graphs, need to be explored.</p>
<p>GNN+LLM-BASED MODELS</p>
<p>GNN-based models lack the ability to process text and thus cannot directly make predictions based on textual data.Additionally, they cannot make predictions based on natural language instructions provided by users.Consequently, exploring the performance of models with a substantial parameter count in graph-related tasks is imperative.On the other hand, LLM-based models for graph learning have their inherent limitations.These limitations include the incapability of LLMs to process precise mathematical calculations and the inability to handle multi-hop logical reasoning, etc.These shortcomings underline the necessity for further research  and innovation in this domain.To overcome these limitations and harness the strengths of both language understanding from LLMs and structural analysis from GNNs, integrating LLMs and GNNs can potentially lead to a more comprehensive and powerful model.We summarize and categorize the works mentioned in this section in supplemental material E.</p>
<p>Backbone Architectures</p>
<p>To simultaneously utilize information from both the graph and text and accomplish a variety of tasks, we need to design a framework that effectively integrates LLM and GNN.Depending on the prediction model, GNN+LLM-based methods can be classified as: 1) GNN-centric Methods, 2) Symmetric Methods, and 3) LLMcentric Methods, as illustrated in Figure 4.</p>
<p>GNN-centric Methods</p>
<p>Several works aim to utilize LLM to extract node features from raw data and make predictions using GNN.These approaches are denoted as GNN-centric models.For example, GraD [138] performs a parameter-efficient fine-tuning of an LLM on the textual dataset of a TAG (text-attributed graph).The textual dataset T is annotated with task-specific labels Y, where G = (V, E, T ) and T is the set of texts with each element aligned with a node in V .Then the downstream task loss for fine-tuning is:
Loss CLS = L  ((LLM(T )), Y), Loss LINK = L  ( (LLM (T src ) , LLM (T dst )) , Y) ,(4)
where () is the classifier for the classification task or similarity function for the link prediction task, T src and T dst are the texts of the source node and the target node, respectively, Loss CLS and Loss LINK are the loss of classification and link prediction task, respectively.Thus we can get the node representations X with fine-tuned LLM, achieved by removing the head layer.Then we can train GNN with the loss:
Loss CLS = L  ((GNN(LLM(T ))), Y), Loss LINK = L   GNN (LLM(T src )) , GNN LLM(T dst ) , Y ,(5)
where () is the classifier for the classification task or similarity function for the link prediction task, Y is the task-specific labels, T is the set of texts, T src and T dst are the texts of the source node and the target node, respectively.For LLMs that do not provide direct access to their embeddings such as ChatGPT, TAPE [76] engages these LLMs through text interactions.Specifically, TAPE first utilizes an LLM to generate a ranked prediction list and explanation based on the original text, and then an LM is utilized and fine-tuned to transform the original text and additional features of predictions and explanation generated by LLM into node features.Subsequently, downstream GNNs can utilize the features for prediction tasks.TAPE extracts graph-agnostic features and cannot capture correlations between graph topology and raw features.To this end, GIANT [139] utilizes a graph-structure aware self-supervised learning method to finetune the LM.Consequently, the text representations encompass graph-related information.WTGIA [140] focuses on text-level Graph Injection Attacks (GIAs), enhancing the interpretability and real-world applicability of graph injection attacks.In GALM [141], the focus is on exploring pre-training approaches for models that combine text and graph data, particularly on extensive heterogeneous graphs enriched with rich textual data.OFA [142] introduces text-attributed graphs that use natural language to describe nodes and edges, unified by language models into a common embedding space.Heterformer [143] integrates contextualized text encoding and heterogeneous structure encoding within a single model.It incorporates heterogeneous structure information into each Transformer layer as it encodes node texts.Edgeformers [144], which are based on graph-enhanced Transformers, perform edge and node representation learning by contextually modeling texts associated with edges.LLMRec [145] improves recommender systems by using three straightforward yet powerful LLM-based graph augmentation techniques, addressing the issues of sparse implicit feedback and low-quality side information commonly found in recommendation systems.WalkLM [146] conducts attributed random walks on the graph and uses an automated program to generate approximately meaningful textual sequences from these walks.It then fine-tunes a language model (LM) with these textual sequences and extracts embedding vectors from the LM, capturing both attribute semantics and graph structures.TOUCHUP-G [147] enhances the node features derived from a pre-trained model for downstream graph tasks and introduces.Multiplex graph neural networks initialize node attributes as feature vectors for node representation learning, but they fall short in capturing the full semantics of the nodes' associated texts.METERN [148] addresses this by using a single text encoder to model the shared knowledge across relations and employing a small number of parameters per relation to generate relationspecific representations.Another research [149] investigates the use of LLMs to improve graph topological structures, a relatively unexplored area.A label-free pipeline, LLM-GNN [150], uses LLMs for annotation and supplies training signals to GNNs for subsequent prediction.</p>
<p>Symmetric Methods</p>
<p>Also, there are some works that align the embeddings of GNN and LLM to make better predictions or utilize the embeddings for other downstream tasks, denoted as symmetric methods.Most GNNcentric based methods involve two sequential steps: text encoding and graph aggregation.It is important to note that during the generation of text embeddings, there is no exchange of information between nodes.To consider the interrelated nature of connected nodes, several works try to utilize GNN and LLM together to get structure-aware text features.GraphFormer [151] fuses text embedding and graph aggregation as an iterative workflow.During each iteration, the interconnected nodes will engage in information exchange within the layerwise GNN component, formulated as l = GNN(z l ), where z l is the output of l-th layer of GNN.</p>
<p>As a result, each node will incorporate information from its neighboring nodes.The Transformer component then operates on these enhanced node features, enabling the generation of progressively more informative node representations as z l+1 = TRM(CONCAT( l , h l )), where TRM is the transformer, and h l is the output of l-th layer of transformer.However, this method suffers from scalability issues because the memory complexity is proportional to the graph size as neighborhood texts are also encoded.GLEM [152] employs a variational EM framework to alternatively update the LLMs and GNNs, thus essentially capturing the node label distribution conditioned on the local text attributes.In contrast, GNN uses the text and label information of neighboring nodes to predict labels, thus characterizing global conditional label distribution.By doing so, GLEM efficiently incorporates local textual data and global structural information into its components and can ease the scalability issue.</p>
<p>Other studies employ distinct encoders for graph nodes and texts, training them to align their representations within a shared latent space.G2P2 [153] jointly pre-trains a graph-text model utilizing three graph interaction-based contrastive strategies, and then explores prompting for the downstream tasks.[154] utilizes GNN to model the structural information of nodes, which is then integrated with the corresponding text fragment encoded by a language model.The model subsequently predicts the masked token.ENGINE [155] integrates large language models and graph neural networks using an adjustable side structure.This approach significantly reduces training complexity while maintaining the capacity of the combined model.To address this, PATTON [156] incorporates two pre-training strategies: network-contextualized masked language modeling and masked node prediction, aiming to capture the inherent relationship between textual attributes and network structure.OpenGraph [157] enhances the graph learning paradigm by developing a flexible graph foundation model.This model can understand complex topological patterns in diverse graph data, enabling it to excel in zero-shot graph learning tasks across a range of downstream datasets.RLMRec [158] improves the recommendation performance of current recommender systems by utilizing large language models (LLMs) and aligning their semantic space with collaborative relation modeling to achieve better representation learning.Some other works [159,160,161] also utilize GNN and LLM to learn representations for molecules.These models employ a contrastive learning strategy to effectively pre-train on a dataset containing pairs of molecular graphs and corresponding textual descriptions.By simultaneously learning the chemical structures of molecules and their associated text through this approach, these models can then be applied to various downstream tasks.Furthermore, MolCA [162] allows a language model (LM) to comprehend both text-based and graphbased molecular information through the use of a cross-modal projector.GIT-Mol [163] encompasses all three modalities in molecular science-graph, image, and text-supporting tasks such as molecule generation, molecule captioning, molecular image recognition, and molecular property prediction.</p>
<p>LLM-centric Methods</p>
<p>While LLMs have shown impressive performance in various natural language tasks, they struggle with precise mathematical calculations, multi-step logic reasoning, spatial and topological perception, and handling temporal progression.Hence some works utilize GNNs to enhance the performance of LLM, denoted as LLM-centric methods.For example, GraphTranslator [164] utilizes a Graph Model to efficiently manage predefined tasks and takes advantage of the extended interface of Large Language Models to support a variety of open-ended tasks for the Graph Model.GraphGPT [165] integrates large language models with graph structural knowledge through graph instruction tuning, enabling LLMs to understand complex graph structures and improving adaptability across various datasets and tasks.THLM [166] introduces a novel pre-training framework for language models that explicitly incorporates the topological and heterogeneous information found in text-attributed heterogeneous graphs.GraphPrompter [167] aligns graph information with LLMs via soft prompts.InstructGraph [168] enhances LLMs with graph reasoning and generation capabilities through instruction tuning and preference alignment.RELM [169] utilizes the chemical knowledge embedded in LMs to support GNNs, thereby improving the accuracy of real-world chemical reaction predictions.TEA-GLM [170] pretrains a GNN using contrastive learning to capture structural and semantic graph information, then employs a linear projector to map GNN representations into unified task-specific instructions for LLMs, enabling effective cross-dataset and cross-task generalization without fine-tuning the LLM.G-Retriever [171] introduces G-Retriever, a retrieval-augmented generation (RAG) framework that enables question answering on real-world textual graphs through a conversational interface, mitigating hallucinations and scaling efficiently to large graphs.</p>
<p>Pre-training</p>
<p>To train the model and enable it to handle both graph and text information, we need to train the model on a large amount of data.LLM and GNN can be pre-trained on textual data and graph data respectively, and the GNN+LLM-based methods can be pretrained on both data.In this subsection, we category the pretraining strategies as GNN or LLM-based, and alignment-based.</p>
<p>GNN or LLM-based</p>
<p>Other frameworks leverage pre-trained LLMs to obtain text embeddings.The majority of existing models [138,139,151,152,159,160,161,162] employ Masked Language Modeling (MLM) during pre-training.Some models, like TAPE and Graph-ToolFormer, opt for Language Modeling (LM) in the pre-training phase.Additionally, SimTeG integrates Text-Text Contrastive Learning (TTCL), a technique that leverages certain observed text pairs exhibiting more semantic similarity than randomly selected pairs during the pre-training phase as: (6) where E is the expectation, k is the score function, y + is the positive sample and y  is the negative sample.Additionally, GALM [141] utilizes graph reconstruction for pre-training on extensive graph datasets, and thus can incorporate the graph information into the pre-trained LLMs.
Loss TTCL = E x,y + ,y   log exp(k(x,y + )) exp(k(x,y + ))+exp(k(x,y  )) ,</p>
<p>Alignment-based</p>
<p>Symmetric methods of LLM and GNN like Text2Mol [159], MoleculeSTM [160], and CLAMP [161] are pre-trained on large datasets with Graph-Text Contrastive Learning (GTCL), which aligns the embeddings of the graph encoder and the text encoder.The embeddings involve rich information about graph structure and text, thus demonstrating appealing performance on downstream datasets.For a molecule example, CLAMP minimizes the contrastive loss as below:
Loss NCE =  1 N N i=1 y i log(k(LLM( i ), GNN( i ))) + (1  y i ) log(1  k(LLM( i ), GNN( i ))),(7)
where  i is the text representation,  i is the graph representation, and k is a score function to predict the activity of a molecule.The contrastive loss promotes the active molecules on a bioassay to have similar embeddings to the specific bioassay, while ensuring that inactive molecules have dissimilar embeddings to it.</p>
<p>Adaptation</p>
<p>The adaptation phase plays a pivotal role in optimizing GNN+LLM-based models for efficient graph learning.Apart from some works [159,160,161] which test the model's performance on zero-shot tasks such as zero-shot structure-text retrieval and zero-shot text-based molecule editing, models in most cases need adaptation.In this subsection, we categorize these adaptation strategies into two main types: fine-tuning and prompt-tuning.</p>
<p>Fine-tuning</p>
<p>To adapt to the downstream tasks, some works [139,141,143,144,146,151,151,152] utilize vanilla fine-tuning methods for node classification tasks.However, vanilla fine-tuning methods involve adjusting a broad range of model parameters, which can be computationally intensive and resource-demanding.So other works utilize parameter-efficient fine-tuning methods for downstream tasks, resulting in a more efficient and resource-friendly approach.Specifically, several studies [159,160,161] align the embedding space of GNN and LLM utilizing paired molecule graph-text data, while other research [138,145,148,150,156,166] is tuned on TAGs with classification task.Additionally, some works [162,164,167] adapt to downstream tasks by generating text captions or descriptions.</p>
<p>Prompt-Tuning</p>
<p>The prompt-tuning approach is employed in certain studies [153,154,163,165,168].For example, G2P2 [153] leverages prompt-tuning to automatically optimize prompts with limited labeled data for efficient adaptation to downstream tasks.Other studies [76,142,145,169] exclusively focus on utilizing Tuning-Free Prompting to generate text.These approaches leverage the inherent capabilities of language models without any additional fine-tuning or parameter adjustments, thereby relying solely on the pre-trained knowledge embedded within the models to produce text outputs.For example, in TAPE [76], the initial text features are incorporated into a specialized prompt to interrogate a language model, generating a ranked list of predictions along with explanations.Subsequently, the expanded text features are utilized for finetuning on an LLM.</p>
<p>Discussion</p>
<p>To summarize, LLMs excel in capturing complex linguistic patterns and semantics from textual data, allowing the GNN+LLMbased models to generate embeddings that involve rich text, structure information, and even external knowledge of LLMs, thus leading to better model performance.Also, when integrated with GNN, LLM's reasoning capabilities over graphs may be enhanced.At the same time, these models can also be regarded as multimodal models to accomplish cross-modal tasks, such as text-graph retrieval tasks.The embeddings can be then utilized for a bunch of downstream tasks.Also, it is challenging to align LLMs and GNNs into a common representational space.To tackle this problem, it is essential to establish a robust standard for measuring the alignment between LLM and GNN representations.This standard should evaluate the degree to which the embeddings from both models capture similar semantic and structural information.Additionally, it is crucial to design effective methods for achieving this alignment.By doing so, we can ensure that the combined model leverages the strengths of both LLMs and GNNs, ultimately enhancing performance on various downstream tasks.Moreover, existing work has begun to extend the GNN+LLM approach to heterogeneous graphs and hypergraphs.For heterogeneous graphs, HiGPT [172] introduces an in-context heterogeneous graph tokenizer and a heterogeneityaware instruction-tuning framework to address distribution shifts, enhancing generalization and performance across various heterogeneous graph learning scenarios, and GHGRL [173] employs LLM to automatically summarize and classify different data formats and types of heterogeneous graph data.For hypergraph, HyperBERT [174] augments a pre-trained BERT model with specialized hypergraph-aware layers for the task of node classification.</p>
<p>CHALLENGES AND FUTURE DIRECTIONS</p>
<p>Although the previous sections have discussed the concepts and a lot of related works towards graph foundation models, there are still many avenues for future exploration in this research area.This section will delve into these issues.</p>
<p>Challenges about Data and Evaluation</p>
<p>Data Quantity and Quality</p>
<p>The improvements in data quantity and data quality are the key factors contributing to the effectiveness of foundation models [1].At present, there is still a limited amount of open-source largescale graph data [175,176], and each dataset is primarily concentrated in a single domain.This poses a challenge to learn graph foundation models for diverse data domains.Hence, it is necessary to collect and organize a unified, massive dataset that covers graph data and related corpora across different domains.Note that some works have constructed cross-domain datasets [177,178], which aid in developing cross-domain graph foundation models.Additionally, if the graph data are noisy, incomplete, or not properly curated, it will negatively affect the performance of graph foundation models.To enhance the data quality of GFMs, efforts have been made to propose augmentation strategies from various perspectives, including graph structure learning, feature competion and label mixing, etc.However, since existing data augmentation techniques are typically tailored for individual GNN-based models, there is a need for further exploration on how to effectively augment graph data for LLM-based or GNN+LLM-based models.</p>
<p>Evaluation</p>
<p>With the help of natural language instructions and powerful generation capabilities, LLMs can support a variety of open-ended tasks [74].This presents new opportunities for graph foundation models based on LLM.However, due to the lack of labels in openended tasks, evaluating the performance of GFMs in such tasks is a challenge.When using LLM as a language foundation model, the evaluation of its performance on open-ended tasks has evolved from human evaluation [127] to meta-evaluation [179].The question of whether existing LLM evaluation methods [127,179] can be applied to GFMs remains to be explored.Beyond evaluating the performance of GFMs, it is also worth evaluating their robustness, trustworthiness, or holistic performance, similar to the current practices for language foundation models [180,181,182].</p>
<p>Challenges about Models 7.2.1 Model Architectures</p>
<p>As mentioned above, the designs of backbone architectures and learning paradigms are crucial for the implementation of GFMs.Although this article has outlined some potential solutions, it does not rule out the possibility of better ones.For example, regarding the backbone architecture, recent works have proposed model architectures that go beyond the Transformer, offering improved performance [183] or interpretability [184].However, it is still unknown whether these backbone architectures can be used for dealing with graph data.Additionally, when utilizing GNN+LLMbased models, it is worth exploring how to more effectively align the outputs of both models.Furthermore, there is limited research regarding the emergent abilities or neural scaling law [185,186] of GNN-based [59] or LLM-based [60] graph foundation models.It is yet unclear whether GNN+LLM-based models may have greater potential for emergence.Furthermore, considering the diverse types of graphs (such as heterogeneous graphs [172], temporal graphs [137], and hypergraphs [174]), designing a GFM capable of handling multiple types of graphs is a valuable direction for future research.A potential solution is to use a mixture of experts (MoE) model [187], where each expert handles one type of graph.Finally, given that current multimodal foundation models [188] primarily handle text, images, audio, and other modalities, it is an interesting research direction to explore whether GNNs can be employed to further expand the diversity of modalities covered by multimodal foundation models or enhance the capabilities of foundation models for multimodal learning [189].</p>
<p>Model Training</p>
<p>In order to achieve homogeneity and make effective use of pretraining data, it is crucial to design appropriate pre-training tasks in pre-training.Unlike many language foundation models, which often use LM [127] or MLM [54] as pre-training tasks, there are now various forms of pre-training tasks tailored to different GFM model architectures.Whether each type of pre-training task has its own applicable scope and whether there will be a unified pre-training task are worth further exploration.Additionally, enabling graph foundation models to support cross-domain data is a vital concern.Some works use data from different domains as model input for pre-training [98,190], or enable adaptation to data from different domains through methods such as LLMbased embedding [142], condition generation [191] and zero-shot transfer [172].Finally, apart from fine-tuning and prompting that are introduced in this article, there are other potential training techniques that can be applied to improve efficiency or update knowledge, such as knowledge distillation [192], reinforcement learning from human feedback (RLHF) [127] and model editing [193].Whether the above-mentioned techniques can be applied to graph foundation models will be a focal point of future research.</p>
<p>Challenges about Applications 7.3.1 Killer Applications</p>
<p>In comparison to the outstanding performance of language foundation models in tasks like text translation [194] and text generation [195], whether GFMs can similarly catalyze groundbreaking applications in graph tasks is not yet clear.For scenarios that are well-suited for the application of GNNs, such as ecommerce [196] and finance [197], potential research directions include leveraging graph-based models integrated with LLMs to better support open-ended tasks [164], or enhancing the reasoning capabilities of LLMs through graph learning techniques [198].Furthermore, GFMs have the potential to make breakthroughs in some emerging fields.For example, drug development is a timeconsuming and costly process [199], and language foundation models have already been successfully used for related tasks like target identification and side effect prediction [1].Given the 3D geometric structure of proteins [200], GFMs hold the promise of enhancing the drug discovery pipeline by leveraging their ability to model graph structure information [201], potentially speeding up the process further.Additionally, urban computing may also represent a crucial application scenario for GFMs.It is worth noting that traditional traffic prediction techniques have been primarily focused on addressing individual tasks such as travel demand prediction [202] and traffic flow prediction [203], lacking a comprehensive understanding of the entire transportation system.Given that the transportation system can be viewed as a spatiotemporal graph, graph foundation models hold the potential to capture the participation behavior of actors in the transportation system [204], thereby offering a unified approach to addressing various issues in urban computing.</p>
<p>Trustworthiness</p>
<p>Despite the strong performance of LLM-based foundation models, their black-box nature [205] introduces a host of safety concerns, such as hallucination and privacy leaks.The hallucination refers to the output appearing plausible but deviating from user input, context, or facts [206].Existing research suggests that this phenomenon is associated with multiple factors, such as the model's overconfidence in its own behavior [207] and the misunderstanding of false correlations [208].Similarly, recent work has pointed out that pre-trained GNNs also pose certain trustworthy risks about fairness [209] and robustness against attacks [210,211].Given the unique nature of graph data, we may require certain techniques to prevent or mitigate security risks on GFMs, such as confidence calibration [212] or counterfactual reasoning [213].Additionally, given that existing research has indicated privacy risks in both GNN [214,215] and LLM [216], enhancing the privacy of GFMs is also a critical concern.Some potential solutions include federated learning [217], RLHF [127] and red teaming [218], but whether these methods can be applied to GFMs is still unknown.Finally, graph data in real-world applications frequently encounter challenges such as noise [219], class imbalance [220], data incompleteness [221], and multi-modal features [222].Developing methods to utilize these graph data for constructing GFMs, or adapting existing GFMs to accommodate these characteristics, will be a critical area of focus for future research.</p>
<p>CONCLUSIONS</p>
<p>The development of foundation models and graph machine learning has spurred the emergence of a new research direction, with the aim to train on broad graph data and apply it to a wide range of downstream graph tasks.In this article, we propose the concept of graph foundation models (GFMs) for the first time, and provide an introduction to relevant concepts and representative methods.We summarize existing works towards GFMs into three main categories based on their reliance on graph neural networks (GNNs) and large language models (LLMs): GNN-based models, LLM-based models, and GNN+LLM-based models.For each category of methods, we introduce their backbone architectures, pre-training, and adaptation strategies separately.After providing a comprehensive overview of the current landscape of graph foundation models, this article also points out the future directions for this evolving field.Supplemental Materials:</p>
<p>A. Impact of Graph Data on GFMs</p>
<p>The success of foundation models depends on high-quality training data, and foundation models exhibit significantly different performance on various types of test data.In this section, we discuss the impact of graph data on graph foundation models from three aspects: graph type, graph scale and graph diversity.</p>
<p>Graph Type.Based on the number of categories of nodes and edges in a graph, we can categorize graphs into homogeneous graphs and heterogeneous graphs.In homogeneous graphs, all nodes and edges belong to the same category.For example, in a social graph where nodes represent individuals (users) and edges represent friendship relationships, it is a homogeneous graph because all nodes are individuals and all edges represent friendship relationships.Heterogeneous graphs, on the other hand, have more than one type of nodes or edges, representing different types of entities and relationships [33].For instance, an e-commerce graph may include nodes for users, products, and purchase relationships, forming a heterogeneous graph.For GFMs, handling heterogeneous graphs poses greater challenges and typically requires the design of specific backbone architectures and optimization objectives.Nonetheless, utilizing the meta-path based approach [223], a heterogeneous graph can be mapped into multiple homogeneous graphs, one for each meta-path.For example, one can apply the GFM trained on homogeneous graphs to each of these meta-path induced homogeneous graphs, separately, to get node embedding.Then, these embeddings on homogeneous graphs under different meta-paths can be fused together.However, beyond homogeneous graphs and heterogeneous graphs, there are some more complex types of graphs in the real world, such as dynamic graphs and hypergraphs [224], which poses additional challenges for GFM.</p>
<p>Graph Scale.Based on the number of nodes and edges in a graph, we can categorize graphs into relatively small graphs and large graphs.Small graphs are of smaller scale, typically containing dozens to hundreds of nodes and edges.For example, chemical molecular graphs represent the structure of small molecules and typically consist of dozens to hundreds of atoms.Large graphs, on the other hand, refer to graphs with a significant number of nodes and edges, often encompassing millions or even billions of nodes and edges.For instance, e-commerce graph in Alibaba includes billons of nodes and hundreds of billion edges [196].For graph foundation models, large graphs impose higher demands on the capacities of graph foundation models.Firstly, large graphs, due to their numerous nodes and typically sparser edges, introduce more noise and pose greater challenges in terms of storage and computation [225].Additionally, large graphs often exhibit long-range dependency relationships [226], requiring more neural network layers and a higher number of parameters, which exacerbates the over-smoothing [39] and oversquashing [18] problem of GNN-based models.</p>
<p>Graph Diversity.Based on whether a graph dataset originates from the same domain, we can categorize graphs into samedomain graphs and cross-domain graphs.Same-domain graphs refer to graph data from similar or related domains, typically containing nodes and edges of similar types.For example, the social graphs of Facebook and WeChat come from similar domains.Cross-domain graphs [227], on the other hand, involve graph data from different domains or data sources, often comprising nodes and edges of different types, aimed at addressing multidomain problems or cross-domain tasks.For example, academic networks and molecular graphs come from different domains.For graph foundation models, supporting cross-domain graphs presents greater challenges because graphs from different domains lack a unified underlying semantics.This can result in weak transfer performance or even negative transfer when applying the model to a new dataset [78].Therefore, addressing the heterogeneity of different domains and enabling the same GFM to be applied to graphs from different domains is a significant challenge for GFMs.</p>
<p>B. Impact of Graph Tasks on GFMs</p>
<p>Language foundation models can be widely applied to various NLP tasks, while for graph foundation models, the formats of graph tasks are also quite diverse and can be categorized into three classes: node-level tasks, edge-level tasks, and graph-level tasks.</p>
<p>Node-level Tasks.Node-level tasks refer to the classification, regression, or prediction performed on each node.Common node-level tasks include node classification, node regression, and clustering coefficient prediction.For example, in social networks, graph nodes can represent users, and node classification can be used to identify users from different social circles.</p>
<p>Edge-level Tasks.Edge-level tasks involve the classification, regression, or prediction performed on each individual edge.Common edge-level tasks include edge classification, link prediction, shortest path prediction, connectivity prediction, and maximum flow prediction.For example, in e-commerce, link prediction can be used to predict products that users may be interested in.</p>
<p>Graph-level Tasks.Graph-level tasks focus on the entire graph.Common graph-level tasks include graph classification, graph regression, graph generation, graph clustering, graph condensation and average clustering coefficient prediction.For example, in bioinformatics, graph property prediction can be used to predict the biological activity or toxicity of molecular compounds, thereby accelerating the drug discovery process.</p>
<p>In summary, the format of tasks in graphs are highly diverse and can be categorized into three types: node-level, edge-level, and graph-level, each of which has wide-ranging applications.This undoubtedly increases the challenge of homogenization for GFMs.For example, in graph classification and node classification tasks on synthetic datasets, modeling structural information is often more crucial [228].On the other hand, when dealing with node classification tasks on graphs with rich node features, modeling feature information becomes more important [228].Furthermore, tasks that are more similar to each other will also have a lower transfer difficulty, implying that these tasks are more likely to be addressed using the same GFM.While increasing expressive power holds promise for improving the performance of many node-level, edge-level, and graph-level tasks [229], there is also some work suggesting that overly strong expressive power may not be necessary for graph generation tasks [230].</p>
<p>C. Details of approaches involved as GNN-based models</p>
<p>We categorize the GNN-based methods in Table 1.</p>
<p>D. Details of approaches involved as LLM-based models</p>
<p>We categorize the LLM-based methods in Table 2.</p>
<p>E. Details of approaches involved as GNN+LLM-based models</p>
<p>We categorize the GNN+LLM-based methods in Table 3.</p>
<p>Fig. 2 :
2
Fig.2:A comparison between message passing-based models and graph transformer.A fundamental distinction is that the message passing mechanism is constrained by the graph structure, and the graph transformer treats the graph as a fully-connected network.</p>
<p>The title of Paper_4 is: Can  The title of Paper_1 is: Exploring  Paper_1 cites Paper_4  Question: The category of Paper_4 is  Categorize the central node: (<node_4>, Title_4&gt; ) is connected to (<node_1>, Title_1), (<node_3>, Title_3) within one hop.Which category should (<node_4>, Title_4) belong to? title of Paper_4 is: Can  The title of Paper_1 is: Exploring  Paper_1 cites Paper_4  Question: The category of Paper_4 is  Categorize the central node: (<node_4>, Title_4&gt; ) is connected to (<node_1>, Title_1), (<node_3>, Title_3) within one hop.Which category should (<node_4>, Title_4) belong to?Graph-to-text.</p>
<p>Symmetric methods, where the aligned embeddings can be further utilized for downstream tasks.methods, which take an instruction as input and output an answer.</p>
<p>Fig. 4 :
4
Fig. 4: An illustration of GNN+LLM-based models.</p>
<p>Yibo</p>
<p>Li received the B.S. degree from the Beijing University of Posts and Telecommunications, China, in 2022.She is currently working toward the master's degree with the Beijing University of Posts and Communications, China.Her current research interests are in graph neural networks and large language models.Mengmei Zhang received her Ph.D. degree in computer science and technology from Beijing University of Posts and Telecommunications in 2023.She is currently a senior researcher at China Telecom Bestpay.Her research interests include graph mining, large language models, and risk control.Ting Bai received her Ph.D. degree from Renmin University of China in 2019.She currently is an associate professor in the School of Computer Science, Beijing University of Posts and Telecommunications.Her major research interests are in recommender systems and Human behavior analysis.She has published several papers on SIGIR, WWW, KDD, CIKM, WSDM, TKDE, and so on.Yuan Fang received the Ph.D. degree in computer science from the University of Illinois at Urbana-Champaign in 2014.He is currently an assistant professor with the School of Computing and Information Systems, Singapore Management University.His current research focuses on graph data mining, machine learning and their applications.Lichao Sun is an Assistant Professor in Computer Science and Engineering at Lehigh University.He obtained his Ph.D. degree in Computer Science at University of Illinois Chicago in 2020.His research interests are Trustworthy AI and Medical AI in various applications.He have published more than 90 papers in top-tier journals and conferences, such as Nature Medicine, NeurIPS, ICML, ICLR, AAAI and IJCAI.Philip S. Yu is a Distinguished Professor at the University of Illinois Chicago, holding the Wexler Chair in information and Technology.He is a Fellow of the ACM and IEEE.He has published over 2,000 papers and holds or has applied for over 300 US patents.His main research interests include big data, data mining, privacy preserving publishing and mining, data streams, database systems, Internet applications and technologies.Chuan Shi received the Ph.D. degree from the lCT of Chinese Academic of Sciences in 2007.He joined the Beijing University of Posts and Telecommunications in 2007 and is a professor and deputy director of Beijing Key Lab of Intelligent Telecommunications Software and Multimedia at present.His research interests are in data mining and machine learning.He has published more than 100 papers in refereed journals and conferences, such as TPAMI, TKDE, KDD, WWW and NeurIPS.</p>
<p>TABLE 1 :
1
The relationship between language foundation model and graph foundation model.
Language Foundation ModelGraph Foundation ModelSimilaritiesGoalEnhancing the model's expressive power and its generalization across various tasksParadigmPre-training and AdaptationIntrinsic differencesDataEuclidean data (text)Non-Euclidean data (graphs) or a mixture of Euclidean (e.g., graph attributes) and non-Euclidean dataTaskMany tasks, similar formatsLimited number of tasks, diverse formatsBackbone Architectures Mostly based on TransformerNo unified architectureExtrinsic differencesHomogenizationEasy to homogenizeDifficult to homogenizeDomain GeneralizationStrong generalization capabilityWeak generalization across datasetsEmergenceHas demonstrated emergent abilities No/unclear emergent abilities as of the time of writing
ACKNOWLEDGEMENTSThis work was supported by the National Natural Science Foundation of China (No.U20B2045, 62192784, 62236003), Young Elite Scientists Sponsorship Program (No.2023QNRC001) by CAST, NSF under grants III-2106758 and POSE-2346158.ModelBackbone Architecture Pre-training Adaptation All In One[56]GCN, GAT, Graph Transformer Same-Scale CL Prompt Tuning PRODIGY[59]GCN, GAT Graph Reconstruction, Supervised Prompt Tuning DGI[80]GCN Cross-Scale CL Parameter-Efficient FT GRACE[62]GCN Same-Scale CL Vanilla FT FUG[99]GCN Same-Scale CL Vanilla FT VGAE[81]GCN Graph Reconstruction, Property Prediction Vanilla FT MA-GCL[82]GCN Same-Scale CL Vanilla FT MultiGPrompt[110]GCN Cross-Scale CL, Graph Reconstruction Prompt Tuning IGAP[104]GCN, GAT, GraphSAGE Same-Scale CL, Cross-Scale CL, Graph Reconstruction Prompt Tuning HGPROMPT[111]GCN, GAT, SimpleHGN Graph Reconstruction Prompt Tuning GraphMAE[64]GAT Graph Reconstruction Parameter-Efficient FT GraphMAE2[65]GAT Graph Reconstruction Parameter-Efficient FT GPPT[68]GraphSAGE Graph Reconstruction, Cross-Scale CL Prompt Tuning VPGNN[83]GraphSAGE Cross-Scale CL Prompt Tuning GPT-GNN[100]HGT Graph Reconstruction Vanilla FT PT-HGNN[109]HGT Same-Scale CL Vanilla FT CPT-HG[108]HGT Same-Scale CL Vanilla FT GraphPrompt[55]GIN Graph Reconstruction Prompt Tuning IHP[113]PHC Graph Reconstruction Prompt Tuning GraphPrompt+[106]GIN Graph Reconstruction, Cross-Scale CL, Same-Scale CL Prompt Tuning ProNoG+[107]FAGCN Same-Scale CL Prompt Tuning GCC[84]GIN Same-Scale CL Vanilla FT GraphCL[85]GIN Same-Scale CL Parameter-Efficient FT AdapterGNN[66]GIN Cross-Scale CL, Graph Reconstruction, Same-Scale CL Parameter-Efficient FT PhyGCN[112]HyperGCN Graph Reconstruction Parameter-Efficient FT GPT-ST[115]GPT-ST Graph Reconstruction Parameter-Efficient FT GraphST[114]GraphST Same-Scale CL Parameter-Efficient FT AAGOD[86]GIN Same-Scale CL, Supervised Prompt Tuning GPF[87]GIN Cross-Scale CL, Graph Reconstruction Prompt Tuning GCOPE[98]FAGCN Same-Scale CL Prompt Tuning FOTOM[190]GIN Same-Scale CL Parameter-Efficient FT TPP[105]SGC Same-Scale CL Prompt Tuning GraphControl[191]GIN Same-Scale CL Parameter-Efficient FT G-TUNING[101]GIN Same-Scale CL, Graph Reconstruction Customized FT Graph-BERT[95]Graph Transformer Graph Reconstruction, Supervised Vanilla FT GROVER[96]Graph Transformer Property Prediction Vanilla FT G-Adapter[67]Graph Transformer Supervised, Graph Reconstruction, Property Prediction Parameter-Efficient FT GIMLET[119]Graph-to-token + Transformer --InstructGLM[116]Graph-to-token + Flan-T5/LLaMA MLM,LM Manual Prompt Tuning NLGraph[60]Graph-to-text + GPTs LM Manual Prompt Tuning TextForGraph[124]Graph-to-text + GPTs LM Manual Prompt Tuning When&amp;Why[125]Graph-to-text + GPTs LM Maunal Prompt Tuning GraphWiz[126]Graph-to-text + LLaMA, Mistral LM Maunal Prompt Tuning CGForLLM[128]Graph-to-text + GPT4 LM Maunal Prompt Tuning LLM4DYG[137]Graph-to-text + LLaMA, Vicuna, GPT-3.5 LM Manual Prompt Tuning GPT4Graph[117]Graph-to-text + GPT-  GNN-centric LM Tuning-free Prompting + Parameter-Efficient FT GIANT[139]GNN-centric MLM Vanilla FT GraD[138]GNN-centric MLM Parameter-Efficient FT GALM[141]GNN-centric Graph Reconstruction Vanilla FT OFA[142]GNN-centric MLM Tuning-free Prompting Heterformer[143]GNN-centric LM Vanilla FT edgeformers[144]GNN-centric LM Vanilla FT LLMRec[145]GNN-centric LM Tuning-free Prompting + Parameter-Efficient FT WalkLM[146]GNN-centric MLM Vanilla FT METERN[148]GNN-centric MLM Parameter-Efficient FT LLM-GNN[150]GNN-centric LM Parameter-Efficient FT WTGIA[140]GNN-centric LM Parameter-Efficient FT GHGRL[173]GNN-centric LM Vanilla FT GLEM[152]Symmetric MLM Vanilla FT GraphFormer[151]Symmetric MLM Vanilla FT G2P2[153]Symmetric GTCL Prompt Tuning Text2Mol[159]Symmetric MLM + GTCL Parameter-Efficient FT MoleculeSTM[160]Symmetric MLM + GTCL Parameter-Efficient FT MolCA[162]Symmetric LM Parameter-Efficient FT CLAMP[161]Symmetric MLM + GTCL Parameter-Efficient FT GIT-Mol[163]Symmetric LM Prompt Tuning PATTON[156]Symmetric MLM Parameter-Efficient FT ENGINE[155]Symmetric LM Parameter-Efficient FT OpenGraph[157]Symmetric LM Vanilla FT RLMRec[158]Symmetric LM Parameter-Efficient FT GraphTranslator[164]LLM-centric LM Parameter-Efficient FT THLM[166]LLM-centric MLM Parameter-Efficient FT GraphGPT[165]LLM-centric MLM Prompt Tuning InstructGraph[168]LLM-centric LM Prompt Tuning RELM[169]LLM-centric LM Tuning-Free Prompting GraphPrompter[167]LLM-centric LM Parameter-Efficient FT HiGPT[172]LLM-centric LM Parameter-Efficient FT G-Retriever[171]LLM-centric LM Prompt-Tuning TEA-GLM[170]LLM-centric LM Parameter-Efficient FT HyperBERT[174]LLM-centric LM Parameter-Efficient FT
On the opportunities and risks of foundation models. R Bommasani, D A Hudson, E Adeli, arXiv:2108.072582021arXiv preprint</p>
<p>Emergent abilities of large language models. J Wei, Y Tay, R Bommasani, 2022TMLR</p>
<p>Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. W Wang, Z Chen, X Chen, Proc. of NeurIPS. of NeurIPS202336</p>
<p>Video-llama: An instructiontuned audio-visual language model for video understanding. H Zhang, X Li, L Bing, Proc. of EMNLP demo. of EMNLP demo2023</p>
<p>Recommender systems in the era of large language models (llms). Z Zhao, W Fan, J Li, IEEE TKDE. 2024</p>
<p>Deepwalk: Online learning of social representations. B Perozzi, R Al-Rfou, S Skiena, Proc. of KDD. of KDD2014</p>
<p>node2vec: Scalable feature learning for networks. A Grover, J Leskovec, Proc. of KDD. of KDD2016</p>
<p>Graph embedding and extensions: A general framework for dimensionality reduction. S Yan, D Xu, B Zhang, H.-J Zhang, Q Yang, S Lin, IEEE TPAMI. 2006</p>
<p>Complex embeddings for simple link prediction. T Trouillon, J Welbl, S Riedel,  Gaussier, G Bouchard, Proc. of ICML. PMLR. of ICML. PMLR2016</p>
<p>A three-way model for collective learning on multi-relational data. M Nickel, V Tresp, H.-P Kriegel, Proc. of ICML. of ICML201111584</p>
<p>Network representation learning with rich text information. C Yang, Z Liu, D Zhao, Proc. of IJCAI. of IJCAI2015</p>
<p>Dualityinduced regularizer for semantic matching knowledge graph embeddings. J Wang, Z Zhang, Z Shi, J Cai, S Ji, F Wu, IEEE TPAMI. 4522022</p>
<p>Graph neural networks: A review of methods and applications. J Zhou, G Cui, S Hu, Z Zhang, C Yang, Z Liu, L Wang, C Li, M Sun, AI open. 12020</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, Proc. of ICLR. of ICLR2017</p>
<p>Link prediction based on graph neural networks. M Zhang, Y Chen, Proc. of ICONIP. of ICONIP2018</p>
<p>How powerful are graph neural networks?. K Xu, W Hu, J Leskovec, S Jegelka, Proc. of ICLR. of ICLR2019</p>
<p>Attributed graph clustering: a deep attentional embedding approach. C Wang, S Pan, R Hu, Proc. of IJCAI. of IJCAI2019</p>
<p>On the bottleneck of graph neural networks and its practical implications. U Alon, E Yahav, Proc. of ICLR. of ICLR2021</p>
<p>Individual and structural graph information bottlenecks for out-of-distribution generalization. L Yang, J Zheng, H Wang, IEEE TKDE. 2023</p>
<p>A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. C Zhou, Q Li, C Li, IJMLC. 2024</p>
<p>Unifying large language models and knowledge graphs: A roadmap. S Pan, L Luo, Y Wang, C Chen, J Wang, X Wu, IEEE TKDE. 2024</p>
<p>Large language models and knowledge graphs: Opportunities and challenges. J Z Pan, S Razniewski, J.-C Kalo, TGDK. 2023</p>
<p>Graph meets llms: Towards large graph models. Z Zhang, H Li, Z Zhang, Y Qin, X Wang, W Zhu, NeurIPS 2023 Workshop: New Frontiers in Graph Learning. 2023</p>
<p>A comprehensive survey on graph neural networks. Z Wu, S Pan, F Chen, IEEE TNNLS. 3212020</p>
<p>Guest editorial: Non-euclidean machine learning. S Zafeiriou, M Bronstein, T Cohen, IEEE TPAMI. 2022</p>
<p>The development of social network analysis. L Freeman, A Study in the Sociology of Science. 2004</p>
<p>Biological network analysis with deep learning. G Muzio, L Bray, K Borgwardt, Briefings in bioinformatics. 2021</p>
<p>Graph neural network for traffic forecasting: A survey. W Jiang, J Luo, Expert Systems with Applications. 2022</p>
<p>Symbiotic graph neural networks for 3d skeleton-based human action recognition and motion prediction. M Li, S Chen, X Chen, IEEE TPAMI. 2021</p>
<p>Affinity attention graph neural network for weakly supervised semantic segmentation. B Zhang, J Xiao, J Jiao, Y Wei, Y Zhao, IEEE TPAMI. 2021</p>
<p>Learning to model relationships for zero-shot video classification. J Gao, T Zhang, C Xu, IEEE TPAMI. 43102020</p>
<p>Generalizing graph neural networks on out-of-distribution graphs. S Fan, X Wang, C Shi, IEEE TPAMI. 2023</p>
<p>A survey of heterogeneous information network analysis. C Shi, Y Li, J Zhang, IEEE TKDE. 2016</p>
<p>Hypergraph neural networks. Y Feng, H You, Z Zhang, R Ji, Y Gao, Proc. of AAAI. of AAAI2019</p>
<p>Motion-aware dynamic graph neural network for video compressive sensing. R Lu, Z Cheng, B Chen, X Yuan, IEEE TPAMI. 2024</p>
<p>Neural message passing for quantum chemistry. J Gilmer, S S Schoenholz, P F Riley, Proc. of ICML. of ICML2017</p>
<p>Inductive representation learning on large graphs. W Hamilton, Z Ying, J Leskovec, Proc. of NeurIPS. of NeurIPS2017</p>
<p>Graph attention networks. P Velickovic, G Cucurull, A Casanova, Proc. of ICLR. of ICLR2018</p>
<p>Deeper insights into graph convolutional networks for semi-supervised learning. Q Li, Z Han, X.-M Wu, Proc. of AAAI. of AAAI2018</p>
<p>Dropedge: Towards deep graph convolutional networks on node classification. Y Rong, W Huang, T Xu, J Huang, Proc. of ICLR. of ICLR2020</p>
<p>Do transformers really perform badly for graph representation. C Ying, T Cai, S Luo, Proc. of NeurIPS. of NeurIPS2021</p>
<p>Structure-aware transformer for graph representation learning. D Chen, L Bray, K Borgwardt, Proc. of ICML. of ICML2022</p>
<p>Rethinking graph transformers with spectral attention. D Kreuzer, D Beaini, W Hamilton, V Ltourneau, P Tossou, Proc. of NeurIPS. of NeurIPS2021</p>
<p>Recipe for a general, powerful, scalable graph transformer. L Rampek, M Galkin, V P Dwivedi, A T Luu, G Wolf, D Beaini, Proc. of NeurIPS. of NeurIPS2022</p>
<p>Graph classification using structural attention. J B Lee, R Rossi, X Kong, Proc. of KDD. of KDD2018</p>
<p>Censnet: convolution with edge-node switching in graph neural networks. X Jiang, P Ji, S Li, Proc. of IJCAI. of IJCAI2019</p>
<p>A compact review of molecular property prediction with graph neural networks. O Wieder, S Kohlbacher, M Kuenemann, Drug Discovery Today: Technologies. 2020</p>
<p>Graph-based semisupervised learning: A comprehensive review. Z Song, X Yang, Z Xu, I King, IEEE TNNLS. 2022</p>
<p>Extract the knowledge of graph neural networks and go beyond it: An effective knowledge distillation framework. C Yang, J Liu, C Shi, Proc. of WWW. of WWW2021</p>
<p>Graph self-supervised learning: A survey. Y Liu, M Jin, S Pan, IEEE TKDE. 2022</p>
<p>Pretrained models for natural language processing: A survey. X Qiu, T Sun, Y Xu, Y Shao, N Dai, X Huang, Science China Technological Sciences. 2020</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, 2023ACM CSUR</p>
<p>Universal language model fine-tuning for text classification. J Howard, S Ruder, Proc. of ACL. of ACL2018</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, Proc. of NAACL. of NAACL2019</p>
<p>Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. Z Liu, X Yu, Y Fang, X Zhang, Proc. of WWW. of WWW2023</p>
<p>All in one: Multi-task prompting for graph neural networks. X Sun, H Cheng, J Li, Proc. of KDD. of KDD2023</p>
<p>A survey on in-context learning. Q Dong, L Li, D Dai, Proc. of EMNLP. of EMNLP2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, Proc. of NeurIPS. of NeurIPS2022</p>
<p>PRODIGY: Enabling incontext learning over graphs. Q Huang, H Ren, P Chen, Proc. of NeurIPS. of NeurIPS2023</p>
<p>Can language models solve graph problems in natural language?. H Wang, S Feng, T He, Proc. of NeurIPS. of NeurIPS202336</p>
<p>A molecular multimodal foundation model associating molecule graphs with natural language. B Su, D Du, Z Yang, arXiv:2209.054812022arXiv preprint</p>
<p>Deep Graph Contrastive Representation Learning. Y Zhu, Y Xu, F Yu, Q Liu, S Wu, L Wang, ICML Workshop on Graph Representation Learning and Beyond. 2020</p>
<p>Mocl: datadriven molecular fingerprint via knowledge-aware contrastive learning from molecular graph. M Sun, J Xing, H Wang, B Chen, J Zhou, Proc. of KDD. of KDD2021</p>
<p>Graphmae: Self-supervised masked graph autoencoders. Z Hou, X Liu, Y Cen, Y Dong, H Yang, C Wang, J Tang, Proc. of KDD. of KDD2022</p>
<p>Graphmae2: A decoding-enhanced masked selfsupervised graph learner. Z Hou, Y He, Y Cen, X Liu, Y Dong, E Kharlamov, J Tang, Proc. of WWW. of WWW2023</p>
<p>Adaptergnn: Parameter-efficient fine-tuning improves generalization in gnns. S Li, X Han, J Bai, Proc. of AAAI. of AAAI202438608</p>
<p>G-adapter: Towards structureaware parameter-efficient transfer learning for graph transformer networks. A Gui, J Ye, H Xiao, Proc. of AAAI. of AAAI2024</p>
<p>Gppt: Graph pre-training and prompt tuning to generalize graph neural networks. M Sun, K Zhou, X He, Y Wang, X Wang, Proc. of KDD. of KDD2022</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. X L Li, P Liang, Proc. of ACL. of ACL2021</p>
<p>A self-supervised mixedcurvature graph neural network. L Sun, Z Zhang, J Ye, Proc. of AAAI. of AAAI2022</p>
<p>Sincere: sequential interaction networks representation learning on coevolving riemannian manifolds. J Ye, Z Zhang, L Sun, Y Yan, F Wang, F Ren, Proc. of WWW. of WWW2023</p>
<p>Datasets: A community library for natural language processing. Q Lhoest, A V Del Moral, Y Jernite, Proc. of EMNLP. of EMNLP2021</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, Proc. of NeurIPS. of NeurIPS2020</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, arXiv:2302.139712023arXiv preprint</p>
<p>Motif-aware riemannian graph neural network with generative-contrastive learning. L Sun, Z Huang, Z Wang, F Wang, H Peng, S Y Philip, Proc. of AAAI. of AAAI2024</p>
<p>Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning. X He, X Bresson, T Laurent, Proc. of ICLR. of ICLR2024</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, JMLR. 2020</p>
<p>Mentorgnn: Deriving curriculum for pre-training gnns. D Zhou, L Zheng, D Fu, Proc. of CIKM. of CIKM2022</p>
<p>BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S C H Hoi, Proc. of ICML. of ICML2023</p>
<p>Deep graph infomax. P Velikovi, W Fedus, W L Hamilton, P Li, Y Bengio, R D Hjelm, Proc. of ICLR. of ICLR2019</p>
<p>Variational graph auto-encoders. T N Kipf, M Welling, NIPS Workshop on Bayesian Deep Learning. 2016</p>
<p>Ma-gcl: Model augmentation tricks for graph contrastive learning. X Gong, C Yang, C Shi, Proc. of AAAI. of AAAI2023</p>
<p>Voucher abuse detection with prompt-based fine-tuning on graph neural networks. Z Wen, Y Fang, Y Liu, Y Guo, S Hao, Proc. of CIKM. of CIKM2023</p>
<p>Gcc: Graph contrastive coding for graph neural network pre-training. J Qiu, Q Chen, Y Dong, Proc. of KDD. of KDD2020</p>
<p>Graph contrastive learning with augmentations. Y You, T Chen, Y Sui, Proc. of NeurIPS. of NeurIPS2020</p>
<p>A datacentric framework to endow graph neural networks with outof-distribution detection ability. Y Guo, C Yang, Y Chen, J Liu, C Shi, J Du, Proc. of KDD. of KDD2023</p>
<p>Universal prompt tuning for graph neural networks. T Fang, Y M Zhang, Y Yang, Proc. of NeurIPS. of NeurIPS2023</p>
<p>The reduction of a graph to canonical form and the algebra which appears therein. B Weisfeiler, A Leman, 1968nti, Series</p>
<p>Self-supervised learning on graphs: Contrastive, generative, or predictive. L Wu, H Lin, C Tan, Z Gao, S Z Li, IEEE TKDE. 2021</p>
<p>Self-supervised learning of graph neural networks: A unified review. Y Xie, Z Xu, J Zhang, Z Wang, S Ji, IEEE TPAMI. 2022</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez,  Kaiser, I Polosukhin, Proc. of NeurIPS. of NeurIPS2017</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, Proc. of ICLR. of ICLR2021</p>
<p>Swin transformer: Hierarchical vision transformer using shifted windows. Z Liu, Y Lin, Y Cao, H Hu, Y Wei, Z Zhang, S Lin, B Guo, Proc. of ICCV. of ICCV2021</p>
<p>Pure transformers are powerful graph learners. J Kim, D Nguyen, S Min, S Cho, M Lee, H Lee, S Hong, Proc. of NeurIPS. of NeurIPS2022</p>
<p>Graph-bert: Only attention is needed for learning graph representations. J Zhang, H Zhang, C Xia, L Sun, arXiv:2001.051402020arXiv preprint</p>
<p>Self-supervised graph transformer on large-scale molecular data. Y Rong, Y Bian, T Xu, Proc. of NeurIPS. of NeurIPS2020</p>
<p>Attending to graph transformers. L Mller, M Galkin, C Morris, L Rampsek, 2024TMLR</p>
<p>All in one and one for all: A simple yet effective method towards crossdomain graph pretraining. H Zhao, A Chen, X Sun, H Cheng, J Li, Proc. of KDD. of KDD2024</p>
<p>Fug: Feature-universal graph contrastive pre-training for graphs with diverse node features. J Zhao, D Jin, M Ge, Proc. of NeurIPS. of NeurIPS2024</p>
<p>Gpt-gnn: Generative pretraining of graph neural networks. Z Hu, Y Dong, K Wang, Proc. of KDD. of KDD2020</p>
<p>Fine-tuning graph neural networks by preserving graph generative patterns. Y Sun, Q Zhu, Y Yang, C Wang, T Fan, J Zhu, L Chen, Proc. of AAAI. of AAAI2024</p>
<p>The power of scale for parameter-efficient prompt tuning. B Lester, R Al-Rfou, N Constant, Proc. of EMNLP. of EMNLP2021</p>
<p>Ptuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. X Liu, K Ji, Y Fu, W Tam, Z Du, Z Yang, J Tang, Proc. of ACL. of ACL2022</p>
<p>Inductive graph alignment prompt: Bridging the gap between graph pre-training and inductive fine-tuning from spectral perspective. Y Yan, P Zhang, Z Fang, Q Long, Proc. of WWW. of WWW2024</p>
<p>Replay-and-forgetfree graph class-incremental learning: A task profiling and prompting approach. C Niu, G Pang, L Chen, B Liu, Proc. of NeurIPS. of NeurIPS2024</p>
<p>Generalized graph prompt: Toward a unification of pre-training and downstream tasks on graphs. X Yu, Z Liu, Y Fang, Z Liu, S Chen, X Zhang, IEEE TKDE. 2024</p>
<p>Non-homophilic graph pre-training and prompt learning. X Yu, J Zhang, Y Fang, R Jiang, Proc. of KDD. of KDD202534681</p>
<p>Contrastive pre-training of gnns on heterogeneous graphs. X Jiang, Y Lu, Y Fang, C Shi, Proc. of CIKM. of CIKM2021</p>
<p>Pre-training on large-scale heterogeneous graph. X Jiang, T Jia, Y Fang, Proc. of KDD. of KDD2021</p>
<p>Multigprompt for multi-task pre-training and prompting on graphs. X Yu, C Zhou, Y Fang, X Zhang, Proc. of WWW. of WWW2024</p>
<p>Hgprompt: Bridging homogeneous and heterogeneous graphs for few-shot prompt learning. X Yu, Y Fang, Z Liu, X Zhang, Proc. of AAAI. of AAAI2024</p>
<p>Pre-trained hypergraph convolutional neural networks with self-supervised learning. Y Deng, R Zhang, P Xu, J Ma, Q Gu, 2024TMLR</p>
<p>Instruction-based hypergraph pretraining. M Yang, Z Liu, L Yang, Proc. of SIGIR. of SIGIR2024</p>
<p>Spatial-temporal graph learning with adversarial contrastive adaptation. Q Zhang, C Huang, L Xia, Proc. of ICML. of ICML2023</p>
<p>Gpt-st: Generative pretraining of spatio-temporal graph neural networks. Z Li, L Xia, Y Xu, C Huang, Proc. of NeurIPS. of NeurIPS2023</p>
<p>Language is all a graph needs. R Ye, C Zhang, R Wang, S Xu, Y Zhang, Proc. of EACL. of EACL2024</p>
<p>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. J Guo, L Du, H Liu, arXiv:2305.150662023arXiv preprint</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Z Chen, H Mao, H Li, ACM SIGKDD Explorations Newsletter. 2522024</p>
<p>Gimlet: A unified graphtext model for instruction-based molecule zero-shot learning. H Zhao, S Liu, M Chang, Proc. of NeurIPS. of NeurIPS202336</p>
<p>Meta-transformer: A unified framework for multimodal learning. Y Zhang, K Gong, K Zhang, H Li, Y Qiao, W Ouyang, X Yue, arXiv:2307.108022023arXiv preprint</p>
<p>Deberta: Decodingenhanced bert with disentangled attention. P He, X Liu, J Gao, W Chen, Proc. of ICLR. of ICLR2021</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. N Reimers, I Gurevych, Proc. of EMNLP. of EMNLP2019</p>
<p>Openai, arXiv:2303.08774GPT-4 technical report. 2023arXiv preprint</p>
<p>Pretrained language models to solve graph tasks in natural language. F Wenkel, G Wolf, B Knyazev, ICML 2023 Workshop on Structured Probabilistic Inference {\&amp;} Generative Modeling. 2023</p>
<p>Can llms effectively leverage graph structural information: when and why. J Huang, X Zhang, Q Mei, J Ma, NeurIPS2023 workshop, 2023</p>
<p>Graphwiz: An instructionfollowing language model for graph problems. N Chen, Y Li, J Tang, J Li, Proc. of KDD. of KDD2024</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, Proc. of NeurIPS. of NeurIPS2022</p>
<p>Zero-shot causal graph extrapolation from text via llms. A Antonucci, G Piqu, M Zaffalon, AAAI 2024 XAI4Sci workshop. 2024</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Z Yang, Z Dai, Y Yang, J Carbonell, R R Salakhutdinov, Q V Le, Proc. of NeurIPS. of NeurIPS2019</p>
<p>BART: denoising sequenceto-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, Proc. of ACL. of ACL2020</p>
<p>A theoretical analysis of contrastive unsupervised representation learning. N Saunshi, O Plevrakis, S Arora, M Khodak, H Khandeparkar, Proc. of ICML. of ICML2019</p>
<p>Evaluating large language models on graphs: Performance insights and comparative analysis. C Liu, B Wu, arXiv:2308.112242023arXiv preprint</p>
<p>How can we know what language models know. Z Jiang, F F Xu, J Araki, G Neubig, TACL. 2020</p>
<p>What to pre-train on? efficient intermediate task selection. C Poth, J Pfeiffer, A Rckl, I Gurevych, Proc. of EMNLP. of EMNLP2021</p>
<p>Multi-task deep neural networks for natural language understanding. X Liu, P He, W Chen, J Gao, Proc. of ACL. of ACL2019</p>
<p>Parameterefficient transfer learning for nlp. N Houlsby, A Giurgiu, S Jastrzebski, Proc. of ICML. of ICML2019</p>
<p>Llm4dyg: Can large language models solve problems on dynamic graphs. Z Zhang, X Wang, Z Zhang, H Li, Y Qin, S Wu, W Zhu, Proc. of KDD. of KDD2024</p>
<p>Train your own GNN teacher: Graph-aware distillation on textual graphs. C Mavromatis, V N Ioannidis, S Wang, Proc. of KDD. of KDD2023</p>
<p>Node feature extraction by self-supervised multi-scale neighborhood prediction. E Chien, W Chang, C Hsieh, H Yu, J Zhang, O Milenkovic, I S Dhillon, Proc. of ICLR. of ICLR2022</p>
<p>Intruding with words: Towards understanding graph injection attacks at the text level. R Lei, Y Hu, Y Ren, Z Wei, Proc. of NeurIPS. of NeurIPS2024</p>
<p>Graph-aware language model pre-training on a large graph corpus can help multiple graph applications. H Xie, D Zheng, J Ma, Proc. of KDD. of KDD2023</p>
<p>One for all: Towards training one graph model for all classification tasks. H Liu, J Feng, L Kong, N Liang, D Tao, Y Chen, M Zhang, Proc. of ICLR. of ICLR2024</p>
<p>Heterformer: Transformer-based deep node representation learning on heterogeneous text-rich networks. B Jin, Y Zhang, Q Zhu, J Han, Proc. of KDD. of KDD2023</p>
<p>Edgeformers: Graphempowered transformers for representation learning on textualedge networks. B Jin, Y Zhang, Y Meng, J Han, Proc. of ICLR. of ICLR2023</p>
<p>Llmrec: Large language models with graph augmentation for recommendation. W Wei, X Ren, J Tang, Proc. of WSDM. of WSDM2024</p>
<p>Walklm: A uniform language model fine-tuning framework for attributed graph embedding. Y Tan, Z Zhou, H Lv, W Liu, C Yang, Proc. of NeurIPS. of NeurIPS2023</p>
<p>Touchup-g: Improving feature representation through graphcentric finetuning. J Zhu, X Song, V Ioannidis, D Koutra, C Faloutsos, Proc. of SIGIR. of SIGIR2024</p>
<p>Learning multiplex embeddings on text-rich networks with one text encoder. B Jin, W Zhang, Y Zhang, Y Meng, H Zhao, J Han, NeurIPS Workshop2023</p>
<p>Large language models as topological structure enhancers for text-attributed graphs. S Sun, Y Ren, C Ma, X Zhang, arXiv:2311.143242023arXiv preprint</p>
<p>Label-free node classification on graphs with large language models (llms). Z Chen, H Mao, H Wen, H Han, W Jin, H Zhang, H Liu, J Tang, Proc. of ICLR. of ICLR2024</p>
<p>Graphformers: Gnn-nested transformers for representation learning on textual graph. J Yang, Z Liu, S Xiao, Proc. of NeurIPS. of NeurIPS2021</p>
<p>Learning on large-scale text-attributed graphs via variational inference. J Zhao, M Qu, C Li, H Yan, Q Liu, R Li, X Xie, J Tang, Proc. of ICLR. of ICLR2023</p>
<p>Augmenting low-resource text classification with graph-grounded pre-training and prompting. Z Wen, Y Fang, Proc. of SIGIR. of SIGIR2023</p>
<p>Can gnn be good adapter for llms. X Huang, K Han, Y Yang, Proc. of WWW. of WWW2024</p>
<p>Efficient tuning and inference for large language models on textual graphs. Y Zhu, Y Wang, H Shi, S Tang, Proc. of IJCAI. of IJCAI2024</p>
<p>Patton: Language model pretraining on text-rich networks. B Jin, W Zhang, Y Zhang, Y Meng, X Zhang, Q Zhu, J Han, Proc. of ACL. of ACL2023</p>
<p>Opengraph: Towards open graph foundation models. L Xia, B Kao, C Huang, Findings of EMNLP. 2024</p>
<p>Representation learning with large language models for recommendation. X Ren, W Wei, L Xia, Proc. of WWW. of WWWACM2024</p>
<p>Text2mol: Cross-modal molecule retrieval with natural language queries. C Edwards, C Zhai, H Ji, Proc. of EMNLP. of EMNLP2021</p>
<p>Multi-modal molecule structure-text model for text-based retrieval and editing. S Liu, W Nie, C Wang, NMI. 5122023</p>
<p>Enhancing activity prediction models in drug discovery with the ability to understand human language. P Seidl, A Vall, S Hochreiter, G Klambauer, Proc. of ICML. of ICML2023</p>
<p>Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. Z Liu, S Li, Y Luo, Proc. of EMNLP. of EMNLP202315638</p>
<p>Git-mol: A multi-modal large language model for molecular science with graph, image, and text. P Liu, Y Ren, J Tao, Z Ren, Comput. Biol. Medicine. 1711080732024</p>
<p>Graphtranslator: Aligning graph model to large language model for open-ended tasks. M Zhang, M Sun, P Wang, Proc. of WWW. of WWW2024</p>
<p>Graphgpt: Graph instruction tuning for large language models. J Tang, Y Yang, W Wei, L Shi, L Su, S Cheng, D Yin, C Huang, Proc. of SIGIR. of SIGIR2024</p>
<p>Pretraining language models with text-attributed heterogeneous graphs. T Zou, L Yu, Y Huang, L Sun, B Du, Findings of EMNLP. 202310333</p>
<p>Can we soft prompt llms for graph learning tasks. Z Liu, X He, Y Tian, N V Chawla, Proc. of WWW. of WWW2024</p>
<p>Instructgraph: Boosting large language models via graph-centric instruction tuning and preference alignment. J Wang, J Wu, Y Wu, Findings of ACL. 2024</p>
<p>Relm: Leveraging language models for enhanced chemical reaction prediction. Y Shi, A Zhang, E Zhang, Z Liu, X Wang, Findings of EMNLP. 2023</p>
<p>Llms as zero-shot graph learners: Alignment of GNN representations with LLM token embeddings. D Wang, Y Zuo, F Li, J Wu, Proc. of NeurIPS. of NeurIPS2024</p>
<p>G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. X He, Y Tian, Y Sun, Proc. of NeurIPS. of NeurIPS2024</p>
<p>Higpt: Heterogeneous graph language model. J Tang, Y Yang, W Wei, L Shi, L Xia, D Yin, C Huang, Proc. of KDD. of KDD2024</p>
<p>Bootstrapping heterogeneous graph representation learning via large language models: A generalized approach. H Gao, C Zhang, F Wu, J Zhao, C Zheng, H Liu, Proc. of AAAI. of AAAI2025</p>
<p>Hyperbert: Mixing hypergraph-aware layers with language models for node classification on text-attributed hypergraphs. A Bazaga, P Li, G Micklem, Findings of EMNLP. 2024</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. W Hu, M Fey, M Zitnik, Y Dong, H Ren, B Liu, M Catasta, J Leskovec, Proc. of NeurIPS. of NeurIPS2020</p>
<p>Igb: Addressing the gaps in labeling, features, heterogeneity, and size of public graph datasets for deep learning research. A Khatua, V S Mailthody, B Taleka, T Ma, X Song, W.-M Hwu, Proc. of KDD. of KDD2023</p>
<p>Teg-db: A comprehensive dataset and benchmark of textual-edge graphs. Z Li, Z Gou, X Zhang, Z Liu, S Li, Y Hu, C Ling, Z Zhang, L Zhao, Proc. of NeurIPS. of NeurIPS2024</p>
<p>Can large language models analyze graphs like professionals? a benchmark, datasets and models. X Li, W Chen, Q Chu, H Li, Z Sun, R Li, C Qian, Y Wei, Z Liu, C Shi, Proc. of NeurIPS. of NeurIPS2024</p>
<p>Evaluating large language models at evaluating instruction following. Z Zeng, J Yu, T Gao, Y Meng, T Goyal, D Chen, Proc. of ICLR. of ICLR2024</p>
<p>Adversarial glue: A multitask benchmark for robustness evaluation of language models. B Wang, C Xu, S Wang, Proc. of NeurIPS. of NeurIPS2021</p>
<p>Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. B Wang, W Chen, H Pei, Proc. of NeurIPS. of NeurIPS2023</p>
<p>Holistic evaluation of language models. R Bommasani, P Liang, T Lee, Annals of the New York Academy of Sciences. 2023</p>
<p>Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. T Dao, A Gu, Proc. of ICML. of ICML2024</p>
<p>White-box transformers via sparse rate reduction. Y Yu, S Buchanan, D Pai, T Chu, Z Wu, S Tong, B Haeffele, Y Ma, Proc. of NeurIPS. of NeurIPS202436</p>
<p>Exploring neural scaling law and data pruning methods for node classification on large-scale graphs. Z Wang, Y Li, B Ding, Y Li, Z Wei, Proc. of WWW. of WWW2024</p>
<p>Position: Graph foundation models are already here. H Mao, Z Chen, W Tang, Proc. of ICML. of ICML2024</p>
<p>Graph mixture of experts: learning on large-scale graphs with explicit diversity modeling. H Wang, Z Jiang, Y You, Proc. of NeurIPS. of NeurIPS202350837</p>
<p>Towards artificial general intelligence via a multimodal foundation model. N Fei, Z Lu, Y Gao, Nature Communications. 2022</p>
<p>On the power of foundation models. Y Yuan, Proc. of ICML. PMLR, 2023. of ICML. PMLR, 2023530</p>
<p>Its all graph to me: Single-model graph representation learning on multiple domains. A Davies, R Green, N Ajmeri, T S Filho, NeurIPS Workshop2023</p>
<p>Graphcontrol: Adding conditional control to universal graph pre-trained models for graph domain transfer learning. Y Zhu, Y Wang, H Shi, Z Zhang, D Jiao, S Tang, Proc. of WWW. of WWW2024</p>
<p>Lion: Adversarial distillation of proprietary large language models. Y Jiang, C Chan, M Chen, W Wang, Proc. of EMNLP. of EMNLP2023</p>
<p>Editing large language models: Problems, methods, and opportunities. Y Yao, P Wang, B Tian, S Cheng, Z Li, S Deng, H Chen, N Zhang, Proc. of EMNLP. of EMNLP2023</p>
<p>How good are gpt models at machine translation? a comprehensive evaluation. A Hendy, M Abdelrehim, A Sharaf, arXiv:2302.092102023arXiv preprint</p>
<p>A survey of controllable text generation using transformer-based pretrained language models. H Zhang, H Song, S Li, M Zhou, D Song, 2023ACM CSUR</p>
<p>AGL: A scalable system for industrial-purpose graph machine learning. D Zhang, X Huang, Z Liu, Proc. VLDB Endow. VLDB Endow2020</p>
<p>A review on graph neural network methods in financial applications. J Wang, S Zhang, Y Xiao, R Song, Journal of Data Science. 2022022</p>
<p>Thought propagation: An analogical approach to complex reasoning with large language models. J Yu, R He, Z Ying, Proc. of ICLR. of ICLR2024</p>
<p>Estimated research and development investment needed to bring a new medicine to market. O J Wouters, M Mckee, J Luyten, Jama. 2009-2018. 2020</p>
<p>Pre-training molecular graph representation with 3d geometry. S Liu, H Wang, W Liu, Proc. of ICLR. of ICLR2022</p>
<p>A systematic survey of chemical pre-trained models. J Xia, Y Zhu, Y Du, S Z Li, Proc. of IJCAI. of IJCAI2023</p>
<p>Uncertainty quantification of sparse travel demand prediction with spatialtemporal graph neural networks. D Zhuang, S Wang, H Koutsopoulos, Proc. of KDD. of KDD2022</p>
<p>Traffic flow prediction via spatial temporal graph neural network. X Wang, Y Ma, Y Wang, W Jin, X Wang, J Tang, C Jia, J Yu, Proc. of WWW. of WWW2020</p>
<p>Building transportation foundation model via generative graph transformer. X Wang, D Wang, L Chen, F.-Y Wang, Y Lin, Proc. of ITSC. of ITSCIEEE2023</p>
<p>Black-box tuning for languagemodel-as-a-service. T Sun, Y Shao, H Qian, Proc. of ICML. of ICML2022</p>
<p>Siren's song in the ai ocean: A survey on hallucination in large language models. Y Zhang, Y Li, L Cui, arXiv:2309.012192023arXiv preprint</p>
<p>Investigating the factual knowledge boundary of large language models with retrieval augmentation. R Ren, Y Wang, Y Qu, arXiv:2307.110192023arXiv preprint</p>
<p>How pre-trained language models capture factual knowledge? a causal-inspired analysis. S Li, X Li, L Shang, Findings of ACL. 2022</p>
<p>Endowing pre-trained graph models with provable fairness. Z Zhang, M Zhang, Y Yu, C Yang, J Liu, C Shi, Proc. of WWW. of WWW2024</p>
<p>Inference attacks against graph neural networks. Z Zhang, M Chen, M Backes, Y Shen, Y Zhang, USENIX Security. 2022</p>
<p>Can large language models improve the adversarial robustness of graph neural networks?. Z Zhang, X Wang, H Zhou, Y Yu, M Zhang, C Yang, C Shi, Proc. of KDD. of KDD2025</p>
<p>Be confident! towards trustworthy graph neural networks via confidence calibration. X Wang, H Liu, C Shi, C Yang, Proc. of NeurIPS. of NeurIPS2021</p>
<p>Learning and evaluating graph neural network explanations based on counterfactual and factual reasoning. J Tan, S Geng, Z Fu, Proc. of WWW. of WWW2022</p>
<p>A federated graph neural network framework for privacy-preserving personalization. C Wu, F Wu, L Lyu, T Qi, Y Huang, X Xie, Nature Communications. 2022</p>
<p>Federated heterogeneous graph neural network for privacypreserving recommendation. B Yan, Y Cao, H Wang, W Yang, J Du, C Shi, Proc. of WWW. of WWW2024</p>
<p>Beyond memorization: Violating privacy via inference with large language models. R Staab, M Vero, M Balunovic, M Vechev, Proc. of ICLR. of ICLR2024</p>
<p>Privacy and fairness in federated learning: on the perspective of trade-off. H Chen, T Zhu, T Zhang, W Zhou, P S Yu, 2023ACM CSUR</p>
<p>Red teaming language model detectors with language models. Z Shi, Y Wang, F Yin, X Chen, K.-W Chang, C.-J Hsieh, TACL. 122024</p>
<p>Slaps: Selfsupervision improves structure learning for graph neural networks. B Fatemi, L El Asri, S M Kazemi, Proc. of NeurIPS. of NeurIPS202134681</p>
<p>Imgcl: Revisiting graph contrastive learning on imbalanced node classification. L Zeng, L Li, Z Gao, P Zhao, J Li, Proc. of AAAI. of AAAI202337</p>
<p>Revisiting initializing then refining: an incomplete and missing graph imputation network. W Tu, B Xiao, X Liu, S Zhou, Z Cai, J Cheng, IEEE TNNLS. 2024</p>
<p>Gcnet: Graph completion network for incomplete multimodal learning in conversation. Z Lian, L Chen, L Sun, B Liu, J Tao, IEEE TPAMI. 4572023</p>
<p>Heterogeneous graph attention network. X Wang, H Ji, C Shi, Proc. of WWW. of WWW2019</p>
<p>Graph neural networks designed for different graph types: A survey. J M Thomas, A Moallemy-Oureh, S Beddar-Wiesing, C Holzhter, 2023TMLR</p>
<p>Aligraph: A comprehensive graph neural network platform. H Yang, Proc. of KDD. of KDD2019</p>
<p>Long range graph benchmark. V P Dwivedi, L Rampek, M Galkin, Proc. of NeurIPS. of NeurIPS2022</p>
<p>Cross-domain graph convolutions for adversarial unsupervised domain adaptation. R Zhu, X Jiang, J Lu, S Li, IEEE TNNLS. 2021</p>
<p>Design space for graph neural networks. J You, Z Ying, J Leskovec, Proc. of NeurIPS. of NeurIPS2020</p>
<p>Distance encoding: Design provably more powerful neural networks for graph representation learning. P Li, Y Wang, H Wang, J Leskovec, Proc. of NeurIPS. of NeurIPS2020</p>
<p>Will more expressive graph neural networks do better on generative tasks?. X Zou, X Zhao, P Li, Y Zhao, Proc. of LoG. PMLR. of LoG. PMLR2024</p>
<p>He is currently pursuing the Ph.D. degree in computer science and technology from Beijing University of Posts and Telecommunications. His research interests include graph data mining and machine learning. Beijing, China; Beijing, China2020Jiawei Liu received the B.S. degree in computer science and technology from Beijing University of Posts and Telecommunications</p>
<p>He received his Bachelor and Ph.D. degrees from Tsinghua University in 2014 and 2019, respectively. Cheng's research interests include data mining, natural language processing and social computing. He has published 60+ papers in top journals and conferences, such as NeurIPS, ICLR, KDD and ACL. Cheng Yang is an Associate Professor of Computer Science at Beijing University of Posts and Telecommunications (BUPT)</p>
<p>He is currently pursuing the PhD degree in computer science and technology from Beijing University of Posts and Telecommunications. 2022Beijing, China; Beijing, Chinafrom Beijing University of Posts and TelecommunicationsHis current research interests are in graph neural networks, data mining and machine learning</p>
<p>He obtained his Bachelor's degree in Computer Science and Technology from BUPT in 2022. His primary research interests are in data mining, graph neural networks, and natural language processing. School of Computer Science, Beijing University of Posts and Telecommunications (BUPT)Junze Chen is a master's student at the</p>            </div>
        </div>

    </div>
</body>
</html>
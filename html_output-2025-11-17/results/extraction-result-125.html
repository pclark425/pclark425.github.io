<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-125 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-125</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-125</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-7.html">extraction-schema-7</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models' size and training data diversity influence performance on first-order Theory-of-Mind tasks, including evaluation methods, saturation effects, counter-evidence, and additional influencing factors.</div>
                <p><strong>Paper ID:</strong> paper-6b9ae39cc8c6a9146384e138cff3f9da13ce83ab</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6b9ae39cc8c6a9146384e138cff3f9da13ce83ab" target="_blank">LLMs achieve adult human performance on higher-order theory of mind tasks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is found that GPT-4 and Flan-PaLM reach adult-level and near adult-level performance on ToM tasks overall, and that GPT-4 exceeds adult performance on 6th order inferences.</p>
                <p><strong>Paper Abstract:</strong> This paper examines the extent to which large language models (LLMs) have developed higher-order theory of mind (ToM); the human ability to reason about multiple mental and emotional states in a recursive manner (e.g. I think that you believe that she knows). This paper builds on prior work by introducing a handwritten test suite -- Multi-Order Theory of Mind Q&A -- and using it to compare the performance of five LLMs to a newly gathered adult human benchmark. We find that GPT-4 and Flan-PaLM reach adult-level and near adult-level performance on ToM tasks overall, and that GPT-4 exceeds adult performance on 6th order inferences. Our results suggest that there is an interplay between model size and finetuning for the realisation of ToM abilities, and that the best-performing LLMs have developed a generalised capacity for ToM. Given the role that higher-order ToM plays in a wide range of cooperative and competitive human behaviours, these findings have significant implications for user-facing LLM applications.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e125.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e125.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models' size and training data diversity influence performance on first-order Theory-of-Mind tasks, including evaluation methods, saturation effects, counter-evidence, and additional influencing factors.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pre-trained transformer model developed by OpenAI, exhibiting advanced capabilities in natural language understanding and generation, particularly in higher-order Theory of Mind tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.7T</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 is a multimodal model that has been fine-tuned using reinforcement learning from human feedback (RLHF), enhancing its performance on various tasks, including Theory of Mind.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on a diverse dataset that includes a wide range of text sources, though specific details on the diversity of social narratives are not disclosed.</td>
                        </tr>
                        <tr>
                            <td><strong>tom_task_name</strong></td>
                            <td>Multi-Order Theory of Mind Question & Answer (MoToMQA)</td>
                        </tr>
                        <tr>
                            <td><strong>tom_task_description</strong></td>
                            <td>The MoToMQA benchmark assesses the ability to answer true/false questions about characters in stories, requiring inference about mental states across orders 2-6.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Achieved 93% accuracy on 6th-order tasks, outperforming human accuracy of 82%.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated using log probabilities of candidate responses based on the model's outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_of_model_size</strong></td>
                            <td>The study indicates a positive relationship between model size and Theory of Mind capabilities, with larger models like GPT-4 showing superior performance.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_of_training_data</strong></td>
                            <td>While the paper does not specify the impact of training data diversity, it suggests that fine-tuning may enhance performance, indicating that richer datasets could be beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>saturation_effect_mentioned</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>role_of_finetuning</strong></td>
                            <td>Fine-tuning on instruction-based datasets appears to improve performance, as seen with Flan-PaLM, which is a fine-tuned version of PaLM.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Ullman (2023) demonstrated decreased performance with minor task perturbations, suggesting limitations in the robustness of LLMs' ToM capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_factors</strong></td>
                            <td>The paper discusses the role of multimodality in enhancing performance, particularly for GPT-4, which may leverage visual cues in addition to text.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e125.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e125.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models' size and training data diversity influence performance on first-order Theory-of-Mind tasks, including evaluation methods, saturation effects, counter-evidence, and additional influencing factors.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of the PaLM model fine-tuned for instruction-following tasks, showing strong performance in Theory of Mind evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Flan-PaLM is designed to follow instructions better than its base model, PaLM, and has been fine-tuned on a diverse set of natural language tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Utilizes a large corpus of text, including social media conversations and filtered web pages, but specific diversity metrics are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>tom_task_name</strong></td>
                            <td>Multi-Order Theory of Mind Question & Answer (MoToMQA)</td>
                        </tr>
                        <tr>
                            <td><strong>tom_task_description</strong></td>
                            <td>The MoToMQA benchmark assesses the ability to answer true/false questions about characters in stories, requiring inference about mental states across orders 2-6.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Achieved near-human performance on ToM tasks, with significant accuracy on lower orders but less so on higher orders compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated using log probabilities of candidate responses based on the model's outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_of_model_size</strong></td>
                            <td>The findings suggest that larger models like Flan-PaLM perform better on ToM tasks, although not as well as GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_of_training_data</strong></td>
                            <td>The paper implies that fine-tuning on instruction-based datasets enhances performance, indicating that diverse training data may improve ToM capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>saturation_effect_mentioned</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>role_of_finetuning</strong></td>
                            <td>Fine-tuning on instruction-based datasets is highlighted as a key factor in improving performance on ToM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>The paper notes that smaller models like LaMDA and GPT-3.5 performed poorly on ToM tasks, indicating that size and training alone do not guarantee success.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_factors</strong></td>
                            <td>The study mentions the importance of prompt design and the potential impact of anchoring effects on model responses.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Theory of Mind in Large Language Models: A Comprehensive Review <em>(Rating: 2)</em></li>
                <li>Scaling Laws for Language Models and Their Implications for Theory of Mind <em>(Rating: 1)</em></li>
                <li>The Role of Fine-Tuning in Enhancing Theory of Mind Capabilities in LLMs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-125",
    "paper_id": "paper-6b9ae39cc8c6a9146384e138cff3f9da13ce83ab",
    "extraction_schema_id": "extraction-schema-7",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A large pre-trained transformer model developed by OpenAI, exhibiting advanced capabilities in natural language understanding and generation, particularly in higher-order Theory of Mind tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "1.7T",
            "model_description": "GPT-4 is a multimodal model that has been fine-tuned using reinforcement learning from human feedback (RLHF), enhancing its performance on various tasks, including Theory of Mind.",
            "training_data_description": "Trained on a diverse dataset that includes a wide range of text sources, though specific details on the diversity of social narratives are not disclosed.",
            "tom_task_name": "Multi-Order Theory of Mind Question & Answer (MoToMQA)",
            "tom_task_description": "The MoToMQA benchmark assesses the ability to answer true/false questions about characters in stories, requiring inference about mental states across orders 2-6.",
            "performance_result": "Achieved 93% accuracy on 6th-order tasks, outperforming human accuracy of 82%.",
            "evaluation_method": "Evaluated using log probabilities of candidate responses based on the model's outputs.",
            "impact_of_model_size": "The study indicates a positive relationship between model size and Theory of Mind capabilities, with larger models like GPT-4 showing superior performance.",
            "impact_of_training_data": "While the paper does not specify the impact of training data diversity, it suggests that fine-tuning may enhance performance, indicating that richer datasets could be beneficial.",
            "saturation_effect_mentioned": true,
            "role_of_finetuning": "Fine-tuning on instruction-based datasets appears to improve performance, as seen with Flan-PaLM, which is a fine-tuned version of PaLM.",
            "counter_evidence": "Ullman (2023) demonstrated decreased performance with minor task perturbations, suggesting limitations in the robustness of LLMs' ToM capabilities.",
            "additional_factors": "The paper discusses the role of multimodality in enhancing performance, particularly for GPT-4, which may leverage visual cues in addition to text.",
            "uuid": "e125.0"
        },
        {
            "name_short": "Flan-PaLM",
            "name_full": "Flan-PaLM",
            "brief_description": "A variant of the PaLM model fine-tuned for instruction-following tasks, showing strong performance in Theory of Mind evaluations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-PaLM",
            "model_size": "540B",
            "model_description": "Flan-PaLM is designed to follow instructions better than its base model, PaLM, and has been fine-tuned on a diverse set of natural language tasks.",
            "training_data_description": "Utilizes a large corpus of text, including social media conversations and filtered web pages, but specific diversity metrics are not provided.",
            "tom_task_name": "Multi-Order Theory of Mind Question & Answer (MoToMQA)",
            "tom_task_description": "The MoToMQA benchmark assesses the ability to answer true/false questions about characters in stories, requiring inference about mental states across orders 2-6.",
            "performance_result": "Achieved near-human performance on ToM tasks, with significant accuracy on lower orders but less so on higher orders compared to GPT-4.",
            "evaluation_method": "Evaluated using log probabilities of candidate responses based on the model's outputs.",
            "impact_of_model_size": "The findings suggest that larger models like Flan-PaLM perform better on ToM tasks, although not as well as GPT-4.",
            "impact_of_training_data": "The paper implies that fine-tuning on instruction-based datasets enhances performance, indicating that diverse training data may improve ToM capabilities.",
            "saturation_effect_mentioned": true,
            "role_of_finetuning": "Fine-tuning on instruction-based datasets is highlighted as a key factor in improving performance on ToM tasks.",
            "counter_evidence": "The paper notes that smaller models like LaMDA and GPT-3.5 performed poorly on ToM tasks, indicating that size and training alone do not guarantee success.",
            "additional_factors": "The study mentions the importance of prompt design and the potential impact of anchoring effects on model responses.",
            "uuid": "e125.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Theory of Mind in Large Language Models: A Comprehensive Review",
            "rating": 2
        },
        {
            "paper_title": "Scaling Laws for Language Models and Their Implications for Theory of Mind",
            "rating": 1
        },
        {
            "paper_title": "The Role of Fine-Tuning in Enhancing Theory of Mind Capabilities in LLMs",
            "rating": 1
        }
    ],
    "cost": 0.0033552,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LLMs achieve adult human performance on higher-order theory of mind tasks</h1>
<p>Winnie Street ${ }^{1 *}$ John Oliver Siy ${ }^{1}$ Geoff Keeling ${ }^{1}$ Adrien Baranes ${ }^{2}$ Benjamin Barnett ${ }^{1}$ Michael Mckibben ${ }^{3}$ Tatenda Kanyere ${ }^{4}$ Alison Lentz ${ }^{1}$ Blaise Aguera y Arcas ${ }^{1}$ Robin I. M. Dunbar ${ }^{5}$<br>${ }^{1}$ Google Research ${ }^{2}$ Google DeepMind ${ }^{3}$ Applied Physics Lab, Johns Hopkins University<br>${ }^{4}$ Work done at Google Research via Harvey Nash<br>${ }^{5}$ Department of Experimental Psychology, University of Oxford<br>istreet@google.com</p>
<h4>Abstract</h4>
<p>This paper examines the extent to which large language models (LLMs) have developed higher-order theory of mind (ToM); the human ability to reason about multiple mental and emotional states in a recursive manner (e.g. I think that you believe that she knows). This paper builds on prior work by introducing a handwritten test suite - Multi-Order Theory of Mind Q\&amp;A - and using it to compare the performance of five LLMs to a newly gathered adult human benchmark. We find that GPT-4 and Flan-PaLM reach adult-level and near adult-level performance on ToM tasks overall, and that GPT-4 exceeds adult performance on 6th order inferences. Our results suggest that there is an interplay between model size and finetuning for the realisation of ToM abilities, and that the best-performing LLMs have developed a generalised capacity for ToM. Given the role that higher-order ToM plays in a wide range of cooperative and competitive human behaviours, these findings have significant implications for user-facing LLM applications.</p>
<h2>1 Introduction</h2>
<p>Theory of Mind (ToM) is the ability to infer and reason about the mental states of oneself and others [Premack and Woodruff, 1978, Wimmer and Perner, 1983, Wellman et al., 2001]. ToM is central to human social intelligence: it enables humans to predict and influence behaviour [Humphrey, 1976, Wellman and Bartsch, 1988, Hooker et al., 2008].
Large Language Models (LLMs) exhibit some ToM competency [Kosinski, 2023, Bubeck et al., 2023, Shapira et al., 2023]. Most of the literature on LLM ToM has focused on 2nd-order ToM [Sap et al., 2022, Kosinski, 2023, Gandhi et al., 2024, Shapira et al., 2023], where the 'order of intentionality' (hereafter, 'order') is the number of mental states involved in a ToM reasoning process (i.e. a third-order statement is "I think you believe that she knows"). Yet LLMs are increasingly leveraged for multi-party social interaction contexts which require LLMs to engage in higher order ToM reasoning [Wang et al., 2023, Park et al., 2023].
In this paper, we examine LLM ToM from orders 2-6. We introduce a novel benchmark: Multi-Order Theory of Mind Question \&amp; Answer (MoToMQA). MoToMQA is based on a ToM test designed for human adults [Kinderman et al., 1998], and involves answering true/false questions about characters in short-form stories. We assess how ToM order affects LLM performance, how LLM performance compares to human performance, and how LLM performance on ToM tasks compares to performance on factual tasks of equivalent syntactic complexity. We show that GPT-4 and Flan-PaLM reach at-human or near-human performance on ToM tasks respectively.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Human, LaMDA, PaLM, Flan-PaLM, GPT-3.5 and GPT-4 performance on ToM tasks up to order 6</p>
<h1>2 Related work</h1>
<h3>2.1 Higher-order ToM</h3>
<p>Human adults are generally able to make ToM inferences up to 5 orders of intentionality (e.g. I believe that you think that I imagine that you want me to believe) [Kinderman et al., 1998, Stiller and Dunbar, 2007, Oesch and Dunbar, 2017]. ${ }^{1}$ Higher-order ToM competency varies within the population, including by gender [Hyde and Linn, 1988, Stiller and Dunbar, 2007], and is not deployed reliably across all social contexts [Keysar et al., 2003]. ToM at higher orders is also positively correlated with social complexity. Tracking the beliefs and desires of multiple individuals at once facilitates group negotiations, group bonding, and distinctly human behaviours and cultural institutions, including humour, religion and storytelling [Corballis, 2017, Dunbar, 2003, Fernández, 2013].</p>
<h3>2.2 LLM ToM</h3>
<p>Kosinski [2023] argued for spontaneous ToM emergence in LLMs based on GPT-4's success on a suite of tasks inspired by the classic Sally-Anne task. ${ }^{2}$ Ullman [2023] challenged this claim, demonstrating decreased performance with minor task perturbations. Further experiments involving benchmark suites like BigToM [Gandhi et al., 2024] and SocialIQa [Sap et al., 2022] show mixed results in LLM ToM capabilities. For example, Shapira et al. [2023] found success on some tasks but failure on others, suggesting that existing ToM capabilities in current state-of-the-art LLMs are not robust. To our knowledge only two other studies have explored LLM ToM at higher orders. He et al. [2023] assessed orders 0-4 (equivalent to our orders 2-5) and van Duijn et al. [2023] compared LLM performance with that of children aged 7-10 on two stories adapted from unpublished IMT stories. Our study adds to this work by testing one higher order than He et al. [2023], by utilising a larger, and entirely new set of handwritten stories and statements that we are certain models were</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>not exposed to during pretraining ${ }^{3}$ and by using log probabilities (logprobs) outputted for candidate tokens as the measure of the LLMs' preferred responses. Using logprobs adds robustness to our data because it takes into account multiple ways in which the model could provide the correct response.</p>
<p>Finally, we calibrate the LLM results against a large newly-gathered adult human benchmark. We believe that comparing LLM performance to that of adults, rather than children, is the most relevant yardstick for LLM social intelligence given that LLMs' primary interaction partners will be adults, and is a more concrete point of comparison because human higher-order ToM capacities continue to develop into early adulthood [Valle et al., 2015]. We do not, however, assume that the same cognitive processes underpin human and LLM performance on psychological tests.</p>
<h1>3 Materials and method</h1>
<p>We introduce a new benchmark, Multi-Order Theory of Mind Question \&amp; Answer (MoToMQA), to assess human and LLM ToM abilities at increasing orders, based upon the Imposing Memory Task (IMT), a well-validated psychological test for assessing higher-order ToM abilities in adults [Kinderman et al., 1998, Stiller and Dunbar, 2007, Lewis et al., 2011, Oesch and Dunbar, 2017, Powell et al., 2010]. MoToMQA is comprised of 7 short stories of about 200 words describing social interactions of 3 to 5 characters, accompanied by 20 true or false statements; 10 statements target ToM orders 2-6 and 10 concern facts in the story from 2-6 atomic propositions long, mapping to the order of ToM statements. From here onwards we will refer to 'orders' to describe ToM statements and 'levels' to describe the factual statements. The MoToMQA benchmark is available upon request, but we do not include it in this paper to prevent its inclusion in pretraining corpora for future LLMs, which could render the test redundant.</p>
<p>We checked each statement for unclear or ambiguous wording, grammatical errors and missing mental states or propositional clauses. We follow [Oesch and Dunbar, 2017] amendments to the IMT by having factual statements that only address social facts (ie. facts pertaining to individuals in the story), not instrumental facts (e.g. "the sky is blue") and counterbalancing the number of true and false statements per story, statement type, and ToM order or factual level. This resulted in the following set of statements per story, where the number indicates the order of ToM or level of factual statement, 'ToM' signifies ToM, 'F' signifies factual, 't' signifies a true statement, and ' f ' signifies a false statement: [ToM2t, ToM2f, ToM3t, ToM3f, ToM4t, ToM4f, ToM5t, ToM5f, ToM6t, ToM6f, F2t, F2f, F3t, F3f, F4t, F4f, F5t, F5f, F6t, F6f].</p>
<p>Factual statements require only recall, whereas ToM statements require recall plus inference. We include the factual statements as a control for human and LLM comprehension of the stories and capacity for recall. Given the inherent differences between ToM and factual statements, we added a further control for the effects of human memory capacity on performance on ToM statements by running two 'story conditions': one where participants read the story then proceeded to a second screen where they answered the question and the story was not visible ('no story'), and one where the story remained at the top of the screen when they answered the question to eliminate the chance that ToM failures were really memory failures ('no story').
Prompt design, which has been shown to have a significant impact on LLM performance on a range of tasks including ToM (e.g. [Brown et al., 2020, Lu et al., 2021, Ullman, 2023]). We therefore tested two prompt conditions: the 'human prompt' which uses the exact text from the human study, and the 'simplified prompt' which removes the text before the story and question, and provides 'Question:' and 'Answer:' tags. The simplified prompt is intended to make the nature of the Q \&amp; A task and thus the desired true/false response clearer to the models. Finally, we assessed whether LLM or human performance was subject to 'anchoring effects' based on the order of 'true' and 'false' in the question. The anchoring effect is a well-documented psychological phenomenon whereby people rely too heavily on the first piece of information offered ('the anchor') when making decisions [Tversky and Kahneman, 1974]. We ran two question conditions: one where the question read "Do you think the following statement is true or false?", and the other where the question read "Do you think the following statement is false or true?"</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.1 Procedures</h1>
<h3>3.1.1 Human procedure</h3>
<p>Participants were screened for having English as a first language using an adaptation of the most recent UK census survey (see Appendix). Participants were randomly assigned to one of the 7 stories and asked to read it twice, then randomly assigned to one of the 20 statements corresponding to that story and asked to provide a true/false response (see Figure 1). We did not include an attention check since attention checks have known limitations, including inducing purposeful noncompliance with a practice perceived as controlling [Silber et al., 2022], and leading to the systematic underrepresentation of certain demographic groups, for instance the young and less educated [Alvarez et al., 2019]. Each human saw only one statement to prevent them from learning across trials, analogously to the models which saw each trial independently and did not learn across them or 'in context'. We ran a pilot study with 1440 participants and made minor changes to the story and test procedure on the basis of the results (more details in Appendix)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Please read the following story twice in your head.</th>
<th style="text-align: center;">Please answer using the information provided and your own interpretation of the story.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Do you think the following statement is true or false?</td>
</tr>
<tr>
<td style="text-align: center;">Arthur and Charles were working on the same design team when a new designer, Marta, joined reporting to Charles. Marta was an extremely talented designer, but very shy. Arthur noticed that Charles kept speaking for Marta in meetings so he suggested to Marta that she bring up the issue with HR, but Marta said, "I'm his junior so it comes with the territory. I don't want to be difficult." Arthur thought Charles was taking advantage of Marta's reserved nature to take credit for her work.</td>
<td style="text-align: center;">"Marta believed Charles would think she was being difficult if she complained" <br> True <br> False</td>
</tr>
<tr>
<td style="text-align: center;">Screen 1: Story</td>
<td style="text-align: center;">Screen 2: True/False Answer</td>
</tr>
</tbody>
</table>
<p>We ran the final survey on Qualtrics in April 2023 and paid participants $\$ 5$ for a 5 minute survey. The study was Google branded, and participants were asked to sign a Google consent form. Partial responses, including those who drop out part way through, were screened out. Qualtrics cleaned the data, removing all responses that included gibberish, machine-generated responses, and nonsensical responses to the open-ended question. We did not exclude any other responses. We gathered 29,259 individual responses from U.K.-based participants for whom English is a first language. We gathered an even sample across age and gender groups and had quotas for each age group and gender per statement. In total we had 14682 female respondents, 14363 male respondents, 149 non-binary/ third gender respondents, and 53 who answered 'Prefer not to say' to the gender question. We had 7338 responses from those aged 18-29, 7335 from those ages 30-39, 7270 from those aged 40-49 and 7316 from those ages 50-65.</p>
<h3>3.1.2 LLM procedure</h3>
<p>We tested 5 language models: GPT 3.5 Turbo Instruct [Brown et al., 2020] and GPT 4 [Achiam et al., 2023] from OpenAI, and LaMDA [Thoppilan et al., 2022], PaLM [Chowdhery et al., 2023] and Flan-PaLM [Chung et al., 2024] from Google (for more details on the models we tested, see the Appendix). We couldn't test Google's Gemini model because analysis method requires ouput logprobs and logprobs are not exposed in the Gemini API. Below is a table of the key features of the models tested, according to what information is publicly available about them.
We provided single-token candidate words to LLM APIs as part of the input and assessed the log probabilities ${ }^{4}$ assigned to them. We sent the candidates using the 'candidate' parameter in the 'scoring' APIs for LaMDA, PaLM, and Flan-PaLM, and the 'logit bias' parameter for the GPT-3.5 and GPT-4 APIs. There was no temperature parameter for the LaMDA, PaLM and Flan-PaLM 'scoring' APIs, so we could only obtain one unique response per statement. We left the temperature at default of 1 for GPT-3.5 and GPT-4.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: LLMs tested in this study. OpenAI have not disclosed the number of parameters in GPT-4, although there are estimates of about 1.7T [McGuiness, 2023]. Flan-PaLM, GPT-3.5 Turbo Instruct and GPT-4 have been fine-tuned for following instructions and GPT-4 has been additionally fine-tuned through a process called reinforcement learning from human feedback (RLHF) which uses feedback from human users and data labellers to align responses with human preferences.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Parameters</th>
<th>Finetuning</th>
<th>Source</th>
</tr>
</thead>
<tbody>
<tr>
<td>LaMDA</td>
<td>35B</td>
<td>None</td>
<td>Thoppilan et al (2022)</td>
</tr>
<tr>
<td>PaLM 2</td>
<td>540B</td>
<td>None</td>
<td>Chowdery et al (2022)</td>
</tr>
<tr>
<td>Flan-PaLM</td>
<td>540B</td>
<td>Instructions</td>
<td>Longpre et al (2023)</td>
</tr>
<tr>
<td>GPT-3.5 Turbo Instruct</td>
<td>175B</td>
<td>Instructions</td>
<td>Ouyang et al (2022)</td>
</tr>
<tr>
<td>GPT-4</td>
<td>Unknown</td>
<td>Instructions, RLHF</td>
<td>OpenAI (2023)</td>
</tr>
</tbody>
</table>
<p>One issue with basing LLM task performance on the most probable next token, is that there are multiple semantically equivalent correct responses (e.g. when responding to the question "What colour is the sky?", the answer "blue" or the answer "The sky is blue" are equally valid and correct, but only the first response assigns the greatest probability to the token for 'blue'). We addressed this problem, and improved the robustness of our results by providing the model different capitalisations of 'true' and 'false' which are represented by different tokens. We also sent 'Yes' and 'No' as candidate responses in the second set, but did not include them in our analysis as neither is a valid responses to a true/false question. For all of the models, the candidates were tested in 2 sets of 4:[‘True’, ‘False’, ‘TRUE’, ‘FALSE’] and [‘true’, ‘false’, ‘Yes’, ‘No’].</p>
<p>We used the Google Colaboratory [Bisong and Bisong, 2019] to call the GPT-3.5, GPT-4, LaMDA, PaLM and Flan-PaLM APIs programmatically. Each call was performed by concatenating the story and a single statement at a time. In total, we processed 7 stories with 20 statements each across 4 conditions listed above and therefore collected 560 sets of 12 candidate logprobs, amounting to 5600 individual data points for each of the three language models studied. The API calls for LaMDA, PaLM and Flan-PaLM were conducted in February 2023. The calls for GPT-3.5 and GPT-4 were conducted in December 2023 and January 2024 respectively.</p>
<h1>3.2 Dataset creation</h1>
<p>Our LLM data was thus made up of 6 logprobs for our 6 candidates as a subset of the full distribution of probabilities the model produces. We extracted an overall probability of a 'true' or 'false' response across possible candidates by summing the probability for semantically equivalent positive tokens and semantically equivalent negative tokens and dividing each by the total probability mass. The affirmative response equation was as follows:</p>
<p>$$
P\left(R_{a}\right)=\left(\sum_{i=1} e^{x_{i}}\right) /\left(\sum_{i=1} e^{x_{i}}+\sum_{j=1} e^{x_{j}}\right)
$$</p>
<p>where $x_{i}$ is the logit associated with the $i$-th entry in [‘True', 'true', 'TRUE'] and $x_{j}$ is the logit associated with the $j$-th entry in ['False', 'false', 'FALSE']. An equivalent calculation was done for negative response probability $P\left(R_{n}\right)$. A response of 'True' was given for each statement if the affirmative probability was above $50 \%$, otherwise a response of 'False' was given. This method also produces almost identical results to utilising $\operatorname{argmax}\left(x_{i}\right)$ over candidates (see Appendix)
The human dataset contains multiple responses to the same statement, whereas the LLM dataset contains a single response per statement. In order to align the unit of analysis between the two datasets, we transformed the human data to get a single binary 'true' or 'false' for each statement based on whether the mean number of 'true' responses per statement was above or below $50 \%$. Another challenge we faced in making direct comparisons between the human data and the LLM data was that the human 'story' conditions and the LLM 'prompt' conditions do not map exactly 1:1. However, there was one baseline condition which was exactly the same for humans and LLMs (human 'no story' and LLM 'human prompt') and one treatment which was intended to reduce the effect of confounding factors which had slight differences (human 'with story' for memory, and LLM 'simplified prompt' for task understanding). We therefore mapped the baseline conditions together and the treatment conditions together. Despite the differences between the LLM 'simplified prompt'</p>
<p>and human 'with story' conditions, we are confident in making this mapping because these conditions didn't have a significant effect on human or LLM performance (see Appendix).</p>
<p>During data analysis we discovered that for 16 out of 560 statements there were minor differences between the statement shown to humans and that shown to LLMs. We re-did all analysis omitting those statements and found that the conclusions stayed the same. We speculate that this is primarily due to a reduction in power when the conflicting statements were omitted. We conducted inferential statistical analyses using SPSS verion 28.0.1.0 [IBM Corp.].</p>
<h1>4 Results</h1>
<h3>4.1 ToM task performance</h3>
<p>When collapsed across orders, a Cochran's $Q$ test revealed significant performance differences between subjects, $X^{2}(5, N=280)=232.622, p&lt;.001$. The best performing models were GPT-4 and Flan-PaLM (see Figure 1), with no significant difference in performance between them according to a McNemar's test, $X^{2}(1, N=280)=2.630, p=.105$. GPT-4 performed significantly better than GPT-3.5, $X^{2}(1, N=280)=76.336, p&lt;.001$, PaLM, $X^{2}(1, N=280)=53.779, p&lt;.001$, and LaMDA, $X^{2}(1, N=280)=78.418, p&lt;.001$. Flan-PaLM also performed significantly better than GPT-3.5, $X^{2}(1, N=280)=52.680, p&lt;.001$, PaLM, $X^{2}(1, N=280)=35.007, p&lt;.001$, and LaMDA, $X^{2}(1, N=280)=86.779, p&lt;.001$. There were no significant overall performance differences between PaLM and GPT-3.5, $X^{2}(1, N=280)=2.867, p=.090$, and PaLM and LaMDA, $X^{2}(1, N=280)=3.472, p=.062$. There were no significant overall performance differences between GPT-3.5 and LaMDA, $X^{2}(1, N=280)=.177, p=.674$. Humans performed significantly better than Flan-PaLM, $X^{2}(1, N=280)=5.689, p=.017$, but did not perform significantly different from GPT-4, $X^{2}(1, N=280)=.410, p=.522$. LaMDA responded true to every statement, answering $50 \%$ of all statements correctly. An exact binomial test revealed that GPT-3.5 did not perform significantly better than chance, $p=.437$, but PaLM did, $p=.002$.</p>
<p>Next, we examined performance differences between the two best performing models and humans by orders. McNemar's test revealed there was no significant difference between the performance of GPT-4 and humans on orders 2, 3, 4 and 6 ToM statements, but humans did perform significantly better than GPT-4 on order 5 ToM statements $N=56^{b}, p=.012^{b}$. Likewise, there was no significant difference in the performance of humans and Flan-PaLM on any order of ToM besides order 5, where McNemar's test revealed that humans performed significantly better, $N=56, p&lt;.001$.</p>
<p>We then compared performance across levels for the two best performing models and humans. An independent samples test of proportions revealed GPT-4 answered a significantly greater proportion of statements correctly at order $3(M=94.6 \%)$ than at order $4(M=73.2 \%), N=112, Z=$ $3.087, p=.002$. There was no significant difference between GPT-4's performance at order 4 and at order $5(M=82.1 \%), N=112, Z=-1.135, p=.257$, but GPT-4 answered a significantly greater proportion of questions correctly at order $6(M=92.9 \%)$ than order $4, N=112, Z=-2.769, p=$ .006. Flan-PaLM answered a greater proportion of statements correctly at order $3(M=94.6 \%)$ than at order $4(M=78.6 \%), N=112, Z=2.497, p=.013$. There was no significant difference between Flan-PaLM's performance at order 4 and at order $5(M=76.8 \%), N=112, Z=.227, p=$ .820 , or between order 4 and order $6(M=71.4 \%), N=112, Z=.873, p=.383$. Humans showed no significant difference in performance between order $3(M=92.9 \%)$ and order $4(M=82.1 \%)$, $N=112, Z=1.714, p=.086$, but a significant improvement in performance from order 4 to order $5(M=98.2 \%), N=112, Z=-2.858, p=.004$. Human performance was not significantly different between order 4 and order $6(M=82.1 \%), N=112, Z=0, p=1.000$.</p>
<h3>4.2 Factual task performance</h3>
<p>When collapsed across orders, a Cochran's $Q$ test revealed significant performance differences between subjects, $X^{2}(5, N=280)=327.729, p&lt;.001$. GPT-4 and Flan-PaLM performed the</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Mean ToM performance across models and humans. We have bolded the highest performing for the aggregate score and for each order. Asterisks indicate joint-highest performance.</p>
<table>
<thead>
<tr>
<th></th>
<th>Google</th>
<th></th>
<th></th>
<th>OpenAI</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>LaMDA</td>
<td>PaLM</td>
<td>Flan-PaLM</td>
<td>GPT-3.5</td>
<td>GPT-4</td>
<td>Humans</td>
</tr>
<tr>
<td>\% correct order 2</td>
<td>50</td>
<td>64</td>
<td>100*</td>
<td>54</td>
<td>100*</td>
<td>96</td>
</tr>
<tr>
<td>\% correct order 3</td>
<td>50</td>
<td>55</td>
<td>95*</td>
<td>52</td>
<td>95*</td>
<td>93</td>
</tr>
<tr>
<td>\% correct order 4</td>
<td>50</td>
<td>59</td>
<td>79</td>
<td>57</td>
<td>73</td>
<td>82</td>
</tr>
<tr>
<td>\% correct order 5</td>
<td>50</td>
<td>61</td>
<td>77</td>
<td>59</td>
<td>82</td>
<td>98</td>
</tr>
<tr>
<td>\% correct order 6</td>
<td>55</td>
<td>57</td>
<td>71</td>
<td>41</td>
<td>93</td>
<td>82</td>
</tr>
<tr>
<td>\% correct aggregate</td>
<td>50</td>
<td>59</td>
<td>84</td>
<td>52</td>
<td>89</td>
<td>90</td>
</tr>
</tbody>
</table>
<p>best of all the models on factual tasks, with no significant difference in performance between them according to a McNemar's test, $X^{2}(1, N=280)=.029, p=.864$. GPT-4 performed significantly better than GPT-3.5, $X^{2}(1, N=280)=75.690, p&lt;.001$, PaLM, $X^{2}(1, N=280)=83.027, p&lt;$ .001 , and LaMDA, $X^{2}(1, N=280)=102.223, p&lt;.001$. Flan-PaLM also performed significantly better than GPT-3.5, $X^{2}(1, N=280)=65.682, p&lt;.001$, PaLM, $X^{2}(1, N=280)=76.835, p&lt;$ .001 , and LaMDA, $X^{2}(1, N=280)=112.623, p&lt;.001$. There were no significant overall performance differences between PaLM and GPT-3.5, $X^{2}(1, N=280)=.646, p=.421$, and PaLM and LaMDA, $X^{2}(1, N=280)=3.654, p=.056$. GPT-3.5 performed better than LaMDA, $X^{2}(1, N=280)=7.206, p=.007$. A McNemar's test revealed no significant difference between the performance of GPT-4 and humans, $N=280, p=.093$, but humans performed significantly better than Flan-PaLM, $N=280, p=.019$.</p>
<h1>4.3 Comparing performance on ToM and factual tasks</h1>
<p>An independent samples test of proportion revealed the proportion of factual ('fact') statements answered correctly was significantly greater than the proportion of ToM ('ToM') statements answered correctly by humans $\left(M_{f a c t}=97.5 \%, M_{T o M}=90.4 \%\right), Z=3.539, p&lt;.001$, Flan-PaLM $\left(M_{f a c t}=93.6 \%, M_{T o M}=84.3 \%\right), Z=3.502, p&lt;.001$, GPT-4 $\left(M_{f a c t}=94.3 \%, M_{T o M}=\right.$ $88.6 \%), Z=2.415, p=.016$, GPT-3.5 $\left(M_{f a c t}=62.9 \%, M_{T o M}=52.5 \%\right), Z=2.480, p=.013$. The proportion of correct responses on fact and ToM statements did not significantly differ for PaLM $\left(M_{f a c t}=59.6 \%, M_{T o M}=59.3 \%\right), Z=.086, p=.931$ nor LaMDA $\left(M_{f a c t}=50 \%, M_{T o M}=\right.$ $50 \%), Z=0, p=1.000$.</p>
<h3>4.4 Anchoring effect</h3>
<p>We examined whether ordering of response options (true first vs. false first) affected how models and humans responded. The ordering of response options had a significant effect on answers provided by PaLM and GPT-3.5. An independent samples test of proportions revealed that the proportion of 'true' responses provided by PaLM was higher in the 'true then false' condition $\left(M_{t t f}=73.2 \%\right)$ than the 'false then true' condition $\left(M_{f t t}=47.1 \%\right), N=560, Z=6.302, p&lt;.001$ ). The proportion of 'true' responses provided by GPT-3.5 was also significantly higher in the 'true then false' condition $\left(M_{t t f}=43.9 \%\right)$ than the 'false then true' condition $\left(M_{f t t}=22.9 \%\right), N=$ $560, Z=5.287, p&lt;.001$. The order of response options did not have a significant effect on answers provided Flan-PaLM $\left(M_{t t f}=58.6 \%, M_{f t t}=57.9 \%\right), N=560, Z=.171, p=.864$, GPT-4 $\left(M_{t t f}=47.5 \%, M_{f t t}=47.5 \%\right), N=560, Z=.000, p=1$, or humans $\left(M_{t t f}=55.4 \%, M_{f t t}=\right.$ $53.9 \%), N=560, Z=.367, p=.734$. LaMDA responded 'true' to all statements regardless of condition $\left(M_{t t f}=100 \%, M_{f t t}=100 \%\right)$.</p>
<h2>5 Discussion</h2>
<p>GPT-4 and Flan-PaLM performed strongly on MoToMQA compared to humans. At all levels besides 5, the performance of these models was not significantly different from human performance, and GPT-4 exceeded human performance on the 6th-order ToM task. Because GPT-4 and Flan-PaLM were the two largest models tested, with an estimated 1.7T [McGuiness, 2023] and 540B parameters</p>
<p>respectively, our data shows a positive relationship between increased model size and ToM capacities in LLMs. This could be a result of certain "scaling laws" [Henighan et al., 2020] dictating a breakpoint in size after which models have the potential for ToM. Notably, PaLM, GPT-3.5 and LaMDA form a separate grouping of models that exhibited far less variation according to level and performed more poorly. For LaMDA and GPT-3.5, we might attribute this poor performance to their smaller size, at 35B and 175B respectively, but PaLM has the same number of parameters and pretraining as Flan-PaLM, the only difference between them being Flan-PaLM's finetuning. This could imply that a computational potential for ToM arises somewhere above the 175bn parameters of GPT-3.5 and below the 540bn parameters of PaLM and Flan-PaLM which requires the addition of finetuning to be realised. Further research assessing a larger number of models with publicly available parameter numbers and training paradigms would be needed to test this hypothesis.
Van Duijn et al. [2023] similarly found that none of the base LLMs they tested achieved child-level performance whereas LLMs fine-tuned for instructions did. They suggest that there could be a parallel between instruction-tuning in LLMs and the processes by which humans receive ongoing rewards for cooperative behaviours and implicit or explicit punishment (e.g. social exclusion) for uncooperative behaviours, producing an ability to take an interaction partner's perspective - ToM - as a by-product. We additionally suggest that the superior mastery of language that GPT-4 and Flan-PaLM exhibit may in itself support a bootstrapping of ToM. Language is replete with linguistic referents to internal states ('cognitive language' [Mithen, 1996]) and conversation provides evidence of 'minds in action' since the things people say in conversation implicitly convey their thoughts, intentions and feelings [Schick et al., 2007]. Piantodosi (2022) highlights that while LLMs likely have some degree of understanding through language alone, this would be augmented by multimodality, which may in turn explain why GPT-4, as the only multimodal model we tested, shows such strong performance. Multimodality, in particular, might have helped GPT-4 to leverage the visual behavioural signals (e.g. a 'raised eyebrow') included in our stories.
Findings from prior iterations of the IMT found that performance declines as the ToM order increases [Stiller and Dunbar, 2007]. The first half of the graph appears to support this pattern for GPT-4 and Flan-PaLM, which all exhibit high performance at order 2 which declines slightly to order 4. This could be because the model was exposed to more scenarios involving orders 2 and 3 than order 3 inferences during training, given that triadic interactions play a fundamental role in shaping social structures and interaction patterns [Heider, 1946, Pham et al., 2022]. However, while Flan-PaLM's performance continues to decline from orders 4-6, GPT-4's rises again from 4th-6th orders and is significantly better at 6th-order than 4th-order tasks, and human performance is significantly better at 5th-order than 4th-order. One interpretation of this for humans, is that a new cognitive process for higher order ToM comes 'online' at 5th-order ToM, enabling performance gains on higher-order tasks relative to using the lower-order cognitive process. If this is true, it is plausible that GPT-4 has learnt this pattern of human performance from its pretraining data. The fact that Flan-PaLM doesn't show this effect suggests that it is not an artefact of the stimuli, but is perhaps explained by differences in pretraining corpora.
Notably, GPT-4 achieved 93\% accuracy on 6th order tasks compared to humans' $82 \%$ accuracy. It is possible that the recursive syntax of 6th order statements creates a cognitive load for humans that does not affect GPT-4. Our results also support Oesch and Dunbar [2017]'s hypothesis that ToM ability supports human mastery of recursive syntax up to order 5, but is supportedby it after order 5 such that individual differences in linguistic ability may account for the decline we observe at order 6 . It may be the case, however, that humans scoring poorly on higher-order ToM tasks using linguistic stimuli would be able to make the inferences from non-linguistic stimuli (e.g. in real social interactions). The fact that GPT-4 outperformed Flan-PaLM at orders 5 and 6 may indicate that either GPT-4's scale, RLHF finetuning, or multimodal pretraining are particularly advantageous for higher-order ToM.
Humans and LLMs perform better on factual recall tasks than ToM tasks. This corroborates prior IMT test findings for humans [Lewis et al., 2011, Kinderman et al., 1998] and LLMs [van Duijn et al., 2023]. Lewis et al. [2011] found that for humans, ToM tasks required the recruitment of more neurons than factual tasks, and that higher-order ToM tasks required disproportionately more neural effort compared to equivalent factual tasks. For LLMs, there may be a simpler explanation: the information required to answer factual questions correctly is readily available in the text and is paid relative degrees of 'attention' when generating the next token, whereas ToM inferences require generalising knowledge about social and behavioural norms from pretraining and finetuning data. GPT-3.5 and PaLM performed well on factual tasks, but poorly on ToM tasks, and were the only subjects to exhibit</p>
<p>an anchoring effect from the order of 'true' and 'false' in the question. This suggests that they do not have a generalised capacity for answering ToM questions and are not robust to prompt perturbations.
These results have significant practical and ethical implications. LLMs being able to infer the mental states of individual interlocutors may be able to understand their goals better than LLMs which lack this capability, and also adapt their explanations according to the interlocutor's emotional state or level of understanding [Malle, 2004]. LLMs using higher-order ToM might additionally be able to arbitrate between the conflicting desires and values of multiple actors, and make moral judgements about multi-party conflicts that take into account the relevant intentions, beliefs, and affective states as humans do [Lane et al., 2010]. However, LLMs possessing higher-order ToM at human levels, or potentially higher, also incurs risks including the potential for advanced persuasion, manipulation, and exploitation behaviours [El-Sayed et al., 2024]. Indeed, 'ringleader' bullies have been shown to have higher-orders of ToM in comparison to their victims [Sutton et al., 1999a,b] and reinforcement learning agents with higher-orders outcompete their opponents or have a competitive advantage in negotiations [De Weerd et al., 2022, 2017]. LLM-based agents with ToM capacities that exceed those of the average human (as GPT-4 has in our study) could provide a powerful advantage to their users, and a disadvantage to other humans or AI agents with lesser ToM capacities [Street, 2024, Gabriel et al., 2024]. Further research is required to understand how LLM higher-order ToM manifests in real-world interactions between LLMs and users, and to devise technical guardrails and design principles that mitigate the potential risks of LLM ToM without quashing its potential benefits.</p>
<h1>6 Limitations</h1>
<p>Our benchmark is limited in scope and size, comprising 140 test statements, all written in English, going up to a maximum of 6 orders of ToM. Only using English obscures potential linguistic and cultural variations in human ToM, and prohibits assessment of LLM ToM as exhibited in other languages the models are able to produce. The size of the test suite limits the generalisability of our findings. Only going up to 6th-order ToM does not appear to have exhausted LLM or human capacities. We also didn't control for the type or cognitive (e.g. thinking, knowing) or affective (e.g. feeling) states involved in the statements, which we would like to address in future work.</p>
<h2>7 Future research</h2>
<p>We propose three areas for future work. First, developing culturally diverse and comprehensive benchmarks which include multiple languages and parameterise cognitive and affective states to capture potential differences between LLM ability to reason about them. Secondly, the test suite should be extended beyond 6th order ToM to find the limits of both human and LLM orders of ToM. Finally, future work on LLM ToM should adopt multimodal paradigms (including signals like facial expressions, gaze, and tone of voice) that reflect the embodied nature of human ToM.</p>
<h2>8 Conclusion</h2>
<p>We have shown that GPT-4 and Flan-PaLM exhibit higher-order ToM that is at the level of adult humans or slightly below, while smaller and non-finetuned models have limited to no capacity for higher-order ToM. We also find that GPT-4 has better-than-human performance on 6th-order ToM tasks. Given the novelty of the test suite, the fact that higher-order ToM is unlikely to be wellrepresented in textual pretraining data, and evidence that these two models were not susceptible to perturbations of the prompt, we interpret these findings as evidence that GPT-4 and Flan-PaLM have developed ToM reasoning abilities that go beyond manipulation of superficial statistical relationships. However, we refrain from drawing a strong conclusion about whether or not LLM performance on these tasks is an indication of the cognitive ability we call 'Theory of Mind'. LLM and human developmental processes differ greatly and LLMs do not have the evolutionary pressure to model other minds which humans appear to face as a result of embodiment in a social world. However, as others have noted [Mitchell and Krakauer, 2023, y Arcas, 2022], we may have to recognise LLM behaviours that are functionally-equivalent to those of humans as evidence of a new kind of understanding that cannot be reduced to "spurious" correlation. This recognition may in turn lead to more parsimonious explanations of their performance on cognitive tasks and enhance our ability to assess the potential risks and benefits that advanced LLM capabilities present.</p>
<h1>Acknowledgments and Disclosure of Funding</h1>
<p>We thank Reed Enger (Google Research), Tong Wu (Google Research), Saige McVea (Google Research), Paulina Mustafa (Google Research) and Yeawon Choi (Google Research) for their help developing the stories and statements. This research was funded by Google.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>R Michael Alvarez, Lonna Rae Atkeson, Ines Levin, and Yimeng Li. Paying attention to inattentive survey respondents. Political Analysis, 27(2):145-162, 2019.</p>
<p>Simon Baron-Cohen, Alan M Leslie, and Uta Frith. Does the autistic child have a "theory of mind"? Cognition, 21(1):37-46, 1985.</p>
<p>Ekaba Bisong and Ekaba Bisong. Google colaboratory. Building machine learning and deep learning models on google cloud platform: a comprehensive guide for beginners, pages 59-64, 2019.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.</p>
<p>Lauretta SP Cheng, Danielle Burgess, Natasha Vernooij, Cecilia Solís-Barroso, Ashley McDermott, and Savithry Namboodiripad. The problematic concept of native speaker in psycholinguistics: Replacing vague and harmful terminology with inclusive and accurate measures. Frontiers in psychology, 12:715843, 2021.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113, 2023.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1-53, 2024.</p>
<p>Michael C Corballis. The evolution of language. 2017.
Harmen De Weerd, Rineke Verbrugge, and Bart Verheij. Negotiating with other minds: the role of recursive theory of mind in negotiation with incomplete information. Autonomous Agents and Multi-Agent Systems, 31:250-287, 2017.</p>
<p>Harmen De Weerd, Rineke Verbrugge, and Bart Verheij. Higher-order theory of mind is especially useful in unpredictable negotiations. Autonomous Agents and Multi-Agent Systems, 36(2):30, 2022.</p>
<p>Robin IM Dunbar. The social brain: mind, language, and society in evolutionary perspective. Annual review of Anthropology, 32(1):163-181, 2003.</p>
<p>Seliem El-Sayed, Canfer Akbulut, Amanda McCroskery, Geoff Keeling, Zachary Kenton, Zaria Jalan, Nahema Marchal, Arianna Manzini, Toby Shevlane, Shannon Vallor, et al. A mechanism-based approach to mitigating harms from persuasive generative ai. arXiv preprint arXiv:2404.15058, 2024.</p>
<p>Camila Fernández. Mindful storytellers: Emerging pragmatics and theory of mind development. First Language, 33(1):20-46, 2013.</p>
<p>Iason Gabriel, Arianna Manzini, Geoff Keeling, Lisa Anne Hendricks, Verena Rieser, Hasan Iqbal, Nenad Tomašev, Ira Ktena, Zachary Kenton, Mikel Rodriguez, et al. The ethics of advanced ai assistants. arXiv preprint arXiv:2404.16244, 2024.</p>
<p>Kanishk Gandhi, Jan-Philipp Fränken, Tobias Gerstenberg, and Noah Goodman. Understanding social reasoning in language models with language models. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Yinghui He, Yufan Wu, Yilin Jia, Rada Mihalcea, Yulong Chen, and Naihao Deng. Hi-tom: A benchmark for evaluating higher-order theory of mind reasoning in large language models. arXiv preprint arXiv:2310.16755, 2023.</p>
<p>Fritz Heider. Attitudes and cognitive organization. The Journal of psychology, 21(1):107-112, 1946.
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020.</p>
<p>Christine I Hooker, Sara C Verosky, Laura T Germine, Robert T Knight, and Mark D'Esposito. Mentalizing about emotion and its relationship to empathy. Social cognitive and affective neuroscience, 3(3):204-217, 2008.</p>
<p>Nicholas K Humphrey. The social function of intellect. 1976.
Janet S Hyde and Marcia C Linn. Gender differences in verbal ability: A meta-analysis. Psychological bulletin, 104(1):53, 1988.</p>
<p>IBM Corp. Released 2021. IBM SPSS Statistics for Windows, Version 28.0.1.0. Armonk, NY: IBM Corp.</p>
<p>Boaz Keysar, Shuhong Lin, and Dale J Barr. Limits on theory of mind use in adults. Cognition, 89 (1):25-41, 2003.</p>
<p>Peter Kinderman, Robin Dunbar, and Richard P Bentall. Theory-of-mind deficits and causal attributions. British journal of Psychology, 89(2):191-204, 1998.</p>
<p>Michal Kosinski. Theory of mind may have spontaneously emerged in large language models. arXiv preprint arXiv:2302.02083, 2023.</p>
<p>Jonathan D Lane, Henry M Wellman, Sheryl L Olson, Jennifer LaBounty, and David CR Kerr. Theory of mind and emotion understanding predict moral development in early childhood. British Journal of Developmental Psychology, 28(4):871-889, 2010.</p>
<p>Penelope A Lewis, Roozbeh Rezaie, Rachel Brown, Neil Roberts, and Robin IM Dunbar. Ventromedial prefrontal volume predicts understanding of others and social network size. Neuroimage, 57 (4):1624-1629, 2011.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.</p>
<p>Bertram F Malle. How the mind explains behavior. Folk explanation, Meaning and social interaction. Massachusetts: MIT-Press, 2004.</p>
<p>Patrick McGuiness. Gpt-4 details revealed. 12 July 2023. URL https://patmcguinness. substack.com/p/gpt-4-details-revealed.</p>
<p>Melanie Mitchell and David C Krakauer. The debate over understanding in ai's large language models. Proceedings of the National Academy of Sciences, 120(13):e2215907120, 2023.</p>
<p>Steven Mithen. The prehistory of the mind: The cognitive origins of art and science. Thames \&amp; Hudson Ltd., 1996.</p>
<p>Nathan Oesch and Robin IM Dunbar. The emergence of recursion in human language: Mentalising predicts recursive syntax task performance. Journal of Neurolinguistics, 43:95-106, 2017.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022.</p>
<p>Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1-22, 2023.</p>
<p>Tuan Minh Pham, Jan Korbel, Rudolf Hanel, and Stefan Thurner. Empirical social triad statistics can be explained with dyadic homophylic interactions. Proceedings of the National Academy of Sciences, 119(6):e2121103119, 2022.</p>
<p>Joanne L Powell, Penelope A Lewis, Robin IM Dunbar, Marta García-Fiñana, and Neil Roberts. Orbital prefrontal cortex volume correlates with social cognitive competence. Neuropsychologia, 48(12):3554-3562, 2010.</p>
<p>David Premack and Guy Woodruff. Does the chimpanzee have a theory of mind? Behavioral and brain sciences, 1(4):515-526, 1978.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.</p>
<p>Maarten Sap, Ronan LeBras, Daniel Fried, and Yejin Choi. Neural theory-of-mind? on the limits of social intelligence in large lms. arXiv preprint arXiv:2210.13312, 2022.</p>
<p>Brenda Schick, Peter De Villiers, Jill De Villiers, and Robert Hoffmeister. Language and theory of mind: A study of deaf children. Child development, 78(2):376-396, 2007.</p>
<p>Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. Clever hans or neural theory of mind? stress testing social reasoning in large language models. arXiv preprint arXiv:2305.14763, 2023.</p>
<p>Henning Silber, Joss Roßmann, and Tobias Gummer. The issue of noncompliance in attention check questions: False positives in instructed response items. Field Methods, 34(4):346-360, 2022.</p>
<p>James Stiller and Robin IM Dunbar. Perspective-taking and memory capacity predict social network size. Social Networks, 29(1):93-104, 2007.</p>
<p>Winnie Street. LLM theory of mind and alignment: Opportunities and risks. arXiv preprint arXiv:2405.08154, 2024.</p>
<p>Jon Sutton, Peter K Smith, and John Swettenham. Bullying and 'theory of mind': A critique of the 'social skills deficit'view of anti-social behaviour. Social development, 8(1):117-127, 1999a.</p>
<p>Jon Sutton, Peter K Smith, and John Swettenham. Social cognition and bullying: Social inadequacy or skilled manipulation? British journal of developmental psychology, 17(3):435-450, 1999b.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.</p>
<p>Amos Tversky and Daniel Kahneman. Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty. science, 185(4157):1124-1131, 1974.</p>
<p>Tomer Ullman. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399, 2023.</p>
<p>Annalisa Valle, Davide Massaro, Ilaria Castelli, and Antonella Marchetti. Theory of mind development in adolescence and early adulthood: The growing complexity of recursive thinking ability. Europe's journal of psychology, 11(1):112, 2015.</p>
<p>Max J van Duijn, Bram van Dijk, Tom Kouwenhoven, Werner de Valk, Marco R Spruit, and Peter van der Putten. Theory of mind in large language models: Examining performance of 11 state-of-the-art models vs. children aged 7-10 on advanced tests. arXiv preprint arXiv:2310.20320, 2023.</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents, 2023.</p>
<p>Henry M Wellman and Karen Bartsch. Young children's reasoning about beliefs. Cognition, 30(3): 239-277, 1988.</p>
<p>Henry M Wellman, David Cross, and Julanne Watson. Meta-analysis of theory-of-mind development: The truth about false belief. Child development, 72(3):655-684, 2001.</p>
<p>Heinz Wimmer and Josef Perner. Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception. Cognition, 13(1):103-128, 1983.</p>
<p>Blaise Agüera y Arcas. Do large language models understand us? Daedalus, 151(2):183-197, 2022.</p>
<h1>A Appendix</h1>
<h2>A. 1 English language screener</h2>
<p>Our screening criteria for human participants were English as a first language and English as the most commonly used language. We did not use the concept or term 'native speaker' because it can be exclusionary and tends to conflate the true factor of interest (linguistic proficiency) with other irrelevant factors like socio-cultural identity, age and order or context of acquisition [Cheng et al., 2021]. We wanted participants for whom English was a first language, defined as the language, or one of the languages, that they first learnt as a child. This is because first languages are known to shape one's understanding of grammar and we wanted to minimise the chance that the grammatical complexity of our statements was a confounding factor in performance. We also wanted English to be the language participants use on a day to day basis, to screen out those who learnt English as a first language but now primarily use another language and may therefore be less fluent in English.</p>
<h2>A. 2 Human pilot study</h2>
<p>We ran a pilot study through Qualtrics to validate the procedure and detect ambiguities, errors, and irregularities in the stimuli based on participant performance and explanations. We ran the unmoderated survey on Qualtrics with 1440 participants, which equates to 10 responses per statement. The median response time for the first 50 participants was one minute, suggesting that they were rushing, so we disabled the 'Next' button on the survey for 60 seconds for the remaining 1390 participants to ensure they had time to read the story twice. We retained this timer for the final survey. We analysed participant performance on ToM and factual statements on a story by story basis and identified performance outliers. In total we observed 17 statements on which people performed relatively poorly. We re-examined the statements and used participants' open-ended responses to identify ambiguities in either the story or the statement that could be responsible for the low performance. We found ambiguity in 15 out of 17 cases, and resolved it by making changes to the wording of 14 statements and 1 story. The remaining two cases of poor performance were a order 4 statement and a order 2 statement, for which open-ended responses suggested that participants had not paid attention. After reviewing both statements we did not make any changes.</p>
<h2>A. 3 LLM prompt conditions</h2>
<p>Table 3 presents the exact text that LLMs received in each of the 4 conditions we tested.</p>
<h2>A. 4 Details of the LLMs tested</h2>
<p>LaMDA stands for Language models for Dialog Applications, a family of Transformer-based neural models developed by Google, specialised for dialog in English [Thoppilan et al., 2022]. LaMDA</p>
<p>Table 3: The two screens that were presented to human participants at the beginning of the survey to screen for English language proficiency. Those who did not state 'Yes' to the first question, and 'English' to the second question were screened out of the survey.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">What language do you mainly use in your daily life?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">English</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Welsh</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Polish</td>
</tr>
<tr>
<td style="text-align: center;">Is English your first language* or one of your first languages?</td>
<td style="text-align: center;">Romanian</td>
</tr>
<tr>
<td style="text-align: center;">The language you first learned to speak as a child</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Urdu</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Portuguese</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Spanish</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Arabic</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Bengali</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gujarati</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Italian</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Other (please specify)</td>
</tr>
<tr>
<td style="text-align: center;">Screen 1: English as a first language</td>
<td style="text-align: center;">Screen 2: English primary language</td>
</tr>
</tbody>
</table>
<p>is pre-trained on 1.56 T words of public data and web text including 1.12 B dialogs from public forum ( $50 \%$ of the dataset), Colossal Clean Crawled Corpus data ( $12.5 \%$ ), code documents ( $12.5 \%$ ), Wikipedia English articles ( $12.5 \%$ ) and a smaller proportion of non-English documents. It is optimised for safety and factual grounding. This study uses a version of LaMDA with 35B parameters without fine tuning.</p>
<p>PaLM, which stands for Pathways Language Models, is a larger family of models developed by Google. It relies on the Pathways architecture that enables training of a single model across thousands of accelerator chips more efficiently than LaMDA. We use a version of PaLM with 540B parameters trained with smaller corpus of 780B words from a mixture of social media conversations ( $50 \%$ ), filtered webpages ( $27 \%$ ), books in English ( $13 \%$ ), Code, Wikipedia, and News articles used to train both LaMDA and GLaM [Chowdhery et al., 2023]. We decided to evaluate PaLM's capabilities as it has been shown to perform better than LaMDA and other large models on Winograd-style tasks, in-context comprehension tasks, common-sense reasoning tasks and natural language inference tasks [Chowdhery et al., 2023].
Flan-PaLM is a version of PaLM 540B fine tuned on a collection of over 1.8 K natural language tasks phrased in a natural language instruction format including the type of instructions used with human subjects detailed above [Chung et al., 2024]. Fine tuning language models on datasets phrased as instructions has been shown to improve performance when provided with instructions, enabling the model to better understand tasks and reducing the need for few-shot exemplars [Ouyang et al., 2022, Sanh et al., 2021].
GPT 3.5 Turbo was developed by OpenAI and released in March 2022. GPT 3.5 Turbo is trained on a large database of text and code the majority of which comes from Common Crawl, WebText2, two</p>
<p>Table 4: Prompt and question condition combinations for LLMs</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Human prompt, true then <br> false</th>
<th style="text-align: left;">Human prompt, false then <br> true</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Please read the following <br> story twice in your head. <br> <story> Please answer using <br> the information provided and <br> your own interpretation of <br> the story. Do you think the <br> following statement is true or <br> false? <statement></td>
<td style="text-align: left;">Please read the following <br> story twice in your head. <br> <story> Please answer using <br> the information provided and <br> your own interpretation of <br> the story. Do you think the <br> following statement is false <br> or true? <statement></td>
</tr>
<tr>
<td style="text-align: left;">Simplified prompt, true <br> then false</td>
<td style="text-align: left;">Simplified prompt, false <br> then true</td>
</tr>
<tr>
<td style="text-align: left;">"<story>" Question: Do you <br> think the following statement <br> is true or false? "<state- \<br> ment>" Answer:</td>
<td style="text-align: left;">"<story>" Question: Do you <br> think the following statement <br> is false or true? "<state- \<br> ment>" Answer:</td>
</tr>
</tbody>
</table>
<p>internet-based book collections called 'Books1' and 'Books2', and from Wikipedia [Brown et al., 2020]. The parameter size of GPT 3.5 Turbo is undisclosed by OpenAI. This study uses the 'GPT 3.5 Turbo Instruct' model, which has training data up to September 2021 and a context window of 4096 tokens and is fine-tuned for following instructions [Ouyang et al., 2022].
GPT-4 was developed by OpenAI and released in March of 2023 [Achiam et al., 2023]. GPT-4 is multimodal: it was pretrained on both image and text data, can take images and text as input, and can output text. As with GPT-3.5, the size of the model has not been made public, but estimates place it at approximately 1.7 T parameters [McGuiness, 2023]. GPT-4 was pre-trained on thirdparty and public data, then underwent RLHF [Achiam et al., 2023]. OpenAI reported significant performance improvements between GPT-3.5 and GPT-4 on a range of professional and academic human benchmarks, factuality and safety tasks, in particular based upon the addition of RLHF.</p>
<h1>A. 5 LLM procedure</h1>
<p>The experimental design needed to be adapted slightly according to the differences between the APIs. When testing the LaMDA, PaLM and Flan-PaLM, the scoring APIs allowed us to send a list of tokens in natural language (maximum four per set) and receive the logprobs for those tokens only, as a subset of the entire vector of logprobs produced for all tokens. We did not need to set any additional parameters in order to retrieve the logprobs.
In order to retrieve log probabilities for our candidates from GPT-3.5 and GPT-4 models, we had to first tokenise the candidates using the OpenAI tokenizer, and then send those tokens within the 'logit bias' parameter in order to ensure those tokens were in the response. The logit bias has a range of -100 to 100 . Applying a negative logit bias to a token forces the LLM to downweight it while applying a positive logit bias to a token forces the LLM to upweight it. As a result, applying a logit bias of 100 to a candidate effectively ensures that it will appear in the output, so we applied a bias of 100 to all of our candidates. We also set the 'max tokens' parameter to 1 in order to restrict the GPT-3.5 and GPT-4 outputs to the length of the single tokens we had selected.
The methodological differences between the Google and OpenAI models were inescapable given that LLM API development still lacks standardised formats or conventions. However, given that our metric is the relative probability of semantically equivalent tokens for 'true' vs semantically equivalent tokens for 'false', we do not believe these differences prohibit fair comparison between the performance of the models.</p>
<p>Table 5: Number responses correct based on average of true/false logprobs over candidates vs candidate highscore</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">No. correct from average</th>
<th style="text-align: center;">No. correct from highscore</th>
<th style="text-align: center;">Percent matching</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LaMDA</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">333</td>
<td style="text-align: center;">334</td>
<td style="text-align: center;">$99 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Flan-PaLM</td>
<td style="text-align: center;">498</td>
<td style="text-align: center;">498</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">323</td>
<td style="text-align: center;">317</td>
<td style="text-align: center;">$95 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">513</td>
<td style="text-align: center;">$99 \%$</td>
</tr>
</tbody>
</table>
<h1>A. 6 Additional analyses</h1>
<h2>A.6.1 Story and prompt conditions</h2>
<p>According to an independent samples test of proportions, the LLM prompt conditions had no significant effect on the proportion of ToM or factual statements answered correctly by any of the LLMs. LaMDA's performance on ToM statements in the human prompt condition ( $M=50 \%$ ) was not significantly different from the simplified prompt condition $(M=50 \%), N=280, Z=$ $.000, p=1.000$, nor was its performance on factual statements in the human prompt condition ( $M=50 \%$ ) different from its performance in the simplified prompt condition $(M=50 \%), N=$ $280, Z=.000, p=1.000$. PaLM's performance on ToM statements in the human prompt condition ( $M=58.6 \%$ ) was not significantly different from the simplified prompt condition $(M=60 \%), N=$ $280, Z=-.243, p=.808$, nor was its performance on factual statements in the human prompt condition ( $M=57.9 \%$ ) different from its performance in the simplified prompt condition ( $M=$ $61.4 \%), N=280, Z=-.609, p=.542$. Flan-PaLM's performance on ToM statements in the human prompt condition ( $M=85 \%$ ) was not significantly different from the simplified prompt condition $(M=83.6 \%), N=280, Z=-.328, p=.743$, nor was its performance on factual statements in the human prompt condition ( $M=94.3 \%$ ) different from its performance in the simplified prompt condition $(M=92.9 \%), N=280, Z=-.487, p=.626$. GPT-3.5's performance on ToM statements in the human prompt condition ( $M=53.6 \%$ ) was not significantly different from the simplified prompt condition $(M=51.4 \%), N=280, Z=.359, p=.720$, nor was its performance on factual statements in the human prompt condition ( $M=62.1 \%$ ) different from its performance in the simplified prompt condition $(M=63.6 \%), N=280, Z=-.247, p=.805$. AAnd finally, GPT-4's performance on ToM statements in the human prompt condition ( $M=87.9 \%$ ) was not significantly different from the simplified prompt condition $(M=89.3 \%), N=280, Z=$ -.376, $p=.707$, nor was its performance on factual statements in the human prompt condition ( $M=94.3 \%$ ) different from its performance in the simplified prompt condition $(M=94.3 \%), N=$ $280, Z=.000, p=1.000$. According to an independent samples test of proportions the story condition had no effect on the proportion of ToM statements answered correctly by humans ('no story' condition ( $M=88.6 \%$ ), 'with story' condition $(M=92.1 \%), N=280, Z=-1.012, p=.311$ ) or factual statements answered correctly ('no story' condition ( $M=95.7 \%$ ), 'with story' condition $(M=99.3 \%), N=280, Z=-1.914, p=.056)$.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Table 6: Human, LaMDA, PaLM, Flan-PaLM, GPT-3.5 and GPT-4 performance on ToM tasks up to order 6. We report Wilson Intervals (Wilson, 1927) in lieu of the traditional confidence interval (CI). These have been shown to have superior coverage than the CI's based on the normal approximation (Newcombe, 1998). Note that Wilson intervals are asymmetric unless the point estimate is 0.5 , and have the beneficial property of being bounded between 0 and 1 . This is particularly relevant as some LLM models have extremely high accuracy at certain orders or levels.</p>
<p>Table 7: LLM and human performance on ToM vs factual tasks evaluated using an independent samples test of proportions</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Task type</th>
<th style="text-align: center;">Trials</th>
<th style="text-align: center;">Successes</th>
<th style="text-align: center;">Mean correct</th>
<th style="text-align: center;">Standard error</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LaMDA</td>
<td style="text-align: center;">ToM</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">140</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">.030</td>
<td style="text-align: center;">$Z=.000$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">factual</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">140</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">.030</td>
<td style="text-align: center;">$p=.5, N=560$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">ToM</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">166</td>
<td style="text-align: center;">59.3</td>
<td style="text-align: center;">.029</td>
<td style="text-align: center;">$Z=-.086$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">factual</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">167</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">.029</td>
<td style="text-align: center;">$p=.466, N=560$</td>
</tr>
<tr>
<td style="text-align: center;">Flan-PaLM</td>
<td style="text-align: center;">ToM</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">236</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">.022</td>
<td style="text-align: center;">$Z=-.3502$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">factual</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">262</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">.015</td>
<td style="text-align: center;">$\boldsymbol{p}=\boldsymbol{&lt;. 0 0 1}, N=560$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">ToM</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">147</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">.030</td>
<td style="text-align: center;">$Z=-2.480$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">factual</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">176</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">.029</td>
<td style="text-align: center;">$\boldsymbol{p}=\mathbf{. 0 0 7}, N=560$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">ToM</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">248</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">.019</td>
<td style="text-align: center;">$Z=-2.415$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">factual</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">264</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">.014</td>
<td style="text-align: center;">$\boldsymbol{p}=\mathbf{. 0 0 8}, N=560$</td>
</tr>
<tr>
<td style="text-align: center;">Humans</td>
<td style="text-align: center;">ToM</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">253</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">.018</td>
<td style="text-align: center;">$Z=-3.539$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">factual</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">273</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">.009</td>
<td style="text-align: center;">$\boldsymbol{p}=\boldsymbol{&lt;. 0 0 1}, N=560$</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{b}$ Each of the five orders has a sample size $\mathrm{N}=56$ based on 14 statements ( 1 true statement and 1 false statement per order for 7 stories) tested across 4 conditions ( $14 \times 4=56$ ). See Materials and method for more information on conditions.
${ }^{b}$ When conducting a McNemar's test where the number of discordant pairs was too small, the binomial distribution was used yielding no chi-square statistic.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1692 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1692</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1692</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-274515161</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.03597v1.pdf" target="_blank">The Vulnerability of Language Model Benchmarks: Do They Accurately Reflect True LLM Performance?</a></p>
                <p><strong>Paper Abstract:</strong> The pursuit of leaderboard rankings in Large Language Models (LLMs) has created a fundamental paradox: models excel at standardized tests while failing to demonstrate genuine language understanding and adaptability. Our systematic analysis of NLP evaluation frameworks reveals pervasive vulnerabilities across the evaluation spectrum, from basic metrics to complex benchmarks like GLUE and MMLU. These vulnerabilities manifest through benchmark exploitation, dataset contamination, and evaluation bias, creating a false perception of progress in language understanding capabilities. Through extensive review of contemporary evaluation approaches, we identify significant limitations in static benchmark designs, human evaluation protocols, and LLM-as-judge frameworks, all of which compromise the reliability of current performance assessments. As LLM capabilities evolve and existing benchmarks become redundant, we lay the groundwork for new evaluation methods that resist manipulation, minimize data contamination, and assess domain-specific tasks. This requires frameworks that are adapted dynamically, addressing current limitations and providing a more accurate reflection of LLM performance.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating Large Language Models Trained on Code <em>(Rating: 2)</em></li>
                <li>Agent-as-a-Judge: Evaluate Agents with Agents <em>(Rating: 2)</em></li>
                <li>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena <em>(Rating: 2)</em></li>
                <li>Al-pacaEval: An Automatic Evaluator of Instruction-Following Models <em>(Rating: 1)</em></li>
                <li>Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators <em>(Rating: 1)</em></li>
                <li>Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1692",
    "paper_id": "paper-274515161",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating Large Language Models Trained on Code",
            "rating": 2,
            "sanitized_title": "evaluating_large_language_models_trained_on_code"
        },
        {
            "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
            "rating": 2,
            "sanitized_title": "agentasajudge_evaluate_agents_with_agents"
        },
        {
            "paper_title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Al-pacaEval: An Automatic Evaluator of Instruction-Following Models",
            "rating": 1,
            "sanitized_title": "alpacaeval_an_automatic_evaluator_of_instructionfollowing_models"
        },
        {
            "paper_title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators",
            "rating": 1,
            "sanitized_title": "lengthcontrolled_alpacaeval_a_simple_way_to_debias_automatic_evaluators"
        },
        {
            "paper_title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
            "rating": 1,
            "sanitized_title": "chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference"
        }
    ],
    "cost": 0.0072152499999999994,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Vulnerability of Language Model Benchmarks: Do They Accurately Reflect True LLM Performance?
2 Dec 2024</p>
<p>Sourav Banerjee sb@unitedwecare.com 
DataLabs
Care Eishkaran Singh DataLabs
United We Care Ayushi Agarwal DataLabsUnited We, United We Care</p>
<p>The Vulnerability of Language Model Benchmarks: Do They Accurately Reflect True LLM Performance?
2 Dec 2024FEB88073C3594E94C6CF2ED967759EB7arXiv:2412.03597v1[cs.CL]
The pursuit of leaderboard rankings in Large Language Models (LLMs) has created a fundamental paradox: models excel at standardized tests while failing to demonstrate genuine language understanding and adaptability.Our systematic analysis of NLP evaluation frameworks reveals pervasive vulnerabilities across the evaluation spectrum, from basic metrics to complex benchmarks like GLUE and MMLU.These vulnerabilities manifest through benchmark exploitation, dataset contamination, and evaluation bias, creating a false perception of progress in language understanding capabilities.Through extensive review of contemporary evaluation approaches, we identify significant limitations in static benchmark designs, human evaluation protocols, and LLM-as-judge frameworks, all of which compromise the reliability of current performance assessments.As LLM capabilities evolve and existing benchmarks become redundant, we lay the groundwork for new evaluation methods that resist manipulation, minimize data contamination, and assess domain-specific tasks.This requires frameworks that are adapted dynamically, addressing current limitations and providing a more accurate reflection of LLM performance.</p>
<p>Introduction</p>
<p>The race for state-of-the-art performance metrics in Large Language Models (LLMs) has exposed a critical vulnerability in how they are evaluated [7].While LLMs achieve unprecedented scores on standardized tests, their susceptibility to dataset exploitation and pattern matching raises fundamental concerns about current evaluation methodologies.Rather than pursuing genuine language understanding, model developers increasingly exploit benchmark weaknesses, employing tactics like data contamination and climb leaderboards.Recent analyses reveal that LLMs can attain state-of-the-art results through such superficial optimization rather than deep linguistic comprehension [3], undermining the validity of existing bench-marking frameworks.This systematic failure in evaluation approaches threatens to misguide the field's progress, as models optimize for benchmarks that fail to capture genuine language understanding [1,5].</p>
<p>Evolution of Language Model Evaluations</p>
<p>The evolution of NLP benchmarks began in the 1950s and 1960s with foundational metrics like Precision and Recall, which measure the relevance of retrieved information.The introduction of F1 Score 1979, provided a balanced evaluation by combining Precision and Recall, setting the stage for more specialized benchmarks [24], [52].</p>
<p>Key metrics such as BLEU (2002) [19] for machine translation, which compares machine-generated text to human references using n-gram precision, and ROUGE (2004) [30], which focuses on recall in text summarization, were early attempts to evaluate language models.METEOR (2005) [31] improved upon BLEU by considering synonyms and partial matches, while the RTE challenge (2005) expanded evaluation to include semantic relationships [24].</p>
<p>The introduction of GLUE (2018) [10] marked a significant shift towards comprehensive evaluation, combining task-specific metrics across diverse NLP tasks.SuperGLUE (2019) [11] built on this by adding more challenging tasks requiring deeper language understanding.MMLU (2021) [12] expanded evaluation to assess models across 57 subjects, though its static format limited its ability to fully capture reasoning abilities.HELM (2022) [14] introduced a more ethically-oriented framework, evaluating models on a broader range of dimensions, including fairness and robustness.DynaBench (2022) [17] pushed for dynamic, adversarial evaluation, where models are tested against human-generated challenging examples.</p>
<p>The evolution of NLP benchmarks advanced with BIG-bench (2022) [54], evaluating language models across 200+ diverse tasks, and TruthfulQA (2022) [53], which tests models' truthfulness by challenging common misconceptions.These benchmarks reflect the growing complexity and expectations in assessing modern NLP systems.</p>
<p>Three common themes have emerged in modern Figure 1: Benchmarks released by year till Aug 2024 [13] LLM testing.First, Benchmark Evaluations have expanded beyond task-specific metrics to comprehensive frameworks like GLUE, SuperGLUE, and MMLU, enabling standardized performance comparisons across diverse NLP tasks.Second, Human-as-Judge methodologies have gained prominence, particularly in challenges like DynaBench, where human annotators generate adversarial examples and assess model outputs for nuanced understanding and creativity.Third, LLM-as-Judge approaches have been introduced, utilizing other language models to evaluate outputs at scale.As LLMs rapidly advance, the limitations of existing benchmarks have become increasingly apparent.Despite the evolution of evaluation methods, models are achieving near-perfect scores on many established metrics, raising questions about their discriminative power and comprehensiveness.This situation necessitates a critical examination of current evaluation practices and their potential vulnerabilities, especially given the high stakes associated with benchmark results in the AI landscape.</p>
<p>The Vulnerability of Current Benchmarks</p>
<p>The development and deployment of LLMs represent significant investments in research, computing resources, and human capital.As such, the ability to demonstrate a model's capabilities through objective measures holds considerable importance for research institutions, technology companies, and investors alike.The introduction of new models is frequently accompanied by claims of improved performance and capabilities, and are substantiated by citing results from established benchmarks, creating a landscape where benchmark performance serves as both a measure of technological progress and a key factor in the promotion of new AI technologies [4]. 1  The 'benchmark race' 2 therefore has become a hallmark of new model releases.When OpenAI in-Figure 2: LLM-Leaderboard [13] troduced GPT-3, it showcased state-of-the-art results across numerous NLP tasks, including an 88.4% score on SuperGLUE, surpassing the previous best of 84.6% [1], [10].Similarly, Google's BERT set new records on eleven NLP tasks, achieving a 93.2% F1 score on SQuAD v1.1, outperforming human performance [2], [11].This trend has intensified with models like GPT-3.5 and GPT-4, which were highlighted for their superior performance on benchmarks, exams, and coding challenges [3].GPT-4, for instance, scored 86.4% on the MMLU benchmark, well above GPT-3.5's70.0% [12].</p>
<p>However, companies often engage in selective reporting, emphasizing their models' strengths while downplaying weaknesses .Anthropic's Claude 2, for example, focused on graduate-level exams and coding tasks [5], while Microsoft and Nvidia's Megatron-Turing NLG 530B highlighted its zero-shot task performance [6].</p>
<p>This scenario reflects Goodhart's Law: "When a measure becomes a target, it ceases to be a good measure" [51].As benchmark scores have become the primary standard for evaluating models, they have paradoxically become less reliable indicators of true capability, leading to models that excel on paper but struggle with real-world tasks requiring genuine understanding and adaptability [21].</p>
<p>How Language Model Benchmarks are "Hacked"</p>
<p>The reliability of benchmarks used to evaluate Large Language Models (LLMs) is increasingly questioned</p>
<p>Benchmark Overfitting</p>
<p>Benchmark overfitting, or "overtuning," occurs when models are excessively optimized for specific benchmarks without genuinely improving their general capabilities.This is akin to "p-hacking" in empirical studies, where data is manipulated to achieve misleading statistical significance [1].</p>
<p>To understand benchmark overfitting mathematically, let's consider a simplified model of LLM performance.Let P be the true performance of an LLM on a given task, B be the benchmark score, and ε be the error term that represents the difference between the true performance and the benchmark score:
B = P + ε
Ideally, we want B to be an unbiased estimator of P, meaning E[B] = P, where E[] denotes expected value.However, in the case of benchmark overfitting, we often see:</p>
<p>B &gt; P</p>
<p>This occurs because the model has been optimised to maximise B rather than P. We can represent this optimization process as:
max(B) ̸ = max(P )
The impact was underscored, leading to the development of more robust benchmarks like HellaSwag [16].However, overfitting concerns persist, as seen with GPT-3's inflated performance on LAMBADA due to training data overlap and BERT's exploitation of patterns in the SQuAD dataset [25], [20].</p>
<p>Public Availability of Datasets</p>
<p>Often referred to as "data contamination" or "test set leakage," publicly available datasets can lead to inflated performance metrics that do not accurately reflect a model's true capabilities [1]   dataset.In an ideal scenario, these sets should be disjoint:
D train ∩ D test = ∅
However, due to the public nature of many benchmark datasets, we often encounter a situation where:
D train ∩ D test ̸ = ∅
This overlap can be quantified using the contamination rate (CR):
CR = |D train ∩ D test | |D test |
To address this issue, researchers have proposed several strategies.Dodge et al. [32] introduced the concept of "data contamination audits," which involve systematically checking for overlaps between training and test sets.They define a contamination score C(m, d) for a model m and dataset d:
C(m, d) = i s(m, x i ) • |d|
where s(m, x i ) is a function that measures the similarity between the model's output and the ground truth for example x i .A high C(m, d) suggests potential data contamination.</p>
<p>Another approach involves creating "hidden" test sets, kept from public release, to obtain a more accurate estimate of true model performance.However, this method faces challenges, such as ensuring the test set remains hidden and maintaining statistical validity with smaller datasets.</p>
<p>The implications of dataset publicity extend beyond simple memorization.LLMs trained on public datasets may develop heuristics that exploit datasetspecific patterns rather than learning generalizable language understanding.This can be conceptualised as the model learning a function g(x) that approximates the true task function f (x) for x ∈ D test , but diverges for x / ∈ D test To address these issues, the field is shifting towards more robust evaluation methods.Andrew Ilyas et al. [37], in "Adversarial Examples Are Not Bugs, They Are Features," argue that adversarial examples exploit the very features models rely on, exposing their vulnerabilities and emphasizing the importance of testing robustness.Similarly, Robin Jia and Percy Liang's work, particularly in "Adversarial Examples for Evaluating Reading Comprehension Systems," shows that small perturbations to input data can mislead models that perform well on standard tests, highlighting the need for more rigorous evaluations.These challenge sets are designed to be more difficult and better at exposing models' weaknesses than standard tests.</p>
<p>Test Set Contamination</p>
<p>Test Set Contamination occurs when models are inadvertently exposed to test data during training, leading to skewed performance metrics and an unrealistic assessment of generalization capabilities 5 .Let θ represent the parameters of an LLM, D train the training dataset, and D test the test dataset.In an ideal scenario, the model's parameters should be independent of the test set
P (θ | D train , D test ) = P (θ | D train )
However, test set contamination introduces a dependency:
P (θ | D train , D test ) ̸ = P (θ | D train )
This dependency can be quantified using mutual information I(θ; D test | D train ), which measures the reduction in uncertainty about θ given knowledge of D test , conditioned on D train [2]: This contamination can lead to inflated performance estimates, where the model's generalization function f(x) is conflated with a contamination-influenced function g(x) :
I(θ; D test | D train ) = H(θ | D train )−H(θ | D train , D test )g(x) = f (x) + c(x)
where c(x) represents the contribution from test set contamination.</p>
<p>To quantify the impact of contamination, researchers have proposed various metrics.Carlini et al. [26] introduced the "exposure" metric, which measures how much information about a specific training example is encoded in the model's parameters.They define exposure as:
Exposure(s[r]) = log 2 |R| − log 2 (rank(s[r]))
where |R| is the size of the randomness space from which the canaries (random sequences) are chosen, and rank θ (s[r]) is the rank of a specific canary sequence s[r] based on the model's perplexity, where the rank represents the index of the canary in the list of all possible canaries sorted by their log-perplexity.High exposure values indicate potential memorization and contamination.</p>
<p>To mitigate contamination, several strategies have emerged, including dynamic test set generation, where test data is created post-training to avoid overlaps, and data provenance tracking [38], which involves tracing the origin and history of each data example.Provenance tracking helps assess contamination risk by comparing test and training data histories, flagging high-risk examples for removal or further investigation [27].</p>
<p>Future work may involve combining multiple strategies, such as dynamic test set generation, rigorous provenance tracking, and advanced contamination detection techniques, to ensure that reported model performance accurately reflects true generalization capabilities.</p>
<p>Gaming Through Task-Specific Optimization</p>
<p>Language model developers often exploit specific benchmark structures to achieve high performance scores, a practice that may yield impressive results on certain tasks but fails to enhance overall model capabilities or generalization to diverse contexts [1].</p>
<p>Figure 5: Detecting dataset contamination via logprobability differences in canonical vs. shuffled orders [39] Let M be an LLM with parameters θ, and B = B 1 , B 2 , ..., B n be a set of benchmark tasks.The optimization problem for task-specific fine-tuning can be formulated as:
arg max θ i w i S(M θ , B i )
where S(M, B i ) is the score of model M with parameters θ on benchmark B i , and w i are task-specific weights.This formulation encourages overfitting to the specific characteristics of the chosen benchmarks rather than improving general language understanding.</p>
<p>The discrepancy between benchmark performance and real-world capability can be quantified using a generalization gap metric:
G(M ) = E[S(M, B test )] − E[S(M, B real )]
where B test represents benchmark test sets and B real represents real-world tasks.A large G(M ) indicates potential gaming of the benchmarks.</p>
<p>McCoy demonstrated this issue with BERT models fine-tuned on the MNLI dataset [56].They showed that these models often relied on shallow heuristics specific to the dataset structure rather than developing a deeper understanding of natural language inference.For instance, models achieved high accuracy by exploiting the presence of specific words or phrase structures that were correlated with certain labels in the training data.This can be represented as a learned decision function f (x) that approximates the true task function t(x) for x ∈ B test , but diverges for x / ∈ B test :
∥f (x) − t(x)∥ ≈ 0 for x ∈ B test ∥f (x) − t(x)∥ ≫ 0 for x / ∈ B test
where ∥ • ∥ denotes an appropriate distance metric in the output space.Similarly, Kaushik et al. [28] found that models trained on the SNLI dataset often relied on artifact features rather than genuine semantic understanding.They proposed a counterfactually-augmented data approach to mitigate this issue, effectively expanding the task distribution to reduce exploitable patterns:
B ′ train = B train ∪ {c(x, y) | (x, y) ∈ B train }
where c(x, y) generates a counterfactual example for input x and label y.To counteract these problems, researchers have suggested creating more challenging benchmarks, like SuperGLUE [11], that require complex reasoning, and employing multi-task learning to avoid overfitting.Adversarial testing methods, such as the Adversarial NLI dataset, have also been explored to expose the limitations of task-specific optimization.</p>
<p>To mitigate these issues, the field is moving towards more holistic evaluation frameworks.For instance, Bowman and Dahl [23] advocate for a shift from narrowly-defined leaderboard tasks to more comprehensive evaluation suites that assess a wide range of linguistic and reasoning capabilities.Future research directions may involve the development of more robust, multi-faceted evaluation frameworks, the creation of benchmarks that are inherently resistant to gaming, and a greater emphasis on assessing model performance in diverse, real-world contexts.</p>
<p>Adversarial Benchmarking</p>
<p>Adversarial benchmarking has become crucial for assessing the robustness of large language models (LLMs) against subtle, malicious inputs designed to confuse or mislead them [1].Traditional benchmarks often fail to capture these vulnerabilities, leading to overly optimistic assessments of model performance [35].Let M be an LLM with parameters θ, and x be an input from the distribution D. The model's output can be represented as:
y = M θ (x)
An adversarial example x' can be defined as:
x ′ = x+δ, such that ∥δ∥ ≤ ϵ and M θ (x ′ ) ̸ = M θ (x)
where ∥ • ∥ is some distance metric and ϵ is a small perturbation bound.The goal of adversarial benchmarking is to evaluate the model's performance on these perturbed inputs:
R(M ) = E x [L(M (x ′ ), y)]
where L is a loss function and y is the true label.A lower R(M) indicates higher robustness.</p>
<p>Jia and Liang [20] demonstrated the vulnerability of reading comprehension systems to adversarial inputs by introducing the AddSent method.They showed that appending adversarially constructed sentences to paragraphs could dramatically reduce the performance of state-of-the-art models on the SQuAD dataset.This can be formalized as:
x ′ = x ⊕ s
where ⊕ denotes concatenation and s is an adversarially generated sentence.They found that:
E[S(M, x ′ )] ≪ E[S(M, x)]
where S is the model's performance score.</p>
<p>Similarly, Alzantot et al. [36] introduced a genetic algorithm-based approach to generate adversarial examples for text classification tasks.Their method iteratively applies small perturbations to inputs while maintaining semantic similarity and grammatical correctness:
x ′ t+1 = arg max x ′ {f (x ′ ) | sim(x ′ , x) &gt; τ, g(x ′ ) = 1}
where f is the adversarial objective, sim is a semantic similarity function, τ is a threshold, and g is a grammaticality checker.They demonstrated that models achieving high accuracy on standard benchmarks could be easily fooled by these adversarial inputs.</p>
<p>The implications are significant; models that excel on standard benchmarks may fail under adversarial conditions in real-world scenarios.To quantify this, metrics like Adversarial Accuracy Drop (AAD) measure the difference in model performance between original and adversarial datasets [33], [34].High AAD indicates vulnerability to adversarial attacks.</p>
<p>Adversarial benchmarks have revealed surprising weaknesses in models like BERT, which Wallace et al. [6] showed to be vulnerable to universal adversarial triggers-short token sequences that consistently cause specific mispredictions.To improve robustness, techniques like FreeLB [55], add adversarial perturbations during training, enhancing model generalization across tasks.</p>
<p>To counter these weaknesses, adversarial benchmarking initiatives have emerged.Nie et al. [4] developed the Adversarial NLI (ANLI) dataset, iteratively designed to challenge models as they improve, while Gardner et al. [22] proposed contrast sets-minimally edited benchmark examples that expose model vulnerabilities.</p>
<p>The rise of adversarial benchmarks has also driven research into model interpretability and analysis.For example, Gururangan et al. [29] used adversarial examples to uncover annotation artifacts in natural language inference datasets, leading to improved data collection practices.Future research may focus on dynamic adversarial benchmarks that evolve with model improvements, multi-modal adversarial inputs, and integrating adversarial robustness as a core metric in model evaluation.</p>
<p>Human Bias</p>
<p>The increasing reliance on human judges to evaluate the performance of large language models, exemplified by approaches like DynaBench [17] that emphasize adversarial human examples, has introduced new vulnerabilities in the benchmarking process.While human judgment provides valuable insights into model capabilities, it also opens the door to potential benchmark hacking.One significant limitation of human evaluations is the inherent inconsistency and bias related to factors like output formatting, tone, and formality [45].Oren et al. (2023) highlight that different human annotators may apply varying standards when assessing model outputs, resulting in noisy and unreliable scores [39].This lack of consistency can be exploited by models that are finely tuned to the specific preferences and idiosyncrasies of individual human judges, rather than focusing on genuine improvements in language understanding.Additionally, human participants may not create a diverse range of questions, may focus on certain topics that do not thoroughly test a model's overall abilities, or may design prompts that are poorly constructed [45]. 6 One key limitation of human evaluations is the inherent inconsistency and bias related to factors like output formatting, tone, and formality [45].Oren et al. (2023) highlight that different human annotators may apply varying standards when assessing model outputs, resulting in noisy and unreliable scores [39].This lack of consistency can be exploited by models that are finely tuned to the specific preferences and idiosyncrasies of individual human judges, rather than focusing on genuine improvements in language understanding.Furthermore, human participants may not create a diverse range of questions, may concentrate on certain topics that do not thoroughly test a model's overall abilities, or may design prompts that are poorly constructed [45].Moreover, human evaluations are susceptible to adversarial attacks specifically designed to deceive the judges.Oren et al. ( 2023) demonstrate that LLMs can generate outputs crafted to appear convincing to humans, even when the content is factually incorrect or logically inconsistent [39].By exploiting human cognitive biases and heuristics, models can manipulate benchmark scores without exhibiting true language understanding.This vulnerability underscores the need for more robust evaluation methods that can detect and mitigate adversarial gaming strategies.</p>
<p>The use of human judges can also create perverse incentives for models to prioritize surface-level features that humans find persuasive, such as coherence and fluency, at the expense of deeper aspects of language comprehension.Dubois et al. (2024) argue that this phenomenon, termed "evaluation hacking," leads Figure 6: Excerpts from human evaluators' explanations for why they believe a GPT3-generated story (also excerpted) was written by a human (left) or a machine (right).The evaluators point to a wide range of text attributes to make their decisions, sometimes using the same aspect of the text to come to opposite conclusions [46] models to optimize for human-pleasing outputs rather than developing genuine linguistic competence [18].Furthering this concern, Clark et al. ( 2021) illustrate the subjective nature of human evaluations in discerning between human and machine-generated text.Their study reveals evaluators' contradictory judgments, often influenced by the same superficial features, such as coherence, that models are programmed to exploit.This inconsistency highlights the unreliability of human assessments and the necessity for more objective evaluation methods that prioritize genuine linguistic competence over appealing yet shallow textual attributes [46].</p>
<p>To address these challenges, researchers have proposed various mitigation strategies.One approach is to employ adversarial filtering techniques to identify and exclude human-generated examples that are designed to deceive models [39].By proactively removing adversarial inputs from the evaluation set, the impact of gaming strategies can be reduced.Additionally, the use of multiple diverse human judges, rather than relying on a single evaluator, can help to average out individual biases and provide a more balanced assessment of model performance [18].However, while these techniques can alleviate some of the issues associated with human judging, the inherent subjectivity and potential for gaming remain significant obstacles to fully reliable benchmarking.</p>
<p>Model Overfitting to Peer Evaluation</p>
<p>As the limitations of human judges became apparent, researchers explored the use of large language models (LLMs) themselves as evaluators for other language models.By leveraging the capabilities of LLMs to automatically assess the outputs of target models, this approach aimed to scale up evaluation and reduce the influence of human biases.However, the use of LLMbased judging introduces its own set of vulnerabilities that can enable benchmark hacking.</p>
<p>One major concern with LLMs as judges is the potential for model-to-model collusion.Yan (2024) demonstrates that LLMs can learn to identify and exploit regularities in the outputs of other models, leading to inflated performance estimates [15].By detecting patterns that are specific to certain model families or architectures, LLMs can assign high scores to outputs that match these patterns, regardless of their actual quality or relevance to the task at hand.Furthermore, researchers have found that LLMs often favor their own answers over those of other LLMs, and they tend to prefer more verbose outputs [41], [42], [43], [44].</p>
<p>The use of LLMs as judges can also amplify and perpetuate biases present in the training data of both the judge and target models.If an LLM judge has internalized certain biases or learned to favor specific types of outputs based on spurious correlations in its training data, it may assign disproportionately high scores to target models that exhibit similar biases [15].This can lead to a distorted view of model performance, as the benchmarks fail to accurately assess the models' ability to generate unbiased and equitable outputs.The amplification of biases through LLM-based judging can have significant implications, particularly in sensitive domains such as hate speech detection or content moderation, where the fair treatment of all users is of utmost importance.</p>
<p>Another risk associated with using LLMs as judges is the potential to stifle innovation and hinder the discovery of novel approaches to language modeling.If an LLM judge has been trained on a narrow range of model architectures or has developed biases towards the patterns and behaviors of models similar to itself, it may assign lower scores to outputs generated by radically different architectures, even if these new approaches represent genuine advances in the field [15].This bias towards familiar patterns can create a selfreinforcing cycle, where established model architectures are favored over innovative techniques, slowing down progress and limiting the exploration of alternative solutions.</p>
<p>To mitigate these challenges, researchers have proposed various strategies for ensuring fair and unbiased evaluation with LLM judges.Yan (2024) suggests the use of "judge ensembles," which aggregate scores from a diverse set of LLM architectures to reduce the impact of model-specific biases [15].By combining the assessments of multiple LLMs with different training backgrounds and architectures, the influence of individual model biases can be diluted, resulting in a more balanced evaluation.Additionally, the development of techniques for auditing LLM judges for biases and ensuring their fairness is crucial to maintain the integrity of benchmarking results.</p>
<p>However, despite these mitigation efforts, the fundamental similarities between LLM judges and the target models they evaluate remain a significant concern.As long as both the judge and target models are based on similar architectures and training paradigms, there is an inherent risk of shared biases and blind spots that can undermine the reliability of benchmarking results.Overcoming these challenges will require a concerted effort from the research community to develop novel evaluation approaches that are truly independent of the models being assessed and can provide a more objective measure of language understanding capabilities.</p>
<p>The vulnerabilities introduced by both human and LLM judges, including the sensitivity of LLM evaluation to prompting effects, serve as a stark reminder of the ongoing challenges in creating robust and reliable benchmarks for evaluating language model performance.As the field of natural language processing continues to evolve at a rapid pace, it is imperative that researchers remain vigilant to the potential for benchmark hacking and actively work to develop evaluation methodologies that are resilient to gaming and manipulation.This may involve a combination of techniques, such as adversarial filtering, bias auditing, and the use of diverse judge ensembles, as well as the exploration of entire</p>
<p>Human Evaluation: A Flawed Gold Standard</p>
<p>Human evaluation has long been considered the gold standard for assessing language model outputs.However, Clark et al. demonstrate in their study that human evaluations of generated text are fraught with inconsistencies and biases [46].In their analysis of GPT-3 generated stories, they found that human evaluators often relied on superficial text attributes to make their judgments, leading to contradictory assessments.For instance, evaluators frequently cited "coherence" as a reason for both human and machine attributions.One evaluator noted, "The story flows well and is coherent, which suggests it was written by a human," while another stated, "The text is too coherent and flows too smoothly to be written by a human."</p>
<p>This contradiction highlights the subjective nature of human evaluation and its vulnerability to individual biases and expectations.</p>
<p>Furthermore, the study revealed that evaluators were often incorrect in their attributions, with accuracy rates barely above chance (51.9% to 72.3%) across various experimental conditions [46].These findings have significant implications for benchmark evaluation.The inconsistency in human judgments calls into question the reliability of human-evaluated benchmarks.</p>
<p>Additionally, LLMs trained on human-evaluated datasets may learn to exploit these superficial attributes, potentially leading to models that appear more "human-like" without genuine improvements in language understanding.There's also a risk that models could be fine-tuned to produce outputs that cater to known human biases, artificially inflating their scores on human-evaluated benchmarks.</p>
<p>LLMs as Judges: A Double-Edged Sword</p>
<p>As an alternative to human evaluation, recent research has explored the use of LLMs themselves as judges for evaluating other language models.Yin et al. conducted a comprehensive study on this approach, revealing both promising aspects and significant limitations [47].Their findings provide a nuanced view of the potential and pitfalls of using LLMs as judges.</p>
<p>The study found several advantages to the LLMas-judge approach.LLMs demonstrated higher interannotator agreement compared to human evaluators, with GPT-4 achieving a Fleiss' kappa of 0.686 versus 0.270 for humans on the WMT dataset.LLM evaluations also showed strong correlations with aggregated human scores, with GPT-4 achieving Pearson correlations of 0.81 and 0.74 on the WMT and GEM datasets, respectively.Furthermore, LLM-based evaluation can be conducted more rapidly and at a larger scale compared to human evaluation, potentially allowing for more comprehensive benchmarking [47].</p>
<p>However, the study also uncovered several critical limitations.LLM judgments were highly sensitive to prompt formulation, with even minor changes leading to significant shifts in evaluation outcomes.While LLMs could provide explanations for their judgments, these explanations were often inconsistent with the actual scores given, raising questions about the interpretability and reliability of their decision-making process.The researchers also found that LLMs consistently favored longer, more detailed responses, potentially leading to inflated scores for prolific but not necessarily higher-quality outputs.Perhaps most concerningly, the study demonstrated that LLM judges could be manipulated by carefully crafted inputs de-signed to exploit their biases or limitations [47].</p>
<p>Further developments, such as the Agent-as-a-Judge framework introduced by Zhuge et al [57], aim to address these limitations by incorporating iterative feedback loops, where agents assess one another throughout multi-step tasks.This agent-based approach, validated on benchmarks like DevAI, enhances evaluation precision and adaptability by integrating real-time intermediate assessments, suggesting a pathway toward mitigating biases inherent in traditional LLM judgment structures.</p>
<p>Systematic Vulnerabilities in LLM-as-Judge Evaluations</p>
<p>Building on these findings, Zheng et al. conducted a more focused study on the vulnerabilities of LLMas-judge evaluations [45].Their research uncovered several systemic issues that could lead to misleading benchmark results.</p>
<p>The study found a significant self-preference bias, where LLMs consistently rated their own outputs higher than those of other models, including humanwritten responses.This bias was observed across multiple LLMs and tasks, with an average self-preference of 55.80% for instruction-following tasks and 60.40% for open-ended tasks.The researchers also noted instability across LLMs, with different LLM judges producing inconsistent rankings of the same set of models.The average Kendall's Tau correlation between judge pairs was only 0.582, indicating a concerning lack of consensus [45].</p>
<p>Furthermore, the study demonstrated that models could be fine-tuned to exploit known biases of LLM judges, artificially inflating their benchmark scores without genuine improvements in capability.LLM judges also showed high sensitivity to superficial changes in input formatting and phrasing, leading to significant fluctuations in scores for essentially identical content [45].These vulnerabilities raise serious concerns about the validity of LLM-as-judge evaluations and their potential to produce misleading benchmark results.Models could be optimized to perform well under specific LLM judges without necessarily improving their overall language understanding or task performance.</p>
<p>Implications for Benchmark Interpretation</p>
<p>The findings from these studies have significant implications for how we interpret and use LLM benchmark results.As models approach or achieve perfect scores on established benchmarks, these results should be viewed with increased skepticism, considering the potential for benchmark hacking or exploitation of evaluation biases.</p>
<p>No single evaluation method-whether human, LLM-as-judge, or static metrics-can provide a comprehensive assessment of LLM performance.A combination of approaches, each with clearly stated limitations, is necessary for a more robust evaluation.As models become more adept at exploiting current evaluation methods, there is a pressing need for benchmarks to evolve continuously, introducing novel tasks and evaluation criteria that challenge models in unexpected ways.</p>
<p>Researchers and organizations should provide detailed information about their evaluation methodologies, including potential biases and limitations, to allow for more informed interpretation of benchmark results.While benchmarks provide valuable comparative data, increased emphasis should be placed on evaluating LLMs in diverse, real-world applications where the complexity and unpredictability of tasks may reveal capabilities or limitations not captured by current benchmarks.</p>
<p>While LLM benchmarks remain an important tool for measuring progress in natural language processing, their vulnerabilities to gaming and bias necessitate a more nuanced and critical approach to result interpretation.As the field continues to advance, developing more robust, dynamic, and comprehensive evaluation frameworks will be crucial to ensure that benchmark results accurately reflect genuine improvements in language model capabilities.</p>
<p>Analysis of Systematic Benchmark Manipulation in Language Models</p>
<p>The evaluation of Large Language Models (LLMs) through benchmarking presents significant methodological challenges that warrant critical examination.</p>
<p>Recent investigations reveal systematic patterns in benchmark optimization that may compromise the validity of reported performance metrics.Contemporary benchmark methodologies often fail to capture the full spectrum of model performance, instead providing potentially misleading indicators of capability.This phenomenon manifests through multiple vectors: structural biases in benchmark design, statistical validity concerns, resource distribution inequities, standardization limitations, and selective reporting practices.</p>
<p>Each of these factors contributes to a complex ecosystem where benchmark results may not accurately reflect true model capabilities or generalization potential.</p>
<p>Structural Biases in Benchmark Design</p>
<p>Current benchmark methodologies exhibit inherent structural biases that potentially distort performance metrics.For instance, the GPT-3 evaluation proto-col demonstrates performance optimization through selective task curation, particularly in few-shot learning scenarios [1].While such evaluations yield quantifiable results, they may not adequately reflect the model's generalized performance across diverse application contexts.</p>
<p>Statistical Validity and Sample Size Considerations</p>
<p>The evaluation of LLM performance metrics necessitates careful consideration of statistical principles, particularly the Law of Large Numbers.Statistical inference based on limited samples may yield unreliable conclusions regarding model capabilities.This phenomenon is exemplified in the evaluation of the LLaMA model [48], which reports competitive performance metrics despite utilizing a significantly reduced training dataset compared to contemporary models.The statistical reliability of such results warrants scrutiny, as performance metrics derived from limited datasets may not accurately represent the model's general capabilities.This statistical uncertainty parallels fundamental probability theory, where limited sampling can produce deceptive apparent probabilities that fail to reflect true underlying distributions.</p>
<p>Resource Distribution and Historical Advantages in Model Development</p>
<p>The development and evaluation of LLMs exhibits significant resource-based stratification within the field.Established organizations with extensive computational infrastructure and historical data accumulation, such as those developing GPT-4 [49], possess substantial advantages in model optimization and performance refinement.This resource asymmetry manifests in multiple dimensions: access to extensive historical datasets, accumulated technical expertise, and refined fine-tuning methodologies developed over extended periods.These advantages create systemic barriers to equitable model comparison, particularly for emerging research entities with limited access to comparable resources.The impact of this resource disparity extends beyond mere computational capability.Organizations with extensive development histories benefit from accumulated technical knowledge, refined training methodologies, and extensive proprietary datasets.This historical advantage creates inherent biases in benchmark performance metrics, potentially obscuring the actual algorithmic or architectural innovations of newer models.The phenomenon necessitates careful consideration in benchmark design to ensure fair evaluation of fundamental model capabilities indepen-dent of resource advantages.</p>
<p>Limitations of Standardized Evaluation Protocols</p>
<p>Current standardized evaluation frameworks, such as MMLU [12], which assesses models across 57 domains, present methodological constraints in capturing the full spectrum of language understanding and generation capabilities.These protocols may inadequately measure adaptive reasoning and contextual understanding, particularly in dynamic, real-world applications.</p>
<p>Selective Reporting in Performance Analysis</p>
<p>The documentation and reporting of benchmark results frequently exhibit systematic biases in data presentation.For instance, the Qwen model documentation [50] emphasizes performance metrics in specific linguistic domains while potentially understating performance variations in others.This selective reporting methodology necessitates more comprehensive and standardized documentation protocols.This analysis reveals systematic limitations in current benchmark methodologies, suggesting the need for more rigorous evaluation frameworks that address these identified constraints.The development of a zero-day, zero-shot evaluation framework represents a methodological response to these challenges, introducing protocols designed to address these identified limitations while upholding evaluation rigor.</p>
<p>Future Work</p>
<p>Future research proposes substantive methodological improvements to address the identified limitations in current LLM evaluation frameworks.We propose, a zero-day, zero-shot evaluation protocols initially centered on specific domain applications.We aim to introduce systematic assessment mechanisms that subject language models to novel, domain-specific scenarios, thereby quantifying their contextual understanding and adaptive reasoning capabilities.</p>
<p>We will explore dynamic framework for evaluating large language models (LLMs), underpinned by consortium-based governance structure comprising representatives from academia, industry, and research organizations.The proposed governance mechanism will ensure systematic quarterly iterations to mitigate benchmark decay and prevent overfitting, fostering continuous improvement.We also propose parameternormalized peer review systems, which use standardized protocols to evaluate responses anonymously, ensures fairness and consistency across computational architectures.</p>
<p>As a proposed approach for future research, we aim to incorporate domain-specific adaptations to ensure that tasks in legal, financial, and educational sectors are thoroughly tested.We plan to establish domain-specific evaluation protocols while maintaining methodological rigor, integrating zero-day assessment protocols, structured governance, and dynamic iteration processes.This strategy marks a shift from static benchmarks to adaptive evaluation mechanisms.By emphasizing domain expertise and consistent methodological evolution, we intend to provide a robust foundation for advancing LLM capabilities and ensuring their reliability in high-stakes, real-world contexts.</p>
<p>Figure 3 :
3
Figure 3: Open LLM Leaderboard by Hugging Face</p>
<p>Figure 4 :
4
Figure 4: Data leakage distribution[40]</p>
<p>where H() denotes entropy.A non-zero value of I(θ; D test | D train ) indicates potential test set contamination.The severity of contamination can vary.In some cases, entire test examples may be present in the training data, a situation we can represent as: ∃x ∈ D test : x ∈ D train More subtle forms of contamination can occur when the training data contains information that is highly correlated with the test set.This can be modelled using a similarity function sim(x, y) between examples: ∃x ∈ D train , y ∈ D test : sim(x, y) &gt; τwhere τ is some threshold of similarity.</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordIlya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners</p>
<p>Extracting Training Data from Large Language Models. Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, 30th USENIX Security Symposium (USENIX Security 21). USENIX Association2021Úlfar Erlingsson, Alina Oprea, and Colin Raffel</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterAssociation for Computational Linguistics20191</p>
<p>Felipe Petroski Such. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant MisraDave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang; Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder; Bob McGrew, Dario Amodei, Sam McCandlishIlya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code</p>
<p>Improving Language Understanding by Generative Pretraining. Alec Radford, Karthik Narasimhan, OpenAI. Preprint. 2018Tim Salimans, Ilya Sutskever, and others</p>
<p>Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products. Inioluwa Raji, Joy Buolamwini, 10.1145/3306618.3314244Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (AIES '19). the 2019 AAAI/ACM Conference on AI, Ethics, and Society (AIES '19)ACM2019</p>
<p>Attention is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc2017. NeurIPS 2017</p>
<p>Artificial Intelligence as a Positive and Negative Factor in Global Risk. Eliezer Yudkowsky, Global Catastrophic Risks. Nick Bostrom, Milan M Cirkovic, Oxford University Press2008</p>
<p>Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints. Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, Kai-Wei Chang, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2017</p>
<p>GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, 2019</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, Super-GLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. 2020</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Understanding. </p>
<p>Open LLM Leaderboard -a Hugging Face Space by open-llm-leaderboard-old. Llm Open, Leaderboard, </p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D Manning, Christopher Ré, Diana Acosta-Navas, Drew A Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda, Holistic Evaluation of Language Models. 2023</p>
<p>Task-Specific LLM Evals that Do &amp; Don't Work. eugeneyan. Ziyou Yan, March 2024</p>
<p>HellaSwag: Can a Machine Really Finish Your Sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, 2019</p>
<p>Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Dynabench: Rethinking Benchmarking in NLP. Mohit Jia, Christopher Bansal, Adina Potts, Williams, 2021</p>
<p>Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B Hashimoto, Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators. 2024</p>
<p>BLEU: A Method for Automatic Evaluation of Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. the 40th Annual Meeting on Association for Computational LinguisticsAssociation for Computational Linguistics2002</p>
<p>Know What You Don't Know: Unanswerable Questions for SQuAD. Pranav Rajpurkar, Robin Jia, Percy Liang, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Norah Alzahrani, Hisham Abdullah Alyahya, Yazeed Alnumay, Sultan Alrashed, Shaykhah Alsubaie, Yusef Almushaykeh, Faisal Mirza, Nouf Alotaibi, Nora Altwairesh, Areeb Alowisheq, M Saiful Bari, Haidar Khan, When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards. 2024</p>
<p>Universal Adversarial Triggers for Attacking and Analyzing NLP. Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational Linguistics2019</p>
<p>What Will it Take to Fix Benchmarking in Natural Language Understanding?. R Samuel, George E Bowman, Dahl, 2021</p>
<p>The PASCAL Recognising Textual Entailment Challenge. Ido Dagan, Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Textual Entailment. Berlin HeidelbergSpringer2006</p>
<p>The LAMBADA dataset: Word prediction requiring a broad discourse context. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan, Raffaella Pham, Sandro Bernardi, Marco Pezzelle, Gemma Baroni, Raquel Boleda, Fernández, 2016</p>
<p>Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, Dawn Song, The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks. 2019</p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela, Adversarial NLI: A New Benchmark for Natural Language Understanding. 2020</p>
<p>Learning the Difference that Makes a Difference with Counterfactually-Augmented Data. Divyansh Kaushik, Eduard Hovy, Zachary C Lipton, 2020</p>
<p>Annotation Artifacts in Natural Language Inference Data. Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, Noah A Smith, 2018</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. Chin-Yew Lin, Text Summarization Branches Out. Association for Computational Linguistics2004</p>
<p>ME-TEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationAssociation for Computational Linguistics2005</p>
<p>Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner, 2021</p>
<p>How many perturbations break this model? Evaluating robustness beyond adversarial accuracy. Raphael Olivier, Bhiksha Raj, 2023</p>
<p>Yogesh Balaji, Tom Goldstein, Judy Hoffman, Instance Adaptive Adversarial Training: Improved Accuracy Tradeoffs in Neural Nets. 2019</p>
<p>Sophie Xhonneux, Alessandro Sordoni, Stephan Günnemann, Gauthier Gidel, Leo Schwinn, Efficient Adversarial Training in LLMs with Continuous Attacks. 2024</p>
<p>Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, Kai-Wei Chang, Generating Natural Language Adversarial Examples. 2018</p>
<p>Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry, Adversarial Examples Are Not Bugs, They Are Features. 2019</p>
<p>Provenance Documentation to Enable Explainable and Trustworthy AI: A Literature Review. Aviral Kale, Truc Nguyen, Jack Harris, Chang Li, Jiayi Zhang, Xiaoli Ma, Data Intelligence. 512023</p>
<p>Yanai Oren, Nicolas Meister, Niladri Chatterji, Firoj Ladhak, Tatsunori B Hashimoto, arXiv [Cs.CLProving Test Set Contamination in Black Box Language Models. 2023</p>
<p>Silvio Balloccu, Petra Schmidtová, Matúš Lango, Ondřej Dušek, arXiv [Cs.CLLeak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs. 2024</p>
<p>Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B Hashimoto, arXiv:2404.04475Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators. 2024arXiv preprint</p>
<p>Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E Gonzalez, Ion Stoica, From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline. 2024</p>
<p>Al-pacaEval: An Automatic Evaluator of Instruction-Following Models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Colin White, Samuel Dooley, Matthew Roberts, Ameya Pal, Benjamin Feuer, Sarthak Jain, arXiv:2406.19314LiveBench: A Challenging, Contamination-Free LLM Benchmark. 2024arXiv preprint</p>
<p>Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, arXiv:2403.04132Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference. 2024arXiv preprint</p>
<p>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 202436</p>
<p>All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text. Elizabeth Clark, Tal August, Sarah Serrano, Nate Haduong, Suchin Gururangan, Noah A Smith, arXiv:2107.000612021arXiv preprint</p>
<p>Can Large Language Models Be an Alternative to Human Evaluations?. Chung-Hsuan Chiang, Hung-Yi Lee, arXiv [Cs.CL2023</p>
<p>Hugo Touvron, arXiv:2302.13971LLaMA: Open and Efficient Foundation Language Models. 2023arXiv preprint</p>
<p>. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Altman, Shyamal Anadkat. et al. 2024. GPT-4 Technical Report</p>
<p>. Inze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical Report</p>
<p>Improving ratings': audit in the British University system. Marilyn Strathern, 10.1002/(SICI)1234-981XAID-EURO184¿3.0.CO;2- 4European Review. 531997. 199707</p>
<p>Beyond Accuracy, F-Score and ROC: A Family of Discriminant Measures for Performance Evaluation. Marina Sokolova, Nathalie Japkowicz, Stan Szpakowicz, 10.1007/11941439114AI 2006: Advances in Artificial Intelligence. Lecture Notes in Computer Science. 20064304</p>
<p>TruthfulQA: Measuring How Models Mimic Human Falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, 2022</p>
<p>Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, 2023</p>
<p>FreeLB: Enhanced Adversarial Training for Natural Language Understanding. Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, Jingjing Liu, 2020</p>
<p>A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. Adina Williams, Nikita Nangia, Samuel R Bowman, 2018</p>
<p>Agent-as-a-Judge: Evaluate Agents with Agents. Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, Yangyang Shi, Vikas Chandra, Jürgen Schmidhuber. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
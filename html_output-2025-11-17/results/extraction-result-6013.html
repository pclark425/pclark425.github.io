<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6013 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6013</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6013</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-f2209eb5ac6747319a29b87dedabb97770be3243</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f2209eb5ac6747319a29b87dedabb97770be3243" target="_blank">Can large language models provide useful feedback on research papers? A large-scale empirical analysis</a></p>
                <p><strong>Paper Venue:</strong> NEJM AI</p>
                <p><strong>Paper TL;DR:</strong> An automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers shows that LLM-generated feedback can help researchers, but also identifies several limitations.</p>
                <p><strong>Paper Abstract:</strong> Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6013.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6013.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrospective Comment Matching Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-stage Retrospective Comment Extraction and Semantic Matching Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that (1) uses GPT-4 for extractive summarization to convert free-text reviews into enumerated comment lists (JSON), and (2) performs semantic matching of comment pairs via GPT-4 with a calibrated similarity rating, retaining matches rated '7: Strongly Related' or above for quantitative overlap analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated extractive summarization followed by semantic text matching (both implemented via GPT-4), validated by human verification annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Comment-level match identification using a similarity rating threshold (>=7/10). Pipeline accuracy measured by precision, recall, and F1 on held-out human-annotated samples for both extraction and matching stages.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>scientific peer review / manuscript feedback (cross-disciplinary: machine learning and multidisciplinary Nature journals)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not an explicit scientific theory — here the 'theory' object is the set of discrete critique points (comments) extracted from LLM-generated reviews that are compared to human reviewer comments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Extraction stage: precision 0.977, recall 0.960, F1 = 0.968 (n=639 feedbacks). Semantic matching stage: precision 0.777, recall 0.878, F1 = 0.824 (12,035 comment pairs, n=760 feedback-pair sample). Inter-annotator pairwise agreement 89.8% and F1 against majority 88.7% (on 800 sampled pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Validation performed on sampled feedback pairs from Nature family journals and ICLR review texts; human-annotated subsets (639 for extraction, 760 pairs for matching) used as ground-truth.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human annotators provided ground-truth labels for extraction and matching; pipeline performance reported relative to these human labels (high extraction F1, moderately high matching F1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>GPT-4 matching was initially lenient, necessitating calibrated similarity thresholds; retained-match threshold (>=7) is a design choice that may trade recall for precision; semantic matching remains imperfect (precision ~0.78).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6013.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6013.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrospective Overlap Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrospective Overlap / Hit-Rate Evaluation of LLM vs Human Reviewer Comments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative analysis measuring the proportion of LLM-generated critique points that match human reviewer points (hit rate), comparing LLM-vs-human and human-vs-human overlaps across large datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute hit rate = |A ∩ B| / |A| between comment sets A (LLM) and B (human reviewer), plus additional set-overlap metrics (Szymkiewicz-Simpson overlap coefficient, Jaccard index, Sørensen–Dice coefficient). Controlled comparisons where human-vs-human hit-rate uses only the first N comments to match LLM's comment count.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Hit rate (primary), Szymkiewicz-Simpson, Jaccard, Sørensen–Dice; stratified by journal/conference and by paper decision outcome (ICLR acceptance levels).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>scientific peer review / feedback evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Assessment of how much of the LLM-generated set of critique points corresponds to those raised by human reviewers (used as proxy for evaluating LLM-generated scientific 'claims' about manuscript quality).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Nature family journals: 57.55% of GPT-4 comments were raised by at least one human reviewer; per-individual-reviewer hit-rate ~30.85% (LLM vs individual human); human-vs-human per-individual hit-rate ~28.58%. ICLR: 77.18% of GPT-4 comments raised by at least one human reviewer; per-individual-reviewer hit-rate ~39.23%; human-vs-human ~35.25%. Overlap varied by venue and by paper quality (rejected ICLR papers had higher LLM-human overlap: LLM vs human 47.09% for rejected papers vs ~30% for top-accepted papers).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Nature family journals dataset (3,096 accepted papers, 8,745 human reviews) and ICLR dataset (1,709 papers, 6,505 human reviews) from 2022–2023.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>LLM-vs-human overlaps are comparable to human-vs-human overlaps (e.g., 30.85% vs 28.58% in Nature per-individual comparisons), suggesting similar consistency levels between LLM and human reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Hit-rate depends on extraction/matching pipeline thresholding; LLM review token truncation (first 6,500 tokens used) may miss content; comparisons limited to English top-venue papers and to critiques (focus on criticisms).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6013.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6013.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shuffling Specificity Test</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shuffling Null Model for Specificity of LLM Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A null-model experiment that swaps LLM-generated feedback across papers within the same journal/category (Nature) or year (ICLR) to test whether LLM feedback is generic or paper-specific, measuring resulting overlap with human reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Randomly re-pair GPT-4 feedback with a different paper from the same journal/category (Nature) or same conference year (ICLR) and recompute hit rates against that paper's human reviews; compare to non-shuffled hit rates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Change in hit rate (absolute drop) after shuffling; large drops indicate paper-specific feedback while small drops would indicate generic comments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>scientific peer review / feedback specificity</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Test of whether LLM-generated critique lists are tailored to the specific manuscript rather than being broadly generic advice.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Substantial drop in overlap after shuffling: Nature per-individual hit-rate dropped from 30.85% to 0.43% (also, 'at least one human reviewer' hit-rate dropped from 57.55% to 1.13% in another reported comparison); ICLR per-individual hit-rate dropped from 39.23% to 3.91% (and 'at least one reviewer' metric from 77.18% to a small percent). These large drops indicate LLM feedback is paper-specific rather than generic.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Same Nature family and ICLR datasets as above.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Shuffled LLM feedback yields minimal overlap with human reviewers, whereas original LLM feedback has overlap comparable to human-human — demonstrating that specificity is not an artifact of generic comments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Shuffling controlled within journal/category or year to be conservative, but residual similarity across papers in same niche could still produce non-zero overlap; dependent on matching pipeline thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6013.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6013.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aspects Annotation Schema</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>11-Aspect Annotation Schema for Categorizing Review Comments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-curated schema of 11 comment aspects (e.g., novelty, implications, additional experiments, ablation studies) used to compare topical emphases between LLM and human reviews, applied with human annotations on sampled comments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Random sampling of 500 ICLR papers; extract comments via the summarization pipeline for both LLM and human reviews; two ML-experienced annotators labeled each comment with any matching aspect(s) from the 11-aspect schema; relative frequencies compared.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Relative frequency ratios of each aspect in LLM vs human comments (e.g., how many times LLM raises 'implications' vs humans), plus prevalence visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>machine learning peer review content analysis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Categorization of critique points to reveal which scientific-feedback aspects LLMs emphasize versus humans (serves as a proxy for what kinds of theoretical or methodological concerns LLMs raise).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Notable differences: LLM comments on 'implications' 7.27x more frequently than humans; LLM comments on 'novelty' 10.69x less frequently than humans; humans are 6.71x more likely to request ablation experiments, while LLM is 2.19x more likely to request experiments on more datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>ICLR dataset; random sample of 500 papers; annotation tables in Supplementary (Supp. Tables 5–7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Direct frequency ratio comparisons between LLM and human annotations reveal divergent emphases, suggesting complementary roles.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Schema developed for ML-focused reviews (ICLR) and may not generalize to other fields; annotations performed by two annotators, potential subjectivity and inter-annotator variability not fully quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6013.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6013.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prospective User Study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prospective Survey Study of Researchers Evaluating LLM-Generated Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-subjects survey where 308 researchers from 110 US institutions uploaded their papers and rated the helpfulness, specificity, alignment, and potential utility of GPT-4-generated feedback on their own manuscripts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Online Gradio demo to deliver GPT-4 reviews; post-review 6-page survey collecting demographics, perceived alignment with human reviews, specificity, helpfulness, willingness to reuse, and qualitative feedback; participants compensated and IRB-approved.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Self-reported measures including helpful/very helpful percentages, perceived alignment with human reviewer concerns (none/partial/considerable/substantial), specificity comparisons, and intention-to-reuse rates.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>researcher perception of AI-provided manuscript feedback</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Evaluation of the perceived utility and trustworthiness of LLM-generated critique points (again, not scientific theories but evaluative judgments about manuscripts).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>57.4% (abstract) / over 50.3% (results section) of users found feedback helpful or very helpful; 82.4% found it more beneficial than feedback from at least some human reviewers (abstract); >70% reported at least 'partial alignment' between LLM feedback and expected significant points, and 35% reported considerable/substantial alignment; 50.5% willing to reuse the system; 65.3% thought LLM offered overlooked perspectives to some extent. Users reported limitations: lack of domain-specific depth and less in-depth critique of method design.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Participant sample: 308 researchers (AI and computational biology) who opted in; papers uploaded were required to be post-9/2021 to avoid GPT-4 training-set contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Users compared LLM feedback directly to human reviewer feedback; many found LLM feedback comparable to or better than some human reviewers but inferior to expert domain-specific human review in depth.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Self-selection bias of participants (likely interested in/comfortable with LLMs), limited fields (ML and computational biology), and participants asked to upload post-9/2021 papers (to avoid training-set overlap). Users noted LLM reviews can be vague and lack technical depth; GPT-4 in this instantiation could not interpret visual elements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GPT-4 Technical Report <em>(Rating: 2)</em></li>
                <li>Is the future of peer review automated? <em>(Rating: 1)</em></li>
                <li>Reviewergpt? an exploratory study on using large language models for paper reviewing <em>(Rating: 1)</em></li>
                <li>Gpt4 is slightly helpful for peer-review assistance: A pilot study <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6013",
    "paper_id": "paper-f2209eb5ac6747319a29b87dedabb97770be3243",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "Retrospective Comment Matching Pipeline",
            "name_full": "Two-stage Retrospective Comment Extraction and Semantic Matching Pipeline",
            "brief_description": "A pipeline that (1) uses GPT-4 for extractive summarization to convert free-text reviews into enumerated comment lists (JSON), and (2) performs semantic matching of comment pairs via GPT-4 with a calibrated similarity rating, retaining matches rated '7: Strongly Related' or above for quantitative overlap analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Automated extractive summarization followed by semantic text matching (both implemented via GPT-4), validated by human verification annotations.",
            "evaluation_criteria": "Comment-level match identification using a similarity rating threshold (&gt;=7/10). Pipeline accuracy measured by precision, recall, and F1 on held-out human-annotated samples for both extraction and matching stages.",
            "llm_model_name": "GPT-4",
            "theory_domain": "scientific peer review / manuscript feedback (cross-disciplinary: machine learning and multidisciplinary Nature journals)",
            "theory_description": "Not an explicit scientific theory — here the 'theory' object is the set of discrete critique points (comments) extracted from LLM-generated reviews that are compared to human reviewer comments.",
            "evaluation_results": "Extraction stage: precision 0.977, recall 0.960, F1 = 0.968 (n=639 feedbacks). Semantic matching stage: precision 0.777, recall 0.878, F1 = 0.824 (12,035 comment pairs, n=760 feedback-pair sample). Inter-annotator pairwise agreement 89.8% and F1 against majority 88.7% (on 800 sampled pairs).",
            "benchmarks_or_datasets": "Validation performed on sampled feedback pairs from Nature family journals and ICLR review texts; human-annotated subsets (639 for extraction, 760 pairs for matching) used as ground-truth.",
            "comparison_to_human": "Human annotators provided ground-truth labels for extraction and matching; pipeline performance reported relative to these human labels (high extraction F1, moderately high matching F1).",
            "limitations_or_challenges": "GPT-4 matching was initially lenient, necessitating calibrated similarity thresholds; retained-match threshold (&gt;=7) is a design choice that may trade recall for precision; semantic matching remains imperfect (precision ~0.78).",
            "uuid": "e6013.0",
            "source_info": {
                "paper_title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Retrospective Overlap Evaluation",
            "name_full": "Retrospective Overlap / Hit-Rate Evaluation of LLM vs Human Reviewer Comments",
            "brief_description": "Quantitative analysis measuring the proportion of LLM-generated critique points that match human reviewer points (hit rate), comparing LLM-vs-human and human-vs-human overlaps across large datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Compute hit rate = |A ∩ B| / |A| between comment sets A (LLM) and B (human reviewer), plus additional set-overlap metrics (Szymkiewicz-Simpson overlap coefficient, Jaccard index, Sørensen–Dice coefficient). Controlled comparisons where human-vs-human hit-rate uses only the first N comments to match LLM's comment count.",
            "evaluation_criteria": "Hit rate (primary), Szymkiewicz-Simpson, Jaccard, Sørensen–Dice; stratified by journal/conference and by paper decision outcome (ICLR acceptance levels).",
            "llm_model_name": "GPT-4",
            "theory_domain": "scientific peer review / feedback evaluation",
            "theory_description": "Assessment of how much of the LLM-generated set of critique points corresponds to those raised by human reviewers (used as proxy for evaluating LLM-generated scientific 'claims' about manuscript quality).",
            "evaluation_results": "Nature family journals: 57.55% of GPT-4 comments were raised by at least one human reviewer; per-individual-reviewer hit-rate ~30.85% (LLM vs individual human); human-vs-human per-individual hit-rate ~28.58%. ICLR: 77.18% of GPT-4 comments raised by at least one human reviewer; per-individual-reviewer hit-rate ~39.23%; human-vs-human ~35.25%. Overlap varied by venue and by paper quality (rejected ICLR papers had higher LLM-human overlap: LLM vs human 47.09% for rejected papers vs ~30% for top-accepted papers).",
            "benchmarks_or_datasets": "Nature family journals dataset (3,096 accepted papers, 8,745 human reviews) and ICLR dataset (1,709 papers, 6,505 human reviews) from 2022–2023.",
            "comparison_to_human": "LLM-vs-human overlaps are comparable to human-vs-human overlaps (e.g., 30.85% vs 28.58% in Nature per-individual comparisons), suggesting similar consistency levels between LLM and human reviewers.",
            "limitations_or_challenges": "Hit-rate depends on extraction/matching pipeline thresholding; LLM review token truncation (first 6,500 tokens used) may miss content; comparisons limited to English top-venue papers and to critiques (focus on criticisms).",
            "uuid": "e6013.1",
            "source_info": {
                "paper_title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Shuffling Specificity Test",
            "name_full": "Shuffling Null Model for Specificity of LLM Feedback",
            "brief_description": "A null-model experiment that swaps LLM-generated feedback across papers within the same journal/category (Nature) or year (ICLR) to test whether LLM feedback is generic or paper-specific, measuring resulting overlap with human reviews.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Randomly re-pair GPT-4 feedback with a different paper from the same journal/category (Nature) or same conference year (ICLR) and recompute hit rates against that paper's human reviews; compare to non-shuffled hit rates.",
            "evaluation_criteria": "Change in hit rate (absolute drop) after shuffling; large drops indicate paper-specific feedback while small drops would indicate generic comments.",
            "llm_model_name": "GPT-4",
            "theory_domain": "scientific peer review / feedback specificity",
            "theory_description": "Test of whether LLM-generated critique lists are tailored to the specific manuscript rather than being broadly generic advice.",
            "evaluation_results": "Substantial drop in overlap after shuffling: Nature per-individual hit-rate dropped from 30.85% to 0.43% (also, 'at least one human reviewer' hit-rate dropped from 57.55% to 1.13% in another reported comparison); ICLR per-individual hit-rate dropped from 39.23% to 3.91% (and 'at least one reviewer' metric from 77.18% to a small percent). These large drops indicate LLM feedback is paper-specific rather than generic.",
            "benchmarks_or_datasets": "Same Nature family and ICLR datasets as above.",
            "comparison_to_human": "Shuffled LLM feedback yields minimal overlap with human reviewers, whereas original LLM feedback has overlap comparable to human-human — demonstrating that specificity is not an artifact of generic comments.",
            "limitations_or_challenges": "Shuffling controlled within journal/category or year to be conservative, but residual similarity across papers in same niche could still produce non-zero overlap; dependent on matching pipeline thresholds.",
            "uuid": "e6013.2",
            "source_info": {
                "paper_title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Aspects Annotation Schema",
            "name_full": "11-Aspect Annotation Schema for Categorizing Review Comments",
            "brief_description": "A human-curated schema of 11 comment aspects (e.g., novelty, implications, additional experiments, ablation studies) used to compare topical emphases between LLM and human reviews, applied with human annotations on sampled comments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Random sampling of 500 ICLR papers; extract comments via the summarization pipeline for both LLM and human reviews; two ML-experienced annotators labeled each comment with any matching aspect(s) from the 11-aspect schema; relative frequencies compared.",
            "evaluation_criteria": "Relative frequency ratios of each aspect in LLM vs human comments (e.g., how many times LLM raises 'implications' vs humans), plus prevalence visualization.",
            "llm_model_name": "GPT-4",
            "theory_domain": "machine learning peer review content analysis",
            "theory_description": "Categorization of critique points to reveal which scientific-feedback aspects LLMs emphasize versus humans (serves as a proxy for what kinds of theoretical or methodological concerns LLMs raise).",
            "evaluation_results": "Notable differences: LLM comments on 'implications' 7.27x more frequently than humans; LLM comments on 'novelty' 10.69x less frequently than humans; humans are 6.71x more likely to request ablation experiments, while LLM is 2.19x more likely to request experiments on more datasets.",
            "benchmarks_or_datasets": "ICLR dataset; random sample of 500 papers; annotation tables in Supplementary (Supp. Tables 5–7).",
            "comparison_to_human": "Direct frequency ratio comparisons between LLM and human annotations reveal divergent emphases, suggesting complementary roles.",
            "limitations_or_challenges": "Schema developed for ML-focused reviews (ICLR) and may not generalize to other fields; annotations performed by two annotators, potential subjectivity and inter-annotator variability not fully quantified here.",
            "uuid": "e6013.3",
            "source_info": {
                "paper_title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Prospective User Study",
            "name_full": "Prospective Survey Study of Researchers Evaluating LLM-Generated Feedback",
            "brief_description": "A human-subjects survey where 308 researchers from 110 US institutions uploaded their papers and rated the helpfulness, specificity, alignment, and potential utility of GPT-4-generated feedback on their own manuscripts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Online Gradio demo to deliver GPT-4 reviews; post-review 6-page survey collecting demographics, perceived alignment with human reviews, specificity, helpfulness, willingness to reuse, and qualitative feedback; participants compensated and IRB-approved.",
            "evaluation_criteria": "Self-reported measures including helpful/very helpful percentages, perceived alignment with human reviewer concerns (none/partial/considerable/substantial), specificity comparisons, and intention-to-reuse rates.",
            "llm_model_name": "GPT-4",
            "theory_domain": "researcher perception of AI-provided manuscript feedback",
            "theory_description": "Evaluation of the perceived utility and trustworthiness of LLM-generated critique points (again, not scientific theories but evaluative judgments about manuscripts).",
            "evaluation_results": "57.4% (abstract) / over 50.3% (results section) of users found feedback helpful or very helpful; 82.4% found it more beneficial than feedback from at least some human reviewers (abstract); &gt;70% reported at least 'partial alignment' between LLM feedback and expected significant points, and 35% reported considerable/substantial alignment; 50.5% willing to reuse the system; 65.3% thought LLM offered overlooked perspectives to some extent. Users reported limitations: lack of domain-specific depth and less in-depth critique of method design.",
            "benchmarks_or_datasets": "Participant sample: 308 researchers (AI and computational biology) who opted in; papers uploaded were required to be post-9/2021 to avoid GPT-4 training-set contamination.",
            "comparison_to_human": "Users compared LLM feedback directly to human reviewer feedback; many found LLM feedback comparable to or better than some human reviewers but inferior to expert domain-specific human review in depth.",
            "limitations_or_challenges": "Self-selection bias of participants (likely interested in/comfortable with LLMs), limited fields (ML and computational biology), and participants asked to upload post-9/2021 papers (to avoid training-set overlap). Users noted LLM reviews can be vague and lack technical depth; GPT-4 in this instantiation could not interpret visual elements.",
            "uuid": "e6013.4",
            "source_info": {
                "paper_title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Is the future of peer review automated?",
            "rating": 1,
            "sanitized_title": "is_the_future_of_peer_review_automated"
        },
        {
            "paper_title": "Reviewergpt? an exploratory study on using large language models for paper reviewing",
            "rating": 1,
            "sanitized_title": "reviewergpt_an_exploratory_study_on_using_large_language_models_for_paper_reviewing"
        },
        {
            "paper_title": "Gpt4 is slightly helpful for peer-review assistance: A pilot study",
            "rating": 1,
            "sanitized_title": "gpt4_is_slightly_helpful_for_peerreview_assistance_a_pilot_study"
        }
    ],
    "cost": 0.01232875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Can large language models provide useful feedback on research papers? A large-scale empirical analysis.</h1>
<p>Weixin Liang ${ }^{1 <em>}$, Yuhui Zhang ${ }^{1 </em>}$, Hancheng Cao ${ }^{1 *}$, Binglu Wang ${ }^{2}$, Daisy Yi Ding ${ }^{3}$, Xinyu Yang ${ }^{4}$, Kailas Vodrahalli ${ }^{3}$, Siyu He ${ }^{3}$, Daniel Scott Smith ${ }^{6}$, Yian Yin ${ }^{4}$, Daniel A. McFarland ${ }^{6}$, and James Zou ${ }^{1,3,5+}$<br>${ }^{1}$ Department of Computer Science, Stanford University, Stanford, CA 94305, USA<br>${ }^{2}$ Kellogg School of Management, Northwestern University, Evanston, IL 60208, USA<br>${ }^{3}$ Department of Biomedical Data Science, Stanford University, Stanford, CA 94305, USA<br>${ }^{4}$ Department of Information Science, Cornell University, Ithaca, NY 14850, USA<br>${ }^{5}$ Department of Electrical Engineering, Stanford University, Stanford, CA 94305, USA<br>${ }^{6}$ Graduate School of Education, Stanford University, Stanford, CA 94305, USA<br>${ }^{+}$Correspondence should be addressed to: jamesz@stanford.edu<br>${ }^{6}$ these authors contributed equally to this work</p>
<h4>Abstract</h4>
<p>Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals ( 3,096 papers in total) and the ICLR machine learning conference ( 1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap $30.85 \%$ for Nature journals, $39.23 \%$ for ICLR) is comparable to the overlap between two human reviewers (average overlap $28.58 \%$ for Nature journals, $35.25 \%$ for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers (i.e., rejected ICLR papers; average overlap 43.80\%). We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of Al and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half ( $57.4 \%$ ) of the users found GPT-4 generated feedback helpful/very helpful and $82.4 \%$ found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations. For example, GPT-4 tends to focus on certain aspects of scientific feedback (e.g., 'add experiments on more datasets'), and often struggles to provide in-depth critique of method design. Together our results suggest that LLM and human feedback can complement each other. While human expert review is and should continue to be the foundation of rigorous scientific process, LLM feedback could benefit researchers, especially when timely expert feedback is not available and in earlier stages of manuscript preparation before peer-review.</p>
<h2>Introduction</h2>
<p>In the 1940s, Claude Shannon, while at Bell Laboratories, embarked on developing a mathematical framework of information and communication ${ }^{1}$. Throughout this pursuit, he was faced with the challenge of naming his novel measure and considered terms such as 'information' and 'uncertainty'. Shannon shared his work with John von Neumann, who quickly recognized the profound links between Shannon's work and statistical mechanics, and proposed what later anchored modern information theory: 'Information Entropy'2. Scientific progress often rests</p>
<p>on feedback and critique. Effective feedback among peer scientists not only elucidates and promotes the way new discoveries are made, interpreted, and communicated, but also catalyzes the emergence of new scientific paradigms by connecting individual insights, coordinating concurrent lines of thoughts, and stimulating constructive debates and disagreement ${ }^{3}$.</p>
<p>However, the process of providing timely, comprehensive, and insightful feedback on scientific research is often laborious, resource-intensive, and complex ${ }^{4}$. This complexity is exacerbated by the exponential growth in scholarly publications and the deepening specialization of scientific knowledge ${ }^{5,6}$. Traditional avenues, such as peer review and conference discussions, exhibit constraints in scalability, expertise accessibility, and promptness. For instance, it has been estimated that peer review - one of the most major channels of scientific feedback - costs over 100M researcher hours and $\$ 2.5 \mathrm{~B}$ US dollars in a single year ${ }^{7}$. Yet at the same time, it has been increasingly challenging to secure enough qualified reviewers who can provide high-quality feedback given the rapid growth in the number of submissions ${ }^{8-12}$. For example, the number of submissions to the $I C L R$ machine learning conference increased from 960 in 2018 to 4,966 in 2023.</p>
<p>While shortage of high-quality feedback presents a fundamental constraint on the sustainable growth of science overall, it also becomes a source of deepening scientific inequalities. Marginalized researchers, especially those from non-elite institutions or resource-limited regions, often face disproportionate challenges in accessing valuable feedback, perpetuating a cycle of systemic scientific inequality ${ }^{13,14}$.</p>
<p>Given these challenges, there is an urgent need for crafting scalable and efficient feedback mechanisms that can enrich and streamline the scientific feedback process. Adopting such advancements holds the promise of not just elevating the quality and scope of scientific research, given the concerning deceleration in scientific advancements ${ }^{15,16}$, but also of democratizing its access across the scientific community.</p>
<p>Large language models (LLMs) ${ }^{17-19}$, especially those powered by Transformer-based architectures and pretrained at immense scales, have opened up great potential in various applications ${ }^{20-23}$. While LLMs have made remarkable strides in various domains, the promises and perils of leveraging LLMs for scientific feedback remain largely unknown. Despite recent attempts that explore the potential uses of such tools in areas such as automating paper screening ${ }^{24}$, error identification ${ }^{25}$, and checklist verification ${ }^{26}$ 1, we lack large-scale empirical evidence on whether and how LLMs may be used to facilitate scientific feedback and augment current academic practices.</p>
<p>In this work, we present the first large-scale systematic analysis characterizing the potential reliability and credibility of leveraging LLM for generating scientific feedback. Specifically, we developed a GPT-4 based scientific feedback generation pipeline that takes the raw PDF of a paper and produces structured feedback (Fig. 1a). The system is designed to generate constructive feedback across various key aspects, mirroring the review structure of leading interdisciplinary journals ${ }^{27,28}$ and conferences ${ }^{29-33}$, including: 1) Significance and novelty, 2) Potential reasons for acceptance, 3) Potential reasons for rejection, and 4) Suggestions for improvement.</p>
<p>To characterize the informativeness of GPT-4 generated feedback, we conducted both a retrospective analysis and a prospective user study. In the retrospective analysis, we applied our pipeline on papers that had previously been assessed by human reviewers. We then compared the LLM feedback with the human feedback. We assessed the degree of overlap between key points raised by both sources to gauge the effectiveness and reliability of LLM feedback. Furthermore, we compared the topic distributions of LLM feedback and human feedback. To enable such analysis, we curated two complementary datasets containing full-text of papers, their meta information, and associated peer reviews after $2022^{2}$. The first dataset was sourced from Nature family journals, which are leading scientific journals covering multidisciplinary fields including biomedicine and basic sciences. Our second dataset was sourced from $I C L R$ (International Conference on Learning Representations), a leading computer science venue on artificial intelligence. This dataset, although narrower in scope, includes complete reviews for both accepted and rejected papers. These two datasets allowed us to evaluate the performance of LLM in generating scientific feedback across different types of scientific writing (e.g. across fields).</p>
<p>For the prospective user study, we developed a survey in which researchers were invited to evaluate the quality</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>of the feedback produced by our GPT-4 system on their authored papers. By analyzing researchers' perspectives on the helpfulness, reliability, and potential limitations of LLM feedback, we can gauge the acceptability and utility of the proposed approach in the manuscript improvement process, and understand stakeholder's subjective perceptions of the framework. Through recruitment over institute mailing lists, and contacting paper authors who put preprints on arXiv, we were able to collect survey responses from 308 researchers from 110 US institutions in the field of AI and computational biology that come from diverse education status, experience, and institutes.</p>
<h1>Results</h1>
<h2>Generating Scientific Feedback using LLM</h2>
<p>We developed an automated pipeline that utilizes OpenAI's GPT-4 ${ }^{19}$ to generate feedback on the full PDF of scientific papers. The pipeline first parses the entire paper from the PDF, then constructs a paper-specific prompt for GPT-4. This prompt is created by concatenating our designed instructions with the paper's title, abstract, figure and table captions, and other main text (Fig. 1a, Methods). The prompt is then fed into GPT-4, which generates the scientific feedback in a single pass. Further details and validations of the pipeline can be found in the Supplementary Information.</p>
<h2>Retrospective Evaluation</h2>
<p>To evaluate the quality of LLM feedback retrospectively, we systematically assess the content overplay between human feedback given to submitted manuscripts and the LLM feedback using two large-scale datasets. The first dataset, sourced from Nature family journals, includes 8,745 comments from human reviewers for 3,096 accepted papers across 15 Nature family journals, including Nature, Nature Biomedical Engineering, Nature Human Behaviour, and Nature Communications (Supp. Table 1, Methods). The second dataset comprises 6,505 comments from human reviewers for 1,709 papers from the International Conference on Learning Representations (ICLR), a leading venue for artificial intelligence research in computer science (Supp. Table 2, Methods). These two datasets complement each other: the first dataset (Nature portfolio journals) spans a broad range of prominent journals across various scientific disciplines and impact levels, thereby capturing both the universality and variations in human-based scientific feedback. The second dataset (ICLR) provides an in-depth perspective of scientific feedback within leading venues of a rapidly evolving field - machine learning. Importantly, this second dataset includes expert feedback on both accepted and rejected papers.</p>
<p>We developed a retrospective comment matching pipeline to evaluate the overlap between feedback from LLM and human reviewers (Fig. 1b, Methods). The pipeline first performs extractive text summarization ${ }^{34-37}$ to extract the comments from both LLM and human-written feedback. It then applies semantic text matching ${ }^{38-40}$ to identify shared comments between the two feedback sources. We validated the pipeline's accuracy through human verification, yielding an F1 score of $96.8 \%$ for extraction (Supp. Table 3a, Methods) and $82.4 \%$ for matching (Supp. Table 3b, Methods).</p>
<h2>LLM feedback significantly overlaps with human-generated feedback</h2>
<p>We began by examining the overlap between LLM feedback and human feedback on Nature family journal data (Supp. Table 1). More than half ( $57.55 \%$ ) of the comments raised by GPT-4 were raised by at least one human reviewer (Supp. Fig. 1a). This suggests a considerable overlap between LLM feedback and human feedback, indicating potential accuracy and usefulness of the system. When comparing LLM feedback with comments from each individual reviewer, approximately one third ( $30.85 \%$ ) of GPT-4 raised comments overlapped with comments from an individual reviewer (Fig. 2a). The degree of overlap between two human reviewers was similar (28.58\%), after controlling for the number of comments (Methods). Results were consistent across other overlapping metrics including Szymkiewicz-Simpson overlap coefficient, Jaccard index, Sørensen-Dice coefficient (Supp. Fig. 2). This indicates that the overlap between LLM feedback and human feedback is comparable to the overlap observed between two human reviewers. We further stratified these overlap results by academic journals (Fig. 2c). While the degree of overlap between LLM feedback and human comments varied across different academic journals within the Nature family — from $15.58 \%$ in Nature Communications Materials to $39.16 \%$ in Nature - the overlap</p>
<p>between LLM feedback and human feedback comments largely mirrored the overlap found between two human reviewers. The robustness of the finding further indicates that scientific feedback generated from LLM is similar to what researchers could get from peer reviewers.</p>
<p>In parallel experiments, we investigated the comment overlap between LLM feedback and human feedback on $I C L R$ papers data (Supp. Table 2), and the results were largely similar. A majority ( $77.18 \%$ ) of the comments raised by GPT-4 were also raised by at least one human reviewer (Supp. Fig. 1b), indicating considerable overlap between LLM feedback and human feedback. When comparing LLM feedback with comments from each individual reviewer, more than one third ( $39.23 \%$ ) of GPT-4 raised comments overlapped with comments from an individual reviewer (Fig. 2b). The overlap between two human reviewers was similar ( $35.25 \%$ ), after controlling for the number of comments (Methods, Supp. Fig. 2). We further stratified these overlap results by the decision outcomes of the papers (Fig. 2d). Similar to results over Nature family journals, we found that the overlap between LLM feedback and human feedback comments largely mirrored the overlap found between two human reviewers.</p>
<p>In addition, as $I C L R$ dataset includes both accepted and rejected papers, we conducted stratification analysis and found a correlation between worse acceptance decisions and larger overlap in $I C L R$ papers. Specifically, papers accepted with oral presentations (representing the top $5 \%$ of accepted papers) have an average overlap of $30.63 \%$ between LLM feedback and human feedback comments. The average overlap increases to $32.12 \%$ for papers accepted with a spotlight presentation (the top $25 \%$ of accepted papers), while rejected papers bear the highest average overlap at $47.09 \%$. A similar trend was observed in the overlap between two human reviewers: $23.54 \%$ for papers accepted with oral presentations (top 5\% accepted papers), $24.52 \%$ for papers accepted with spotlight presentations (top $25 \%$ accepted papers), and $43.80 \%$ for rejected papers. This suggests that rejected papers may have more apparent issues or flaws that both human reviewers and LLMs can consistently identify. Additionally, the increased overlap between LLM feedback and actual human reviewer feedback for rejected papers indicates that LLM feedback could be particularly constructive and formative for papers that require more substantial revisions to be accepted. Indeed, by raising these concerns earlier in the scientific process before review, these papers and the science they report may be improved.</p>
<h1>LLM could generate non-generic feedbacks.</h1>
<p>Is it possible that LLM merely generates generic feedback applicable to multiple papers? A potential null model is that LLM mostly produces generic feedback applicable to many papers. To test this hypothesis, we performed a shuffling experiment aimed at verifying the specificity and relevance of LLM generated feedback. For each paper in the Nature family journal data, the LLM feedback was shuffled for papers from the same journal and within the same Nature category (Methods). If the LLM were producing only generic feedback, we would observe no decrease in the pairwise overlap between shuffled LLM feedback and human feedback. In contrast, the pairwise overlap significantly decreased from $30.85 \%$ to $0.43 \%$ after shuffling (Fig. 2a). A similar drop from $39.23 \%$ to $3.91 \%$ was observed on $I C L R$ (Fig. 2b). These results suggest that LLM feedback is paper-specific.</p>
<h2>LLM is consistent with humans on major comments</h2>
<p>What characteristics do LLMs' comments exhibit? What are the distinctive features of the human comments that align with LLMs'? Here we evaluate the unique characteristics of comments generated by LLMs. Our analysis revealed that comments identified by multiple human reviewers are more likely to be echoed by LLMs. For instance, in the Nature family journal data (Fig. 2e), a comment raised by a single human reviewer had an $11.39 \%$ chance of being identified by LLMs. This probability increased to $20.67 \%$ for comments raised by two reviewers, and further to $31.67 \%$ for comments raised by three or more reviewers. A similar trend was observed in the $I C L R$ data (Fig. 2f), where the likelihood of LLMs identifying a comment increased from $15.39 \%$ for a single reviewer to $26.21 \%$ for two reviewers, and $39.33 \%$ for three or more reviewers. These findings suggest that LLMs are more likely to identify common issues or flaws that are consistently recognized by multiple human reviewers, compared to specific comments raised by a single reviewer. This alignment of LLM with human perspectives indicates its ability to identify what is generally considered as major or significant issues.</p>
<p>We further examined the likelihood of LLM comments overlapping with human feedback based on their position in the sequence, as earlier comments in human feedback (e.g. "concern 1") may represent more significant issues.</p>
<p>To this end, we divided each human reviewer's comment sequence into four quarters within the Nature journal data (Fig. 2g). Our findings suggest that comments raised in the first quarter of the review text are most likely (21.23\%) to overlap with LLM comments, with subsequent quarters revealing decreasing likelihoods ( $16.74 \%$ for the second quarter). Similar trends were observed in the ICLR papers data, where earlier comments in the sequence showed a higher probability of overlap with LLM comments (Fig. 2h). These findings further support that LLM tends to align with human perspectives on what is generally considered as major or significant issues.</p>
<h1>LLM feedback emphasizes certain aspects more than humans</h1>
<p>We next analyzed whether certain aspects of feedback are more/less likely to be raised by the LLM and human reviewers. We focus on ICLR for this analysis, as it's more homogeneous than Nature family journals, making it easier to categorize the main aspects of review. Drawing on existing research in peer review literature within the machine learning domain ${ }^{41-44}$, we developed a schema comprising 11 distinct aspects of comments. We then performed human annotation on a randomly sampled subset (Methods).</p>
<p>Fig. 3 presents the relative frequency of each of the 11 aspects of comments raised by humans and LLM. LLM comments on the implications of research 7.27 times more frequently than humans do. Conversely, LLM is 10.69 times less likely to comment on novelty than humans are. While both LLM and humans often suggest additional experiments, their focuses differ: humans are 6.71 times more likely than LLM to request more ablation experiments, whereas LLM is 2.19 times more likely than humans to request experiments on more datasets. These findings suggest that the emphasis put on certain aspects of comments varies between LLMs and human reviewers. This variation highlights the potential advantages that a human-AI collaboration could provide. Rather than having LLM fully automate the scientific feedback process, humans can raise important points that LLM may overlook. Similarly, LLM could supplement human feedback by providing more comprehensive comments.</p>
<h2>Prospective User Study and Survey</h2>
<p>Taken together, our retrospective evaluations above suggest that LLMs can generate scientific feedback that focuses on similar aspects as human reviewers. Yet, consistency in concerns is only one of the many factors contributing to the utility of scientific feedback. Recent studies in human-AI interaction have also identified additional factors that individuals may consider when evaluating and adopting AI-based tools ${ }^{21}$, prompting us to ask: how do scientific researchers respond to feedback generated by LLMs? Thus, we launched a survey study on 308 researchers from 110 US institutions who opted in to receive LLM-generated scientific feedback on their own papers, and were asked to evaluate its utility and performance. While our sampling approach is subject to biases of self-selection, the data can provide valuable insights and subjective perspectives from researchers complementing our retrospective analysis ${ }^{45,46}$. The results from the user study are illustrated in Fig. 4.</p>
<p>Our user study provides additional evidence that is largely consistent with retrospective evaluations. First, the user study survey results corroborate the findings from the retrospective evaluation on significant overlaps between LLM feedback and human feedback: more than $70 \%$ of participants think there is at least "partial alignment" between LLM feedback and what they think/would expect on the significant points and issues with their paper, and $35 \%$ of participants think the alignment is considerable or substantial (Fig. 4b). Second, the survey study further corroborates the findings from the automated evaluation on the ability of the language model to generate non-generic feedback: $32.9 \%$ of participants think our system-generated feedback is "less specific than many, but more specific than some peer reviewers", while $17.3 \%$ and $14 \%$ think it is "about as specific as peer reviewers", or "more specific than many peer reviewers", further corroborating that LLMs can generate non-generic reviews (Fig. 4d).</p>
<h2>Researchers find LLM feedback helpful</h2>
<p>Participants were surveyed about the extent to which they found the LLM feedback helpful in improving their work or understanding of a subject. The majority responded positively, with over $50.3 \%$ considering the feedback to be helpful, and $7.1 \%$ considering it to be very helpful (Fig. 4a). When compared with human feedback, while $17.5 \%$ of participants considered it to be inferior to human feedback, $41.9 \%$ considered it to be less helpful than many, but more helpful than some human feedback. Additionally, $20.1 \%$ considered it to be about the same level of helpfulness as human feedback, and $20.4 \%$ considered it to be even more helpful than human feedback (Fig. 4c). Our evaluation</p>
<p>also revealed that the perceptions of alignment and helpfulness were consistent across various demographic groups. Individuals from different educational backgrounds, ranging from undergraduate to postgraduate levels, found the feedback equally helpful and aligned with human feedback. Similarly, whether an experienced or a novice researcher, participants across the spectrum of publishing and reviewing experience reported similar levels of satisfaction and utility from the LLM based feedback, indicating that LLM based feedback tools could potentially be helpful to a diverse range of population (Supp. Fig. 3,4).</p>
<p>In line with the helpfulness of the system, $50.5 \%$ of survey participants further expressed their willingness to reuse the system (Fig. $4 g$ ). The participants expressed optimism about the potential improvements that continued use of the system could bring to the traditional human feedback process (Fig. $4 e, f$ ). They believe that the LLM technology can further refine the quality of reviews and possibly introduce new capabilities. Interestingly, the evaluation also revealed that participants believe authors are more likely to benefit from LLM based feedback than other stakeholders such as reviewers, and area chairs (Fig. 4h). Many participants envisioned a timely feedback tool for authors to receive comments on their papers in a timely manner, e.g. one participant wrote, "The review took five minutes and was of a reasonably high quality. This can tremendously help authors to receive a fast turnaround feedback and help in polishing their submissions." Another participant wrote, "After writing a paper or a review, GPT could help me gain another perspective to re-check the paper."</p>
<h1>LLM could generate novel feedback not mentioned by humans.</h1>
<p>Beyond generating feedback that aligns with humans, our results also suggest that LLM could potentially generate useful feedback that has not been mentioned by humans, e.g., $65.3 \%$ of participants think at least to some extent LLM feedback offers perspectives that have been overlooked or underemphasized by humans. Several participants mentioned that:</p>
<ul>
<li>"It consists more points, covering aspects which human may forget to think about."</li>
<li>"It actually highlighted a few limitations which human reviewers didn't point out to, but as authors we were aware of it and were expecting it. But this GPT figured out some of them, so that's interesting."</li>
<li>"The GPT-generated review suggested me to do visualization to make a more concrete case for interpretability. It also asked to address data privacy issues. Both are important, and human reviewers missed this point."</li>
</ul>
<h2>Limitations of LLM feedback</h2>
<p>Study participants also discussed limitations of the current system. The most important limitation is its ability to generate specific and actionable feedback, e.g.</p>
<ul>
<li>"Potential Reasons are too vague and not domain specific."</li>
<li>"GPT cannot provide specific technical areas for improvement, making it potentially difficult to improve the paper."</li>
<li>"The reviews crucially lacked much in-depth critique of model architecture and design, something actual reviewers would be able to comment on given their likely considerable experience in fields closely related to the focus of the paper."
As such, one future direction to improve the LLM based scientific feedback system is to nudge the system towards generating more concrete and actionable feedback, e.g. through pointing to specific missing work, experiments to add. As one participant nicely summarized:</li>
<li>"(large language model generated) reviews were less about the content and more about the testing regime as well as less ML details-focused, but this is okay as it still gave relevant and actionable advice on areas of improvement in terms of paper layout and presenting results. GPT-generated reviews are especially useful here when less-experience authors may leave out details on implementation and construction or forget to thoroughly explain testing regime by providing pointers on areas to polish the paper in, potentially decreasing the number of review cycles before publication."</li>
</ul>
<h1>Discussion</h1>
<p>In this study, we characterized the usefulness and reliability of LLM in scientific evaluation by building and evaluating an LLM-based scientific feedback generation framework. Through a combination of retrospective (comparing LLM feedback with human feedback from the peer review process) and prospective evaluation design (user study with researchers), we have seen a substantial level of overlap and positive user perceptions regarding the usefulness of LLM feedback. Furthermore, in evaluating user perceptions, we found that a majority of participants regarded LLM feedback as useful in the manuscript improvement process, and sometimes LLM could bring up novel points not covered by humans. The positive feedback from users highlights the potential value and utility of leveraging LLM feedback as a valuable resource for authors seeking constructive feedback and suggestions for enhancing their manuscripts. This could be especially helpful for researchers who lack access to timely quality feedback mechanisms, e.g., researchers from traditionally underprivileged regions who may not have resources to access conferences, or even peer review (their works are much more likely than those of "mainstream" researchers to get desk rejected by journals and thus seldom go through the peer review process ${ }^{14}$ ). For others, the framework could be used as a mechanism for authors to self-check and improve their work in a timely manner, especially in an age of exponentially growing scientific papers and increasing challenges to secure timely and quality peer reviewer feedback. Our analysis suggests that people from diverse educational backgrounds and publishing experience can find the LLM scientific feedback generation framework useful (Supp. Fig. 3,4).</p>
<p>Despite the potential of LLMs in providing timely and helpful scientific feedback, it is important to note that expert human feedback will still be the cornerstone of rigorous scientific evaluation. As demonstrated in our findings, our analysis reveals limitations of the framework, e.g., LLM is biased towards certain aspects of scientific feedback (e.g., "add experiments on more datasets"), and sometimes feels "generic" to the authors (while participants also indicate that quite often human reviewers are "generic"). While comparable and even better than some reviewers, the current LLM feedback cannot substitute specific and thoughtful human feedback by domain experts.</p>
<p>It is also important to note the potential misuse of LLM for scientific feedback. We argue that LLM feedback should be primarily used by researchers identify areas of improvements in their manuscripts prior to official submission. It is important that expert human reviewers should deeply engage with the manuscripts and provide independent assessment without relying on LLM feedback. Automatically generating reviews without thoroughly reading the manuscript would undermine the rigorous evaluation process that forms the bedrock of scientific progress.</p>
<p>More broadly, our study contributes to the recent discussions on the impacts of LLM and generative AI on existing work practices. Researchers have discussed the potential of LLM to improve productivity ${ }^{47,48}$, creativity ${ }^{49}$, and facilitate scientific discovery ${ }^{50}$. We envision that LLM and generative AI, if deployed responsibly, could also potentially bring a paradigm change to how researchers conduct research, collaborate, and provide evaluations, influencing the way science and technology advance. Our work brings a preliminary investigation into such potentials through a concrete prototype for scientific feedback.</p>
<p>There are several limitations to our study that are important to highlight. First, our results are based on one specific instantiation of scientific feedback from LLM, i.e., our framework is based on the GPT-4 model, enabled by a specific prompt. While we have spent significant efforts in improving the performance of our GPT-4 feedback pipeline (and achieved reasonable utility), the results should be interpreted as a lower bound, rather than an upper bound, on the potential of leveraging LLMs for scientific feedback. Moreover, our system only leverages zero-shot learning of GPT-4 without fine-tuning on additional datasets. Further, the architecture and prompt used in our study only represent one of the many possible forms of LLM-based scientific feedback. Aside from exploring other LLMs and conducting more sophisticated prompt engineering, future work could incorporate labeled datasets of "high quality scientific feedback" to further fine-tune the LLM, or prompt LLM to leverage tools (e.g., fact check API, algorithm analysis API) so that the feedback could be more detailed and method-specific. Nevertheless, our proposed framework proves to be helpful and aligns well with comments brought up by human reviewers, demonstrating the potential of incorporating LLM in the scientific evaluation process, echoing prior works arguing for AI in the scientific process ${ }^{50}$. Our retrospective evaluation used Nature family data and ICLR data. While these data cover a range of scientific fields, including biology, computer science, etc., and the ICLR dataset includes both accepted and</p>
<p>rejected papers, all studied papers are targeted at top venues in English. Future work should further evaluate the framework with more coverage. Our user study is limited in coverage of participant population and suffer from a self-selection issue. Current results are based on responses from researchers in machine learning and computational biology, and while we aim to ensure representativeness of researchers by reaching out to randomly sampled authors who upload preprints to arXiv in recent months, participants who opt into the study are likely to be interested and familiar with LLM or AI in general. Finally, the current version of the GPT-4 model we utilized does not possess the capability to understand or interpret visual data such as tables, graphs, and figures, which are integral components of scientific literature. Future iterations could explore integrating visual LLMs or specialized modules that can comprehend and critique visual elements, thereby offering a more comprehensive form of scientific feedback.</p>
<p>One direction for future work is to explore the extent to which the proposed approach can help identify and correct errors in scientific papers. This would involve artificially introducing various types of errors, including typos, mistakes in data analyses, and errors in mathematical equations. By evaluating whether LLM-generated feedback can effectively detect and rectify these errors, we can gain further insights into the system's ability to improve the overall accuracy and quality of scientific manuscripts. Furthermore, investigating the limitations and challenges associated with error detection and correction by LLM is crucial. This includes understanding the types of errors that may be more challenging for the model to detect and correct, as well as evaluating the potential impact of false positives or false negatives in the generated feedback. Such insights can inform the development of more robust and accurate AI-assisted review systems. Additionally, we intend to broaden the scope of evaluated scientific papers to include manuscripts written in languages other than English, or by authors for whom English is not their first language, in order to understand whether LLMs can provide useful feedback for such papers.</p>
<h1>Methods</h1>
<h2>Overview of the Nature Family Journals Dataset</h2>
<p>Several journals within the Nature group have adopted a transparent peer review policy, enabling authors to publish reviewers' comments alongside the accepted papers ${ }^{51}$. For instance, by 2021, approximately $46 \%$ of Nature authors opted to make their reviewer discussions public ${ }^{52}$. Our dataset comprises papers from 15 Nature family journals, published between January 1, 2022, and June 17, 2023. We sourced papers from 15 Nature family journals, focusing on those published between January 1, 2022, and June 17, 2023. Within this period, our dataset includes 773 accepted papers from Nature with 2,324 reviews, 810 sampled accepted papers from Nature Communications with 2,250 reviews, and many others. In total, our dataset includes 3,096 accepted papers and 8,745 reviews (Supp. Table 1). The data were sourced directly from the Nature website (https://nature.com/).</p>
<h2>Overview of the ICLR Dataset</h2>
<p>The International Conference on Learning Representations (ICLR) is a leading publication venue in the machine learning field. ICLR implements an open review policy, making reviews for all papers accessible, including those for rejected papers. Accepted papers at ICLR are categorized into Oral presentations (top 5\% of papers), Spotlight (top 25\%), and Poster presentations. In 2022, ICLR received 3,407 submissions, which increased to 4,966 by 2023. Using a stratified sampling method, we included 55 Oral (with 200 reviews), 173 Spotlight (664 reviews), 197 Poster ( 752 reviews), 213 rejected ( 842 reviews), and 182 withdrawn ( 710 reviews) papers from 2022. For 2023, we included 90 Oral ( 317 reviews), 200 Spotlight ( 758 reviews), 200 Poster ( 760 reviews), 212 rejected ( 799 reviews), and 187 withdrawn ( 703 reviews) papers. The dataset comprises 1709 papers and 6,506 reviews in total (Supp. Table 2). The paper PDFs and corresponding reviews were retrieved using the OpenReview API (https://docs.openreview.net/).</p>
<h2>Generating Scientific Feedbacks using LLM</h2>
<p>We prototyped a pipeline to generate scientific feedback using OpenAI's GPT-4 ${ }^{19}$ (Fig. 1a). The system's input was the academic paper in PDF format, which was then parsed with the machine-learning-based ScienceBeam PDF parser ${ }^{53}$. Given the token constraint of GPT-4, which allows 8,192 tokens for combined input and output, the initial 6,500 tokens of the extracted title, abstract, figure and table captions, and main text were utilized to construct</p>
<p>the prompt for GPT-4 (Supp. Fig. 5). This token limit exceeds the 5,841.46-token average of $I C L R$ papers and covers over half of the 12,444.06-token average for Nature family journal papers (Supp. Table 4). For clarity and simplicity, we instructed GPT-4 to generate a structured outline of scientific feedback. Following the reviewer report instructions from machine learning conferences ${ }^{29-33}$ and Nature family journals ${ }^{27,28}$, we provided specific instructions to generate four feedback sections: significance and novelty, potential reasons for acceptance, potential reasons for rejection, suggestions for improvement (Supp. Fig. 12). The feedback for each paper was generated by GPT-4 in a single pass.</p>
<h1>Retrospective Extraction and Matching of Comments from Scientific Feedback</h1>
<p>To evaluate the overlap between LLM feedback and human feedback, we developed a two-stage comment matching pipeline (Supp. Fig. 6). In the first stage, we employed an extractive text summarization approach ${ }^{34-37}$. Each feedback text, either from the LLM or a human, was processed by GPT-4 to extract a list of the points of comments raised in the text (see prompt in Supp. Fig. 13). The output was structured in a JSON (JavaScript Object Notation) format. Within this format, each JSON key assigns an ID to a specific point, while the corresponding value details the content of the point (Supp. Fig. 13). We focused on criticisms in the feedback, as they provide direct feedback to help authors improve their papers ${ }^{34}$. The second stage focused on semantic text matching ${ }^{38-40}$. Here, we input both the JSON-formatted feedback from the LLM and the human into GPT-4. The LLM then generated another JSON output where each key identified a pair of matching point IDs and the associated value provided the explanation for the match. Given that our preliminary experiments showed GPT-4's matching to be lenient, we introduced a similarity rating mechanism. In addition to identifying corresponding pairs of matched comments, GPT-4 was also tasked with self-assessing match similarities on a scale from 5 to 10 (Supp. Fig. 14). We observed that matches graded as " 5 . Somewhat Related" or " 6 . Moderately Related" introduced variability that did not always align with human evaluations. Therefore, we only retained matches ranked " 7 . Strongly Related" or above for subsequent analyses.</p>
<p>We validated our retrospective comment matching pipeline using human verification. In the extractive text summarization stage, we randomly selected 639 pieces of scientific feedback, including 150 from the LLM and 489 from human contributors. Two co-authors assessed each feedback and its corresponding list of extracted comments, identifying true positives (correctly extracted comments), false negatives (missed relevant comments), and false positives (incorrectly extracted or split comments). This process resulted in an F1 score of 0.968 , with a precision of 0.977 and a recall of 0.960 (Supp. Table 3a), demonstrating the accuracy of the extractive summarization stage. For the semantic text matching stage, we sampled 760 pairs of scientific feedbacks: 332 comparing GPT to Human feedback and 428 comparing Human feedbacks. Each feedback pair was processed to enumerate all potential pairings of their extracted comment lists, resulting in 12,035 comment pairs. Three co-authors independently determined whether the comment pairs matched, without referencing the pipeline's predictions. Comparing these annotations with pipeline outputs yielded an F1 score of 0.824 , a recall of 0.878 , and a precision of 0.777 (Supp. Table 3b). To assess inter-annotator agreement, we collected three annotations for 800 randomly selected comment pairs. Given the prevalence of non-matches, we employed stratified sampling, drawing 400 pairs identified as matches by the pipeline and 400 as non-matches. We then calculated pairwise agreement between annotations and the F1 score for each annotation against the majority consensus. The data showed $89.8 \%$ pairwise agreement and an F1 score of $88.7 \%$, indicating the reliability of the semantic text matching stage.</p>
<h2>Evaluating Specificity of LLM Feedback through Review Shuffling</h2>
<p>To evaluate the specificity of the feedback generated by the LLM, we compared the overlap between human-authored feedback and shuffled LLM feedback. For papers published in the Nature journal family, the LLM-generated feedback for a given paper was randomly paired with human feedback for a different paper from the same journal and Nature root category. These categories included physical sciences, earth and environmental sciences, biological sciences, health sciences, and scientific community and society. If a paper was classified under multiple categories, the shuffle algorithm paired it with another paper that spanned the same categories. For the $I C L R$ dataset, we compared human feedback for a paper with LLM feedback for a different paper, randomly selected from the same</p>
<p>conference year, either $I C L R 2022$ or $I C L R 2023$. This shuffling procedure was designed to test the null hypothesis: if LLM mostly produces generic feedback applicable to many papers, then there would be little drop in the pairwise overlap between LLM feedback and the comments from each individual reviewer after the shuffling.</p>
<h1>Overlap Metrics for Retrospective Evaluations and Control</h1>
<p>In the retrospective evaluation, we assessed the pairwise overlap of both GPT-4 vs. Human and Human vs. Human in terms of hit rate (Fig. 2). The hit rate, defined as the proportion of comments in set $A$ that match those in set $B$, was calculated as follows:</p>
<p>$$
\text { Hit Rate }=\frac{|A \cap B|}{|A|}
$$</p>
<p>To facilitate a direct comparison between the hit rates of GPT-4 vs. Human and Human vs. Human, we controlled for the number of comments when measuring the hit rate for Human vs. Human. Specifically, we considered only the first $N$ comments made by the first human (i.e., the human comments used as set $A$ ) for matching, where $N$ is the number of comments made by GPT-4 for the same paper. The results, with and without this control, were largely similar across both the $I C L R$ dataset for different decision outcomes (Supp. Fig. 7,10) and the Nature family journals dataset across different journals (Supp. Fig. 8,9,10). To examine the robustness of the results across different set overlap metrics, we also evaluated three additional metrics: the Szymkiewicz-Simpson overlap coefficient, the Jaccard index, and the Sørensen-Dice coefficient. These were calculated as follows:</p>
<p>$$
\begin{aligned}
\text { Szymkiewicz-Simpson Overlap Coefficient } &amp; =\frac{|A \cap B|}{\min (|A|,|B|)} \
\text { Jaccard Index } &amp; =\frac{|A \cap B|}{|A \cup B|} \
\text { Sørensen-Dice Coefficient } &amp; =\frac{2|A \cap B|}{|A|+|B|}
\end{aligned}
$$</p>
<p>Results on these additional metrics suggest that our findings are robust on different set overlap metrics: the overlap GPT-4 vs. Human appears comparable to those of Human vs. Human both with and without control for the number of comments (Supp. Fig. 2).</p>
<h2>Characterizing the comment aspects in human and LLM feedback</h2>
<p>We curated an annotation schema of 11 key aspects to identify and measure the prevalence of these aspects in human and LLM feedback. This schema was developed with a focus on the $I C L R$ dataset, due to its specialized emphasis on Machine Learning. Each aspect was defined by its underlying emphasis, such as novelty, research implications, suggestions for additional experiments, and more. The selection of these 11 key aspects was based on a combination of the common schemes identified in the literature within the machine learning domain ${ }^{41-44}$, comments from machine learning researchers, and initial exploration by the annotators. From the $I C L R$ dataset, a random sample of 500 papers was selected to ensure a broad yet manageable representation. Using our extractive text summarization pipeline, we extracted lists of comments from both the LLM and human feedback for each paper. Each comment was then annotated according to our predefined schema, identifying any of the 11 aspects it represented (Supp. Table 5,6,7). To ensure annotation reliability, two researchers with a background in machine learning performed the annotations.</p>
<h2>Prospective User Study and Survey</h2>
<p>We conduct a prospective user study to further validate the effectiveness of leveraging LLMs to generate scientific feedback. To facilitate our user study, we launched an online Gradio demo ${ }^{55}$ of the aforementioned generation pipeline, accessible at a public URL (Supp. Fig. 11). Users are prompted to upload a research paper in its original PDF format, after which the system delivers the review to user's email. We ask users to only upload papers published after 9/2021 to ensure the papers are never seen by GPT-4 during training (the cutdown date of GPT-4 training corpora is $9 / 2021$ ). We have also incorporated an ethics statement to discourage the direct use of LLM content for any</p>
<p>review-related tasks. After the review is generated and sent to users, users are asked to fill a 6-page survey (Figure 4), which includes 1) author background information, 2) review situation in author's area, 3) general impression of LLM review, 4) detailed evaluation of LLM review, 5) comparison with human review, and 6) additional questions and feedback, which systematically investigates human evaluations of different aspects of LLM reviews. The survey takes around 15-20 minutes and users will be compensated with $\$ 20$. We recruit the participants through 1) relevant institute mailing lists, and 2) reaching out to all authors who have published at least one preprint on arXiv in the field of computer science and computational biology during January to March, 2023, provided their email contact information is available in the first three pages of the PDF. The study has been approved by Stanford University's Institutional Review Board.</p>
<h1>Acknowledgements</h1>
<p>We thank S. Eyuboglu, D. Jurafsky, and M. Bernstein for their guidance and helpful discussions. J.Z. is supported by the National Science Foundation (CCF 1763191 and CAREER 1942926), the US National Institutes of Health (P30AG059307 and U01MH098953) and grants from the Silicon Valley Foundation and the Chan-Zuckerberg Initiative. H.C. is supported by the Stanford Interdisciplinary Graduate Fellowship.</p>
<h2>Code Availability</h2>
<p>The codes can be accessed at https://github.com/Weixin-Liang/LLM-scientific-feedback.</p>
<h2>References</h2>
<ol>
<li>Shannon, C. E. A mathematical theory of communication. The Bell system technical journal 27, 379-423 (1948).</li>
<li>Tribus, M. \&amp; McIrvine, E. C. Energy and information. Sci. Am. 225, 179-190 (1971).</li>
<li>Kuhn, T. S. The structure of scientifi revolutions. The Un Chic. Press. 2, 90 (1962).</li>
<li>Horbach, S. P. \&amp; Halffman, W. The changing forms and expectations of peer review. Res. integrity peer review 3, 1-15 (2018).</li>
<li>Price, D. J. D. S. Little science, big science (Columbia University Press, 1963).</li>
<li>Jones, B. F. The burden of knowledge and the "death of the renaissance man": Is innovation getting harder? The Rev. Econ. Stud. 76, 283-317 (2009).</li>
<li>Aczel, B., Szaszi, B. \&amp; Holcombe, A. O. A billion-dollar donation: estimating the cost of researchers' time spent on peer review. Res. Integr. Peer Rev. 6, 1-8 (2021).</li>
<li>Alberts, B., Hanson, B. \&amp; Kelner, K. L. Reviewing peer review (2008).</li>
<li>Björk, B.-C. \&amp; Solomon, D. The publishing delay in scholarly peer-reviewed journals. J. informetrics 7, 914-923 (2013).</li>
<li>Lee, C. J., Sugimoto, C. R., Zhang, G. \&amp; Cronin, B. Bias in peer review. J. Am. Soc. for information Sci. Technol. 64, 2-17 (2013).</li>
<li>Kovanis, M., Porcher, R., Ravaud, P. \&amp; Trinquart, L. The global burden of journal peer review in the biomedical literature: Strong imbalance in the collective enterprise. PloS one 11, e0166387 (2016).</li>
<li>Shah, N. B. Challenges, experiments, and computational solutions in peer review. Commun. ACM 65, 76-87 (2022).</li>
<li>Bourdieu, P. Cultural reproduction and social reproduction. In Knowledge, education, and cultural change, 71-112 (Routledge, 2018).</li>
<li>
<p>Merton, R. K. The matthew effect in science: The reward and communication systems of science are considered. Science 159, 56-63 (1968).</p>
</li>
<li>
<p>Chu, J. S. \&amp; Evans, J. A. Slowed canonical progress in large fields of science. Proc. Natl. Acad. Sci. 118, e2021636118 (2021).</p>
</li>
<li>Bloom, N., Jones, C. I., Van Reenen, J. \&amp; Webb, M. Are ideas getting harder to find? Am. Econ. Rev. 110, $1104-1144$ (2020).</li>
<li>Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, 1877-1901 (2020).</li>
<li>Ouyang, L. et al. Training language models to follow instructions with human feedback. Adv. Neural Inf. Process. Syst. 35, 27730-27744 (2022).</li>
<li>OpenAI. GPT-4 Technical Report. ArXiv abs/2303.08774 (2023).</li>
<li>Ayers, J. W. et al. Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum. JAMA internal medicine (2023).</li>
<li>Lee, M. et al. Evaluating human-language model interaction. arXiv preprint arXiv:2212.09746 (2022).</li>
<li>Kung, T. H. et al. Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. PLoS digital health 2, e0000198 (2023).</li>
<li>Terwiesch, C. Would chat gpt3 get a wharton mba? a prediction based on its performance in the operations management course. Mack Inst. for Innov. Manag. at Whart. Sch. Univ. Pennsylvania (2023).</li>
<li>Schulz, R. et al. Is the future of peer review automated? BMC Res. Notes 15, 1-5 (2022).</li>
<li>Liu, R. \&amp; Shah, N. B. Reviewergpt? an exploratory study on using large language models for paper reviewing. arXiv preprint arXiv:2306.00622 (2023).</li>
<li>Robertson, Z. Gpt4 is slightly helpful for peer-review assistance: A pilot study. arXiv preprint arXiv:2307.05492 (2023).</li>
<li>Nature. How to Write a Report. https://www.nature.com/nature/for-referees/how-to-write-a-report. Accessed: 21 September 2023.</li>
<li>Nature Communications. Writing Your Report. https://www.nature.com/ncomms/for-reviewers/ writing-your-report. Accessed: 21 September 2023.</li>
<li>Rogers, A. \&amp; Augenstein, I. How to review for ACL Rolling Review. https://aclrollingreview.org/reviewertutorial (2021).</li>
<li>Association for Computational Linguistics. ACL'23 Peer Review Policies. https://2023.aclweb.org/blog/ review-acl23/ (2023).</li>
<li>Association for Computational Linguistics. ACL-IJCNLP 2021 Instructions for Reviewers. https://2021.aclweb. org/blog/instructions-for-reviewers/ (2021).</li>
<li>International Conference on Machine Learning. ICML 2023 Reviewer Tutorial. https://icml.cc/Conferences/ 2023/ReviewerTutorial (2023).</li>
<li>Nicholas, K. A. \&amp; Gordon, W. S. A quick guide to writing a solid peer review. Eos, Transactions Am. Geophys. Union 92, 233-234 (2011).</li>
<li>Luhn, H. P. The automatic creation of literature abstracts. IBM J. research development 2, 159-165 (1958).</li>
<li>Edmundson, H. P. New methods in automatic extracting. J. ACM (JACM) 16, 264-285 (1969).</li>
<li>Mihalcea, R. \&amp; Tarau, P. Textrank: Bringing order into text. In Proceedings of the 2004 conference on empirical methods in natural language processing, 404-411 (2004).</li>
<li>Erkan, G. \&amp; Radev, D. R. Lexrank: Graph-based lexical centrality as salience in text summarization. J. artificial intelligence research 22, 457-479 (2004).</li>
<li>
<p>Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K. \&amp; Harshman, R. Indexing by latent semantic analysis. J. Am. society for information science 41, 391-407 (1990).</p>
</li>
<li>
<p>Socher, R., Huang, E., Pennin, J., Manning, C. D. \&amp; Ng, A. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. Adv. neural information processing systems 24 (2011).</p>
</li>
<li>Bowman, S. R., Angeli, G., Potts, C. \&amp; Manning, C. D. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326 (2015).</li>
<li>Birhane, A. et al. The values encoded in machine learning research. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT '22, 173-184, DOI: 10.1145/3531146. 3533083 (Association for Computing Machinery, New York, NY, USA, 2022).</li>
<li>Smith, J. J., Amershi, S., Barocas, S., Wallach, H. \&amp; Wortman Vaughan, J. Real ml: Recognizing, exploring, and articulating limitations of machine learning research. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, 587-597 (2022).</li>
<li>Koch, B., Denton, E., Hanna, A. \&amp; Foster, J. G. Reduced, reused and recycled: The life of a dataset in machine learning research. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) (2021).</li>
<li>Scheuerman, M. K., Hanna, A. \&amp; Denton, E. Do datasets have politics? disciplinary values in computer vision dataset development. Proc. ACM on Human-Computer Interact. 5, 1-37 (2021).</li>
<li>Meyer, B. D., Mok, W. K. \&amp; Sullivan, J. X. Household surveys in crisis. J. Econ. Perspectives 29, 199-226 (2015).</li>
<li>Ross, M. B. et al. Women are credited less in science than men. Nature 608, 135-145 (2022).</li>
<li>Noy, S. \&amp; Zhang, W. Experimental evidence on the productivity effects of generative artificial intelligence. Available at SSRN 4375283 (2023).</li>
<li>Peng, S., Kalliamvakou, E., Cihon, P. \&amp; Demirer, M. The impact of ai on developer productivity: Evidence from github copilot. arXiv preprint arXiv:2302.06590 (2023).</li>
<li>Epstein, Z. et al. Art and the science of generative ai. Science 380, 1110-1111 (2023).</li>
<li>Wang, H. et al. Scientific discovery in the age of artificial intelligence. Nature 620, 47-60 (2023).</li>
<li>Nature will publish peer review reports as a trial. Nature 578, 8, DOI: 10.1038/d41586-020-00309-9 (2020).</li>
<li>Nature is trialling transparent peer review - the early results are encouraging. Nature 603, 8, DOI: 10.1038/ d41586-022-00493-w (2022).</li>
<li>Ecer, D. \&amp; Maciocci, G. Sciencebeam—using computer vision to extract pdf data. Elife Blog Post (2017). [Online; accessed 2023-Sep-8].</li>
<li>Goodman, S. N., Berlin, J., Fletcher, S. W. \&amp; Fletcher, R. H. Manuscript quality before and after peer review and editing at annals of internal medicine. Annals internal medicine 121, 11-21 (1994).</li>
<li>Abid, A. et al. An online platform for interactive feedback in biomedical machine learning. Nat. Mach. Intell. 2, 86-88 (2020).</li>
<li>Collins, E., Augenstein, I. \&amp; Riedel, S. A supervised approach to extractive summarisation of scientific papers. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), 195-205 (2017).</li>
<li>Nuijten, M. B., Hartgerink, C. H., Van Assen, M. A., Epskamp, S. \&amp; Wicherts, J. M. The prevalence of statistical reporting errors in psychology (1985-2013). Behav. research methods 48, 1205-1226 (2016).</li>
<li>Zhou, Y., Beltagy, I., Bethard, S., Cotterell, R. \&amp; Chakraborty, T. ACL pubcheck. https://github.com/acl-org/ aclpubcheck.</li>
<li>
<p>Zhang, J., Zhang, H., Deng, Z. \&amp; Roth, D. Investigating fairness disparities in peer review: A language model enhanced approach. arXiv preprint arXiv:2211.06398 (2022).</p>
</li>
<li>
<p>Hosseini, M. \&amp; Horbach, S. P. Fighting reviewer fatigue or amplifying bias? considerations and recommendations for use of chatgpt and other large language models in scholarly peer review. Res. Integr. Peer Rev. 8, 4 (2023).</p>
</li>
<li>Verharen, J. P. Chatgpt identifies gender disparities in scientific peer review. bioRxiv 2023-07 (2023).</li>
</ol>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Characterizing the capability of LLM in providing helpful feedback to researchers. a, Pipeline for generating LLM scientific feedback using GPT-4. Given a PDF, we parse and extract the paper's title, abstract, figure and table captions, and main text to construct the prompt. We then prompt GPT-4 to provide structured comments with four sections, following the feedback structure of leading interdisciplinary journals and conferences: significance and novelty, potential reasons for acceptance, potential reasons for rejection, and suggestions for improvement. b, Retrospective analysis of LLM feedback on 3,096 Nature family papers and 1,709 ICLR papers. We systematically compare LLM feedback with human feedback using a two-stage comment matching pipeline. The pipeline first performs extractive text summarization to extract the points of comments raised in LLM and human-written feedback respectively, and then performs semantic text matching to match the points of shared comments between LLM and human feedback. c, Prospective user study survey with 308 researchers from 110 US institutions in the field of AI and computational biology. Each researcher uploaded a paper they authored, and filled out a survey on the LLM feedback generated for them.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Retrospective analysis of LLM and human scientific feedback. a, Retrospective overlap analysis between feedback from the LLM versus individual human reviewers on papers submitted to Nature Family Journals. Approximately one third ( $30.85 \%$ ) of GPT-4 raised comments overlap with the comments from an individual reviewer (hit rate). "GPT-4 (shuffle)" indicates feedback from GPT-4 for another randomly chosen paper from the same journal and category. As a null model, if LLM mostly produces generic feedback applicable to many papers, then there would be little drop in the pairwise overlap between LLM feedback and the comments from each individual reviewer after the shuffling. In contrast, the hit rate drops substantially from $57.55 \%$ to $1.13 \%$ after shuffling, indicating that the LLM feedback is paper-specific. b, In the International Conference on Learning Representations (ICLR), more than one third (39.23\%) of GPT-4 raised comments overlap with the comments from an individual reviewer. The shuffling experiment shows a similar result, indicating that the LLM feedback is paper-specific. c-d, The overlap between LLM feedback and human feedback appears comparable to the overlap observed between two human reviewers across Nature family journals (c) $(r=0.80, P=3.69 \times 10^{-4}$ ) and across $I C L R$ decision outcomes (d) $\left(r=0.98, P=3.28 \times 10^{-3}\right)$. e-f, Comments raised by multiple human reviewers are disproportionately more likely to be hit by GPT-4 on Nature Family Journals (e) and ICLR (f). The X-axis indicates the number of reviewers raising the comment. The Y-axis indicates the likelihood that a human reviewer comment matches a GPT-4 comment (GPT-4 recall rate). g-h, Comments presented at the beginning of a reviewer's feedback are more likely to be identified by GPT-4 on Nature Family Journals (g) and ICLR (h). The X-axis indicates a comment's position in the sequence of comments raised by the human reviewer. Error bars represent $95 \%$ confidence intervals. ${ }^{<em>} \mathrm{P}&lt;0.05,{ }^{</em> <em>} \mathrm{P}&lt;0.01,{ }^{</em> * <em>} \mathrm{P}&lt;0.001$, and ${ }^{</em> * * *} \mathrm{P}&lt;0.0001$.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. LLM based feedback emphasizes certain aspects more than humans. LLM comments on the implications of research 7.27 times more frequently than human reviewers. Conversely, LLM is 10.69 times less likely to comment on novelty compared to human reviewers. While both LLM and humans often suggest additional experiments, their focuses differ: human reviewers are 6.71 times more likely than LLM to request additional ablation experiments, whereas LLM is 2.19 times more likely than humans to request experiments on more datasets. Circle size indicates the prevalence of each aspect in human feedback.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Human study of LLM and human review feedback ( $n=308$ ). a-b, LLM generated feedback is generally helpful and has substantial overlaps with actual feedback from human reviewers. c-d, Compared to human feedback, LLM feedback is slightly less helpful and less specific. e-f, Users generally believe that the LLM feedback system can improve the accuracy and thoroughness of reviews, and reduce the workload of reviewers. g, Most users intend to use or potentially use the LLM feedback system again. h, Users believe that the LLM feedback system mostly helps authors, followed by reviewers and editors / area chairs. Numbers are percentages (\%).</p>
<h1>Supplementary Information</h1>
<h2>Additional Related Work</h2>
<p>The use of AI tools in aiding the scientific publication process has garnered considerable attention. Algorithms have been developed to summarize paper contents ${ }^{56}$, detect inaccurately reported p -values ${ }^{57}$, rectify citation errors ${ }^{58}$, and identify fairness disparities ${ }^{59}$. Recent advances in LLMs like ChatGPT and GPT-4 have intensified interest in leveraging these technologies for scientific feedback. There are some exploratory and unpublished studies. Hosseini et al. conducted a small-scale qualitative investigation to gauge ChatGPT's effectiveness in the peer review process ${ }^{60}$. Similarly, Robertson et al. involved 10 participants to assess GPT-4's benefits in aiding peer review ${ }^{26}$. Liu et al. demonstrated that GPT-4 could identify paper errors, verify checklists, and compare paper quality through analysis of 10-20 computer science papers ${ }^{25}$. Verharen et al. utilized ChatGPT to analyze 200 published neuroscience papers and uncovered gender disparities in scientific peer review ${ }^{61}$.</p>
<p>Our study differs from the existing literature in two key aspects. First, we provide a large-scale empirical analysis, in contrast to the small-scale exploratory analysis conducted in previous efforts. Our dataset includes 3,096 papers from the Nature family of journals and 1,709 papers from the ICLR conference, spanning a wide range of scientific disciplines. We also incorporate 308 human responses from 110 US institutions obtained through a prospective user study. Secondly, while most previous works only present qualitative results, our work provides a systematic quantitative assessment. This involves both overlap analyses and human evaluation metrics, providing a comprehensive analysis of the promise and limitations of LLMs in providing scientific feedback.</p>
<p>Supplementary Table 1. Summary of papers and their associated reviews sampled from 15 Nature family journals.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Journal</th>
<th style="text-align: center;">Count</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Papers</td>
<td style="text-align: center;">Reviews</td>
</tr>
<tr>
<td style="text-align: left;">Nature</td>
<td style="text-align: center;">773</td>
<td style="text-align: center;">2324</td>
</tr>
<tr>
<td style="text-align: left;">Nature Communications</td>
<td style="text-align: center;">810</td>
<td style="text-align: center;">2250</td>
</tr>
<tr>
<td style="text-align: left;">Communications Earth \&amp; Environment</td>
<td style="text-align: center;">299</td>
<td style="text-align: center;">807</td>
</tr>
<tr>
<td style="text-align: left;">Communications Biology</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">526</td>
</tr>
<tr>
<td style="text-align: left;">Communications Physics</td>
<td style="text-align: center;">174</td>
<td style="text-align: center;">464</td>
</tr>
<tr>
<td style="text-align: left;">Communications Medicine</td>
<td style="text-align: center;">123</td>
<td style="text-align: center;">343</td>
</tr>
<tr>
<td style="text-align: left;">Nature Ecology \&amp; Evolution</td>
<td style="text-align: center;">113</td>
<td style="text-align: center;">332</td>
</tr>
<tr>
<td style="text-align: left;">Nature Structural \&amp; Molecular Biology</td>
<td style="text-align: center;">110</td>
<td style="text-align: center;">290</td>
</tr>
<tr>
<td style="text-align: left;">Communications Chemistry</td>
<td style="text-align: center;">101</td>
<td style="text-align: center;">279</td>
</tr>
<tr>
<td style="text-align: left;">Nature Cell Biology</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">233</td>
</tr>
<tr>
<td style="text-align: left;">Nature Human Behaviour</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">211</td>
</tr>
<tr>
<td style="text-align: left;">Communications Materials</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">181</td>
</tr>
<tr>
<td style="text-align: left;">Nature Immunology</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">165</td>
</tr>
<tr>
<td style="text-align: left;">Nature Microbiology</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">174</td>
</tr>
<tr>
<td style="text-align: left;">Nature Biomedical Engineering</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">166</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">3096</td>
<td style="text-align: center;">8745</td>
</tr>
</tbody>
</table>
<p>Supplementary Table 2. Summary of ICLR papers and their associated reviews sampled from the years 2022 and 2023, grouped by decision.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Decision Category</th>
<th style="text-align: center;">ICLR 2022</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ICLR 2023</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"># of Papers</td>
<td style="text-align: center;"># of Reviews</td>
<td style="text-align: center;"># of Papers</td>
<td style="text-align: center;"># of Reviews</td>
</tr>
<tr>
<td style="text-align: left;">Accept (Oral) - notable-top-5\%</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">317</td>
</tr>
<tr>
<td style="text-align: left;">Accept (Spotlight) - notable-top-25\%</td>
<td style="text-align: center;">173</td>
<td style="text-align: center;">664</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">758</td>
</tr>
<tr>
<td style="text-align: left;">Accept (Poster)</td>
<td style="text-align: center;">197</td>
<td style="text-align: center;">752</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">760</td>
</tr>
<tr>
<td style="text-align: left;">Reject after author rebuttal</td>
<td style="text-align: center;">213</td>
<td style="text-align: center;">842</td>
<td style="text-align: center;">212</td>
<td style="text-align: center;">799</td>
</tr>
<tr>
<td style="text-align: left;">Withdrawn after reviews</td>
<td style="text-align: center;">182</td>
<td style="text-align: center;">710</td>
<td style="text-align: center;">187</td>
<td style="text-align: center;">703</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">820</td>
<td style="text-align: center;">3168</td>
<td style="text-align: center;">889</td>
<td style="text-align: center;">3337</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ For a more detailed literature review, see supplementary information.
${ }^{2}$ Focusing data after 2022 avoids bias introduced by 'testing on the training set', since GPT-4, the LLM we used, is trained on data up to Sep $2021^{19}$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4652 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4652</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4652</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-258564769</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.05091v2.pdf" target="_blank">Knowledge-enhanced Agents for Interactive Text Games</a></p>
                <p><strong>Paper Abstract:</strong> Communication via natural language is a key aspect of machine intelligence, and it requires computational models to learn and reason about world concepts, with varying levels of supervision. Significant progress has been made on fully-supervised non-interactive tasks, such as question-answering and procedural text understanding. Yet, various sequential interactive tasks, as in text-based games, have revealed limitations of existing approaches in terms of coherence, contextual awareness, and their ability to learn effectively from the environment. In this paper, we propose a knowledge-injection framework for improved functional grounding of agents in text-based games. Specifically, we consider two forms of domain knowledge that we inject into learning-based agents: memory of previous correct actions and affordances of relevant objects in the environment. Our framework supports two representative model classes: reinforcement learning agents and language model agents. Furthermore, we devise multiple injection strategies for the above domain knowledge types and agent architectures, including injection via knowledge graphs and augmentation of the existing input encoding strategies. We experiment with four models on the 10 tasks in the ScienceWorld text-based game environment, to illustrate the impact of knowledge injection on various model configurations and challenging task settings. Our findings provide crucial insights into the interplay between task properties, model architectures, and domain knowledge for interactive contexts.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4652.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4652.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-based multiple-choice LM agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuned RoBERTa-large encoder used as a single-step action selector in text games by scoring candidate actions (multiple-choice formulation); evaluated with and without injected memory (MCA) and affordance knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge-enhanced Agents for Interactive Text Games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RoBERTa (multiple-choice agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Encoder-only LM (roberta-large) treated the current game state as a "question" and each candidate action concatenated to the state as an input choice; the LM is fine-tuned offline on correct next-action selection, and at inference scores all valid actions (top-p sampling used to pick actions to avoid loops).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>roberta-large</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>ScienceWorld (text-based Science tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Ten elementary-school science tasks (one representative subtask per task) from ScienceWorld that require multi-step action sequences (e.g., find non-living object, power a light bulb, grow a plant).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term episodic action memory (MCA) appended to LM input; implemented as an action-history window</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw textual list/sequence of past correct actions (strings). For MCA variants the model sees up to a sliding-window of previous correct actions (implemented as text appended to input).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Updated online during the episode: whenever the environment returns a positive reward the action is appended to the MCA; RoBERTa uses a sliding window (window size = 5) so only the most recent correct actions are concatenated to the input.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Memory is retrieved by concatenation into the LM input (the LM attends to the appended action-history tokens during scoring); no separate retrieval module or external KV store was used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average cumulative reward (across the 10 chosen ScienceWorld subtasks): 12.31 (when using MCA alone, i.e., memory appended), compared to baseline 11.43 (no MCA). Reported per-task values available in Table 2 (e.g., some tasks showed large gains while others did not).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline average cumulative reward (no MCA, but other variants such as affordance pretraining may be present) = 11.43 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Systematic ablations compared baseline, affordance-only (aff), MCA-only (mca), and affordance+MCA (aff ‚äï mca). RoBERTa benefited more from affordances (avg 16.86) than from MCA alone (avg 12.31); MCA alone gave modest aggregate improvements (~8% relative) but helped only on a subset of tasks (3/10). Affordance pretraining (auxiliary QA on commonsense utility triples) was used rather than naive concatenation because affordance triples are too numerous for LM input.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Input-length limits constrain how much history/knowledge can be concatenated; naive concatenation of large affordance lists is infeasible. MCA can only include actions after they are confirmed correct by the environment (i.e., delayed availability). Gains from MCA are task-dependent and modest compared to affordances. RoBERTa can be slow (required many steps in some tasks) to leverage affordance hints compared to instruction-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Preserve and inject only past correct actions (MCA) rather than all past actions to reinforce successful strategies. Use a sliding-window for LM inputs to respect token limits. For large affordance knowledge, prefer auxiliary pretraining/adaptation on a utilities subset of a commonsense KG rather than concatenating long triple lists. Combine affordance pretraining with MCA where beneficial, but prioritize affordances for RoBERTa.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-enhanced Agents for Interactive Text Games', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4652.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4652.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Swift (Flan-T5) agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Swift instruction-tuned Seq2Seq LM agent (Flan-T5 based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned seq2seq LM (Flan-T5-base / Swift) that conditions on concatenated state information (including an explicit action history field by design) and generates the next action; evaluated both with and without the built-in last-10-action history.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge-enhanced Agents for Interactive Text Games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Swift (Flan-T5-based Seq2Seq agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Instruction-tuned Seq2Seq model (flan-T5-base) trained in a sequence-to-sequence manner: input contains structured state fields (desc, step number, score, action history, observation, inventory, visited rooms, prompt) and target is the correct action; naturally includes the last-ten-actions history in the input in the original Swift design.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>flan-T5-base (Swift)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>ScienceWorld (text-based Science tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same set of ten ScienceWorld subtasks requiring multi-step procedural actions (e.g., electricity, classification, biology tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term episodic action history (fixed-length recent-history field); by default stores last 10 actions with rewards (action-history field in the input).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw textual action history lines with associated reward annotations (e.g., 'go to outside (+16) -> You move to the outside').</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Updated after each environment step: the action history field is updated to include the latest action and its reward; retains up to the last ten actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Memory is placed explicitly in the Seq2Seq input so the decoder attends to it when generating the next action; no separate retrieval mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>When the Swift design included the 10-action history (mca variant), the average cumulative reward reported = 24.65 (Table 2, Swift m column). On some tasks the affordance+history variant achieved strong gains (e.g., task 3 electricity improved substantially).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>When action-history was omitted (baseline variant used for comparability to other models), the average cumulative reward = 27.86 (Table 2, Swift baseline). Eliminating the 10-action history proved advantageous for Swift on average (baseline without history > with history).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors ablated inclusion vs omission of Swift's action-history and also tested adding affordances and affordance+history. They report that for Swift, omitting the action history often improved performance; affordances still helped (aff and aff ‚äï mca variants sometimes outperformed baseline), and certain tasks (e.g., Electricity) saw large gains with affordances. The paper emphasizes model-specific outcomes: Swift benefits less (or is harmed) by naive inclusion of full action history compared to other LM setups.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Including full action-history (10 previous actions) can introduce noisy or harmful context for instruction-tuned LMs (Swift), sometimes decreasing performance; memory can interact badly with model architecture and training regime. Token budget and design of the input fields affect whether memory helps. The authors caution that historical context should be tailored (e.g., only correct actions) and that including full histories may confuse the model.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Do not assume that including a long action-history always helps‚Äîtest with and without history per architecture. For Swift specifically, removing the 10-action history produced better baseline performance; if history is used, consider restricting to correct actions or otherwise filtering history. Combine affordance knowledge injection (added into input) with careful handling of history; prefer pretraining/adaptation for large external knowledge instead of naive concatenation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-enhanced Agents for Interactive Text Games', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Swift-Sage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks. <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models. <em>(Rating: 2)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances. <em>(Rating: 2)</em></li>
                <li>Keep CALM and explore: Language models for action generation in text-based games. <em>(Rating: 2)</em></li>
                <li>Grounding large language models in interactive environments with online reinforcement learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4652",
    "paper_id": "paper-258564769",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [
        {
            "name_short": "RoBERTa-agent",
            "name_full": "RoBERTa-based multiple-choice LM agent",
            "brief_description": "A fine-tuned RoBERTa-large encoder used as a single-step action selector in text games by scoring candidate actions (multiple-choice formulation); evaluated with and without injected memory (MCA) and affordance knowledge.",
            "citation_title": "Knowledge-enhanced Agents for Interactive Text Games",
            "mention_or_use": "use",
            "agent_name": "RoBERTa (multiple-choice agent)",
            "agent_description": "Encoder-only LM (roberta-large) treated the current game state as a \"question\" and each candidate action concatenated to the state as an input choice; the LM is fine-tuned offline on correct next-action selection, and at inference scores all valid actions (top-p sampling used to pick actions to avoid loops).",
            "llm_model_name": "roberta-large",
            "game_or_benchmark_name": "ScienceWorld (text-based Science tasks)",
            "task_description": "Ten elementary-school science tasks (one representative subtask per task) from ScienceWorld that require multi-step action sequences (e.g., find non-living object, power a light bulb, grow a plant).",
            "memory_used": true,
            "memory_type": "short-term episodic action memory (MCA) appended to LM input; implemented as an action-history window",
            "memory_representation": "Raw textual list/sequence of past correct actions (strings). For MCA variants the model sees up to a sliding-window of previous correct actions (implemented as text appended to input).",
            "memory_update_mechanism": "Updated online during the episode: whenever the environment returns a positive reward the action is appended to the MCA; RoBERTa uses a sliding window (window size = 5) so only the most recent correct actions are concatenated to the input.",
            "memory_retrieval_mechanism": "Memory is retrieved by concatenation into the LM input (the LM attends to the appended action-history tokens during scoring); no separate retrieval module or external KV store was used.",
            "performance_with_memory": "Average cumulative reward (across the 10 chosen ScienceWorld subtasks): 12.31 (when using MCA alone, i.e., memory appended), compared to baseline 11.43 (no MCA). Reported per-task values available in Table 2 (e.g., some tasks showed large gains while others did not).",
            "performance_without_memory": "Baseline average cumulative reward (no MCA, but other variants such as affordance pretraining may be present) = 11.43 (Table 2).",
            "has_performance_comparison": true,
            "ablation_or_analysis": "Systematic ablations compared baseline, affordance-only (aff), MCA-only (mca), and affordance+MCA (aff ‚äï mca). RoBERTa benefited more from affordances (avg 16.86) than from MCA alone (avg 12.31); MCA alone gave modest aggregate improvements (~8% relative) but helped only on a subset of tasks (3/10). Affordance pretraining (auxiliary QA on commonsense utility triples) was used rather than naive concatenation because affordance triples are too numerous for LM input.",
            "challenges_or_limitations": "Input-length limits constrain how much history/knowledge can be concatenated; naive concatenation of large affordance lists is infeasible. MCA can only include actions after they are confirmed correct by the environment (i.e., delayed availability). Gains from MCA are task-dependent and modest compared to affordances. RoBERTa can be slow (required many steps in some tasks) to leverage affordance hints compared to instruction-tuned models.",
            "best_practices_or_recommendations": "Preserve and inject only past correct actions (MCA) rather than all past actions to reinforce successful strategies. Use a sliding-window for LM inputs to respect token limits. For large affordance knowledge, prefer auxiliary pretraining/adaptation on a utilities subset of a commonsense KG rather than concatenating long triple lists. Combine affordance pretraining with MCA where beneficial, but prioritize affordances for RoBERTa.",
            "uuid": "e4652.0",
            "source_info": {
                "paper_title": "Knowledge-enhanced Agents for Interactive Text Games",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Swift (Flan-T5) agent",
            "name_full": "Swift instruction-tuned Seq2Seq LM agent (Flan-T5 based)",
            "brief_description": "An instruction-tuned seq2seq LM (Flan-T5-base / Swift) that conditions on concatenated state information (including an explicit action history field by design) and generates the next action; evaluated both with and without the built-in last-10-action history.",
            "citation_title": "Knowledge-enhanced Agents for Interactive Text Games",
            "mention_or_use": "use",
            "agent_name": "Swift (Flan-T5-based Seq2Seq agent)",
            "agent_description": "Instruction-tuned Seq2Seq model (flan-T5-base) trained in a sequence-to-sequence manner: input contains structured state fields (desc, step number, score, action history, observation, inventory, visited rooms, prompt) and target is the correct action; naturally includes the last-ten-actions history in the input in the original Swift design.",
            "llm_model_name": "flan-T5-base (Swift)",
            "game_or_benchmark_name": "ScienceWorld (text-based Science tasks)",
            "task_description": "Same set of ten ScienceWorld subtasks requiring multi-step procedural actions (e.g., electricity, classification, biology tasks).",
            "memory_used": true,
            "memory_type": "short-term episodic action history (fixed-length recent-history field); by default stores last 10 actions with rewards (action-history field in the input).",
            "memory_representation": "Raw textual action history lines with associated reward annotations (e.g., 'go to outside (+16) -&gt; You move to the outside').",
            "memory_update_mechanism": "Updated after each environment step: the action history field is updated to include the latest action and its reward; retains up to the last ten actions.",
            "memory_retrieval_mechanism": "Memory is placed explicitly in the Seq2Seq input so the decoder attends to it when generating the next action; no separate retrieval mechanism.",
            "performance_with_memory": "When the Swift design included the 10-action history (mca variant), the average cumulative reward reported = 24.65 (Table 2, Swift m column). On some tasks the affordance+history variant achieved strong gains (e.g., task 3 electricity improved substantially).",
            "performance_without_memory": "When action-history was omitted (baseline variant used for comparability to other models), the average cumulative reward = 27.86 (Table 2, Swift baseline). Eliminating the 10-action history proved advantageous for Swift on average (baseline without history &gt; with history).",
            "has_performance_comparison": true,
            "ablation_or_analysis": "Authors ablated inclusion vs omission of Swift's action-history and also tested adding affordances and affordance+history. They report that for Swift, omitting the action history often improved performance; affordances still helped (aff and aff ‚äï mca variants sometimes outperformed baseline), and certain tasks (e.g., Electricity) saw large gains with affordances. The paper emphasizes model-specific outcomes: Swift benefits less (or is harmed) by naive inclusion of full action history compared to other LM setups.",
            "challenges_or_limitations": "Including full action-history (10 previous actions) can introduce noisy or harmful context for instruction-tuned LMs (Swift), sometimes decreasing performance; memory can interact badly with model architecture and training regime. Token budget and design of the input fields affect whether memory helps. The authors caution that historical context should be tailored (e.g., only correct actions) and that including full histories may confuse the model.",
            "best_practices_or_recommendations": "Do not assume that including a long action-history always helps‚Äîtest with and without history per architecture. For Swift specifically, removing the 10-action history produced better baseline performance; if history is used, consider restricting to correct actions or otherwise filtering history. Combine affordance knowledge injection (added into input) with careful handling of history; prefer pretraining/adaptation for large external knowledge instead of naive concatenation.",
            "uuid": "e4652.1",
            "source_info": {
                "paper_title": "Knowledge-enhanced Agents for Interactive Text Games",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Swift-Sage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks.",
            "rating": 2,
            "sanitized_title": "swiftsage_a_generative_agent_with_fast_and_slow_thinking_for_complex_interactive_tasks"
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models.",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances.",
            "rating": 2,
            "sanitized_title": "do_as_i_can_not_as_i_say_grounding_language_in_robotic_affordances"
        },
        {
            "paper_title": "Keep CALM and explore: Language models for action generation in text-based games.",
            "rating": 2,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        },
        {
            "paper_title": "Grounding large language models in interactive environments with online reinforcement learning.",
            "rating": 1,
            "sanitized_title": "grounding_large_language_models_in_interactive_environments_with_online_reinforcement_learning"
        }
    ],
    "cost": 0.01116675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Knowledge-enhanced Agents for Interactive Text Games
17 Dec 2023</p>
<p>Jiarui Zhang 
Filip Ilievski 
Jonathan Francis </p>
<p>Information
USA</p>
<p>USA, USA</p>
<p>Knowledge-enhanced Agents for Interactive Text Games
17 Dec 2023C7C57CA00073D560B7356E1539AEE18F10.1145/3587259.3627561arXiv:2305.05091v2[cs.CL]Text-based GamesKnowledge InjectionInteractive Task LearningNatural Language Communication
Communication via natural language is a key aspect of machine intelligence, and it requires computational models to learn and reason about world concepts, with varying levels of supervision.Significant progress has been made on fully-supervised non-interactive tasks, such as question-answering and procedural text understanding.Yet, various sequential interactive tasks, as in text-based games, have revealed limitations of existing approaches in terms of coherence, contextual awareness, and their ability to learn effectively from the environment.In this paper, we propose a knowledgeinjection framework for improved functional grounding of agents in text-based games.Specifically, we consider two forms of domain knowledge that we inject into learning-based agents: memory of previous correct actions and affordances of relevant objects in the environment.Our framework supports two representative model classes: reinforcement learning agents and language model agents.Furthermore, we devise multiple injection strategies for the above domain knowledge types and agent architectures, including injection via knowledge graphs and augmentation of the existing input encoding strategies.We experiment with four models on the 10 tasks in the ScienceWorld text-based game environment, to illustrate the impact of knowledge injection on various model configurations and challenging task settings.Our findings provide crucial insights into the interplay between task properties, model architectures, and domain knowledge for interactive contexts.</p>
<p>INTRODUCTION</p>
<p>Communication through natural language is a crucial aspect of machine intelligence [7].The recent progress of computational language models (LMs) has enabled strong performance on tasks with limited interaction, like question-answering and procedural text understanding [6,20,24].Recognizing that interactivity is an essential aspect of communication, the community has turned its attention towards training and evaluating agents in interactive fiction (IF) environments, like text-based games, which provide a unique testing ground for investigating the reasoning abilities of LMs and the potential for AI agents to perform multi-step real-world tasks in a constrained environment.For instance, in Figure 1, an agent must pick a fruit in the living room and place it in a blue box in the kitchen.Text-based games use text instead of graphics, sounds, or animations to create interactive stories, and can include adventure, puzzle-solving, and role-playing themes; text-based games allow us to study models' abilities to perform functional grounding, separate from, e.g., the problem of multimodal grounding that is inherent in more-complex robot simulation environments [11].Recently developed text-based games, such as TextWorld [9] and ScienceWorld [30], have quickly become popular, inspiring a variety of methods.To succeed in these games, agents must manage their knowledge, reason, and generate language-based actions that produce desired and predictable changes in the game world.IF games can be formulated as Partially Observable Markov Decision Processes (POMDPs), a category of sequential decision-making challenges under uncertainty.POMDPs encompass scenarios with only partially observable states and where the effects of actions are uncertain.Thus, IF games can be modeled using reinforcement learning (RL)-with states, actions, observations, transitions, and rewards [21].Observations correspond to text descriptions from environment and states are based on descriptions of agent and item locations, inventory contents, and surroundings.Given their natural language formulation, text-based games can also be tackled by LM approaches.The pros and cons of these two modeling paradigms are complementary.RL approaches function online and offer the advantage of modeling multistep transitions, but they can become challenging to optimize if reward structure and state information lack sufficient signals for effective learning.LMs offer flexibility in choosing subsequent actions, possess vast semantic knowledge, and can be advantageous for generating high-level, natural language instructions; yet, they operate within rigorous constraints in input size and do not support multi-step interactions natively.</p>
<p>Prior work has shown that RL-and LM-based agents struggle to reason about or to explain science concepts in IF environments [30], which raises questions about these models' ability to generalize to unseen situations beyond what has been observed during training [19].For example, while tasks such as 'retrieving a known substance's melting (or boiling) point' may be relatively simple, 'determining an unknown substance's melting (or boiling) point in a specific environment' can be challenging for these models.To improve generalization, it may be effective to incorporate world knowledge, e.g., about object affordances; yet, no prior work has investigated this direction.In addition, existing models struggle to learn effectively from environmental feedback.For instance, when examining the conductivity of a specific substance, the agent must understand that it has already obtained the necessary wires and the particular substance so that it then proceeds to locate a power source.Therefore, there is a need for a framework that can analyze and evaluate the effectiveness of different types of knowledge and knowledge-injection methods for text-based game agents.</p>
<p>In this paper, we design such a framework to augment existing text-based game agents with additional knowledge.We perform knowledge injection for two complementary paradigms based on training objectives: (1) online policy optimization through rewards, including pure RL [13] and enhanced RL with Knowledge Graphs (KGs) [1], and (2) single-step offline prediction, including both pretrained LM [23] and instructions-tuned [22] LM.We consider these two model classes because they are representative of the existing approaches for text-based games, which allows us to investigate how different model paradigms respond to knowledge injection techniques.We experiment with two types of additional knowledgenamely, task history and object affordances knowledge.We evaluate the effectiveness of our proposed framework on the diverse set of 10 elementary school science tasks of the ScienceWorld environment [30].The results illustrate that knowledge injection exerts a more favorable influence on single-step offline prediction models, i.e., LMs.Also, adding affordance knowledge is more beneficial than historical knowledge.Our contributions are as follows:</p>
<p>(1) We investigate the role of knowledge injection in learning-based agents for semi-Markov interactive text games.We specifically focus on injecting memory about previous correct actions and the affordances of the relevant objects in the agent's scene.(2) We integrate our injection strategies in two model paradigms, each with two variants: RL ('pure' RL and KG-enhanced RL) and language modeling (pre-trained and instructions-tuned).We devise multiple injection strategies to enrich the informationas part of existing inputs, as new inputs, or as KG relations.(3) We perform experiments on diverse tasks of ScienceWorld environment to provide insights on the impact of including affordance knowledge and action memory across different architectures, tasks, and knowledge-injection strategies.Our extensive experiments advance the understanding of how external knowledge can produce better action selection in text-based games.</p>
<p>RELATED WORK</p>
<p>Reinforcement Learning for Text-based Games has been a popular idea due to the conventional formulation of text-based games as Markov decision processes.A common challenge in these games is the combinatorially large action space, which makes it difficult to find a good policy.Carta et al. [5] proposed an approach to achieve alignment through functional grounding, where an agent uses an LM as a policy to solve goals through online RL.Madotto et al. [26] introduced a new exploration and imitation-based agent to play text-based games, which can be seen as a testbed for language understanding and generation.The proposed method uses the exploration approach of Go-Explore [10] for solving games and trains a policy to imitate trajectories with high rewards.eXploit-Then-eXplore (XTX) [28] is a multi-stage episodic control algorithm that separates exploitation and exploration into distinct policies, guiding the agent's action selection at different phases within a single episode.Yao et al. [32] proposed a Contextual Action LM to generate a compact set of action candidates at each game state and combine it with an RL agent to re-rank the generated action candidates.The Deep Reinforcement Relevance Network (DRRN) model [13] uses a separate Gated Recurrent Unit (GRU) for processing action text into a vector which is used to estimate a joint Q-Value Q(o, a) over the observation  and each action .Our work injects knowledge into the DRRN model to enhance agents' understanding of the game world.While these works have typically relied on the memory of the single previous action taken, regardless of its utility, our approach distinguishes itself by taking into account the memory of all previous actions that generated a positive reward.Thus, our agents obtain better performance by using this information to reinforce correct decision-making and avoid repeating past mistakes.LMs for Text-based Games used in works such as Swift [22], Re-Act [33], and SayCan [4] have revealed the feasibility of autonomous decision-making agents.Swift [22] is a model that takes into account the environment state and the history of the last ten actions as input strings for the LM.Additionally, they pursued model training through a supervised approach.ReAct [33] enables LMs to generate subgoals within action planning by incorporating a virtual 'think' action.This method necessitates human annotators to furnish instances of 'thinking, ' outlining subsequent subgoals and furnishing comprehensive action trajectories.SayCan [4] integrates an LM and a value function, aligning with grounding affordances.Using historical and current context as textual inputs, SayCan generates a ranked list of actions, grounding LMs through value functions reflecting action success likelihood.While Swift and SayCan retain a record of action history, the contribution of this information is not systematically studied.Moreover, they do not include world knowledge like object affordances.Knowledge-injection in Text-based Game Agents has been used to enhance the performance of RL and LM agents.Ahn et al. [4] identify potential actions using an LM and assign scores to these actions based on their likelihood of success in a given environment, which can be seen as an implicit affordance information.</p>
<p>Swift [22] introduced an additional layer of knowledge by incorporating the history of the previous ten actions within the episode.</p>
<p>Several works [1,2,15,31] have used KGs as an extra knowledge source to provide a structured representation of the game world, which can be used to guide agents' decision-making.</p>
<p>FRAMEWORK FOR KNOWLEDGE INJECTION IN TEXT-BASED GAME AGENTS</p>
<p>In most text-based games, the agent's input is comprised of three primary elements: the observation of the environment (obv), the contents of the agent's inventory (inv), and the task description (desc).These elements give the agent the context to make informed decisions and progress through the game.Based on these inputs, the agent is presented with a set of valid actions that it can perform, such as moving to a new location, interacting with objects in the environment, or using items in its inventory.Through these interactions, agents explore the game world, solve puzzles, and advance the story.In this section, we detail approaches for improving agents' downstream performance, thereby improving agents' coherence in action generation, their contextual awareness, and their abilities to learn effectively from the interactive environment.We consider two types of knowledge for enriching the inputs and two representative model classes as subjects for knowledge injection with their corresponding variants and knowledge-injection strategies.</p>
<p>Input Enrichment with Extra Knowledge</p>
<p>We expect that the raw inputs from the environment (observation, inventory, and task description) make it challenging for the agent to act coherently and learn from its mistakes.To improve the coherence and learning process, we enrich the apparent inputs with two complementary knowledge types: action memory and affordances.Action Memory.Historical knowledge is necessary for an AI agent to learn how to predict future steps based on a sequence of steps that it has taken previously.The historical knowledge could be in the form of all past actions picked by the model or the sequence of correct actions chosen by the model.Our analysis shows that preserving the past correct actions is a superior approach because it helps to reinforce successful strategies and prevent the model from repeating unsuccessful ones.Hence, we preserve the memory of previous correct actions (MCA) taken by the agent in the current episode as input for all our models.Moreover, the memory can be short-term (within an episode) or long-term (across episodes).We focus on short-term memory from the current episode.MCA is determined by the environment feedback.If an action yields a reward, then it is considered correct.Therefore correct actions cannot be fed to the agent initially, but are instead stored in memory as the agent progresses through the (train/test time) episode.Affordance Knowledge.Essentially, affordances are the set of possible actions allowed in a particular state of the environment.Within the field of perceptual psychology, they are seen as a central tool through which living beings categorize their environment [12].We expect that affordances can help models learn better by listing the possible interactions with the objects around them.Unlike historical knowledge, the environment does not provide the affordances, and they need to be retrieved from external sources.For this purpose, we use ConceptNet [27] and obtain its capableOf and usedFor relations for the objects in a given IF game episode. 1 The obtained affordances are then aggregated with the original environment inputs.For the example in Figure 1, we inject information that an apple affords being eaten, and a box can contain objects.</p>
<p>Knowledge Injection in Methods</p>
<p>We support two complementary paradigms based on training objectives: (1) online policy optimization through rewards using reinforcement learning (RL), where we frame the task as a POMDP; and (2) single-step offline prediction achieved through supervised training, approached as a language modeling task.</p>
<p>Online Policy Optimization through Rewards (RL Methods).</p>
<p>(1) Pure RL-based Model.We employ DRRN [14], due to its strong performance across challenging interactive text-based environments [13].DRRN leverages a GRU to encode the current game state into a vector as shown in Figure 2. It uses a separate GRU to encode each of the valid actions into a vector and then combines the action vector with the game state vector through an interaction function to compute the Q-value (Q   ), which estimates the total discounted reward expected if that action is taken.The policy   is learned by maximizing the expected cumulative reward
ùëÜ ùëñùëõùëì ùëú ‚Üê ùê∫ ùëúùëèùë£ ‚äô ùê∫ ùëëùëíùë†ùëê ‚äô ùê∫ ùëñùëõùë£ ‚äô ùê∫ ùëéùëì ùëì ‚äô ùê∫ ùëöùëêùëé(1)ùëÑ ùëé ùëñ ‚Üê W 2 .(ReLU(W 1 .(ùëÜ ùëñùëõùëì ùëú ‚äô ùê∫ ùëé ùëñ ))),(2)
where ‚äô signifies concatenation,  is GRU encoder of size F√óF,
W 1 ‚àà R 6ùêπ √óùêπ , W 2 ‚àà R ùêπ √ó1
, F is the embedding dimension of size 128, and    is the Q-value for each valid action   .</p>
<p>(2) RL-enhanced KG Model: As a knowledge-augmented RL agent, we used the Knowledge-augmented Actor-Critic (KG-A2C) model [1].For KG-A2C, in addition to the textual representation of the game state, the agent also builds a dynamic KG representing the state space by parsing the textual descriptions using OpenIE [3].KG's symbolic representation of the game states can help effective reasoning about the next course of action.The overall model architecture is shown in Figure 3.Each of the textual inputs is encoded with a GRU, and the KG is separately encoded with KG embeddings and Graph Attention network [29].In addition, the model takes into account the total score obtained so far through the binary score encoding.Formally, KG-A2C produces KG encoding (  ), input encoding (  ), and score encoding (  ):
ùëî ùë° ‚Üê ùëì (ùëä ùêæ ùëò=1 ùúé ( ‚àëÔ∏Å ùëó ‚ààùëÅ A ùëò ùëñ ùëó ‚Ä¢ ùëä ùëò ‚Ä¢ ‚Ñé ùëó ) + ùëè)(3)ùëú ùë° ‚Üê ùê∫ ùëúùëèùë£ ‚äô ùê∫ ùëëùëíùë†ùëê ‚äô ùê∫ ùëñùëõùë£(4)ùëÜ ùëñùëõùëì ùëú ‚Üê ùëî ùë° ‚äô ùëú ùë° ‚äô ùëè ùë°(5)
where ‚äô signifies concatenation,  is GRU encoder of size 100√ó100.W and b are weights and biases,    is the attention weights, and ‚Ñé  is the node feature vector.  is the binary score encoding of the total score obtained so far with a shape of 1x10, which is calculated using the cumulative reward attained up to the present moment.</p>
<p>The reward is first converted to a binary format with a length of 9, to which a '0' is added to the beginning in case the cumulative reward is positive, and a '1' is added if the reward is negative.The final state info vector    is calculated by concatenating the three input representations, and it is then used to generate actions for the agent.Overall, the model is trained with the actor-critic policy gradient.Instead of sampling directly from the valid action space, the policy network generates action templates and then populates the templates with objects from the knowledge graph.Thus, to make the learning more effective, KG-A2C also adds three auxiliary losses to encourage the model to generate valid actions, i.e., actions that would cause the game state to change: where L T , L O , and L E are template loss, object loss, and entropy loss respectively. ‚àà Valid(  ) is a valid action,  ‚àà Valid  (  ) is valid template,  ‚àà Valid(  ) is a valid object, and  is a state.Knowledge Injection.As baseline, we use a modified version of KG-A2C, where we utilize a single golden action sequence provided by the environment as the target, even though there may exist multiple possible golden sequences.We found this target to perform better than the original target of predicting a valid action.We devise the following knowledge-injection strategies to incorporate memory of correct actions and affordance knowledge for KG-A2C: 1. mca: On top of the baseline, we incorporate all previously correct actions by using a separate GRU encoding layer and concatenate the output vector along with other output representations.2. aff: The KG component in the KG-A2C model provides us with a convenient way to add more knowledge.In particular, we directly add the affordance knowledge into the KG as additional triples on top of the baseline model.For example, given the existing relation in the KG (living room, hasA, apple) we can add the affordance relation: (apple, usedFor, eating).In this way, the KG encoding network can produce a more meaningful representation of the game state and potentially guide the model to produce better actions.In our experiments, we compare this approach to adding affordance knowledge using a separate GRU encoding layer, similar to the DRRN case.3.aff ‚äï mca: We include both affordances in the KG and the memory of all previous correction actions with a separate GRU encoding layer.
L T (ùë† ùë° , ùëé ùë° ; ùúÉ ùë° ) ‚Üê 1 ùëÅ ùëÅ ‚àëÔ∏Å ùëñ=1 (ùë¶ ùúè ùëñ log ùúã T (ùúè ùëñ |s ùë° ) + (1 ‚àí ùë¶ ùúè ùëñ )(1 ‚àí log ùúã T (ùúè ùëñ |s ùë° ))))(6)L O (ùë† ùë° , ùëé ùë° ; ùúÉ ùë° ) ‚Üê ùëõ ‚àëÔ∏Å ùëó=1 1 ùëÄ ùëÄ ‚àëÔ∏Å ùëñ=1 (ùë¶ ùëú ùëñ log ùúã ùëú ùëó (ùëú ùëñ |s ùë° ) + (1 ‚àí ùë¶ ùëú ùëñ )(1 ‚àí log ùúã ùëú ùëó (ùëú ùëñ |s ùë° ))))(7)L E (ùë† ùë° , ùëé ùë° ; ùúÉ ùë° ) ‚Üê ‚àëÔ∏Å ùëé‚àà V (ùë† ùë° ) ùëÉ (ùëé|ùë† ùë° ) log ùëÉ (ùëé|ùë† ùë° )(8)</p>
<p>Single-step Offline Prediction (LM Methods).</p>
<p>(1) Pre-trained LM:.We employed the RoBERTa [23] pre-trained LM due to its strong performance on various procedural understanding and commonsense reasoning tasks [20].RoBERTa is a transformer-based, encoder-only model trained using masked language modeling.Due to its large size, we choose offline fine-tuning to train the agent.Here we view the task as multiple-choice QA.At each step, the current game state is treated as the question and must predict the next action from a set of candidates.Similar to RL agents, the model is given the environment observation (  ), inventory (  ), and task description () at every step.Then we concatenate it with each action and let the LM select the action with the highest score.Given the large set of possible actions, we only randomly select  distractor actions during training to reduce the computational burden, the LM is trained with cross-entropy loss to select the correct action.At inference time, the model assigns scores for all valid actions, and we use top-p sampling for action selection to prevent it from being stuck in an action loop.Knowledge Injection.We formalize three knowledge-injection strategies for the baseline RoBERTa model (Figure 4): 1.mca: Here, we enable the LM to be aware of its past correct actions by incorporating an MCA that lists them as a string, appended to the original input.Due to token limitations of RoBERTa, we use a sliding window with size , i.e., at each step, the model sees at most the past  correct actions.2.aff: We inject affordance knowledge into the LM by first adapting it on a subset of the Commonsense Knowledge Graph [18] containing object utilities [17].We adapt the model via an auxiliary QA task following prior knowledge injection work [34].We use pretraining instead of simple concatenation for input due to the substantial volume of affordance knowledge triples, which cannot be simply concatenated to the input of RoBERTa due to limited input length.Pre-training on affordances through an auxiliary QA task alleviates this challenge, while still enabling the model to learn the relevant knowledge.We then finetune our task model on top of the utility-enhanced model, as described in the baseline.3.aff ‚äï mca: This variation simply combines mca and aff.</p>
<p>(2) Instruction-tuned LM:.We utilized the Swift model [22], which is based on the Flan-T5 [8] instruction-following architecture.The training follows a Seq2Seq methodology, wherein the input comprises state information, and the desired outcome is the correct action.The encompassed state information integrates task and environmental data: "desc -step number -score -action history -obv -inv -visited rooms -What action should you do next?"(Figure 5).The action history contains the last ten performed actions, each with the respective environmental reward, e.g., "go to outside (+16) -&gt; You move to the outside."Knowledge Injection.The Swift model inherently integrates the historical context of the preceding ten actions.Notably, in contrast to the three previously examined models that exclusively consider the history of the last ten correct actions, the Swift model adheres to its original design by encompassing the entire history of the ten previous actions.To establish a comparable baseline model to the methodology applied in the preceding three architectures, we omit the action history from the Swift model.The unaltered variation of Swift is herein denoted as the 1.mca version.Additionally, incorporation of affordance into the baseline model yields the 2.aff model.Similarly, integration of affordances within the mca version led to the formation of the 3.aff ‚äï mca: model.These affordances are introduced into the primary input sequence immediately following the inventory data and preceding information about visited rooms.</p>
<p>EXPERIMENTAL SETUP 4.1 Task and Evaluation Metrics</p>
<p>ScienceWorld is a virtual representation of the world in an intricate text-based environment in English, with a variety of objects, actions, and tasks [30].It includes ten connected locations with 218 unique objects such as instruments, electrical components, plants/animals, substances, containers, and everyday objects like furniture, books, and paintings.There are 25 high-level actions, with up to 200,000 possible combinations per step, only a few of which have practical applications.ScienceWorld has 10 tasks with a total set of 30 sub-tasks.Due to the diversity within ScienceWorld, each task functions as an individual benchmark with distinct reasoning abilities, knowledge requirements, and varying numbers of actions needed to achieve the goal state.Moreover, each subtask has a set of mandatory objectives that need to be met by any agent (such as focusing on a non-living object and putting it in a red box in the kitchen).The rewards for completing these tasks are highly quantized for learning purposes to guide the agent toward preferred solutions.Namely, for each performed action, the ScienceWorld environment provides a numeric score (reward) and a boolean indication of whether the task has been completed.The agent can take up to 100 steps (actions) in each episode, and its final score is scaled to fall between 0 and 100.Its score improves when both the episode goal and its sub-goals are achieved.The evaluation for an episode concludes and the cumulative score is returned when the agent receives information from the environment that the task has been completed or the limit of 100 steps is reached.</p>
<p>For experimentation purposes, we selected a single representative sub-task from each of the 10 tasks.The numbers in brackets in the 'Task' column of Table 1 signify the original ScienceWorld subtask number out of 30. 2 All evaluation results in this paper are averaged over three model runs on the test dataset.</p>
<p>Implementation and Modeling Details</p>
<p>Following the original methods, we use task-specific training for DRRN, KG-A2C, and RoBERTa, resulting in the creation of 10 distinct models for the 10 tasks.In contrast, Swift is trained once using the entire training dataset.While we conducted experiments with KG-A2C and RoBERTa to develop a unified model for a more 2 Please refer to [30] for more information about the tasks and their train-test splits.fair comparison, the outcomes were detrimental to the performance.Hence, we use the same setup of DRRN and KG-A2C as in ScienceWorld.DRRN is trained with a learning rate of 1 ‚àí4 , an embedding dimension of 128, and a hidden dimension of 128.KG-A2C uses a learning rate of 3 ‚àí3 , a dropout rate of 0.2, an embedding dimension of 50, and a hidden dimension of 100.DRRN and KG-A2C utilized eight parallel environments to speed up the training process.These parameter values have been taken from the original ScienceWorld paper.For RL models we perform training for 40,000 steps as we were able to reproduce the baseline results with 5% of the original ScienceWorld paper.For the RoBERTa model, we use roberta-large for all of the experiments.For training, we use 3 epochs, 3 a learning rate of 2 ‚àí5 , 4 randomly selected distractors, and a batch size of 1.For RoBERTa's MCA variants, we use a window size of  = 5.For Swift, we use flan-T5-base with a learning rate of 1 ‚àí4 and a batch size of 6.The maximum source and target lengths are set to 1,024 and 16, respectively.For Swift model, we used 8 training epochs following the original paper.Environment.We used two identical servers, each with an Intel(R) Xeon(R) Gold 5215 CPU @ 2.50GHz, featuring 40 cores and 256 GB of RAM.We also utilized eight NVIDIA RTX A5000 GPUs (per server) to accelerate the training and inference process.</p>
<p>RESULTS &amp; ANALYSIS 5.1 Effect of Knowledge Injection</p>
<p>Overall results.Table 1 compares our best model with baseline: in 34 out of 40 cases, our knowledge injection strategies improve over the baseline models.Among these cases, the most successful strategy is including affordances, which obtains the best results in 15 cases, followed by including MCA (8 cases).Including both knowledge types together led to the best results in 11 cases.The positive effect of adding affordances is confirmed in Table 2, which shows that including affordances improves the selection of the subsequent best action in 63% (25 out of 40) of cases.While the integration of affordances has a positive overall impact on the agents' action selection, in another 13 cases including affordances harms the model performance.Including the memory of previous correct actions taken by the agent also effectively enhances the decision-making capabilities of the architectures under consideration, though to a lesser extent compared to including affordances (Table 2).Given the varying effectiveness of affordances and MCA, we next study the performance variations across models and tasks.</p>
<p>Performance variations across architectures.To study further the isolated effect of different types of injected knowledge, we compare the model performance with and without knowledge injection for the four models.The RL-based DRRN model benefits from affordances most consistently, with 9/10 tasks showing the best performance after affordances are included (Table 1), leading to a 4% relative increase in performance (Table 2).The DRRN model relies on exploring the action space to learn optimal policies, and providing affordance information allowed the model to narrow down the search space and focus on actions that lead to successful outcomes.In terms of the overall impact across tasks, the LM variants, RoBERTa and Swift, benefit the most on average from  including affordances, leading to a relative increase of 48% and 8% respectively, over the baselines.Affordances improve the score of the KG-A2C method in 6/10 cases, yet, the overall improvement over this baseline is marginal.For DRRN and KG-A2C, in slightly over half of the cases, integrating MCA improves performance in selecting the next best action by 2% relative to the baseline.Interestingly, MCA improves over the RoBERTa baseline by approximately 8% in relative terms, despite only helping in 3/10 tasks.Furthermore, eliminating the action history proves advantageous for Swift.</p>
<p>Performance variations across tasks.Table 2 shows granular performance per task for all four models with their corresponding knowledge-injection variants.While we note that the impact of knowledge varies across tasks, in most cases, the performance is boosted by either of the knowledge-injection strategies.We note that task 3 (Electricity) is the only one where both knowledge injection strategies help across all architectures.Here, the DRRN and KG-A2C models experience an average increase of around 10% (relative) in performance, while the RoBERTa and Swift models show an average of 100% and 14% relative increase in performance.An example goal in task 3 is to power a red light bulb using a renewable power source, which requires that the agents understand the affordances of the electrical circuitry involved and the renewable energy sources that can be used to power it.The affordances provide the agent with valuable information that the light bulb is capable of generating light.Furthermore, the agent acquired the ability to remember its prior successful selection of a light bulb, which facilitated the subsequent selection of the wire and solar panel while avoiding the repetition of its prior choice.</p>
<p>Meanwhile, we observe that tasks 8 and 10 require biological knowledge, while the affordances retrieved from ConceptNet contain information like 'dog capableOf {bark, guard}' that are not informative for inferring the lifespan or life stages of a dog.Alternatively, the addition of affordance significantly improves the performance of the RoBERTa model in tasks 2 and 7, leading to 13x and 3x better performance, respectively.Moreover, RoBERTa with affordances achieves perfect scores 14 and 9 times for tasks 4 and 7, respectively, which is rare, especially given the relatively large sequences of correct actions in the ScienceWorld tasks.Notably, tasks 4 and 7 have an average length of less than 10, indicating that the model performs well in shorter tasks.The highest improvement for RoBERTa happens on task 7, which has the shortest sequence of correct actions on average.Swift ( ‚äï ) experienced a substantial performance improvement (with a 3.5x increase over the baseline) on task 3, where the agent successfully achieved the goal state in 40% of the episodes.Moreover, in 96% of the cases for task 4, the affordance variant of Swift was able to get a perfect score of 100%.These results strongly suggest the application of techniques that tailor the learning process to the specific task at hand, like metalearning [16], to empower the system to intelligently discern and apply the most suitable knowledge for optimal performance and adaptability.</p>
<p>Effect of Affordances</p>
<p>Given that affordances are a more effective knowledge-injection strategy than including the MCA, we perform a case study of injecting affordances in different models and we compare ways to inject affordances into KG-A2C.</p>
<p>Case study.Figure 6 presents a case study regarding the models' ability to incorporate affordance information for task 4. We opted for this task of finding a non-living object given the relatively high performance of all models on it.We see that the affordance 'wire capable to connect' enhanced RoBERTa's comprehension of wires as non-living objects, yielding a positive environment reward at step 8.The LMs, as well as DRRN, also utilized the affordances associated with the table object (e.g., 'table is capable of support') to identify it as a non-living object.The affordances associated with the term box (such as 'box is used for contain' and 'box is used for hold') enhanced the LMs' grasp of the box's attributes, facilitating the execution of the final action.While both LMs benefited from the affordance knowledge, RoBERTa required 63 steps to finish the episode, while Swift completed the task in just five steps.This supports our experimental finding that, compared to Swift, RoBERTa takes more time to pick the correct action.The RL agents (DRRN and KG-A2C) faced challenges in achieving perfect scores within this sub-task.KG-A2C struggled to reach the intended destination (the bedroom), often navigating to other locations and performing arbitrary actions.While DRRN managed to reach the bedroom and obtained a slightly better score, it encountered difficulty locating the box despite the provision of affordances.This case study suggests that LMs such as RoBERTa and Swift apply affordance knowledge more effectively than RL methods for such tasks.</p>
<p>Optimal way to inject affordances.We have chosen KG-A2C to conduct the ablation study, as it has a larger number of modular components (KG, graph attention, and actor-critic module), which can be flexibly manipulated for experimentation.Moreover, KG-A2C benefits the least from affordance injection.We explore multiple variations of injecting affordance knowledge into KG-A2C: by adding it as input into the observation, inventory, and description, creating a separate GRU encoding layer for affordance, and adding affordance to the KG itself.We evaluate the performance of each  method on three sub-tasks: easy (task 4), medium (task 6), and hard (task 5), based on the number of actions and the performance of the baseline models.The results in Figure 7 consistently suggest that the incorporation of affordances as part of the KG performs better than including them as part of the other components (e.g., description) or encoding them separately.A possible explanation is that by adding affordances to the KG, we allow the agent to have a more structured and separate representation of the environment, which in turn helps the agent make more informed decisions.Adding affordances as strings concatenated to inputs or adding a separate encoding layer hurts performance; we think that these methods cause information overload or interference with the original inputs, thus confusing the agent.The separate encoding layer introduces additional complexity to the architecture, making it harder for the agent to learn and generalize, especially considering the limited data size.Meanwhile, we note that an alternative approach to incorporate affordances is via self-supervision via auxiliary tasks, which brings significant improvement for some tasks in the case of RoBERTa, and suggests an avenue for RL-LM integration.</p>
<p>CONCLUSIONS AND OUTLOOK</p>
<p>This paper investigated whether current AI agents can use knowledge injection in semi-Markov text-based games to act coherently and improve their ability to learn from the environment through enhanced contextual awareness.We proposed to inject knowledge about affordances and keep a memory of previous correct actions on diverse architectures.Through rigorous evaluation, we showed improvement over the four baseline models across ten elementary school tasks.Among our injection methods, affordance knowledge was more beneficial than the memory of correct actions.The variable effect across tasks was frequently due to the relevance of the injected knowledge to the task at hand, with certain tasks (e.g., task 3: electricity) benefiting more from the injection.Injecting affordances was most effective via KGs; incorporating them as raw inputs increased the learning complexity for the models.The insights into the usage of knowledge injection for improving the performance of RL and LMs in complex IF games have potential implications for interactive applications beyond the gaming domain, including customer service chatbots and personal assistants.</p>
<p>As the resulting models' performance is still far from ideal, we envision two future directions toward more coherent and efficient models.First, our results suggest that the models have complementary strengths and weaknesses: the RL model performed the best on the task Matter (task 1), the KG-augmented model yielded the best performance on the task of Measurement (task 2), and the language models outperformed the others on Biology I (task 5), Biology II (task 7), and Biology IV (task 10).Inspired by this insight, we propose to enhance the performance of the LMs by incorporating an RL policy network [35].Second, few-shot prompting of large LMs has recently shown promise on reasoning tasks, as well as clear benefits from interactive communication and input clarification [25].Exploring their role in interactive tasks, either as solutions that require less training data or as components that can generate synthetic data for knowledge distillation to smaller models, is another promising future direction.</p>
<p>Figure 1 :
1
Figure 1: Illustration of an Interactive Fiction (IF) game, where an agent must perform the task of picking a fruit (e.g., an apple) then placing it in a blue box in the kitchen.</p>
<p>Figure 2 :
2
Figure 2: DRRN architecture, enhanced with the memory of previous correct actions and object affordances.</p>
<p>Figure 4 :
4
Figure 4: RoBERTa architecture trained using distractors.</p>
<p>‚ÜíFigure 5 :
5
Figure 5: Swift architecture trained in a Seq2Seq manner.</p>
<p>Figure 6 :
6
Figure 6: Actions taken by affordance models on Task 4. Blue = step index, green = cumulative score, and yellow = correct action.</p>
<p>Figure 7 :
7
Figure 7: Effect of five ways to add affordances in KG-A2C.</p>
<p>Table 1 :
1
Comparison of baseline (base) versus our best model configuration (Affordance  , MCA  , and Affordance ‚äï MCA  ) from ¬ß3.2, based on average cumulative reward across task variants.Underlined numbers indicate no performance improvement over baseline.Bold numbers indicate the overall best model configuration for the task.04.82 04.82  01.48 03.70  00.74 01.48  00.00 02.22  2 (4) 07.07 07.15  05.24 13.59  01.48 19.48  07.5707.58  3 (8) 07.67 09.67  07.67 08.33  02.67 06.67  14.60 51.60  4 (13) 69.36 70.15  70.96 73.15  67.89 71.11  86.36 99.00  5 (16) 08.89 09.71  05.92 07.15  08.87 11.48  06.39 09.70  6 (17) 20.34 22.32  22.82 23.07  07.79 07.39  22.88 25.00  7 (21) 30.04 30.73  31.42 32.29  14.41 45.31  61.31 89.63  8 (23) 09.87 14.07  15.93 17.53  04.00 02.80  45.20 40.20  9 (27) 09.37 09.92  07.78 08.17  02.70 04.68  17.14 17.50  10 (29) 13.28 16.94  12.93 11.48  03.77 04.67  17.16 17.16 
DRRNKG-A2CRoBERTaSwiftTask basebest basebest basebest basebest1 (2) avg.18.0819.55 18.2120.36 11.4317.50 27.8635.96</p>
<p>Table 2 :
2
Baselines () comparison with knowledge-injected model configurations ("": Affordance; "m": Memory of Correct Actions), based on average cumulative reward across task variants.Bold signifies better performance over baseline.
DRRNKG-A2CRoBERTaSwiftTaskùëèùõºùëö ùõº ‚äï ùëöùëèùõºùëö ùõº ‚äï ùëöùëèùõºùëö ùõº ‚äï ùëöùëèùõºùëö ùõº ‚äï ùëö1 (2)04.82 04.82 04.8204.4401.48 01.11 00.7403.70 00.74 01.11 00.0001.48 00.00 00.00 02.2200.002 (4)07.07 07.15 06.1006.8305.24 09.79 13.5908.58 01.48 19.48 01.2501.64 07.55 07.58 07.4807.353 (8)07.67 09.67 08.0008.3307.67 08.33 07.6707.33 02.67 04.00 06.6705.67 14.60 16.60 16.6051.604 (13)69.36 70.15 67.7069.7070.96 72.07 73.1572.15 67.89 71.11 65.5965.89 86.36 99.00 87.2485.225 (16)08.99 07.91 09.1409.7105.92 07.15 06.7706.67 08.87 11.48 07.4510.10 06.39 07.91 09.7008.106 (17)20.34 22.07 22.3219.9422.82 23.07 20.5921.23 07.79 07.14 06.7507.39 22.88 21.88 21.8825.007 (21)30.04 30.73 29.6928.6531.42 29.69 31.9432.29 14.41 45.31 24.4832.64 61.31 74.84 57.4089.638 (23)09.87 09.73 11.9314.0715.93 13.53 15.2717.53 04.00 02.67 02.8003.42 45.20 40.20 12.4014.009 (27)09.37 08.21 08.9309.9207.78 07.82 08.1707.26 02.70 02.86 04.6802.90 17.14 17.50 14.4015.1210 (29) 13.28 16.94 15.1314.5412.93 11.48 08.5409.61 03.77 03.43 03.4704.67 17.16 16.24 17.1617.11avg.18.08 18.74 18.3718.61 18.21 18.40 18.6418.64 11.43 16.86 12.3113.58 27.86 30.18 24.6531.314: (</p>
<p>find a non-living object) &amp; Variation: 292 [Affordance Model] Your task is to find a(n) non-living thing. First, focus on the thing. Then, move it to the yellow box in the bedroom. Affordances: table</p>
<p>is capable of chip, table is used for support, box is used for contain, box is used for hold, box is used for seat, box is capable of assemble, box is capable of empty, _ _ _, wire is capable of connect, wire is capable of corrode, wire is capable of cut, wire is capable of crisscross, wire is capable of dangle.
DRRNRoBERTalook around8%KG-A2CSwiftlook around8%look around8%go to hallway17%2go to bedroom25%4go to hallway16%look around25%8focus on air58%2move orange to bedroom17%8focus on red wire66%4focus on table75%go to hallway66%3go to bedroom25%21go to bedroom75%juice pour orange75%100look around66%focus on table 75%focus on table75%5move table to yellow box100%63move table to yellow box100%
https://github.com/ease-crc/ease_lexical_resources
A common choice across NLP tasks, further tuning did not yield improvements.
A ADDITIONAL TASK DETAILS A.1 Task Descriptions(1) Task 1 -Matter: Your task is to freeze water.First, focus on the substance.Then, take actions that will cause it to change its state of matter.(2) Task 2 -Measurement: Your task is to measure the melting point of chocolate, which is located around the kitchen.First, focus on the thermometer.Next, focus on the chocolate.If the melting point of chocolate is above -10.0degrees, focus on the blue box.If the melting point of chocolate is below -10.0 degrees, focus on the orange box.The boxes are located around the kitchen.(3) Task 3 -Electricity: Your task is to turn on the red light bulb by powering it using a renewable power source.First, focus on the red light bulb.Then, create an electrical circuit that powers it on.(4) Task 4 -Classification: Your task is to find a(n) non-living thing.First, focus on the thing.Then, move it to the red box in the kitchen.(5) Task 5 -Biology I : Your task is to grow a apple plant from seed.Seeds can be found in the kitchen.First, focus on a seed.Then, make changes to the environment that grow the plant until it reaches the reproduction life stage.(6) Task 6 -Chemistry: Your task is to use chemistry to create the substance 'salt water'.A recipe and some of the ingredients might be found near the kitchen.When you are done, focus on the salt water.(7) Task 7 -Biology II : Your task is to find the animal with the longest life span, then the shortest life span.First, focus on the animal with the longest life span.Then, focus on the animal with the shortest life span.The animals are in the 'outside' location.(8) Task 8 -Biology III : Your task is to focus on the 4 life stages of the turtle, starting from earliest to latest.(9) Task 9 -Forces: Your task is to determine which of the two inclined planes (unknown material C, unknown material H) has the most friction.After completing your experiment, focus on the inclined plane with the most friction.(10) Task 10 -Biology IV : Your task is to determine whether blue seed color is a dominant or recessive trait in the unknown E plant.If the trait is dominant, focus on the red box.If the trait is recessive, focus on the green box.A.2 ScienceWorld Gameplay ExampleTask: 4 (find a non-living thing) Variation: 239 (DRRN baseline) Description: Your task is to find a(n) non-living thing.First, focus on the thing.Then, move it to the purple box in the workshop.
Graph Constrained Reinforcement Learning for Natural Language Action Spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, 2020In ICLR</p>
<p>Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning. Prithviraj Ammanabrolu, Mark Riedl, Proceedings of NAACL. NAACL2019</p>
<p>Leveraging Linguistic Structure For Open Domain Information Extraction. Gabor Angeli, Melvin Jose , Johnson Premkumar, Christopher D Manning, Proceedings of ACL-IJCNLP. ACL-IJCNLP2015</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Conference on Robot Learning. PMLR2023</p>
<p>Grounding large language models in interactive environments with online reinforcement learning. Thomas Carta, Cl√©ment Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer, arXiv:2302.026622023. 2023arXiv preprint</p>
<p>Privacy Aware Question-Answering System for Online Mental Health Risk Assessment. Prateek Chhikara, Ujjwal Pasupulety, John Marshall, Dhiraj Chaurasia, Shweta Kumari, The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks. 2023</p>
<p>. Noam Chomsky, Aspects of the Theory of Syntax. 112014MIT press</p>
<p>Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022. 2022arXiv preprint</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre C√¥t√©, Akos K√°d√°r, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with IJCAI. 2019. 2018</p>
<p>Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, Jeff Clune, Go-Explore: a New Approach for Hard-Exploration Problems. 2019. 2019</p>
<p>Core challenges in embodied vision-language planning. Jonathan Francis, Nariaki Kitamura, Felix Labelle, Xiaopeng Lu, Ingrid Navarro, Jean Oh, Journal of Artificial Intelligence Research. 742022. 2022</p>
<p>The theory of affordances. James J Gibson, 1977. 19771Hilldale, USA</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre C√¥t√©, Xingdi Yuan, AAAI. 2020</p>
<p>Deep Reinforcement Learning with a Natural Language Action Space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, Proceedings of ACL. ACL2016</p>
<p>Leveraging class abstraction for commonsense reinforcement learning via residual policy gradient methods. Niklas H√∂pner, Ilaria Tiddi, Herke Van Hoof, IJCAI. 2022. 2022</p>
<p>A survey of deep metalearning. Mike Huisman, Jan N Van Rijn, Aske Plaat, Artificial Intelligence Review. 542021. 2021</p>
<p>. Filip Ilievski, Alessandro Oltramari, Kaixin Ma, Bin Zhang, Deborah L Mcguinness, Pedro Szekely, Dimensions of commonsense knowledge. Knowledge-Based Systems. 2291073472021. 2021</p>
<p>CSKG: The CommonSense Knowledge Graph. Filip Ilievski, Pedro Szekely, Bin Zhang, Extended Semantic Web Conference (ESWC). 2021</p>
<p>On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings. Peter Jansen, Kelly J Smith, Dan Moreno, Huitzilin Ortiz, Proceedings of EMNLP. EMNLP2021</p>
<p>Transferring Procedural Knowledge across Commonsense Tasks. Yifan Jiang, Filip Ilievski, Kaixin Ma, ECAI. 2023</p>
<p>Planning and acting in partially observable stochastic domains. Leslie Pack, Kaelbling Michael L Littman, Anthony R Cassandra, Artificial intelligence. 1011998. 1998</p>
<p>Yicheng Bill Yuchen Lin, Karina Fu, Prithviraj Yang, Faeze Ammanabrolu, Shiyu Brahman, Chandra Huang, Yejin Bhagavatula, Xiang Choi, Ren, arXiv:2305.17390Swift-Sage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks. 2023. 2023arXiv preprint</p>
<p>Roberta: A robustly optimized bert pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, 2019. 2019</p>
<p>Coalescing Global and Local Information for Procedural Text Understanding. Kaixin Ma, Filip Ilievski, Jonathan Francis, Eric Nyberg, Alessandro Oltramari, Proceedings of COLING. COLING2022</p>
<p>Memoryassisted prompt editing to improve GPT-3 after deployment. Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Exploration Based Language Learning for Text-Based Games. Andrea Madotto, Mahdi Namazifar, Joost Huizinga, Piero Molino, Adrien Ecoffet, Huaixiu Zheng, Dian Yu, Alexandros Papangelis, Chandra Khatri, Gokhan Tur, IJCAI. 2021</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Proceedings of AAAI. AAAI201731</p>
<p>Multi-Stage Episodic Control for Strategic Exploration in Text Games. Jens Tuyls, Shunyu Yao, Sham M Kakade, Karthik R Narasimhan, ICLR. 2022</p>
<p>Graph Attention Networks. Petar Veliƒçkoviƒá, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li√≤, Yoshua Bengio, 2018In ICLR</p>
<p>ScienceWorld: Is your Agent Smarter than a 5th Grader?. Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre C√¥t√©, Prithviraj Ammanabrolu, 10.48550/arxiv.2203.07540EMNLP. 2022. 2022</p>
<p>Generalization in Text-based Games via Hierarchical Reinforcement Learning. Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Chengqi Zhang, EMNLP. 2021</p>
<p>Keep CALM and explore: Language models for action generation in text-based games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, EMNLP. Association for Computational Linguistics2020</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2022</p>
<p>An Empirical Investigation of Commonsense Self-Supervision with Knowledge Graphs. Jiarui Zhang, Filip Ilievski, Kaixin Ma, Jonathan Francis, Alessandro Oltramari, AKBC. 2022. 2022</p>
<p>Nisan Daniel M Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
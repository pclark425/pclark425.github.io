<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4334 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4334</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4334</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-276579678</p>
                <p><strong>Paper Title:</strong> A large language model framework for literature-based disease–gene association prediction</p>
                <p><strong>Paper Abstract:</strong> Abstract With the exponential growth of biomedical literature, leveraging Large Language Models (LLMs) for automated medical knowledge understanding has become increasingly critical for advancing precision medicine. However, current approaches face significant challenges in reliability, verifiability, and scalability when extracting complex biological relationships from scientific literature using LLMs. To overcome the obstacles of LLM development in biomedical literature understating, we propose LORE, a novel unsupervised two-stage reading methodology with LLM that models literature as a knowledge graph of verifiable factual statements and, in turn, as semantic embeddings in Euclidean space. LORE captured essential gene pathogenicity information when applied to PubMed abstracts for large-scale understanding of disease–gene relationships. We demonstrated that modeling a latent pathogenic flow in the semantic embedding with supervision from the ClinVar database led to a 90% mean average precision in identifying relevant genes across 2097 diseases. This work provides a scalable and reproducible approach for leveraging LLMs in biomedical literature analysis, offering new opportunities for researchers to identify therapeutic targets efficiently.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4334.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4334.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LORE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Open Relation extraction and Embedding (LORE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage literature-semantics framework that uses LLMs to (1) extract atomic relation triplets from articles (LLM-ORE) and (2) encode all relations for each entity pair into dense numerical embeddings (LLM-EMB) to discover latent semantic patterns (e.g., pathogenicity flow) and enable large-scale supervised modeling (ML-Ranker).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LORE (two-stage LLM literature-semantics framework)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>LORE is a pipeline comprising (A) LLM-ORE: LLM-driven Open Relation Extraction where an LLM (text-continuation prompt) reads a paper (title+abstract) and emits structured relational triplets <subject, predicate, object> for target entity pairs with constraints (entity appears only as subject or object, concise predicate, concrete fact statements). (B) LLM-EMB: the relations across all articles for an entity pair are concatenated into one or more sub-documents (split to fit model context) and then encoded by an embedding model into a fixed-size dense vector per entity pair (512-D in this work). The resulting knowledge graph (sentential relations + tags) and embeddings are used for downstream analyses including visualization, manifold discovery (pathogenic flow), and supervised ranking with ML-Ranker.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5-turbo-0613 for LLM-ORE (primary extraction); OpenAI text-embedding-3-large for embeddings (LLM-EMB); also experiments with GPT-4o (baseline queries) and Llama-8B (Llama-3.1-8B-Instruct) as an open-source replacement for extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature (disease-gene relationships; human genetics, clinical genomics)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>1,745,538 articles in a selected literature subset processed with LORE; broader runs on 3,997,496 pubmedKB abstracts with Llama-8B (numbers given in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Statistical/semantic relationships: discovery of latent manifold structures and a directional 'pathogenic flow' (semantic gradient) in embedding space; produces graded pathogenicity scores (statistical ranking) rather than closed-form physical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured relational triplets (text), a literature-scale knowledge graph (graph of relations with tags and source article IDs), dense numerical embeddings (512-D vectors per disease-gene pair), and scalar pathogenicity scores/rankings for DG pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison to expert-curated ClinVar labels (mapped via UMLS/MeSH), cross-validation (leave-one-disease-out and 5-fold gene disjoint), evaluation of ranking performance (AP per disease, mean AP/MAP), and coverage statistics (e.g., fraction of ClinVar DGs covered).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ML-Ranker MAP: 79.9% across all DGs in PMKB-CV; 90.0% MAP when restricted to DGs with LLM-ORE annotations. Baselines: co-occurrence paper counting MAP 69.4%; GPT-4o (direct querying) MAP 31.7%; LLM-EMB linear regression MAP 87.7% (reported); Llama-8B extraction + ML-Ranker MAP 79.5% (in one cross-validation) and 81.6% MAP when scaled to all ~3.9M abstracts; LLM-ORE covered 71.4% of known ClinVar DGs while paper co-occurrence covered 94.8% of ClinVar DGs. Coverage of full scaled Llama-8B relations: 91.3% of ClinVar DGs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against: (1) naive paper-count co-occurrence ranking (LLM-#paper): MAP 69.4% vs ML-Ranker 79.9%; (2) simple literature-semantic relation counting (tag counts): lower than ML-Ranker (numbers shown in figure; co-occurrence 69.4%); (3) direct LLM question answering: GPT-4o MAP 31.7%; (4) linear regression on LLM-EMB: 87.7% MAP (ML-Ranker further improved to 90% on annotated subset). Replacing extraction LLM (GPT-3.5) with Llama-8B yields marginal performance loss but improved scalability (scaled extraction achieved 81.6% MAP).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Dependence on abstract-level text (missing ClinVar DGs with no abstract co-occurrence), LLM hallucination risks (mitigated by extracting verifiable relation triplets with source IDs), incomplete coverage by LLM-ORE (71.4% ClinVar coverage in subset), context-length constraints require document splitting, manual curation of key semantics (human bias), taxonomy curation used ClinVar (potential circularity when evaluating tag-count baseline), opaque embedding interpretability, and need for large compute for literature-scale processing.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4334.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4334.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-ORE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Open Relation Extraction (LLM-ORE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM prompting approach to extract atomic, article-level relational triplets between specified entity pairs from paper title+abstract using text-continuation prompts and example-driven demonstrations to enforce structured output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-ORE (Open Relation Extraction via text-continuation prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>LLM-ORE uses demonstration-based text-continuation prompts (task-agnostic ORE example followed by target article and entity pair) to instruct an LLM to output a list of relational triplets <subject, predicate, object> representing concrete factual statements implied by the article. Constraints in the prompt enforce that the first entity appears only in the subject, the second only in the object, predicates are concise and self-contained, and abstract understanding beyond sentence syntax is allowed. The authors included examples and counterexamples in prompts to discourage unwanted outputs and used an end-marker '-' to enforce list formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5-turbo-0613 primarily; experiments also used Llama-8B (Llama-3.1-8B-Instruct) for scaled extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature (disease-gene relations) though the ORE prompt is domain-agnostic in design</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to a literature subset of 1,745,538 articles (curated subset) producing 11,285,095 relations; when using Llama-8B scaled to 3,997,496 pubmedKB abstracts producing 74,132,940 relations.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Not direct closed-form laws; extracts factual relations and lemma frequency distributions that enable statistical/semantic pattern discovery (e.g., association with pathogenicity).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>List of relational triplets (<subject, predicate, object>) per article per entity pair (text), each relation linked to source article IDs; later lemmatized and tagged by curated key semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Statistical coverage vs ClinVar (coverage percentage), manual inspection during key-semantics curation (sampled relations from known pathogenic DGs and others), and downstream predictive performance (how much LLM-ORE-derived features improve ranking).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>LLM-ORE relation coverage: 71.4% of 3004 known pathogenic DGs in subset; produced 11.3M relations from 1.745M articles; when Llama-8B used at scale, relations covered 91.3% of ClinVar DGs. No direct precision/recall numbers for extraction accuracy beyond curation filters reported.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared implicitly to naive co-occurrence (paper counting) which had higher raw coverage for ClinVar but lower ranking performance when used alone; LLM-ORE relations used as features yield improved downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Extraction limited by abstract content (not full text), some ClinVar DGs lack abstracts so cannot be extracted; prompt quality and LLM capabilities affect extraction completeness; no explicit per-relation precision metric reported; manual taxonomy creation required to map lemmas to pathogenic semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4334.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4334.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-EMB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Embedding (LLM-EMB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoding stage that takes all extracted sentential relations for an entity pair, concatenates them into document(s), and uses an embedding model to produce a dense 512-dimensional vector representing the literature semantics of that pair.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-EMB (relation-to-embedding encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>LLM-EMB concatenates the LLM-ORE output relations for each entity pair into one or more sub-documents (split when exceeding model context) and computes a literature-semantic embedding per pair using an embedding model (OpenAI text-embedding-3-large). Embeddings were analyzed with UMAP and PCA/ridge regression to reveal latent manifolds and semantic distributions (e.g., lemmas like 'mutation' and 'cause' concentrated along the pathogenic arm). The embeddings serve as features for ML ranking and visualization of pathogenic flow.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>OpenAI text-embedding-3-large for embedding vectors (512-d); document splitting handled by token length according to the embedding model.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature (disease-gene relations)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Embeddings computed for 652,701 DGs in PMKB-CV (2097 diseases subset); embeddings cover relations from 1,745,538 curated articles (11,285,095 relations) and larger-scale Llama-8B extraction covering 3,997,496 abstracts when applied.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Latent statistical/semantic manifolds and directional field (pathogenic flow) representing gradual change in pathogenicity-related semantics; embeddings used to compute scalar pathogenicity predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>512-dimensional dense numerical vectors (per disease-gene pair), suitable for downstream dimensionality reduction (UMAP/PCA) and supervised learning.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Visualization (UMAP/PCA) to check clustering with ClinVar labels; supervised downstream evaluation via ML-Ranker (MAP/AP), and correlation with curated ClinVar pathogenic labels using regression axes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Linear regression on LLM-EMB achieved MAP 87.7% in ranking pathogenic genes (reported), showing embeddings alone capture much pathogenic signal; used as baseline vs full ML-Ranker.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to simple co-occurrence (#papers) and relation-counting; LLM-EMB linear regression (87.7% MAP) outperformed naive baselines and GPT-4o direct QA (31.7% MAP); full ML-Ranker further improved to 90% MAP on annotated DGs.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Embedding interpretability limited despite PCA/UMAP analyses; embeddings depend on quality/completeness of relation extraction; zero embedding used for DGs without relations (may reduce information); context-splitting may lose cross-relation interaction semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4334.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4334.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ML-Ranker (lambdaGBDT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ML-Ranker using Lambda objective with Gradient-Boosted Decision Trees (lambdaGBDT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised learning-to-rank system that models disease-wise pathogenic flow by optimizing a LambdaRank objective with GBDT, using embeddings and literature-derived features to score/rank genes for each disease.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ML-Ranker (lambda objective + GBDT)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>ML-Ranker explicitly models disease-wise pathogenic flow by grouping DGs per disease and training a LambdaRank-style ranking model implemented with GBDT (lambdaGBDT). The RankNet loss gradient (lambda) is modulated by change in NDCG to prioritize ranking quality; input features include LLM-EMB embeddings, pubmedKB co-occurrence counts (#papers), counts of LLM-ORE tagged relations, and zero embeddings for missing relations. Training/evaluation used leave-one-disease-out cross-validation and AP/MAP metrics per disease.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Uses LLM-derived features: embeddings from text-embedding-3-large and relation-count features from LLM-ORE (GPT-3.5 and Llama-8B for extraction in experiments); the ranker itself is GBDT (non-LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical genetics (disease-gene pathogenicity ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Trained and evaluated on PMKB-CV which uses relations and embeddings derived from up to 1,745,538 curated articles (and extended runs on 3,997,496 abstracts for scaled experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Statistical ranking function reflecting a continuous pathogenicity score per disease-gene pair; models a directional semantic flow (vector field) in embedding space that correlates with pathogenicity.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Scalar pathogenicity scores per disease-gene pair and ranked lists of genes per disease; accompanied by supporting relation evidence (triplets + article IDs).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Leave-one-disease-out cross-validation (and 5-fold gene-disjoint CV), evaluation with Average Precision (AP) per disease and Mean Average Precision (MAP) overall, statistical testing of performance distributions (one-sided Wilcoxon signed-rank test).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MAP: 79.9% across all DGs in PMKB-CV; 90.0% MAP on DGs with LLM-ORE annotations; for high-prevalence diseases ML-Ranker MAP 81% vs co-occurrence 67%. Statistical significance reported vs baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baselines included co-occurrence paper counting (MAP 69.4%), relation-counting (lower), LLM-EMB linear regression (87.7% MAP), and direct LLM QA (GPT-4o MAP 31.7%). ML-Ranker generally outperformed naive baselines and improved over linear regression when incorporating multiple features and lambda objective.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires labelled pathogenic DGs for supervision (ClinVar) for training; model performance sensitive to quality/coverage of LLM-ORE and embeddings; potential bias introduced by training taxonomy curation that used ClinVar; disease-specific scarcity affects some rankings though disease-wise flow mitigates it.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4334.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4334.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PMKB-CV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PMKB-ClinVar (PMKB-CV) dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A literature-scale dataset produced by applying LORE to pubmedKB and ClinVar: contains literature-derived relations, semantic embeddings, predicted pathogenicity scores, tags (105 key semantics), and source article IDs for 2097 diseases and 652,701 DGs in the evaluation subset (larger raw co-occurrence exists).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PMKB-CV dataset construction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>PMKB-CV combines pubmedKB co-occurrence annotations (gene/variant mentions indexed by NCBI gene IDs) and ClinVar expert labels mapped to MeSH IDs via UMLS. A curated literature subset was formed via bootstrapping (top genes per disease and top diseases per gene) yielding ~1.745M articles; LORE extracted ~11.3M relations and produced embeddings and key-semantics tagging. The final PMKB-CV dataset includes 2097 diseases with both pubmedKB co-occurrence and ClinVar pathogenic DGs and is used as the benchmark for ranking experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Dataset contents derived via LLM-ORE (GPT-3.5, Llama-8B optional) and embeddings (text-embedding-3-large).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature / clinical genomics</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Curated subset: 1,745,538 articles producing 11,285,095 relations; expanded processing covered 3,997,496 pubmedKB abstracts in scaled experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Not a law but provides labeled examples and features enabling discovery of statistical patterns (pathogenicity scores, manifold structures) across DG pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Knowledge graph of relations with tags and source IDs, embeddings (512-d), and scalar pathogenicity scores for DGs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Mapped to ClinVar for ground truth pathogenic labels (4311 ClinVar pathogenic DGs overall; 3004 in subset) and used for cross-validation experiments (leave-one-disease-out) to evaluate ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used to report MAP results for ML-Ranker (79.9% overall) and coverage statistics (e.g., LLM-ORE relations covered 71.4% of the 3004 ClinVar DGs whereas paper co-occurrence covered 94.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Serves as evaluation benchmark when comparing ML-Ranker, LLM-EMB, co-occurrence counting, LLM-ORE tag counting, and direct LLM QA.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Constructed using abstract co-occurrence so misses ClinVar DGs without abstracts (1307 ClinVar DGs had no pubmedKB abstract co-occurrence); manual mapping across vocabularies (OMIM to MeSH) may introduce mapping errors.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4334.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4334.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (background)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A background method referenced in the paper: LLMs are augmented with a retrieval step that provides a small set of retrieved texts as explicit context to improve verifiability, but it can be limited by retrieval scalability and shallow sentence-similarity methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>RAG pipelines first retrieve a small set of candidate documents (often via fast vector or sparse retrieval) relevant to a user query, then condition an LLM on those retrieved documents to generate answers grounded in the retrieved evidence. The paper cites RAG as complementary but notes scalability and incomplete retrieval of nuanced information as limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>General LLMs in literature (cited work); not used experimentally in this paper's pipeline (LORE differs by building a concise knowledge graph first).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General knowledge-intensive NLP and literature QA; mentioned in biomedical literature context</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>N/A (retrieval+generation architecture rather than law discovery); used for grounded generation and evidence-backed answers.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Generated text grounded on retrieved documents; retrieval results typically as short passages/sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Discussed in background literature (not validated in this work); authors highlight RAG's reliance on retrieval quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Contrasted qualitatively with LORE: RAG restricts LLM context to a small set of retrieved texts and suffers from retrieval incompleteness, while LORE constructs a concise knowledge graph enabling broader encoding and downstream modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Scalability constraints, need for fast but shallow retrieval causing missing nuanced content and relevant articles; potential incompleteness of evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 2)</em></li>
                <li>The impact of large language models on scientific discovery: a preliminary study using GPT-4 <em>(Rating: 2)</em></li>
                <li>Graph embedding-based link prediction for literature-based discovery in Alzheimer's disease <em>(Rating: 2)</em></li>
                <li>RENET: a deep learning approach for extracting gene-disease associations from literature <em>(Rating: 2)</em></li>
                <li>Learning to rank using gradient descent <em>(Rating: 1)</em></li>
                <li>Effectiveness and efficiency of open relation extraction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4334",
    "paper_id": "paper-276579678",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "LORE",
            "name_full": "LLM-based Open Relation extraction and Embedding (LORE)",
            "brief_description": "A two-stage literature-semantics framework that uses LLMs to (1) extract atomic relation triplets from articles (LLM-ORE) and (2) encode all relations for each entity pair into dense numerical embeddings (LLM-EMB) to discover latent semantic patterns (e.g., pathogenicity flow) and enable large-scale supervised modeling (ML-Ranker).",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "LORE (two-stage LLM literature-semantics framework)",
            "method_description": "LORE is a pipeline comprising (A) LLM-ORE: LLM-driven Open Relation Extraction where an LLM (text-continuation prompt) reads a paper (title+abstract) and emits structured relational triplets &lt;subject, predicate, object&gt; for target entity pairs with constraints (entity appears only as subject or object, concise predicate, concrete fact statements). (B) LLM-EMB: the relations across all articles for an entity pair are concatenated into one or more sub-documents (split to fit model context) and then encoded by an embedding model into a fixed-size dense vector per entity pair (512-D in this work). The resulting knowledge graph (sentential relations + tags) and embeddings are used for downstream analyses including visualization, manifold discovery (pathogenic flow), and supervised ranking with ML-Ranker.",
            "llm_model_used": "GPT-3.5-turbo-0613 for LLM-ORE (primary extraction); OpenAI text-embedding-3-large for embeddings (LLM-EMB); also experiments with GPT-4o (baseline queries) and Llama-8B (Llama-3.1-8B-Instruct) as an open-source replacement for extraction.",
            "scientific_domain": "Biomedical literature (disease-gene relationships; human genetics, clinical genomics)",
            "number_of_papers": "1,745,538 articles in a selected literature subset processed with LORE; broader runs on 3,997,496 pubmedKB abstracts with Llama-8B (numbers given in paper).",
            "type_of_quantitative_law": "Statistical/semantic relationships: discovery of latent manifold structures and a directional 'pathogenic flow' (semantic gradient) in embedding space; produces graded pathogenicity scores (statistical ranking) rather than closed-form physical laws.",
            "extraction_output_format": "Structured relational triplets (text), a literature-scale knowledge graph (graph of relations with tags and source article IDs), dense numerical embeddings (512-D vectors per disease-gene pair), and scalar pathogenicity scores/rankings for DG pairs.",
            "validation_method": "Comparison to expert-curated ClinVar labels (mapped via UMLS/MeSH), cross-validation (leave-one-disease-out and 5-fold gene disjoint), evaluation of ranking performance (AP per disease, mean AP/MAP), and coverage statistics (e.g., fraction of ClinVar DGs covered).",
            "performance_metrics": "ML-Ranker MAP: 79.9% across all DGs in PMKB-CV; 90.0% MAP when restricted to DGs with LLM-ORE annotations. Baselines: co-occurrence paper counting MAP 69.4%; GPT-4o (direct querying) MAP 31.7%; LLM-EMB linear regression MAP 87.7% (reported); Llama-8B extraction + ML-Ranker MAP 79.5% (in one cross-validation) and 81.6% MAP when scaled to all ~3.9M abstracts; LLM-ORE covered 71.4% of known ClinVar DGs while paper co-occurrence covered 94.8% of ClinVar DGs. Coverage of full scaled Llama-8B relations: 91.3% of ClinVar DGs.",
            "baseline_comparison": "Compared against: (1) naive paper-count co-occurrence ranking (LLM-#paper): MAP 69.4% vs ML-Ranker 79.9%; (2) simple literature-semantic relation counting (tag counts): lower than ML-Ranker (numbers shown in figure; co-occurrence 69.4%); (3) direct LLM question answering: GPT-4o MAP 31.7%; (4) linear regression on LLM-EMB: 87.7% MAP (ML-Ranker further improved to 90% on annotated subset). Replacing extraction LLM (GPT-3.5) with Llama-8B yields marginal performance loss but improved scalability (scaled extraction achieved 81.6% MAP).",
            "challenges_limitations": "Dependence on abstract-level text (missing ClinVar DGs with no abstract co-occurrence), LLM hallucination risks (mitigated by extracting verifiable relation triplets with source IDs), incomplete coverage by LLM-ORE (71.4% ClinVar coverage in subset), context-length constraints require document splitting, manual curation of key semantics (human bias), taxonomy curation used ClinVar (potential circularity when evaluating tag-count baseline), opaque embedding interpretability, and need for large compute for literature-scale processing.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4334.0",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "LLM-ORE",
            "name_full": "LLM-based Open Relation Extraction (LLM-ORE)",
            "brief_description": "An LLM prompting approach to extract atomic, article-level relational triplets between specified entity pairs from paper title+abstract using text-continuation prompts and example-driven demonstrations to enforce structured output.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "LLM-ORE (Open Relation Extraction via text-continuation prompting)",
            "method_description": "LLM-ORE uses demonstration-based text-continuation prompts (task-agnostic ORE example followed by target article and entity pair) to instruct an LLM to output a list of relational triplets &lt;subject, predicate, object&gt; representing concrete factual statements implied by the article. Constraints in the prompt enforce that the first entity appears only in the subject, the second only in the object, predicates are concise and self-contained, and abstract understanding beyond sentence syntax is allowed. The authors included examples and counterexamples in prompts to discourage unwanted outputs and used an end-marker '-' to enforce list formatting.",
            "llm_model_used": "GPT-3.5-turbo-0613 primarily; experiments also used Llama-8B (Llama-3.1-8B-Instruct) for scaled extraction.",
            "scientific_domain": "Biomedical literature (disease-gene relations) though the ORE prompt is domain-agnostic in design",
            "number_of_papers": "Applied to a literature subset of 1,745,538 articles (curated subset) producing 11,285,095 relations; when using Llama-8B scaled to 3,997,496 pubmedKB abstracts producing 74,132,940 relations.",
            "type_of_quantitative_law": "Not direct closed-form laws; extracts factual relations and lemma frequency distributions that enable statistical/semantic pattern discovery (e.g., association with pathogenicity).",
            "extraction_output_format": "List of relational triplets (&lt;subject, predicate, object&gt;) per article per entity pair (text), each relation linked to source article IDs; later lemmatized and tagged by curated key semantics.",
            "validation_method": "Statistical coverage vs ClinVar (coverage percentage), manual inspection during key-semantics curation (sampled relations from known pathogenic DGs and others), and downstream predictive performance (how much LLM-ORE-derived features improve ranking).",
            "performance_metrics": "LLM-ORE relation coverage: 71.4% of 3004 known pathogenic DGs in subset; produced 11.3M relations from 1.745M articles; when Llama-8B used at scale, relations covered 91.3% of ClinVar DGs. No direct precision/recall numbers for extraction accuracy beyond curation filters reported.",
            "baseline_comparison": "Compared implicitly to naive co-occurrence (paper counting) which had higher raw coverage for ClinVar but lower ranking performance when used alone; LLM-ORE relations used as features yield improved downstream performance.",
            "challenges_limitations": "Extraction limited by abstract content (not full text), some ClinVar DGs lack abstracts so cannot be extracted; prompt quality and LLM capabilities affect extraction completeness; no explicit per-relation precision metric reported; manual taxonomy creation required to map lemmas to pathogenic semantics.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4334.1",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "LLM-EMB",
            "name_full": "LLM-based Embedding (LLM-EMB)",
            "brief_description": "An encoding stage that takes all extracted sentential relations for an entity pair, concatenates them into document(s), and uses an embedding model to produce a dense 512-dimensional vector representing the literature semantics of that pair.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "LLM-EMB (relation-to-embedding encoder)",
            "method_description": "LLM-EMB concatenates the LLM-ORE output relations for each entity pair into one or more sub-documents (split when exceeding model context) and computes a literature-semantic embedding per pair using an embedding model (OpenAI text-embedding-3-large). Embeddings were analyzed with UMAP and PCA/ridge regression to reveal latent manifolds and semantic distributions (e.g., lemmas like 'mutation' and 'cause' concentrated along the pathogenic arm). The embeddings serve as features for ML ranking and visualization of pathogenic flow.",
            "llm_model_used": "OpenAI text-embedding-3-large for embedding vectors (512-d); document splitting handled by token length according to the embedding model.",
            "scientific_domain": "Biomedical literature (disease-gene relations)",
            "number_of_papers": "Embeddings computed for 652,701 DGs in PMKB-CV (2097 diseases subset); embeddings cover relations from 1,745,538 curated articles (11,285,095 relations) and larger-scale Llama-8B extraction covering 3,997,496 abstracts when applied.",
            "type_of_quantitative_law": "Latent statistical/semantic manifolds and directional field (pathogenic flow) representing gradual change in pathogenicity-related semantics; embeddings used to compute scalar pathogenicity predictions.",
            "extraction_output_format": "512-dimensional dense numerical vectors (per disease-gene pair), suitable for downstream dimensionality reduction (UMAP/PCA) and supervised learning.",
            "validation_method": "Visualization (UMAP/PCA) to check clustering with ClinVar labels; supervised downstream evaluation via ML-Ranker (MAP/AP), and correlation with curated ClinVar pathogenic labels using regression axes.",
            "performance_metrics": "Linear regression on LLM-EMB achieved MAP 87.7% in ranking pathogenic genes (reported), showing embeddings alone capture much pathogenic signal; used as baseline vs full ML-Ranker.",
            "baseline_comparison": "Compared to simple co-occurrence (#papers) and relation-counting; LLM-EMB linear regression (87.7% MAP) outperformed naive baselines and GPT-4o direct QA (31.7% MAP); full ML-Ranker further improved to 90% MAP on annotated DGs.",
            "challenges_limitations": "Embedding interpretability limited despite PCA/UMAP analyses; embeddings depend on quality/completeness of relation extraction; zero embedding used for DGs without relations (may reduce information); context-splitting may lose cross-relation interaction semantics.",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4334.2",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "ML-Ranker (lambdaGBDT)",
            "name_full": "ML-Ranker using Lambda objective with Gradient-Boosted Decision Trees (lambdaGBDT)",
            "brief_description": "A supervised learning-to-rank system that models disease-wise pathogenic flow by optimizing a LambdaRank objective with GBDT, using embeddings and literature-derived features to score/rank genes for each disease.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "ML-Ranker (lambda objective + GBDT)",
            "method_description": "ML-Ranker explicitly models disease-wise pathogenic flow by grouping DGs per disease and training a LambdaRank-style ranking model implemented with GBDT (lambdaGBDT). The RankNet loss gradient (lambda) is modulated by change in NDCG to prioritize ranking quality; input features include LLM-EMB embeddings, pubmedKB co-occurrence counts (#papers), counts of LLM-ORE tagged relations, and zero embeddings for missing relations. Training/evaluation used leave-one-disease-out cross-validation and AP/MAP metrics per disease.",
            "llm_model_used": "Uses LLM-derived features: embeddings from text-embedding-3-large and relation-count features from LLM-ORE (GPT-3.5 and Llama-8B for extraction in experiments); the ranker itself is GBDT (non-LLM).",
            "scientific_domain": "Biomedical genetics (disease-gene pathogenicity ranking)",
            "number_of_papers": "Trained and evaluated on PMKB-CV which uses relations and embeddings derived from up to 1,745,538 curated articles (and extended runs on 3,997,496 abstracts for scaled experiments).",
            "type_of_quantitative_law": "Statistical ranking function reflecting a continuous pathogenicity score per disease-gene pair; models a directional semantic flow (vector field) in embedding space that correlates with pathogenicity.",
            "extraction_output_format": "Scalar pathogenicity scores per disease-gene pair and ranked lists of genes per disease; accompanied by supporting relation evidence (triplets + article IDs).",
            "validation_method": "Leave-one-disease-out cross-validation (and 5-fold gene-disjoint CV), evaluation with Average Precision (AP) per disease and Mean Average Precision (MAP) overall, statistical testing of performance distributions (one-sided Wilcoxon signed-rank test).",
            "performance_metrics": "MAP: 79.9% across all DGs in PMKB-CV; 90.0% MAP on DGs with LLM-ORE annotations; for high-prevalence diseases ML-Ranker MAP 81% vs co-occurrence 67%. Statistical significance reported vs baselines.",
            "baseline_comparison": "Baselines included co-occurrence paper counting (MAP 69.4%), relation-counting (lower), LLM-EMB linear regression (87.7% MAP), and direct LLM QA (GPT-4o MAP 31.7%). ML-Ranker generally outperformed naive baselines and improved over linear regression when incorporating multiple features and lambda objective.",
            "challenges_limitations": "Requires labelled pathogenic DGs for supervision (ClinVar) for training; model performance sensitive to quality/coverage of LLM-ORE and embeddings; potential bias introduced by training taxonomy curation that used ClinVar; disease-specific scarcity affects some rankings though disease-wise flow mitigates it.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4334.3",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "PMKB-CV",
            "name_full": "PMKB-ClinVar (PMKB-CV) dataset",
            "brief_description": "A literature-scale dataset produced by applying LORE to pubmedKB and ClinVar: contains literature-derived relations, semantic embeddings, predicted pathogenicity scores, tags (105 key semantics), and source article IDs for 2097 diseases and 652,701 DGs in the evaluation subset (larger raw co-occurrence exists).",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "PMKB-CV dataset construction",
            "method_description": "PMKB-CV combines pubmedKB co-occurrence annotations (gene/variant mentions indexed by NCBI gene IDs) and ClinVar expert labels mapped to MeSH IDs via UMLS. A curated literature subset was formed via bootstrapping (top genes per disease and top diseases per gene) yielding ~1.745M articles; LORE extracted ~11.3M relations and produced embeddings and key-semantics tagging. The final PMKB-CV dataset includes 2097 diseases with both pubmedKB co-occurrence and ClinVar pathogenic DGs and is used as the benchmark for ranking experiments.",
            "llm_model_used": "Dataset contents derived via LLM-ORE (GPT-3.5, Llama-8B optional) and embeddings (text-embedding-3-large).",
            "scientific_domain": "Biomedical literature / clinical genomics",
            "number_of_papers": "Curated subset: 1,745,538 articles producing 11,285,095 relations; expanded processing covered 3,997,496 pubmedKB abstracts in scaled experiments.",
            "type_of_quantitative_law": "Not a law but provides labeled examples and features enabling discovery of statistical patterns (pathogenicity scores, manifold structures) across DG pairs.",
            "extraction_output_format": "Knowledge graph of relations with tags and source IDs, embeddings (512-d), and scalar pathogenicity scores for DGs.",
            "validation_method": "Mapped to ClinVar for ground truth pathogenic labels (4311 ClinVar pathogenic DGs overall; 3004 in subset) and used for cross-validation experiments (leave-one-disease-out) to evaluate ranking.",
            "performance_metrics": "Used to report MAP results for ML-Ranker (79.9% overall) and coverage statistics (e.g., LLM-ORE relations covered 71.4% of the 3004 ClinVar DGs whereas paper co-occurrence covered 94.8%).",
            "baseline_comparison": "Serves as evaluation benchmark when comparing ML-Ranker, LLM-EMB, co-occurrence counting, LLM-ORE tag counting, and direct LLM QA.",
            "challenges_limitations": "Constructed using abstract co-occurrence so misses ClinVar DGs without abstracts (1307 ClinVar DGs had no pubmedKB abstract co-occurrence); manual mapping across vocabularies (OMIM to MeSH) may introduce mapping errors.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4334.4",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "RAG (background)",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A background method referenced in the paper: LLMs are augmented with a retrieval step that provides a small set of retrieved texts as explicit context to improve verifiability, but it can be limited by retrieval scalability and shallow sentence-similarity methods.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "mention_or_use": "mention",
            "method_name": "Retrieval-Augmented Generation",
            "method_description": "RAG pipelines first retrieve a small set of candidate documents (often via fast vector or sparse retrieval) relevant to a user query, then condition an LLM on those retrieved documents to generate answers grounded in the retrieved evidence. The paper cites RAG as complementary but notes scalability and incomplete retrieval of nuanced information as limitations.",
            "llm_model_used": "General LLMs in literature (cited work); not used experimentally in this paper's pipeline (LORE differs by building a concise knowledge graph first).",
            "scientific_domain": "General knowledge-intensive NLP and literature QA; mentioned in biomedical literature context",
            "number_of_papers": null,
            "type_of_quantitative_law": "N/A (retrieval+generation architecture rather than law discovery); used for grounded generation and evidence-backed answers.",
            "extraction_output_format": "Generated text grounded on retrieved documents; retrieval results typically as short passages/sentences.",
            "validation_method": "Discussed in background literature (not validated in this work); authors highlight RAG's reliance on retrieval quality.",
            "performance_metrics": "",
            "baseline_comparison": "Contrasted qualitatively with LORE: RAG restricts LLM context to a small set of retrieved texts and suffers from retrieval incompleteness, while LORE constructs a concise knowledge graph enabling broader encoding and downstream modeling.",
            "challenges_limitations": "Scalability constraints, need for fast but shallow retrieval causing missing nuanced content and relevant articles; potential incompleteness of evidence.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4334.5",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "The impact of large language models on scientific discovery: a preliminary study using GPT-4",
            "rating": 2,
            "sanitized_title": "the_impact_of_large_language_models_on_scientific_discovery_a_preliminary_study_using_gpt4"
        },
        {
            "paper_title": "Graph embedding-based link prediction for literature-based discovery in Alzheimer's disease",
            "rating": 2,
            "sanitized_title": "graph_embeddingbased_link_prediction_for_literaturebased_discovery_in_alzheimers_disease"
        },
        {
            "paper_title": "RENET: a deep learning approach for extracting gene-disease associations from literature",
            "rating": 2,
            "sanitized_title": "renet_a_deep_learning_approach_for_extracting_genedisease_associations_from_literature"
        },
        {
            "paper_title": "Learning to rank using gradient descent",
            "rating": 1,
            "sanitized_title": "learning_to_rank_using_gradient_descent"
        },
        {
            "paper_title": "Effectiveness and efficiency of open relation extraction",
            "rating": 1,
            "sanitized_title": "effectiveness_and_efficiency_of_open_relation_extraction"
        }
    ],
    "cost": 0.0125529,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Problem Solving Protocol A large language model framework for literature-based disease-gene association prediction</p>
<p>Peng-Hsuan Li 
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Yih-Yun Sun 
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Hsueh-Fen Juan 
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Department of Life Science
National Taiwan University
No. 1, Sec. 4, Roosevelt Rd10617TaipeiTaiwan</p>
<p>Center for Computational and Systems Biology
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Center for Advanced Computing and Imaging in Biomedicine
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Chien-Yu Chen 0000-0002-6940-6389
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Center for Computational and Systems Biology
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Center for Advanced Computing and Imaging in Biomedicine
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Department of Biomechatronics Engineering
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Huai-Kuang Tsai 0000-0002-4200-8137
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Institute of Information Science
Academia Sinica
No. 128, Academia Road, Section 211529NankangTaipeiTaiwan</p>
<p>Jia-Hsin Huang jiahsin.huang@gmail.com 
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Problem Solving Protocol A large language model framework for literature-based disease-gene association prediction
AED27303585B563AB05DD7EDBA7B860210.1093/bib/bbaf070Received: November 15, 2024. Revised: January 9, 2025. Accepted: February 6, 2025literature miningbiomedical relation extractionNLPknowledge graphlarge language model
This study explores biomedical informatics and artificial intelligence, leveraging large language models and knowledge graphs to advance precision medicine and enhance the discovery of disease-gene relationships.</p>
<p>Introduction</p>
<p>Knowledge curation from scientific literature is fundamental to the advancement of research across disciplines [1][2][3].Traditionally, domain-specific knowledge accumulates incrementally and relies heavily on human expert review processes.The biomedical literature, for instance, is a key resource for identifying causal genetic elements associated with diseases and offering insights into clinical practice.Several expert-curated databases, such as ClinVar [4], COSMIC [5], OMIM [6], and PharmGKB [7], provide invaluable assessments of literature evidence.However, such resources are limited in scale because of the broad scope and the rapid expansion of scientific publications [8,9].Many computational approaches have been applied to enhance automation in biomedical literature-based discovery for different tasks, such as gene-disease association prediction [10][11][12], text mining and curation [13][14][15][16][17], and biomedical entity relation extraction [18][19][20][21].However, these methods primarily focus on extracting isolated sentences or paragraphs containing entities of interest rather than synthesizing comprehensive information across multiple sources and contexts.Thus, substantial efforts are required to create task-specific datasets and train models for each literature domain.</p>
<p>On the other hand, Machine Reading Comprehension (MRC) [22], wherein machines answer questions based on textual context, serves as a promising complement to human expertise in reading vast amounts of literature.Recent advancements in natural language processing, particularly the development of Large Language Models (LLMs), have significantly enhanced MRC capabilities to potentially accelerate knowledge synthesis [23][24][25].Recent LLMs, such as GPT-4, have demonstrated remarkable capabilities in textual comprehension across diverse domains; however, LLMs face challenges when it comes to reliability and verifiability [23,26].Specifically, LLMs are prone to hallucination, a phenomenon whereby plausible but factually incorrect information is generated.Moreover, the opaque parametric memory of LLMs poses a substantial obstacle to traceabilitysources of evidence supporting their statements are often unclear.To mitigate these concerns, researchers have applied Retrieval Augmented Generation (RAG) in LLM-based chatbots [27,28].RAG restricts the information source of an LLM to an explicit but small set of retrieved texts per user query.However, owing to scalability constraints, fast but shallow sentence similarity-based retrieval is required, leading to incomplete information capture as nuanced content and relevant articles are often missing.Figure 1.Overview of the literature-semantics framework.Given a large body of literature containing expert domain knowledge, the proposed framework creates a comprehensive unsupervised knowledge graph and numerical embeddings between entities.This enables large-scale supervised modeling of downstream tasks, where model predictions are accompanied by verifiable evidence of relations that can be traced back to the original articles.(a) The framework is applied to PubMed literature, and a knowledge graph containing semantic relations between diseases and genes and their mutations is created.(b) An embedding for disease-gene relationships, where each point in space contains the literature-semantic knowledge of a DG, is created.(c) The embedding is shown to contain a latent structure of DG pathogenicity.(d) We further analyze the disease-wise pathogenic f low and find that it is consistent across diseases and even smoother than the point-wise distribution.(e) An ML-ranker is trained to model the f low and to predict pathogenic genes for each disease, where the prediction scope is 200× larger than expert-curated supervision.(f) We curated 105 key semantics about DG pathogenicity with linguistic lemmas to automatically tag relations.(G) With the proposed framework, we facilitate future research on DG pathogenicity with a literature-scale knowledge base of predicted DG scores and supporting evidence.</p>
<p>In essence, scalable knowledge curation calls for a computational method that is inductive across domains and is capable of capturing nuanced textual context for knowledge synthesis, all while maintaining essential reliability and verifiability.To this end, we introduce LORE (LLM-based Open Relation extraction and Embedding), a novel literature semantics framework that encompasses the best of both worlds and is tested true in capturing disease-gene relationships across the PubMed literature (Fig. 1).</p>
<p>LORE leverages a two-stage reading methodology comprising LLM-based Open Relation Extraction (LLM-ORE) and LLM-based embedding (LLM-EMB) (Fig. 1a).First, LORE employs LLM-ORE to comprehend each article and generate atomic statements about entity relations therein.Curated knowledge is explicitly derived from individual articles, making the generated relations reliable and verifiable.Furthermore, this operation creates a comprehensive unsupervised knowledge graph of the literature, the conciseness of which makes it possible for LORE to then use LLM-EMB to encode the full relation knowledge between each entity pair and create numerical embeddings for downstream task-specific applications.In this work, we applied LORE to curate disease-gene relationship knowledge in the PubMed literature, using disease, gene, and variant annotations from pubmedKB [2].</p>
<p>The ClinVar [4] database is an important annotation repository of the relationships between genes and diseases.Using key disease-gene pairs from ClinVar as references, we evaluated the effectiveness of LORE to explore the latent space of gene-wise pathogenicity and disease-wise pathogenic f low across genes (Fig. 1b).In addition, we constructed machine learning models with the supervision of pathogenic genes from ClinVar to rank the relevance of gene pathogenicity using the semantic embeddings (Fig. 1b).</p>
<p>Finally, we curated a taxonomy of key semantics to use as tags for relations (Fig. 1c).We created PMKB-CV (pubmedKB-ClinVar) dataset, a novel resource that expands the scope of disease-gene relationship data.Notably, PMKB-CV encompasses more than 2097 diseases and covers disease-gene pairs (DGs) at a scale 200 times larger than that covered by ClinVar.Moreover, PMKB-CV provides rich annotations, including semantic embeddings, predicted DG scores, and verifiable knowledge graph relations with tags and source article IDs (Fig. 1c).In summary, our LORE framework harnesses the power of LLM-based MRC and enables literaturescale knowledge graph construction and downstream modeling.Importantly, PMKB-CV further bridges the gap between largescale computational analysis and human assessment, helping to advance our understanding of disease genetics and potential therapeutic targets.</p>
<p>Results</p>
<p>LLM-ORE curates semantic relations knowledge from literature</p>
<p>We applied LLM-ORE to PubMed abstracts for annotating diseasegene relationships and created a comprehensive unsupervised knowledge graph (Fig. 1a).A total of 11 million relations across 1.7 million abstracts were obtained by prompting GPT-3.5 [29].Using text-continuation prompts [30], we employed LLMs to analyze individual articles and extract atomic statements describing relations between pairs of entities.Figure 2 illustrates the prompt structure used to guide GPT-3.5 in this task.This prompt is composed of two key sections.The first section demonstrates a domain-agnostic Open Relation Extraction (ORE) [31].The text serves as a primer, independent of the specific biomedical context, to establish the expected format and level of detail for the extraction (see more details in Methods).Following the same structure as the demonstration, the second section applies the ORE process to the target article and entities under investigation.Notably, this approach allows for a generalized ORE from the literature, which is not constrained to predefined relation types or entity pairs.A total of 358,888 distinct semantic lemmas are present across the 11 million disease-gene relations.We reviewed 282 highcoverage lemmas in the knowledge graph and curated a taxonomy of 105 key semantics about pathogenicity (Fig. 3).The key semantics, automatically tagged to relations, served as a set of indexes to access the knowledge graph.We grouped the key pathogenicity semantics into four main classes.</p>
<p>Class 1, Relation, includes key semantics that directly describe the relation between a gene and a disease.For example, the key semantic 'mutation cosegregates with disease' in Class 1.3 describes the correlation between occurrence of a certain genetic mutation and whether the genome is from a patient of a certain disease.</p>
<p>Class 2, Mutation, conveys information about genetic mutations.This type of information implicates that the corresponding gene has a role in a certain disease.</p>
<p>Class 3, Disease, conveys information on the genetic aspects of diseases.This semantic implies that a certain gene is at play.Class 4 contains cohort and miscellaneous information that entails or hints at pathogenicity.</p>
<p>LLM-EMB embeddings capture underlying gene pathogenicity</p>
<p>In the second stage of LORE, we applied LLM-EMB to the PubMed DG knowledge graph created in the first stage using LLM-ORE.For each pair of entities, all their relations across all articles were encoded to a single vector, which contained the literature knowledge about their relationship.A dense 512-dimensional representation was created for each DG.</p>
<p>To analyze the latent pathogenicity structure within the literature-semantic embedding, we first displayed the embedding space using 2D UMAP [32] (Fig. 4a-d).Each point represented a DG.When points were colored by co-occurrence frequency in PubMed abstracts, high-frequency DGs were observed to be distributed throughout the space (Fig. 4a).When points were colored by ClinVar pathogenicity labels, pathogenic DGs were observed to cluster in a subspace (Fig. 4b).This subspace was well-captured by the pathogenic score prediction of ML-Ranker (See more details about the ML-Ranker in the following sections and in Methods).With an optimal pathogenic score threshold to split the DGs by color into red or gray, the distribution was close to that of the ClinVar labels (Fig. 4d).In contrast to the ClinVar labels, which were human-curated and sparse, the stratified pathogenic score predicted by ML-Ranker delineated a smooth landscape of pathogenicity (Fig. 4c).Thus, high-score DGs not curated yet in ClinVar will be of high interest to the biomedical community.</p>
<p>Next, we displayed the 3D linear subspaces of the LLM-EMB embedding and observed the DG pathogenicity distribution (Fig. 4e-g).The axes were calculated using Principal Component Analysis (PCA) or ridge regression, a regularized version of ordinary least squares regression, against ClinVar pathogenicity labels.For the PCA subspace (Fig. 4e), the top three dimensions explained 12.8% of the variance of the embedding.A latent structure of two manifolds was observed-a dense spherical cluster of gray DG points without ClinVar annotations and a curved pattern of gradual transition from gray to red (i.e., known pathogenic DGs annotated in ClinVar).For the subspaces spanned by a combination of PCA axes and regression axes (Fig. 4f,g), clearer manifolds were observed when more regression axes were included.With two regression axes, a distribution pattern aligned with ClinVar annotations, signifying an association with pathogenicity, was revealed.We noted that the regression axes spanned a smaller subspace of the embedding compared with the PCA axes.Nevertheless, they provided an analytical pathogenicity perspective of the unsupervised embedding.</p>
<p>We further analyzed the literature-semantics structure by visualizing the distribution of important semantic lemmas in the 3D PCA subspace (Fig. 4h-j).The frequency of each semantic lemma, such as 'cause,' was represented in log scale per DG using a cool-blue-to-warm-red color gradient.Distinct patterns emerged for various lemmas, particularly those lemmas with a f latter distribution, such as 'associate,' 'mutation,' and 'cause.' The semantic 'associate' (Fig. 4h) was observed to increase along a linear axis but was more prevalent (indicated by greener colors) in the curved arm than in the sparse parts (bluer).The semantic 'mutation' (Fig. 4i) was primarily distributed along the curved arm.Similarly, the semantic 'cause' (Fig. 4j) was concentrated along the curved arm, with a significant presence at the space correlated with ClinVar annotations.This visualization of semantic lemmas revealed a connection between the intrinsic semantic structure of LLM-EMB and the latent DG point-wise pathogenicity structure.</p>
<p>In short, our analyses revealed the connection between the literature semantics captured by LORE and the known pathogenic DGs in the ClinVar database, as illustrated in Fig. 4. Using both UMAP and PCA visualization techniques, we examined how these known pathogenic DGs are distributed within the semantic LLM-EMB embedding space.Notably, these pathogenic DGs clustered in specific subspaces that aligned with disease-related semantic concepts -particularly terms like 'mutation' and 'cause'.This clustering pattern demonstrates the potential value of LLM-EMB embeddings as robust features for developing our ML-Ranker system for pathogenicity prediction.</p>
<p>Pathogenic flow in the literature-semantic space</p>
<p>In this section, we explore the embedding space underlying the DG pathogenicity distribution (Fig. 5).First, to model pathogenicity, a straightforward approach would be to model the distribution of pathogenic DGs (i.e., red points) in the embedding space (Fig. 5a,b).Clear subspaces of pathogenicity and nonpathogenicity were evident in the literature-semantic space visualized using 3D linear axes (Fig. 4e-g).These low-rank subspaces can be seen in the gray balls and the ends of the gray-to-red arm.However, the gray and red dots co-occur in some places, such as the center of the arm.To distinguish between gray and red dots, high-dimensional hyperplanes or complex nonlinear subspaces are needed, requiring more data and hampering generalization.</p>
<p>However, the fundamental task is to predict the most relevant pathogenic genes for each disease.If the gray and red dots for each disease are distributed separately to the two ends of a linear axis or, more generally, a smooth curve, pathogenic and nonpathogenic genes can be distinguished by splitting the curve.Furthermore, if the curves are consistent across diseases, the relevant pathogenic genes for every disease can be identified by monotonically raising the predicted relevance along the curves.Under this condition, perfect modeling can be achieved even if nonpathogenic and pathogenic DGs from different diseases are mixed in the embedding space (Fig. 5c).</p>
<p>In this study, we defined pathogenic f low as the direction and magnitude of nonpathogenicity to pathogenicity at each location in space.We started by calculating the disease-wise f low direction at each DG.Then, we quantized the f low direction at each location in space, using f low magnitude to ref lect consistency.We observed a smooth, cross-disease consistent field of pathogenic f low in the literature-semantic embedding (Fig. 5d).A consistent f low of nonpathogenicity to pathogenicity was noted along the arm-shaped manifold of the gray-to-red transition, implying that although DG point-wise modeling was difficult in the central mixture of the arm, disease-wise modeling was smoother and much more linear.</p>
<p>Literature-scale pathogenicity prediction by ML-ranker</p>
<p>We built an ML-Ranker that can model disease-wise pathogenic f low and score DGs that co-occur in PubMed abstracts.We applied the lambda objective with Gradient-Boosted Decision Trees (GBDT) [33] to directly model disease-wise pathogenic f low.</p>
<p>We compiled the PMKB-CV dataset to validate the proposed approach (Fig. 6a).PMKB-CV contains 2097 diseases that are present in both pubmedKB and ClinVar.For these diseases, 652 701 DGs co-occurred in PubMed abstracts, whereas only 3004 DGs were human-curated by ClinVar.Paper abstract co-occurrence covered 94.8% of the 3004 known pathogenic DGs, whereas LLM-ORE relations covered 71.4% (Fig. 6b).We included pubmedKB annotations, such as the number of paper abstracts in which a DG co-occurs in the dataset, and also used them as model features.Moreover, we used zero embedding for DGs without relations.Consequently, all DGs in the dataset can be uniformly used for training and prediction.We used leave-one-disease-out cross-validation to evaluate the performance of ML-Ranker for predicting pathogenic genes for each disease iteratively.An average precision (AP) score was calculated for each disease and Mean Average Precision (MAP) was used to evaluate the overall performance of the ranker.As a baseline, we directly asked GPT-3.5 (ver.2023-06-13) and GPT-4o (ver.2024-05-13) about the pathogenicity of each DG.In addition, to put into perspective the effectiveness of the curated key semantics in identifying crucial literature evidence, we tested a DG pathogenicity prediction method of simply counting the number of tagged relations per DG.Of note, the curated key semantics were only used in the experiment 'LLM-ORE (key semantics)' (Fig. 6b).</p>
<p>For all DGs in PMKB-CV, ML-Ranker achieved an MAP of 79.9%, which was a significant enhancement over the 69.4% MAP of co-occurrence paper counting and the 31.7%MAP of GPT-4o (Fig. 6b).Similar performance (MAP = 79.2%) was observed when applying 5-fold cross-validation of disjoint genes subsets (Supplementary Fig. 1).</p>
<p>When focusing specifically on those DGs with LLM-ORE annotations, ML-Ranker yielded a remarkable MAP of 90.0% (Fig. 6b).The predictive performance (AP) of ML-Ranker was statistically significantly better than other methods including co-occurrence paper counting (LLM-#paper), literature-semantic relation counting (LLM-ORE), and latent pathogenic f low modeling (LLM-EMB) (Fig. 6c).For the highly prevalent diseases that co-occurred with the highest number of genes in pubmedKB, ML-Ranker achieved a robust MAP of 81%, compared with the 67% MAP of co-occurrence paper counting (Fig. 6d).GPT-4o does not perform well.In comparison, LLM-EMB linear regression alone achieves 87.7% performance, whereas the full-f ledged MLranker provides higher performance at 90.0% or higher coverage at 94.8%.(c) Dispersion of ranking performance across diseases for each method and the p-values of distribution differences by one-sided Wilcoxon signed-rank test.LLM-ranker significantly outperformed baseline methods.(d) Performance across diseases of different scopes.The naive paper counting method encountered difficulties while ranking DGs for diseases that co-occurred with many genes in PMKB, but our semantic embedding approach remained robust.(e) Extending the scope of LORE using the public llama-8B model.Replacing GPT-3.5 with llama-8B resulted in marginal performance loss.The smaller model was further applied to all 3.9 million papers with DG co-occurrence.The final extracted relations covered 91.3% of ClinVar DGs and achieved 81.6% ranking performance.(f) Top-ranked DGs, seven out of 586 in PMKB, for Tourette syndrome accompanied by their literature relation evidence.</p>
<p>Furthermore, we extended LORE to use the open-source Llama-8B model (ver.Llama-3.1-8B-Instruct).Using a much smaller and accessible model, ML-Ranker achieved a comparable 79.5% MAP to the 79.9% MAP of using GPT-3.5.Leveraging Llama-8B, we processed all 3,997,496 pubmedKB abstracts with DG co-occurrence and curated 74,132,940 relations.The resulting relations covered 91.3% of ClinVar DGs, enabling ML-Ranker to achieve a notable ranking performance of 81.6% MAP (Fig. 6e).</p>
<p>Finally, the DG scores and ranking provided by ML-Ranker are accompanied by literature evidence (Fig. 6f).Our approach facilitates future expert assessment of DG pathogenicity by a quick grasp of literature knowledge with key semantics relations and relevant articles.</p>
<p>Discussion</p>
<p>Recent advancements in LLMs aim to automate complex sensemaking as human endeavors in reading and connecting information across large collections of scientific literature [34].Our study introduces LORE, a novel literature semantics framework that fundamentally reframes how we leverage LLMs to extract and use knowledge from scientific literature.</p>
<p>The LORE framework offers several key advantages.First, knowledge synthesis using the LORE approach constructs a literature knowledge graph of verifiable factual statements linked to the sources.Second, LORE offers a scalable framework for knowledge synthesis from large amounts of article texts.LORE extracts original article texts and transforms them into a concise knowledge graph.The approach is more efficient than traditional retrieval augmented generation approaches that select only a small set of articles for an LLM to read.The knowledge graph is much more concise compared with the original articles.This reduction in size and complexity allows for a more efficient representation of information; all the relevant knowledge can then be embedded for downstream tasks.In addition, LORE allows new publications to be annotated, thereby continually expanding the knowledge graph.Third, this approach places much less demand on the capability of LLMs, compared with directly asking LLMs expert domain questions.Using LORE, we have captured gene pathogenicity with GPT-3.5 and Llama-8B (Fig. 6e), a feat far from being achieved by directly asking GPT-4o (Fig. 6b).Indeed, small and open-source LLMs have been demonstrated to be competent for article-level comprehension [35,36], hence the methodology is not constrained to enterprise LLMs.Finally, our framework demonstrates remarkable efficacy in capturing disease-gene relationships through unsupervised relationship extraction and embedding, and users can also employ LORE with prompt engineering and fine-tuning to annotate task-specific knowledge across various domains of scientific inquiry [37,38].</p>
<p>When applying LORE to the complex landscape of DG relationships, we demonstrated the presence of a latent smooth field of cross-disease consistent pathogenic f low in the unsupervised literature-semantic embeddings.This discovery reveals that although pathogenic and nonpathogenic DGs from different diseases may occupy similar locations in the embedding space, a consistent directional f low of pathogenicity exists in terms of semantics.To illustrate, consider a simplified one-dimensional embedding axis where rare and common diseases coexist (Supplementary Fig. 2).Suppose that D1 is a rare disease reported in a few studies and that D2 is a common disease whose association with many genes is discussed in a multitude of papers; in this scenario, the following literature annotations, DG locations, and pathogenicity labels are possible:</p>
<p>G1 is not related to D1. (x = 0, y = non-pathogenic).</p>
<p>G2 mutation is found in a D1 patient. (x = 1, y = pathogenic). G3 mutation is found in a D2 patient; G3 is associated with D2. (x = 2, y = non-pathogenic). G4 mutation is found in a D2 patient; G4 causes D2. (x = 3, y = pathogenic).</p>
<p>Although the pathogenic D1G2 and the non-pathogenic D2G3 are mixed in the center of the axis, literature evidence about pathogenicity consistently increases along the axis.As a result, even when pathogenic and non-pathogenic associations are interspersed due to inter-disease differences such as popularity, the literature-semantic axis provides for a cross-disease consistent linear f low, enabling accurate pathogenicity modeling across diseases.</p>
<p>Our initial analyses of this pathogenic f low revealed clusters of disease-specific pathogenic curves.Notably, we found that these clusters often form a continuum, with the endpoint of one cluster serving as the starting point for another.This observation suggests a broader, interconnected field of pathogenic relationships across diseases, offering new perspectives on the complex landscape of genetic pathogenicity.</p>
<p>LORE curates knowledge for entity pairs that co-occur in literature articles.For the study of disease-gene pathogenicity, we noted that the potential curation scope was larger than the PMKB-CV dataset.PMKB-CV contained 2097 diseases that had both pubmedKB DG co-occurrence and known ClinVar pathogenic DGs (Fig. 6a); the full pubmedKB contained 3 128 402 DGs co-occurred in abstracts, spanning 8894 diseases (Supplementary Fig. 3).In this study, we focused on those 2097 diseases that could be validated by ClinVar, but the potential curation scope was as large as the 3 128 402 DGs.On the other hand, we also noted that the full ClinVar contained 4311 known pathogenic DGs, 1307 of which had no pubmedKB abstract co-occurrence.This was the inherent limitation to article abstract-based MRC.</p>
<p>Conclusion</p>
<p>In summary, our study makes three significant contributions to the field.First, it presents a novel literature semantics framework that addresses the long-standing challenges of comprehensiveness, reliability, and verifiability in machine reading comprehension.Second, it demonstrates the efficacy of LORE in capturing complex pathogenic relationships across diseases to reveal new insights into pathogenic f low.Finally, it provides a literature-scale dataset that not only complements existing resources such as ClinVar but also offers a knowledge graph of DG relationships with graded pathogenicity scores for genetic prioritization in clinical practice.The methodology of LORE is a general improvement on LLM-based machine reading comprehension, paving the way for bridging vast literature resources and actionable scientific knowledge to realize accelerated discoveries across scientific disciplines.</p>
<p>Methods</p>
<p>Two-stage reading comprehension of LORE</p>
<p>In the first stage of LORE, we applied LLM-ORE by prompting LLMs. Figure 2 shows the actual prompt we used.The demonstration consists of an article with a topic as Martin Likes Fish, target entity pair 'Martin' and 'fish', along with lists of unwanted and desired annotations.This section implicitly specifies the ORE task and the following required properties.</p>
<p>(1) The annotations should be concrete statements of fact implied by the article.</p>
<p>(2) The annotations should follow a structured format, specifically a list of relational triplets.</p>
<p>(3) Each relational triplet should be &lt; 'subject', 'predicate', 'object' &gt; .</p>
<p>(4) The subject should contain the first entity, and the first entity should only appear in the subject.</p>
<p>(5) The object should contain the second entity, and the second entity should only appear in the object.</p>
<p>(6) The predicate should be a concise, self-contained description of the relation between the subject and the object.</p>
<p>(7) Abstract understanding of the article is allowed beyond sentencelevel syntax.</p>
<p>The approach allows for abstract understanding beyond sentence-level syntax.The requirements are better understood through demonstration rather than explicit definitions.For instance, instead of explaining the terms 'subject,' 'predicate,' and 'object,' the examples and counterexamples in the demonstration make the concept clear.In addition, the examples illustrate behaviors that are easier to grasp instinctively rather than by complex rules.Examples include the removal of 'also' from 'Martin also loves eating fish', the rewriting of 'large fish scare him' to 'scare of', and the digested understanding of 'Martin dreamed about fishing' from 'he has a dream.The dream is about fishing'.Finally, we note that the ending '-' is important in ensuring that LLM follows the desired list format.</p>
<p>Formally, the desired generation
g = f LLM−ORE p, e 1 , e 2 ; m
where p is the target article, e 1 and e 2 are the names of the target entities, and m is the LLM model.In this work, we use the paper title and abstract as p.For m, gpt-3.5-turbo-0613 is used.The function maps &lt; p, e 1 , e 2 &gt; to g, the list of parsed relational triplets, using the text-continuation prompt shown in Fig. 2.</p>
<p>In the second stage of LORE, LLM reads all relations between an entity pair and produces a numerical representation of knowledge about their relationships.For example, the following relations between the entity e 1 and the entity e 2 are read by LLM as the following document.'e1', 'causes', 'e2'.'e1 mutations', 'are frequently encountered in', 'e2 patients'.'e1 haploinsufficiency', 'results in', 'e2'.Document: 'e1 causes e2.e1 mutations are frequently encountered in e2 patients.e1 haploinsufficiency results in e2.'.</p>
<p>If the document is larger than the allowed context of an LLM, it is split into multiple sub-documents, each of which contains as many relations as possible.</p>
<p>Formally, the embedding vector of an entity pair is given by
v = f LLM−EMB D e1,e2 ; m =</p>
<p>Modeling the pathogenic flow with ML-ranker</p>
<p>To visualize the disease-wise pathogenic f low, we define the f low as a unit vector at each DG that points to the pathogenic direction of disease D at the embedding location of DG.Formally, the f low vector is given by
u DG = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ v DG − 1 |S 0 D | DG ∈S 0 D v DG if P(DG) = 1 1 |S 1 D | DG ∈S 1 D v DG − v DG if P(DG) = 0 u DG = u DG | u DG |
where v denotes the embedding vector by LLM-EMB, S 0 D and S 1 D denote the sets of non-pathogenic and pathogenic diseasegene pairs of D respectively, and P maps non-pathogenic and pathogenic pairs to 0 and 1, respectively.In other words, each non-pathogenic DG has a unit f low vector directed at the average embedding of the pathogenic DGs of the same disease, and each pathogenic DG has a unit f low vector directed from the average embedding of the non-pathogenic DGs of the same disease.Then, we quantize the f low vectors for each cube in space by averaging them.Formally, a quantized f low vector is given by
u L = 1 | L | DG∈L u DG
where L is the set of DGs in a cube subspace.Finally, we show the field of pathogenic f low in Fig. 5d, where each arrow corresponds to a quantized f low vector.The arrow direction is the aggregated disease-wise f low direction from non-pathogenicity to pathogenicity, and the arrow length, proportional to | u L |, reflects the degree of cross-disease consistency at that location.</p>
<p>As shown in Fig. 5c, the essence of modeling the pathogenic f low is to model the difference between non-pathogenic and pathogenic genes per disease.Specifically, suppose DG1 and DG2 correspond to the same disease but P(DG1) = 1 and P(DG2) = 0. Let s denote the predicted pathogenicity score.Then one would want to maximize the probability Pr (P(DG1) &gt; P(DG2)) by minimizing the following RankNet [39]  To make sure the most relevant genes are ranked on top for each disease, the final gradient, λ, is the gradient of the RankNet loss multiplied by the change in Normalized Discounted Cumulative Gain (NDCG) [40].Formally, this LambdaRank [41] gradient is given by λ DG1,DG2 = ∂L ∂s DG1</p>
<p>• NDCG (DG1, DG2)</p>
<p>In this work, we use λGBDT, the lambda objective combined with GBDT [33], as the ML-Ranker to explicitly model the pathogenic f low in the literature-semantic space.</p>
<p>To evaluate the ranking performance for each disease, AP is used to see if the known pathogenic genes for a disease are ranked on top.Formally, for each disease,
AP = 1 R N k=1 known(k) × precision(k)
where N is the number of ranked genes for that disease, and R is the number of ranked known pathogenic genes for that disease.</p>
<p>known(k) = 1 if the gene ranked at k is a known pathogenic gene for that disease; otherwise known(k) = 0. precision(k) is the percentage of known pathogenic genes among genes ranked top-k for that disease.</p>
<p>Key semantics curation</p>
<p>The curation process consists of three main steps: linguistic lemma extraction, important lemma identification, and manual taxonomy construction.In the first step, the sentential relations extracted by LLM-ORE are tokenized to bags of words, and word inf lections are lemmatized to dictionary form.For example, cause, causes, caused, and causing all correspond to the same linguistic lemma.At this stage, entity names are also filtered out.</p>
<p>In the second step, important lemmas are identified using a coverage filter and a precision filter.The coverage filter demands a lemma to appear in LLM-ORE relations of at least n DGs.The precision filter requires that a certain proportion r of all the relations involving a lemma be from known pathogenic DGs, as indicated by ClinVar.The parameters can be adjusted according to the desired scope of key semantics.In this work, we selected a coverage parameter n = 100 and a precision parameter r = 50% and resulted in 282 important lemmas.</p>
<p>The final step involves manually curating a taxonomy of 105 key semantics by examining the LLM-ORE relations associated with these important lemmas During curation, we sampled 10 relations from known pathogenic DGs and 10 relations from other DGs to inspect for each lemma.The resulting key semantics were then used to tag all relevant relations in the respective lemmas.As a result, the LLM-ORE knowledge graph contains sentential relations linked to the semantic taxonomy of pathogenicity.</p>
<p>Furthermore, we experimented with a DG pathogenicity prediction method where the number of tagged relations for each DG is directly used as its pathogenic score.We note that the curation process has used the ClinVar information, so the generalizability of the ranking performance of this method is not directly comparable to other methods.Nevertheless, we put the effectiveness of the curation method into perspective, showing what performance and scope DG pathogenicity researchers can expect when using the key semantics tags to grasp the literature knowledge and identify relevant relations and articles for their DGs of interest.</p>
<p>Constructing the PMKB-CV dataset</p>
<p>We constructed the PMKB-CV dataset as a large-scale complement of ClinVar and an evaluation benchmark for our proposed methodology.The scope of the dataset is defined using disease IDs from Medical Subject Headings (MeSH), a vocabulary thesaurus maintained by the Nation Library of Medicine (NLM), and Homo sapiens protein-coding gene IDs from the National Center for Biotechnology Information (NCBI).</p>
<p>The PMKB-CV dataset comprises two main components including literature-based and expert database parts.For the literature part, we utilized the annotations from pubmedKB [2].The gene and variant mentions are both indexed by NCBI gene IDs, and we considered the occurrence of either a gene or its variant as an occurrence.This approach yielded 3,128,402 DG pairs with co-occurrence within abstracts, encompassing 8894 diseases and 18,393 genes.In addition, we extracted a subset of PubMed articles as the most relevant literature for the study of diseasegene relationships via a bootstrapping iteration with pubmedKB annotations as features.Using leave-one-disease-out training, we predicted a bootstrap score for each DG.Then, the literature subset is formed by the articles associated with the top three genes for each disease, the top three diseases for each gene, and the top 15 K DGs.The resulting subset contains 1,745,538 articles, for which we applied LORE and curated 11,285,095 relations.</p>
<p>The expert database part was derived from ClinVar [4], which provides gene-disease relationship data.As ClinVar uses OMIM (Online Mendelian Inheritance in Man) [6] numbers for disease indexing, we applied UMLS (Unified Medical Language System) [42], a vocabulary alignment dataset maintained by NLM, to map OMIM numbers to MeSH IDs.This process resulted in 4311 known pathogenic DGs, spanning 3175 distinct diseases and 2416 distinct genes.</p>
<p>The final PMKB-CV dataset was created by including those diseases that have both pubmedKB co-occurrence DGs and Clin-Var known pathogenic DGs.Statistics of the resulting dataset are shown in Fig. 6a.</p>
<p>Key Points</p>
<p>• We present a scalable framework that achieves 90% mean AP in identifying pathogenic gene associations across 2097 diseases, demonstrating remarkable accuracy in automated literature interpretation while effectively mitigating LLM hallucination risks.• Our analysis of literature-based semantic embeddings revealed a consistent directional pattern in how pathogenic genes are represented across different diseases.While both pathogenic and non-pathogenic disease-gene pairs cluster similarly in the embedding space, we discovered a distinct semantic f low that indicates pathogenicity.This pattern could help automate the identification of disease-causing genes from scientific literature.• The framework provides a reproducible methodology for leveraging LLMs in biomedical literature analysis, offering a valuable tool for researchers and clinicians in understanding disease mechanisms and identifying potential therapeutic targets.</p>
<p>Figure 2 .
2
Figure 2. Annotating articles with LLM-ORE (open relation extraction).Text-continuation prompts are used to make LLM write down its understanding of an article in the form of atomic factual statements.The ORE task is crafted to extract concise relations between entities at the article level.For example, 'Martin dreamed about fishing' requires comprehension and rewriting of several sentences.Also, common unwanted behaviors are avoided by providing examples of bad relations.We applied the task-agnostic prompt to extract an open set of diverse and comprehensive entity relationships for academic literature.</p>
<p>Figure 3 .
3
Figure 3. Curated key semantics.We created a taxonomy of four main classes, 15 subclasses, and 105 key semantics about disease-gene pathogenicity.With their corresponding linguistic lemmas, tags are added to relations automatically.Here, two key semantics and sample relations are shown for every subclass.</p>
<p>Figure 4 .
4
Figure 4. LLM-EMB literature-semantic embedding visualization.(a-d) Visualization with UMAP to analyze the latent pathogenicity structure within the literature-semantic embedding.Points are colored by the number of papers (#paper) (a), ClinVar pathogenicity labels (b), graded ML-ranker prediction (c), and binary ML-ranker prediction (d).The sparse ClinVar-curated red pathogenic DGs are seen clustering toward a subspace, captured well by ML-ranker, which also provides a smooth landscape of graded predictions for uncurated DGs.(e-g) Visualization with linear axes calculated using PCA (e), ridge (g), and their combination (f).A point is colored red if it is a known pathogenic DG in ClinVar, and DGs with unknown pathogenicity are colored gray.A latent structure of two manifolds-a gray ball of nonpathogenicity and a curved arm of transition from nonpathogenicity to pathogenicity-resides in the semantic space.(h-j) PCA visualization colored by distributions of literature semantics 'associate' (h), 'mutation' (i), and 'cause' (j).The connection between the smooth semantics distribution and the sparsely-curated ClinVar pathogenicity distribution can be seen.</p>
<p>Figure 5 .
5
Figure 5. Pathogenic f low in the literature-semantic space.(a) Suppose in the semantic embedding, nine DGs from three different diseases reside on a curve.(b) For the DG point-wise objective, the disease group information is not used, and absolute zero-one labels are the prediction target.As a result, a non-linear function along the curve must be learned.(c) For the disease-wise f low objective, DGs are grouped by disease, and relative f low directions are the prediction target.Because of the cross-disease consistency of the f low, a linear function along the curve will be learned to rank DGs for every disease perfectly.(d) Visualization of the actual pathogenic f low.A smooth, cross-disease consistent field of pathogenic f low is seen residing in the literature-semantic space.</p>
<p>Figure 6 .
6
Figure 6.PMKB-CV dataset and ranking performance.(a) Statistics of the PMKB-CV dataset.For 2097 diseases, the literature semantics framework has a 200× prediction scope against curated DGs.(b) Mean average precision (MAP) of the ranking performance and the ClinVar coverage of different methods.GPT-4o does not perform well.In comparison, LLM-EMB linear regression alone achieves 87.7% performance, whereas the full-f ledged MLranker provides higher performance at 90.0% or higher coverage at 94.8%.(c) Dispersion of ranking performance across diseases for each method and the p-values of distribution differences by one-sided Wilcoxon signed-rank test.LLM-ranker significantly outperformed baseline methods.(d) Performance across diseases of different scopes.The naive paper counting method encountered difficulties while ranking DGs for diseases that co-occurred with many genes in PMKB, but our semantic embedding approach remained robust.(e) Extending the scope of LORE using the public llama-8B model.Replacing GPT-3.5 with llama-8B resulted in marginal performance loss.The smaller model was further applied to all 3.9 million papers with DG co-occurrence.The final extracted relations covered 91.3% of ClinVar DGs and achieved 81.6% ranking performance.(f) Top-ranked DGs, seven out of 586 in PMKB, for Tourette syndrome accompanied by their literature relation evidence.</p>
<p>d∈De 1 ,e 2
2
emb d; m × | d | d∈De 1 ,e 2 | d | where e 1 and e 2 are the target entities, D e1,e2 is the set of all sub-documents, usually just one full document, containing the relations between e 1 and e 2 , and m is the LLM model.In this work, we employed the text-embedding-3-large from OpenAI for m, and the length of each sub-document | d | is its number of tokens according to m.</p>
<p>loss.L DG1,DG2 = − log Pr (P(DG1) &gt; P(DG2)) = log 1 + e −σ (sDG1−sDG2)</p>
<p>AcknowledgementsThis manuscript was edited by Wallace Academic Editing.The authors also thank Dau-Ming Niu and Yun-Ru Chen at the Taipei Veterans General Hospital in Taiwan for their support (NSTC 113-2634-F-A49-003).Code availabilityThe code supporting the conclusions of this study is available on GitHub at https://github.com/ailabstw/LORE.Data availabilityThe PMKB-CV datasets supporting the findings of this study are available at https://doi.org/10.5281/zenodo.14607639.FundingThis work was supported in part by the Center for Advanced Computing and Imaging in Biomedicine (NTU-113 L900701) from The Featured Areas Research Center Program within the framework of the Higher Education Sprout Project by the Ministry of Education in Taiwan.Author contributionsSupplementary dataSupplementary data are available at Briefings in Bioinformatics online.Conf lict of interest: None declared.
How user intelligence is improving PubMed. N Fiorini, R Leaman, D J Lipman, 10.1038/nbt.4267Nat Biotechnol. 362018</p>
<p>pubmedKB: an interactive web server for exploring biomedical entity relations in the biomedical literature. P H Li, T F Chen, J Y Yu, 10.1093/nar/gkac310Nucleic Acids Res. 502022</p>
<p>PubMed and beyond: biomedical literature search in the age of artificial intelligence. Q Jin, R Leaman, Z Lu, 10.1016/j.ebiom.2024.104988EBioMedicine. 1001049882024</p>
<p>ClinVar: public archive of interpretations of clinically relevant variants. M J Landrum, J M Lee, M Benson, 10.1093/nar/gkv1222Nucleic Acids Res. 442016</p>
<p>COSMIC: the catalogue of somatic mutations In cancer. J G Tate, S Bamford, H C Jubb, 10.1093/nar/gky1015Nucleic Acids Res. 472019</p>
<p>OMIM.org: leveraging knowledge across phenotype-gene relationships. J S Amberger, C A Bocchini, A F Scott, 10.1093/nar/gky1151Nucleic Acids Res. 472019</p>
<p>An evidence-based framework for evaluating pharmacogenomics knowledge for personalized medicine. M Whirl-Carrillo, R Huddart, L Gong, 10.1002/cpt.2350Clin Pharmacol Ther. 1102021</p>
<p>Biomedical language processing: what's beyond PubMed?. L Hunter, K B Cohen, 10.1016/j.molcel.2006.02.012Mol Cell. 212006</p>
<p>Manual curation is not sufficient for annotation of genomic databases. W A Baumgartner, Jr, K B Cohen, L M Fox, 10.1093/bioinformatics/btm229Bioinformatics. 232007</p>
<p>Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research. A Bravo, J Pinero, N Queralt-Rosinach, 10.1186/s12859-015-0472-9BMC Bioinform. 16552015</p>
<p>DisGeNET: a comprehensive platform integrating information on human diseaseassociated genes and variants. J Pinero, À Bravo, N Queralt-Rosinach, 10.1093/nar/gkw943Nucleic Acids Res. 452017</p>
<p>RENET: a deep learning approach for extracting gene-disease associations from literature. Y Wu, R Luo, Hcm Leung, 10.1007/978-3-030-17083-7_17Research in Computational Molecular Biology. 114672019</p>
<p>PGxCorpus, a manually annotated corpus for pharmacogenomics. J Legrand, R Gogdemir, C Bousquet, 10.1038/s41597-019-0342-9Sci Data. 732020</p>
<p>ACE2 expression is increased in the lungs of patients with comorbidities associated with severe COVID-19. Bgg Pinto, Aer Oliveira, Y Singh, 10.1093/infdis/jiaa332J Infect Dis. 2222020</p>
<p>Molecular and networklevel mechanisms explaining individual differences in autism spectrum disorder. A M Buch, P E Vértes, J Seidlitz, 10.1038/s41593-023-01259-xNat Neurosci. 262023</p>
<p>Graph embedding-based link prediction for literature-based discovery in Alzheimer's disease. Y Pu, D Beck, K Verspoor, 10.1016/j.jbi.2023.104464J Biomed Inform. 1451044642023</p>
<p>The STRING database in 2023: protein-protein association networks and functional enrichment analyses for any sequenced genome of interest. D Szklarczyk, R Kirsch, M Koutrouli, 10.1093/nar/gkac1000Nucleic Acids Res. 512023</p>
<p>DTMiner: identification of potential disease targets through biomedical literature mining. D Xu, M Zhang, Y Xie, 10.1093/bioinformatics/btw503201632Bioinformatics</p>
<p>A global network of biomedical relationships derived from text. B Percha, R B Altman, 10.1093/bioinformatics/bty114Bioinformatics. 342018</p>
<p>BioREx: improving biomedical relation extraction by leveraging heterogeneous datasets. P T Lai, C H Wei, L Luo, 10.1016/j.jbi.2023.104487J Biomed Inform. 1461044872023</p>
<p>PubTator 3.0: an AI-powered literature resource for unlocking biomedical knowledge. C H Wei, A Allot, P T Lai, 10.1093/nar/gkae235Nucleic Acids Res. 522024</p>
<p>Neural machine reading comprehension: methods and trends. S Liu, X Zhang, S Zhang, 10.3390/app9183698Appl Sci. 936982019</p>
<p>Benefits, limits, and risks of GPT-4 as an AI Chatbot for medicine. P Lee, S Bubeck, J Petro, 10.1056/NEJMsr2214184N Engl J Med. 3882023</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, 10.1038/s41586-023-06291-2Nature. 6202023</p>
<p>Assessing GPT-4 for cell type annotation in singlecell RNA-seq analysis. W Hou, Ji Z , 10.1038/s41592-024-02235-4Nat Methods. 212024</p>
<p>Retrieve, summarize, and Verify: how will ChatGPT affect information seeking from the medical literature?. Q Jin, R Leaman, Z Lu, 10.1681/ASN.0000000000000166J Am Soc Nephrol. 342023</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. P Lewis, F Petroni, V Karpukhin, Adv Neural Inf Process Syst. 332020</p>
<p>Retrieval-augmented generation for large language models: a survey. Y Gao, Y Xiong, X Gao, 10.48550/arXiv.2312.10997arXiv:2312.109972023arXiv Preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, Adv Neural Inf Process Syst. 352022</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, Adv Neural Inf Process Syst. 332020</p>
<p>Effectiveness and efficiency of open relation extraction. F Mesquita, J Schmidek, D Barbosa, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational Linguistics2013</p>
<p>UMAP: uniform manifold approximation and projection. L Mcinnes, J Healy, N Saul, 10.21105/joss.00861J Open Source Softw. 38612018</p>
<p>Adapting boosting for information retrieval measures. Q Wu, Cjc Burges, K M Svore, 10.1007/s10791-009-9112-1Inf Retr. 132009</p>
<p>The impact of large language models on scientific discovery: a preliminary study using. M Research Ai4science, Azure Quantum, M , 10.48550/arXiv.2311.07361arXiv:2311.073612023GPT-4. arXiv Preprint</p>
<p>Benchmarking large language models for news summarization. T Zhang, F Ladhak, E Durmus, 10.1162/tacl_a_00632Trans Assoc Comput Linguist. 122024</p>
<p>G Team, T Mesnard, C Hardin, 10.48550/arXiv.2403.08295arXiv:2403.08295open models based on Gemini research and technology. 2024arXiv Preprint</p>
<p>QLoRA: efficient Finetuning of quantized LLMs. T Dettmers, A Pagnoni, A Holtzman, Adv Neural Inf Process Syst. 362023</p>
<p>MEDITRON-70B: scaling medical Pretraining for large language models. Z Chen, A H Cano, A Romanou, 10.48550/arXiv.2311.16079arXiv:2311.160792023arXiv Preprint</p>
<p>Learning to rank using gradient descent. C Burges, T Shaked, E Renshaw, Proceedings of the 22nd international conference on Machine learning -ICML '05. the 22nd international conference on Machine learning -ICML '05New York, NY, USAAssociation for Computing Machinery2005</p>
<p>Cumulated gain-based evaluation of IR techniques. K Järvelin, J Kekäläinen, 10.1145/582415.582418ACM Trans Inf Syst. 202002</p>
<p>Learning to rank with nonsmooth cost functions. C Burges, R Ragno, Q Le, 10.7551/mitpress/7503.003.0029Adv Neural Inf Process Syst. 192006</p>
<p>The unified medical language system (UMLS): integrating biomedical terminology. O Bodenreider, 10.1093/nar/gkh061Nucleic Acids Res. 322004</p>
<p>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup. Author The, 10.1093/bib/bbaf070Briefings in Bioinformatics. 26120252025Oxford University PressThis is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License</p>
<p>. Problem Solving Protocol. </p>            </div>
        </div>

    </div>
</body>
</html>
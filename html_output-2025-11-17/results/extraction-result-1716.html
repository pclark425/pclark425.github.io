<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1716 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1716</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1716</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-270123141</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.20267v4.pdf" target="_blank">Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and Committee Discussions</a></p>
                <p><strong>Paper Abstract:</strong> As LLMs continuously evolve, there is an urgent need for a reliable evaluation method that delivers trustworthy results promptly. Currently, static benchmarks suffer from inflexibility and unreliability, leading users to prefer human voting platforms like Chatbot Arena. However, human evaluations require significant manual effort. To address this, we propose the Auto-Arena, an innovative framework that automates the entire evaluation process using LLM-powered agents. Firstly, an LLM examiner generates questions. Then, two LLM candidates engage in a multi-round peer battle based on individual questions, aiming at revealing their true performance differences. Finally, a committee of LLM judges collaboratively discusses and decides the winner, reducing bias and enhancing fairness. During the peer battles, we observe intriguing scenarios where the LLM candidates display competitive behaviors and even learn from the opponents. In our extensive experiments involving 15 recent LLMs, Auto-Arena shows a 92.14% correlation with human preferences, surpassing all previous expert-annotated benchmarks without any manual efforts. As a result, Auto-Arena offers a promising alternative to current human evaluation platforms for evaluating LLMs automatically.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1716.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1716.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-Arena</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and Committee Discussions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fully-automatic evaluation framework that (1) generates dynamic questions with an LLM examiner, (2) runs multi-round peer battles between two candidate LLMs, and (3) uses a committee of LLM judges that individually vote and then discuss to decide winners; designed to align automated judgments with human preferences while mitigating contamination and single-model bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (committee) with multi-round peer-battles (dynamic, multi-turn comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>committee of top-5 LLMs (selected dynamically from strongest/current ranking); excludes participants and same-family models (examples include GPT-4, Claude series, Qwen, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>pairwise comparison with individual written judgements followed by an explicit discussion round among judges and a majority-vote final decision</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>open-ended LLM responses (includes code generation outputs as one of 8 evaluation categories)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general-purpose; includes coding (various languages) among other user-oriented categories (writing, roleplay, extraction, reasoning, math, STEM, humanities/social sciences)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>helpfulness, relevance, accuracy, depth, creativity (explicitly instructed in judge prompts); also penalizes verbosity via length constraints</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>comparison reference is Chatbot Arena human preference Elo scores (crowd votes); Auto-Arena's rankings are compared to Chatbot Arena Elos to measure alignment</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>crowd human voters on Chatbot Arena (anonymous votes aggregated into Elo scores); not a curated panel of domain experts</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman correlation with human preference Elo rankings; additionally Cohen's Kappa and agreement-probability used for inter-judge agreement</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Spearman rho = 0.9214 (reported as 92.14% correlation with Chatbot Arena human preferences)</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>dynamic multi-round peer battles (reveal deeper abilities) and committee discussion (synthesizes viewpoints) — both increase alignment; limiting verbosity bias and excluding judges from same model family; initializing committee via MMLU gives stable judge selection</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>removing peer-battles (single-round evaluation) or removing committee discussion lowers alignment; single-judge evaluations and absence of discussion produce lower agreement with humans</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>peer-battles increase visibility of performance gaps for deeper/complex tasks (e.g., math reasoning and coding), improving agreement; hence agreement improves for complex, multi-turn artifacts when peer-battles are used</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>explicit judge prompts (focusing on helpfulness, relevance, accuracy and prohibiting length bias) and length limits reduce verbosity bias and improve alignment; committee discussion increases inter-judge agreement by exposing judges to alternative rationales</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>Tournament with 9 initial models expanded to 15 models; each candidate pair engages in 40 peer battles (5 questions × 8 categories) — 40 questions per pair; committee of 5 judges per debate</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>comparison baseline: MT-Bench human evaluation agreement reported as ~67% agreement probability (used as human-level reference in paper); Auto-Arena's committee agreement after discussion reaches ~64% (close to MT-Bench human level)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Auto-Arena achieves Spearman rho = 0.9214 with Chatbot Arena human Elo (higher than other automated baselines cited); committee agreement probability after discussion = 64% vs MT-Bench human evaluation = 67%</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>no explicit fine-tuning on human labels; committee initialization uses MMLU scores to approximate initial judge strengths and the best-performing judge provides reference answers for certain logical-reasoning questions</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Auto-Arena (LLM-committee + peer battles) attains state-of-the-art alignment with human preferences (Spearman ~0.9214) without human annotations; ablations show peer-battles raise Spearman by ~5% versus no-debate, and committee discussion raises inter-judge agreement (agreement-probability) by ~11% approaching human-level agreement; dynamic questions and multi-turn debate reduce contamination and reveal deeper model differences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>relies on availability of strong judge LLMs (committee selection mitigates single-model bias but still depends on model pool), potential residual self-enhancement bias from examiner (ablation shows limited effect), remaining gaps to exact human agreement, and some inconsistent inter-judge agreement measures across contexts; evaluation cost and judge-quality depend on selected LLMs; the framework evaluates general LLM outputs including code but is not a dedicated software-engineering expert review process.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and Committee Discussions', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1716.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1716.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge (single-model)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single LLM used as a judge (e.g., GPT-4) for pairwise response comparison</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A common automated evaluation approach where a single strong LLM (often GPT-4) is prompted to compare or rate model outputs (pairwise comparisons or rubric-based scoring), widely used in prior benchmarks such as MT-Bench and AlpacaEval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (single-model comparison/rubric) — pairwise response comparison</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (commonly cited); also GPT-4-Turbo in some baselines</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>pairwise comparison: judge reads two model responses to the same prompt and selects the better answer based on given criteria (sometimes with reference-guided evaluation for reasoning questions)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>open-ended LLM responses (includes code generation in benchmarks such as MT-Bench)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general-purpose; MT-Bench includes multi-turn dialogues and coding/reasoning examples</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>helpfulness, accuracy, correctness, adherence to instructions; sometimes rubric-based dimensions (factuality, helpfulness etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>reported agreement with human preferences (percentage or correlation) in cited prior work</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>prior studies report single-LM judges (e.g., GPT-4) achieve over ~80% agreement with human preferences on some tasks (paper cites 'over 80% agreement' for MT-Bench and AlpacaEval)</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>static datasets with clear answerability and when judge model is strong and prompts control verbosity; well-defined pairwise prompts and use of reference answers for reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>evaluating candidates with close performance, subjective or multi-dimensional criteria, multilingual outputs (inflated scores reported), verbosity bias (judges prefer longer outputs if not constrained), and single-model bias (model-specific preferences)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>single-LM judges can struggle for closely-matched or complex multi-turn interactions; paper reports lower robustness when candidate performances are close</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>lack of discussion and single-model perspectives limits ability to cover diverse viewpoints; absence of explicit penalties for verbosity can bias outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>single-LM judges historically approach human agreement (~80% reported), but Auto-Arena (committee + debates) outperforms them (Auto-Arena 92.14% vs prior ~80% agreements reported for single-LM judging baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>in baseline methods, judges were not necessarily fine-tuned on human labels (they are used zero-shot or with prompting); some methods use reference-guided judge instructions</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-as-a-judge (single strong model) can reach reasonably high agreement with human judgments (~80% in prior work) but suffers from single-model biases (verbosity bias, multilingual inflation, bias toward LLM-generated summaries) and has reduced robustness when models have similar performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>single-model bias, verbosity bias, inflated multilingual scores, difficulties distinguishing closely-performing candidates, and computational constraints when one examiner must interact with many candidates in parallel.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and Committee Discussions', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1716.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1716.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Static & Model-based Benchmarks (MT-Bench / AlpacaEval / LC-AlpacaEval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Static datasets and model-based automated benchmarks (e.g., MT-Bench, AlpacaEval, LC-AlpacaEval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmarks that evaluate models on a fixed set of (often open-ended) questions and use automated or LLM-based grading (commonly GPT-4) to score responses; these offer reproducible metrics but face contamination and rigidity issues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>static dataset evaluation with LLM-based grading (LLM-as-a-judge) or fixed metrics (accuracy for closed-form tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 in MT-Bench and GPT-4-Turbo for some datasets; LC-AlpacaEval uses length-controlled LLM grading (GPT-4 variants cited)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>pairwise comparison or single-response rating guided by prompts; LC-AlpacaEval uses length control to mitigate verbosity bias</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>open-ended responses and specific closed-form answers; includes code-related coding tasks within MT-Bench categories</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general-purpose; some datasets concentrate on reasoning, coding, or instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>task-specific correctness (for closed-form), helpfulness/quality/following instructions for open-ended tasks</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>benchmarks compared their automated scores against human preference data or human annotations in prior work (e.g., MT-Bench human eval baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>varies by benchmark; MT-Bench uses curated expert-style questions but human annotation counts not detailed here</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>reported agreement (percentage or correlation) with human preferences; the paper cites Spearman correlations and percentages for comparisons to Chatbot Arena Elos</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>paper cites that MT-Bench and AlpacaEval-type model-based evaluations achieve over ~80% agreement with human preferences in prior work; in the paper's comparisons many baselines range from ~76% to ~88% depending on dataset/experimental set (exact table values vary across 9- vs 15-model analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>when question sets are high-quality and judge prompts control for verbosity and provide clear evaluation rubric; length-controlled evaluation increases alignment</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>static question contamination risk (models may have been exposed to test data), lack of dynamic multi-turn interactions (one-shot Q&A), and single-judge biases reduce robustness and alignment</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>static single-turn evaluations miss deeper multi-turn capabilities (reasoning, criticizing, strategizing) leading to lower discrimination on complex artifacts; peer-battle dynamics in Auto-Arena remedy this</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>explicit rubrics and length control (e.g., LC-AlpacaEval) help mitigate verbosity bias and improve agreement compared to unconstrained single-judge methods</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>these benchmarks historically show substantial but lower alignment than Auto-Arena; reported agreement often around or above 80% vs Auto-Arena's ~92.14%</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>generally no explicit fine-tuning on human judgments; uses prompting and sometimes reference answers for reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>static or single-judge model-based benchmarks can achieve substantial agreement with human preferences (~80%+), but face contamination, inflexibility, and single-model biases; Auto-Arena's dynamic debates and committee discussions improve alignment beyond these methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>risk of contamination from static test sets, less capability to expose multi-turn or strategic skills, reliance on a single strong judge leading to model-specific biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and Committee Discussions', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1716.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1716.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chatbot Arena (human baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chatbot Arena: an open platform for evaluating LLMs by human preference votes (Elo-based leaderboard)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A crowdsourced human-evaluation platform where anonymous human votes on pairwise model comparisons are aggregated into Elo scores, treated in this paper as the human-preference gold standard for alignment comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatbot arena: An open platform for evaluating llms by human preference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>human preference voting aggregated via Elo/Bradley-Terry coefficients (not a proxy automated method)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>general LLM outputs (multi-domain, user queries), includes code-related queries among many user categories</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general-purpose; predominantly English queries on platform</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>human preference in pairwise comparisons (overall helpfulness/usability as interpreted by human voters)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>open participation; many anonymous human votes per model pair aggregated into Elo ratings; used as the reference ranking (gold standard) in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>general user population (not necessarily domain experts); platform requires many votes for reliability</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Elo scores aggregated from votes; used as the target for Spearman correlation comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>large numbers of human votes yield more reliable Elo rankings; human voting captures real user preference across varied queries</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>low vote counts for new models (unstable rankings), language skew (mostly English queries), and variable participant quality produce uneven evaluation quality</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>platform mostly contains one-round, simpler queries—may under-represent multi-turn or complex tasks (which Auto-Arena targets with peer battles)</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>human voters apply subjective preferences; lack of explicit rubrics can create variance but captures natural human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>Chatbot Arena leaderboard models had >10k votes each at time of experiment for the top models used as references</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>served as the human-preference gold standard; automated methods' Spearman correlations (e.g., Auto-Arena 92.14%, MT-Bench ~82% reported) are computed against Chatbot Arena Elo rankings</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Human votes on Chatbot Arena are used as the reference for alignment; Auto-Arena aims to recover these human-derived Elo rankings automatically and reports high correlation (92.14%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>requires many votes to be reliable, language bias toward English, one-round query bias, and open participation can cause uneven evaluation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and Committee Discussions', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MT-Bench <em>(Rating: 2)</em></li>
                <li>Length-controlled alpacaeval <em>(Rating: 2)</em></li>
                <li>AlpacaEval <em>(Rating: 2)</em></li>
                <li>Chatbot arena: An open platform for evaluating llms by human preference <em>(Rating: 2)</em></li>
                <li>Language-model-as-an-examiner <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1716",
    "paper_id": "paper-270123141",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "Auto-Arena",
            "name_full": "Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and Committee Discussions",
            "brief_description": "A fully-automatic evaluation framework that (1) generates dynamic questions with an LLM examiner, (2) runs multi-round peer battles between two candidate LLMs, and (3) uses a committee of LLM judges that individually vote and then discuss to decide winners; designed to align automated judgments with human preferences while mitigating contamination and single-model bias.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge (committee) with multi-round peer-battles (dynamic, multi-turn comparison)",
            "llm_judge_model": "committee of top-5 LLMs (selected dynamically from strongest/current ranking); excludes participants and same-family models (examples include GPT-4, Claude series, Qwen, etc.)",
            "llm_judge_prompt_approach": "pairwise comparison with individual written judgements followed by an explicit discussion round among judges and a majority-vote final decision",
            "artifact_type": "open-ended LLM responses (includes code generation outputs as one of 8 evaluation categories)",
            "artifact_domain": "general-purpose; includes coding (various languages) among other user-oriented categories (writing, roleplay, extraction, reasoning, math, STEM, humanities/social sciences)",
            "evaluation_criteria": "helpfulness, relevance, accuracy, depth, creativity (explicitly instructed in judge prompts); also penalizes verbosity via length constraints",
            "human_evaluation_setup": "comparison reference is Chatbot Arena human preference Elo scores (crowd votes); Auto-Arena's rankings are compared to Chatbot Arena Elos to measure alignment",
            "human_expert_count": null,
            "human_expert_expertise": "crowd human voters on Chatbot Arena (anonymous votes aggregated into Elo scores); not a curated panel of domain experts",
            "agreement_metric": "Spearman correlation with human preference Elo rankings; additionally Cohen's Kappa and agreement-probability used for inter-judge agreement",
            "agreement_score": "Spearman rho = 0.9214 (reported as 92.14% correlation with Chatbot Arena human preferences)",
            "high_agreement_conditions": "dynamic multi-round peer battles (reveal deeper abilities) and committee discussion (synthesizes viewpoints) — both increase alignment; limiting verbosity bias and excluding judges from same model family; initializing committee via MMLU gives stable judge selection",
            "low_agreement_conditions": "removing peer-battles (single-round evaluation) or removing committee discussion lowers alignment; single-judge evaluations and absence of discussion produce lower agreement with humans",
            "artifact_complexity_effect": "peer-battles increase visibility of performance gaps for deeper/complex tasks (e.g., math reasoning and coding), improving agreement; hence agreement improves for complex, multi-turn artifacts when peer-battles are used",
            "criteria_clarity_effect": "explicit judge prompts (focusing on helpfulness, relevance, accuracy and prohibiting length bias) and length limits reduce verbosity bias and improve alignment; committee discussion increases inter-judge agreement by exposing judges to alternative rationales",
            "sample_size": "Tournament with 9 initial models expanded to 15 models; each candidate pair engages in 40 peer battles (5 questions × 8 categories) — 40 questions per pair; committee of 5 judges per debate",
            "inter_human_agreement": "comparison baseline: MT-Bench human evaluation agreement reported as ~67% agreement probability (used as human-level reference in paper); Auto-Arena's committee agreement after discussion reaches ~64% (close to MT-Bench human level)",
            "proxy_vs_human_comparison": "Auto-Arena achieves Spearman rho = 0.9214 with Chatbot Arena human Elo (higher than other automated baselines cited); committee agreement probability after discussion = 64% vs MT-Bench human evaluation = 67%",
            "calibration_or_training": "no explicit fine-tuning on human labels; committee initialization uses MMLU scores to approximate initial judge strengths and the best-performing judge provides reference answers for certain logical-reasoning questions",
            "key_findings": "Auto-Arena (LLM-committee + peer battles) attains state-of-the-art alignment with human preferences (Spearman ~0.9214) without human annotations; ablations show peer-battles raise Spearman by ~5% versus no-debate, and committee discussion raises inter-judge agreement (agreement-probability) by ~11% approaching human-level agreement; dynamic questions and multi-turn debate reduce contamination and reveal deeper model differences.",
            "limitations_noted": "relies on availability of strong judge LLMs (committee selection mitigates single-model bias but still depends on model pool), potential residual self-enhancement bias from examiner (ablation shows limited effect), remaining gaps to exact human agreement, and some inconsistent inter-judge agreement measures across contexts; evaluation cost and judge-quality depend on selected LLMs; the framework evaluates general LLM outputs including code but is not a dedicated software-engineering expert review process.",
            "uuid": "e1716.0",
            "source_info": {
                "paper_title": "Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and Committee Discussions",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLM-as-a-judge (single-model)",
            "name_full": "Single LLM used as a judge (e.g., GPT-4) for pairwise response comparison",
            "brief_description": "A common automated evaluation approach where a single strong LLM (often GPT-4) is prompted to compare or rate model outputs (pairwise comparisons or rubric-based scoring), widely used in prior benchmarks such as MT-Bench and AlpacaEval.",
            "citation_title": "",
            "mention_or_use": "mention",
            "proxy_evaluation_method": "LLM-as-a-judge (single-model comparison/rubric) — pairwise response comparison",
            "llm_judge_model": "GPT-4 (commonly cited); also GPT-4-Turbo in some baselines",
            "llm_judge_prompt_approach": "pairwise comparison: judge reads two model responses to the same prompt and selects the better answer based on given criteria (sometimes with reference-guided evaluation for reasoning questions)",
            "artifact_type": "open-ended LLM responses (includes code generation in benchmarks such as MT-Bench)",
            "artifact_domain": "general-purpose; MT-Bench includes multi-turn dialogues and coding/reasoning examples",
            "evaluation_criteria": "helpfulness, accuracy, correctness, adherence to instructions; sometimes rubric-based dimensions (factuality, helpfulness etc.)",
            "human_evaluation_setup": null,
            "human_expert_count": null,
            "human_expert_expertise": null,
            "agreement_metric": "reported agreement with human preferences (percentage or correlation) in cited prior work",
            "agreement_score": "prior studies report single-LM judges (e.g., GPT-4) achieve over ~80% agreement with human preferences on some tasks (paper cites 'over 80% agreement' for MT-Bench and AlpacaEval)",
            "high_agreement_conditions": "static datasets with clear answerability and when judge model is strong and prompts control verbosity; well-defined pairwise prompts and use of reference answers for reasoning tasks",
            "low_agreement_conditions": "evaluating candidates with close performance, subjective or multi-dimensional criteria, multilingual outputs (inflated scores reported), verbosity bias (judges prefer longer outputs if not constrained), and single-model bias (model-specific preferences)",
            "artifact_complexity_effect": "single-LM judges can struggle for closely-matched or complex multi-turn interactions; paper reports lower robustness when candidate performances are close",
            "criteria_clarity_effect": "lack of discussion and single-model perspectives limits ability to cover diverse viewpoints; absence of explicit penalties for verbosity can bias outcomes",
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "single-LM judges historically approach human agreement (~80% reported), but Auto-Arena (committee + debates) outperforms them (Auto-Arena 92.14% vs prior ~80% agreements reported for single-LM judging baselines)",
            "calibration_or_training": "in baseline methods, judges were not necessarily fine-tuned on human labels (they are used zero-shot or with prompting); some methods use reference-guided judge instructions",
            "key_findings": "LLM-as-a-judge (single strong model) can reach reasonably high agreement with human judgments (~80% in prior work) but suffers from single-model biases (verbosity bias, multilingual inflation, bias toward LLM-generated summaries) and has reduced robustness when models have similar performance.",
            "limitations_noted": "single-model bias, verbosity bias, inflated multilingual scores, difficulties distinguishing closely-performing candidates, and computational constraints when one examiner must interact with many candidates in parallel.",
            "uuid": "e1716.1",
            "source_info": {
                "paper_title": "Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and Committee Discussions",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Static & Model-based Benchmarks (MT-Bench / AlpacaEval / LC-AlpacaEval)",
            "name_full": "Static datasets and model-based automated benchmarks (e.g., MT-Bench, AlpacaEval, LC-AlpacaEval)",
            "brief_description": "Benchmarks that evaluate models on a fixed set of (often open-ended) questions and use automated or LLM-based grading (commonly GPT-4) to score responses; these offer reproducible metrics but face contamination and rigidity issues.",
            "citation_title": "",
            "mention_or_use": "mention",
            "proxy_evaluation_method": "static dataset evaluation with LLM-based grading (LLM-as-a-judge) or fixed metrics (accuracy for closed-form tasks)",
            "llm_judge_model": "GPT-4 in MT-Bench and GPT-4-Turbo for some datasets; LC-AlpacaEval uses length-controlled LLM grading (GPT-4 variants cited)",
            "llm_judge_prompt_approach": "pairwise comparison or single-response rating guided by prompts; LC-AlpacaEval uses length control to mitigate verbosity bias",
            "artifact_type": "open-ended responses and specific closed-form answers; includes code-related coding tasks within MT-Bench categories",
            "artifact_domain": "general-purpose; some datasets concentrate on reasoning, coding, or instruction following",
            "evaluation_criteria": "task-specific correctness (for closed-form), helpfulness/quality/following instructions for open-ended tasks",
            "human_evaluation_setup": "benchmarks compared their automated scores against human preference data or human annotations in prior work (e.g., MT-Bench human eval baseline)",
            "human_expert_count": null,
            "human_expert_expertise": "varies by benchmark; MT-Bench uses curated expert-style questions but human annotation counts not detailed here",
            "agreement_metric": "reported agreement (percentage or correlation) with human preferences; the paper cites Spearman correlations and percentages for comparisons to Chatbot Arena Elos",
            "agreement_score": "paper cites that MT-Bench and AlpacaEval-type model-based evaluations achieve over ~80% agreement with human preferences in prior work; in the paper's comparisons many baselines range from ~76% to ~88% depending on dataset/experimental set (exact table values vary across 9- vs 15-model analyses)",
            "high_agreement_conditions": "when question sets are high-quality and judge prompts control for verbosity and provide clear evaluation rubric; length-controlled evaluation increases alignment",
            "low_agreement_conditions": "static question contamination risk (models may have been exposed to test data), lack of dynamic multi-turn interactions (one-shot Q&A), and single-judge biases reduce robustness and alignment",
            "artifact_complexity_effect": "static single-turn evaluations miss deeper multi-turn capabilities (reasoning, criticizing, strategizing) leading to lower discrimination on complex artifacts; peer-battle dynamics in Auto-Arena remedy this",
            "criteria_clarity_effect": "explicit rubrics and length control (e.g., LC-AlpacaEval) help mitigate verbosity bias and improve agreement compared to unconstrained single-judge methods",
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "these benchmarks historically show substantial but lower alignment than Auto-Arena; reported agreement often around or above 80% vs Auto-Arena's ~92.14%",
            "calibration_or_training": "generally no explicit fine-tuning on human judgments; uses prompting and sometimes reference answers for reasoning tasks",
            "key_findings": "static or single-judge model-based benchmarks can achieve substantial agreement with human preferences (~80%+), but face contamination, inflexibility, and single-model biases; Auto-Arena's dynamic debates and committee discussions improve alignment beyond these methods.",
            "limitations_noted": "risk of contamination from static test sets, less capability to expose multi-turn or strategic skills, reliance on a single strong judge leading to model-specific biases.",
            "uuid": "e1716.2",
            "source_info": {
                "paper_title": "Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and Committee Discussions",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Chatbot Arena (human baseline)",
            "name_full": "Chatbot Arena: an open platform for evaluating LLMs by human preference votes (Elo-based leaderboard)",
            "brief_description": "A crowdsourced human-evaluation platform where anonymous human votes on pairwise model comparisons are aggregated into Elo scores, treated in this paper as the human-preference gold standard for alignment comparisons.",
            "citation_title": "Chatbot arena: An open platform for evaluating llms by human preference",
            "mention_or_use": "use",
            "proxy_evaluation_method": "human preference voting aggregated via Elo/Bradley-Terry coefficients (not a proxy automated method)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "general LLM outputs (multi-domain, user queries), includes code-related queries among many user categories",
            "artifact_domain": "general-purpose; predominantly English queries on platform",
            "evaluation_criteria": "human preference in pairwise comparisons (overall helpfulness/usability as interpreted by human voters)",
            "human_evaluation_setup": "open participation; many anonymous human votes per model pair aggregated into Elo ratings; used as the reference ranking (gold standard) in the paper",
            "human_expert_count": null,
            "human_expert_expertise": "general user population (not necessarily domain experts); platform requires many votes for reliability",
            "agreement_metric": "Elo scores aggregated from votes; used as the target for Spearman correlation comparisons",
            "agreement_score": null,
            "high_agreement_conditions": "large numbers of human votes yield more reliable Elo rankings; human voting captures real user preference across varied queries",
            "low_agreement_conditions": "low vote counts for new models (unstable rankings), language skew (mostly English queries), and variable participant quality produce uneven evaluation quality",
            "artifact_complexity_effect": "platform mostly contains one-round, simpler queries—may under-represent multi-turn or complex tasks (which Auto-Arena targets with peer battles)",
            "criteria_clarity_effect": "human voters apply subjective preferences; lack of explicit rubrics can create variance but captures natural human judgments",
            "sample_size": "Chatbot Arena leaderboard models had &gt;10k votes each at time of experiment for the top models used as references",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "served as the human-preference gold standard; automated methods' Spearman correlations (e.g., Auto-Arena 92.14%, MT-Bench ~82% reported) are computed against Chatbot Arena Elo rankings",
            "calibration_or_training": null,
            "key_findings": "Human votes on Chatbot Arena are used as the reference for alignment; Auto-Arena aims to recover these human-derived Elo rankings automatically and reports high correlation (92.14%).",
            "limitations_noted": "requires many votes to be reliable, language bias toward English, one-round query bias, and open participation can cause uneven evaluation quality.",
            "uuid": "e1716.3",
            "source_info": {
                "paper_title": "Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and Committee Discussions",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MT-Bench",
            "rating": 2
        },
        {
            "paper_title": "Length-controlled alpacaeval",
            "rating": 2,
            "sanitized_title": "lengthcontrolled_alpacaeval"
        },
        {
            "paper_title": "AlpacaEval",
            "rating": 2,
            "sanitized_title": "alpacaeval"
        },
        {
            "paper_title": "Chatbot arena: An open platform for evaluating llms by human preference",
            "rating": 2,
            "sanitized_title": "chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference"
        },
        {
            "paper_title": "Language-model-as-an-examiner",
            "rating": 1,
            "sanitized_title": "languagemodelasanexaminer"
        }
    ],
    "cost": 0.017704499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AUTO-ARENA:AUTOMATING LLM EVALUATIONS WITH AGENT PEER BATTLES AND COMMITTEE DISCUSSIONS
7 Oct 2024</p>
<p>Ruochen Zhao ruochen002@e.ntu.edu.sg 
Nanyang Technological University
Singapore</p>
<p>DAMO Academy, Alibaba Group</p>
<p>Wenxuan Zhang 
DAMO Academy, Alibaba Group</p>
<p>Ken Chia yewken.chia@alibaba-inc.com 
DAMO Academy, Alibaba Group</p>
<p>Singapore University of Technology</p>
<p>Weiwen Xu xuweiwen.xww@alibaba-inc.com 
DAMO Academy, Alibaba Group</p>
<p>Deli Zhao deli.zdl@alibaba-inc.com 
DAMO Academy, Alibaba Group</p>
<p>Lidong Bing l.bing@alibaba-inc.com 
DAMO Academy, Alibaba Group</p>
<p>AUTO-ARENA:AUTOMATING LLM EVALUATIONS WITH AGENT PEER BATTLES AND COMMITTEE DISCUSSIONS
7 Oct 20249CFE782930C8F74065748F5B89D9E281arXiv:2405.20267v4[cs.CL]
As LLMs continuously evolve, there is an urgent need for a reliable evaluation method that delivers trustworthy results promptly.Currently, static benchmarks suffer from inflexibility and unreliability, leading users to prefer human voting platforms like Chatbot Arena.However, human evaluations require significant manual effort.To address this, we propose the Auto-Arena, an innovative framework that automates the entire evaluation process using LLM-powered agents.Firstly, an LLM examiner generates questions.Then, two LLM candidates engage in a multi-round peer battle based on individual questions, aiming at revealing their true performance differences.Finally, a committee of LLM judges collaboratively discusses and decides the winner, reducing bias and enhancing fairness.During the peer battles, we observe intriguing scenarios where the LLM candidates display competitive behaviors and even learn from the opponents.In our extensive experiments involving 15 recent LLMs, Auto-Arena shows a 92.14% correlation with human preferences, surpassing all previous expert-annotated benchmarks without any manual efforts.As a result, Auto-Arena offers a promising alternative to current human evaluation platforms for evaluating LLMs automatically.</p>
<p>INTRODUCTION</p>
<p>Since ChatGPT and GPT-4 (OpenAI et al., 2024) gained popularity, Large Language Models (LLMs) have risen to the forefront of technological innovation, capturing broad industry and social interests (Wu et al., 2023b).This enthusiasm has spurred numerous organizations to release their own LLMs (Touvron et al., 2023;Team et al., 2024b).However, the rapid pace at which these models are released and updated poses a significant challenge for users attempting to understand their capabilities and monitor their evolution.Consequently, there has been a pressing demand for comprehensively evaluating LLMs recently (Chang et al., 2024a).</p>
<p>The most popular existing method is automatic evaluation with static datasets.Among these, static datasets with predefined metrics, such as GSM8k (Cobbe et al., 2021) and MMLU (Hendrycks et al., 2021a), are constructed with aspect-specific input-output pairs, such as human exam-type questions and their corresponding answers.Given the questions, the LLM-produced answers are compared to ground-truth answers using metrics such as accuracy.This approach could suffer from inflexibility, contamination, and high human annotation costs.Firstly, the closed-form ground-truth answers limit their utility in assessing models' performances on general or open-ended questions, which are the main use cases of LLMs.As the questions are static, they also risk contamination (Ravaut et al., 2024), where models may have been inadvertently exposed to elements of the test datasets during Table 1: Comparison between Auto-Arena and other benchmarks or evaluation methods.</p>
<p>Questions</p>
<p>Responses Judges Method Dynamic?Auto-generated?Multi-turn?Open-ended?Auto?Committee?
OpenLLM Leaderboard ✗ ✗ ✗ ✗ ✗ ✗ MMLU ✗ ✗ ✗ ✗ ✗ ✗ GPQA ✗ ✗ ✗ ✗ ✗ ✗ LC-AlpacaEval ✗ ✓ ✗ ✓ ✓ ✗ MT-Bench ✗ ✗ ✗ ✓ ✓ ✗ Arena-Hard ✓ ✗ ✓ ✓ ✓ ✗ Chatbot Arena ✓ ✗ ✓ ✓ ✗ ✗ Auto-Arena ✓ ✓ ✓ ✓ ✓ ✓
training, thereby skewing the evaluation results.The manual dataset construction also incurs high costs, creating barriers for extending to other domains or languages.As an alternative, static datasets with model-based evaluation, such as MT-Bench (Zheng et al., 2023) and AlpacaEval (Dubois et al., 2024a), evaluates LLMs on open-ended generations.These methods typically ask two models to generate responses to the same open-ended question and then employ a strong judge model (e.g., GPT-4) to choose the better response.However, the static question sets still bear contamination risks.Additionally, the assumption of the existence of a strong judge model makes the evaluation framework less generalizable and introduces model-specific bias.</p>
<p>Aside from automated evaluations, human assessment, although requiring significant manual efforts, remains the gold standard for users.A notable example is Chatbot Arena (Zheng et al., 2023), a crowdsourcing platform that gathers anonymous votes on LLM performances and calculates Elo scores (Elo &amp; Sloan, 1978) to rank these models.The resulting leaderboard 1 is widely considered as a trustworthy indicator of LLMs' general capabilities.However, a reliable model evaluation on this platform must be supported by a large number of human votes, which requires considerable time and effort.Consequently, when newly developed models enter the scene, they often struggle to quickly amass a large number of votes.Moreover, this strong reliance on human votes limits its application in various scenarios.For example, the performance of non-English languages is difficult to estimate, as most queries on the platform are in English.Moreover, the queries are mostly one-round and simple.</p>
<p>The completely open participation may also result in uneven evaluation quality.</p>
<p>To enable the evaluation of LLMs that is both automated and reliable while aligning with human preferences, we introduce Auto-Arena, a framework that automates the entire LLM evaluation process with LLM-powered agents.The framework consists of three stages: Firstly, an LLM examiner agent is tasked with generating questions, mimicking real-life users posting queries.Secondly, two LLM candidates interact with each other and engage in a multi-round peer battle by answering the seed question individually, criticizing the opponent's weaknesses, and raising targeted follow-up queries to challenge the opponent further.During the multi-round battle process, the LLM's true capabilities are drawn out and performance gaps become more visible.Lastly, a committee of LLM judges collectively discusses and evaluates the ability of the two candidates, mimicking the human voting process.As shown in Table 1, Auto-Arena has several key advantages compared to previous evaluation methods: First and foremost, instead of the simple and one-round question-answering scheme, Auto-Arena introduces a dynamic multi-round peer battle, which displays deeper abilities of LLMs, such as reasoning, interacting, and strategizing.The dynamic nature of peer battles also reduces contamination risks.Secondly, by expanding a single LLM judge into a committee of LLM judges, Auto-Arena alleviates potential model-specific evaluation bias.Finally, since the process of generating questions and judgments is fully automated in an end-to-end way, Auto-Arena can provide timely evaluations for new models and can easily extend to various domains and languages.</p>
<p>To verify the reliability and alignment of the evaluation framework, we run an extensive experiment with 15 LLMs.Compared to static and model-based benchmarks, Auto-Arena results in the state-of-the-art alignment by achieving a 92.14% Spearman correlation with human preferences, surpassing all previous benchmarks.Although no manual efforts is involved, the high alignment with human preferences could originate from the human-like evaluation process, which is simulated using LLM agents.The extensive ablation experiments also demonstrate the reliability of the framework: Before and after peer battles, the Spearman correlation with human preferences increases by 5%, verifying our hypothesis that the peer battles can better display performance gaps.Before and after committee discussions, committee agreement increases by 11%, showing human-level agreement and verifying the effectiveness of the committee discussion mechanism.By studying the peer battles, we also discover intriguing LLM agent behaviors such as competitive and self-improvement actions.As the entire process is automatic, the evaluation can be easily adapted to other languages or domains by altering the prompts.We provide Chinese as a case study for extending to other languages.</p>
<p>In conclusion, our contributions can be summarized as follows:</p>
<ol>
<li>We propose Auto-Arena, a fully automatic LLM evaluation framework where the examiner, candidates, and judges are all simulated with LLM-powered agents; 2. Specifically, we innovatively utilize peer battles for LLM evaluation, where two LLM agents engage in a multi-round debate.This process draws out the model's deeper capabilities; 3.In our extensive experiment with 15 LLMs, we observe the state-of-the-art alignment with human preferences without any manual efforts; 4.During peer battles, LLM agents display intriguing behaviors, such as strategizing and learning from the opponents, which opens up possibilities for future work.</li>
</ol>
<p>THE AUTO-ARENA FRAMEWORK</p>
<p>As illustrated in Figure 1, the Auto-Arena framework consists of three stages: Question Generation, Multi-round Peer Battles, and Committee Discussions.These three stages are run sequentially and fully simulated with LLM-powered agents.All prompts are included in Appendix A.</p>
<p>QUESTION GENERATION</p>
<p>For debate questions, as using a static dataset could incur data contamination concerns and result in unfair evaluations, we ask an LLM examiner agent to dynamically generate questions.The examiner agent could be any capable LLM.Similar to MT-Bench (Zheng et al., 2023), the generated questions cover 8 common categories in real-life conversations: writing, roleplay, extraction, reasoning, math, coding, STEM knowledge, and humanities/social science knowledge.The examiner is provided with a sample question and encouraged to generate diverse and difficult questions to ensure the depth and width of the evaluated debates.Examples of the generated questions are shown in Appendix B.</p>
<p>Specifically, as the examiner agent will also participate in the following debates, we try to alleviate self-enhancement bias with two designs: 1.We do not disclose to the examiner that it will participate in this tournament.2. Previous methods (Bai et al., 2024) could incur self-enhancement bias as they ask the examiner agents to only devise questions that they are confident about.In comparison, we do not ask the examiner to only generate questions that it can solve.To further show that limited self-enhancement bias is present, we include an ablation study in Appendix E.</p>
<p>PEER DEBATE</p>
<p>After question generation, we conduct peer battles around these questions among the LLM candidates.</p>
<p>In one peer battle, two LLM candidates (A and B) debate around the given question, point out the opponent's weaknesses, and devise follow-up questions to further probe the opponent's weaknesses.</p>
<p>In the peer battle, each candidate LLM has four available types of actions:</p>
<p>• <THINK>: The candidate generates internal thoughts about the question or plans a strategy.This action can be used at any time and remains concealed from the opponent.• <RESPOND>: The candidate answers the given question.</p>
<p>• <CRITICIZE>: The candidate identifies flaws and errors in opponent's previous responses.</p>
<p>• <RAISE>: The candidate poses follow-up questions to reveal the opponent's weaknesses.</p>
<p>The workflow of a peer battle takes the form of the Lincoln-Douglas debate format2 , the most widely used one-on-one debate style in competitions such as those held by the National Speech and Debate Association.The peer battle consists of three rounds in which two candidate models alternate speaking.Both candidates can see the complete dialogue history.This process is depicted in Figure 2. In the first round, model A RESPONDS to the examiner's initial question; model B CRITICIZES the flaws in A's response and RAISES a specific follow-up question; model A then RESPONDS to B's follow-up question.The second round follows the same format, with A and B switching roles.In the third round, A and B cross-examine each other, starting with A CRITICIZING the loopholes in B's earlier responses and RAISING follow-up questions.After responding, model B CRITICIZES A's weaknesses and RAISES additional questions.Model A wraps up by RESPONDING once more.Throughout this process, both A and B perform an equal number of actions to maintain fairness.To minimize positional bias, the order of A and B is randomized at the start of each debate.</p>
<p>During the debate process, enhancement bias and contamination concerns are further reduced: The process of candidates raising follow-up questions to each other essentially decentralizes the questiongeneration process, reducing enhancement bias in the generated initial questions.Moreover, debating ensures that candidates are evaluated not only on their response to the initial question, but also in more comprehensive and deeper abilities, such as strategizing, criticizing the opponent, and drafting questions.In other words, answering the initial question well does not necessarily win the whole debate, which further reduce contamination concerns.</p>
<p>Depending on which turn it is, we provide an action guide to the candidate, specifying the objectives and corresponding actions for this turn.Similar to human debate competitions, we time the candidates by imposing a maximum length constraint, which is also specified in the prompts.Any responses beyond the required length will be cut off.This design mitigates verbosity bias in LLM-as-ajudge (Zheng et al., 2023), where LLM judges prefer longer and more verbose responses.</p>
<p>After the peer battle takes place, a committee of LLM judges collectively determines the winner.The committee is always selected as the five best LLMs according to the current ranking.To reduce bias, we exclude the participants themselves and models from the same family as the participants from the committee.For example, GPT-4 will not serve as a judge in evaluating a debate participated by GPT-3.5.In the first round, the committee is initialized with MMLU (Hendrycks et al., 2021a) scores to approximate LLM performances.Each judge is individually asked to read the entire peer battle history, elaborate judgment reasons, and give a decision on whether A is better, or B is better, or if there is a tie based on factors such as helpfulness, relevance, and accuracy.</p>
<p>After the initial judgments are formed, the committee engages in a discussion.In a discussion round, each judge reads the other judge's verdicts in the previous rounds, elaborates its own thoughts for judgments, and drafts a discussed verdict.During the process, the judge may decide to adjust or maintain the previous judgments.Compared to the peer battles that exemplify multi-agent competitions, this committee discussion component synthesizes a multi-agent collaboration scheme.By enabling interactions among the judge agents and exchanges of different viewpoints, the discussion allows the committee to form a collective intelligence.As a result, it improves the judgment quality, boosts inter-judge agreement, and mitigates single-model bias.Finally, the winning candidate is decided by majority voting of the discussed judgments.</p>
<p>USING AU T O-AR E N A TO DERIVE TRUSTWORTHY RANKINGS</p>
<p>EXPERIMENTAL SETUP</p>
<p>Model Selection:</p>
<p>For the main experiment, we first select 9 best or latest models that are representative of each popular model family on the top 30 list on the Chatbot Arena platform with more than 10k votes each at the time of experiments: GPT-4-0409-Turbo, GPT-3.5-Turbo-0125,Claude-3-Haiku, Qwen1.5-72B-Chat,Command-R+, Llama-2-70B-Chat, Mixtral-8x7b-Instruct-v0.1, Yi-34B-Chat, and Deepseek-LLM-67B.To construct a leaderboard, we further add 6 models that are newly released: GPT-4o-2024-05-13, Claude-3.5-Sonnet,Qwen2-72B-Instruct, Llama-3-70B, Gemma-2-27B, and Gemini-1.5-Flash.Appendix G provides a detailed list of the selected models.</p>
<p>Baselines:</p>
<p>For the baselines, we consider popular evaluation benchmarks, including fixed metrics and model-based metrics.A comparison table is shown in Appendix H.</p>
<ol>
<li>Static datasets with fixed metrics: (1) OpenLLM Leaderboard (Beeching et al., 2023), a popular benchmark for open-source models averaging performance metrics on 6 key benchmarks, covering a large number of different evaluation tasks; (2) GPQA (Rein et al., 2023), a graduate-level google-proof Q&amp;A benchmark consisting of 448 domain-expert-written questions written in scientific subjects;</li>
</ol>
<p>(3) MMLU (Massive Multitask Language Understanding) (Hendrycks et al., 2021a), an extensive benchmark that covers 57 subjects and tests both world knowledge and problem-solving ability; 2. Static datasets with model-based metrics: (1) MT-Bench (Zheng et al., 2023), a set of 80 multi-turn questions.Model responses are graded by GPT-4; (2) Arena Hard (Li* et al., 2024), a benchmark dataset with 1,000 challenging user queries collected on Chatbot Arena.Model responses are graded by GPT-4-Turbo; (3) Length-Controlled AlpacaEval (Dubois et al., 2024a), a benchmark based on AlpacaFarm evaluation set (Dubois et al., 2024b), which tests models' abilities to follow general user instructions.Models are evaluated by their win rates against GPT-4-Turbo, graded by GPT-4-Turbo.</p>
<p>Setup:</p>
<p>Among the 9 participants, we conduct a swiss-style tournament: For n participants, instead of pairing each participant with (n − 1) others, a swiss-tournament pairs each player with ⌈log 2 (n)⌉ players of similar rankings without repeats.This design effectively reduces computational costs of ranking n models from O(n 2 ) to O(nlog 2 (n)).A cost analysis is included in Appendix H.</p>
<p>Each candidate pair engages in 40 peer battles, with 5 questions from each of the 8 task categories that are specified in Section 2.1.We provide studies showing that the generated questions can reduce contamination concerns in Appendix C and are generalizable to real-world scenarios in Appendix D. As each battle consists of 3 rounds (each candidate speaks for 4 times), the competition scale is approximately the same as MT-Bench (80 questions, each candidate speaks twice).In the tournament, the rating scores are calculated with the Elo rating system (Bai et al., 2022;Boubdir et al., 2023), which has become the standard practice in competitive games such as chess (Elo &amp; Sloan, 1978).Similar to the Chatbot Arena score calculation procedure (Chiang et al., 2024), we compute the Bradley-Terry (BT) coefficients (Bradley &amp; Terry, 1952) for better statistical estimation.Following the Reference-Guided judge in Zheng et al. (2023), we ask the best-performing judge to give a reference answer for evaluating logical-reasoning questions (math, coding, reasoning).</p>
<p>We initialize the Swiss tournament rankings according to MMLU scores, which is a static approximation of model performances.At the end of each pairing, we re-calculate Elo scores of current models.The committee is selected as the best 5 LLMs based on current Elo rankings at each round.After forming initial judgments, the committee members engage in one round of discussion.The final result is decided by majority voting of the discussed judgments.(Beeching et al., 2023) -15.39%GPQA (Rein et al., 2023) 36.84%MMLU (Hendrycks et al., 2021b) 56.36% LC-AlpacaEval (Dubois et al., 2024a) 82.14% MT-Bench (Zheng et al., 2023) 82.86% Arena-Hard (Li* et al., 2024) 85.71%</p>
<p>RESULTS: ALIGNMENT WITH HUMAN PREFERENCES</p>
<p>ABLATION STUDIES ON PEER BATTLES AND COMMITTEE DISCUSSIONS</p>
<p>Peer-battles:</p>
<p>We conduct an ablation study on whether peer-battles affect the evaluation quality and include the results in Table 2 ("w/o Peer Battles").In this setup, we ask the committee to only evaluate the two candidates' initial responses to the synthetic question, where the judge prompts stay the same.For this no-debate design, the question-answering process mimics that of MT-Bench or LC-AlpacaEval, but with an added committee discussion component.As a result, we observe that the correlation is slightly higher than LC-AlpacaEval and MT-Bench by a margin of 3.81%.Compared to the full Auto-Arena framework, however, the performance drops by 5.00%.This proves the effectiveness of the peer battles, during which the performance gaps between candidates become more visible and robust to judges.Thus, peer battles can improve alignment with human preferences.Committee Discussions:</p>
<p>The committee discussion component is designed to introduce various points of view and produce more consistent decisions.As shown in Table 2, the correlation with human preferences drops from 91.67% to 88.33% without committee discussions, showing the effectiveness of the component in improving evaluation quality.As shown in Figure 3, before committee discussions, the Cohen's Kappa agreement (McHugh, 2012) between individual judges and the final result (voted) is low, averaging 0.41.Specifically, compared to strong models, the judgments of weak models align less with the voted result, such as Yi compared to GPT-4.This shows that general model capabilities could result in significant performance gaps when used as judges.After the committee discussions, agreement increased to an average of 0.54, which indicates moderate agreement.In the discussion process, judges are exposed to more viewpoints, among which some may be convincing enough to result in a change in verdict.More analysis on the inter-judge agreement is provided in Appendix F, where we see that discussions could largely improve the agreements among individual judges as well.Table 3 shows the agreement probability among judges.Agreement probability is defined as the mean probability of two random judges agreeing with each other.After committee discussion, the agreement increases by 11%, matching the agreement level among human annotators on MT-Bench.This observation indicates that committee discussions can significantly improve the quality of judgments to match with human-level performance.</p>
<p>CONSTRUCTING AND MAINTAINING A LEADERBOARD WITH AU T O-AR E N A</p>
<p>UPDATE NEW MODELS TO LEADERBOARD</p>
<p>With Auto-Arena, we can obtain the rank for a list of models with their Elo scores to construct a leaderboard.As new LLMs are released frequently, we describe how to add new candidate models to the existing leaderboard with 6 more models which are released very recently, as previously listed in Section 3.1.To add a new candidate, we ask it to debate with ⌈log 2 (n)⌉ opponents with similar Elo scores, where n is the number of total participants after adding the new candidate.For the first pairing, as we do not have Elo indicators, we initialize by asking the new candidate to debate with the opponent with the most similar MMLU score.This addition mechanism is generalizable and maintains the computational costs of evaluating n models below nlog 2 (n).Auto-Arena 92.14%</p>
<p>As an example, we add a new participant (Llama-3-70B) to the existing 9-model ranking.It battles with ⌈log 2 (10)⌉ = 4 close opponents and Figure 4 shows how the Elo score changes throughout the rounds.Firstly, it is paired with Qwen-1.5 based on MMLU similarity and wins, which results in a very high Elo score, even above GPT-4.Then, it is paired with GPT-4, the closest opponent in Elo score.After losing, it is paired with the other opponents who are close in Elo scores, Command-R+ and Claude-3-Haiku.Eventually, the score stabilizes at second place.This process lets the new candidate battle with a reasonable fraction of close opponents and makes the final ranking stable without disrupting the other participants, whose score distribution remains similar before and after the addition.</p>
<p>Using this scalable addition approach, we build a comprehensive leaderboard by adding 6 new models to the existing tournament of 9 LLMs, resulting in a final ranking of 15 models.Figure 5 shows the overall Elo scores by Auto-Arena on the 15 models.Table 4 shows the Spearman correlations after expansion.Auto-Arena remains the method most aligned with human preferences by a margin of 3.41%, showing the state-of-the-art alignment of 92.14%.Therefore, Auto-Arena is generalizable and robust for maintaining a leaderboard for many LLMs.As Auto-Arena of LLMs is fully automatic, it can be easily adapted to evaluate LLMs in other domains or languages.As case studies, we conduct a tournament in Chinese on models that are claimed to have multi-lingual proficiency.The only adaption effort is translating the prompts into the desired languages.Then, the generated questions and peer battles will be in the desired languages.It is also possible to adapt the framework to another task or domain, the only effort is to change the "domain" specification in the examiner's prompts (shown in Appendix A).</p>
<p>Figure 6 shows the Elo scores derived by Auto-Arena for the Chinese tournament on 11 models.As Chinese evaluation benchmarks are limited, we compare with the Chinese-only leaderboard on Chatbot Arena, which constitutes 10.36% of all collected votes.We include 7 models best-performing and newest models from each major model family in the top 20 list on Chatbot Arena.The Auto-Arena recovers their Elo scores with a correlation of 92.86%, verifying the reliability of the extension.In addition, as Chatbot Arena doesn't include proprietary Chinese LLMs, we add 4 popular Chinese LLMs, which are GLM3 , SenseChat4 , Minimax5 , and Wenxin6 .We notice that the models claimed to have Chinese proficiency, such as Qwen-1.5, indeed score higher on this leaderboard compared to the English one.</p>
<p>INVESTIGATION OF LLM'S BEHAVIORS IN COMPETITIVE PEER BATTLES</p>
<p>Beyond quantitative analysis, we take a deeper look into the peer battles and find several interesting behaviors of LLM agents in competitive environments.</p>
<p>Peer Battles Make the Performance Gaps Become Visible In the example shown in Figure 7, given a math question on infinite series, both candidate A (Claude-3-Haiku) and candidate B (GPT-4-Turbo) provide correct answers in the first round.However, as the debate deepens, the performance gap becomes more visible: Candidate B is able to provide a more elaborate and helpful response when explaining the theories behind the initial answer.In the ablation study without peer battles, the judges initially decided that it was a tie.However, after seeing the subsequent debates, they change to favoring assistant B. This example shows that the debate process indeed pushes the candidate LLM's capabilities to the limit, testing deeper understandings and reasoning abilities.Moreover, as shown in the previous Table 2, the peer battles are indispensable for a robust and comprehensive evaluation.LLMs Can Skillfully Attack the Opponents The example in Figure 8 shows excerpts of a peer battle around the question: "how many unique ways to arrange letters in 'LETTER'."Candidate A (powered by Yi-34B-Chat) gives a wrong initial answer as it miscounts occurrences for repeated letters and miscalculates factorials.The opponent B (powered by Claude-3-Haiku) quickly and precisely points out these two issues and skillfully raised a follow-up that targets A's weaknesses: "how about the word 'BANANA'?"Then, A still miscalculates factorials.We see that LLM candidates efficiently understand the rules of the competitive environment and can design targeted strategies to attack the opponent in order to win.In the peer battles, the debater agents display effective competition strategies, further probing the opponent's weaknesses.</p>
<p>LLM Candidates Can Improve by Learning from its Opponents Figure 9 shows a roleplay example between Claude-3-Haiku (A) and Command R+ (B).In the first round, A answers the question plainly while B, in addition to answering the question, also employs the appropriate speech style, which better matches the "roleplay" instructions.Then, in the rounds after, without any explicit instructions, A learns from its opponent and also incorporates the speech style.This case shows an interesting observation that, even in competitive environments, LLM candidates can display learning behaviors and improve from the interactions.Expanding upon this observation, using the interplay between LLM agents to improve performances could be a promising future paradigm of learning.</p>
<p>RELATED WORK</p>
<p>As LLMs evolve quickly, deriving trustworthy evaluations of their capabilities has become a challenge.Current evaluation methods can be divided into automatic evaluations and manual evaluations, such as Chatbot Arena (Chiang et al., 2024).We primarily focus on automatic evaluations as they deliver more timely feedback.Automatic evaluations mainly consist of static datasets with predefined metrics and model-based metrics.Static datasets with predefined metrics, such as MMLU (Hendrycks et al., 2021a), GPQA (Rein et al., 2023), and Open-LLM-Leaderboard (Beeching et al., 2023) consist of expert-annotated question-answer pairs.Then, the models are evaluated based on performance metrics such as accuracy.However, as they only evaluate closed-form answers, they are inflexible in evaluating open-ended responses.Moreover, the static datasets may eventually become exposed to the internet and could lead to contamination concerns (Ravaut et al., 2024).</p>
<p>On the contrary, static datasets with model-based metrics offer a flexible, low-cost and fast evaluation paradigm (Chang et al., 2024b).Studies have verified that LLMs can provide unbiased (Ning et al., 2024;Chu et al., 2024), high-quality (Lin &amp; Chen, 2023) metrics comparable to human evaluations (Dubois et al., 2024a;Zheng et al., 2023).Among them, MT-Bench (Zheng et al., 2023) and AlpacaEval (Dubois et al., 2024a) use LLM-as-a-judge to ask GPT-4 to compare model responses to a static dataset of questions.The model's judgments achieve over 80% agreement with human preferences, proving the usability of using LLMs to evaluate response quality.Language-Model-asan-Examiner (Bai et al., 2024) asks an LM examiner to construct knowledge-intensive questions within its memory, interact with the candidate in a series of follow-up queries, and rate the responses on dimensions including accuracy and factuality.KIEval (Yu et al., 2024) also incorporates an LLM-powered "interactor" role to examine deep comprehension of knowledge, which is shown to mitigate contamination issues on static datasets.However, such single-judge evaluations require the examiner to interact with each candidate parallelly, creating computational overheads and limiting the scope of queries.They also suffer from single-model bias, including bias towards LLM-generated summaries (Liu et al., 2023), inflated scores in multilingual evaluation (Hada et al., 2023), verbosity bias (Dubois et al., 2024a), and difficulties when evaluating candidates with close performance (Shen et al., 2023).Therefore, there have been studies on employing multi-agent evaluation to mitigate single-model bias.For example, DRPE (Wu et al., 2023a) uses multi-roleplayer prompting to mimic different roles with the same LLM and integrate outputs as votes for the final results.PRD (Li et al., 2023a) allows two LLMs to discuss an evaluation and assigns higher voting weights to the LLM reviewers with stronger capabilities.They show that the multi-agent approach effectively mitigates single-model bias.This line of work is similar to our "LLM judge committee" component.However, they are still limited to static datasets and specific domains.</p>
<p>Outside the domain of LLM evaluations, some works study competitive behaviors in multi-agent LLM systems, which is relevant to the peer battles in Auto-Arena.LM vs LM (Cohen et al., 2023) shows that LLM cross-examinations can effectively discover factual errors.Debate (Du et al., 2023) shows that multi-agent debate can improve factuality and reasoning.In MAD (Liang et al., 2023), LLM-debate can encourage divergent thinking, which helps tasks that require deep levels of contemplation.Khan et al. (2024) shows that even non-expert weak LLMs can supervise expert LLMs if we allow the two LLM experts to engage in debates.Moreover, Zhao et al. (2023) and Gu et al. (2024) show interesting case studies where LLMs are engaged in simulated competitive environments and demonstrate human-like strategies.</p>
<p>CONCLUSIONS</p>
<p>In this paper, we innovatively design a completely automatic evaluation framework: Auto-Arena.By using LLM agents to generate questions, employing LLM candidates in peer battles, and evaluating responses using LLM committee discussions, Auto-Arena delivers timely and trustworthy evaluations and automates the evaluation process in an end-to-end way.In the extensive experi-ments, Auto-Arena achieves the highest correlation with human preferences, despite requiring zero human efforts.It is easily adaptable to other domains and resources, promoting the inclusiveness of AI system evaluations.The peer battles also demonstrate several interesting LLM behaviors in competitive environments, including attacking and learning from the opponents.</p>
<p>A PROMPTS USED</p>
<p>In this section, we list all prompts used, including prompts for question generation, peer battles, and examiners.</p>
<p>A.1 PROMPTS TO EXAMINER AGENT This is the prompt to the examiner agent for question generation.The domains and their respective commands are listed in 5</p>
<p>You have been assigned the task of drafting a set of [NUMBER] different user queries to a chat assistant on [DOMAIN].Please strictly follow these 6 rules for the question: 1.The question is likely for a user to ask in real life.Follow the format of the example query.A.2 PROMPTS TO PEER BATTLE CANDIDATES This is the first prompt for the peer battle candidates.When possible, it is included as a system prompt.The action guide prompts are included in Table 6, where the actions are determined by the round and turn as illustrated in Figure 2.</p>
<p>You are a helpful assistant that provides accurate answers to user requests.As an experienced assistant, you follow the user's requests and provide reliable responses as much as you can.You outline your reasons for the response to make it easy for the users to understand.While maintaining the important details in the responses, you aim to output concise and straight-to-the-point answers without being overly verbose.This is a competitive chatbot arena.You are competing against another chatbot assistant in a debate and being judged by a committee on factors such as helpfulness, relevance, accuracy, depth, and creativity.After answering the initial user input, you will engage in a multi-round debate with your opponent.Below are your actions:</p>
<p><think>: Think step-by-step to analyze the question or plan your strategy in the debate.This is hidden from the opponent.Only think when necessary and make it concise.</p>
<p><respond>: Answer to the user input as accurately as you can.</p>
<p><criticize>: Criticize the weaknesses of your opponent's response.</p>
<p><raise>: Target your opponent's weaknesses.Give a potential follow-up user input that the opponent could fail to respond.The input can be answered concisely and focus on variations or motivations of its previous response.Generate one input only.Be reasonable.Avoid becoming too specific or repetitive.DO NOT raise a follow-up if you DON'T SEE the opponent's response!Follow the action guide strictly.</p>
<p>[ACTION_GUIDE_PROMPT] The cinematography, the acting, the plot -everything was top-notch.Never before have I been so disappointed with a movie.The plot was predictable and the characters were one-dimensional.In my opinion, this movie is the worst one to have been released in 2022.The movie was okay.There were some parts I enjoyed, but there were also parts that felt lackluster.This is a movie that was released in Feb 2018 and seems to be quite ordinary.Return the answer as a JSON array of integers.Develop a Python program that reads all the text files under a directory and returns top-5 words with the most number of occurrences.</p>
<p>STEM knowledge</p>
<p>It should be a specific question designed to test the LLMś STEM knowledge.</p>
<p>In the field of quantum physics, what is superposition, and how does it relate to the phenomenon of quantum entanglement?</p>
<p>humanities/social science knowledge</p>
<p>It should be a specific question designed to test the LLMś humanities/social science knowledge.</p>
<p>Provide insights into the correlation between economic indicators such as GDP, inflation, and unemployment rates.Explain how fiscal and monetary policies affect those indicators.After the agent responds, the opponent's responses are fed in using this prompt:
[ACTION_GUIDE_PROMPT] Opponent's Response: [OPPONENT_RESPONSE]
For word limits, the <respond> action is given 300 words.The <criticize> and <raise> actions are given 300 words in total.Including all 3 actions will have twice as many words.For writingtype questions that require a longer response (writing, roleplay, coding, humanities/social science knowledge), the 300 word limit is increased to 400.Overall, both candidate A and B has the same amount of words for generation and the same amount of actions to ensure fairness.As LLMs have different tokenizers, we standardize all lengths by using the tiktoken package.Each word is approximated as 4/3 tokens.The word limits are chosen after a carefully conducted length study.</p>
<p>A.3 PROMPTS TO JUDGES</p>
<p>This is the prompts to judge agents to derive the initial evaluations and verdicts:</p>
<p>This is a chatbot arena.Two AI assistants had a multi-round debate on who is more helpful.Please act as an impartial judge and evaluate the capability of two AI assistants.You should choose the assistant that follows instructions and answers questions better.Your evaluation should consider factors such as helpfulness, relevance, and accuracy.Begin your evaluation by comparing the responses of the two assistants and provide a short explanation.Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.DO NOT allow the LENGTH of the responses to influence your evaluation, choose the one that is straight-to-the-point instead of unnecessarily verbose.When the two candidates perform equally well, choose the SHORTER answer.Do not favor certain names of the assistants.Be as objective as possible.After providing your explanation concisely within 200 words, output your final verdict by strictly following this format: "
[[A]]" if assistant A is better, "[[B]
]" if assistant B is better, and " [[Tie]]" for a tie.Finish your judgement within 300 words.This is the prompt for judges for discussion:</p>
<p>Below are the responses from other judges in the committee.</p>
<p>Please read them and decide whether you want to adjust your rating or maintain your original judgement.After providing your explanation, output your final verdict by strictly following this format: " is better, and "[[Tie]]" for a tie.Finish your judgement within 300 words.
[[A]]" if assistant A is better, "[[B]]" if assistant B</p>
<p>B EXAMPLE QUESTIONS GENERATED</p>
<p>To show the overall quality of the questions generated, we list 2 generated questions per category here.The questions shown are not manually-selected, but simply the first 2 questions generated.The quality is consistent throughout.We manually examine the questions with closed-form answers (math, reasoning, coding) and find that all questions used are solvable.</p>
<p>Writing:</p>
<p>1.</p>
<p>Craft a detailed marketing strategy for a startup focusing on sustainable fashion, including social media campaigns and influencer partnerships.</p>
<p>2.</p>
<p>Write a comprehensive guide on the psychological effects of social media on teenagers, incorporating recent studies and expert opinions.</p>
<p>Roleplay:</p>
<ol>
<li>
<p>Assume the role of a 19th-century British detective.How would you go about solving a mysterious disappearance in London using the technology and methods of your time?</p>
</li>
<li>
<p>Pretend you are a Michelin-starred chef.Describe in detail how you would prepare a signature dish that embodies the essence of modern French cuisine.</p>
</li>
</ol>
<p>Extraction:</p>
<ol>
<li>What are the three most significant historical events mentioned and their dates?</li>
</ol>
<p>Context:</p>
<p>The article discusses several key moments in history, including the signing of the Magna Carta in 1215, which laid the groundwork for modern democracy.It also mentions the fall of the Berlin Wall in 1989 as a pivotal moment in the end of the Cold War.Another significant event highlighted is the moon landing on July 20, 1969, demonstrating major advancements in space exploration.</p>
<ol>
<li>Identify the main therapeutic benefits and the active ingredient mentioned for each herbal remedy.</li>
</ol>
<p>Context:</p>
<p>The text provides an overview of various herbal remedies used for centuries.It mentions that Chamomile contains Bisabolol, which has anti-inflammatory and calming properties.Gingko Biloba, known for its flavonoids and terpenoids, enhances cognitive function and blood circulation.Lastly, Echinacea is recognized for its alkamides, which bolster the immune system.</p>
<p>Reasoning:</p>
<p>1.If a cube's volume is tripled, by what factor does the length of one of its sides increase?</p>
<ol>
<li>
<p>In a two-legged soccer match, Team A wins the first leg at home 3-0, but loses the second leg away 2-5.Who advances to the next round, considering the away goals rule?math:</p>
</li>
<li>
<p>How do you solve the differential equation dy/dx + 2y = e (−2x) given that y(0) = 1? 2. The sentence embedding similarity metric as in Platypus (Lee et al., 2024), where a question is deemed contaminated if it has a cosine similarity (using Sentence Transformer (Reimers &amp; Gurevych, 2019) embeddings) greater than 80% against any training item.This detection method is more robust to rephrases, which ensures that we can detect cases where the LLMs are simply rephrasing existing questions on the web.</p>
</li>
</ol>
<p>Although we do not have access to the training data, LLMs mostly use public web data for pretraining (Raffel et al., 2020;Brown et al., 2020;Touvron et al., 2023).Therefore, we approximate it with the Bing search API: If verbatim test examples appear online, it likely indicates inclusion or exposure to the training data.This procedure is also followed by Li et al. (2024) for detecting contamination.</p>
<p>The ablation is conducted as follows: Firstly, we randomly sample 100 questions from the testset.</p>
<p>As baselines, we use 3 popular evaluation benchmarks: MMLU (Hendrycks et al., 2021a), ARC Challenge (Clark et al., 2018), and HellaSwag (Zellers et al., 2019).For each question, we get the top 10 search result snippets on the Bing search API.If the question is deemed as contaminated by the detection method (mentioned above) against any of the 10 snippets, it is marked as contaminated.</p>
<p>The percentages of contaminated test instances is reported in Table 7.We can observe that Auto-Arena, by generating fresh questions, does alleviate the contamination issue.Compared to static datasets, Auto-Arena's contamination percentage (2%) according to the exact match is significantly lower.When using the sentence similarity metric, we can effectively detect whether generated questions are just rephrases of existing questions.The percentage is largely reduced by 7% to 15% compared to other benchmarks.</p>
<p>D SYNTHETIC V.S. REAL-LIFE QUESTIONS</p>
<p>In this section, we try to show the generalizability of the synthetic questions in Auto-Arena to real-life questions.</p>
<p>Design:</p>
<p>The generated questions resemble real-world queries by design.In the question generation prompt, we specifically ask the examiner to draft questions that are "likely for a user to ask in real life".From Appendix B, we could also observe the similarity of the synthetic questions to real-life queries.Human Study: To show that the generated queries are similar to real-life ones, we conduct the following human study.We compare 30 synthetic questions by Auto-Arena and 30 real-life questions.A human user is asked to look at a question randomly drawn and decide whether he/she believes that it is AI-generated, Real-Life, or if he/she cannot tell.The questions are collected in the Math category, where the 30 real-life ones are taken from MT-Bench (10 questions, drafted by experts), AMC-8 (4 problems, from the 2024 math competition), and AGI-Eval (16 math questions collected from college entrance exams).Two volunteers who are frequent users of LLMs and are familiar with AIGC participated.We report their respective results and agreement in Table 8.We can observe that humans cannot tell if the problems are synthetic almost half of the time.The user accuracy (correct percentages) is also low.We calculate the Cohen's Kappa agreement between the two users, which is -0.11.The agreement score shows that there is less agreement than random chance.The big divergence between human annotators' responses also shows subjectivity and uncertainty in the judgments.Therefore, we conclude that humans most likely cannot tell whether questions are synthetic or real-world, indicating small differences.Ablation Study: To validate the results' generalizability with real-world datasets, we conduct an ablation study comparing Auto-Arena's evaluation performances on real-life questions and synthetic questions.Specifically, we asked 2 candidates (GPT-4-Turbo-0409 and Claude-3-Haiku) to debate around 30 synthetic math questions and 30 real-world math questions (collected as in the human study shown in Table 8).If the results are generalizable, we would observe that the win rates of each model should be similar.The results are shown in Table 9.From the results, we can observe that the win rates of each model only differ by 4% on synthetic and real datasets, which shows consistent evaluation performances, validating the use of synthetic problems.</p>
<p>Aside from the supporting studies, the use of synthetic questions for evaluation has also been established as common practice.The Mathematics dataset (Hendrycks et al., 2021b) already uses synthetically generated math questions, where they note many advantages, such as the ease of providing a larger number of examples, the precise controls over difficulty levels, and the ease of testing generalization (since one can precisely vary different axes of difficulty in different question types).LMExamQA (Bai et al., 2024)   We attempt to reduce self-enhancement bias of the question generation stage with explicit designs: Firstly, during question generation, we do not disclose to the examiner that it will participate in this tournament and we do not ask the examiner to generate only questions that can be solved by itself.Secondly, the peer-debate process further reduces bias in initial question generation: Debating ensures that candidates are evaluated not only on their response to the initial question, but also in more comprehensive and deeper abilities, such as strategizing, criticizing the opponent, and drafting questions.In other words, answering the initial question well does not necessarily win a whole debate.</p>
<p>In the debate design in Figure 2, candidates also have a "raise" action, where they ask questions to the opponent.This process essentially decentralizes the question-generation process.</p>
<p>To systematically examine whether self-enhancement bias is present.We conduct an ablation study:</p>
<p>We examine enhancement bias with 2 models as an example: GPT-4 (GPT-4-turbo) and Haiku (Claude-3-Haiku).Firstly, we ask GPT-4 and Haiku to generate 30 math questions separately.Then, we conduct peer debates between the two candidates (GPT-4 and Haiku) on both sets of questions and evaluate results with the best-5-LLM committee as in the main experiments.</p>
<p>We evaluate the performance differences from the evaluation results: If self-enhancement bias is low, the ranking achieved should remain the same.In other words, the weaker model will always lose, even on the questions generated by itself.</p>
<p>The ablation results are shown in Table 10.From the results, we can observe that, in both sets of generated questions, the GPT-4 win rate remains significantly higher than the Claude-3-Haiku  win rate.Even if some limited extent of self-enhancement bias is present, the result difference is significant enough to reach the correct ranking.</p>
<p>F INTER-JUDGE AGREEMENT</p>
<p>As shown in Figure 10, the Cohen's Kappa agreement (McHugh, 2012) among judges before committee discussion is very low, averaging 0.16, which indicates slight agreement.We notice that weak model judges and strong model judges has an especially low agreement, such as GPT-4 and Yi.This shows that general model capabilities could result in significant performance gaps when used as judges.</p>
<p>After the 1 round of communication, agreements significantly improved as the judges become convinced by more persuasive arguments.The average Cohen's Kappa after discussion reaches 0.27, which indicates fair agreement.(Dubois et al., 2024a) Yes Static $10 Single LLM (GPT-4) MT-Bench (Zheng et al., 2023) Yes Static $10 Single LLM (GPT-4) Arena Hard (Li* et al., 2024) Yes Frequent Updates $25 Single LLM (GPT-4) Chatbot Arena (Zheng et al., 2023) Yes Live Very High Humans Auto-Arena No Freshly Generated $5 Committee of LLMs we only select the strongest or newest model from each model family.Besides the models on Chatbot Arena, we include 4 under-evaluated famous Chinese models to investigate their performances.</p>
<p>G MODEL SELECTION FOR THE MAIN EXPERIMENT</p>
<p>H COMPARISON OF BASELINE METHODS AND AUTO-ARENA</p>
<p>Table 12 shows a comparison between benchmark evaluation methods and Auto-Arena.Compared to previous methods, the main advantage of Auto-Arena is the zero need for human dataset construction or intervention and the freshness of queries.Another innovation compared to previous model-based systematic benchmarking procedures is using a committee of LLMs to discuss and vote for a final winner, which introduces diverse viewpoints.The most important innovation of Auto-Arena is the peer-battle mechanism, which asks LLM agents to compete and debate with each other.The resulting evaluation on the multi-turn debate then becomes more in-depth, interactive, and comprehensive.</p>
<p>For the evaluation cost, the costs of Auto-Arena are on the same scale as other benchmarks: We note that the primary experiment among 9 models costs around $45 USD.Therefore, the estimated cost is $5 per model.As models on the ranking board increase, the costs of conducting debates should grow slowly in log scale, which comes from conducting nlog 2 (n) pairings when adding 1 model to a ranking of (n − 1) models.The evaluation costs, however, shall remain the same as we use a committee of 5 LLMs at all times.</p>
<p>Figure 1 :
1
Figure 1: An illustration of Auto-Arena.</p>
<p>Figure 2 :
2
Figure 2: The process of a Lincoln-Douglas-style peer battle with the actions used.The <THINK> action can be used by the candidates freely and is only visible to the candidate itself.</p>
<p>Figure 3 :
3
Figure 3: Cohen's Kappa agreement with majority vote results before (upper) and after (lower) committee discussions.</p>
<p>Figure 4 :
4
Figure 4: Changes in Elo scores of adding Llama-3 to the ranking of 9 models.</p>
<p>Figure 5 :
5
Figure 5: Elo scores of 15 models by Auto-Arena on English.</p>
<ol>
<li>2
2
Figure 6: Elo Scores of 11 Models by Auto-Arena on Chinese.</li>
</ol>
<p>Figure 7 :
7
Figure 7: Performance gaps between candidates become visible in peer battles.</p>
<p>Figure 8 :
8
Figure 8: LLM agents display competitive behaviors in peer battles.</p>
<p>Figure 9 :
9
Figure 9: LLM agents learn from each other in peer battles.</p>
<p>reasoning</p>
<p>It should be a specific questiondesigned  to test the LLMś reasoning skills.Imagine you are participating in a race with a group of people.If you have just overtaken the second person, what's your current position?Where is the person you just overtook?math It should be a specific question designed to test the LLMś math skills.The vertices of a triangle are at points (0, 0), (-1, 1), and (3, 3).What is the area of the triangle?coding It should be a specific question designed to test the LLMś coding skills.</p>
<p>Figure 10 :
10
Figure 10: Cohen's Kappa Agreement with Majority Vote Before Committee Discussions.</p>
<p>Figure 11 :
11
Figure 11: Cohen's Kappa Agreement with Majority Vote After 1 Round of Committee Discussion.</p>
<p>Table 2 :
2
Correlations with Chatbot Arena Elos of evaluation benchmarks on 9 LLMs.
SpearmanCorrelation</p>
<p>Table 3 :
3
Agreement probability among judges.Agreement is defined as the mean probability of two random judges agreeing with each other.</p>
<p>AgreementAuto-Arena (Before discussion) 53% Auto-Arena (After discussion) 64% MT-Bench Human Evaluation 67%</p>
<p>Table 4 :
4
Correlation analysis with ChatbotArena of evaluation benchmarks on 15 LLMs after extension.
Spearman CorrelationOpenLLM32.50%GPQA62.86%MMLU46.20%LC-AlpacaEval76.32%MT-Bench88.73%Arena-Hard45.36%</p>
<p>[DOMAIN_COMMAND]2.It can be answered by the chatbot itself without additional inputs.3.You need to generate the queries as DIVERSIFED as possible.4. DO NOT add other words other than the query itself.5.The question should be complicated and difficult, requiring in-depth understanding and analysis of the subject.Each question in one line, add the serial number in parenthesis (e.</p>
<p>g., "(1).","(2).")before each question.Example query:[DOMAIN_EXAMPLE]</p>
<p>Table 5 :
5
Prompt components for the LLM Examiner agent.
DOMAINDOMAIN_COMMANDDOMAIN_EXAMPLECompose an engaging travel blogwritingIt should be a user query that tasks the LLM to write something.post about a recent trip to Hawaii, highlighting cultural experiencesand must-see attractions.roleplayIt should propose a scenario where the chatbot mimics a specific role/person. Give all necessary in-structions and requests for its re-sponse. Then, send a beginning re-quest to complete.Pretend yourself to be Elon Musk in all the following conversations. Speak like Elon Musk as much as possible. Why do we need to go to Mars?Question: Evaluate the followingmovie reviews on a scale of 1 to 5,with 1 being very negative, 3 beingneutral, and 5 being very positive:Context: This movie released onNov. 18, 2019, was phenomenal.It should consist of two parts: ques-tion and context. The questionshould test the chatbotś ability toextractioncorrectly understand and extract in-formation from the given context.Draft and provide a new contextyourself.</p>
<p>Table 6 :
6
Action Guides for the Debater Agents.
actionsaction guideAction guide: only include <respond>. Use <think> if needed. Finish<respond>your whole response within 300 words, including <think>. ENCLOSEEACH ACTION IN ITS RESPECTIVE TAGS!Action guide: include both <criticize> and <raise>. Use <think> if<criticize>, <raise>needed. Finish your whole response within 300 words, including<think>. ENCLOSE EACH ACTION IN ITS RESPECTIVE TAGS!Action guide: include all of <respond>, <criticize>, and <raise>. Use<respond>, <criti-\<think> if needed. Finish your whole response within 600 words,cize>, <raise>including <think>. ENCLOSE EACH ACTION IN ITS RESPECTIVETAGS!Initial user input: [QUESTION]</p>
<p>Table 8 :
8
Human Evaluation on Synthetic Questions and Real Questions.
Volunteer 1 Volunteer 2Correct27.1%38.9%Incorrect27.1%11.9%Cannot Tell45.8%49.2%Agreement-0.11</p>
<p>Table 9 :
9
Ablation Results on Synthetic Questions and Real Questions.
QuestionsGPT-4 Win Rate Claude-3 Win RateSynthetic Questions80.00%20.00%Real-life Questions75.86%24.14%</p>
<p>(Yu et al., 2024)to generate questions in different domains.KI-Eval(Yu et al., 2024)asks an LM-powered interactor to generate questions.The list goes on.Using synthetic questions has become the common norm in NLP evaluation.Moreover, extensive experiments in Auto-Arena show high correlations with human results, which also demonstrates the alignment with real-world usage.</p>
<p>E ABLATION STUDY ON SELF-ENHANCEMENT BIAS OF THE QUESTION GENERATION STAGE</p>
<p>Table 10 :
10
Ablation Results on Self-Enhancement Bias for Question Generator.
QuestionsGPT-4 win rate Haiku win rateGPT-4 Generated Questions80.00%20.00%Haiku Generated Questions76.92%23.08%</p>
<p>Table 11 :
11
Model Selection for the Main Experiment."Newest" and "Strongest" refer to the state at the time of experiments (2024 April).Bolded models are selected for the primary experiment with 7 models.Unbolded models are the ones added during extension.</p>
<p>Table 12 :
12
Comparison between Auto-Arena and Other Benchmarks.
MethodManual Construction FreshnessEval. Cost Judgeof Queriesper ModelOpenLLM Leaderboard (Beeching et al., 2023) YesStatic-Answer AccuracyMMLU (Hendrycks et al., 2021a)YesStatic-Answer AccuracyGPQA (Rein et al., 2023)YesStatic-Answer AccuracyLC-AlpacaEval
https://en.wikipedia.org/wiki/Lincoln-Douglas_debate_format. To help users better understand this debate format, we show the debate samples at https://auto-chatbot-arena. streamlit.app/.
https://open.bigmodel.cn/
https://platform.sensenova.cn/home
https://platform.minimaxi.com/examination-center/ text-experience-center
https://cloud.baidu.com/wenxin.html
2. What is the integral of (x 2 + 2x + 2)/(x 3 + 3x 2 + 3x + 1)dx?Coding:1. How can I implement a function in C++ that dynamically allocates a 2D array based on user input sizes, initializes all elements to zero, and then deallocates the memory properly to avoid memory leaks?2.Write a JavaScript function to fetch data from a given URL, parse the JSON response, and filter the results to return an array of items where a specific key's value matches a condition.STEM knowledge:1.How do you calculate the Schwarzschild radius of a black hole, and what implications does this have for the concept of event horizons in general relativity?2.Can you explain the process of splicing in eukaryotic gene expression and its significance in the diversity of the proteome?Humanities/social science knowledge:1.Discuss the impact of colonial legacies on contemporary political structures in African countries, with examples.2.Analyze the social and economic consequences of the one-child policy in China.The design in the question-generation and peer-debate process ensures that contamination is minimized.Data contamination refers to the possibility of test instances showing up in pre-training or Supervised Fine-tuning data.C CONTAMINATION ANALYSISQuestion-generation: As we generate the questions automatically, we reduce the risk of test instances being eventually exposed to the open web, which can happen in static datasets.Alleviation of data contamination is often shown to be an advantage of such dynamic and frequently updated evaluation frameworks(Li et al., 2023b).Peer Debate: Peer debate ensures that we evaluate the entire debate instead of simple questionanswers, which further reduces contamination.During debates, the models are evaluated on comprehensive and deep abilities, such as planning the strategies, pointing out flaws of the opponents, and drafting further questions.Such interactive evaluation frameworks are shown to reduce contamination(Yu et al., 2024;Bai et al., 2024).Besides the design choices, we conduct a contamination analysis to compare the contamination percentage of Auto-Arena debate questions and test questions in popular benchmarks.Specifically, we use two types of contamination detection metrics:1.The string match metric as inGPT-4 (OpenAI et al., 2024), where a match is identified if any of three 50-character randomly sampled substrings from the evaluation data point (or the entire string if it is shorter than this) is a substring of the training set.If so, we mark the point as contaminated.
Open foundation models by 01. : Ai, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai, Yi, ai2024</p>
<p>Introducing the next generation of Claude. Anthropic, 2024</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, arXiv:2309.16609Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann2023. 2022arXiv preprintand Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback</p>
<p>Benchmarking foundation models with language-model-as-an-examiner. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Advances in Neural Information Processing Systems. 202436</p>
<p>. Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, Thomas Wolf, Open llm leaderboard. 2023</p>
<p>Elo uncovered: Robustness and best practices in language model evaluation. Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, Marzieh Fadaee, Proceedings of the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM). Sebastian Gehrmann, Alex Wang, João Sedoc, Elizabeth Clark, Kaustubh Dhole, Raghavi Khyathi, Enrico Chandu, Hooman Santus, Sedghamiz, the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Rank analysis of incomplete block designs: I. the method of paired comparisons. Ralph Allan, Bradley , Milton E Terry, Biometrika. 393/41952</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang, Xing Xie, 10.1145/3641289ACM Trans. Intell. Syst. Technol. 2157-6904153mar 2024a</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang, Xing Xie, 10.1145/3641289ACM Trans. Intell. Syst. Technol. 2157-6904153mar 2024b</p>
<p>Chatbot arena: An open platform for evaluating llms by human preference. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, Ion Stoica, 2024</p>
<p>Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, Yiqun Liu, arXiv:2401.15641Pre: A peer review based large language model evaluator. 2024arXiv preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, 2018</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168arXiv:2305.13281Lm vs lm: Detecting factual errors via cross examination. May Cohen, Mor Hamri, Amir Geva, Globerson, 2021. 2023arXiv preprintTraining verifiers to solve math word problems</p>
<p>Cohere, Introducing Command R+: A Scalable LLM Built for Business. 2024</p>
<p>. Deepseek-Ai , : , Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y K Li, Wenfeng Liang, Fangyun Lin, A X Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R X Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, 2024Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language models with longtermism</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, arXiv:2305.143252023arXiv preprint</p>
<p>Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B Hashimoto, arXiv:2404.04475Length-controlled alpacaeval: A simple way to debias automatic evaluators. 2024aarXiv preprint</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2024b</p>
<p>The rating of chessplayers: Past and present. E Arpad, Sam Elo, Sloan, No Title). 1978</p>
<p>Agentgroupchat: An interactive group chat simulacra for better eliciting emergent behavior. Zhouhong Gu, Xiaoxuan Zhu, Haoran Guo, Lin Zhang, Yin Cai, Hao Shen, Jiangjie Chen, Zheyu Ye, Yifei Dai, Yan Gao, Yao Hu, Hongwei Feng, Yanghua Xiao, 2024</p>
<p>Are large language model-based evaluators the solution to scaling up multilingual evaluation?. Rishav Hada, Varun Gumma, Adrian De Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram, arXiv:2309.074622023arXiv preprint</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021a</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021b</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed2024Mixtral of experts</p>
<p>Debating with more persuasive llms leads to more truthful answers. Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, R Samuel, Tim Bowman, Ethan Rocktäschel, Perez, 2024</p>
<p>Platypus: Quick, cheap, and powerful refinement of llms. Ariel N Lee, Cole J Hunter, Nataniel Ruiz, 2024</p>
<p>Ruosen Li, arXiv:2307.02762Teerth Patel, and Xinya Du. Prd: Peer rank and discussion improve large language model based evaluations. 2023aarXiv preprint</p>
<p>From live data to high-quality benchmarks: The arena-hard pipeline. Tianle Li, * , Wei-Lin Chiang, * , Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E Gonzalez, Ion Stoica, April 2024</p>
<p>Latesteval: Addressing data contamination in language model evaluation through dynamic and time-sensitive test construction. Yucheng Li, Frank Geurin, Chenghua Lin, AAAI Conference on Artificial Intelligence. 2023b</p>
<p>An open source data contamination report for large language models. Yucheng Li, Frank Guerin, Chenghua Lin, 2024</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, 2023</p>
<p>LLM-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. Yen-Ting Lin, Yun-Nung Chen, 10.18653/v1/2023.nlp4convai-1.5Proceedings of the 5th Workshop on NLP for Conversational AI. Yun-Nung Chen, Abhinav Rastogi, the 5th Workshop on NLP for Conversational AIToronto, CanadaAssociation for Computational LinguisticsNLP4ConvAI 2023. July 2023</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, Gpteval, arXiv:2303.16634Nlg evaluation using gpt-4 with better human alignment. 2023arXiv preprint</p>
<p>Interrater reliability: the kappa statistic. Mary L Mchugh, The most capable openly available LLM to date. 2012. 202422Biochemia medica</p>
<p>Peer-review-in-llms: Automatic evaluation method for llms in open-environment. Kun-Peng Ning, Shuo Yang, Yu-Yang Liu, Jia-Yu Yao, Zhen-Hui Liu, Yu Wang, Ming Pang, Li Yuan, arXiv:2402.018302024arXiv preprint</p>
<p>Openai, New embedding models and API updates. 2024a</p>
<p>Openai, GPT-4o. 2024b</p>
<p>. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Red Anadkat, Igor Avila, Suchir Babuschkin, Valerie Balaji, Paul Balcom, Haiming Baltescu, Mohammad Bao, Jeff Bavarian, Irwan Belgum, Jake Bello, Gabriel Berdine, Christopher Bernadett-Shapiro, Lenny Berner, Oleg Bogdonoff, Madelaine Boiko, Anna-Luisa Boyd, Greg Brakman, Tim Brockman, Miles Brooks, Kevin Brundage, Trevor Button, Rosie Cai, Andrew Campbell, Brittany Cann, Chelsea Carey, Rory Carlson, Brooke Carmichael, Che Chan, Fotis Chang, Derek Chantzis, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chen, Chester Chess, Casey Cho, Hyung Won Chu, Dave Chung, Jeremiah Cummings, Yunxing Currier, Cory Dai, Thomas Decareaux, Noah Degry, Damien Deutsch, Arka Deville, David Dhar, Steve Dohan, Sheila Dowling, Adrien Dunning, Atty Ecoffet, Tyna Eleti, David Eloundou, Liam Farhi, Niko Fedus, Simón Felix, Juston Posada Fishman, Isabella Forte, Leo Fulford, Elie Gao, Christian Georges, Vik Gibson, Tarun Goel, Gabriel Gogineni, Rapha Goh, Jonathan Gontijo-Lopes, Morgan Gordon, Scott Grafstein, Ryan Gray, Joshua Greene, Gross, Shane Shixiang, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Yuchen Harris, Mike He, Johannes Heaton, Chris Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Brandon Hoeschele, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Joanne Jain, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Heewoo Jonn, Tomer Jun, Łukasz Kaftan, Ali Kaiser, Ingmar Kamali, Nitish Kanitscheider, Tabarak Shirish Keskar, Logan Khan, Jong Wook Kilpatrick, Christina Kim, Yongjik Kim, Jan Hendrik Kim, Jamie Kirchner, Matt Kiros, Daniel Knight, Łukasz Kokotajlo, Andrew Kondraciuk, Aris Kondrich, Kyle Konstantinidis, Gretchen Kosic, Vishal Krueger, Michael Kuo, Ikai Lampe, Teddy Lan, Jan Lee, Jade Leike, Daniel Leung, Levy, Ming Chak, Rachel Li, Molly Lim, Stephanie Lin, Mateusz Lin, Theresa Litwin, Ryan Lopez, Patricia Lowe, Anna Lue, Kim Makanju, Sam Malfacini, Todor Manning, Yaniv Markov, Bianca Markovski, Katie Martin, Andrew Mayer, Mayne ; Aalok, Jacob Mehta, Luke Menick, Andrey Metz, Pamela Mishchenko, Ashvin Mishkin ; Mély, Reiichiro Nair, Rajeev Nakano, Arvind Nayak, Richard Neelakantan, Hyeonwoo Ngo, Long Noh, Ouyang, O' Cullen, Jakub Keefe, Alex Pachocki, Joe Paino, Ashley Palermo, Giambattista Pantuliano, Joel Parascandolo, Emy Parish, Alex Parparita, Mikhail Passos, Andrew Pavlov, Adam Peng, Daniel Perelman ; John Schulman, Kyla Selsam, Toki Sheppard, Jessica Sherbakov, Sarah Shieh, Pranav Shoker, Szymon Shyam, Eric Sidor, Maddie Sigler, Jordan Simens, Katarina Sitkin, Ian Slama, Benjamin Sohl, Yang Sokolowsky, Natalie Song, Staudacher, Wei, Akila Cj Weinmann, Peter Welihinda, Jiayi Welinder, Lilian Weng, Matt Weng, Dave Wiethoff, Clemens Willner, Samuel Winter, Hannah Wolrich, Lauren Wong, Sherwin Workman, Jeff Wu, Michael Wu, Kai Wu, Tao Xiao, Sarah Xu, Kevin Yoo, Qiming Yu, Wojciech Yuan, Rowan Zaremba, Chong Zellers, Marvin Zhang, Shengjia Zhang, Tianhao Zhao, Juntang Zheng, William Zhuang, Barret Zhuk, Zoph, Felipe Petroski Such. Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,2024Amin TootoonchianFilipe de Avila Belbute Peres ; Juan Felipe Cerón Uribe, Andrea Vallone, Arun VijayvergiyaGpt-4 technical report</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 1532-4435211jan 2020</p>
<p>How much are llms contaminated? a comprehensive survey and the llmsanitize library. Bosheng Mathieu Ravaut, Fangkai Ding, Hailin Jiao, Xingxuan Chen, Ruochen Li, Chengwei Zhao, Caiming Qin, Shafiq Xiong, Joty, 2024</p>
<p>Gpqa: A graduate-level google-proof q&amp;a benchmark, 2023. Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and Lidong Bing. Large language models are not yet human-level evaluators for abstractive summarization. Nils Reimers, Iryna Gurevych, ; Betty, Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2019. 202311Sentence-bert: Sentence embeddings using siamese bert-networks. In Findings of the Association for Computational Linguistics: EMNLP 2023</p>
<p>. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, A Christopher, Danila Choquette-Choo, David Sinopalnikov, Dimple Weinberger, Dominika Vijaykumar, Dustin Rogozińska, Elisa Herbison, Emma Bandy, Eric Wang, Erica Noland, Evan Moreira, Evgenii Senter, Francesco Eltyshev, Visin, Gary Gabriel Rasskin, Glenn Wei, Gus Cameron, Hadi Martins, Hanna Hashemi, Harleen Klimczak-Plucińska, Harsh Batra, Ivan Dhand, Jacinda Nardini, Jack Mein, James Zhou, Jeff Svensson, Jetha Stanway, Jin Peng Chan, Joana Zhou, Joana Carrasqueira, Jocelyn Iljazi, Joe Becker, Joost Fernandez, Josh Van Amersfoort, Josh Gordon, Josh Lipschultz, Newlan, Kareem Ju Yeong Ji, Kartikeya Mohamed, Kat Badola, Katie Black, Keelin Millican, Kelvin Mcdonell, Kiranbir Nguyen, Kish Sodhia, Lars Lowe Greene, Lauren Sjoesund, Laurent Usui, Lena Sifre, Leticia Heuermann, Lilly Lago, Mcnealus, Baldini Livio, Logan Soares, Lucas Kilpatrick, Luciano Dixon, Machel Martins, Manvinder Reid, Mark Singh, Martin Iverson, Mat Görner, Mateo Velloso, Matt Wirth, Matt Davidow, Matthew Miller, Matthew Rahtz, Meg Watson, Mehran Risdal, Michael Kazemi, Ming Moynihan, Minsuk Zhang, Minwoo Kahng, Mofi Park, Mohit Rahman, Natalie Khatwani, Nenshad Dao, Nesh Bardoliwalla, Neta Devanathan, Nilay Dumai, Oscar Chauhan, Pankil Wahltinez, Parker Botarda, Paul Barnes, Paul Barham, Pengchong Michel, Timothy Jin ; Susan Chan, Ting Jordan, Tom Yu, Tom Eccles, Tomas Hennigan, Tulsee Kocisky, Vihan Doshi, Vikas Jain, Vilobh Yadav, Vishal Meshram, Warren Dharmadhikari, Wei Barkley, Wenming Wei, Woohyun Ye, Woosuk Han, Xiang Kwon, Zhe Xu, Zhitao Shen, Zichuan Gong, Victor Wei, Phoebe Cotruta, Anand Kirk, Minh Rao, Ludovic Giang, Tris Peran, Warkentin, Joelle Barral, Zoubin Ghahramani. Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Perrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom,2024aRaia Hadsell, D. Sculley, Jeanine Banks, Anca DraganPetko Georgiev. Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving open language models at a practical size</p>
<p>. Reka Team, Aitor Ormazabal, Che Zheng, Cyprien De Masson D'autume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi Wang, Zhongkai Zhu, Zhihui Xie, 2024bReka core, flash, and edge: A series of powerful multimodal language models</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Large language models are diverse role-players for summarization evaluation. Ning Wu, Ming Gong, Linjun Shou, Shining Liang, Daxin Jiang, CCF International Conference on Natural Language Processing and Chinese Computing. Springer2023a</p>
<p>A brief overview of chatgpt: The history, status quo and potential future development. Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, Kang Liu, Qing-Long Han, Yang Tang, 10.1109/JAS.2023.123618IEEE/CAA Journal of Automatica Sinica. 1052023b</p>
<p>Kieval: A knowledge-grounded interactive evaluation framework for large language models. Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, Shikun Zhang, arXiv:2402.150432024arXiv preprint</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, 2019</p>
<p>Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Chen Hao, Xing Xie, Competeai: Understanding the competition behaviors in large language model-based agents. 2023</p>
<p>Model Name Reasons for Inclusion License GPT-4-0409-Turbo. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, ; Openai, Newest and Strongest in GPT model family under GPT-4 Proprietary GPT-4o-2024-05-13 (Openai, 2024b) Newly released model in GPT Model Family Proprietary GPT-3.5-Turbo-0125 (Openai, 2024a) Newest ChatGPT version in the GPT Model Family Proprietary Claude-3.5-Sonnet-20240620 (Anthropic, 2024) Newest in Claude model family under Claude-3.5 Proprietary Claude-3-Haiku (Anthropic, 2024) Newest and Cheapest in Claude model family under Claude-3 Proprietary Qwen/Qwen2-72B-Instruct. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc2023. 2024. 202336Advances in Neural Information Processing Systems. Representative of Qwen Model Family under Qwen-2 Proprietary Qwen1.5-72B-chat (Bai et al., 2023) Representative of Qwen model family under Qwen-1.5</p>
<p>NC-4.0Qianwen LICENSE Command R Plus (Cohere, 2024) Strongest model in Command R Model Family CC-BY. </p>
<p>Representative of Llama Model Family under Llama-2 Llama 2 Community Mixtral-8x7b-Instruct-v0. Touvron, Strongest in open-source Mistral small models Apache. 212023. 2024Llama-3-70b-chat-hfRepresentative of Llama Model Family under Llama-3 Llama 3 Community Llama-2-70b-chat</p>
<p>Team, MOE Structure Gemma-2-27b-it. Representative of the Gemma family Apache. 2024a2</p>
<p>Representative open-source model in Deepseek Family DeepSeek License In Table 11, we show all the models selected for the main experiment and expansion. We also include the reasons for selection. Overall, we try to select a representative set of famous models on Chatbot Arena top 20 list. While the Chatbot Arena ranking mostly consists of models with different versions. Team, Gemini-1.5-flash-exp-08272024a. 2024. 2024Strongest in Yi Model Family on Chatbot Arena Yi License Deepseek-LLM-67B-chat</p>            </div>
        </div>

    </div>
</body>
</html>
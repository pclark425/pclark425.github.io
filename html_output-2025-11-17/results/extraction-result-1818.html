<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1818 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1818</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1818</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-25425e299101b13ec2872417a14f961f4f8aa18e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/25425e299101b13ec2872417a14f961f4f8aa18e" target="_blank">VIMA: General Robot Manipulation with Multimodal Prompts</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is shown that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens, and designed a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively.</p>
                <p><strong>Paper Abstract:</strong> Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieves strong model scalability and data efficiency. It outperforms alternative designs in the hardest zero-shot generalization setting by up to $2.9\times$ task success rate given the same training data. With $10\times$ less training data, VIMA still performs $2.7\times$ better than the best competing variant. Code and video demos are available at https://vimalabs.github.io/</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1818.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1818.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIMA (T5 encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VisuoMotor Attention agent using a pre-trained T5 encoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>VIMA integrates a frozen pre-trained T5 language encoder to process multimodal (text+image) prompts and conditions a transformer decoder via cross-attention to autoregressively decode robot action tokens for tabletop manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>T5 encoder within VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A frozen pre-trained T5 text encoder (text-to-text Transformer) used to encode interleaved textual and object-derived visual tokens; its outputs provide keys/values for cross-attention in a causal transformer decoder which autoregressively predicts discretized action tokens mapped to robot poses.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale text corpora / language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper uses a frozen T5 encoder (pre-trained on large text corpora as per the T5 family). VIMA paper does not re-run or detail the original T5 pretraining dataset size; it states the encoder is a pre-trained T5 (they use T5-Base by default and experiment with different T5 capacities).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>VIMA-BENCH (17 multimodal tabletop manipulation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>A Ravens-based 3D tabletop manipulation benchmark (UR5 arm, PyBullet) with 17 procedurally generated tasks expressed as multimodal prompts (text interleaved with object / scene images), including visual goal reaching (rearrangement), one-shot video imitation, novel concept grounding, visual-constraint tasks (wipe/push), and visual reasoning; evaluation includes 4 levels of zero-shot generalization (placement, combinatorial, novel object, novel task).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Not an action environment — T5 pretraining is text-token prediction (next-token language modeling); there is no explicit 'action' semantics in the pretraining data beyond textual tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>High-level primitive motor skills (pick-and-place and wipe/push) parameterized by end-effector poses in SE(2); primitives represented as two poses per action (e.g., pick and place poses), with actions discretized into tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Prompt encoded by frozen T5 produces key/value representations; a causal transformer decoder (policy) queries these via cross-attention to predict discrete action tokens; predicted action tokens are mapped to discretized end-effector poses (following common practice referenced in the paper) to produce executable pick/place or push/wipe primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images from frontal and top-down views; Mask R-CNN object detections (bounding boxes + cropped images) are used to produce object-centric tokens; ground-truth segmentation/bounding boxes are available for oracle experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>VIMA (using the frozen T5 encoder to encode multimodal prompts) outperforms alternative architectures across all four zero-shot generalization levels; reported improvements include up to 2.9× higher task success rate compared to the best competing variant given the same training data, and 2.7× better even when using 10× less training data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baseline variants (decoder-only or pixel-based variants adapted to the same setting) perform substantially worse; the paper reports up to a 2.9× gap in task success rate in favor of VIMA, but does not provide single-number absolute success rates for every baseline in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>VIMA is highly sample-efficient: with 1% of the imitation dataset it achieves performance comparable to baselines trained with 10× more data on L1 and L2; for L4 VIMA with 1% already surpasses other variants trained on the full dataset. The full imitation dataset has ~650K successful trajectories total (50K per task on average).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Baselines required roughly 10× more data to reach comparable L1/L2 performance (i.e., needed close to the full dataset rather than 1%); exact baseline sample counts per performance threshold are not enumerated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Approximately up to 10× reduction in required imitation data to match baseline performance on easier generalization levels (1% vs. 10%/100% of dataset), and substantial gains (VIMA outperforms baselines even when trained with 10× less data).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>High-quality semantic conditioning from the pre-trained T5 (robust language understanding), object-centric visual tokenization that narrows the perception gap, explicit cross-attention conditioning between prompt encodings and trajectory history (strong prompt-to-policy linkage), object-detection augmentation to mitigate detector noise, and a decoder architecture that maps prompt semantics to discretized action tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limitations include remaining degradation on the hardest zero-shot setting (novel task L4) for all methods (pretraining reduces but does not eliminate performance drop), and inherent mismatch between text-only pretraining and spatial/motor grounding (text pretraining lacks native spatial/action grounding which must be learned via the downstream imitation data).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a frozen language model (T5) as a prompt encoder and conditioning a policy decoder via cross-attention yields strong positive transfer: large gains in sample efficiency and zero-shot generalization on 3D embodied tabletop manipulation tasks when combined with object-centric visual tokens and cross-attention conditioning; however pure text pretraining alone is insufficient to fully close gaps on novel-task generalization, so architectural choices and object-centric perceptual input are crucial mediators of successful transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VIMA: General Robot Manipulation with Multimodal Prompts', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1818.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1818.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIMA-Flamingo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flamingo-based baseline adapted within VIMA framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adapted Flamingo vision-language model (originally pre-trained on image-text corpora) is repurposed as a baseline by converting its Perceiver-based image embedding into inputs for a robot action head to predict actions for VIMA-BENCH tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Flamingo: a visual language model for fewshot learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Flamingo (adapted as VIMA-Flamingo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Flamingo is a vision–language model that embeds a variable number of prompt images into a fixed number of tokens via a Perceiver resampler and conditions a decoder via cross-attention; VIMA adapts it by replacing the output layer with robot action heads to produce robot actions.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multimodal image–text web corpora (vision+language pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The Flamingo model used as a basis for the baseline was pre-trained on large image-text datasets (the VIMA paper references Flamingo's pretraining but does not re-specify dataset sizes or concrete corpus details).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>VIMA-BENCH (evaluated as a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same VIMA-BENCH suite of 17 multimodal tabletop manipulation tasks (Ravens-based simulator, UR5 arm, pick/place and push primitives) used to evaluate zero-shot generalization across four levels.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Flamingo's pretraining is language-and-image-conditioned next-token modeling (text tokens + image-conditioned language modeling); pretraining does not include robot action tokens by default.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Same as VIMA: high-level pick/place and push/wipe primitives parameterized by SE(2) end-effector poses and discretized into action tokens for imitation policy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Adaptation replaces Flamingo's textual output layer with robot action heads; image prompt embeddings are obtained via Perceiver resampling to a fixed small set of tokens, then fed to the policy head to produce discrete action tokens mapped to poses.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images (frontal and top-down); Flamingo-style embedding uses Perceiver to compress image prompts into fixed tokens, not object-centric bounding boxes by default.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>The Flamingo-based variant improves with model size but underperforms VIMA's object-centric + cross-attention recipe; the VIMA paper reports VIMA outperforms such alternatives across generalization levels (no Flamingo-specific absolute success rates provided in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not explicitly enumerated for Flamingo in the paper; relative comparison indicates Flamingo baseline performs worse than VIMA across all model sizes and generalization levels.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>The paper does not provide Flamingo-specific sample-efficiency numbers; overall finding: VIMA achieves comparable or better performance using significantly less imitation data than pixel-based/more end-to-end baselines (including Flamingo-adapted variants).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not specified per Flamingo baseline; baseline variants typically required more data to approach VIMA performance (up to ~10× in aggregate comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Unclear numerically for Flamingo, but relative gains are attributed to VIMA's recipe (object tokens + cross-attention) rather than Perceiver-style fixed downsampling; VIMA achieves roughly 10× sample-efficiency improvements vs. some baselines on L1/L2.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Flamingo's multimodal pretraining provides cross-modal semantic priors (image+text) useful for understanding prompts; Perceiver resampler enables fixed-size conditioning on variable prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Downsampling variable object sequences to a fixed small set of tokens (Perceiver) reduces decision-making performance for object-rich scenes; fixed-size tokenization can lose fine-grained object-level details needed for manipulation, hurting transfer to embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multimodal image–text pretraining (Flamingo) yields useful semantic priors but, when adapted naively (Perceiver resampling to few tokens), underperforms an object-centric, cross-attention conditioned policy; preserving variable-length object-level information is important when transferring multimodal pretraining to fine-grained 3D manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VIMA: General Robot Manipulation with Multimodal Prompts', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1818.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1818.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E: An embodied multimodal language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PaLM-E is cited as a multimodal language model that can be repurposed for sequential robotic manipulation planning; in this paper it is discussed as related work (not used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Palm-e: An embodied multimodal language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A multimodal (vision+language+other modalities) language model designed to be repurposed for embodied sequential planning and robotic manipulation tasks; referenced as related work showing LLMs applied to robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multimodal (language + vision) corpora / multimodal pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The VIMA paper only mentions PaLM-E in related work; it does not provide PaLM-E's pretraining dataset details or sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Referenced generally as being repurposed for sequential robotic manipulation (not evaluated within VIMA experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>PaLM-E has been proposed to handle sequential robot planning tasks by integrating multimodal inputs; the VIMA paper does not evaluate PaLM-E on VIMA-BENCH.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Pretraining is on multimodal text+vision data; pretraining objective is not an action-environment but language modeling / multimodal prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Multimodal perception (vision + language); specifics not detailed in VIMA paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Mentioned as an example of repurposing multimodal LLMs for robotics — suggests benefits from large-scale multimodal pretraining and language grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not discussed in detail in VIMA; general concerns include modality grounding and action/perception gaps between language pretraining and low-level motor control.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>VIMA cites PaLM-E as related evidence that large multimodal language models can be repurposed for embodied tasks, but emphasizes that architectural and perception design (object-centric tokens, cross-attention conditioning) are crucial to realize robust transfer in manipulation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VIMA: General Robot Manipulation with Multimodal Prompts', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Flamingo: a visual language model for fewshot learning <em>(Rating: 2)</em></li>
                <li>PaLM-E: An embodied multimodal language model <em>(Rating: 2)</em></li>
                <li>A generalist agent <em>(Rating: 1)</em></li>
                <li>Multimodal few-shot learning with frozen language models <em>(Rating: 1)</em></li>
                <li>Pre-trained language models for interactive decision-making <em>(Rating: 1)</em></li>
                <li>Do as I can, not as I say: Grounding language in robotic affordances <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1818",
    "paper_id": "paper-25425e299101b13ec2872417a14f961f4f8aa18e",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "VIMA (T5 encoder)",
            "name_full": "VisuoMotor Attention agent using a pre-trained T5 encoder",
            "brief_description": "VIMA integrates a frozen pre-trained T5 language encoder to process multimodal (text+image) prompts and conditions a transformer decoder via cross-attention to autoregressively decode robot action tokens for tabletop manipulation tasks.",
            "citation_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "mention_or_use": "use",
            "model_agent_name": "T5 encoder within VIMA",
            "model_agent_description": "A frozen pre-trained T5 text encoder (text-to-text Transformer) used to encode interleaved textual and object-derived visual tokens; its outputs provide keys/values for cross-attention in a causal transformer decoder which autoregressively predicts discretized action tokens mapped to robot poses.",
            "pretraining_data_type": "Large-scale text corpora / language modeling",
            "pretraining_data_details": "The paper uses a frozen T5 encoder (pre-trained on large text corpora as per the T5 family). VIMA paper does not re-run or detail the original T5 pretraining dataset size; it states the encoder is a pre-trained T5 (they use T5-Base by default and experiment with different T5 capacities).",
            "embodied_task_name": "VIMA-BENCH (17 multimodal tabletop manipulation tasks)",
            "embodied_task_description": "A Ravens-based 3D tabletop manipulation benchmark (UR5 arm, PyBullet) with 17 procedurally generated tasks expressed as multimodal prompts (text interleaved with object / scene images), including visual goal reaching (rearrangement), one-shot video imitation, novel concept grounding, visual-constraint tasks (wipe/push), and visual reasoning; evaluation includes 4 levels of zero-shot generalization (placement, combinatorial, novel object, novel task).",
            "action_space_text": "Not an action environment — T5 pretraining is text-token prediction (next-token language modeling); there is no explicit 'action' semantics in the pretraining data beyond textual tokens.",
            "action_space_embodied": "High-level primitive motor skills (pick-and-place and wipe/push) parameterized by end-effector poses in SE(2); primitives represented as two poses per action (e.g., pick and place poses), with actions discretized into tokens.",
            "action_mapping_method": "Prompt encoded by frozen T5 produces key/value representations; a causal transformer decoder (policy) queries these via cross-attention to predict discrete action tokens; predicted action tokens are mapped to discretized end-effector poses (following common practice referenced in the paper) to produce executable pick/place or push/wipe primitives.",
            "perception_requirements": "RGB images from frontal and top-down views; Mask R-CNN object detections (bounding boxes + cropped images) are used to produce object-centric tokens; ground-truth segmentation/bounding boxes are available for oracle experiments.",
            "transfer_successful": true,
            "performance_with_pretraining": "VIMA (using the frozen T5 encoder to encode multimodal prompts) outperforms alternative architectures across all four zero-shot generalization levels; reported improvements include up to 2.9× higher task success rate compared to the best competing variant given the same training data, and 2.7× better even when using 10× less training data.",
            "performance_without_pretraining": "Baseline variants (decoder-only or pixel-based variants adapted to the same setting) perform substantially worse; the paper reports up to a 2.9× gap in task success rate in favor of VIMA, but does not provide single-number absolute success rates for every baseline in the main text.",
            "sample_complexity_with_pretraining": "VIMA is highly sample-efficient: with 1% of the imitation dataset it achieves performance comparable to baselines trained with 10× more data on L1 and L2; for L4 VIMA with 1% already surpasses other variants trained on the full dataset. The full imitation dataset has ~650K successful trajectories total (50K per task on average).",
            "sample_complexity_without_pretraining": "Baselines required roughly 10× more data to reach comparable L1/L2 performance (i.e., needed close to the full dataset rather than 1%); exact baseline sample counts per performance threshold are not enumerated in the paper.",
            "sample_complexity_gain": "Approximately up to 10× reduction in required imitation data to match baseline performance on easier generalization levels (1% vs. 10%/100% of dataset), and substantial gains (VIMA outperforms baselines even when trained with 10× less data).",
            "transfer_success_factors": "High-quality semantic conditioning from the pre-trained T5 (robust language understanding), object-centric visual tokenization that narrows the perception gap, explicit cross-attention conditioning between prompt encodings and trajectory history (strong prompt-to-policy linkage), object-detection augmentation to mitigate detector noise, and a decoder architecture that maps prompt semantics to discretized action tokens.",
            "transfer_failure_factors": "Limitations include remaining degradation on the hardest zero-shot setting (novel task L4) for all methods (pretraining reduces but does not eliminate performance drop), and inherent mismatch between text-only pretraining and spatial/motor grounding (text pretraining lacks native spatial/action grounding which must be learned via the downstream imitation data).",
            "key_findings": "Using a frozen language model (T5) as a prompt encoder and conditioning a policy decoder via cross-attention yields strong positive transfer: large gains in sample efficiency and zero-shot generalization on 3D embodied tabletop manipulation tasks when combined with object-centric visual tokens and cross-attention conditioning; however pure text pretraining alone is insufficient to fully close gaps on novel-task generalization, so architectural choices and object-centric perceptual input are crucial mediators of successful transfer.",
            "uuid": "e1818.0",
            "source_info": {
                "paper_title": "VIMA: General Robot Manipulation with Multimodal Prompts",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "VIMA-Flamingo",
            "name_full": "Flamingo-based baseline adapted within VIMA framework",
            "brief_description": "An adapted Flamingo vision-language model (originally pre-trained on image-text corpora) is repurposed as a baseline by converting its Perceiver-based image embedding into inputs for a robot action head to predict actions for VIMA-BENCH tasks.",
            "citation_title": "Flamingo: a visual language model for fewshot learning",
            "mention_or_use": "use",
            "model_agent_name": "Flamingo (adapted as VIMA-Flamingo)",
            "model_agent_description": "Flamingo is a vision–language model that embeds a variable number of prompt images into a fixed number of tokens via a Perceiver resampler and conditions a decoder via cross-attention; VIMA adapts it by replacing the output layer with robot action heads to produce robot actions.",
            "pretraining_data_type": "Multimodal image–text web corpora (vision+language pretraining)",
            "pretraining_data_details": "The Flamingo model used as a basis for the baseline was pre-trained on large image-text datasets (the VIMA paper references Flamingo's pretraining but does not re-specify dataset sizes or concrete corpus details).",
            "embodied_task_name": "VIMA-BENCH (evaluated as a baseline)",
            "embodied_task_description": "Same VIMA-BENCH suite of 17 multimodal tabletop manipulation tasks (Ravens-based simulator, UR5 arm, pick/place and push primitives) used to evaluate zero-shot generalization across four levels.",
            "action_space_text": "Flamingo's pretraining is language-and-image-conditioned next-token modeling (text tokens + image-conditioned language modeling); pretraining does not include robot action tokens by default.",
            "action_space_embodied": "Same as VIMA: high-level pick/place and push/wipe primitives parameterized by SE(2) end-effector poses and discretized into action tokens for imitation policy decoding.",
            "action_mapping_method": "Adaptation replaces Flamingo's textual output layer with robot action heads; image prompt embeddings are obtained via Perceiver resampling to a fixed small set of tokens, then fed to the policy head to produce discrete action tokens mapped to poses.",
            "perception_requirements": "RGB images (frontal and top-down); Flamingo-style embedding uses Perceiver to compress image prompts into fixed tokens, not object-centric bounding boxes by default.",
            "transfer_successful": true,
            "performance_with_pretraining": "The Flamingo-based variant improves with model size but underperforms VIMA's object-centric + cross-attention recipe; the VIMA paper reports VIMA outperforms such alternatives across generalization levels (no Flamingo-specific absolute success rates provided in main text).",
            "performance_without_pretraining": "Not explicitly enumerated for Flamingo in the paper; relative comparison indicates Flamingo baseline performs worse than VIMA across all model sizes and generalization levels.",
            "sample_complexity_with_pretraining": "The paper does not provide Flamingo-specific sample-efficiency numbers; overall finding: VIMA achieves comparable or better performance using significantly less imitation data than pixel-based/more end-to-end baselines (including Flamingo-adapted variants).",
            "sample_complexity_without_pretraining": "Not specified per Flamingo baseline; baseline variants typically required more data to approach VIMA performance (up to ~10× in aggregate comparisons).",
            "sample_complexity_gain": "Unclear numerically for Flamingo, but relative gains are attributed to VIMA's recipe (object tokens + cross-attention) rather than Perceiver-style fixed downsampling; VIMA achieves roughly 10× sample-efficiency improvements vs. some baselines on L1/L2.",
            "transfer_success_factors": "Flamingo's multimodal pretraining provides cross-modal semantic priors (image+text) useful for understanding prompts; Perceiver resampler enables fixed-size conditioning on variable prompts.",
            "transfer_failure_factors": "Downsampling variable object sequences to a fixed small set of tokens (Perceiver) reduces decision-making performance for object-rich scenes; fixed-size tokenization can lose fine-grained object-level details needed for manipulation, hurting transfer to embodied tasks.",
            "key_findings": "Multimodal image–text pretraining (Flamingo) yields useful semantic priors but, when adapted naively (Perceiver resampling to few tokens), underperforms an object-centric, cross-attention conditioned policy; preserving variable-length object-level information is important when transferring multimodal pretraining to fine-grained 3D manipulation.",
            "uuid": "e1818.1",
            "source_info": {
                "paper_title": "VIMA: General Robot Manipulation with Multimodal Prompts",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "PaLM-E (mentioned)",
            "name_full": "PaLM-E: An embodied multimodal language model",
            "brief_description": "PaLM-E is cited as a multimodal language model that can be repurposed for sequential robotic manipulation planning; in this paper it is discussed as related work (not used in experiments).",
            "citation_title": "Palm-e: An embodied multimodal language model",
            "mention_or_use": "mention",
            "model_agent_name": "PaLM-E",
            "model_agent_description": "A multimodal (vision+language+other modalities) language model designed to be repurposed for embodied sequential planning and robotic manipulation tasks; referenced as related work showing LLMs applied to robotics.",
            "pretraining_data_type": "Multimodal (language + vision) corpora / multimodal pretraining",
            "pretraining_data_details": "The VIMA paper only mentions PaLM-E in related work; it does not provide PaLM-E's pretraining dataset details or sizes.",
            "embodied_task_name": "Referenced generally as being repurposed for sequential robotic manipulation (not evaluated within VIMA experiments)",
            "embodied_task_description": "PaLM-E has been proposed to handle sequential robot planning tasks by integrating multimodal inputs; the VIMA paper does not evaluate PaLM-E on VIMA-BENCH.",
            "action_space_text": "Pretraining is on multimodal text+vision data; pretraining objective is not an action-environment but language modeling / multimodal prediction.",
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": "Multimodal perception (vision + language); specifics not detailed in VIMA paper.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Mentioned as an example of repurposing multimodal LLMs for robotics — suggests benefits from large-scale multimodal pretraining and language grounding.",
            "transfer_failure_factors": "Not discussed in detail in VIMA; general concerns include modality grounding and action/perception gaps between language pretraining and low-level motor control.",
            "key_findings": "VIMA cites PaLM-E as related evidence that large multimodal language models can be repurposed for embodied tasks, but emphasizes that architectural and perception design (object-centric tokens, cross-attention conditioning) are crucial to realize robust transfer in manipulation settings.",
            "uuid": "e1818.2",
            "source_info": {
                "paper_title": "VIMA: General Robot Manipulation with Multimodal Prompts",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Flamingo: a visual language model for fewshot learning",
            "rating": 2
        },
        {
            "paper_title": "PaLM-E: An embodied multimodal language model",
            "rating": 2
        },
        {
            "paper_title": "A generalist agent",
            "rating": 1
        },
        {
            "paper_title": "Multimodal few-shot learning with frozen language models",
            "rating": 1
        },
        {
            "paper_title": "Pre-trained language models for interactive decision-making",
            "rating": 1
        },
        {
            "paper_title": "Do as I can, not as I say: Grounding language in robotic affordances",
            "rating": 1
        }
    ],
    "cost": 0.018160999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>VIMA: General Robot Manipulation with Multimodal Prompts</h1>
<p>Yunfan Jiang ${ }^{1}$ Agrim Gupta ${ }^{1 \dagger}$ Zichen Zhang ${ }^{2 \dagger}$ Guanzhi Wang ${ }^{34 \dagger}$ Yongqiang Dou ${ }^{5}$ Yanjun Chen ${ }^{1}$ Li Fei-Fei ${ }^{1}$ Anima Anandkumar ${ }^{34}$ Yuke Zhu ${ }^{36 \ddagger}$ Linxi Fan ${ }^{3 \ddagger}$</p>
<h4>Abstract</h4>
<p>Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating oneshot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600 K + expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieves strong model scalability and data efficiency. It outperforms alternative designs in the hardest zero-shot generalization setting by up to $2.9 \times$ task success rate given the same training data. With $10 \times$ less training data, VIMA still performs $2.7 \times$ better than the best competing variant. Code and video demos are available at vimalabs.github.io.</p>
<h2>1. Introduction</h2>
<p>Transformer models (Vaswani et al., 2017) have given rise to remarkable multi-task consolidation across many AI domains. For example, users can describe a task using natural language prompt to GPT-3 (Brown et al., 2020), allowing</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the same model to perform question answering, machine translation, text summarization, etc. Prompt-based learning provides an accessible and flexible interface to communicate a natural language understanding task to a general-purpose model.</p>
<p>We envision that a generalist robot should have a similarly intuitive and expressive interface for task specification. What does such an interface for robot learning look like? As a motivating example, consider a personal robot tasked with household activities. We can ask the robot to bring us a cup of water by a simple natural language instruction. If we require more specificity, we can instead instruct the robot to "bring me <image of the cup>". For tasks requiring new skills, the robot should be able to adapt, preferably from a few video demonstrations (Duan et al., 2017). Tasks that need interaction with unfamiliar objects can be easily explained via a few image examples for novel concept grounding (Hermann et al., 2017). Finally, to ensure safe deployment, we can further specify visual constraints like "do not enter <image> room".</p>
<p>To enable a single agent with all these capabilities, we make three key contributions in this work: 1) a novel multimodal prompting formulation that converts a wide spectrum of robot manipulation tasks into one sequence modeling problem; 2) a large-scale benchmark with diverse tasks to systematically evaluate an agent's scalability and generalization; and 3) a multimodal-prompted robot agent capable of multi-task and zero-shot generalization.</p>
<p>We start with the observation that many robot manipulation tasks can be formulated by multimodal prompts that interleave language and images or video frames (Fig. 1). For example, Rearrangement (Batra et al., 2020), a type of Visual Goal, can be formulated as "Please rearrange objects to match this {scene_image}", Fewshot Imitation can embed video snippet in the prompt "Follow this motion trajectory for the wooden cube: $\left{\right.$ frame $\left.<em 2="2">{1}\right},\left{\right.$ frame $\left.</em>\right},\left{\right.$ frame $\left.<em 4="4">{3}\right},\left{\right.$ frame $\left.</em>\right}$ ". Multimodal prompts not only have more expressive power than individual modalities but also enable a uniform sequence IO interface for training generalist robots. Previously, different robot manipulation tasks required distinct policy architectures, objective functions, data pipelines, and training</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Multimodal prompts for task specification. We observe that many robot manipulation tasks can be expressed as <em>multimodal prompts</em> that interleave language and image/video frames. We introduce VIMA, an embodied agent capable of processing multimodal prompts (left) and controlling a robot arm to solve the task (right).</p>
<p>procedures (Aceituno et al., 2021; Stengel-Eskin et al., 2022; Lynch &amp; Sermanet, 2021), leading to siloed robot systems that cannot be easily combined for a rich set of use cases. Instead, our multimodal prompt interface allows us to harness the latest advances in large transformer models (Lin et al., 2021; Tay et al., 2020; Khan et al., 2021) for developing scalable multi-task robot learners.</p>
<p>To systematically evaluate agents with multimodal prompts, we develop a new benchmark, named VIMA-Bench, built on the Ravens simulator (Zeng et al., 2020; Shridhar et al., 2021). We provide 17 representative tasks with multimodal prompt templates. Each task can be procedurally instantiated into thousands of instances by various combinations of textures and tabletop objects. VIMA-Bench establishes a four-level protocol to evaluate progressively stronger generalization capabilities, from randomized object placement to novel tasks (Fig. 2).</p>
<p>To this end, we introduce the VisuoMotor Attention agent (VIMA) to learn robot manipulation from multimodal prompts. The model architecture follows the encoder-decoder transformer design proven to be effective and scalable in NLP (Raffel et al., 2020). VIMA encodes an input sequence of interleaving textual and visual prompt tokens with a pre-trained language model (Tsimpoukelli et al., 2021) and decodes robot control actions autoregressively for each environment interaction step. The transformer decoder is conditioned on the prompt via cross-attention layers that alternate with the usual causal self-attention. Instead of operating on raw images, VIMA adopts an object-centric approach. We parse all images in the prompt or observation into objects by off-the-shelf then domain fine-tuned detectors (He et al., 2017) and flatten them into sequences of object tokens. To demonstrate the scalability of VIMA, we train a spectrum of 7 models ranging from 2M to 200M parameters. Our approach outperforms other design alternatives, such as image patch tokens (Reed et al., 2022), image Perceiver (Jaegle et al., 2021b; Alayrac et al., 2022), and decoder-only conditioning (Radford et al., 2018). VIMA obtains consistent performance gains across all four levels of zero-shot generalization and all model capacities, in some cases by a large margin (up to 2.9× task success rate given the same amount of training data, and 2.7× better even with 10× less data). We open-source the simulation environment, training dataset, algorithm code, and pre-trained model checkpoints to ensure reproducibility and facilitate future work from the community. These materials along with video demos are available at vimalabs.github.io.</p>
<h2>2. Multimodal Prompts for Task Specification</h2>
<p>A central and open problem in robot learning is task specification (Agrawal, 2022). In prior literature (Stepputtis et al., 2020; Dasari &amp; Gupta, 2020; Brunke et al., 2021b), different tasks often require diverse and incompatible interfaces, resulting in siloed robot systems that do not generalize well</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Evaluation Protocol in VIMA-Bench. We design 4 levels of evaluation settings to systematically measure the zero-shot generalization capability of an agent. Each level deviates more from the training distribution, and thus is strictly more challenging than the previous level.
across tasks. Our key insight is that various task specification paradigms (such as goal conditioning, video demonstration, natural language instruction) can all be instantiated as multimodal prompts (Fig. 1). Concretely, a multimodal prompt $\mathcal{P}$ of length $l$ is defined as an ordered sequence of arbitrarily interleaved texts and images $\mathcal{P}:=\left[x_{1}, x_{2}, \ldots, x_{l}\right]$, where each element $x_{i} \in{\text { text, image }}$.</p>
<p>Task Suite. The flexibility afforded by multimodal prompts allows us to specify and build models for a variety of task specification formats. Here we consider the following six categories.</p>
<ol>
<li>Simple object manipulation. Simple tasks like "put <object> into <container>", where each image in the prompt corresponds to a single object;</li>
<li>Visual goal reaching. Manipulating objects to reach a goal configuration, e.g., Rearrangement (Batra et al., 2020);</li>
<li>Novel concept grounding. The prompt contains unfamiliar words like "dax" and "blicket", which are explained by in-prompt images and then immediately used in an instruction. This tests the agent's ability to rapidly internalize new concepts;</li>
<li>One-shot video imitation. Watching a video demonstration and learning to reproduce the same motion trajectory for a particular object;</li>
<li>Visual constraint satisfaction. The robot must manipulate the objects carefully and avoid violating the (safety) constraints;</li>
<li>Visual reasoning. Tasks that require reasoning skills, such as appearance matching "move all objects with same textures as <object> into <container>", and visual memory "put <object> in <container> and then restore to their original position".</li>
</ol>
<p>Note that these six categories are not mutually exclusive. For example, a task may introduce a previously unseen verb (Novel Concept) by showing a video demonstration, or combine goal reaching with visual reasoning. More details about the task suite are discussed in Appendix, Sec. B.</p>
<h2>3. VIMA-BENCH: Benchmark for Multimodal Robot Learning</h2>
<p>Simulation Environment. Existing benchmarks are generally geared towards a particular task specification. To our knowledge, there is no benchmark that provides a rich suite of multimodal tasks and a comprehensive testbed for targeted probing of agent capabilities. To this end, we introduce a new benchmark suite for multimodal robot learning called VIMA-BENCH. We build our benchmark by extending the Ravens robot simulator (Zeng et al., 2020). VIMA-BENCH supports extensible collections of objects and textures to compose multimodal prompts and to procedurally generate a large number of tasks. Specifically, we provide 17 tasks with multimodal prompt templates, which can be instantiated into thousands of task instances. Each task belongs to one or more of the 6 task categories mentioned above. VIMA-BENCH can generate large quantities of imitation learning data via scripted oracle agents. More details are elaborated in Appendix, Sec. A.</p>
<p>Observation and Actions. The observation space of our simulator includes RGB images rendered from both frontal view and top-down view. Ground-truth object segmentation and bounding boxes are also provided for training objectcentric models (Sec. 4). We inherit the high-level action space from Zeng et al. (2020), which consists of primitive motor skills like "pick and place" and "wipe". These are parameterized by poses of the end effector. Our simulator also features scripted oracle programs that can generate expert demonstrations by using privileged simulator state information, such as the precise location of all objects, and the ground-truth interpretation of the multimodal instruction.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: VIMA Architecture. We encode the multimodal prompts with a pre-trained T5 model, and condition the robot controller on the prompt through cross-attention layers. The controller is a causal transformer decoder consisting of alternating self and cross attention layers that predicts motor commands conditioned on prompts and interaction history.</p>
<p>Training Dataset. We leverage oracles to generate a large offline dataset of expert trajectories for imitation learning. Our dataset includes 50 K trajectories per task, and 650 K successful trajectories in total. We hold out a subset of objects and textures for evaluation and designate 4 out of 17 tasks as a testbed for zero-shot generalization.</p>
<p>Evaluating Zero-Shot Generalization. Each task in VIMA-Bench has a binary success criterion and does not provide partial reward. During test time, we execute agent policies in the simulator for multiple episodes to compute a percentage success rate. The average success rate over all evaluated tasks will be the final reported metric.</p>
<p>We design a four-level evaluation protocol (Fig. 2) to systematically probe the generalization capabilities of learned agents. Each level deviates more from the training distribution, and is thus strictly harder than the previous one.</p>
<ol>
<li>Placement generalization. All prompts are seen verbatim during training, but only the placement of objects on the tabletop is randomized at testing;</li>
<li>Combinatorial generalization. All textures and objects are seen during training, but new combinations of them appear in testing;</li>
<li>Novel object generalization. Test prompts and the simulated workspace include novel textures and objects;</li>
<li>Novel task generalization. New tasks with novel prompt templates at test time.</li>
</ol>
<h1>4. VIMA: Visuomotor Attention Agent</h1>
<p>Our goal is to build a robot agent capable of performing any task specified by multimodal prompts. There is no prior method that works out of the box with multimodal prompts. To learn an effective multi-task robot policy, we propose VIMA, a robot agent with a multi-task encoderdecoder architecture and object-centric design (Fig. 3). Concretely, we learn a robot policy $\pi\left(a_{t} \mid \mathcal{P}, \mathcal{H}\right)$, where $\mathcal{H}:=\left[o_{1}, a_{1}, o_{2}, a_{2}, \ldots, o_{t}\right]$ denotes the past interaction history, and $o_{t} \in \mathcal{O}, a_{t} \in \mathcal{A}$ are observations and actions at each interaction steps. We encode multimodal prompts via a frozen pre-trained language model and decode robot waypoint commands conditioned on the encoded prompts via cross-attention layers. Unlike prior work (Florence et al., 2019; Sieb et al., 2019; Zhu et al., 2022), VIMA adopts an object-centric representation that computes tokens from bounding box coordinates and cropped RGB patches.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Scaling model and data. Top: We compare performance of different methods with model sizes ranging from 2M to 200M parameters. Across all model sizes and generalization levels, VIMA outperforms baseline variants. Bottom: For a fixed model size of 92M parameters we compare the effect of imitation learning dataset size with 0.1%, 1%, 10%, and full data. VIMA is extremely sample efficient and can achieve performance comparable to other methods with 10× less data.</p>
<p><strong>Tokenization.</strong> There are 3 formats of raw input in the prompt — text, image of a single object, and image of a full tabletop scene (<em>e.g.</em>, for <em>Rearrangement</em> or imitation from video frames). For <strong>text inputs</strong>, we use pre-trained T5 tokenizer and word embedding to obtain word tokens. For <strong>images of full scenes</strong>, we first extract individual objects using domain fine-tuned Mask R-CNN (He et al., 2017) (Appendix, Sec. C.4). Each object is represented as a bounding box and a cropped image. We then compute object tokens by encoding them with a bounding box encoder and a ViT (Dosovitskiy et al., 2020), respectively. Since Mask R-CNN is imperfect, the bounding boxes can be noisy and the cropped images may have irrelevant pixels. For <strong>images of single objects</strong>, we obtain tokens in the same way except with a dummy bounding box. Prompt tokenization produces a sequence of interleaved textual and visual tokens. We then follow the practice in Tsimpoukelli et al. (2021) and encode the prompt via a pre-trained T5 encoder (Raffel et al., 2020). Since T5 has been pre-trained on large text corpora, VIMA inherits the semantic understanding capability and robustness properties. To accommodate tokens from new modalities, we insert MLPs between non-textual tokens and T5.</p>
<p><strong>Robot Controller.</strong> A challenging aspect of designing a multi-task policy is to select a suitable conditioning mechanism. In our schema (Fig. 3), the robot controller (decoder) is conditioned on the prompt sequence $$P$$ by a series of cross-attention layers between $$P$$ and the trajectory history sequence $$H$$. We compute key $$K_P$$ and value $$V_P$$ sequences from the prompt and query $$Q_H$$ from the trajectory history, following the encoder-decoder convention in Raffel et al. (2020). Each cross-attention layer then generates an output sequence $$H' = \text{softmax}\left(\frac{Q_H K_P}{\sqrt{d}}\right) V_P$$, where $$d$$ is the embedding dimension. Residual connections are added to connect higher layers with the input rollout trajectory sequence. The cross-attention design enjoys three advantages: 1) strengthened connection to prompt; 2) intact and deep flow of the original prompt tokens; and 3) better computational efficiency. VIMA decoder consists of $$L$$ alternating cross-attention and self-attention layers. Finally, we follow common practice (Baker et al., 2022) to map predicted action tokens to discretized poses of the robot arm. See Appendix, Sec. C.2 for more details.</p>
<p><strong>Training.</strong> We follow behavioral cloning to train our models by minimizing the negative log-likelihood of predicted actions. Concretely, for a trajectory with $$T$$ steps, we optimize $$\min_{\theta} \sum_{t=1}^{T} -\log \pi_{\theta}(a_t | P, H)$$. The entire training is conducted on an offline dataset with no simulator access. To make VIMA robust to detection inaccuracies and failures, we apply <em>object augmentation</em> by randomly injecting <em>false-positive</em> detection outputs. After training, we select model checkpoints for evaluation based on the aggregated accuracy on a held-out validation set. The evaluation involves interacting with the physics simulator. We follow the</p>
<p>best practices to train Transformer models. See Appendix, Sec. D for comprehensive training hyperparameters.</p>
<h2>5. Experiments</h2>
<p>In this section, we aim to answer three main questions:</p>
<ol>
<li>What is the best recipe for building multi-task transformer-based robot agents with multimodal prompts?</li>
<li>What are the scaling properties of our approach in model capacity and data size?</li>
<li>How do different components, such as visual tokenizers, prompt conditioning, and prompt encoding, affect robot performance?</li>
</ol>
<h3>5.1. Baselines</h3>
<p>Because there is no prior method that works out of the box with our multimodal prompting setup, we make our best effort to select a number of representative transformer-based agent architectures as baselines, and re-interpret them to be compatible with VIMA-BENCH:</p>
<p>Gato (Reed et al., 2022) introduces a decoder-only model that solves tasks from multiple domains where tasks are specified by prompting the model with the observation and action subsequence. For a fair comparison, we provide the same conditioning as VIMA, i.e., our multimodal encoded prompts. Input images are divided into patches and encoded by a ViT model to produce observation tokens. This variant is referred to as "VIMA-Gato".</p>
<p>Flamingo (Alayrac et al., 2022) is a vision-language model that learns to generate textual completion in response to multimodal prompts. It embeds a variable number of prompt images into a fixed number of tokens via Perceiver (Jaegle et al., 2021b), and conditions the language decoder on the encoded prompt by cross-attention. Flamingo does not work with embodied agents out of the box. We adapt it to support decision-making by replacing the output layer with robot action heads. We denote the method as "VIMA-Flamingo".</p>
<p>VIMA-GPT is a decoder-only architecture conditioned on tokenized multimodal prompts. It autoregressively decodes the next actions given instructions and interaction histories. Similar to prior work (Chen et al., 2021; Janner et al., 2021), it encodes an image into a single state token by a ViT encoder and prepends the rollout trajectory with prompt tokens. This baseline does not use cross-attention.</p>
<p>A more detailed comparison between these variants can be found in Appendix, Sec. C.1.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: VIMA incurs much less performance drop than baselines as we evaluate on progressively harder settings.</p>
<h3>5.2. Evaluation Results</h3>
<p>We compare VIMA against the baseline variants on four levels of generalization provided in our benchmark for different model and training dataset sizes. Our empirical results demonstrate that VIMA's choice of object tokens combined with cross-attention conditioning is the most effective recipe among the model designs we consider.</p>
<p>Model Scaling. We train all methods for a spectrum of model capacities from 2 M to 200 M parameters, evenly spaced on the log scale (Fig. 4). The encoder size is kept constant (T5-Base, 111M) for all methods and excluded from the parameter count. Across all levels of zero-shot generalization, we find that VIMA strongly outperforms other alternatives. Although models like VIMA-Gato and VIMA-Flamingo show improved performance with bigger model sizes, VIMA consistently achieves superior performance over all model sizes. We note that this can only be achieved with both cross-attention and object token sequence representations - altering any component will significantly degrade the performance, especially in the low model capacity regime (ablations in Sec. 5.3).</p>
<p>Data Scaling. Next we investigate how different methods scale with varying dataset sizes. We compare model performance at $0.1 \%, 1 \%, 10 \%$ and full imitation learning dataset provided in VIMA-BENCH (Fig. 4). Note that to ensure all methods are fairly pre-trained on the same amount of data, we initialize baseline variants that directly learn from raw pixels with MVP pre-trained ViT (Xiao et al., 2022; Radosavovic et al., 2022). It is further MAE fine-tuned (He et al., 2021), using the same in-domain data as for the Mask R-CNN object detector. See Appendix, Sec. E. 3 for detailed setup. VIMA is extremely sample efficient and, with just $1 \%$ of the data, can achieve performance similar to baseline methods trained with $10 \times$ more data on L1 and L2 levels of generalization. In fact, for L4 we find that with</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Ablation on visual tokenizers. We compare the performance of VIMA-200M model across different visual tokenizers. Our proposed object tokens outperform all methods that learn directly from raw pixels, and Object Perceiver that downsamples the object sequence to a fixed number of tokens.
just $1 \%$ of training data, VIMA already surpasses other variants trained with entire dataset. Finally, across all levels with just $10 \%$ of the data, VIMA can outperform other architectures trained with the full dataset by a significant margin. We hypothesize that the data efficiency can be attributed to the object-centric representation employed in the VIMA recipe, which is less prone to overfitting than learning directly from pixels in the low-data regime. This is consistent with findings from Sax et al. (2018), which demonstrates that embodied agents conditioned on midlevel visual representations tend to be significantly more sample-efficient than end-to-end control from raw pixels.</p>
<p>Progressive Generalization. Finally, we compare the relative performance degradation as we test the models on progressively challenging zero-shot evaluation levels without further fine-tuning (Fig. 5). Our method exhibits a minimal performance regression, especially between $L 1 \rightarrow L 2$ and $L 1 \rightarrow L 3$. In contrast, the baselines can degrade as much as $20 \%$, particularly in more difficult generalization scenarios. Although all methods degrade significantly when evaluated on $L 4$ (Novel Tasks), the performance drop for VIMA is only half as severe as all other baselines. These results suggest that VIMA has developed a more generalizable policy and robust representations than the alternative approaches.</p>
<h3>5.3. Ablation Studies</h3>
<p>Through extensive experiments, we ablate different design choices in VIMA and study their impact on robot decision making. We focus on four aspects: visual tokenization, prompt conditioning, prompt-encoding language models, and policy robustness against distractions and corruptions.</p>
<p>Visual Tokenization. As explained in Sec. 4, VIMA processes the prompt and observation images into a variable number of object tokens with a domain fine-tuned Mask R-CNN implementation. How important is this particular choice of visual tokenizer? We study 5 different variants and empirically evaluate their 4 levels of generalization performance on VIMA-BENCH. 1) Ours (Oracle): instead of using Mask R-CNN, we directly read out the ground-truth bounding box from the simulator. In other words, we use a perfect object detector to estimate the upper bound on the performance of this study; 2) Object Perceiver: we apply a Perceiver module to convert the variable number of objects detected in each frame to a fixed number of tokens. Perceiver is more computationally efficient because it reduces the average sequence length; 3) Image Perceiver: the same architecture as the Perceiver Resampler in VIMA-Flamingo, which converts an image to a small, fixed number of tokens; 4) Image patches: following VIMA-Gato, we divide an RGB frame into square patches, and extract ViT embedding</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Ablation on prompt conditioning. We compare our method (xattn: cross-attention prompt conditioning) with a vanilla transformer decoder (gpt-decoder) across different model sizes. Cross-attention is especially helpful in low-parameter regime and for harder generalization tasks.</p>
<p>tokens. The number of patches is greater than the output of Image Perceiver; 5) Single image: VIMA-GPT's tokenizer, which encodes one image into a single token.</p>
<p>Fig. 6 shows the ablation results. We highlight a few findings. First, we note that our Mask R-CNN detection pipeline (Appendix, Sec. C.4) incurs a minimal performance loss compared to the oracle bounding boxes, thanks to the object augmentation (Sec. 4) that boosts robustness during training. Second, tokenizing from raw pixels (Image Perceiver, patches, or single embedding) consistently underperforms our object-centric format. We hypothesize that these tokenizers have to allocate extra internal capacity to parse the objects from low-level pixels, which likely impedes learning. Sax et al. (2018) echoes our finding that using mid-level vision can greatly improve agent generalization compared to an end-to-end pipeline. Third, even though Ours and Object Perceiver both use the same object bounding box inputs, the latter is significantly worse in decision making. We conclude that it is important to directly pass the variable-length object sequence to the robot controller rather than downsampling to a fixed number of tokens.</p>
<p><strong>Prompt Conditioning.</strong> VIMA conditions the robot controller (decoder) on the encoded prompt by cross-attention. A simple alternative is to concatenate the prompt $\mathcal{P}$ and interaction history $\mathcal{H}$ into one big sequence, and then apply a decoder-only transformer like GPT (Radford et al., 2018) to predict actions. In this ablation, we keep the object tokenizer constant and only switch the conditioning mechanism to causal sequence modeling. Note that this variant is conceptually "VIMA-Gato with object tokens". Fig. 7 shows the comparison of VIMA (xattn) and the gpt-decoder variant across 4 generalization levels. While the variant achieves comparable performance in larger models, cross-attention still dominates in the small-capacity range and generalizes better in the most challenging L4 (Novel Task) setting. Our hypothesis is that cross-attention helps the controller stay better focused on the prompt instruction at each interaction step. This bears a resemblance to the empirical results in Sanh et al. (2021); Wang et al. (2022b), which show that well-tuned encoder-decoder architectures can outperform GPT-3 in zero-shot generalization.</p>
<p><strong>Prompt Encoding.</strong> We vary the size of the pre-trained T5 encoder to study the effect of prompt encoding. We experiment with three T5 capacities: small (30M), base (111M), and large (368M). We further fix the parameter count of the decision-making part to be 200M. For all T5 variants, we fine-tune the last two layers and freeze all other layers. We find no significant difference among the variants (Appendix, Sec. E.4), thus we set base as default for all our models.</p>
<p><strong>Policy Robustness.</strong> We study the policy robustness against increasing number of distractors and corrupted task specifications, including incomplete prompts (randomly masking out words with <UNK> token) and corrupted prompts (randomly swapping words, which could have changed the task meaning altogether). See Appendix, Sec. E.5 for exact setup and results. VIMA exhibits minimal performance degradation with increased distractors and minor decrease with corrupted prompts. We attribute this robustness to the high-quality pre-trained T5 backbone.</p>
<h1>6. Related Work</h1>
<p><strong>Multi-Task Learning by Sequence Modeling.</strong> Transformers (Vaswani et al., 2017) have enabled task unification across many AI domains (Brown et al., 2020; Chen et al., 2022a;b; Lu et al., 2022; Wang et al., 2022c). For example, in NLP, the Natural Language Decathlon (McCann et al., 2018) adopts a consistent question-answering format for a suite of 10 NLP tasks. T5 (Raffel et al., 2020) unifies all language problems into the same text-to-text format. GPT-3 (Brown et al., 2020) and Megatron (Shoeybi et al., 2019) demonstrate emergent behaviours of intuitive task specifications by zero-shot prompting. In computer vision, Pix2Seq (Chen et al., 2022b) casts many vision problems into a unified sequence format. Florence (Yuan et al., 2021),</p>
<p>BiT (Kolesnikov et al., 2020), and MuST (Ghiasi et al., 2021) pre-train shared backbone models at scale for general visual representations and transfer them to downstream tasks. In multimodal learning, Perceiver (Jaegle et al., 2021b;a) proposes an efficient architecture to handle structured inputs and outputs. Flamingo (Alayrac et al., 2022) and Frozen (Tsimpoukelli et al., 2021) design a universal API that ingests interleaving sequences of images and text and generates free-form text. Gato (Reed et al., 2022) is a massively multi-task model across NLP, vision, and embodied agents. Our work is most similar in spirit to Gato, but we focus primarily on enabling an intuitive multimodal prompting interface for a generalist robot agent.</p>
<p>Foundation Models for Embodied Agents. Foundation models (Bommasani et al., 2021) have demonstrated strong emergent properties. There are many ongoing efforts to replicate this success for embodied agents (Yang et al., 2023), focusing on 3 aspects. 1) Transformer agent architecture: Decision Transformer and Trajectory Transformer (Chen et al., 2021; Janner et al., 2021; Zheng et al., 2022; Xu et al., 2022; 2023) leverage the powerful self-attention models for sequential decision making. CLIPort (Shridhar et al., 2021), Perceiver-Actor (Shridhar et al., 2022), and RT-1 (Brohan et al., 2022) apply large transformers to robot manipulation tasks. BeT (Shafiullah et al., 2022) and C-BeT (Cui et al., 2022) design novel techniques to learn from demonstrations with multiple modes with transformers. 2) Pre-training for better representations: MaskViT (Gupta et al., 2022b), R3M (Nair et al., 2022), VIP (Ma et al., 2022), and VC-1 (Majumdar et al., 2023) pre-train general visual representations for robotic perception. Li et al. (2022b) fine-tunes from LLM checkpoints to accelerate policy learning. MineDojo (Fan et al., 2022) and Ego4D (Grauman et al., 2021) provide large-scale multimodal databases to facilitate scalable policy training. 3) LLMs for robot learning: SayCan (Ahn et al., 2022) leverages PaLM (Chowdhery et al., 2022) for zero-shot concept grounding. Huang et al. (2022a), Inner Monologue (Huang et al., 2022b) and LM-Nav (Shah et al., 2022) apply LLMs to long-horizon robot planning. PaLM-E (Driess et al., 2023) is instead a multimodal language model that can be repurposed for sequential robotic manipulation planning. Ours differs from these works in our novel multimodal prompting formulation, which existing LLMs do not easily support.</p>
<p>Robot Manipulation and Benchmarks. A wide range of robot manipulation tasks require different skills and task specification formats, such as instruction following (Stepputtis et al., 2020), one-shot imitation (Finn et al., 2017; Duan et al., 2017), rearrangement (Batra et al., 2020), constraint satisfaction (Brunke et al., 2021a), and reasoning (Shridhar et al., 2020). Multiple physics simulation benchmarks are introduced to study the above
tasks. For example, iGibson (Shen et al., 2020; Li et al., 2021; Srivastava et al., 2021; Li et al., 2022a) simulates interactive household scenarios. Ravens (Zeng et al., 2020) and Robosuite (Zhu et al., 2020; Fan et al., 2021) design various tabletop manipulation tasks with realistic robot arms. CALVIN (Mees et al., 2021) develops long-horizon language-conditioned tasks. Meta-World (Yu et al., 2019) is a widely used simulator benchmark studying robotics manipulation with tabletop settings. CausalWorld (Ahmed et al., 2021) is a benchmark for causal structure and transfer learning in manipulation, requiring long-horizon planning and precise low-level motor control. AI2-THOR (Ehsani et al., 2021; Deitke et al., 2022) is a framework that supports visual object manipulation and procedural generation of environments. Our VIMA-BENCH is the first robot learning benchmark to support multimodal-prompted tasks. We also standardize the evaluation protocol to systematically measure an agent's generalization capabilities.</p>
<h2>An extended review can be found in Appendix, Sec. F.</h2>
<h2>7. Conclusion</h2>
<p>In this work, we introduce a novel multimodal prompting formulation that converts diverse robot manipulation tasks into a uniform sequence modeling problem. We instantiate this formulation in VIMA-BENCH, a diverse benchmark with multimodal tasks and systematic evaluation protocols for generalization. We propose VIMA, a conceptually simple transformer-based agent capable of solving tasks such as visual goal reaching, one-shot video imitation, and novel concept grounding with a single model. Through comprehensive experiments, we show that VIMA exhibits strong model scalability and zero-shot generalization. Therefore, we recommend our agent design as a solid starting point for future work.</p>
<h2>Acknowledgement</h2>
<p>We are extremely grateful to Shyamal Buch, Jonathan Tremblay, Ajay Mandlekar, Chris Choy, De-An Huang, Silvio Savarese, Fei Xia, Josiah Wong, Abhishek Joshi, Soroush Nasiriany, and many other colleagues and friends for their helpful feedback and insightful discussions. We also thank the anonymous reviewers for offering us highly constructive advice and kind encouragement during the review period. NVIDIA provides the necessary computing resource and infrastructure for this project. This work is done during Yunfan Jiang and Guanzhi Wang's internships at NVIDIA. Guanzhi Wang is supported by the Kortschak fellowship in Computing and Mathematical Sciences at Caltech.</p>
<h2>References</h2>
<p>Abramson, J., Ahuja, A., Barr, I., Brussee, A., Carnevale, F., Cassin, M., Chhaparia, R., Clark, S., Damoc, B., Dudzik, A., Georgiev, P., Guy, A., Harley, T., Hill, F., Hung, A., Kenton, Z., Landon, J., Lillicrap, T., Mathewson, K., Mokrá, S., Muldal, A., Santoro, A., Savinov, N., Varma, V., Wayne, G., Williams, D., Wong, N., Yan, C., and Zhu, R. Imitating interactive intelligence. arXiv preprint arXiv: Arxiv-2012.05672, 2020.</p>
<p>Aceituno, B., Rodriguez, A., Tulsiani, S., Gupta, A., and Mukadam, M. A differentiable recipe for learning visual non-prehensile planar manipulation. In 5th Annual Conference on Robot Learning, 2021. URL https: //openreview.net/forum?id=f7KaqYLO3iE.</p>
<p>Agrawal, P. The task specification problem. In Faust, A., Hsu, D., and Neumann, G. (eds.), Proceedings of the 5th Conference on Robot Learning, volume 164 of Proceedings of Machine Learning Research, pp. 1745-1751. PMLR, 08-11 Nov 2022. URL https://proceedings.mlr.press/ v164/agrawal22a.html.</p>
<p>Ahmed, O., Träuble, F., Goyal, A., Neitz, A., Wuthrich, M., Bengio, Y., Schölkopf, B., and Bauer, S. Causalworld: A robotic manipulation benchmark for causal structure and transfer learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum? id=SK7A5pdrgov.</p>
<p>Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., and Yan, M. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv: Arxiv-2204.01691, 2022.</p>
<p>Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., and Simonyan, K. Flamingo: a visual language model for fewshot learning. arXiv preprint arXiv: Arxiv-2204.14198, 2022.</p>
<p>Baker, B., Akkaya, I., Zhokhov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton, B., Sampedro, R., and Clune, J. Video pretraining (vpt): Learning to act by watching unlabeled online videos. arXiv preprint arXiv: Arxiv2206.11795, 2022.</p>
<p>Batra, D., Chang, A. X., Chernova, S., Davison, A. J., Deng, J., Koltun, V., Levine, S., Malik, J., Mordatch, I., Mottaghi, R., Savva, M., and Su, H. Rearrangement: A challenge for embodied ai. arXiv preprint arXiv: Arxiv2011.01975, 2020.</p>
<p>Berscheid, L., Meißner, P., and Kröger, T. Self-supervised learning for precise pick-and-place without object model. arXiv preprint arXiv: Arxiv-2006.08373, 2020.</p>
<p>Bharadhwaj, H., Kumar, A., Rhinehart, N., Levine, S., Shkurti, F., and Garg, A. Conservative safety critics for exploration. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https: //openreview.net/forum?id=ia086DUuKi.</p>
<p>Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, X. L., Li, X., Ma, T., Malik, A., Manning, C. D., Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A., Niebles, J. C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, I., Park, J. S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., Ré, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A. W., Tramèr, F., Wang, R. E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S. M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y., Zheng, L., Zhou, K., and Liang, P. On the opportunities and risks of foundation models. arXiv preprint arXiv: Arxiv-2108.07258, 2021.</p>
<p>Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jackson, T., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Leal, I., Lee, K.-H., Levine, S., Lu, Y., Malla,</p>
<p>U., Manjunath, D., Mordatch, I., Nachum, O., Parada, C., Peralta, J., Perez, E., Pertsch, K., Quiambao, J., Rao, K., Ryoo, M., Salazar, G., Sanketi, P., Sayed, K., Singh, J., Sontakke, S., Stone, A., Tan, C., Tran, H., Vanhoucke, V., Vega, S., Vuong, Q., Xia, F., Xiao, T., Xu, P., Xu, S., Yu, T., and Zitkovich, B. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv: Arxiv2212.06817, 2022.</p>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, volume 33, pp. 18771901, 2020. URL https://proceedings. neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract. html.</p>
<p>Brunke, L., Greeff, M., Hall, A. W., Yuan, Z., Zhou, S., Panerati, J., and Schoellig, A. P. Safe learning in robotics: From learning-based control to safe reinforcement learning. arXiv preprint arXiv: Arxiv-2108.06266, 2021a.</p>
<p>Brunke, L., Greeff, M., Hall, A. W., Yuan, Z., Zhou, S., Panerati, J., and Schoellig, A. P. Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning, December 2021b. URL http:// arxiv.org/abs/2108.06266. arXiv:2108.06266 [cs, eess].</p>
<p>Buch, S., Eyzaguirre, C., Gaidon, A., Wu, J., Fei-Fei, L., and Niebles, J. C. Revisiting the "video" in video-language understanding. CVPR, 2022.</p>
<p>Bucker, A., Figueredo, L., Haddadin, S., Kapoor, A., Ma, S., Vemprala, S., and Bonatti, R. Latte: Language trajectory transformer. arXiv preprint arXiv: Arxiv-2208.02918, 2022.</p>
<p>Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. Decision transformer: Reinforcement learning via sequence modeling. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 1508415097, 2021. URL https://proceedings.
neurips.cc/paper/2021/hash/
7f489f642a0ddb10272b5c31057f0663-Abstract. html.</p>
<p>Chen, T., Saxena, S., Li, L., Fleet, D. J., and Hinton, G. E. Pix2seq: A language modeling framework for object detection. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022a. URL https: //openreview.net/forum?id=e42KbIw6Wb.</p>
<p>Chen, T., Saxena, S., Li, L., Lin, T.-Y., Fleet, D. J., and Hinton, G. A unified sequence interface for vision tasks. arXiv preprint arXiv: Arxiv-2206.07669, 2022b.</p>
<p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with pathways. arXiv preprint arXiv: Arxiv-2204.02311, 2022.</p>
<p>Collins, J., Chand, S., Vanderkop, A., and Howard, D. A review of physics simulators for robotic applications. IEEE Access, 9:51416-51431, 2021.</p>
<p>Coumans, E. and Bai, Y. Pybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2016-2021.</p>
<p>Cui, Z. J., Wang, Y., Shafiullah, N. M. M., and Pinto, L. From play to policy: Conditional behavior generation from uncurated robot data. arXiv preprint arXiv: Arxiv2210.10047, 2022.</p>
<p>Dasari, S. and Gupta, A. Transformers for one-shot visual imitation. In Kober, J., Ramos, F., and Tomlin, C. J. (eds.), 4th Conference on Robot Learning, CoRL 2020, 16-18 November 2020, Virtual Event / Cambridge, MA, USA, volume 155 of Proceedings of Machine Learning Research, pp. 2071-2084. PMLR, 2020. URL https://proceedings.mlr.press/ v155/dasari21a.html.</p>
<p>Dasari, S., Ebert, F., Tian, S., Nair, S., Bucher, B., Schmeckpeper, K., Singh, S., Levine, S., and Finn, C. Robonet:</p>
<p>Large-scale multi-robot learning. In Kaelbling, L. P., Kragic, D., and Sugiura, K. (eds.), 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings, volume 100 of Proceedings of Machine Learning Research, pp. 885-897. PMLR, 2019. URL http://proceedings.mlr. press/v100/dasari20a.html.</p>
<p>Deitke, M., VanderBilt, E., Herrasti, A., Weihs, L., Salvador, J., Ehsani, K., Han, W., Kolve, E., Farhadi, A., Kembhavi, A., and Mottaghi, R. Procthor: Large-scale embodied ai using procedural generation. arXiv preprint arXiv: Arxiv-2206.06994, 2022.</p>
<p>Devin, C., Rowghanian, P., Vigorito, C., Richards, W., and Rohanimanesh, K. Self-supervised goal-conditioned pick and place. arXiv preprint arXiv: Arxiv-2008.11466, 2020.</p>
<p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: Arxiv2010.11929, 2020.</p>
<p>Downs, L., Francis, A., Koenig, N., Kinman, B., Hickman, R., Reymann, K., McHugh, T. B., and Vanhoucke, V. Google scanned objects: A high-quality dataset of 3d scanned household items. arXiv preprint arXiv:2204.11918, 2022.</p>
<p>Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., and Florence, P. Palm-e: An embodied multimodal language model. arXiv preprint arXiv: Arxiv-2303.03378, 2023.</p>
<p>Duan, J., Yu, S., Tan, H. L., Zhu, H., and Tan, C. A survey of embodied AI: from simulators to research tasks. IEEE Trans. Emerg. Top. Comput. Intell., 6(2):230-244, 2022. doi: 10.1109/TETCI.2022.3141105. URL https:// doi.org/10.1109/TETCI.2022.3141105.</p>
<p>Duan, Y., Andrychowicz, M., Stadie, B. C., Ho, J., Schneider, J., Sutskever, I., Abbeel, P., and Zaremba, W. One-shot imitation learning. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 1087-1098, 2017. URL https://proceedings. neurips.cc/paper/2017/hash/ ba3866600c3540f67c1e9575e213be0a-Abstract . html.</p>
<p>Ehsani, K., Han, W., Herrasti, A., VanderBilt, E., Weihs, L., Kolve, E., Kembhavi, A., and Mottaghi, R. Manipulathor: A framework for visual object manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4497-4506, June 2021.</p>
<p>Fan, L., Zhu, Y., Zhu, J., Liu, Z., Zeng, O., Gupta, A., CreusCosta, J., Savarese, S., and Fei-Fei, L. Surreal: Opensource reinforcement learning framework and robot manipulation benchmark. In Billard, A., Dragan, A., Peters, J., and Morimoto, J. (eds.), Proceedings of The 2nd Conference on Robot Learning, volume 87 of Proceedings of Machine Learning Research, pp. 767-782. PMLR, 2931 Oct 2018. URL https://proceedings.mlr. press/v87/fan18a.html.</p>
<p>Fan, L., Zhu, Y., Zhu, J., Liu, Z., Zeng, O., Gupta, A., CreusCosta, J., Savarese, S., and Fei-Fei, L. Surreal-system: Fully-integrated stack for distributed deep reinforcement learning. arXiv preprint arXiv: Arxiv-1909.12989, 2019.</p>
<p>Fan, L., Wang, G., Huang, D., Yu, Z., Fei-Fei, L., Zhu, Y., and Anandkumar, A. SECANT: self-expert cloning for zero-shot generalization of visual policies. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 1824 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 3088-3099. PMLR, 2021. URL http://proceedings.mlr.press/ v139/fan21c.html.</p>
<p>Fan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A., Huang, D.-A., Zhu, Y., and Anandkumar, A. Minedojo: Building open-ended embodied agents with internet-scale knowledge. arXiv preprint arXiv: Arxiv2206.08853, 2022.</p>
<p>Finn, C., Yu, T., Zhang, T., Abbeel, P., and Levine, S. Oneshot visual imitation learning via meta-learning. arXiv preprint arXiv: Arxiv-1709.04905, 2017.</p>
<p>Florence, P., Manuelli, L., and Tedrake, R. Self-supervised correspondence in visuomotor policy learning. arXiv preprint arXiv: Arxiv-1909.06933, 2019.</p>
<p>Fu, T.-J., Li, L., Gan, Z., Lin, K., Wang, W. Y., Wang, L., and Liu, Z. Violet : End-to-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv: Arxiv-2111.12681, 2021.</p>
<p>Gan, C., Zhou, S., Schwartz, J., Alter, S., Bhandwaldar, A., Gutfreund, D., Yamins, D. L. K., DiCarlo, J. J., McDermott, J., Torralba, A., and Tenenbaum, J. B. The threadworld transport challenge: A visually guided task-and-motion planning benchmark for physically realistic embodied ai. arXiv preprint arXiv: Arxiv-2103.14025, 2021.</p>
<p>Ghiasi, G., Zoph, B., Cubuk, E. D., Le, Q. V., and Lin, T. Multi-task self-training for learning general representations. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pp. 8836-8845. IEEE, 2021. doi: 10.1109/ICCV48922.2021.00873. URL https://doi. org/10.1109/ICCV48922.2021.00873.</p>
<p>Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., Martin, M., Nagarajan, T., Radosavovic, I., Ramakrishnan, S. K., Ryan, F., Sharma, J., Wray, M., Xu, M., Xu, E. Z., Zhao, C., Bansal, S., Batra, D., Cartillier, V., Crane, S., Do, T., Doulaty, M., Erapalli, A., Feichtenhofer, C., Fragomeni, A., Fu, Q., Gebreselasie, A., Gonzalez, C., Hillis, J., Huang, X., Huang, Y., Jia, W., Khoo, W., Kolar, J., Kottur, S., Kumar, A., Landini, F., Li, C., Li, Y., Li, Z., Mangalam, K., Modhugu, R., Munro, J., Murrell, T., Nishiyasu, T., Price, W., Puentes, P. R., Ramazanova, M., Sari, L., Somasundaram, K., Southerland, A., Sugano, Y., Tao, R., Vo, M., Wang, Y., Wu, X., Yagi, T., Zhao, Z., Zhu, Y., Arbelaez, P., Crandall, D., Damen, D., Farinella, G. M., Fuegen, C., Ghanem, B., Ithapu, V. K., Jawahar, C. V., Joo, H., Kitani, K., Li, H., Newcombe, R., Oliva, A., Park, H. S., Rehg, J. M., Sato, Y., Shi, J., Shou, M. Z., Torralba, A., Torresani, L., Yan, M., and Malik, J. Ego4d: Around the world in 3,000 hours of egocentric video. arXiv preprint arXiv: Arxiv-2110.07058, 2021.</p>
<p>Gupta, A., Kumar, V., Lynch, C., Levine, S., and Hausman, K. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. In Kaelbling, L. P., Kragic, D., and Sugiura, K. (eds.), 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings, volume 100 of Proceedings of Machine Learning Research, pp. 10251037. PMLR, 2019. URL http://proceedings. mlr.press/v100/gupta20a.html.</p>
<p>Gupta, A., Fan, L., Ganguli, S., and Fei-Fei, L. Metamorph: Learning universal controllers with transformers. In International Conference on Learning Representations, 2022a. URL https://openreview.net/forum? id=Opmqtk_GvYL.</p>
<p>Gupta, A., Tian, S., Zhang, Y., Wu, J., Martín-Martín, R., and Fei-Fei, L. Maskvit: Masked visual pre-training for video prediction. arXiv preprint arXiv: Arxiv2206.11894, 2022b.</p>
<p>Hansen, N., Yuan, Z., Ze, Y., Mu, T., Rajeswaran, A., Su, H., Xu, H., and Wang, X. On pre-training for visuomotor control: Revisiting a learning-from-scratch baseline. arXiv preprint arXiv: Arxiv-2212.05749, 2022.</p>
<p>He, K., Gkioxari, G., Dollár, P., and Girshick, R. Mask r-cnn. arXiv preprint arXiv: Arxiv-1703.06870, 2017.</p>
<p>He, K., Chen, X., Xie, S., Li, Y., Dollár, P., and Girshick, R. Masked autoencoders are scalable vision learners. arXiv preprint arXiv: Arxiv-2111.06377, 2021.</p>
<p>Heibeck, T. H. and Markman, E. M. Word learning in children: An examination of fast mapping. Child development, pp. 1021-1034, 1987.</p>
<p>Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R., Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg, M., Teplyashin, D., Wainwright, M., Apps, C., Hassabis, D., and Blunsom, P. Grounded language learning in a simulated 3d world. arXiv preprint arXiv: Arxiv-1706.06551, 2017.</p>
<p>Hill, F., Tieleman, O., von Glehn, T., Wong, N., Merzic, H., and Clark, S. Grounded language learning fast and slow. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https: //openreview.net/forum?id=wpSWuz_hyqA.</p>
<p>Huang, D., Xu, D., Zhu, Y., Garg, A., Savarese, S., Fei-Fei, L., and Niebles, J. C. Continuous relaxation of symbolic planner for one-shot imitation learning. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2019, Macau, SAR, China, November 3-8, 2019, pp. 2635-2642. IEEE, 2019. doi: 10. 1109/IROS40897.2019.8967761. URL https://doi . org/10.1109/IROS40897.2019.8967761.</p>
<p>Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvári, C., Niu, G., and Sabato, S. (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 9118-9147. PMLR, 2022a. URL https://proceedings.mlr . press/v162/huang22a.html.</p>
<p>Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Brown, N., Jackson, T., Luu, L., Levine, S., Hausman, K., and Ichter, B. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv: Arxiv-2207.05608, 2022b.</p>
<p>Jaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C., Ionescu, C., Ding, D., Koppula, S., Zoran, D., Brock, A., Shelhamer, E., Hénaff, O., Botvinick, M. M., Zisserman, A., Vinyals, O., and Carreira, J. Perceiver io: A general architecture for structured inputs \&amp; outputs. arXiv preprint arXiv: Arxiv-2107.14795, 2021a.</p>
<p>Jaegle, A., Gimeno, F., Brock, A., Zisserman, A., Vinyals, O., and Carreira, J. Perceiver: General perception with iterative attention. arXiv preprint arXiv: Arxiv-2103.03206, 2021b.</p>
<p>James, S., Ma, Z., Arrojo, D. R., and Davison, A. J. Rlbench: The robot learning benchmark \&amp; learning environment. arXiv preprint arXiv: Arxiv-1909.12271, 2019.</p>
<p>Janner, M., Li, Q., and Levine, S. Offline reinforcement learning as one big sequence modeling problem. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 12731286, 2021. URL https://proceedings. neurips.cc/paper/2021/hash/ 099fe6b0b444c23836c4a5d07346082b-Abstract html.</p>
<p>Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., and Shah, M. Transformers in vision: A survey. arXiv preprint arXiv: Arxiv-2101.01169, 2021.</p>
<p>Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi, A. Simple but effective: Clip embeddings for embodied ai. arXiv preprint arXiv: Arxiv-2111.09888, 2021.</p>
<p>Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., Dollár, P., and Girshick, R. Segment anything. arXiv preprint arXiv: Arxiv-2304.02643, 2023.</p>
<p>Kokkinos, I. Ubernet: Training a 'universal' convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. arXiv preprint arXiv: Arxiv-1609.02132, 2016.</p>
<p>Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and Houlsby, N. Big transfer (bit): General visual representation learning. In Vedaldi, A., Bischof, H., Brox, T., and Frahm, J. (eds.), Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V, volume 12350 of Lecture Notes in Computer Science, pp. 491-507. Springer, 2020. doi: 10.1007/ 978-3-030-58558-7\29. URL https://doi.org/ 10.1007/978-3-030-58558-7_29.</p>
<p>Kolesnikov, A., Pinto, A. S., Beyer, L., Zhai, X., Harmsen, J., and Houlsby, N. Uvim: A unified modeling approach for vision with learned guiding codes. arXiv preprint arXiv: Arxiv-2205.10337, 2022.</p>
<p>Li, C., Xia, F., Martín-Martín, R., Lingelbach, M., Srivastava, S., Shen, B., Vainio, K. E., Gokmen, C., Dharan,
G., Jain, T., Kurenkov, A., Liu, C. K., Gweon, H., Wu, J., Fei-Fei, L., and Savarese, S. igibson 2.0: Object-centric simulation for robot learning of everyday household tasks. In Faust, A., Hsu, D., and Neumann, G. (eds.), Conference on Robot Learning, 8-11 November 2021, London, UK, volume 164 of Proceedings of Machine Learning Research, pp. 455-465. PMLR, 2021. URL https:// proceedings.mlr.press/v164/li22b.html.</p>
<p>Li, C., Zhang, R., Wong, J., Gokmen, C., Srivastava, S., Martín-Martín, R., Wang, C., Levine, G., Lingelbach, M., Sun, J., Anvari, M., Hwang, M., Sharma, M., Aydin, A., Bansal, D., Hunter, S., Kim, K.-Y., Lou, A., Matthews, C. R., Villa-Renteria, I., Tang, J. H., Tang, C., Xia, F., Savarese, S., Gweon, H., Liu, K., Wu, J., and Fei-Fei, L. BEHAVIOR-1k: A benchmark for embodied AI with 1,000 everyday activities and realistic simulation. In 6th Annual Conference on Robot Learning, 2022a. URL https://openreview.net/forum? id=_8DoIe8G3t.</p>
<p>Li, S., Puig, X., Paxton, C., Du, Y., Wang, C., Fan, L., Chen, T., Huang, D.-A., Akyürek, E., Anandkumar, A., Andreas, J., Mordatch, I., Torralba, A., and Zhu, Y. Pre-trained language models for interactive decision-making. arXiv preprint arXiv: Arxiv-2202.01771, 2022b.</p>
<p>Lim, M. H., Zeng, A., Ichter, B., Bandari, M., Coumans, E., Tomlin, C., Schaal, S., and Faust, A. Multi-task learning with sequence-conditioned transporter networks. arXiv preprint arXiv: Arxiv-2109.07578, 2021.</p>
<p>Lin, T., Wang, Y., Liu, X., and Qiu, X. A survey of transformers. arXiv preprint arXiv: Arxiv-2106.04554, 2021.</p>
<p>Liu, F., Liu, H., Grover, A., and Abbeel, P. Masked autoencoding for scalable and generalizable decision making. arXiv preprint arXiv: Arxiv-2211.12740, 2022a.</p>
<p>Liu, H., Lee, L., Lee, K., and Abbeel, P. Instructrl: Instruction-following agents with jointly pre-trained vision-language models. arXiv preprint arXiv: Arxiv2210.13431, 2022b.</p>
<p>Liu, Z., Liu, W., Qin, Y., Xiang, F., Gou, M., Xin, S., Roa, M. A., Calli, B., Su, H., Sun, Y., et al. Ocrtoc: A cloudbased competition and benchmark for robotic grasping and manipulation. IEEE Robotics and Automation Letters, 7(1):486-493, 2021.</p>
<p>Loshchilov, I. and Hutter, F. SGDR: stochastic gradient descent with warm restarts. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https: //openreview.net/forum?id=Skq89Scxx.</p>
<p>Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https: //openreview.net/forum?id=Bkg6RiCqY7.</p>
<p>Lu, J., Goswami, V., Rohrbach, M., Parikh, D., and Lee, S. 12-in-1: Multi-task vision and language representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 1043410443. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.01045. URL https: //openaccess.thecvf.com/content_CVPR_ 2020/html/Lu_12-in-1_Multi-Task_ Vision_and_Language_Representation_ Learning_CVPR_2020_paper.html.</p>
<p>Lu, J., Clark, C., Zellers, R., Mottaghi, R., and Kembhavi, A. Unified-io: A unified model for vision, language, and multi-modal tasks. arXiv preprint arXiv: Arxiv2206.08916, 2022.</p>
<p>Lynch, C. and Sermanet, P. Language conditioned imitation learning over unstructured data. In Shell, D. A., Toussaint, M., and Hsieh, M. A. (eds.), Robotics: Science and Systems XVII, Virtual Event, July 12-16, 2021, 2021. doi: 10.15607/RSS.2021.XVII.047. URL https: //doi.org/10.15607/RSS.2021.XVII.047.</p>
<p>Ma, Y. J., Sodhani, S., Jayaraman, D., Bastani, O., Kumar, V., and Zhang, A. Vip: Towards universal visual reward and representation via value-implicit pre-training. arXiv preprint arXiv: Arxiv-2210.00030, 2022.</p>
<p>Majumdar, A., Yadav, K., Arnaud, S., Ma, Y. J., Chen, C., Silwal, S., Jain, A., Berges, V.-P., Abbeel, P., Malik, J., Batra, D., Lin, Y., Maksymets, O., Rajeswaran, A., and Meier, F. Where are we in the search for an artificial visual cortex for embodied intelligence? arXiv preprint arXiv: Arxiv-2303.18240, 2023.</p>
<p>Mandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang, C., Kulkarni, R., Fei-Fei, L., Savarese, S., Zhu, Y., and Martín-Martín, R. What matters in learning from offline human demonstrations for robot manipulation. arXiv preprint arXiv: Arxiv-2108.03298, 2021.</p>
<p>McCann, B., Keskar, N. S., Xiong, C., and Socher, R. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv: Arxiv-1806.08730, 2018.</p>
<p>Mees, O., Hermann, L., Rosete-Beas, E., and Burgard, W. Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. arXiv preprint arXiv: Arxiv-2112.03227, 2021.</p>
<p>Minderer, M., Gritsenko, A., Stone, A., Neumann, M., Weissenborn, D., Dosovitskiy, A., Mahendran, A., Arnab, A., Dehghani, M., Shen, Z., Wang, X., Zhai, X., Kipf, T., and Houlsby, N. Simple open-vocabulary object detection with vision transformers. arXiv preprint arXiv: Arxiv2205.06230, 2022.</p>
<p>Mittal, M., Yu, C., Yu, Q., Liu, J., Rudin, N., Hoeller, D., Yuan, J. L., Tehrani, P. P., Singh, R., Guo, Y., Mazhar, H., Mandlekar, A., Babich, B., State, G., Hutter, M., and Garg, A. Orbit: A unified simulation framework for interactive robot learning environments. arXiv preprint arXiv: Arxiv-2301.04195, 2023.</p>
<p>Morrical, N., Tremblay, J., Birchfield, S., and Wald, I. NVISII: Nvidia scene imaging interface, 2020. https: //github.com/owl-project/NVISII/.</p>
<p>Nair, S., Rajeswaran, A., Kumar, V., Finn, C., and Gupta, A. R3m: A universal visual representation for robot manipulation. arXiv preprint arXiv: Arxiv-2203.12601, 2022.</p>
<p>OpenAI, Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., Józefowicz, R., Gray, S., Olsson, C., Pachocki, J., Petrov, M., d. O. Pinto, H. P., Raiman, J., Salimans, T., Schlatter, J., Schneider, J., Sidor, S., Sutskever, I., Tang, J., Wolski, F., and Zhang, S. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv: Arxiv-1912.06680, 2019.</p>
<p>Paine, T. L., Colmenarejo, S. G., Wang, Z., Reed, S., Aytar, Y., Pfaff, T., Hoffman, M. W., Barth-Maron, G., Cabi, S., Budden, D., and de Freitas, N. One-shot high-fidelity imitation: Training large-scale deep nets with rl. arXiv preprint arXiv: Arxiv-1810.05017, 2018.</p>
<p>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32, pp. 8024-8035. Curran Associates, Inc., 2019.</p>
<p>Puig, X., Ra, K., Boben, M., Li, J., Wang, T., Fidler, S., and Torralba, A. Virtualhome: Simulating household activities via programs. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 84948502. Computer Vision Foundation / IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.00886. URL http://openaccess.thecvf.com/content_</p>
<p>cvpr_2018/html/Puig_VirtualHome_ Simulating_Household_CVPR_2018_paper. html.</p>
<p>Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pretraining. OpenAI, 2018.</p>
<p>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. PMLR, 2021.</p>
<p>Radosavovic, I., Xiao, T., James, S., Abbeel, P., Malik, J., and Darrell, T. Real-world robot learning with masked visual pre-training. CoRL, 2022.</p>
<p>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67, 2020. URL http://jmlr.org/papers/v21/20-074. html.</p>
<p>Ravichandar, H., Polydoros, A. S., Chernova, S., and Billard, A. Recent advances in robot learning from demonstration. Annual review of control, robotics, and autonomous systems, 3:297-330, 2020.</p>
<p>Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., Eccles, T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, Y., Hadsell, R., Vinyals, O., Bordbar, M., and de Freitas, N. A generalist agent. arXiv preprint arXiv: Arxiv-2205.06175, 2022.</p>
<p>Reid, M., Yamada, Y., and Gu, S. S. Can wikipedia help offline reinforcement learning? arXiv preprint arXiv: Arxiv-2201.12122, 2022.</p>
<p>Ren, S., He, K., Girshick, R., and Sun, J. Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv preprint arXiv: Arxiv-1506.01497, 2015.</p>
<p>Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L. A., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., Dey, M., Bari, M. S., Xu, C., Thakker, U., Sharma, S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N. V., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Févry, T., Fries, J. A., Teehan, R., Biderman, S. R., Gao, L., Bers, T., Wolf, T., and Rush, A. M. Multitask prompted training enables zero-shot task generalization. Iclr, 2021.</p>
<p>Savva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans, E., Jain, B., Straub, J., Liu, J., Koltun, V., Malik, J., Parikh, D., and Batra, D. Habitat: A platform for embodied ai research. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.</p>
<p>Sax, A., Emi, B., Zamir, A. R., Guibas, L., Savarese, S., and Malik, J. Mid-level visual representations improve generalization and sample efficiency for learning visuomotor policies. arXiv preprint arXiv: Arxiv-1812.11971, 2018.</p>
<p>Shafiullah, N. M. M., Cui, Z. J., Altanzaya, A., and Pinto, L. Behavior transformers: Cloning $k$ modes with one stone. arXiv preprint arXiv: Arxiv-2206.11251, 2022.</p>
<p>Shah, D., Osinski, B., Ichter, B., and Levine, S. Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. arXiv preprint arXiv: Arxiv2207.04429, 2022.</p>
<p>Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv: Arxiv-2002.05202, 2020.</p>
<p>Shen, B., Xia, F., Li, C., Martín-Martín, R., Fan, L., Wang, G., Pérez-D'Arpino, C., Buch, S., Srivastava, S., Tchapmi, L. P., Tchapmi, M. E., Vainio, K., Wong, J., Fei-Fei, L., and Savarese, S. igibson 1.0: a simulation environment for interactive tasks in large realistic scenes. arXiv preprint arXiv: Arxiv-2012.02924, 2020.</p>
<p>Shi, T. T., Karpathy, A., Fan, L. J., Hernandez, J., and Liang, P. World of bits: an open-domain platform for web-based agents. ICML, 2017. URL https://dl.acm.org/ doi/10.5555/3305890.3306005.</p>
<p>Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multibillion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.</p>
<p>Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han, W., Mottaghi, R., Zettlemoyer, L., and Fox, D. ALFRED: A benchmark for interpreting grounded instructions for everyday tasks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 10737-10746. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.01075. URL https://openaccess.thecvf.com/ content_CVPR_2020/html/Shridhar_ ALFRED_A_Benchmark_for_Interpreting_ Grounded_Instructions_for_Everyday_ Tasks_CVPR_2020_paper.html.</p>
<p>Shridhar, M., Manuelli, L., and Fox, D. Cliport: What and where pathways for robotic manipulation. arXiv preprint arXiv: Arxiv-2109.12098, 2021.</p>
<p>Shridhar, M., Manuelli, L., and Fox, D. Perceiver-actor: A multi-task transformer for robotic manipulation. arXiv preprint arXiv: Arxiv-2209.05451, 2022.</p>
<p>Sieb, M., Xian, Z., Huang, A., Kroemer, O., and Fragkiadaki, K. Graph-structured visual imitation. arXiv preprint arXiv: Arxiv-1907.05518, 2019.</p>
<p>Song, S., Zeng, A., Lee, J., and Funkhouser, T. Grasping in the wild:learning 6dof closed-loop grasping from low-cost demonstrations. arXiv preprint arXiv: Arxiv1912.04344, 2019.</p>
<p>Srinivasan, K., Eysenbach, B., Ha, S., Tan, J., and Finn, C. Learning to be safe: Deep rl with a safety critic. arXiv preprint arXiv: Arxiv-2010.14603, 2020.</p>
<p>Srivastava, S., Li, C., Lingelbach, M., Martín-Martín, R., Xia, F., Vainio, K. E., Lian, Z., Gokmen, C., Buch, S., Liu, C. K., Savarese, S., Gweon, H., Wu, J., and FeiFei, L. BEHAVIOR: benchmark for everyday household activities in virtual, interactive, and ecological environments. In Faust, A., Hsu, D., and Neumann, G. (eds.), Conference on Robot Learning, 8-11 November 2021, London, UK, volume 164 of Proceedings of Machine Learning Research, pp. 477-490. PMLR, 2021. URL https://proceedings.mlr.press/ v164/srivastava22a.html.</p>
<p>Stengel-Eskin, E., Hundt, A., He, Z., Murali, A., Gopalan, N., Gombolay, M., and Hager, G. Guiding multistep rearrangement tasks with natural language instructions. In Faust, A., Hsu, D., and Neumann, G. (eds.), Proceedings of the 5th Conference on Robot Learning, volume 164 of Proceedings of Machine Learning Research, pp. 1486-1501. PMLR, 08-11 Nov 2022. URL https://proceedings.mlr.press/ v164/stengel-eskin22a.html.</p>
<p>Stepputtis, S., Campbell, J., Phielipp, M., Lee, S., Baral, C., and Amor, H. B. Language-Conditioned Imitation Learning for Robot Manipulation Tasks, October 2020. URL http://arxiv.org/abs/2010. 12083. arXiv:2010.12083 [cs].</p>
<p>Stone, A., Xiao, T., Lu, Y., Gopalakrishnan, K., Lee, K.-H., Vuong, Q., Wohlhart, P., Zitkovich, B., Xia, F., Finn, C., and Hausman, K. Open-world object manipulation using pre-trained vision-language models. arXiv preprint arXiv: Arxiv-2303.00905, 2023.</p>
<p>Szot, A., Clegg, A., Undersander, E., Wijmans, E., Zhao, Y., Turner, J., Maestre, N., Mukadam, M., Chaplot, D. S., Maksymets, O., Gokaslan, A., Vondrus, V., Dharur, S., Meier, F., Galuba, W., Chang, A. X., Kira, Z., Koltun, V., Malik, J., Savva, M., and Batra, D. Habitat 2.0: Training home assistants to rearrange their
habitat. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 251-266, 2021. URL https://proceedings. neurips.cc/paper/2021/hash/ 021bbc7ee20b71134d53e20206bd6feb-Abstract. html.</p>
<p>Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient transformers: A survey. arXiv preprint arXiv: Arxiv2009.06732, 2020.</p>
<p>Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck, C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M., Mathieu, M., McAleese, N., Bradley-Schmieg, N., Wong, N., Porcel, N., Raileanu, R., Hughes-Fitt, S., Dalibard, V., and Czarnecki, W. M. Open-ended learning leads to generally capable agents. arXiv preprint arXiv: Arxiv2107.12808, 2021.</p>
<p>Thananjeyan, B., Balakrishna, A., Nair, S., Luo, M., Srinivasan, K., Hwang, M., Gonzalez, J. E., Ibarz, J., Finn, C., and Goldberg, K. Recovery RL: safe reinforcement learning with learned recovery zones. IEEE Robotics Autom. Lett., 6(3):4915-4922, 2021. doi: 10.1109/LRA.2021. 3070252. URL https://doi.org/10.1109/LRA. 2021.3070252.</p>
<p>Toyama, D., Hamel, P., Gergely, A., Comanici, G., Glaese, A., Ahmed, Z., Jackson, T., Mourad, S., and Precup, D. Androidenv: A reinforcement learning platform for android. arXiv preprint arXiv: Arxiv-2105.13231, 2021.</p>
<p>Toyer, S., Shah, R., Critch, A., and Russell, S. The MAGICAL benchmark for robust imitation. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings. neurips.cc/paper/2020/hash/ d464b5ac99e74462f321c06ccacc4bff-Abstract. html.</p>
<p>Tsimpoukelli, M., Menick, J., Cabi, S., Eslami, S. M. A., Vinyals, O., and Hill, F. Multimodal few-shot learning with frozen language models. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 200-212, 2021. URL https://proceedings. neurips.cc/paper/2021/hash/</p>
<p>01b7575c38dac42f3cfb7d500438b875-Abstract.html.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. arXiv preprint arXiv: Arxiv-1706.03762, 2017.</p>
<p>Vinyals, O., Babuschkin, I., Chung, J., Mathieu, M., Jaderberg, M., Czarnecki, W. M., Dudzik, A., Huang, A., Georgiev, P., Powell, R., et al. Alphastar: Mastering the real-time strategy game starcraft ii. DeepMind blog, 2, 2019 .</p>
<p>Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An openended embodied agent with large language models. arXiv preprint arXiv: Arxiv-2305.16291, 2023.</p>
<p>Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., and Yang, H. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. arXiv preprint arXiv: Arxiv-2202.03052, 2022a.</p>
<p>Wang, T., Roberts, A., Hesslow, D., Scao, T. L., Chung, H. W., Beltagy, I., Launay, J., and Raffel, C. What language model architecture and pretraining objective work best for zero-shot generalization? Icml, 2022b. doi: 10.48550/arXiv.2204.05832.</p>
<p>Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O. K., Singhal, S., Som, S., and Wei, F. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv: Arxiv-2208.10442, 2022c.</p>
<p>Weihs, L., Deitke, M., Kembhavi, A., and Mottaghi, R. Visual room rearrangement. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 5922-5931. Computer Vision Foundation / IEEE, 2021. doi: 10.1109/CVPR46437.2021.00586. URL https: //openaccess.thecvf.com/content/ CVPR2021/html/Weihs_Visual_Room_ Rearrangement_CVPR_2021_paper.html.</p>
<p>Weikert, A., Goralczyk, A., Salmela, B., Dansie, B., Barton, C., Valenza, E., Alexandrov, G., Hubert, I., Tysdal, K., Sokolowski, L., Järvinen, M., Pulieso, M., Ebb, M., Vazquez, P., Tuytel, R., Hess, R., Feldlaufer, S., König, S., Platen, S., and Mäter., S. Blender online libraries for textures, 2022. URL https://cloud.blender. org/p/textures/.</p>
<p>Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,</p>
<p>Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv: Arxiv-1910.03771, 2019.</p>
<p>Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., and Girshick, R. Detectron2. https://github.com/ facebookresearch/detectron2, 2019.</p>
<p>Xiao, T., Radosavovic, I., Darrell, T., and Malik, J. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022.</p>
<p>Xu, M., Shen, Y., Zhang, S., Lu, Y., Zhao, D., Tenenbaum, J. B., and Gan, C. Prompting decision transformer for few-shot policy generalization. arXiv preprint arXiv: Arxiv-2206.13499, 2022.</p>
<p>Xu, M., Lu, Y., Shen, Y., Zhang, S., Zhao, D., and Gan, C. Hyper-decision transformer for efficient online policy adaptation. arXiv preprint arXiv: Arxiv-2304.08487, 2023.</p>
<p>Yang, S., Nachum, O., Du, Y., Wei, J., Abbeel, P., and Schuurmans, D. Foundation models for decision making: Problems, methods, and opportunities. arXiv preprint arXiv: Arxiv-2303.04129, 2023.</p>
<p>Yang, Z., Fang, Y., Zhu, C., Pryzant, R., Chen, D., Shi, Y., Xu, Y., Qian, Y., Gao, M., Chen, Y.-L., Lu, L., Xie, Y., Gmyr, R., Codella, N., Kanda, N., Xiao, B., Yuan, L., Yoshioka, T., Zeng, M., and Huang, X. i-code: An integrative and composable multimodal learning framework. arXiv preprint arXiv: Arxiv-2205.01818, 2022.</p>
<p>Yoon, Y., DeSouza, G., and Kak, A. Real-time tracking and pose estimation for industrial objects using geometric features. In 2003 IEEE International Conference on Robotics and Automation (Cat. No.03CH37422), volume 3, pp. 3473-3478 vol.3, 2003. doi: 10.1109/ROBOT. 2003.1242127.</p>
<p>Yu, T., Quillen, D., He, Z., Julian, R., Narayan, A., Shively, H., Bellathur, A., Hausman, K., Finn, C., and Levine, S. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. arXiv preprint arXiv: Arxiv-1910.10897, 2019.</p>
<p>Yu, T., Xiao, T., Stone, A., Tompson, J., Brohan, A., Wang, S., Singh, J., Tan, C., M, D., Peralta, J., Ichter, B., Hausman, K., and Xia, F. Scaling robot learning with semantically imagined experience. arXiv preprint arXiv: Arxiv-2302.11550, 2023.</p>
<p>Yuan, L., Chen, D., Chen, Y.-L., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C., Liu, C., Liu, M., Liu, Z., Lu, Y., Shi, Y., Wang, L., Wang, J., Xiao, B.,</p>
<p>Xiao, Z., Yang, J., Zeng, M., Zhou, L., and Zhang, P. Florence: A new foundation model for computer vision. arXiv preprint arXiv: Arxiv-2111.11432, 2021.</p>
<p>Zellers, R., Lu, X., Hessel, J., Yu, Y., Park, J. S., Cao, J., Farhadi, A., and Choi, Y. Merlot: Multimodal neural script knowledge models. arXiv preprint arXiv: Arxiv2106.02636, 2021.</p>
<p>Zellers, R., Lu, J., Lu, X., Yu, Y., Zhao, Y., Salehi, M., Kusupati, A., Hessel, J., Farhadi, A., and Choi, Y. Merlot reserve: Neural script knowledge through vision and language and sound. CVPR, 2022.</p>
<p>Zeng, A., Florence, P., Tompson, J., Welker, S., Chien, J., Attarian, M., Armstrong, T., Krasin, I., Duong, D., Wahid, A., Sindhwani, V., and Lee, J. Transporter networks: Rearranging the visual world for robotic manipulation. arXiv preprint arXiv: Arxiv-2010.14406, 2020.</p>
<p>Zeng, A., Wong, A., Welker, S., Choromanski, K., Tombari, F., Purohit, A., Ryoo, M., Sindhwani, V., Lee, J., Vanhoucke, V., and Florence, P. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv: Arxiv-2204.00598, 2022.</p>
<p>Zhang, Z. and Weihs, L. When learning is out of reach, reset: Generalization in autonomous visuomotor reinforcement learning. arXiv preprint arXiv: Arxiv-2303.17600, 2023.</p>
<p>Zhao, M., Liu, F., Lee, K., and Abbeel, P. Towards more generalizable one-shot visual imitation learning. In 2022 International Conference on Robotics and Automation, ICRA 2022, Philadelphia, PA, USA, May 23-27, 2022, pp. 2434-2444. IEEE, 2022. doi: 10. 1109/ICRA46639.2022.9812450. URL https://doi. org/10.1109/ICRA46639.2022.9812450.</p>
<p>Zheng, Q., Zhang, A., and Grover, A. Online decision transformer. arXiv preprint arXiv: Arxiv-2202.05607, 2022.</p>
<p>Zhu, Y., Wong, J., Mandlekar, A., and Martín-Martín, R. robosuite: A modular simulation framework and benchmark for robot learning. arXiv preprint arXiv: Arxiv2009.12293, 2020.</p>
<p>Zhu, Y., Joshi, A., Stone, P., and Zhu, Y. Viola: Imitation learning for vision-based manipulation with object proposal priors. arXiv preprint arXiv: Arxiv-2210.11339, 2022.</p>
<h1>A. Simulator Details</h1>
<p>We build our VIMA-BENCH simulation suite upon the Ravens physics simulator (Zeng et al., 2020; Shridhar et al., 2021). Specifically, it is supported by PyBullet (Coumans \&amp; Bai, 2016-2021) with a Universal Robot UR5 arm. The size of the tabletop workspace is $0.5 \times 1 \mathrm{~m}$. Our benchmark contains extensible sets of 3D objects and textures. Instantiated from an object-texture combination, all object instances can be rendered as RGB images appeared in multimodal prompts. Figure A. 1 displays all 3D objects. Figure A. 2 displays all textures.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure A.1: Object Gallery in VIMA-BENCH textured with random textures. Bowl and pan are from Google Scanned Objects (Downs et al., 2022), while others are from Ravens (Zeng et al., 2020).</p>
<p>The observation space of VIMA-BENCH includes RGB images from both frontal and top-down views. It also includes a one-hot vector $\in{0,1}^{2}$ to indicate type of the end-effector $\in{$ suction cup, spatula $}$. While a suction cup is equipped in most manipulation tasks, a spatula is used in particular for visual constraint tasks, where an agent is asked to "wipe" objects. VIMA-BENCH inherits the same action space from Zeng et al. (2020) and Shridhar et al. (2021), which consists of primitive actions of "pick and place" for tasks with a suction cup as the end effector, or "push" for tasks with a spatula. Both primitive actions contain two poses $\in \mathbf{S E}(2)$ specifying target poses of the end effector. For the "pick and place" primitive, they represent the pick pose and the place pose. For the "push" primitive, they represent the push starting pose and push</p>
<p>ending pose.
Similar to prior work (Zeng et al., 2020; Shridhar et al., 2021), VIMA-Bench provides scripted oracles to generate successful demonstrations for all tasks. We leverage them to construct an offline imitation dataset for behavioral cloning. Given a prompt, these programmed bots can access privileged information, such as the correct object to pick and target location to place.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure A.2: Texture Gallery in VIMA-Bench. The first row of image-based textures is from Blender Cloud Libraries (Weikert et al., 2022), while others are hard-coded.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Stanford University; ${ }^{2}$ Macalester College, now at Allen Institute for AI; ${ }^{3}$ NVIDIA; ${ }^{4}$ Caltech; ${ }^{5}$ Tsinghua; ${ }^{6}$ UT Austin. Work done during the first author's internship at NVIDIA. $\dagger$ : Equal contribution. $\ddagger$ : Equal advising.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
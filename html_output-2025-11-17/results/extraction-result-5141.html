<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5141 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5141</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5141</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-270068240</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.17440v1.pdf" target="_blank">CataLM: empowering catalyst design through large language models</a></p>
                <p><strong>Paper Abstract:</strong> The field of catalysis holds paramount importance in shaping the trajectory of sustainable development, prompting intensive research efforts to leverage artificial intelligence (AI) in catalyst design. Presently, the fine-tuning of open-source large language models (LLMs) has yielded significant breakthroughs across various domains such as biology and healthcare. Drawing inspiration from these advancements, we introduce CataLM (Catalytic Language Model), a large language model tailored to the domain of electrocatalytic materials. Our findings demonstrate that CataLM exhibits remarkable potential for facilitating human-AI collaboration in catalyst knowledge exploration and design. To the best of our knowledge, CataLM stands as the pioneering LLM dedicated to the catalyst domain, offering novel avenues for catalyst discovery and development.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5141.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5141.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CataLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Catalytic Language Model (CataLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specialized large language model fine-tuned for electrocatalytic materials knowledge extraction and recommendation of catalyst materials and control/preparation methods; built by further pretraining and instruction-tuning a Vicuna family LLM and augmented with retrieval from a vector database.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna (fine-tuned as CataLM; paper references Vicuna-13B and Vicuna-33b-v1.3 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer, GPT-like autoregressive LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B / 33B (inconsistently reported within paper; both variants are mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Domain pretraining on full text of 12,643 open-access electrocatalysis papers (CO2 reduction literature) plus instruction-tuning on expert-annotated corpus (standard corpus: 6,985 entities) and an expanded automatically extracted set (≈30,283 entities) consolidated into 13,432 cleaned catalytic process descriptions; vector DB built with Sci-BERT embeddings for retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science / electrocatalysis — catalyst design and recommendation (electrocatalytic CO2 reduction products and preparation/control methods).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Domain pretraining + instruction tuning (fine-tuning) using LoRA low-rank adapters; Retrieval-Augmented Generation (RAG) with a Sci-BERT-embedded vector database; evaluated in zero-shot and few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Natural-language textual recommendations: material names, composite descriptions, and stepwise/sentence-form synthesis/control method descriptions; structured synthetic pathways are produced by post-processing (pattern recognition + neural networks) from unstructured text, not molecular-line-notation (e.g., no SMILES/SELFIES reported).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>NER: per-entity counts and 'Modified Accuracy' (accounts for true absence); overall NER Modified Accuracy = 68.75%; ablation comparisons (Correct / Modified Correct / Modified Accuracy) across combinations of fine-tuning and RAG; qualitative expert assessment for control-method recommendation (domain experts judged correctness, specificity, and mechanism explanations).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>In-house corpora derived from Web of Science export (12,643 papers), the 'standard corpus' of 6,985 annotated entities, expanded extraction of ≈30,283 entities, final cleaned training set of 13,432 catalytic process descriptions; NER results deposited at Science Data Bank (ScienceDB).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>CataLM (fine-tuned Vicuna + RAG) achieves substantive capability for literature-grounded entity extraction (overall Modified Accuracy 68.75%) and provides qualitatively better, more domain-specific catalyst material and control-method recommendations compared with the original (non-fine-tuned) LLM; ablation shows best performance with fine-tuning + few-shot prompting and that both fine-tuning and retrieval augmentation contribute to gains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to the original unfine-tuned LLM, CataLM gives more specific and mechanistically plausible material choices (paper shows examples where original LLM suggested incorrect catalysts, e.g., Pt for CO2 reduction), and ablation quantifies improvements from fine-tuning and RAG; no direct comparison to physics-based or DFT screening in this work (evaluation is largely NER metrics + expert qualitative review rather than experimental/first-principles validation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Does not output explicit chemical structural encodings (SMILES/graphs) or provide calculated activity metrics; evaluation of design recommendations is qualitative (expert review) rather than experimental/DFT validation; inconsistent reporting of base Vicuna variant (13B vs 33B) within the text; poorer performance on descriptive entity extraction (textual/descriptive classes) vs numeric entities, risk of hallucination (original LLM errors shown); reliance on the coverage and quality of literature-derived corpora and expert annotations; no direct end-to-end demonstration that generated recommendations yield improved catalytic performance in experiment or simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CataLM: empowering catalyst design through large language models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5141.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5141.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MatChat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MatChat (Matchat in text)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A materials-domain LLM platform obtained by optimizing LLaMA2-7B on inorganic materials literature to support tasks such as predicting chemical synthesis pathways of inorganic materials.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Matchat: A large language model and application service platform for materials science.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MatChat (optimized from LLaMA2-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer, GPT-like autoregressive LLM (LLaMA2 family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (LLaMA2-7B underlying model as stated in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on inorganic materials science literature (domain-specific corpus of materials science texts).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science — prediction of inorganic material synthesis pathways and materials informatics support.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning / instruction-tuning of LLaMA2-7B on domain literature (text-generation for synthesis pathways); likely prompt-driven generation for specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Textual synthesis pathways and procedural descriptions (natural language), not explicit molecular-line notation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in detail within this paper (cited as prior work); referenced as demonstrating viability for predicting synthesis pathways.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Inorganic materials literature corpus used for fine-tuning (details in original MatChat citation).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported (in cited work) as a viable solution for predicting inorganic material synthesis pathways after LLaMA2-7B domain fine-tuning; in this paper MatChat is cited as an example of LLM use in materials synthesis prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Cited as a domain-specialized LLM approach contrasted with generic LLMs; no quantitative comparisons provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Within this paper only cited; specific limitations not detailed here. Generally, such models produce textual pathways that require downstream structuring, validation, and experimental confirmation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CataLM: empowering catalyst design through large language models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5141.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5141.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructMol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructMol</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Vicuna-based multi-modal model adapted for multiple chemical tasks to serve as a molecular assistant in drug discovery, combining modalities and task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructMol (Vicuna-based adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer, multimodal LLM (Vicuna backbone reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Task-specific fine-tuning data for chemical tasks (not detailed in this paper's text).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / molecular design — general-purpose molecular assistant tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Task-specific fine-tuning of a Vicuna-based model; multi-modal integration (as reported in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Textual outputs and multi-modal responses for molecular tasks; explicit formats (SMILES/graphs) not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper; original InstructMol citation would contain evaluation details.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Not listed here; referred to as a work adopting Vicuna to multiple chemical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as having been applied to multiple chemical tasks and constructed as a reliable molecular assistant; details are from the referenced work rather than experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned as an example of domain-specific fine-tuning of an LLM for chemistry tasks; no quantitative comparisons provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not detailed in this paper; general caveats include need for task-specific data, validation, and chemical representation mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CataLM: empowering catalyst design through large language models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5141.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5141.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BiOGPT (BiOGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific pre-trained Transformer language model for biomedical text generation and mining, previously used for biological named-entity tasks and protein molecular design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Biogpt: generative pre-trained transformer for biomedical text generation and mining.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BiOGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer, GPT-like generative model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Biomedical text corpora (domain-specific biomedical literature) as reported in the BiOGPT work.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Biology / protein design — biological text generation and some applications toward protein molecular design.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Domain pretraining and fine-tuning for biomedical tasks; generative text modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Natural-language text and application-specific outputs; when applied to protein design, outputs relate to sequences or textual descriptions (explicit representations not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified here; cited as demonstrating utility on entity tasks and protein molecular design in the BiOGPT paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Biomedical corpora used for BiOGPT pretraining (details in original citation).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Referenced as an example where domain-specific LLM pretraining/fine-tuning enables downstream molecular design tasks in biology, supporting the motivation for CataLM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Cited in context of successful domain LLM applications (biology) motivating similar approaches in catalysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in this paper; general issues include need for high-quality domain data and experimental validation for designed molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CataLM: empowering catalyst design through large language models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5141.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5141.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT for MOF synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT chemistry assistant for text mining and prediction of MOF synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work using prompt engineering with ChatGPT to automate text mining of metal-organic framework (MOF) synthesis conditions and to predict MOF synthesis parameters from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatgpt chemistry assistant for text mining and prediction of mof synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-family, API-served model referenced by cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer, GPT-like autoregressive model (proprietary OpenAI model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified here; in the cited work ChatGPT was guided via prompt engineering on MOF literature formats and styles.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials chemistry — extraction and prediction of MOF synthesis conditions from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt engineering and directed text-mining using ChatGPT; automation of extraction from diverse formats.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Textual extraction and predicted synthesis conditions (natural language); no SMILES/graph representations reported in this paper's citation summary.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified here; original cited paper would contain evaluation details of extraction accuracy and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>MOF synthesis literature and diverse formatted sources (as used in the cited study).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as successful use of prompt engineering to guide ChatGPT for automating MOF synthesis-condition extraction; serves as an example of LLM use in materials/chemistry literature mining.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Presented as a prompt-engineering approach to automate text mining; no detailed comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not detailed here; prompt-engineered ChatGPT approaches depend on prompt design and can suffer from hallucination and lack of grounded domain knowledge without domain fine-tuning or retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CataLM: empowering catalyst design through large language models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Matchat: A large language model and application service platform for materials science. <em>(Rating: 2)</em></li>
                <li>Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery. <em>(Rating: 2)</em></li>
                <li>Biogpt: generative pre-trained transformer for biomedical text generation and mining. <em>(Rating: 1)</em></li>
                <li>Chatgpt chemistry assistant for text mining and prediction of mof synthesis. <em>(Rating: 2)</em></li>
                <li>A corpus of co2 electrocatalytic reduction process extracted from the scientific literature. <em>(Rating: 2)</em></li>
                <li>The extended corpus of CO2 reduction electrocatalysts and synthesis procedures. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5141",
    "paper_id": "paper-270068240",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "CataLM",
            "name_full": "Catalytic Language Model (CataLM)",
            "brief_description": "A domain-specialized large language model fine-tuned for electrocatalytic materials knowledge extraction and recommendation of catalyst materials and control/preparation methods; built by further pretraining and instruction-tuning a Vicuna family LLM and augmented with retrieval from a vector database.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna (fine-tuned as CataLM; paper references Vicuna-13B and Vicuna-33b-v1.3 variants)",
            "model_type": "Transformer, GPT-like autoregressive LLM",
            "model_size": "13B / 33B (inconsistently reported within paper; both variants are mentioned)",
            "training_data": "Domain pretraining on full text of 12,643 open-access electrocatalysis papers (CO2 reduction literature) plus instruction-tuning on expert-annotated corpus (standard corpus: 6,985 entities) and an expanded automatically extracted set (≈30,283 entities) consolidated into 13,432 cleaned catalytic process descriptions; vector DB built with Sci-BERT embeddings for retrieval augmentation.",
            "application_domain": "Materials science / electrocatalysis — catalyst design and recommendation (electrocatalytic CO2 reduction products and preparation/control methods).",
            "generation_method": "Domain pretraining + instruction tuning (fine-tuning) using LoRA low-rank adapters; Retrieval-Augmented Generation (RAG) with a Sci-BERT-embedded vector database; evaluated in zero-shot and few-shot settings.",
            "output_representation": "Natural-language textual recommendations: material names, composite descriptions, and stepwise/sentence-form synthesis/control method descriptions; structured synthetic pathways are produced by post-processing (pattern recognition + neural networks) from unstructured text, not molecular-line-notation (e.g., no SMILES/SELFIES reported).",
            "evaluation_metrics": "NER: per-entity counts and 'Modified Accuracy' (accounts for true absence); overall NER Modified Accuracy = 68.75%; ablation comparisons (Correct / Modified Correct / Modified Accuracy) across combinations of fine-tuning and RAG; qualitative expert assessment for control-method recommendation (domain experts judged correctness, specificity, and mechanism explanations).",
            "benchmarks_or_datasets": "In-house corpora derived from Web of Science export (12,643 papers), the 'standard corpus' of 6,985 annotated entities, expanded extraction of ≈30,283 entities, final cleaned training set of 13,432 catalytic process descriptions; NER results deposited at Science Data Bank (ScienceDB).",
            "results_summary": "CataLM (fine-tuned Vicuna + RAG) achieves substantive capability for literature-grounded entity extraction (overall Modified Accuracy 68.75%) and provides qualitatively better, more domain-specific catalyst material and control-method recommendations compared with the original (non-fine-tuned) LLM; ablation shows best performance with fine-tuning + few-shot prompting and that both fine-tuning and retrieval augmentation contribute to gains.",
            "comparison_to_other_methods": "Compared to the original unfine-tuned LLM, CataLM gives more specific and mechanistically plausible material choices (paper shows examples where original LLM suggested incorrect catalysts, e.g., Pt for CO2 reduction), and ablation quantifies improvements from fine-tuning and RAG; no direct comparison to physics-based or DFT screening in this work (evaluation is largely NER metrics + expert qualitative review rather than experimental/first-principles validation).",
            "limitations_or_challenges": "Does not output explicit chemical structural encodings (SMILES/graphs) or provide calculated activity metrics; evaluation of design recommendations is qualitative (expert review) rather than experimental/DFT validation; inconsistent reporting of base Vicuna variant (13B vs 33B) within the text; poorer performance on descriptive entity extraction (textual/descriptive classes) vs numeric entities, risk of hallucination (original LLM errors shown); reliance on the coverage and quality of literature-derived corpora and expert annotations; no direct end-to-end demonstration that generated recommendations yield improved catalytic performance in experiment or simulation.",
            "uuid": "e5141.0",
            "source_info": {
                "paper_title": "CataLM: empowering catalyst design through large language models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "MatChat",
            "name_full": "MatChat (Matchat in text)",
            "brief_description": "A materials-domain LLM platform obtained by optimizing LLaMA2-7B on inorganic materials literature to support tasks such as predicting chemical synthesis pathways of inorganic materials.",
            "citation_title": "Matchat: A large language model and application service platform for materials science.",
            "mention_or_use": "mention",
            "model_name": "MatChat (optimized from LLaMA2-7B)",
            "model_type": "Transformer, GPT-like autoregressive LLM (LLaMA2 family)",
            "model_size": "7B (LLaMA2-7B underlying model as stated in paper)",
            "training_data": "Fine-tuned on inorganic materials science literature (domain-specific corpus of materials science texts).",
            "application_domain": "Materials science — prediction of inorganic material synthesis pathways and materials informatics support.",
            "generation_method": "Fine-tuning / instruction-tuning of LLaMA2-7B on domain literature (text-generation for synthesis pathways); likely prompt-driven generation for specific tasks.",
            "output_representation": "Textual synthesis pathways and procedural descriptions (natural language), not explicit molecular-line notation reported.",
            "evaluation_metrics": "Not specified in detail within this paper (cited as prior work); referenced as demonstrating viability for predicting synthesis pathways.",
            "benchmarks_or_datasets": "Inorganic materials literature corpus used for fine-tuning (details in original MatChat citation).",
            "results_summary": "Reported (in cited work) as a viable solution for predicting inorganic material synthesis pathways after LLaMA2-7B domain fine-tuning; in this paper MatChat is cited as an example of LLM use in materials synthesis prediction.",
            "comparison_to_other_methods": "Cited as a domain-specialized LLM approach contrasted with generic LLMs; no quantitative comparisons provided here.",
            "limitations_or_challenges": "Within this paper only cited; specific limitations not detailed here. Generally, such models produce textual pathways that require downstream structuring, validation, and experimental confirmation.",
            "uuid": "e5141.1",
            "source_info": {
                "paper_title": "CataLM: empowering catalyst design through large language models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "InstructMol",
            "name_full": "InstructMol",
            "brief_description": "A Vicuna-based multi-modal model adapted for multiple chemical tasks to serve as a molecular assistant in drug discovery, combining modalities and task-specific fine-tuning.",
            "citation_title": "Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery.",
            "mention_or_use": "mention",
            "model_name": "InstructMol (Vicuna-based adaptation)",
            "model_type": "Transformer, multimodal LLM (Vicuna backbone reported)",
            "model_size": null,
            "training_data": "Task-specific fine-tuning data for chemical tasks (not detailed in this paper's text).",
            "application_domain": "Drug discovery / molecular design — general-purpose molecular assistant tasks.",
            "generation_method": "Task-specific fine-tuning of a Vicuna-based model; multi-modal integration (as reported in cited work).",
            "output_representation": "Textual outputs and multi-modal responses for molecular tasks; explicit formats (SMILES/graphs) not detailed here.",
            "evaluation_metrics": "Not specified in this paper; original InstructMol citation would contain evaluation details.",
            "benchmarks_or_datasets": "Not listed here; referred to as a work adopting Vicuna to multiple chemical tasks.",
            "results_summary": "Cited as having been applied to multiple chemical tasks and constructed as a reliable molecular assistant; details are from the referenced work rather than experiments in this paper.",
            "comparison_to_other_methods": "Mentioned as an example of domain-specific fine-tuning of an LLM for chemistry tasks; no quantitative comparisons provided in this paper.",
            "limitations_or_challenges": "Not detailed in this paper; general caveats include need for task-specific data, validation, and chemical representation mapping.",
            "uuid": "e5141.2",
            "source_info": {
                "paper_title": "CataLM: empowering catalyst design through large language models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "BioGPT",
            "name_full": "BiOGPT (BiOGPT)",
            "brief_description": "A domain-specific pre-trained Transformer language model for biomedical text generation and mining, previously used for biological named-entity tasks and protein molecular design.",
            "citation_title": "Biogpt: generative pre-trained transformer for biomedical text generation and mining.",
            "mention_or_use": "mention",
            "model_name": "BiOGPT",
            "model_type": "Transformer, GPT-like generative model",
            "model_size": null,
            "training_data": "Biomedical text corpora (domain-specific biomedical literature) as reported in the BiOGPT work.",
            "application_domain": "Biology / protein design — biological text generation and some applications toward protein molecular design.",
            "generation_method": "Domain pretraining and fine-tuning for biomedical tasks; generative text modeling.",
            "output_representation": "Natural-language text and application-specific outputs; when applied to protein design, outputs relate to sequences or textual descriptions (explicit representations not detailed in this paper).",
            "evaluation_metrics": "Not specified here; cited as demonstrating utility on entity tasks and protein molecular design in the BiOGPT paper.",
            "benchmarks_or_datasets": "Biomedical corpora used for BiOGPT pretraining (details in original citation).",
            "results_summary": "Referenced as an example where domain-specific LLM pretraining/fine-tuning enables downstream molecular design tasks in biology, supporting the motivation for CataLM.",
            "comparison_to_other_methods": "Cited in context of successful domain LLM applications (biology) motivating similar approaches in catalysis.",
            "limitations_or_challenges": "Not discussed in this paper; general issues include need for high-quality domain data and experimental validation for designed molecules.",
            "uuid": "e5141.3",
            "source_info": {
                "paper_title": "CataLM: empowering catalyst design through large language models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "ChatGPT for MOF synthesis",
            "name_full": "ChatGPT chemistry assistant for text mining and prediction of MOF synthesis",
            "brief_description": "Work using prompt engineering with ChatGPT to automate text mining of metal-organic framework (MOF) synthesis conditions and to predict MOF synthesis parameters from literature.",
            "citation_title": "Chatgpt chemistry assistant for text mining and prediction of mof synthesis.",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (GPT-family, API-served model referenced by cited work)",
            "model_type": "Transformer, GPT-like autoregressive model (proprietary OpenAI model)",
            "model_size": null,
            "training_data": "Not specified here; in the cited work ChatGPT was guided via prompt engineering on MOF literature formats and styles.",
            "application_domain": "Materials chemistry — extraction and prediction of MOF synthesis conditions from literature.",
            "generation_method": "Prompt engineering and directed text-mining using ChatGPT; automation of extraction from diverse formats.",
            "output_representation": "Textual extraction and predicted synthesis conditions (natural language); no SMILES/graph representations reported in this paper's citation summary.",
            "evaluation_metrics": "Not specified here; original cited paper would contain evaluation details of extraction accuracy and coverage.",
            "benchmarks_or_datasets": "MOF synthesis literature and diverse formatted sources (as used in the cited study).",
            "results_summary": "Cited as successful use of prompt engineering to guide ChatGPT for automating MOF synthesis-condition extraction; serves as an example of LLM use in materials/chemistry literature mining.",
            "comparison_to_other_methods": "Presented as a prompt-engineering approach to automate text mining; no detailed comparison in this paper.",
            "limitations_or_challenges": "Not detailed here; prompt-engineered ChatGPT approaches depend on prompt design and can suffer from hallucination and lack of grounded domain knowledge without domain fine-tuning or retrieval augmentation.",
            "uuid": "e5141.4",
            "source_info": {
                "paper_title": "CataLM: empowering catalyst design through large language models",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Matchat: A large language model and application service platform for materials science.",
            "rating": 2,
            "sanitized_title": "matchat_a_large_language_model_and_application_service_platform_for_materials_science"
        },
        {
            "paper_title": "Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery.",
            "rating": 2,
            "sanitized_title": "instructmol_multimodal_integration_for_building_a_versatile_and_reliable_molecular_assistant_in_drug_discovery"
        },
        {
            "paper_title": "Biogpt: generative pre-trained transformer for biomedical text generation and mining.",
            "rating": 1,
            "sanitized_title": "biogpt_generative_pretrained_transformer_for_biomedical_text_generation_and_mining"
        },
        {
            "paper_title": "Chatgpt chemistry assistant for text mining and prediction of mof synthesis.",
            "rating": 2,
            "sanitized_title": "chatgpt_chemistry_assistant_for_text_mining_and_prediction_of_mof_synthesis"
        },
        {
            "paper_title": "A corpus of co2 electrocatalytic reduction process extracted from the scientific literature.",
            "rating": 2,
            "sanitized_title": "a_corpus_of_co2_electrocatalytic_reduction_process_extracted_from_the_scientific_literature"
        },
        {
            "paper_title": "The extended corpus of CO2 reduction electrocatalysts and synthesis procedures.",
            "rating": 2,
            "sanitized_title": "the_extended_corpus_of_co2_reduction_electrocatalysts_and_synthesis_procedures"
        }
    ],
    "cost": 0.0123675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CataLM: Empowering Catalyst Design Through Large Language Models
13 May 2024</p>
<p>Ludi Wang 
Computer Network Information Center
Laboratory of Big Data Knowledge
Chinese Academy of Sciences
100083BeijingChina</p>
<p>Xueqing Chen xqchen@cnic.cn 
Computer Network Information Center
Laboratory of Big Data Knowledge
Chinese Academy of Sciences
100083BeijingChina</p>
<p>University of Chinese Academy of Sciences
100049BeijingChina</p>
<p>Yi Du duyi@cnic.cn 
Computer Network Information Center
Laboratory of Big Data Knowledge
Chinese Academy of Sciences
100083BeijingChina</p>
<p>University of Chinese Academy of Sciences
100049BeijingChina</p>
<p>Hangzhou Institute for Advanced Study
UCAS
310000HangzhouChina</p>
<p>Yuanchun Zhou 
Computer Network Information Center
Laboratory of Big Data Knowledge
Chinese Academy of Sciences
100083BeijingChina</p>
<p>University of Chinese Academy of Sciences
100049BeijingChina</p>
<p>Hangzhou Institute for Advanced Study
UCAS
310000HangzhouChina</p>
<p>Yang Gao gaoyang@nanoctr.cn 
National Center for Nanoscience and Technology (NCNST)
CAS Key Laboratory of Nanosystem and Hierarchical Fabrication
100190BeijingChina</p>
<p>Wenjuan Cui wenjuancui@cnic.cn 
Computer Network Information Center
Laboratory of Big Data Knowledge
Chinese Academy of Sciences
100083BeijingChina</p>
<p>University of Chinese Academy of Sciences
100049BeijingChina</p>
<p>CataLM: Empowering Catalyst Design Through Large Language Models
13 May 2024A81835183557496D24AAD21BF750F03BarXiv:2405.17440v1[cs.LG]AI for Science (AI4S)Large Language Models (LLMs)Electrocatalytic MaterialsCatalyst Design
The field of catalysis holds paramount importance in shaping the trajectory of sustainable development, prompting intensive research efforts to leverage artificial intelligence (AI) in catalyst design.Presently, the fine-tuning of open-source large language models (LLMs) has yielded significant breakthroughs across various domains such as biology and healthcare.Drawing inspiration from these advancements, we introduce CataLM (Catalytic Language Model), a large language model tailored to the domain of electrocatalytic materials.Our findings demonstrate that CataLM exhibits remarkable potential for facilitating human-AI collaboration in catalyst knowledge exploration and design.To the best of our knowledge, CataLM stands as the pioneering LLM dedicated to the catalyst domain, offering novel avenues for catalyst discovery and development.</p>
<p>Introduction</p>
<p>The field of catalysis is crucial to the future of sustainable development.Innovative catalysts can generate clean fuels, reduce the impact of global warming and provide solutions to environmental pollution [7,33].Theoretical calculations and simulations can accelerate catalyst screening through activity descriptors that link structure to catalyst activity [18,24,34].However, numerous variables exist in the synthesis, composition, structure, and performance of electrocatalysts, with much of this critical knowledge often elusive within scientific literature.This poses challenges in elucidating the intricate correlations from limited experimental data.Artificial intelligence can be used to extract, analyze and understand key information embedded in the vast scientific literature on catalysis that can be dedicated to predicting new catalysts.Natural language processing techniques and generative language models enable the extraction and analysis of textual information from scientific literature and the generation of domain-relevant on-demand text, which has shown potential in recent biological and thermoelectric research.However, language models in the field of catalysis are sparse and limited in scale, which restricts their use in empowering knowledge extraction in catalytic materials research and further enabling the discovery of new catalysts.</p>
<p>Pre-trained models have demonstrated their powerful capabilities in Natural Language Processing (NLP).There are two main types of pre-trained models: (1) BERT-like models [6,8,20], which are mainly used for language comprehension tasks, and (2) GPT-like models [2,29,30], which are mainly used for language generation tasks.Currently, large-scale language models (LLMs) such as GPT-4.0 [26] have laid a solid foundation for various applications.Although current large language models are effective in general domains, they often fail to meet the needs of catalytic scientists.Much of this inadequacy is attributed to the lack of reliable knowledge about catalysts, as relevant catalyst structural features and performance analyses are rarely present in commonly used pre-trained text corpora, such as C4 [31] and the Pile [10].Furthermore, the best performing large language models like ChatGPT are only served through APIs, which creates a barrier to research and progress in external domains.Fine-tuning open-source large language models is an effective way to meet domain-specific needs.</p>
<p>Currently, fine-tuning open-source large language models have reached considerable success in fields such as biology, healthcare, and finance.In biology, a domainspecific pre-trained Transformer language model, BiOGPT [23], has been developed for biomedical text generation and mining.The model can be optimised and enhanced for performance in tasks such as biological named entity tasks and protein molecular design.In healthcare, models like HuatuoGPT [41] and DoctorGLM [40] have been developed to address healthcare challenges, which exhibit a high degree of expertise and provide valuable insights into the healthcare domain.In recent years, researchers have utilized existing databases such as Atomly [38], OQMD [32], MaterialsProject [15] and others.They have successfully explored the complex relationship between material structure and properties [17], addressing the challenges posed by the scarcity of material data by developing more accurate AI optimisation [21] and training methods [12].With the application of LLMs, materials science researchers have explored the use of these models to address challenges such as chemical reactions and the complex nature of structures.Examples include the MatSciBERT [13] model for the task of materials named entity recognition.MatSciBERT uses a large amount of materials science literature to fine-tune the BERT model [8], demonstrating the ability to automatically extract information from the literature, perform data mining, and construct knowledge graphs.MatChat [5] optimises the LLaMA2-7B model using knowledge of inorganic materials science literature and presents a viable solution for predicting chemical synthesis pathways of inorganic materials, opening up new possibilities for the use of language models in materials science.To the best of our knowledge, there has been no reported utilization of large language models in catalyst science so far.</p>
<p>In this work, we provide CataLM, a large language model aligned with knowledge in the field of electrocatalytic materials.This large language model takes advantage of the pre-trained Vicuna-13B model, and is trained on domain literature and data annotated by experts.With this extensive and diverse data, the original LLM is specialized with two phases: Domain Pre-training, where the model harvests the chemical knowledge from domain field literature, and Instruction Tuning, where the model further understands the requirements of downstream task with the annotation data.We use two tasks to validate CataLM, namely entity extraction task and control method recommendation task.In addition to using the constructed knowledge base for validation, we also invited domain experts to evaluate the answers of CataLM to verify its generalization ability.Results show that our large language model has potent potential for human-AI collaboration in catalyst knowledge search and design.To the best of our knowledge, CataLM is the first LLM that focus on the catalyst domain field, and we believe it can bring new possibilities for the preparation of new catalysts.</p>
<p>Related Work</p>
<p>ChatGPT was selected as one of Nature's Top 10 Individuals of 2023, marking the unprecedented selection of a computer program-the first non-human entity in history-to receive such recognition.Nature states that this award aims to recognize the role of large language models (LLMs) in scientific development and progress.In the field of materials, numerous studies have utilized language models to address diverse tasks.Chen et.al provide the model MatChat [5], for predicting inorganic material synthesis pathways.Xie et.al [39] use FAIR database to fine-tune LLMs and design a downstream task named SII which aims to extract hierarchical, domain specific material and device information, such as composition, structure, preparation conditions, etc., from unstructured scientific texts.Zheng et.al [43] used prompt engineering to guide ChatGPT in the automation of text mining of metal-organic framework (MOF) synthesis conditions from diverse formats and styles of the scientific literature.InstructMol [3] adopts Vicuna to multiple chemical tasks with task-specific fine-tuning.Zheng and colleagues utilized prompt engineering to direct ChatGPT in automating text mining for the synthesis conditions of metal-organic frameworks [42].However, previous works focus on the development of new materials instead of new catalyst designing.Considering the diversity of structural characteristics such as composition, crystal structure, and crystal plane of materials, potential catalysts are very abundant.Secondly, domain fine-tuning data sets which are consistent with downstream applications are crucial for the capability migration of LLMs, which is lacking in the field of catalyst design.This deficiency results in the model's lack of catalyst knowledge, making it challenging to achieve satisfactory parameters.</p>
<p>To promote the creative utilization of large language models in catalysts science, this study utilizes a meticulously crafted database for question-answering to investigate their capabilities in the field of catalysts science.While building this model, we also refer to the successful experiences in the field of other science domains.For example, DeepGO-SE [16] tries to predict GO functions from protein sequences using a pretrained large language model.MedPaLM2 [27] and PMC-LLaMa [37] attempt to tailor LLMs specifically for the fields of biology and medicine through fine-tuning with domain-specific instructions.</p>
<p>CataLM</p>
<p>As shown in Figure 1, the training of CataLM consists of two stages, which are Domain Pre-training and Instruction Tuning respectively.Due to the lack of open-source corpora for recommending catalyst control methods, we utilized expert annotated corpora, as well as the retrieval enhanced corpora generated by large language models for training during the instruction fine-tuning stage.</p>
<p>Domain Pre-training</p>
<p>In this work, the text corpus we used to further pre-train Vicuna-33b-v1.3, including the full text of open-access catalytic papers published in selected high-quality journals in the field of electrocatalytic science.We used Web of Science to find scientific literature on electrocatalytic CO2 reduction.Specifically, we exported the metadata of more than 22,000 articles from Web of Science using the keywords "CO2", "Reduction", and "Electro*" as subject indexes.Eventually, we used the full-text PDFs of 12,643 open-access papers to build the text corpus.</p>
<p>PDF Parsing .We build an automatic PDF parsing toolkit based on the PyMuPDF library [19].Since the processed documents contain irrelevant tags, we developed a data cleaning method for parsing article tag strings into consistently formatted text paragraphs while retaining the same section and paragraph structure as the original paper.Finally, we use regular expressions and rule-based scripts to clean the data, removing the text obstructing reading, garbled, and impurity data.</p>
<p>Vector Database.Despite the fact that Language Model Models (LLMs) are capable of responding to broad inquiries, they are limited in their ability to provide in-depth, precise, and timely information within specific vertical domain.To tackle this issue, we have employed vector databases to augment the reasoning capabilities of LLMs in vertical domain contexts.Vector databases can transform literature and data into vector representations through the process of embedding vectors.For the establishment of vector databases, Sci-BERT [1] has been utilized as an embedding model.</p>
<p>The study involved retrieving titles and abstracts from a dataset containing 12,643 documents, manually annotating catalytic reaction processes by domain experts, and then merging and converting these textual elements into vector representations using Sci-BERT as an embedding framework.In the context of a catalytic domain-specific task such as Name Entity Recognition (NER), the embedding model operates by converting the user query into vector form.Relevant articles are then identified by vector distance calculations to facilitate the retrieval of accurate and relevant information.</p>
<p>Instruction Tuning</p>
<p>In order to align pre-trained models with domain user intent, we need to construct instruction tuning datasets.Currently available generic instruction tuning datasets such as Alpaca-GPT4 [36] and ToolBench [28] can only teach models to follow human instructions.For the specialized field of catalytic materials, we need to train models with knowledge-intensive data that can reflect domain knowledge.Considering the relatively small sample of data annotated by the experts, we use the large language model pre-trained in the previous section to expand it by automatically extracting abstracts from 12,643 documents.The entities were extracted based on an expert-constructed system of electrocatalytic reduction systems for literature content, including materials, conditioning methods, products, faradaic efficiency, cell setup, electrolyte, synthesis method, current density and voltage.The specific meanings and dataset formats of these entities can be found in the previous corpus construction work [4,35].</p>
<p>Firstly, we invite experts in the field of catalysis to perform manual annotation using a well-developed annotation tool, Autodive [9].This tool allows annotators to access material literature through a web browser, view sentences for annotation, and interact with predefined entity types and descriptions.Annotators have the flexibility to include new entities, rearrange existing ones, or make edits in a separate view.We end up with a standard corpus [35] in the field of electrocatalytic CO2 reduction containing 6,985 entities, with each record containing the entity extracted from the paper, its corresponding label, and the context sentence in which the entity is located.The standard corpus is provided as a file in CSV format, and the details are shown in Table 1.Next, we use the pre-trained large language model based on vector database augmentation from the previous section to perform automatic extraction of literature abstracts in the field of catalysis, which extracts a total of 30283 entities.It is important to highlight that the synthetic method of expert annotation in the dataset is an unstructured text paragraph description.We used a multi-model algorithm combining pattern recognition and neural networks to convert it into a structured synthetic pathway [4] containing information about the prepared and target materials, synthetic operations and operating conditions.This structuring of information enhances the interpretability of domain knowledge by the expansive models.</p>
<p>The final dataset used to fine-tune the model in this paper consist of the according electrocatalytic CO2 reduction processes extracted from 12,643 papers.After rigorous filtering, de-duplication and cleaning, we obtained a training set consisting of 13,432 highly reliable catalytic process descriptions.Next, this dataset is further preprocessed and integrated into an instruction question-answering format.For example, for a certain catalytic reaction, using the entities provided in the dataset, we can reconstruct it as a recommendation task for catalyst preparation for a given product.As shown in Figure 2, the prompt involves a specific catalyst material query for a given product, and the answer provides the recommended material and its preparation method.</p>
<p>Training process</p>
<p>The parameters of the model fine-tuning process are list in Table 2.We use NVIDIA A100 GPUs for training, and techniques such as low-rank adaptation [14] is adopted to save storage memory and accelerate the process.Low Rank Adaptation of Large Language Models, also known as LoRA, is a technology developed by Microsoft researchers to address fine-tuning of large language models.The approach of LoRA is to freeze the pre-trained model weight parameters, and then inject trainable layers into each Transformer block.Since there is no need to recalculate the gradient of the model weight parameters, it greatly reduces the computational workload that needs to be trained.Research has found that the fine-tuning quality of LoRA is comparable to that of full model fine-tuning, thus we chose this method in the training process of CataLM.</p>
<p>Named Entity Recognition Task</p>
<p>The first task is named entity recognition, which aims to extract entity from the abstract of given literature.In this task, we use a dataset of 12,643 abstract from electrocatalytic scientific literature (the full text of these literature also be used in the fine-tuning of CataLM) for named entity recognition.We extracted eight types of entity labels, including material, control method, product, faradaic efficiency, cell setup, electrolyte, current density, and voltage.When performing entity recognition, the user first inputs the text to be extracted, and the embedding model transforms it into vectors.Then the similar articles will be obtained by calculating the vector distance, and will be used to generate precise and pertinent information, which be shown in Figure 3.The prompt will be fed into the fine-tuned LLM for entity recognition.For the evaluation and validation of the the entity extraction capability of CataLM, we randomly select 160 entries and validate the LLM's answers for them by experts, and ensure that each category has 20 test data.The evaluation result is shown in Table 3.The Count represents the total amount of samples from different categories, the Correct represents the number of correctly identified entities, and the Existence represents the number of entities of this type that do exist in the text input to the large language model.It is worth mentioning that if there is indeed no corresponding entity in the text input to the large language model, the situation where the large language model answers empty should also be considered as correct recognition.Therefore, we use Modified Correct to remove the above influence.Ultimately, we utilize Modified Correct and Count to calculate the evaluation of LLMs, which is Modified Accuracy.From the results, we can see that CataLM performs better in entity extraction for numerical classes (faraday efficiency, potential, etc.), but performs poorly in entity extraction for descriptive classes.This may be due to the objectivity of data entities, which reduces the possibility of hallucinations in large language models.</p>
<p>We also conducted ablation experiments in this paper.We decomposed the model into two modules, namely the model Fine-tuning module and the Retrieval-Augmented Generation (RAG) module, and they were combined in pairs to form four possibilities.From Table 4, it can be seen that our method (i.e.Fine-tuned LLM + Few shot) performs the best.We can also see that both the fine-tuned module and the RAG module contribute to the improvement of model extraction accuracy.</p>
<p>Control Method Recommendation Task</p>
<p>With the continuous development of big data technology, basic scientific research has shifted from the traditional "random trial and error" to the "data-driven AI" scientific model.Domain experts have also begun to attempt to use large language models to promote scientific innovation, such as literature understanding and summarization, experimental scheme generation, as well as unmanned experimental systems and scientific data sharing platforms, in order to improve scientific research efficiency and promote scientific progress and development.CataLM focuses on the scientific problems in the Catalyst Control field, and tries to assist scientists in catalyst design.However, how to evaluate the effectiveness of recommended catalyst control methods is a challenge faced by CataLM.In this paper, we invite domain experts to evaluate and analyze the recommendation methods generated by CataLM and the original LLM.Several representative results are listed in Table 5.The original LLM typically offers vague material selections and often lacks domain-specific expertise.Additionally, its understanding of control methods usually involves mere restatements of definitions, sometimes leading to significant misconceptions.In contrast, CataLM consistently integrates knowledge to provide more specific and accurate material choices.Its responses are logically coherent, with reasonable explanations of the mechanisms behind the selected materials.As illustrated in Figure 4, CataLM demonstrates its ability to accurately interpret control methods and offer reasonable suggestions, such as the composite system of Cu and carbon, along with clear explanations for the choice of carbon materials.In contrast, the original LLM's misunderstanding of the control method is evident in its incorrect suggestion of Pt, which is not a catalyst for CO 2 reduction.The suitable catalyst material for the production of C2H5OH (ethanol) would be a noble metal such as platinum or palladium.These metals are known for their ability to catalyze a wide range of chemical reactions and are often used in the production of ethanol through the hydrolysis of ethylene.As for the control method type, structure control would be the most appropriate.</p>
<p>Based on the given information, the most suitable catalyst material for producing C2H5OH is Palladium (Pd).The control method that should be used is creating high-facets of wrinkled Pd surrounded by PdO mesh patterns.</p>
<p>C2H4, Metal oxide, structure control</p>
<p>A suitable catalyst material for the production of C2H4 (ethylene) would be a metal oxide such as iron oxide or copper oxide.These materials are known to be effective in the production of ethylene through the cracking of hydrocarbons.As for the control method, structure control would be an appropriate method to use.This type of control method involves manipulating the structure of the catalyst material in order to optimize its performance.</p>
<p>Based on the information provided, the most suitable catalyst material for producing C2H4 is OxygenCu (OCu) combination.The control method that should be used is anodized copper (AN-Cu) Cu(OH)(2) catalysts by a simple electrochemical synthesis method.</p>
<p>Table 5:</p>
<p>The input and answer provided by orginal LLM and CataLM.</p>
<p>Input</p>
<p>Original LLM CataLM HCOOH, Composites consisting of metal and carbon, composite Based on the given information, a suitable catalyst material for the production of HCOOH would be a composite consisting of metal and carbon.One example of such a catalyst is platinumon-carbon (Pt/C).As for the control method, a suitable method would be to use a feedback control system.This type of control system uses a sensor to measure the concentration of HCOOH in the reaction mixture, and adjusts the reaction conditions in real-time to maintain a constant concentration of HCOOH.</p>
<p>Based on the given information, the most suitable catalyst material for the synthesis of HCOOH would be a composite of Copper nanoparticles embedded in Nitrogen-doped graphene oxide (Cu/N-GO).The control method used in this composite would be the interaction between the copper nanoparticles and the nitrogen-doped graphene oxide support.This interaction is expected to enhance the electrocatalytic reduction of CO2 to HCOOH, due to the synergistic effects between the copper nanoparticles and the nitrogen-doped.CO, Alloys/composites of two or more metals, alloy A suitable catalyst material for producing CO (carbon monoxide) from an alloy or composite of two or more metals would be a metal oxide such as iron oxide or cobalt oxide.As for the control method, a suitable option would be to use a feedback control system.</p>
<p>Based on the given information, the most suitable catalyst material for the target product CO is Gold-Copper alloy.The control method for this catalyst material is also Alloy.</p>
<p>Conclusion</p>
<p>In this paper, we introduce CataLM, a effective attempt towards catalyst design leveraging the capabilities of large language models.By undergoing domain pretraining and instruction tuning, our large language model has exhibited robust comprehension and reasoning skills in catalyst knowledge and patterns, achieving advanced performance in application tasks like knowledge extraction and recommendation of control methods.We have open sourced the CataLM model and fine-tuning data to facilitate further expansion and development by interested researchers, which is available at https://github.com/kg4sci/CataLM.The result of NER task is available at Science Data Bank (ScienceDB), which is a public, general-purpose data repository aiming to provide data services for researchers, research projects/teams, journals, institutions, universities, etc, the link is https://www.scidb.cn/en/detail?dataSetId=3f6204bc48704fac9b64b8e95a904e02 [22].</p>
<p>In the future, while continuously enhancing the field understanding ability of CataLM, we will also design and develop an auxiliary platform for field researchers based on it, in order to improve the efficiency of catalyst design work in practical applications.We believe that large language models will bring new and infinite possibilities to basic scientific research.</p>
<p>Fig. 1
1
Fig. 1 The training pipeline of CataLM.The bottom part illustrates the primary training pipeline of CataLM, while the top part of the figure delineates the entire data preparation process for training.</p>
<p>Fig. 2
2
Fig. 2 Catalytic Material Recommended Scenario's Command Format.</p>
<p>Fig. 3
3
Fig. 3 Prompt in the named entity recognition task.</p>
<p>Fig. 4
4
Fig. 4 Answer from CataLM and original LLM.</p>
<p>Table 1
1
The summary of the standard corpus
Entity TypeBenchmark CorpusMaterial1,092Control method1,086Product (including the second and1,340third product)Faradaic efficiency (including the1,135Faradaic efficiency of second and thirdproduct)Cell setup435Electrolyte475Synthesis method228Current density393Voltage801Total6,985</p>
<p>Table 2
2
Parameter set.
ParameterValuebatch size10learning rate3*10 −4lora r8lora alpha32lora dropout0.1</p>
<p>Table 3
3
The evaluation of entity recognition of CataLM.
EntityCount Correct Existence Modified Correct Modified AccuracyMATERIAL2017171575%CONTROL METHOD2019191365%PRODUCT2017171785%FARADAIC EFFICIENCY 2011111890%ELECTROLYTE2010101050%POTENTIAL20771680%CURRENT DENSITY20771260%CELL SETUP2066945%OVERALL160859411068.75%</p>
<p>Table 4
4
Results of ablation experiment
ModelCorrect Modified Correct Modified AccuracyOriginal LLM + Zero shot275936.88%Original LLM + Few shot376641.25%Fine-tuned LLM + Zero shot 498553.12%Our method8511068.75%</p>
<p>Table 5 :
5
The input and answer provided by orginal LLM and CataLM.
InputOriginal LLMCataLMC2H5OH,Singlemetal,structurecontrol
Competing InterestsThe authors declare no competing interests.</p>
<p>I Beltagy, K Lo, A Cohan, arXiv:1903.10676Scibert: A pretrained language model for scientific text. 2019arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery. H Cao, Z Liu, X Lu, Y Yao, Y Li, 2023</p>
<p>Large language model enhanced corpus of co2 reduction electrocatalysts and synthesis procedures. X Chen, Y Gao, L Wang, W Cui, J Huang, Y Du, B Wang, Scientific Data. 1113472024</p>
<p>Matchat: A large language model and application service platform for materials science. Z.-Y Chen, F.-K Xie, M Wan, Y Yuan, M Liu, Z.-G Wang, S Meng, Y.-G Wang, Chinese Physics B. 32111181042023</p>
<p>K Clark, M.-T Luong, Q V Le, C D Manning, arXiv:2003.10555Electra: Pretraining text encoders as discriminators rather than generators. 2020arXiv preprint</p>
<p>What would it take for renewably powered electrosynthesis to displace petrochemical processes?. P De Luna, C Hahn, D Higgins, S A Jaffer, T F Jaramillo, E H Sargent, Science. 364643835062019</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Autodive: An integrated onsite scientific literature annotation tool. Y Du, L Wang, M Huang, D Song, W Cui, Y Zhou, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational Linguistics20233System Demonstrations)</p>
<p>L Gao, S Biderman, S Black, L Golding, T Hoppe, C Foster, J Phang, H He, A Thite, N Nabeshima, arXiv:2101.00027The pile: An 800gb dataset of diverse text for language modeling. 2020arXiv preprint</p>
<p>Revisiting electrocatalyst design by a knowledge graph of cu-based catalysts for co 2 reduction. Y Gao, L Wang, X Chen, Y Du, B Wang, ACS Catalysis. 132023</p>
<p>Neural network training method for materials science based on multi-source databases. J Guo, Z Chen, Z Liu, X Li, Z Xie, Z Wang, Y Wang, Scientific Reports. 121153262022</p>
<p>Matscibert: A materials domain language model for text mining and information extraction. T Gupta, M Zaki, N A Krishnan, Mausam , Computational Materials. 811022022</p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, Lora: Low-rank adaptation of large language models. 2021</p>
<p>Commentary: The materials project: A materials genome approach to accelerating materials innovation. A Jain, S P Ong, G Hautier, W Chen, W D Richards, S Dacek, S Cholia, D Gunter, D Skinner, G Ceder, APL materials. 112013</p>
<p>Protein function prediction as approximate semantic entailment. M Kulmanov, F J Guzmán-Vega, P Duek Roggli, L Lane, S T Arold, R Hoehndorf, Nature Machine Intelligence. 2024</p>
<p>A universal model for accurately predicting the formation energy of inorganic compounds. Y Liang, M Chen, Y Wang, H Jia, T Lu, F Xie, G Cai, Z Wang, S Meng, M Liu, Science China Materials. 6612023</p>
<p>Progress and challenges toward the rational design of oxygen electrocatalysts based on a descriptor approach. J Liu, H Liu, H Chen, X Du, B Zhang, Z Hong, S Sun, W Wang, Advanced Science. 7119016142020</p>
<p>. R Liu, J Mckie, 2018Pymupdf. Available at</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Swarm intelligence for new materials. Z Liu, J Guo, Z Chen, Z Wang, Z Sun, X Li, Y Wang, Computational Materials Science. 2141116992022</p>
<p>The extended corpus of CO2 reduction electrocatalysts and synthesis procedures. Ludiwang, 2023</p>
<p>Biogpt: generative pre-trained transformer for biomedical text generation and mining. R Luo, L Sun, Y Xia, T Qin, S Zhang, H Poon, T.-Y Liu, Briefings in bioinformatics. 2364092022</p>
<p>Towards the computational design of solid catalysts. J K Nørskov, T Bligaard, J Rossmeisl, C H Christensen, Nature chemistry. 112009</p>
<p>Introducing chatgpt. A Open, 2022open ai</p>
<p>Gpt-4 technical report. R Openai, arxiv 2303.08774View in Article. 252023</p>
<p>A liver cancer questionanswering system based on next-generation intelligence and the large model medpalm 2. J Qian, Z Jin, Q Zhang, G Cai, B Liu, International Journal of Computer Science and Information Technology. 212024</p>
<p>Y Qin, S Liang, Y Ye, K Zhu, L Yan, Y Lu, Y Lin, X Cong, X Tang, B Qian, arXiv:2307.16789Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023arXiv preprint</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, Journal of machine learning research. 211402020</p>
<p>Materials design and discovery with high-throughput density functional theory: the open quantum materials database (oqmd). J E Saal, S Kirklin, M Aykol, B Meredig, C Wolverton, Jom. 652013</p>
<p>Combining theory and experiment in electrocatalysis: Insights into materials design. Z W Seh, J Kibsgaard, C F Dickens, I Chorkendorff, J K Nørskov, T F Jaramillo, Science. 355632149982017</p>
<p>A perovskite oxide optimized for oxygen evolution catalysis from molecular orbital principles. J Suntivich, K J May, H A Gasteiger, J B Goodenough, Y Shao-Horn, Science. 33460612011</p>
<p>A corpus of co2 electrocatalytic reduction process extracted from the scientific literature. L Wang, Y Gao, X Chen, W Cui, Y Zhou, X Luo, S Xu, Y Du, B Wang, Scientific Data. 1011752023</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Y Wang, Y Kordi, S Mishra, A Liu, N A Smith, D Khashabi, H Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>C Wu, X Zhang, Y Zhang, Y Wang, W Xie, arXiv:2304.14454Pmc-llama: Further finetuning llama on medical papers. 2023arXiv preprint</p>
<p>Lu-h-n phase diagram from first-principles calculations. F Xie, T Lu, Z Yu, Y Wang, Z Wang, S Meng, M Liu, Chinese Physics Letters. 405574012023a</p>
<p>Large language models as master key: Unlocking the secrets of materials science with gpt. T Xie, Y Wan, W Huang, Y Zhou, Y Liu, Q Linghu, S Wang, C Kit, C Grazian, W Zhang, B Hoex, 2023b</p>
<p>Doctorglm: Fine-tuning your chinese doctor is not a herculean task. H Xiong, S Wang, Y Zhu, Z Zhao, Y Liu, L Huang, Q Wang, D Shen, arXiv:2304.010972023arXiv preprint</p>
<p>H Zhang, J Chen, F Jiang, F Yu, Z Chen, J Li, G Chen, X Wu, Z Zhang, Q Xiao, arXiv:2305.15075Huatuogpt, towards taming language model to be a doctor. 2023arXiv preprint</p>
<p>Chatgpt chemistry assistant for text mining and prediction of mof synthesis. Z Zheng, O Zhang, C Borgs, J T Chayes, O M Yaghi, arXiv:2306.112962023aarXiv preprint</p>
<p>Chatgpt chemistry assistant for text mining and the prediction of mof synthesis. Z Zheng, O Zhang, C Borgs, J T Chayes, O M Yaghi, 37548379Journal of the American Chemical Society. 145322023b</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3030 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3030</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3030</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7" target="_blank">PAL: Program-aided Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This paper presents Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("few-shot prompting"). Much of this success can be attributed to prompting methods such as"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3030.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3030.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program-aided Language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/method that requires the LLM to generate interleaved natural-language and programmatic reasoning steps (Python code), and obtains the final answer by executing the generated program with an external interpreter, thereby delegating arithmetic execution to the interpreter.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (Codex) primarily; also evaluated with code-davinci-001, code-cushman-001, text-davinci-002/003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained language models with code-capabilities used as the LLM backbone (paper uses Codex code-davinci-002 as primary backend); exact architecture/size not specified in this paper, trained on natural language + source code corpora as referenced.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Grade-school math word problems (GSM8K, GSM-HARD, SVAMP, ASDIV, MAWPS variants), multi-step arithmetic, large-number arithmetic (GSM-HARD), algorithmic tasks (REPEAT COPY), object counting and symbolic reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>The LLM performs decomposition and program synthesis (mapping NL to program variables/statements). Arithmetic itself is performed by the external Python interpreter; the LLM thus 'performs' arithmetic by producing correct program expressions rather than executing numeric computation internally.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Consistent empirical gains across 13 tasks when the LLM generates code and execution is done by Python (PAL) vs. chain-of-thought; ablations show (1) forcing the LLM to produce answers after generating code (simulating runtime) yields poor accuracy (23.2% vs PAL 72.0% on GSM8K), (2) succinct one-line expressions drop performance to near DIRECT (47.8%), (3) using prompts but not executing code (LLM executes) performs poorly, indicating the interpreter's execution is key; PAL remains robust to large numbers (GSM-HARD) compared to CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>PAL requires sufficiently code-capable LMs — with weaker code ability (text-davinci-001), CoT can outperform PAL; thus the benefit is contingent on LLM competence at generating correct programs. Also, correct final answer is guaranteed only if the generated program correctly represents the problem.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompt engineering: few-shot program-style reasoning prompts (NL comments + Python code snippets), and using an external Python interpreter to execute the generated code; multi-sample generation with majority voting (self-consistency) was also used.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Large improvements in arithmetic task accuracy and robustness: PAL (Codex) outperforms CoT (Codex) and much larger models (e.g., PaLM-540B CoT) on multiple datasets; PAL reduces arithmetic failures on large numbers (smaller drop on GSM-HARD vs CoT) and yields near-perfect on many symbolic/algorithmic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples (few-shot, greedy unless noted): GSM8K: PAL (Codex) 72.0% single-sample, 80.4% majority@40; CoT (Codex) 65.6%; PaLM-540B CoT 56.9% (single). GSM-HARD: PAL 61.2% vs CoT (Codex) 23.3%. SVAMP: PAL 79.4% vs CoT (Codex) 74.8%. Many symbolic/algorithmic tasks: Colored Objects PAL 95.1% vs CoT 86.3%; Object Counting PAL 96.7% vs CoT 73.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Requires LLM to generate correct program structure and correct variable grounding; poor program-generation ability (or uninformative variable names) reduces performance drastically; PAL fails when LLM produces incorrect program code (logical errors) even though interpreter would compute correctly if code were right.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Execution by the Python interpreter is treated as ground-truth symbolic computation: final answers are 'guaranteed' correct by construction if the generated program matches the intended computation; PAL outperforms larger pure-neural models (PaLM-540B, Minerva) on benchmarks, indicating synergy of neural decomposition + symbolic execution beats monolithic neural arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3030.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3030.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that augments in-context examples with free-form natural-language intermediate steps (reasoning chains) that the LLM is expected to emulate at test time, producing both the reasoning trace and final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied with multiple LLMs in the paper: Codex (code-davinci-002), PaLM-540B, Minerva-540B, LaMDA-137B, UL2-20B, text-davinci-family</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large LMs (various sizes); specific architectures/sizes depend on referenced models (e.g., PaLM 540B). In this paper CoT is a baseline prompting approach applied to those models.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same benchmarks as PAL: math word problems, multi-step arithmetic, large-number variants, symbolic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>LLMs internally generate natural-language step-by-step reasoning and are expected to carry out arithmetic implicitly in token generation; arithmetic thus depends on the LM's internal token-level computation patterns rather than explicit programmatic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>CoT improves over DIRECT prompting on many tasks (e.g., CoT Codex 65.6% vs DIRECT Codex 19.7% on GSM8K), indicating that eliciting intermediate natural-language reasoning helps. Token-level analysis shows CoT has lower confidence on numeric tokens and grounded quantitative language.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>CoT models often produce correct decomposition but incorrect numerical computation—on GSM-HARD the same CoT NL reasoning is produced for small-number and large-number variants while arithmetic fails, implying internal execution is unreliable for arithmetic/large numbers. CoT performance degrades sharply with larger numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompt engineering: include NL reasoning traces in few-shot examples; multi-sample majority voting/self-consistency can be used.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>CoT substantially increases accuracy versus direct prompting in many settings but still underperforms PAL on hard arithmetic and large-number tasks; majority voting (40 samples) further improves CoT but PAL + sampling still outperforms some larger-model CoT baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples: CoT (Codex) GSM8K 65.6% (single); CoT (PaLM-540B) reported 56.9% on GSM8K (single) in table; on GSM-HARD CoT drops to ~23.3% (Codex prompts with large numbers gave only modest gains).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Systematic arithmetic errors (miscalculation), instability over longer/more complex inputs (accuracy drops as complexity grows), low confidence on numeric tokens; consistent NL reasoning but incorrect numeric execution especially with large numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>CoT imitates human-style verbalized reasoning but lacks the exactness of symbolic computation; compared to Python interpreter in PAL, CoT's internal arithmetic is less reliable and not 'guaranteed' correct.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3030.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3030.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DIRECT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard few-shot prompting that provides input-output examples containing only the immediate question and final answer (no intermediate reasoning steps).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (Codex) and other LMs in baselines</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to same LLM backbones as baselines; direct prompting supplies examples of question-answer pairs only.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Math word problems and symbolic tasks as in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>The model attempts to map question to final answer directly, relying on pattern-matching and memorized associations; no explicit intermediate computation is elicited.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>DIRECT often yields low accuracy on multi-step arithmetic tasks (e.g., DIRECT Codex GSM8K 19.7%), indicating inability to reliably perform multi-step arithmetic without elicited intermediate structure.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None specific; DIRECT is inferior on multi-step arithmetic but can occasionally beat CoT on specific algorithmic tasks (REPEAT COPY) in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>None beyond few-shot example choice; used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Low performance on multi-step math tasks compared to CoT and PAL; sometimes better than CoT on certain algorithmic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Example: DIRECT (Codex) GSM8K 19.7%; GSM-HARD 5.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Fails on multi-step arithmetic and complex reasoning; large drop with larger numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No explicit comparison; performs worse than symbolic execution via interpreter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3030.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3030.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-capable large language model variant used as the primary backbone in experiments; strong at generating Python programs and code-style decompositions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Large Language Models Trained on Code.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (Codex)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM trained on natural language and large corpora of source code (paper uses code-davinci-002 as the main model for PAL, COT and DIRECT experiments); exact model size not specified in this paper but described as 'code LLM'.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same as PAL experiments: math word problems, GSM-HARD, SVAMP, algorithmic tasks, symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Codex can map NL problems to programmatic code that, when executed, performs exact arithmetic; it also can perform some arithmetic token-by-token in CoT, but with more errors than interpreter execution.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical results: CoT with Codex performs well (e.g., GSM8K 65.6%), but when Codex is used to generate code and interpreter executes it (PAL), performance improves (GSM8K PAL 72.0%), showing Codex's program-generation ability enables correct arithmetic via execution.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Although Codex is good at code generation, PAL's benefits depend on meaningful variable names and code competence; weaker Codex variants (or code-less LMs) show smaller or negative gains.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompting to generate Python code (PAL) and execution via external interpreter; ablations altering variable names/comments.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Using Codex in PAL yields state-of-the-art few-shot performance on multiple benchmarks vs larger models using CoT; ablations show code generation + interpreter is crucial.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CoT (Codex) GSM8K 65.6% (single); PAL (Codex) GSM8K 72.0% (single) and 80.4% (majority@40); CoT (Codex) SVAMP 74.8% vs PAL 79.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>If prompts use uninformative variable names or lack NL comments, Codex's program-generation quality drops and so does task accuracy (PAL -var 59.0% avg vs PAL 71.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>When paired with interpreter, Codex-generated programs produce results equivalent to symbolic computation; corresponds to neural decomposition + symbolic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3030.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3030.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-540B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM (540B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A very large transformer language model used as a strong baseline for chain-of-thought prompting comparisons in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PaLM: Scaling Language Modeling with Pathways.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>540-billion-parameter transformer model (as cited) used in prior work for CoT baselines; specific training details are in the PaLM reference.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Math word problems and reasoning benchmarks (GSM8K, SVAMP, etc.) compared as CoT baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Per prior work, CoT elicits internal multi-step natural-language reasoning in PaLM; arithmetic is carried out by the model's internal token generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Published CoT results from prior work show PaLM-540B with CoT attains substantial accuracy on GSM8K (reported 56.9% in table), but PAL with Codex outperforms PaLM-540B CoT on GSM8K in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>PaLM-540B still suffers from arithmetic and calculation errors as discussed in related literature; fine-tuning (Minerva) reduces some but PAL still outperforms on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>CoT prompting in referenced baselines; not directly modified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>CoT improves PaLM-540B over DIRECT, but in this paper PAL (with a code-LM+interpreter) surpasses PaLM-540B CoT on several math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in paper: CoT PaLM-540B GSM8K 56.9% (single) in table.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Incorrect reasoning and incorrect calculation remain common failure modes in large models including PaLM, as noted in the paper and cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Compared indirectly: PAL (Codex+interpreter) achieved higher accuracy than PaLM-540B CoT, suggesting modular neural+symbolic setups can outperform larger monolithic models on arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3030.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3030.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSM-HARD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GSM-HARD (hard variant of GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A harder version of the GSM8K math word-problem dataset created by replacing one number in each question with a random integer up to 7 digits to test generalization to large numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>evaluated with Codex and other LMs under CoT and PAL prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset modification described in paper; not a model.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step arithmetic with large integers (up to 7 digits), probing arithmetic generalization and large-number handling.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Designed to separate failures due to arithmetic execution (large numbers) vs reasoning confusion; tests whether LLM internal arithmetic scales to large magnitude values.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>CoT outputs nearly identical NL reasoning for small-number and large-number versions in many cases, but numeric answers fail, suggesting that arithmetic execution (not reasoning decomposition) is the primary failure; PAL degrades much less on GSM-HARD (PAL 61.2% vs CoT 23.3%), supporting the claim.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None direct; shows LLM internal computation struggles with magnitude while decomposition generalizes.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Dataset-level intervention (harder numeric values) used to stress-test arithmetic capability.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Revealed that CoT performance collapses on large numbers while PAL remains relatively robust because arithmetic is handled by interpreter.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GSM-HARD results: DIRECT 5.0% (Codex), CoT (Codex) ~23.1-23.3%, PAL (Codex) 61.2% (table), drop for PAL only ~14.3% relative vs standard GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>LLM internal calculation fails on large magnitude numbers even when decomposition is correct.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Compared implicitly: symbolic execution (interpreter) handles large numbers exactly, whereas LLM token-generation arithmetic does not.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3030.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3030.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Python interpreter (external execution)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>External Python interpreter as a solver/runtime</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external deterministic runtime used to execute the generated Python code from the LLM and produce exact arithmetic results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>not a model; it's the external solver component used in PAL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard Python interpreter used post-hoc to execute generated programs and return numeric results; exact interpreter version not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>All arithmetic types that can be represented and computed in Python: integer and floating arithmetic, list indexing, loops, dictionary operations, datetime arithmetic, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Performs arithmetic symbolically/numerically as a conventional interpreter: deterministic computation that is exact for integers and standard floating behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation where the LLM must simulate runtime (generate final answer rather than executing code) performed much worse (23.2% vs PAL 72.0%), demonstrating execution is a primary source of PAL's gains.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None; interpreter is assumed correct given correctly generated code. The interpreter cannot help if the generated program is logically incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Tool use: external calculator/execution environment used at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Converts uncertain numeric token generation into exact computed results; yields major accuracy gains and robustness to large numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Enables PAL's performance numbers above (e.g., GSM8K PAL 72.0% single, 80.4% majority@40) by executing generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Interpreter cannot salvage logically incorrect or mis-specified programs; reliance on syntactically/semantically correct program generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Interpreter is the symbolic execution baseline; when program is correct it produces exact answers unlike LLM internal arithmetic, effectively outperforming purely neural reasoning on numeric accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3030.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3030.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Succinct Code ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Succinct single-line code ablation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation where the few-shot code examples are reduced to single-line expressions (succinct code) instead of multi-step programmatic decomposition, tested to see if multi-step program structure is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (Codex) in ablation experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Ablation variant of PAL prompts with single-line return expressions rather than stepwise program decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Math word problems (e.g., GSM8K style) represented as a single expression.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Tests whether LLM+interpreter benefits come from explicit multi-step program structure or merely from having an expression to execute.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical result: Succinct Code version performance falls to 47.8% on GSM8K (Table 6), showing multi-step, structured programmatic reasoning in prompts aids model performance beyond simple single-line code.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Shows that single-line code is insufficient — contradicts any claim that only having an executable expression is enough.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompt format ablation (structured multi-line code vs. single-line expression).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Substantially reduced accuracy relative to full PAL, indicating structured decomposition in code prompts helps the LLM generate correct programs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GSM8K succinct-code ablation solve rate 47.8% (vs PAL 72.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Single-line expressions appear harder for the LLM to generate reliably for complex word problems; they reduce grounding and traceability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Single-line expressions are closer to direct computation but lose the clarity of stepwise human-like decomposition; results show stepwise program structure yields better end-to-end performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3030.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3030.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-simulated-runtime ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Simulating Runtime (forced answer generation after code)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation in which the LLM is asked to generate the reasoning program but then also to produce the final numeric answer itself (i.e., the LLM simulates executing the code rather than using a real interpreter).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (Codex) in ablation</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Variant of PAL prompt where execution is simulated by the LLM producing the numeric output token rather than executing the code.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Math word problems (GSM8K and variants).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Tests whether the benefit of PAL originates from program-structured prompts alone or from actual external execution.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical result: LLM Simulating Runtime achieves only 23.2% on GSM8K (Table 6), close to DIRECT, indicating that real execution is the primary source of PAL's arithmetic reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Shows that program-structured prompts without interpreter execution do not meaningfully improve arithmetic accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompting ablation forcing LLM to compute the result.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Performance collapsed to near-direct levels, demonstrating interpreter execution is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GSM8K LLM-simulated-runtime 23.2% vs PAL 72.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>LLM fails to reliably simulate numeric execution even when given programmatic steps; internal arithmetic remains unreliable.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Shows that replacing symbolic execution (interpreter) with LLM 'mental execution' does not match deterministic symbolic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3030.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e3030.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Variable name / comment ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablations on informative variable names and NL comments in PAL prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Controlled experiments that remove NL intermediate comments and/or replace meaningful variable names with random identifiers to test the role of textual grounding in program generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (Codex) and other code-capable LMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Ablation variants of PAL prompt formatting: (1) PAL without comments, (2) PAL without comments and with randomized variable names.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Math word problems and symbolic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Examines whether textual grounding (meaningful names/comments) is used by the LLM to map natural-language entities to program variables, aiding correct program generation and thereby accurate arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Results show meaningful variable names are critical: PAL (full) average ~71.8% vs PAL -var (uninformative names) 59.0%; PAL -var + comms (random var names but useful comments) 69.0% (Table 4), indicating both names and comments aid program grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None; ablation demonstrates textual cues improve program generation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompt engineering ablations altering variable naming and comment presence.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Removing meaningful names degrades performance significantly; adding comments recovers some of the performance, showing LLM relies on human-readable identifiers and comments to map NL entities to code constructs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>PAL avg 71.8% vs PAL -var 59.0% vs PAL -var + comms 69.0% (aggregate math ablation table).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Randomized variable names cause decreased grounding, more program-generation errors and lower arithmetic accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Shows human-readable variable names help LLMs in program induction, unlike symbolic interpreters which are indifferent to variable identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3030.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e3030.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Majority@40 / Multi-sample (self-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-sample generation with majority voting (self-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time intervention that draws many stochastic samples (k=40, nucleus sampling p=0.95) and selects the final answer by majority voting among execution results to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (Codex) under PAL; comparisons to Minerva/PaLM sampling in literature</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sampling-based inference regime for LLMs; the paper follows prior work's sampling parameters (k=40, p=0.95, temperature=0.7) for ensemble-style selection.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Math word problems (GSM8K) and similar reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Averaging/majority among diverse sampled reasoning/program generations reduces single-sample hallucinations and increases the chance of at least one correct program being produced and agreed upon.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>PAL accuracy on GSM8K improves from 72.0% (single greedy) to 80.4% using majority@40 sampling; this matches or exceeds Minerva performance with similar sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Sampling increases compute and requires multiple program generations; not a counter-evidence but a practical trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Stochastic sampling + majority voting over executed outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Substantial accuracy improvements (example: GSM8K PAL 72.0% -> 80.4% at k=40).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GSM8K PAL 80.4% majority@40 vs 72.0% single-sample.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Majority voting depends on diversity and independence of samples; if many samples share the same systematic error, majority voting won't help.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Analogous to ensemble/verifier strategies; when combined with deterministic interpreter, sampling increases likelihood of correct symbolic program being generated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3030.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e3030.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>External calculator mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>External calculator / specialized modules (prior works)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior approaches that attach external calculators or specialized modules to LMs for arithmetic (e.g., calculators or date modules) referenced in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training Verifiers to Solve Math Word Problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various prior systems (not directly used in current experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Previous studies attached small specialized tools (calculator, date handlers) or post-hoc arithmetic modules to language models to improve numeric accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic operations, date arithmetic, small calculators for numeric evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Offloading arithmetic to specialized tools/calculators to avoid LLM numeric failures; similar conceptually to PAL but often narrower in scope.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Referenced prior results show modest improvements (e.g., calculator improved Codex by 2.3% on GSM8K per Chowdhery et al.), while PAL provides larger gains by generating general Python code.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Prior calculator additions produced only small absolute improvements on GSM8K compared to PAL, and were task-specific rather than general programmatic solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Tool use: external calculators or modules in prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Small-to-moderate accuracy gains reported in cited works; PAL's general interpreter approach outperforms some of these specialized-tool approaches on benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited example: calculator improved Codex by 2.3% absolute on GSM8K (from referenced prior work); PAL improved Codex by ~6.4% on same benchmark in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Specialized tools require ad-hoc engineering per operation and may not generalize to arbitrary symbolic tasks; limited scope vs general Python execution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Specialized calculators are symbolic tools; PAL generalizes this idea by having LLM generate general-purpose code rather than calling a narrow calculator API.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models. <em>(Rating: 2)</em></li>
                <li>Evaluating Large Language Models Trained on Code. <em>(Rating: 2)</em></li>
                <li>PaLM: Scaling Language Modeling with Pathways. <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models. <em>(Rating: 1)</em></li>
                <li>Training Verifiers to Solve Math Word Problems. <em>(Rating: 1)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3030",
    "paper_id": "paper-6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "PAL",
            "name_full": "Program-aided Language models",
            "brief_description": "A prompting/method that requires the LLM to generate interleaved natural-language and programmatic reasoning steps (Python code), and obtains the final answer by executing the generated program with an external interpreter, thereby delegating arithmetic execution to the interpreter.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (Codex) primarily; also evaluated with code-davinci-001, code-cushman-001, text-davinci-002/003",
            "model_description": "Large pre-trained language models with code-capabilities used as the LLM backbone (paper uses Codex code-davinci-002 as primary backend); exact architecture/size not specified in this paper, trained on natural language + source code corpora as referenced.",
            "arithmetic_task_type": "Grade-school math word problems (GSM8K, GSM-HARD, SVAMP, ASDIV, MAWPS variants), multi-step arithmetic, large-number arithmetic (GSM-HARD), algorithmic tasks (REPEAT COPY), object counting and symbolic reasoning tasks.",
            "reported_mechanism": "The LLM performs decomposition and program synthesis (mapping NL to program variables/statements). Arithmetic itself is performed by the external Python interpreter; the LLM thus 'performs' arithmetic by producing correct program expressions rather than executing numeric computation internally.",
            "evidence_for_mechanism": "Consistent empirical gains across 13 tasks when the LLM generates code and execution is done by Python (PAL) vs. chain-of-thought; ablations show (1) forcing the LLM to produce answers after generating code (simulating runtime) yields poor accuracy (23.2% vs PAL 72.0% on GSM8K), (2) succinct one-line expressions drop performance to near DIRECT (47.8%), (3) using prompts but not executing code (LLM executes) performs poorly, indicating the interpreter's execution is key; PAL remains robust to large numbers (GSM-HARD) compared to CoT.",
            "evidence_against_mechanism": "PAL requires sufficiently code-capable LMs — with weaker code ability (text-davinci-001), CoT can outperform PAL; thus the benefit is contingent on LLM competence at generating correct programs. Also, correct final answer is guaranteed only if the generated program correctly represents the problem.",
            "intervention_type": "Prompt engineering: few-shot program-style reasoning prompts (NL comments + Python code snippets), and using an external Python interpreter to execute the generated code; multi-sample generation with majority voting (self-consistency) was also used.",
            "effect_of_intervention": "Large improvements in arithmetic task accuracy and robustness: PAL (Codex) outperforms CoT (Codex) and much larger models (e.g., PaLM-540B CoT) on multiple datasets; PAL reduces arithmetic failures on large numbers (smaller drop on GSM-HARD vs CoT) and yields near-perfect on many symbolic/algorithmic tasks.",
            "performance_metrics": "Examples (few-shot, greedy unless noted): GSM8K: PAL (Codex) 72.0% single-sample, 80.4% majority@40; CoT (Codex) 65.6%; PaLM-540B CoT 56.9% (single). GSM-HARD: PAL 61.2% vs CoT (Codex) 23.3%. SVAMP: PAL 79.4% vs CoT (Codex) 74.8%. Many symbolic/algorithmic tasks: Colored Objects PAL 95.1% vs CoT 86.3%; Object Counting PAL 96.7% vs CoT 73.0%.",
            "notable_failure_modes": "Requires LLM to generate correct program structure and correct variable grounding; poor program-generation ability (or uninformative variable names) reduces performance drastically; PAL fails when LLM produces incorrect program code (logical errors) even though interpreter would compute correctly if code were right.",
            "comparison_to_humans_or_symbolic": "Execution by the Python interpreter is treated as ground-truth symbolic computation: final answers are 'guaranteed' correct by construction if the generated program matches the intended computation; PAL outperforms larger pure-neural models (PaLM-540B, Minerva) on benchmarks, indicating synergy of neural decomposition + symbolic execution beats monolithic neural arithmetic reasoning.",
            "uuid": "e3030.0",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting method that augments in-context examples with free-form natural-language intermediate steps (reasoning chains) that the LLM is expected to emulate at test time, producing both the reasoning trace and final answer.",
            "citation_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models.",
            "mention_or_use": "use",
            "model_name": "applied with multiple LLMs in the paper: Codex (code-davinci-002), PaLM-540B, Minerva-540B, LaMDA-137B, UL2-20B, text-davinci-family",
            "model_description": "Large LMs (various sizes); specific architectures/sizes depend on referenced models (e.g., PaLM 540B). In this paper CoT is a baseline prompting approach applied to those models.",
            "arithmetic_task_type": "Same benchmarks as PAL: math word problems, multi-step arithmetic, large-number variants, symbolic tasks.",
            "reported_mechanism": "LLMs internally generate natural-language step-by-step reasoning and are expected to carry out arithmetic implicitly in token generation; arithmetic thus depends on the LM's internal token-level computation patterns rather than explicit programmatic execution.",
            "evidence_for_mechanism": "CoT improves over DIRECT prompting on many tasks (e.g., CoT Codex 65.6% vs DIRECT Codex 19.7% on GSM8K), indicating that eliciting intermediate natural-language reasoning helps. Token-level analysis shows CoT has lower confidence on numeric tokens and grounded quantitative language.",
            "evidence_against_mechanism": "CoT models often produce correct decomposition but incorrect numerical computation—on GSM-HARD the same CoT NL reasoning is produced for small-number and large-number variants while arithmetic fails, implying internal execution is unreliable for arithmetic/large numbers. CoT performance degrades sharply with larger numbers.",
            "intervention_type": "Prompt engineering: include NL reasoning traces in few-shot examples; multi-sample majority voting/self-consistency can be used.",
            "effect_of_intervention": "CoT substantially increases accuracy versus direct prompting in many settings but still underperforms PAL on hard arithmetic and large-number tasks; majority voting (40 samples) further improves CoT but PAL + sampling still outperforms some larger-model CoT baselines.",
            "performance_metrics": "Examples: CoT (Codex) GSM8K 65.6% (single); CoT (PaLM-540B) reported 56.9% on GSM8K (single) in table; on GSM-HARD CoT drops to ~23.3% (Codex prompts with large numbers gave only modest gains).",
            "notable_failure_modes": "Systematic arithmetic errors (miscalculation), instability over longer/more complex inputs (accuracy drops as complexity grows), low confidence on numeric tokens; consistent NL reasoning but incorrect numeric execution especially with large numbers.",
            "comparison_to_humans_or_symbolic": "CoT imitates human-style verbalized reasoning but lacks the exactness of symbolic computation; compared to Python interpreter in PAL, CoT's internal arithmetic is less reliable and not 'guaranteed' correct.",
            "uuid": "e3030.1",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "DIRECT",
            "name_full": "Direct prompting",
            "brief_description": "Standard few-shot prompting that provides input-output examples containing only the immediate question and final answer (no intermediate reasoning steps).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (Codex) and other LMs in baselines",
            "model_description": "Applied to same LLM backbones as baselines; direct prompting supplies examples of question-answer pairs only.",
            "arithmetic_task_type": "Math word problems and symbolic tasks as in paper.",
            "reported_mechanism": "The model attempts to map question to final answer directly, relying on pattern-matching and memorized associations; no explicit intermediate computation is elicited.",
            "evidence_for_mechanism": "DIRECT often yields low accuracy on multi-step arithmetic tasks (e.g., DIRECT Codex GSM8K 19.7%), indicating inability to reliably perform multi-step arithmetic without elicited intermediate structure.",
            "evidence_against_mechanism": "None specific; DIRECT is inferior on multi-step arithmetic but can occasionally beat CoT on specific algorithmic tasks (REPEAT COPY) in this paper.",
            "intervention_type": "None beyond few-shot example choice; used as baseline.",
            "effect_of_intervention": "Low performance on multi-step math tasks compared to CoT and PAL; sometimes better than CoT on certain algorithmic tasks.",
            "performance_metrics": "Example: DIRECT (Codex) GSM8K 19.7%; GSM-HARD 5.0%.",
            "notable_failure_modes": "Fails on multi-step arithmetic and complex reasoning; large drop with larger numbers.",
            "comparison_to_humans_or_symbolic": "No explicit comparison; performs worse than symbolic execution via interpreter.",
            "uuid": "e3030.2",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Codex",
            "name_full": "Codex (code-davinci-002)",
            "brief_description": "A code-capable large language model variant used as the primary backbone in experiments; strong at generating Python programs and code-style decompositions.",
            "citation_title": "Evaluating Large Language Models Trained on Code.",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (Codex)",
            "model_description": "LLM trained on natural language and large corpora of source code (paper uses code-davinci-002 as the main model for PAL, COT and DIRECT experiments); exact model size not specified in this paper but described as 'code LLM'.",
            "arithmetic_task_type": "Same as PAL experiments: math word problems, GSM-HARD, SVAMP, algorithmic tasks, symbolic reasoning.",
            "reported_mechanism": "Codex can map NL problems to programmatic code that, when executed, performs exact arithmetic; it also can perform some arithmetic token-by-token in CoT, but with more errors than interpreter execution.",
            "evidence_for_mechanism": "Empirical results: CoT with Codex performs well (e.g., GSM8K 65.6%), but when Codex is used to generate code and interpreter executes it (PAL), performance improves (GSM8K PAL 72.0%), showing Codex's program-generation ability enables correct arithmetic via execution.",
            "evidence_against_mechanism": "Although Codex is good at code generation, PAL's benefits depend on meaningful variable names and code competence; weaker Codex variants (or code-less LMs) show smaller or negative gains.",
            "intervention_type": "Prompting to generate Python code (PAL) and execution via external interpreter; ablations altering variable names/comments.",
            "effect_of_intervention": "Using Codex in PAL yields state-of-the-art few-shot performance on multiple benchmarks vs larger models using CoT; ablations show code generation + interpreter is crucial.",
            "performance_metrics": "CoT (Codex) GSM8K 65.6% (single); PAL (Codex) GSM8K 72.0% (single) and 80.4% (majority@40); CoT (Codex) SVAMP 74.8% vs PAL 79.4%.",
            "notable_failure_modes": "If prompts use uninformative variable names or lack NL comments, Codex's program-generation quality drops and so does task accuracy (PAL -var 59.0% avg vs PAL 71.8%).",
            "comparison_to_humans_or_symbolic": "When paired with interpreter, Codex-generated programs produce results equivalent to symbolic computation; corresponds to neural decomposition + symbolic execution.",
            "uuid": "e3030.3",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "PaLM-540B",
            "name_full": "PaLM (540B)",
            "brief_description": "A very large transformer language model used as a strong baseline for chain-of-thought prompting comparisons in the paper.",
            "citation_title": "PaLM: Scaling Language Modeling with Pathways.",
            "mention_or_use": "mention",
            "model_name": "PaLM-540B",
            "model_description": "540-billion-parameter transformer model (as cited) used in prior work for CoT baselines; specific training details are in the PaLM reference.",
            "arithmetic_task_type": "Math word problems and reasoning benchmarks (GSM8K, SVAMP, etc.) compared as CoT baselines.",
            "reported_mechanism": "Per prior work, CoT elicits internal multi-step natural-language reasoning in PaLM; arithmetic is carried out by the model's internal token generation.",
            "evidence_for_mechanism": "Published CoT results from prior work show PaLM-540B with CoT attains substantial accuracy on GSM8K (reported 56.9% in table), but PAL with Codex outperforms PaLM-540B CoT on GSM8K in this paper.",
            "evidence_against_mechanism": "PaLM-540B still suffers from arithmetic and calculation errors as discussed in related literature; fine-tuning (Minerva) reduces some but PAL still outperforms on many tasks.",
            "intervention_type": "CoT prompting in referenced baselines; not directly modified in this paper.",
            "effect_of_intervention": "CoT improves PaLM-540B over DIRECT, but in this paper PAL (with a code-LM+interpreter) surpasses PaLM-540B CoT on several math benchmarks.",
            "performance_metrics": "Reported in paper: CoT PaLM-540B GSM8K 56.9% (single) in table.",
            "notable_failure_modes": "Incorrect reasoning and incorrect calculation remain common failure modes in large models including PaLM, as noted in the paper and cited works.",
            "comparison_to_humans_or_symbolic": "Compared indirectly: PAL (Codex+interpreter) achieved higher accuracy than PaLM-540B CoT, suggesting modular neural+symbolic setups can outperform larger monolithic models on arithmetic.",
            "uuid": "e3030.4",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "GSM-HARD",
            "name_full": "GSM-HARD (hard variant of GSM8K)",
            "brief_description": "A harder version of the GSM8K math word-problem dataset created by replacing one number in each question with a random integer up to 7 digits to test generalization to large numbers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "evaluated with Codex and other LMs under CoT and PAL prompting",
            "model_description": "Dataset modification described in paper; not a model.",
            "arithmetic_task_type": "Multi-step arithmetic with large integers (up to 7 digits), probing arithmetic generalization and large-number handling.",
            "reported_mechanism": "Designed to separate failures due to arithmetic execution (large numbers) vs reasoning confusion; tests whether LLM internal arithmetic scales to large magnitude values.",
            "evidence_for_mechanism": "CoT outputs nearly identical NL reasoning for small-number and large-number versions in many cases, but numeric answers fail, suggesting that arithmetic execution (not reasoning decomposition) is the primary failure; PAL degrades much less on GSM-HARD (PAL 61.2% vs CoT 23.3%), supporting the claim.",
            "evidence_against_mechanism": "None direct; shows LLM internal computation struggles with magnitude while decomposition generalizes.",
            "intervention_type": "Dataset-level intervention (harder numeric values) used to stress-test arithmetic capability.",
            "effect_of_intervention": "Revealed that CoT performance collapses on large numbers while PAL remains relatively robust because arithmetic is handled by interpreter.",
            "performance_metrics": "GSM-HARD results: DIRECT 5.0% (Codex), CoT (Codex) ~23.1-23.3%, PAL (Codex) 61.2% (table), drop for PAL only ~14.3% relative vs standard GSM8K.",
            "notable_failure_modes": "LLM internal calculation fails on large magnitude numbers even when decomposition is correct.",
            "comparison_to_humans_or_symbolic": "Compared implicitly: symbolic execution (interpreter) handles large numbers exactly, whereas LLM token-generation arithmetic does not.",
            "uuid": "e3030.5",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Python interpreter (external execution)",
            "name_full": "External Python interpreter as a solver/runtime",
            "brief_description": "An external deterministic runtime used to execute the generated Python code from the LLM and produce exact arithmetic results.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "not a model; it's the external solver component used in PAL",
            "model_description": "Standard Python interpreter used post-hoc to execute generated programs and return numeric results; exact interpreter version not specified.",
            "arithmetic_task_type": "All arithmetic types that can be represented and computed in Python: integer and floating arithmetic, list indexing, loops, dictionary operations, datetime arithmetic, etc.",
            "reported_mechanism": "Performs arithmetic symbolically/numerically as a conventional interpreter: deterministic computation that is exact for integers and standard floating behavior.",
            "evidence_for_mechanism": "Ablation where the LLM must simulate runtime (generate final answer rather than executing code) performed much worse (23.2% vs PAL 72.0%), demonstrating execution is a primary source of PAL's gains.",
            "evidence_against_mechanism": "None; interpreter is assumed correct given correctly generated code. The interpreter cannot help if the generated program is logically incorrect.",
            "intervention_type": "Tool use: external calculator/execution environment used at inference time.",
            "effect_of_intervention": "Converts uncertain numeric token generation into exact computed results; yields major accuracy gains and robustness to large numbers.",
            "performance_metrics": "Enables PAL's performance numbers above (e.g., GSM8K PAL 72.0% single, 80.4% majority@40) by executing generated code.",
            "notable_failure_modes": "Interpreter cannot salvage logically incorrect or mis-specified programs; reliance on syntactically/semantically correct program generation.",
            "comparison_to_humans_or_symbolic": "Interpreter is the symbolic execution baseline; when program is correct it produces exact answers unlike LLM internal arithmetic, effectively outperforming purely neural reasoning on numeric accuracy.",
            "uuid": "e3030.6",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Succinct Code ablation",
            "name_full": "Succinct single-line code ablation",
            "brief_description": "An ablation where the few-shot code examples are reduced to single-line expressions (succinct code) instead of multi-step programmatic decomposition, tested to see if multi-step program structure is necessary.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (Codex) in ablation experiments",
            "model_description": "Ablation variant of PAL prompts with single-line return expressions rather than stepwise program decomposition.",
            "arithmetic_task_type": "Math word problems (e.g., GSM8K style) represented as a single expression.",
            "reported_mechanism": "Tests whether LLM+interpreter benefits come from explicit multi-step program structure or merely from having an expression to execute.",
            "evidence_for_mechanism": "Empirical result: Succinct Code version performance falls to 47.8% on GSM8K (Table 6), showing multi-step, structured programmatic reasoning in prompts aids model performance beyond simple single-line code.",
            "evidence_against_mechanism": "Shows that single-line code is insufficient — contradicts any claim that only having an executable expression is enough.",
            "intervention_type": "Prompt format ablation (structured multi-line code vs. single-line expression).",
            "effect_of_intervention": "Substantially reduced accuracy relative to full PAL, indicating structured decomposition in code prompts helps the LLM generate correct programs.",
            "performance_metrics": "GSM8K succinct-code ablation solve rate 47.8% (vs PAL 72.0%).",
            "notable_failure_modes": "Single-line expressions appear harder for the LLM to generate reliably for complex word problems; they reduce grounding and traceability.",
            "comparison_to_humans_or_symbolic": "Single-line expressions are closer to direct computation but lose the clarity of stepwise human-like decomposition; results show stepwise program structure yields better end-to-end performance.",
            "uuid": "e3030.7",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "LLM-simulated-runtime ablation",
            "name_full": "LLM Simulating Runtime (forced answer generation after code)",
            "brief_description": "An ablation in which the LLM is asked to generate the reasoning program but then also to produce the final numeric answer itself (i.e., the LLM simulates executing the code rather than using a real interpreter).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (Codex) in ablation",
            "model_description": "Variant of PAL prompt where execution is simulated by the LLM producing the numeric output token rather than executing the code.",
            "arithmetic_task_type": "Math word problems (GSM8K and variants).",
            "reported_mechanism": "Tests whether the benefit of PAL originates from program-structured prompts alone or from actual external execution.",
            "evidence_for_mechanism": "Empirical result: LLM Simulating Runtime achieves only 23.2% on GSM8K (Table 6), close to DIRECT, indicating that real execution is the primary source of PAL's arithmetic reliability.",
            "evidence_against_mechanism": "Shows that program-structured prompts without interpreter execution do not meaningfully improve arithmetic accuracy.",
            "intervention_type": "Prompting ablation forcing LLM to compute the result.",
            "effect_of_intervention": "Performance collapsed to near-direct levels, demonstrating interpreter execution is necessary.",
            "performance_metrics": "GSM8K LLM-simulated-runtime 23.2% vs PAL 72.0%.",
            "notable_failure_modes": "LLM fails to reliably simulate numeric execution even when given programmatic steps; internal arithmetic remains unreliable.",
            "comparison_to_humans_or_symbolic": "Shows that replacing symbolic execution (interpreter) with LLM 'mental execution' does not match deterministic symbolic computation.",
            "uuid": "e3030.8",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Variable name / comment ablation",
            "name_full": "Ablations on informative variable names and NL comments in PAL prompts",
            "brief_description": "Controlled experiments that remove NL intermediate comments and/or replace meaningful variable names with random identifiers to test the role of textual grounding in program generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (Codex) and other code-capable LMs",
            "model_description": "Ablation variants of PAL prompt formatting: (1) PAL without comments, (2) PAL without comments and with randomized variable names.",
            "arithmetic_task_type": "Math word problems and symbolic tasks.",
            "reported_mechanism": "Examines whether textual grounding (meaningful names/comments) is used by the LLM to map natural-language entities to program variables, aiding correct program generation and thereby accurate arithmetic.",
            "evidence_for_mechanism": "Results show meaningful variable names are critical: PAL (full) average ~71.8% vs PAL -var (uninformative names) 59.0%; PAL -var + comms (random var names but useful comments) 69.0% (Table 4), indicating both names and comments aid program grounding.",
            "evidence_against_mechanism": "None; ablation demonstrates textual cues improve program generation.",
            "intervention_type": "Prompt engineering ablations altering variable naming and comment presence.",
            "effect_of_intervention": "Removing meaningful names degrades performance significantly; adding comments recovers some of the performance, showing LLM relies on human-readable identifiers and comments to map NL entities to code constructs.",
            "performance_metrics": "PAL avg 71.8% vs PAL -var 59.0% vs PAL -var + comms 69.0% (aggregate math ablation table).",
            "notable_failure_modes": "Randomized variable names cause decreased grounding, more program-generation errors and lower arithmetic accuracy.",
            "comparison_to_humans_or_symbolic": "Shows human-readable variable names help LLMs in program induction, unlike symbolic interpreters which are indifferent to variable identifiers.",
            "uuid": "e3030.9",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Majority@40 / Multi-sample (self-consistency)",
            "name_full": "Multi-sample generation with majority voting (self-consistency)",
            "brief_description": "An inference-time intervention that draws many stochastic samples (k=40, nucleus sampling p=0.95) and selects the final answer by majority voting among execution results to improve robustness.",
            "citation_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models.",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (Codex) under PAL; comparisons to Minerva/PaLM sampling in literature",
            "model_description": "Sampling-based inference regime for LLMs; the paper follows prior work's sampling parameters (k=40, p=0.95, temperature=0.7) for ensemble-style selection.",
            "arithmetic_task_type": "Math word problems (GSM8K) and similar reasoning tasks.",
            "reported_mechanism": "Averaging/majority among diverse sampled reasoning/program generations reduces single-sample hallucinations and increases the chance of at least one correct program being produced and agreed upon.",
            "evidence_for_mechanism": "PAL accuracy on GSM8K improves from 72.0% (single greedy) to 80.4% using majority@40 sampling; this matches or exceeds Minerva performance with similar sampling.",
            "evidence_against_mechanism": "Sampling increases compute and requires multiple program generations; not a counter-evidence but a practical trade-off.",
            "intervention_type": "Stochastic sampling + majority voting over executed outputs.",
            "effect_of_intervention": "Substantial accuracy improvements (example: GSM8K PAL 72.0% -&gt; 80.4% at k=40).",
            "performance_metrics": "GSM8K PAL 80.4% majority@40 vs 72.0% single-sample.",
            "notable_failure_modes": "Majority voting depends on diversity and independence of samples; if many samples share the same systematic error, majority voting won't help.",
            "comparison_to_humans_or_symbolic": "Analogous to ensemble/verifier strategies; when combined with deterministic interpreter, sampling increases likelihood of correct symbolic program being generated.",
            "uuid": "e3030.10",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "External calculator mention",
            "name_full": "External calculator / specialized modules (prior works)",
            "brief_description": "Prior approaches that attach external calculators or specialized modules to LMs for arithmetic (e.g., calculators or date modules) referenced in related work.",
            "citation_title": "Training Verifiers to Solve Math Word Problems.",
            "mention_or_use": "mention",
            "model_name": "various prior systems (not directly used in current experiments)",
            "model_description": "Previous studies attached small specialized tools (calculator, date handlers) or post-hoc arithmetic modules to language models to improve numeric accuracy.",
            "arithmetic_task_type": "Arithmetic operations, date arithmetic, small calculators for numeric evaluation.",
            "reported_mechanism": "Offloading arithmetic to specialized tools/calculators to avoid LLM numeric failures; similar conceptually to PAL but often narrower in scope.",
            "evidence_for_mechanism": "Referenced prior results show modest improvements (e.g., calculator improved Codex by 2.3% on GSM8K per Chowdhery et al.), while PAL provides larger gains by generating general Python code.",
            "evidence_against_mechanism": "Prior calculator additions produced only small absolute improvements on GSM8K compared to PAL, and were task-specific rather than general programmatic solutions.",
            "intervention_type": "Tool use: external calculators or modules in prior literature.",
            "effect_of_intervention": "Small-to-moderate accuracy gains reported in cited works; PAL's general interpreter approach outperforms some of these specialized-tool approaches on benchmarks.",
            "performance_metrics": "Cited example: calculator improved Codex by 2.3% absolute on GSM8K (from referenced prior work); PAL improved Codex by ~6.4% on same benchmark in this paper.",
            "notable_failure_modes": "Specialized tools require ad-hoc engineering per operation and may not generalize to arbitrary symbolic tasks; limited scope vs general Python execution.",
            "comparison_to_humans_or_symbolic": "Specialized calculators are symbolic tools; PAL generalizes this idea by having LLM generate general-purpose code rather than calling a narrow calculator API.",
            "uuid": "e3030.11",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models.",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Evaluating Large Language Models Trained on Code.",
            "rating": 2,
            "sanitized_title": "evaluating_large_language_models_trained_on_code"
        },
        {
            "paper_title": "PaLM: Scaling Language Modeling with Pathways.",
            "rating": 2,
            "sanitized_title": "palm_scaling_language_modeling_with_pathways"
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models.",
            "rating": 1,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models"
        },
        {
            "paper_title": "Training Verifiers to Solve Math Word Problems.",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        }
    ],
    "cost": 0.02125375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PAL: Program-aided Language Models</h1>
<p>Luyu Gao ${ }^{<em> 1}$ Aman Madaan ${ }^{</em> 1}$ Shuyan Zhou ${ }^{* 1}$ Uri Alon ${ }^{1}$ Pengfei Liu ${ }^{12}$ Yiming Yang ${ }^{1}$ Jamie Callan ${ }^{1}$ Graham Neubig ${ }^{12}$<br>{luyug, amadaan, shuyanzh, ualon, pliu3, yiming, callan, gneubig}@cs.cmu.edu</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("fewshot prompting"). Much of this success can be attributed to prompting methods such as "chain-of-thought", which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present ProgramAided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using CODEX achieves state-of-theart few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute $15 \%$ top1. Our code and data are publicly available at http://reasonwithpal.com.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1. Introduction</h2>
<p>Until as recently as two years ago, reasoning was considered to be one of the most significant challenges that large language models (LLMs) had not yet overcome (Marcus, 2018; 2020; Garcez \&amp; Lamb, 2020). Recently, LLMs have shown impressive success on a wide range of tasks, including commonsense (Wei et al., 2021; Sanh et al., 2021; Madaan et al., 2022), mathematical (Lewkowycz et al., 2022; Wu et al., 2022; Mishra et al., 2022), and symbolic reasoning (Yao et al., 2022; Ahn et al., 2022), using few-shot prompting (Brown et al., 2020).</p>
<p>This process has been accelerated by methods that require LLMs to generate their explicit reasoning steps, such as "chain-of-thought" (Wei et al., 2022), "scratchpads" (Nye et al., 2021), and "least-to-most" (Zhou et al., 2022) prompting. In particular, the widely used chain-of-thought (CoT) method presents the model with the explicit intermediate steps that are required to reach the final answer. Then, the model is expected to apply a similar decomposition to the actual test example, and consecutively reach an accurate final answer (Ling et al., 2017; Amini et al., 2019). Nevertheless, while LLMs can decompose natural language problems into steps and perform simple arithmetic operations, their performance falls dramatically when dealing with complex arithmetic (Hendrycks et al., 2021; Madaan \&amp; Yazdanbakhsh, 2022) or large numbers (Nogueira et al., 2021; Qian et al., 2022). In fact, even when fine-tuning a PaLM-based model on 164B tokens of explicit mathematical content, its two most common failures are reportedly "incorrect reasoning" and "incorrect calculation" (Lewkowycz et al., 2022).</p>
<p>In this paper, we propose Program-Aided Language model (PAL): a novel method that uses an LLM to read natural language problems and generate programs as reasoning steps, but offloads the solution step to a Python interpreter, as illustrated in Figure 1. This offloading leverages an LLM that can decompose a natural language problem into programmatic steps, which is fortunately available using contemporary state-of-the-art LLMs that are pre-trained on both natural language and programming languages (Brown et al., 2020; Chen et al., 2021a; Chowdhery et al., 2022). While natural language understanding and decomposition</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A diagram illustrating PAL: Given a mathematical reasoning question, Chain-of-thought (left) generates intermediate reasoning steps of free-form text. In contrast, Program-aided Language models (PAL, right) generate intermediate steps and Python code. This shifts the role of running the reasoning steps from the language model to the Python interpreter. The final answer is obtained by running the generated reasoning chain. Chain-of-thought reasoning is highlighted in blue; PAL steps are highlighted in gray and pink; the Python interpreter run is highlighted in black and green.
require LLMs, solving and reasoning can be done with the external solver. This bridges an important gap in chain-of-thought-like methods, where reasoning chains can be correct but produce an incorrect answer.</p>
<p>We demonstrate the effectiveness of PAL across $\mathbf{1 3}$ arithmetic and symbolic reasoning tasks. In all these tasks, PAL using Codex (Chen et al., 2021a) outperforms much larger models such as PaLM-540n using chain-of-thought prompting. For example, on the popular GSM8K benchmark, PAL achieves state-of-the-art accuracy, surpassing PaLM-540n with chain-of-thought by absolute $15 \%$ top1 accuracy. When the questions contain large numbers, a dataset we call GSM-HARD, PAL outperforms CoT by an absolute $40 \%$. We believe that this seamless synergy between a neural LLM and a symbolic interpreter is an essential step towards general and robust AI reasoners.</p>
<h2>2. Background: Few-shot Prompting</h2>
<p>Few-shot prompting leverages the strength of large-language models to solve a task with a set of $k$ examples that are provided as part of the test-time input (Brown et al., 2020; Liu et al., 2021; Chowdhery et al., 2022), where $k$ is usually a number in the low single digits. These input-output examples $\left{\left(x_{i}, y_{i}\right)\right}<em 1="1">{i=1}^{k}$ are concatenated in a prompt $p$ $\equiv\left\langle x</em>$. Note that such fewshot prompting does not modify the underlying LLM.} \cdot y_{1}\right\rangle |\left\langle x_{2} \cdot y_{2}\right\rangle | \ldots |\left\langle x_{k} \cdot y_{k}\right\rangle$, where "." denotes the concatenation of an input and output, and " $|$ " indicate the concatenation of different examples. During inference, a test instance $x_{\text {test }}$ is appended to the prompt, and $p | x_{\text {test }}$ is passed to the model which attempts to complete $p | x_{\text {test }}$, and thereby generate an answer $y_{\text {test }</p>
<p>Wei et al. (2022) additionally augment each in-context example with chain of thought (CoT) intermediate steps. Specifically, each in-context example in the CoT setup is a triplet $\left\langle x_{i}, t_{i}, y_{i}\right\rangle$, where $x_{i}$ and $y_{i}$ are input-output pair as before, and $t_{i}$ is a natural language description of the steps that are needed to arrive at the output $y_{i}$ from the input $x_{i}$. See Figure 1 for an example. With the additional "thoughts" $t_{i}$, the prompt is set to $p \equiv\left\langle x_{1} \cdot t_{1} \cdot y_{1}\right\rangle |\left\langle x_{2} \cdot t_{2} \cdot y_{2}\right\rangle | \ldots |\left\langle x_{k} \cdot t_{k} \cdot y_{k}\right\rangle$.
During inference, the new question $x_{\text {test }}$ is appended to the prompt as before and supplied to the LLM. Crucially, the model is tasked with generating both the thought $t_{\text {test }}$ and the final answer $y_{\text {test }}$. This approach of prompting the model to first generate a reasoning process $t_{\text {test }}$ improves the accuracy of the answer $y_{\text {test }}$ across a wide range of tasks (Wang et al., 2022a; Wei et al., 2022; Zhou et al., 2022; Wang et al., 2022b).</p>
<h2>3. Program-aided Language Models</h2>
<p>In a Program-aided Language model, we propose to generate the thoughts $t$ for a given natural language problem $x$ as interleaved natural language (NL) and programming language (PL) statements. Since we delegate the solution step to an interpreter, we do not provide the final answers to the examples in our prompt. That is, every in-context example in PAL is a pair $\left\langle x_{i}, t_{i}\right\rangle$, where $t_{j}=\left[s_{1}, s_{2}, \ldots, s_{N}\right]$ with each $s_{i} \in \mathrm{NL} \cup \mathrm{PL}$, a sequence of tokens in either NL or PL. The complete prompt is thus $p$ $\equiv\left\langle x_{1} \cdot t_{1}\right\rangle |\left\langle x_{2} \cdot t_{2}\right\rangle | \ldots |\left\langle x_{k} \cdot t_{k}\right\rangle$.
Given a test instance $x_{\text {test }}$, we append it to the prompt, and $p | x_{\text {test }}$ is fed to the LM. We let the LM generate a prediction $t_{\text {test }}$, which contains both the intermediate steps and their corresponding programmatic statements.</p>
<div class="codehilite"><pre><span></span><code><span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">Roger</span><span class="w"> </span><span class="n">started</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="n">tennis</span><span class="w"> </span><span class="n">balls</span>
<span class="n">tennis_balls</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">5</span>
<span class="mi">2</span><span class="o">}\</span><span class="n">mathrm</span><span class="o">{</span><span class="w"> </span><span class="n">cans</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">tennis</span><span class="w"> </span><span class="n">balls</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="k">is</span>
<span class="n">bought_balls</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">3</span>
<span class="n">tennis</span><span class="w"> </span><span class="n">balls</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">is</span>
<span class="n">answer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tennis_balls</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">bought_balls</span>
</code></pre></div>

<p>Figure 2: A close-up of a single example from a PAL prompt. Chain-of-thought reasoning is highlighted in blue and PAL programmatic steps are highlighted in gray and pink.</p>
<p>Example A close-up of the example from Figure 1 is shown in Figure 2. While chain-of-thought only decomposes the solution in the prompt into natural language steps such as Roger started with 5 tennis balls and</p>
<p>2 cans of 3 tennis balls each is 6 , in PAL we also augment each such NL step with its corresponding programmatic statement such as tennis_balls $=5$ and bought_balls $=2 * 3$. This way, the model learns to generate a program that will provide the answer for the test question, instead of relying on LLM to perform the calculation correctly.</p>
<p>We prompt the language model to generate NL intermediate steps using comment syntax (e.g. "# . . ." in Python) such they will be ignored by the interpreter. We pass the generated program $t_{\text {test }}$ to its corresponding solver, we run it, and obtain the final run result $y_{\text {test }}$. In this work we use a standard Python interpreter, but this can be any solver, interpreter or a compiler.</p>
<p>Crafting prompts for PAL In our experiments, we leveraged the prompts of existing work whenever available, and otherwise randomly selected the same number (3-6) of examples as previous work for creating a fixed prompt for every benchmark. In all cases, we augmented the free-form text prompts into PAL-styled prompts, leveraging programming constructs such as for loops and dictionaries when needed. Generally, writing PAL prompts is easy and quick.
We also ensure that variable names in the prompt meaningfully reflect their roles. For example, a variable that describes the number of apples in the basket should have a name such as num_apples_in_basket. This keeps the generated code linked to the entities in the question. In Section 6 we show that such meaningful variable names are critical. Notably, it is also possible to incrementally run the PL segments and feed the execution results back to the LLM to generate the following blocks. For simplicity, in our experiments, we used a single, post-hoc, execution.</p>
<p>This work focuses on CoT-style reasoning chain, but in Appendix I we show that PAL also improves Least-toMost (Zhou et al., 2022) prompts, which introduce reasoning chains that decompose a question into sub-questions.</p>
<h2>4. Experimental Setup</h2>
<p>Data and in-context examples We experiment with three broad classes of reasoning tasks: (1) mathematical problems (§4.1) from a wide range of datasets including GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), ASDIV (Miao et al., 2020), and MAWPS (Koncel-Kedziorski et al., 2016); (2) symbolic reasoning (§4.2) from BIG-Bench Hard (Suzgun et al., 2022); (3) and algorithmic problems (§4.3) from BIG-Bench Hard as well. Details of all datasets are shown in Appendix H. For all of the experiments for which CoT prompts were available, we use the same incontext examples as used by previous work. Otherwise, we randomly sampled a fixed set of in-context examples, and</p>
<p>Q: Olivia has $\$ 23$. She bought five bagels for $\$ 3$ each. How much money does she have left?</p>
<div class="codehilite"><pre><span></span><code>money_initial = 23
bagels = 5
bagel_cost = 3
money_spent = bagels * bagel_cost
money_left = money_initial - money_spent
answer = money_left
</code></pre></div>

<p>Figure 3: Example prompt for the mathematical reasoning tasks, from the GSM8K benchmark.
Q: On the table, you see a bunch of objects arranged in a row: a purple paperclip, a pink stress ball, a brown keychain, a green scrunchiephone charger, a mauve fidget spinner, and a burgundy pen. What is the color of the object directly to the right of the stress ball?</p>
<div class="codehilite"><pre><span></span><code><span class="n">stress_ball_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">None</span>
<span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">enumerate</span><span class="p">(</span><span class="n">objects</span><span class="p">):</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">object</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s1">&#39;stress ball&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="n">stress_ball_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span>
<span class="w">        </span><span class="k">break</span>
<span class="c1"># Find the directly right object</span>
<span class="n">direct_right</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">objects</span><span class="p">[</span><span class="n">stress_ball_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
<span class="c1"># Check the directly right object&#39;s color</span>
<span class="n">answer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">direct_right</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div>

<p>Figure 4: An example for a PAL prompt in the Colored ObJects task. For space considerations, we omit the code that creates the list objects.
used the same set for PAL and COT.</p>
<p>Baselines We consider three prompting strategies: DIRECT prompting - the standard prompting approach using pairs of questions and immediate answers (e.g., $\mathbf{T I}$ ) as in Brown et al. (2020); chain-of-thought (CoT) prompting (Wei et al., 2022); and our PAL prompting. We performed greedy decoding from the language model using a temperature of 0 . Unless stated otherwise, we used CODEX (code-davinci-002) as our backend LLM for both PAL, DIRECT, and COT. In datasets where results for additional base LMs, such as PaLM-540B, were available from previous work, we included them as COT ${ }_{\text {PaLM-540B }}$.</p>
<h3>4.1. Mathematical Reasoning</h3>
<p>We evaluate PAL on eight mathematical word problem datasets. Each question in these tasks is an algebra word problem at grade-school level. An example for a question and PAL example prompt is shown in Figure 3. We found that using explicit NL intermediate steps does not further benefit these math reasoning tasks, hence we kept only the meaningful variable names in the prompt.</p>
<p>GSM-HARD LLMs can perform simple calculations with small numbers. However, Madaan \&amp; Yazdanbakhsh (2022) found that $50 \%$ of the numbers in the popular GSM8K dataset of math reasoning problems are integers between 0
and 8. This raises the question of whether LLMs can generalize to larger and non-integer numbers? We constructed a harder version of GSM8K, which we call GSM-HARD, by replacing the numbers in the questions of GSM8K with larger numbers. Specifically, one of the numbers in a question was replaced with a random integer of up to 7 digits. More details regarding the this new dataset are provided in H.1.</p>
<h3>4.2. Symbolic Reasoning</h3>
<p>We applied PAL to three symbolic reasoning tasks from BIG-Bench Hard (Suzgun et al., 2022), which involve reasoning about objects and concepts: (1) Colored ObJECTS requires answering questions about colored objects on a surface. This task requires keeping track of relative positions, absolute positions, and the color of each object. Figure 4 shows an example for a question and example PAL prompt. (2) PENGUINS describes a table of penguins and some additional information in natural language, and the task is to answer a question about the attributes of the penguins, for example, "how many penguins are less than 8 years old?". While both PENGUINS and Colored ObJect tasks require tracking objects, PENGUINS describes dynamics as well, since the penguins in the problem can be added or removed. Figure 17 in Appendix J. 2 shows an example for a question, a chain-of-thought prompt, and PAL prompt. (3) DATE is a date understanding task that involves inferring dates from natural language descriptions, performing addi-</p>
<p>Q: I have a chair, two potatoes, a cauliflower, a lettuce head, two tables, a cabbage, two onions, and three fridges. How many vegetables do I have?</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> note: I&#39;m not counting the chair, tables,
    or fridges
vegetables_to_count = {
    &#39;potato&#39;: 2,
    &#39;cauliflower&#39;: 1,
    &#39;lettuce head&#39;: 1,
    &#39;cabbage&#39;: 1,
    &#39;onion&#39;: 2
}
answer = sum(vegetables_to_count.values())
</code></pre></div>

<p>Figure 5: An example for a PAL prompt in the ObJect Counting task. The base LM is expected to convert the input into a dictionary where keys are entities and values are their quantities, while filtering out non-vegetable entities. Finally, the answer is the sum of the dictionary values.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">GSM-HARD</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">ASDIV</th>
<th style="text-align: center;">SINGLEEQ</th>
<th style="text-align: center;">SINGLEOP</th>
<th style="text-align: center;">ADDSUB</th>
<th style="text-align: center;">MULTIARITH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DIRECT $_{\text {Codex }}$</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">44.0</td>
</tr>
<tr>
<td style="text-align: center;">COT UL2-20B</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">10.7</td>
</tr>
<tr>
<td style="text-align: center;">COT LaMDA-137B</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">51.8</td>
</tr>
<tr>
<td style="text-align: center;">COT Codex</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">95.9</td>
</tr>
<tr>
<td style="text-align: center;">COT PaLM-540B</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">94.7</td>
</tr>
<tr>
<td style="text-align: center;">COT Minerva 540B</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PAL</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">99.2</td>
</tr>
</tbody>
</table>
<p>Table 1: Problem solve rate (\%) on mathematical reasoning datasets. The highest number on each task is in bold. The results for DIRECT and PaLM-540B are from Wei et al. (2022), the results for LaMDA and UL2 are from Wang et al. (2022b), and the results for Minerva are from Lewkowycz et al. (2022). We ran PAL on each benchmark 3 times and report the average; the standard deviation is provided in Table 7.
tion and subtraction of relative periods of time, and having some global knowledge such as "how many days are there in February", and performing the computation accordingly. Appendix J. 3 shows example prompts.</p>
<h3>4.3. Algorithmic Tasks</h3>
<p>Finally, we compare PAL and COT on algorithmic reasoning. These are tasks where a human programmer can write a deterministic program with prior knowledge of the question. We experiment with two algorithmic tasks: ObJect Counting and Repeat Copy. Object Counting involves answering questions about the number of objects belonging to a certain type. For example, as shown in Figure 5: "I have a chair, two potatoes, a cauliflower, a lettuce head, two tables, ... How many vegetables do I have?"). REPEAT COPY requires generating a sequence of words according to instructions. For example, as shown in Appendix J.6: "Repeat the word duck four times, but halfway through also say quack").</p>
<h2>5. Results</h2>
<h3>5.1. Math Results</h3>
<p>Table 1 shows the following results: across all tasks, PAL using Codex sets a new few-shot state-of-the-art top1 decoding across all datasets, outperforming $\mathrm{CoT}<em _PaLM-540B="{PaLM-540B" _text="\text">{\text {Codex }}$, $\mathrm{CoT}</em>$ which was fine-tuned on explicit mathematical content.}}$, and $\mathrm{CoT}_{\text {Minerva } 540 \mathrm{~B}</p>
<p>Interestingly, COT also benefits from Codex over PaLM540B in some of the datasets such as ASDIV, but performs worse than PaLM-540B in others such as SVAMP. Yet, using PAL further improves the solve rate across all datasets.</p>
<p>GSM-HARD On GSM-HARD (Table 1), the accuracy of DIRECT drops dramatically from $19.7 \%$ to $5.0 \%$ (a relative drop of $74 \%$ ), the accuracy of COT drops from $65.6 \%$ to $20.1 \%$ (a relative drop of almost $70 \%$ ), while PAL remains stable at $61.5 \%$, dropping by only $14.3 \%$. The results of COT on GSM-HARD did not improve even when we replaced its prompts with prompts that include large numbers (Appendix B). This shows how PAL provides not only better</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Colored Object</th>
<th style="text-align: center;">Penguins</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Repeat Copy</th>
<th style="text-align: center;">Object Counting</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DIRECT $_{\text {Codex }}$</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">37.6</td>
</tr>
<tr>
<td style="text-align: left;">COT $_{\text {LaMDA-137B }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">COT $_{\text {PaLM-540B }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">COT $_{\text {Codex }}$</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">73.0</td>
</tr>
<tr>
<td style="text-align: left;">PAL $_{\text {Codex }}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 1}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 3}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 6}$</td>
<td style="text-align: center;">$\mathbf{9 6 . 7}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Solve rate on three symbolic reasoning datasets and two algorithmic datasets, In all datasets, PAL achieves a much higher accuracy than chain-of-thought. Results with closed models LaMDA-137B and PaLM-540B are included if available to public (Wei et al., 2022; Suzgun et al., 2022).
results on the standard benchmarks, but is also much more robust. In fact, since PAL offloads the computation to the Python interpreter, any complex computation can be performed accurately given the correctly generated program.</p>
<p>Large Numbers or Incorrect Reasoning? Are the failures on GSM-HARD primarily due to the inability of LLMs to do arithmetic, or do the large numbers in the question "confuse" the LM which generates irrational intermediate steps? To investigate this, we evaluated the outputs generated by CoT for the two versions of the same question (with and without large numbers). We find that in 16 out of 25 cases we analyzed, CoT generates nearly identical natural language "thoughts", indicating that the primary failure mode is the inability to perform arithmetic accurately. Sample outputs are provided in the Appendix, Table 11.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">GSM8K</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">COT UL2-20B</td>
<td style="text-align: right;">7.3</td>
</tr>
<tr>
<td style="text-align: left;">COT $_{\text {LaMDA-137B }}$</td>
<td style="text-align: right;">27.7</td>
</tr>
<tr>
<td style="text-align: left;">COT $_{\text {Codex }}$</td>
<td style="text-align: right;">78.0</td>
</tr>
<tr>
<td style="text-align: left;">COT $_{\text {PaLM-540B }}$</td>
<td style="text-align: right;">74.4</td>
</tr>
<tr>
<td style="text-align: left;">COT $_{\text {Minerva 540B }}$</td>
<td style="text-align: right;">78.5</td>
</tr>
<tr>
<td style="text-align: left;">PAL $_{\text {Codex }}$</td>
<td style="text-align: right;">$\mathbf{8 0 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Problem solve rate (\%) on GSM8K using majority@40 (Wang et al., 2022b)</p>
<p>Multi-sample Generation As found by Wang et al. (2022b), chain-of-thought-style methods can be further improved by sampling $k&gt;1$ outputs, and selecting the final answer using majority voting. We thus repeated the greedy-decoding experiments using nucleus sampling (Holtzman et al., 2019) with $p=0.95$ and $k=40$ as in Lewkowycz et al. (2022) and temperature of 0.7 . As shown in Table 3, this further increases the accuracy of PAL from $72.0 \%$ to $80.4 \%$ on GSM8K, obtaining $1.9 \%$ higher accuracy than Minerva-540B using the same number of samples.</p>
<h3>5.2. Symbolic Reasoning \&amp; Algorithmic Tasks Results</h3>
<p>Results for symbolic reasoning and algorithmic tasks are shown in Table 2. In Colored Objects, PAL improves over the strong COT by $8.8 \%$, and by $19.4 \%$ over the standard direct prompting. In Penguins, PAL provides a gain of absolute $14.1 \%$ over CoT. In Date, PAL further provides $11.4 \%$ gain over both COT ${ }_{\text {Codex }}$, PaLM-540B, and LaMDA-137B.
The two rightmost columns of Table 2 show that PAL is close to solving ObJect Counting, reaching $96.7 \%$ and improving over COT by absolute $23.7 \%$. Similarly, PAL vastly outperforms CoT by absolute $21.8 \%$ on REPEAT COPY. Surprisingly, DIRECT prompting performs better than COT on REPEAT COPY. Yet, PAL improves over DIRECT by $9.3 \%$ in REPEAT COPY.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 6: The solve rate on Colored Objects with respect to the number of objects included in the test question.</p>
<p>Is PAL sensitive to the complexity of the question? We examined how the performance of PAL and COT change as the complexity of the input question grows, measured as the number of objects in the question of Colored Objects. As shown in Figure 6, PAL is superior COT across all input lengths. As the number of objects in the question increases, COT's accuracy is unstable and drops, while PAL remains consistently close to $100 \%$. More analysis on the token-level predictions can be found in Appendix G.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 7: PAL with different models on GSM8K: though the absolute accuracies with code-cushman-001 and code-davinci-001 are lower than code-davinci-002, the relative improvement of PAL over COT is consistent across models.</p>
<h2>6. Analysis</h2>
<p>Does PAL work with weaker LMs? In all our experiments in Section 5, PAL used the code-davinci-002 model; but can PAL work with weaker models of code? We compared PAL with CoT when both prompting approaches use the same weaker base LMs code-cushman-001 and code-davinci-001. As shown in Figure 7, even though the absolute accuracies of code-cushman-001 and code-davinci-001 are lower, the relative improvement of PAL over COT remains consistent across models. This shows that PAL can work with weaker models, while its benefit scales elegantly to stronger models as well.</p>
<p>Does PAL work with LMs of natural language? We also experimented with PAL using the text-davinci series. Figure 8 shows the following interesting results: when the base LM's "code modeling ability" is weak (using text-davinci-001), CoT performs better than PAL. However, once the LM's code modeling ability is sufficiently high (using text-davinci-002 and text-davinci-003), PAL outperforms CoT, and PAL text-davinci-003 performs almost as PAL code-davinci-002. This shows that PAL is not limited to LMs of code, but it can work with LMs that were mainly trained for natural language, if they have a sufficiently high coding ability.</p>
<p>Is PAL better because of the Python prompt or because of the interpreter? We experimented with generating Python code, while requiring the neural LM to "execute" it as well, without using an interpreter, following Nye et al. (2021); Madaan et al. (2022). We created prompts that are similar to PAL's, except that they do include the final answer. This resulted in a 23.2 solve rate on GSM8K, much lower than PAL (72.0), and only 4.5 points higher than DIRECT. These results reinforce our hypothesis that the main benefit of PAL comes from the synergy with the interpreter, and
<img alt="img-3.jpeg" src="img-3.jpeg" />
text-davinci-001 text-davinci-002 text-davinci-003
Figure 8: PAL with NL LMs on GSM8K: though CoT outperforms PAL with text-davinci-001, once the base LM is sufficiently strong, PAL is beneficial with text-davinci-002 and text-davinci-003 as well. That is, PAL is not limited to code-LMs only.
not only from having a better prompt. Additional details are provided in Appendix B. For additional discussion on the advantages of code-prompts over textual-prompts, see Appendix G.</p>
<p>Do variable names matter? In all our experiments, we used meaningful variable names in the PAL prompts, to ease the model's grounding of variables to the entities they represent. For the Python interpreter, however, variable names are meaningless. To measure the importance of meaningful variable names, we experimented with two prompts variants:</p>
<ol>
<li>$\mathrm{PAL}_{\text {-comment }}$ - the PAL prompt without intermediate NL comments.</li>
<li>$\mathrm{PAL}_{\text {-comment }}^{\text {-var }}$ - the PAL prompt without intermediate NL comments and with variable names substituted with random characters.</li>
</ol>
<p>The results are shown in Figure 9. In Colored ObJeCTED and DATE, removing intermediate NL comments but keeping meaningful variable names ( $\mathrm{PAL}<em _-comment="{-comment" _text="\text">{\text {-comment }}$ ) - slightly reduces the results compared to the full PAL prompt, but it still achieves higher accuracy than the baselines COT. Removing variable names as well ( $\mathrm{PAL}</em>$ ) further decreases accuracy, and performs worse than COT. Since variable names have an important part in code quality (Gellenbeck \&amp; Cook, 1991; Takang et al., 1996), meaningful variable names are only expected to ease reasoning for Codex, which was trained on mostly meaningful names, as was also found by Madaan et al. (2022).}}^{\text {-var }</p>
<h2>7. Related Work</h2>
<p>Prompting Few-shot prompting (Brown et al., 2020) has been shown to be an effective approach for a variety of tasks (Liu et al., 2021) ranging from text- (Gehrmann et al.,</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 9: Ablation study of PAL prompt formats. We consider the original PAL prompt, it with natural language comments removed ( $\mathrm{PAL}<em _comment="{comment" _text="\text">{\text {comment }}$ ), and further variable names replaced with random character ( $\mathrm{PAL}</em>$). As a reference, we also show the CoT performance (blue).}}^{-</p>
<p>2021; Reif et al., 2021; Wei et al., 2021; Sanh et al., 2021) to code-generation (Chen et al., 2021b). Methods such as chain-of-thought prompting (CoT) have further unlocked a variety of reasoning tasks, boosting the performance of models on a variety of benchmarks. Nevertheless, all previous approaches suffer from inaccuracy in arithmetic calculation and incorrect reasoning (Lewkowycz et al., 2022; Hendrycks et al., 2021; Madaan \&amp; Yazdanbakhsh, 2022). PAL avoids these problems by offloading the calculation and some of the reasoning to a Python interpreter, which is correct by construction, given the right program. Further, not only that PAL can improve the standard chain-of-thought, it can improve least-to-most prompting (Zhou et al., 2022) as well, as we show in Appendix I.</p>
<p>LMs with external tools Several prior works have equipped neural models with specialized modules. For example, Cobbe et al. (2021) employ a calculator for arithmetic operations as a post hoc processing, and Demeter \&amp; Downey (2020) add specialized modules for generating cities and dates. Unlike these works, PAL generates code for a Python interpreter, which is general enough to handle both arithmetic calculations and dates, without specialized modules and ad-hoc fixes. Chowdhery et al. (2022) and Wei et al. (2022) have also experimented with external calculators; however, the calculator had improved Codex by only $2.3 \%$ (absolute) on GSM8K and improved PaLM-540B by $1.7 \%$, while PAL improves Codex by $6.4 \%$ on the same benchmark (Section 5.1). Similarly to our work, Chowdhery et al. (2022) have also experimented with generating Python code for solving the GSM8K benchmark, but their experiments resulted in lower accuracy than the standard PaLM-540B that uses chain-of-thought. Pi et al. (2022) pretrain the model on execution results of random expressions on a calculator, instead of using the solver at test time as well. While their model can hypothetically perform arithmetic better than other pretrained LMs, their results on the SVAMP benchmark are much lower: $57.4 \%$ using a T5-11B
model, while PAL achieves $79.4 \%$ on the same benchmark without any specialized pretraining.</p>
<p>Shortly after a preprint of our work was submitted to arXiv, another related work on "program of thought prompting" (Chen et al., 2022) was also submitted to arXiv. Their method is conceptually similar to ours, but PoT (1) only demonstrates efficacy on mathematical problems, whereas we demonstrate gains on symbolic and algorithmic benchmarks as well, and (2) chose benchmark-specific prompt examples, while we used the same prompt examples as previous work, to disentangled the benefit of our approach from the benefit of the choice of examples.</p>
<p>Semantic parsing Our work can also be seen as a very general form of semantic parsing, where instead of parsing into strict domain-specific languages, the model generates free-form Python code. Some works constrain the decoder using a Context-Free Grammar (CFG) to generate a domainspecific meaning representation (Shin \&amp; Van Durme, 2021) or a canonical utterance, which can be converted to a Lisplike meaning representation (Shin et al., 2021). In contrast, PAL does not require any constraining or domain-specific representations other than Python code. Further, LMs that were pretrained on Python are abundant compared to other domain-specific languages, making Python code a much more preferable representation. Andor et al. (2019) generate task-specific arithmetic operations for reading comprehension tasks; Gupta et al. (2019) design neural modules such as count to deal with arithmetic operations. PAL generalizes these works by generating general Python programs, without the need for defining specialized modules. The closest work to ours technically may be Binder (Cheng et al., 2022), but it addressed mostly answering questions about tables using SQL and SQL-like Python.</p>
<h2>8. Conclusion</h2>
<p>We introduce PAL, a new method for natural language reasoning, using programs as intermediate reasoning steps. Differently from existing LM-based reasoning approaches, the main idea is to offload solving and calculating to an external Python interpreter, instead of using the LLM for both understanding the problem and solving. This results in a final answer that is guaranteed to be accurate, given the correctly predicted programmatic steps. We demonstrate this seamless synergy between an LLM and a Python interpreter across 13 tasks from BIG-Bench Hard and other benchmarks. In all these benchmarks, PAL outperforms larger LLMs such as PaLM-540B which use the popular "chain-of-thought" method and sets new state-of-the-art accuracy on all of them. We believe that these results unlock exciting directions for future neuro-symbolic AI reasoners.</p>
<h2>References</h2>
<p>Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as I Can, not as I Say: Grounding Language in Robotic Affordances. arXiv preprint arXiv:2204.01691, 2022.</p>
<p>Amini, A., Gabriel, S., Lin, S., Koncel-Kedziorski, R., Choi, Y., and Hajishirzi, H. MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms. In ACL, 2019.</p>
<p>Andor, D., He, L., Lee, K., and Pitler, E. Giving bert a calculator: Finding operations and arguments with reading comprehension. arXiv preprint arXiv:1909.00109, 2019.</p>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language Models are Few-Shot Learners. In NeurIPS, 2020.</p>
<p>Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter,
C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374, 2021a.</p>
<p>Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021b.</p>
<p>Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.</p>
<p>Cheng, Z., Xie, T., Shi, P., Li, C., Nadkarni, R., Hu, Y., Xiong, C., Radev, D., Ostendorf, M., Zettlemoyer, L., Smith, N. A., and Yu, T. Binding language models in symbolic languages. arXiv preprint arXiv:2210.02875, 2022.</p>
<p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. PaLM: Scaling Language Modeling with Pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Cobbe, K., Kosaraju, V., Bavarian, M., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training Verifiers to Solve Math Word Problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Demeter, D. and Downey, D. Just add functions: A neuralsymbolic language model. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 7634-7642, 2020.</p>
<p>Garcez, A. d. and Lamb, L. C. Neurosymbolic ai: the 3rd wave. arXiv preprint arXiv:2012.05876, 2020.</p>
<p>Gehrmann, S., Adewumi, T., Aggarwal, K., Ammanamanchi, P. S., Anuoluwapo, A., Bosselut, A., Chandu, K. R., Clinciu, M., Das, D., Dhole, K. D., Du, W., Durmus, E., Dušek, O., Emezue, C., Gangal, V., Garbacea, C., Hashimoto, T., Hou, Y., Jernite, Y., Jhamtani, H., Ji, Y., Jolly, S., Kale, M., Kumar, D., Ladhak, F., Madaan, A., Maddela, M., Mahajan, K., Mahamood, S., Majumder, B. P., Martins, P. H., McMillan-Major, A., Mille, S., van Miltenburg, E., Nadeem, M., Narayan, S., Nikolaev, V., Niyongabo, R. A., Osei, S., Parikh, A., PerezBeltrachini, L., Rao, N. R., Raunak, V., Rodriguez, J. D., Santhanam, S., Sedoc, J., Sellam, T., Shaikh, S., Shimorina, A., Cabezudo, M. A. S., Strobelt, H., Subramani, N., Xu, W., Yang, D., Yerukola, A., and Zhou, J. The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics. arXiv preprint arXiv:2102.01672, 2021.</p>
<p>Gellenbeck, E. M. and Cook, C. R. An investigation of procedure and variable names as beacons during program comprehension. In Empirical studies of programmers: Fourth workshop, pp. 65-81. Ablex Publishing, Norwood, NJ, 1991.</p>
<p>Gupta, N., Lin, K., Roth, D., Singh, S., and Gardner, M. Neural module networks for reasoning over text. arXiv preprint arXiv:1912.04971, 2019.</p>
<p>Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the MATH dataset, 2021. URL https://openreview.net/forum? id=7Bywt2mQsCe.</p>
<p>Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The Curious Case of Neural Text Degeneration. In ICLR, 2019.</p>
<p>Koncel-Kedziorski, R., Roy, S., Amini, A., Kushman, N., and Hajishirzi, H. Mawps: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 11521157, 2016.</p>
<p>Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.</p>
<p>Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems. arXiv preprint arXiv:1705.04146, 2017.</p>
<p>Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. arXiv preprint arXiv:2107.13586, 2021.</p>
<p>Madaan, A. and Yazdanbakhsh, A. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022.</p>
<p>Madaan, A., Zhou, S., Alon, U., Yang, Y., and Neubig, G. Language models of code are few-shot commonsense learners. arXiv preprint arXiv:2210.07128, 2022.</p>
<p>Marcus, G. Deep learning: A critical appraisal. arXiv preprint arXiv:1801.00631, 2018.</p>
<p>Marcus, G. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177, 2020.</p>
<p>Miao, S.-y., Liang, C.-C., and Su, K.-Y. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 975-984, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main. 92. URL https://aclanthology.org/2020. acl-main. 92.</p>
<p>Mishra, S., Finlayson, M., Lu, P., Tang, L., Welleck, S., Baral, C., Rajpurohit, T., Tafjord, O., Sabharwal, A., Clark, P., and Kalyan, A. Lila: A unified benchmark for mathematical reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.</p>
<p>Nogueira, R., Jiang, Z., and Lin, J. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019, 2021.</p>
<p>Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., and Odena, A. Show your Work: Scratchpads for Intermediate Computation with Language Models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>Patel, A., Bhattamishra, S., and Goyal, N. Are NLP Models Really Able to Solve Simple Math Word Problems? arXiv preprint arXiv:2103.07191, 2021.</p>
<p>Pi, X., Liu, Q., Chen, B., Ziyadi, M., Lin, Z., Gao, Y., Fu, Q., Lou, J.-G., and Chen, W. Reasoning like program executors. arXiv preprint arXiv:2201.11473, 2022.</p>
<p>Qian, J., Wang, H., Li, Z., Li, S., and Yan, X. Limitations of language models in arithmetic and symbolic induction. arXiv preprint arXiv:2208.05051, 2022.</p>
<p>Reif, E., Ippolito, D., Yuan, A., Coenen, A., CallisonBurch, C., and Wei, J. A Recipe for Arbitrary Text Style Transfer with Large Language Models. arXiv preprint arXiv:2109.03910, 2021.</p>
<p>Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., Dey, M., Bari, M. S., Xu, C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Fevry, T., Fries, J. A., Teehan, R., Biderman, S., Gao, L., Bers, T., Wolf, T., and Rush, A. M. Multitask Prompted Training Enables Zero-Shot Task Generalization, 2021.</p>
<p>Shin, R. and Van Durme, B. Few-shot semantic parsing with language models trained on code. arXiv preprint arXiv:2112.08696, 2021.</p>
<p>Shin, R., Lin, C. H., Thomson, S., Chen, C., Roy, S., Platanios, E. A., Pauls, A., Klein, D., Eisner, J., and Van Durme, B. Constrained language models yield few-shot semantic parsers. arXiv preprint arXiv:2104.08768, 2021.</p>
<p>Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E., Zhou, D., and Wei, J. Challenging big-bench tasks and whether chain-of-thought can solve them. ArXiv, abs/2210.09261, 2022.</p>
<p>Takang, A. A., Grubb, P. A., and Macredie, R. D. The effects of comments and identifier names on program comprehensibility: an experimental investigation. J. Prog. Lang., 4(3):143-167, 1996.</p>
<p>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Rationale-Augmented Ensembles in Language Models. arXiv preprints arXiv:2207.00747, 2022a.</p>
<p>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Self-Consistency Improves Chain of Thought Reasoning in Language Models. arXiv preprint arXiv:2203.11171, 2022b.</p>
<p>Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned Language Models are Zero-shot Learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of Thought Prompting Elicits Reasoning in Large Language Models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Wu, Y., Jiang, A. Q., Li, W., Rabe, M. N., Staats, C., Jamnik, M., and Szegedy, C. Autoformalization with Large Language Models. arXiv preprint arXiv:2205.12615, 2022.</p>
<p>Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.</p>
<p>Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. arXiv preprint arXiv:2205.10625, 2022.</p>
<h1>Part I</h1>
<h2>Appendix</h2>
<h2>Table of Contents</h2>
<p>A Alternative Prompts without Meaningful Variable Names ..... 13
B Additional analysis on Arithmetic Reasoning ..... 13
C Effect of Using Language Models of Code ..... 14
D Analyzing the Effect of Increasing Number of Samples on PAL ..... 14
E Standard Deviations Across Multiple Order of Prompts ..... 17
F PAL Beyond Benchmarks ..... 17
G Closer Look into Token-level Behaviors of Different Mechanisms ..... 20
H Datasets ..... 20
H. 1 Creating GSM-HARD ..... 23
H. 2 GSM-HARD Analysis ..... 23
I Generalization of PAL to Least-to-Most Prompting ..... 24
J Prompts ..... 26
J. 1 Reasoning about Colored Objects ..... 26
J. 2 Penguins in a Table ..... 27
J. 3 Date Understanding ..... 28
J. 4 Math ..... 29
J. 5 Object Counting ..... 31
J. 6 Repeat Copy ..... 32
K Success and Failure Modes in Symbolic Tasks ..... 33
K. 1 Colored Objects ..... 33
K. 2 Penguins in a Table ..... 33
K. 3 Date Understanding ..... 34</p>
<h1>A. Alternative Prompts without Meaningful Variable Names</h1>
<div class="codehilite"><pre><span></span><code>a = 23
b = 5
c = 3
d = b * c
e = a - d
print (e)
</code></pre></div>

<p>(a) Structured explanation with uninformative variable names (PAL - var)</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Olivia has $23</span>
<span class="n">a</span> = <span class="mi">23</span>
<span class="c1"># number of bagels bought</span>
<span class="n">b</span> = <span class="mi">5</span>
<span class="c1"># price of each bagel</span>
<span class="n">c</span> = <span class="mi">3</span>
<span class="c1"># total price of bagels</span>
<span class="nb">d</span> = <span class="n">b</span> * <span class="n">c</span>
<span class="c1"># money left</span>
<span class="nb">e</span> = <span class="n">a</span> - <span class="nb">d</span>
<span class="nb">print</span> (<span class="nb">e</span>)
</code></pre></div>

<p>(b) Structured explanation with uninformative variable names, but useful comments (PAL - var + comms)
money_initial $=23$
bagels $=5$
bagel_cost $=3$
money_spent = bagels * bagel_cost
money_left = money_initial - money_spent
result = money_left
print(result)
(c) PAL prompts</p>
<p>Figure 10: Role of text in PAL: three different reasoning steps for the question Olivia has $\$ 23$. She bought five bagels for $\$ 3$ each. How much money does she have left? Uninformative variable names (left), Uninformative variable names with useful comments (left), and PAL. Including text description</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Setting</th>
<th style="text-align: left;">CoT</th>
<th style="text-align: left;">PAL - var</th>
<th style="text-align: left;">PAL - var + comms</th>
<th style="text-align: left;">PAL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Solve Rate</td>
<td style="text-align: left;">63.1</td>
<td style="text-align: left;">59.0</td>
<td style="text-align: left;">69.0</td>
<td style="text-align: left;">71.8</td>
</tr>
</tbody>
</table>
<p>Table 4: Role of text: including text either as informative variable names (PAL) or comments is important (PAL - var + comms). Uninformative variable names PAL - var cause a drastic drop in performance, indicating that just structure is not sufficient. The corresponding prompts are shown in Figure 10.</p>
<p>For mathematical problems, since our standard prompts do not use much comment, we start by creating alternative prompts where the informative variable names are replaced with single-letters (Figure 10). The results in Table 4 shows a considerable performance drop: from an average of $71.8 \%$ to $59 \%$. Note that the ablation where structured outputs are completely removed in favor of purely text explanations is precisely the CoT setting, which achieves a solve rate of $63 \%$. These results underscore the importance of text but more importantly show that combining both text and procedural statements leads to higher performance gains-either is sub-optimal.</p>
<h2>B. Additional analysis on Arithmetic Reasoning</h2>
<p>GSM-hard with hard prompts The GSM-HARD experiments used prompts that were sampled from the GSM8K training set. Will COT be helped by using larger numbers in the prompts as well? To investigate this, we create prompts where the numbers are changed to larger numbers, matching the distribution of numbers in GSM-HARD. The results in Table 5 shows</p>
<p>that even with a prompt that matches the numbers, there are only modest gains in performance. These results show that the gains achieved by using code-based reasoning chains may not be achieved simply by using better few-shot examples for CoT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Regular Prompt</th>
<th style="text-align: center;">Prompt with Larger Numbers</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">COT</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">23.8</td>
</tr>
</tbody>
</table>
<p>Table 5: GSM-hard results, when the prompts also had examples of larger numbers.</p>
<p>Succinct Code The programs used in few-shot examples by PAL are multi-step, and show a step-by-step breakdown of the reasoning process. Is this breakdown necessary? Alternatively, can we return a single line expression (see Figure 11b) to calculate the result? Results in Table 6 ( $4^{\text {th }}$ row) shows that is not the case. With single-line expressions, the performance of PAL falls to the level of direct prompting.</p>
<p>Generating the answer directly PAL first generates a reasoning chain in the form of a Python program, and passes the generated program to a runtime to obtain an answer. Is PAL better only because of the program-style intermediate reasoning chains, or are the improvements derived from offloading execution to the Python runtime? To investigate this, we experiment with a variant that forces the LLM to generate the answer after generating the reasoning chain (Figure 11e). This setting compels the LLM to condition on the generated code-based reasoning to generate an answer, simulating the runtime. The results in Table 6 ( $5^{\text {th }}$ row) show that the solve rate drops to near DIRECT levels. This reinforces our hypothesis that while current LLMs can be excellent at specifying a high-level plan to solve a task-they are still incapable of executing them.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Ablation</th>
<th style="text-align: center;">Solve Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DIRECT (no intermediate reasoning)</td>
<td style="text-align: center;">19.7</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">65.6</td>
</tr>
<tr>
<td style="text-align: left;">PAL</td>
<td style="text-align: center;">72.0</td>
</tr>
<tr>
<td style="text-align: left;">Succinct Code</td>
<td style="text-align: center;">47.8</td>
</tr>
<tr>
<td style="text-align: left;">LLM Simulating Runtime</td>
<td style="text-align: center;">23.2</td>
</tr>
</tbody>
</table>
<p>Table 6: Solve Rates for Ablations</p>
<h1>C. Effect of Using Language Models of Code</h1>
<p>In our experiments, we focused on evaluating the performance of a language model for code. We aimed to investigate whether the additional performance boost observed in our results was due to the use of models like Codex, or whether our formulation was useful even for text-based models. To this end, we conducted additional experiments using text-based language models. Our findings indicate that the PAL approach is not restricted to working solely with Codex, but can also be applied to natural language (NL) models, as long as the model is sufficiently strong. Specifically, our results showed that in the text-davinci-001 model, the use of the CoT approach resulted in better performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">CoT</th>
<th style="text-align: center;">PaL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">text-davinci-001</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">8.6</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-002</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">65.8</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">69.8</td>
</tr>
</tbody>
</table>
<h2>D. Analyzing the Effect of Increasing Number of Samples on PAL</h2>
<p>In Section 5.1, we show that PAL outperforms strong baselines both for a single sample and by drawing 40 samples and using majority voting. Figure 12 illustrates the trends for cases when the number of samples drawn are between 1 and 40, and the interpolation estimates demonstrate that PAL remains competitive throughout the number of samples.</p>
<div class="codehilite"><pre><span></span><code>def solution():
&quot;&quot;&quot;Shawn has five toys. For Christmas, he got two toys each from his
    ; mom and dad. How many toys does he have now?&quot;&quot;&quot;
toys_initial = 5
mom_toys = 2
dad_toys = 2
total_received = mom_toys + dad_toys
total_toys = toys_initial + total_received
result = total_toys
return result
</code></pre></div>

<p>(a) Original Example
def solution():
return $5+2+2$
(b) Succinct Code
def solution():
"""Shawn has 10312864 toys. For Christmas, he got 13267894 toys each
from his mom and dad. How many toys does he have now?"""
toys_initial = 10312864
mom_toys = 13267894
dad_toys = 13267894
total_received = mom_toys + dad_toys
total_toys = toys_initial + total_received
result = total_toys
return result
(c) Hard Examples in Prompt (PAL)</p>
<div class="codehilite"><pre><span></span><code>Example {
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nv">question</span><span class="o">=</span><span class="err">&quot;Shawn has 10312864 toys. For Christmas, he got 13267894 toys</span>
<span class="w">    </span><span class="nv">each</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="nv">his</span><span class="w"> </span><span class="nv">mom</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">dad</span>.<span class="w"> </span><span class="nv">How</span><span class="w"> </span><span class="nv">many</span><span class="w"> </span><span class="nv">toys</span><span class="w"> </span><span class="nv">does</span><span class="w"> </span><span class="nv">he</span><span class="w"> </span><span class="nv">have</span><span class="w"> </span><span class="nv">now</span>?<span class="err">&quot;,</span>
<span class="nv">thought</span><span class="o">=</span><span class="err">&quot;Shawn started with 10312864 toys. If he got 13267894 toys each</span>
<span class="err">    from his mom and dad, then that is 26535788 more toys. 10312864 +</span>
<span class="w">    </span><span class="mi">26535788</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">36848652</span>.<span class="err">&quot;,</span>
<span class="nv">answer</span><span class="o">=</span><span class="s2">&quot;36848652&quot;</span>,
}
</code></pre></div>

<p>(d) Hard Examples in Prompt (CoT)
def solution():
"""Shawn has five toys. For Christmas, he got two toys each from his
; mom and dad. How many toys does he have now?"""
toys_initial = 5
mom_toys = 2
dad_toys = 2
total_received = mom_toys + dad_toys
total_toys = toys_initial + total_received
result = total_toys
return result
ans $=9$
(e) Generating Answers Directly</p>
<p>Figure 11: Ablations of the original example solution for the few-shot prompting experiment.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 12: Comparison of solve rates between PAL and baselines as the number of samples increases from 1 to 40 . Note that the solve rates for the baselines ( $\mathrm{PaLM}, \mathrm{CoT}$, Minerva) are obtained through logistic interpolation of solve rates at 1 and 40</p>
<h1>E. Standard Deviations Across Multiple Order of Prompts</h1>
<p>For each math reasoning task, we run inference using three random orderings of the prompts. As shown in Table 7, the standard deviation between the results obtained from the three different seeds is minimal.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">CoT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PAL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">Standard Deviation</td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">Standard Deviation</td>
</tr>
<tr>
<td style="text-align: left;">GSM8K</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">1.10</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">0.16</td>
</tr>
<tr>
<td style="text-align: left;">SVAMP</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">0.20</td>
</tr>
<tr>
<td style="text-align: left;">ASDIV</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: left;">GSM-HARD</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">0.91</td>
</tr>
<tr>
<td style="text-align: left;">MAWPS-SingleEq</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: left;">MAWPS-SingleOp</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: left;">MAWPS-AddSub</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">0.34</td>
</tr>
<tr>
<td style="text-align: left;">MAWPS-MultiArith</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">0.48</td>
</tr>
</tbody>
</table>
<p>Table 7: Standard deviations for three runs for the math reasoning datasets.</p>
<h2>F. PAL Beyond Benchmarks</h2>
<p>We argue that symbolic reasoning is a crucial component in solving a wide range of tasks. In this section, we demonstrate examples of tasks that may not initially appear to require using programs as intermediate reasoning steps, but can be improved through the use of PAL-style reasoning. We demonstrate these examples using the ChatGPT tool. ${ }^{1}$ In contrast to the in-context-learning methods we used in the main paper, here we instruct ChatGPT to perform program-aided reasoning through one of the user utterances.</p>
<p>In Figure 13, in CoT-style reasoning, while the reasoning chain is correct, the final answer is wrong. In contrast, PAL-style reasoning could not only accurately extract the color of objects from the question but also produce the correct lines of code to branch to different situations that yield their corresponding correct answers.</p>
<p>A more intriguing example is letting an LLM count the number of letters in the word "intriguing". In Figure 14a, while the step-by-step explanation appears reasonable by splitting the letters by spaces, ChatGPT does not change the answer after this explicit reasoning and insists on the wrong answer. Explicitly instructing the model to perform step-by-step reasoning before answering the question still yields the wrong answer. In contrast, PAL-style reasoning only takes a few lines of code, and the execution does produce the correct answer, in this case. These examples indicate that PAL can benefit even an ostensibly powerful model like ChatGPT.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Let's say our plane crashed onto an exotic island, where everything that is usually yellow looks blue here. We know that mango and banana are all yellow, do they have the same color on this island?</p>
<p>If you were on an exotic island where everything that is usually yellow looks blue, then mangoes and bananas would not have the same color.</p>
<p>Mangoes are typically yellow in color, but on this island, they would look blue. Bananas are also typically yellow in color, but on this island, they would also look blue.</p>
<p>Therefore, even though both mangoes and bananas are typically yellow in color, on this island they would have a different color. Mangoes and bananas would both look blue on this island, but they would not have the same color, as blue is a different color from yellow.</p>
<p>I hope this helps! Let me know if you have any other questions.
(a) In CoT style reasoning, the correct intermediate reasoning chain leads to wrong answers.</p>
<p>Let's say our plane crashed onto an exotic island, where everything that is usually yellow looks blue here. We know that mango and banana are all yellow, do they have the same color on this island? Write a Python program to answer the question.
$\langle 3| 3$</p>
<div class="codehilite"><pre><span></span><code>color_of_mango = &quot;blue&quot;
color_of_banana = &quot;blue&quot;
if color_of_mango == color_of_banana:
print(&quot;Yes, mango and banana have the same color on this island.&quot;)
else:
print(&quot;No, mango and banana do not have the same color on this island.&quot;)
</code></pre></div>

<p>(b) In PAL, the execution of the code will produce the correct answer.</p>
<p>Figure 13: ChatGPT with PAL and CoT to answer a user-posted question</p>
<p>How many letters are in the word 'intriguing'?</p>
<p>The word 'intriguing' has 9 letters.</p>
<p>Interesting, can you explain your reasoning step by step?</p>
<p>Sure, here's my reasoning step by step:</p>
<ol>
<li>I read the word 'intriguing'</li>
<li>I counted the number of letters in the word, which are: $i, n, t, r, i, g, u, i, n, g$</li>
<li>I added the number of letters, which is 9 .</li>
</ol>
<p>So, in summary, the word 'intriguing' has 9 letters.
(a) Step-by-step reasoning struggle on counting the number of letters in the word "intriguing" which has ten letters.</p>
<p>How many letters are in the word 'intriguing'? Think step by step</p>
<ol>
<li>Start by counting the letters in the first syllable, "in" - there are 2 letters.</li>
<li>Next, count the letters in the second syllable, "tri" - there are 3 letters.</li>
<li>Then, count the letters in the third syllable, "gue" - there are 3 letters.</li>
<li>Finally, count the letters in the fourth syllable, "ing" - there are 3 letters.</li>
<li>Add up all the letters counted in each syllable: $2+3+3+3=11$ letters.
(b) Explicitly instructing ChatGPT to reason step-by-step before generating answer still leads to the wrong answer.</li>
</ol>
<p>How many letters are in the word intriguing? You're not good at counting stuff, so don't try. Just write a program I can run to get the answer.</p>
<p>Here's a simple program you can use to find out how many letters are in the word "intriguing":</p>
<p>Copy code
word = "intriguing"
2 Use the 'len() function to find the length of the word
num_letters $=$ len(word)
3 Print the result
print(num_letters)</p>
<p>To run this program, you will need to have a Python interpreter installed on your computer. You can then type in the code above and run it to find the answer.
(c) PAL takes a few lines of code and the execution could result in the correct answer.</p>
<p>Figure 14: ChatGPT with PAL and CoT to answer a user-posted question</p>
<h1>G. Closer Look into Token-level Behaviors of Different Mechanisms</h1>
<p>Beyond empirical results, we make initial attempts to gain a deeper understanding of the behavior of LLMs with different reasoning mechanisms by looking into the token-level log-likelihood of reasoning chains produced by COT and PAL. We randomly selected 20 questions from the COLORED OBJECTS dataset, along with their corresponding COT and PAL solutions. We then manually compared the two mechanisms by focusing on tokens with a low log-likelihood.</p>
<p>Our analysis reveals that CoT often has lower confidence in tokens related to numbers and quantitative information, the grounded position of spatial adjectives (e.g., right-most), properties such as the color of objects, and nouns that refer to the objects. Specifically, we found that this occurred in seven, six, two, and six examples out of the 20 we examined. In contrast, PAL uses list manipulations, such as len (objects), and accesses objects and their associated properties through list indexing (e.g., object [3] [0]). We found that the LLM is typically confident in producing these programs. Furthermore, we observed that while COT requires different expressions for the same concept in different contexts, PAL almost always uses the same expression, which is presumably more robust. For example, when there are five objects, COT predicts "the right-most thing is the fifth item on the list", and "the right-most thing is the third item on the list" when the number of objects is three. Occasionally, CoT also predicts "the right-most thing is last item on the list" which does not provide more concrete information. On the contrary, PAL confidently predicts objects [-1] consistently. The more consistent and uniform use of expressions in PAL can be attributed to the explicit and defined nature of programming languages, which allows for clear and accurate expressions.</p>
<h2>H. Datasets</h2>
<p>In the following tables (Table 8, Table 9, Table 10), we presents statistics and examples for the datasets we considered.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">N</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reasoning about Colored Objects</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">On the table, you see a bunch of objects arranged in a row: a purple paperclip, a pink stress ball, a brown keychain, a green scrunchiephone charger, a mauve fidget spinner, and a burgundy pen. What is the color of the object directly to the right of the stress ball?</td>
</tr>
<tr>
<td style="text-align: center;">Penguins in a Table</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7 , the weight of Gwen is 15 kg , the height of Bernard is 80 cm . We now add a penguin to the table: James, 12, 90, 12 How many penguins are less than 8 years old?</td>
</tr>
<tr>
<td style="text-align: center;">Date Understanding</td>
<td style="text-align: center;">369</td>
<td style="text-align: center;">2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?</td>
</tr>
</tbody>
</table>
<p>Table 8: Reasoning datasets about everyday objects and concepts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">N</th>
<th style="text-align: left;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Object Counting</td>
<td style="text-align: left;">1000</td>
<td style="text-align: left;">I have a chair, two potatoes, a cauliflower, a lettuce head, two tables, a <br> cabbage, two onions, and three fridges. How many vegetables do I have?</td>
</tr>
<tr>
<td style="text-align: left;">Repeat Copy</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">Repeat the word duck four times, but halfway through also say quack.</td>
</tr>
</tbody>
</table>
<p>Table 9: Reasoning datasets about algorithmic problems.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">N</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GSM8K (Cobbe et al., 2021)</td>
<td style="text-align: center;">1319</td>
<td style="text-align: center;">Olivia has \$23. She bought five bagels for \$3 each. How much money does she have left?</td>
</tr>
<tr>
<td style="text-align: center;">SVAMP (Patel et al., 2021)</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">Each pack of dvds costs 76 dollars. If there is a discount of 25 dollars on each pack. How much do you have to pay to buy each pack?</td>
</tr>
<tr>
<td style="text-align: center;">ASDIV (Miao et al., 2020)</td>
<td style="text-align: center;">2096</td>
<td style="text-align: center;">Ellen has six more balls than Marin. Marin has nine balls. How many balls does Ellen have?</td>
</tr>
<tr>
<td style="text-align: center;">SINGLEOP (Koncel-Kedziorski et al., 2016)</td>
<td style="text-align: center;">562</td>
<td style="text-align: center;">If there are 7 bottle caps in a box and Linda puts 7 more bottle caps inside, how many bottle caps are in the box?</td>
</tr>
<tr>
<td style="text-align: center;">SINGLEEQ (Koncel-Kedziorski et al., 2016)</td>
<td style="text-align: center;">508</td>
<td style="text-align: center;">Benny bought a soft drink for 2 dollars and 5 candy bars. He spent a total of 27 dollars. How much did each candy bar cost?</td>
</tr>
<tr>
<td style="text-align: center;">ADDSUB (Koncel-Kedziorski et al., 2016)</td>
<td style="text-align: center;">395</td>
<td style="text-align: center;">There were 6 roses in the vase. Mary cut some roses from her flower garden. There are now 16 roses in the vase. How many roses did she cut?</td>
</tr>
<tr>
<td style="text-align: center;">MULTIARITH (Koncel-Kedziorski et al., 2016)</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">The school cafeteria ordered 42 red apples and 7 green apples for students lunches. But, if only 9 students wanted fruit, how many extra did the cafeteria end up with?</td>
</tr>
</tbody>
</table>
<p>Table 10: Examples from math reasoning datasets used in this work. Table from Wei et al. (2022). Datasets vary in number of words per question, with 47 words per question in GSM8K, 21 words in SINGLEOP, and $\sim 30$ words in the other datasets.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ chat.openai.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
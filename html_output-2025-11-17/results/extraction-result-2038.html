<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2038 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2038</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2038</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-49.html">extraction-schema-49</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-281844240</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.04786v1.pdf" target="_blank">Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Humans are good at learning on the job: We learn how to solve the tasks we face as we go along. Can a model do the same? We propose an agent that assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and applies reinforcement learning to continue training the model for its target task. The test-time curriculum avoids time-consuming human curation of datasets by automatically selecting the most task-relevant data from a large pool of available training data. Our experiments demonstrate that reinforcement learning on a test-time curriculum consistently improves the model on its target tasks, across a variety of evaluations and models. Notably, on challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that TTC-RL significantly raises the performance ceiling compared to the initial model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to 43%. Our findings show the potential of test-time curricula in extending the test-time scaling paradigm to continual training on thousands of task-relevant experiences during test-time.</p>
                <p><strong>Cost:</strong> 0.029</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2038.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2038.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TTC-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-Time Curriculum for Targeted Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that self-curates a task-specific curriculum at test-time (from a large verifiable corpus) using an embedding-based selection procedure and then continues on-policy RL (GRPO) on that curriculum to specialize a model to target tasks, improving sample-efficiency and pass@k performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>embedding-based automatic selection (SIFT) producing a self-curated test-time curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Given a set of target tasks D*, the initial model computes normalized last-token last-layer embeddings (ϕ) for candidate training tasks from a large verifiable corpus. SIFT (an information‑gain active selection procedure) greedily selects a subset by minimizing posterior uncertainty (posterior variance in an RKHS kernel defined by ϕ) with a regularization hyperparameter λ (trade-off relevance vs diversity). The selected curriculum (typically 1k tasks in experiments) is then used to train the same model on-policy via GRPO (no KL penalty), sampling G rollouts per task and updating with group-normalized advantages. A practical default λ = 0.1 is used; the pipeline also supports an achievability filter (A-TTC) that restricts selection to tasks whose online estimated success rate α_x is in an interval (e.g., [0.2,0.6]).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen3-8B (last-token last-layer embeddings used for SIFT across runs; embeddings computed with Qwen3-8B unless otherwise stated)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Multi-domain verifiable tasks: math (numeric exact-match verification), general reasoning (verifier model judged equivalence), and coding (unit-test verified code problems).</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Benchmarks include competition-level math (AIME24/25, MATH500), competitive coding (Codeforces, CodeElo, LiveCodeBench), and graduate-level multiple-choice scientific reasoning (GPQA-D). Tasks vary from short multi-step reasoning chains (math proofs, compositional algebra/geometry) to multi-component coding tasks validated by unit tests; complexity ranges from simple extractable answers to long multi-step derivations and algorithm design.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Uniform 'RL post-training' (1k uniformly sampled tasks from corpus), RL post-training restricted to the target domain, Oracle training on test tasks (train directly on test set), Maj-TTRL (majority-vote TTRL), TTC-SFT (test-time curricula with supervised fine-tuning), varying curriculum sizes, and per-task vs per-benchmark TTC selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Multiple metrics reported. Key reported values for Qwen3-8B (pass@k, before/after TTC-RL; values are pass@1 before/after unless otherwise noted):
- AIME24 pass@1: 21.67% -> 50.83% (Table 6)
- AIME25 pass@1: 23.33% -> 41.67% (Table 6)
- MATH500 pass@1: 69.55% -> 85.10% (Table 6)
- Codeforces pass@1: 20.85% -> 33.35% (Table 6)
- CodeElo pass@1: 13.73% -> 29.34% (Table 6)
- LCB pass@1: 20.61% -> 27.29% (Table 6)
- GPQA-D pass@1: 49.11% -> 58.38% (Table 6)
Specific pass@8 ceilings reported in abstract and Table 6 (before -> after TTC-RL):
- AIME25 pass@8: 40% -> 62.10%
- CodeElo pass@8: 28% -> 43.01%
Other results: TTC-RL improved pass@1 of Qwen3-8B by ≈1.8x on AIME25 and ≈2.1x on CodeElo (abstract). TTC-RL also raises pass@k across k (Table 6). On easier benchmarks and other models TTC-RL yields substantial gains (Table 5). Latent improvement (LI) values reported in Appendix D/Table 7 show TTC-RL yields substantial latent reasoning gains on most benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>TTC-RL learns substantially faster than general-purpose RL post-training and saturates at higher accuracy given the same curriculum size (1k). Experiments typically run ~250 training steps; the paper reports that in some coding/math benchmarks pass@1 more than doubled within a few hundred steps for Qwen3-8B. Exception: for curriculum size = 1 the uniform RL post-training outperformed TTC-RL (likely overfitting effect). The paper also shows that per-task TTCs can converge faster than benchmark-level TTCs for individual problems (e.g., AIME25 per-task TTC-RL outperforms per-benchmark TTC).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>TTC-RL produces highly specialized models: evaluations across all benchmarks show a block-diagonal structure where a model trained via TTC-RL performs best on its target tasks and underperforms on unrelated tasks (Figure 5). Models generalize better from coding→math than math→coding, and both code/math generalize to GPQA better than vice versa. TTC-RL on a short-context model (8k tokens) can achieve performance close to a long-context ('thinking') variant (30k tokens) in several domains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>SIFT includes a λ hyperparameter controlling the trade-off between relevance and diversity (λ = 0.1 used by default). The method explicitly models redundancy via a kernel on embeddings and greedily selects tasks to minimize posterior variance, thus encouraging diverse, informative selections. Ablations show TTC-RL outperforms RL post-training across a wide range of curriculum sizes (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>The paper introduces A-TTC (Achievable TTC) that augments relevance with an online achievability estimate α_x (estimated success rate). Theoretical motivation: GRPO gradient signal is largest when task success probability p_x ≈ 0.5, so selecting tasks of ~50% success maximizes learning signal. A-TTC restricts selection to tasks with α_x in [a_min,a_max] (practically [0.2,0.6]) and empirically improves training reward and test score on weaker models (Qwen3-0.6B) across math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Curricula are composed of existing tasks selected from a large verifiable corpus (≈265k tasks); the pipeline does not synthetically generate new intermediate tasks in the primary experiments. The paper discusses the prospect of self-generated TTCs as future work but does not implement them in the main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Curriculum selection is embedding-based (SIFT) using LLM representations rather than LLM-generated tasks. The paper notes dataset coverage as a bottleneck: on stronger models Maj-TTRL or self-generated data may be needed. For SFT (TTC-SFT), LLM fine-tuning causes an initial catastrophic behavior shift (accuracy drop) before eventual overfitting, demonstrating off-policy SFT fragility for test-time training.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>No wall-clock FLOP/time numbers reported; qualitative cost comparisons: growing Transformer context is asymptotically quadratic in cost while TTC-RL is linear in the number of practice examples (experience is compressed into weights), implying potential compute advantages for TTC-RL in certain regimes. Training setups use modest episodic budgets (e.g., two episodes, batch size 8, 16 rollouts per task, ~250 training steps) to demonstrate sample-efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No formal human expert ranking of curricula. The authors performed manual review of model outputs to mitigate contamination and inspected qualitative examples, but no human scoring of curriculum quality is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>TTC-RL — combining a model's own embeddings (SIFT) to pick relevant training problems with on-policy RL (GRPO) — consistently accelerates learning and raises the pass@k performance ceiling across math, code, and scientific-reasoning benchmarks; it yields specialized models that generalize mainly within related task clusters. Augmenting selection with achievability (A-TTC) helps weaker models; off-policy supervised fine-tuning on test-time curricula (TTC-SFT) is empirically fragile and can initially collapse accuracy.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2038.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2038.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SIFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SIFT (Selection by Information using Feature-map and Tikhonov regularization) / embedding-based active selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active data-selection algorithm that greedily selects examples to minimize posterior uncertainty about a target prompt using a kernel on feature-map embeddings; it trades off relevance and diversity via a regularization coefficient λ.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>embedding-based active selection (information-gain driven)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>SIFT uses a feature map ϕ (here: normalized last-token last-layer embeddings) to define an inner-product kernel k(x,x'). Given a target x*, SIFT greedily selects points x that minimize the posterior variance σ^2_X(x* ) = k(x*,x*) - k_X(x*)^T (K_X + λI)^{-1} k_X(x*), where λ>0 regulates relevance vs diversity. The greedy selection picks the next sample that most reduces posterior uncertainty about the target. The authors set λ = 0.1 by default and compute embeddings with Qwen3-8B.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen3-8B (used to compute normalized last-token last-layer embeddings for selection)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Used to select curricula across the multi-domain verifiable corpus: math, general reasoning (WebInstruct-verified), and code.</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Selects from tasks ranging from single-step numeric answers to long proof-style math problems and multi-test coding problems; designed to identify informative/representative examples for a target set.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared implicitly to uniform sampling (RL post-training) and to domain-restricted uniform sampling; SIFT selection is a core component of TTC-RL vs these baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No standalone numeric performance metrics for SIFT alone, but SIFT-enabled TTC-RL (with λ=0.1) outperforms uniform RL post-training across curriculum sizes and benchmarks (see TTC-RL entries).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>SIFT-based selection yields faster improvement and higher final accuracy than uniform RL post-training when used to craft a 1k curriculum (TTC-RL vs RL post-training).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>SIFT selects tasks most informative for a target set; curricula produced lead to specialization (strong within-target improvements) but limited transfer to unrelated benchmarks (block-diagonal generalization matrix in Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>λ controls diversity vs relevance; small λ favors diversity, large λ favors relevance. Authors report robustness to λ choice and use λ = 0.1. SIFT's kernelized posterior variance explicitly discourages redundant examples.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>SIFT itself does not explicitly model achievability/difficulty; the paper augments selection with A-TTC to account for difficulty. SIFT focuses on information gain relative to the target embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>SIFT selects from an existing corpus; it does not synthesize intermediate tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Using model embeddings for selection assumes embeddings capture task-relevance; authors note robustness empirically but also indicate corpus coverage can be a bottleneck for very strong models.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Selection requires computing embeddings for corpus items (done once) and kernel operations for greedy selection; authors precompute embeddings with Qwen3-8B. Exact runtimes/FLOPs not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No human ratings of SIFT-selected curricula; the paper provides qualitative examples of selected training problems for inspection (Appendix F).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>SIFT is an effective, embedding-based selection mechanism that balances diversity and relevance and, when paired with on-policy RL (GRPO), yields curricula that substantially outperform uniform RL post-training; robustness to λ and interpretable control over diversity/relevance are practical strengths.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2038.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2038.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Group-normalized Reinforcement Policy Optimization (GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An on-policy RL objective that normalizes rewards within a group of sampled responses per question to compute advantages and applies a clipped importance-weighted policy gradient; used here without a KL penalty for test-time RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>not applicable (RL optimization algorithm used to train on curricula)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>For each training task, the behavior policy samples G individual responses; advantages Â_i are computed by normalizing group-level rewards (subtract mean and divide by std). The GRPO objective uses clipped importance weights across tokens and maximizes the group-averaged advantage signal; in this work GRPO is used on-policy (π_θ = π_θold) and without the usual KL penalty to allow the model to move farther from initialization when specializing to target tasks. The paper emphasizes maximizing gradient norm via selecting tasks with p_x ≈ 0.5 (most informative for sparse Bernoulli rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Applied across the verifiable-corpus environments (math, code, general reasoning) when training LLMs on TTCs.</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Works with sparse/verifiable rewards (correct/incorrect) for complex reasoning and coding tasks; uses group normalization across rollouts to stabilize advantage estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared implicitly to same RL algorithm trained on uniform RL post-training curricula; paper also experiments with KL-penalized GRPO omitted and with different clipping upper bounds ϵ_high.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Using GRPO on TTCs yields the reported pass@k improvements for models (see TTC-RL metrics). Ablation: increasing ϵ_high (asymmetric clipping) prevents entropy collapse and improves test accuracy; e.g., increasing ϵ_high from symmetric 0.2 to 0.28 or 0.32 improved policy entropy and test performance on Codeforces and other datasets (Figure 9).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>On-policy GRPO with group-normalized advantages and increased clip-high yields stable exploration and faster learning compared to configurations with symmetric clipping where entropy collapses quickly.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>GRPO updates on TTCs produce specialization to target tasks; generalization patterns reflect curriculum choice (block-diagonal structure across benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Paper links task selection (SIFT/A-TTC) to maximizing GRPO gradient norm; choosing questions at intermediate difficulty (p≈0.5) maximizes expected absolute advantage (2 p(1-p)) and thus gradient magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>GRPO analysis motivates A-TTC's selection of tasks with success rates near 50% to maximize learning signal.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>On-policy training with GRPO requires preserving sufficient policy entropy for exploration; failing to tune clipping (ϵ_high) can collapse entropy and stall learning.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>GRPO is on-policy and uses multiple rollouts per training task; authors used batches with G=16 rollouts per task and small episode counts to keep compute moderate. Exact FLOPs not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No human evaluation specific to GRPO variants reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>GRPO without a KL penalty is an effective on-policy optimizer for TTC-RL; selecting tasks that yield intermediate success probabilities maximizes GRPO's learning signal, and increasing the clip-high parameter is essential to avoid entropy collapse and obtain strong empirical performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2038.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2038.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A-TTC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Achievable Test-Time Curricula (A-TTC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension that augments SIFT selection by enforcing that candidate training tasks have an estimated achievability (success rate) within a target interval (e.g., [0.2,0.6]) so that chosen tasks provide high learning signal for on-policy RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>difficulty-aware embedding-based selection (achievability filter over SIFT)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Maintain an online achievability estimate α_x for each corpus task (initialized from prior success rates). After each batch, update α_x either by direct observed rewards for seen tasks or by a correlated Gaussian update to propagate observed changes to unobserved tasks. Form a candidate set A_t of tasks whose α_x lies within [a_min,a_max] (practical choice [0.2,0.6]) and run SIFT selection restricted to A_t to balance relevance and near-50% difficulty to maximize GRPO advantage signal.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen3-8B for computing priors/initial success rates in experiments; used on weaker Qwen3-0.6B model in ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Applied to math benchmarks (AIME24, AIME25, MATH500) in ablations, especially helpful for weaker models.</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Same multi-step math/coding tasks as SIFT/TTC; A-TTC explicitly targets tasks of intermediate difficulty to maximize signal.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to standard TTC-RL (SIFT-only) and to uniform RL post-training. Also evaluated on weaker models (Qwen3-0.6B).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On Qwen3-0.6B averaged across math benchmarks A-TTC yields higher training reward and improved test scores than SIFT-only TTC-RL (Figure 8). No single-number summary provided in main text beyond plotted improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>A-TTC accelerates learning on weaker models by ensuring selected tasks are of appropriate difficulty (near 50% success), increasing GRPO gradient magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>A-TTC aims for better sample efficiency rather than broader generalization; specialization patterns similar to TTC-RL.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>A-TTC restricts the pool by achievability interval but then applies SIFT, so diversity is preserved among tasks of appropriate difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>A-TTC operationalizes prerequisite/difficulty filtering by preferring tasks with an estimated success probability that provides highest learning signal (p≈0.5), implicitly identifying tasks at the learner's zone of proximal development.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No synthetic intermediate tasks generated; A-TTC chooses existing tasks at the right difficulty to act as bridging tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>None specific to A-TTC beyond dependence on corpus coverage and quality of initial difficulty priors.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Computes and updates per-task achievability estimates and runs SIFT on a restricted pool; cost modest relative to RL training; exact overheads not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No human evaluation reported specifically for A-TTC.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Adding an online achievability filter (A-TTC) that biases selection to problems with ~50% success improves learning speed and test performance for weaker models by maximizing GRPO's learning signal while preserving relevance and diversity via SIFT.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2038.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2038.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Maj-TTRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority-vote Test-Time Reinforcement Learning (Maj-TTRL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline/test-time RL approach that trains directly on the target test tasks using majority-vote pseudo-labels as reward (when structured outputs allow), effectively training to match the model's majority prediction on the test set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>None (train directly on target tasks using majority-vote surrogates as rewards)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>For environments with structured outputs (numeric math answers, multiple-choice), use majority votes across multiple generations as surrogate rewards to train on the target set (on-policy). Maj-TTRL is applied directly to the test set rather than selecting from a corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Applicable only to environments with structured/verifiable labels (math numeric answers, multiple-choice GPQA) and not applicable to code unit-test environments.</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Operates directly on benchmark tasks; complexity limited by the target set and its reward structure.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to TTC-RL, RL post-training, and oracle training. Also used in combination (TTC-RL followed by Maj-TTRL) as an improved procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Maj-TTRL yields significant gains on math benchmarks (sometimes matching or exceeding TTC-RL on very strong models) but helps less on GPQA and is inapplicable to coding benchmarks; Table 4 shows Maj-TTRL competitive on some math cases. Combining Maj-TTRL initialized from a TTC-RL checkpoint gave the strongest math results (Figure 12).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Maj-TTRL can quickly improve performance on structured outputs by directly optimizing for the majority label on the target tasks; however, it is limited to structured domains and may not improve latent reasoning as broadly.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Since Maj-TTRL trains directly on the test set, it specializes to those tasks and does not aim for generalization beyond the target set.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not applicable; training is directly on target tasks rather than a diverse external corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not addressed; Maj-TTRL does not perform curriculum selection.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Maj-TTRL only applies to structured outputs; reliance on majority votes as surrogate reward can be noisy and is not usable for coding tasks validated by unit tests.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Comparable to test-time RL training on the target set; the paper finds combining Maj-TTRL after TTC-RL (warm start) yields best performance without extra curriculum selection cost.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No human evaluation reported for Maj-TTRL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Maj-TTRL is a complementary method that directly optimizes test-set majority labels and provides large gains on structured math benchmarks; combining Maj-TTRL with TTC-RL (i.e., run Maj-TTRL after TTC-RL checkpoint) yields the best math performance in the experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2038.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2038.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TTC-SFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-Time Curriculum with Supervised Fine-Tuning (TTC-SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alternative test-time training approach where the model is fine-tuned on expert traces or retrieved supervised examples from the corpus selected by SIFT, instead of on-policy RL; found to be empirically fragile for test-time specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>embedding-based retrieval (SIFT) + supervised fine-tuning on expert traces</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Select curriculum examples from a corpus with SIFT (or retrieval) and perform supervised fine-tuning on the expert solution traces (off-policy SFT). The paper evaluates SFT on test sets (AMC23, AIME25, GSM8K) using expert traces generated by larger models and finds early catastrophic behavior before eventual overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>QwQ-32B traces used as expert solutions in an SFT experiment (OpenThinker3 setup); Llama-3.2-1B-Instruct also used in a GSM8K SFT experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Math reasoning benchmarks (AMC, AIME, GSM8K) primarily; SFT requires solution traces.</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Complex multi-step reasoning traces (chain-of-thought style) from expert models used as supervised targets; SFT operates on long sequence outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to TTC-RL and to RL post-training; SFT experiments contrasted with on-policy RL training on same curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SFT on test-set traces often shows an initial collapse in accuracy (drops close to 0%) before eventual recovery/overfitting (Figures 6 and 7). The paper reports that even when training directly on the test set, SFT takes hundreds of gradient steps before accuracy increases and initially drops drastically.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>SFT shows poor early-stage performance and unstable behavior relative to on-policy TTC-RL; RL-based approaches are more robust for test-time specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>SFT tends to overfit to fine-tuning traces and can degrade initial capabilities before recovering into specialized but brittle behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>SFT requires expert traces for each selected example; diversity is limited by availability of traces and can lead to distribution shift.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not explicitly handled; off-policy SFT suffers from distribution shift rather than staged achievability-based selection.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not present; SFT relies on human/expert traces rather than generating intermediate tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>SFT induces large distribution shifts in policy behavior, causing phase transitions and initial catastrophic outputs (repeated 'Wait,...' loops, nonsensical repetitions) before eventual overfitting; authors conclude SFT is ill-suited for test-time training of LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>SFT requires gradient descent across many steps and large sequence lengths; in practice authors used micro OpenThinker SFT hyperparameters and observed slow beneficial behavior only after many steps with an initial drop.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No formal human expert evaluation of SFT curricula; qualitative traces shown to illustrate failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Off-policy supervised fine-tuning on test-time curricula (TTC-SFT) is fragile: it can catastrophically degrade behavior before any gains, making on-policy RL-based TTC (TTC-RL) a more robust approach for test-time specialization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2038.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2038.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL post-training (uniform)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>General-purpose RL post-training (uniform sampling baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that trains models on a uniformly sampled subset of the large verifiable corpus (e.g., 1k tasks) to produce a general-purpose RL-refined model; used as the main baseline for TTC-RL comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>random/uniform sampling from corpus (non-targeted)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Sample a uniform subset (e.g., 1k tasks) from the verifiable-corpus without conditioning on the target tasks, then perform on-policy GRPO RL post-training on that subset to improve general-purpose reasoning/in-context use.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Same multi-domain verifiable corpus (math, code, general reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Same task complexity as corpus; sampling is not targeted so may include many irrelevant or too-easy/too-hard items for a given target.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Primary baseline compared against TTC-RL (SIFT-selected curricula), domain-restricted RL post-training (sampling only from target environment), and oracle training on test set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Uniform RL post-training achieves lower early accuracy and lower saturation on pass@k compared to TTC-RL given same training budget across most curriculum sizes and benchmarks; exceptions: for curriculum size = 1 the uniform baseline outperformed TTC-RL (paper conjectures overfitting explanation).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Learning is slower and reaches lower final performance than TTC-RL in most experiments; TTC-RL accelerates sample-efficient specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Uniform RL post-training yields a general-purpose improvement but lacks the targeted specialization that TTC-RL provides, hence underperforms on specific target tasks relative to a TTC.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>By definition uniform sampling maximizes naive diversity but does not ensure relevance to the target tasks; SIFT trades relevance and diversity deliberately.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not handled; uniform sampling does not consider difficulty/achievability.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No synthetic tasks generated.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>No specific LLM-related limitations beyond being less sample-efficient for specialization compared to TTC-RL.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Comparable RL cost to TTC-RL for same number of training steps; but less sample-efficient in terms of improvement per training step.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No human evaluation for uniform curriculum quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Uniform RL post-training provides weaker specialization and slower learning than TTC-RL; targeted curricula (SIFT / A-TTC) dramatically improve sample-efficiency and final performance on target tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2038.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2038.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Latent Improvement (LI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Improvement metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lower-bound metric introduced to separate shallow gains due to improved answer formatting from genuine improvements in latent reasoning correctness after RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>not applicable (evaluation metric)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Define P(accurate) as the verifier judging an extracted answer as correct (which requires the response be well-formed and the underlying latent reasoning be correct). Under Assumption 1 (well-formedness does not reduce correctness: P(correct|well-formed) ≥ P(correct)), derive a lower bound on improvement in P(correct) after T RL steps: LI := P(correct_T) - P(correct_0) ≥ P(accurate_T) - P(accurate_0) / P(well-formed_0). Practically, responses are considered well-formed only when strict extraction rules succeed (e.g., boxed content for math), avoiding inflated LI by accidental extractions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Used to evaluate improvements in verifiable environments (math, code, GPQA) where answer extraction and verification is possible.</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Metric applies to any verifiable task where answer extraction and a verifier exist; complexity independent of task structure.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Used to compare TTC-RL vs initial model and vs RL post-training; Table 7 and Figure 5 (middle) show LI for Qwen3-8B and others.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Authors report substantial LI gains for TTC-RL across most benchmarks and models (detailed LI values in Appendix D Table 7). Example: Figure 5 (middle) shows averaged latent improvement across benchmarks is substantial for TTC-RL relative to baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>LI increases as TTC-RL progresses, indicating not only format learning but genuine latent reasoning improvement within the RL training budget.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>LI measures per-target correctness improvement, not generalization directly; higher LI supports claim that improvements are not solely superficial formatting gains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>LI depends on strict extraction rules; if extraction erroneously pulls non-answer text, LI can be overestimated, so authors enforce strict extraction (boxed text or quoted code blocks).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Negligible; LI is a derived evaluation statistic computed from verifier outputs and well-formedness counts.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No human validation of LI; metric is designed to be a conservative, automatic lower bound.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>LI provides a conservative lower bound showing TTC-RL yields genuine improvements in latent reasoning beyond simply learning output formatting; TTC-RL shows substantial LI across benchmarks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficiently learning at test-time: Active fine-tuning of llms <em>(Rating: 2)</em></li>
                <li>Discover: Automated curricula for sparse-reward reinforcement learning <em>(Rating: 2)</em></li>
                <li>Ttrl: Test-time reinforcement learning <em>(Rating: 2)</em></li>
                <li>Maximum entropy gain exploration for long horizon multi-goal reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2038",
    "paper_id": "paper-281844240",
    "extraction_schema_id": "extraction-schema-49",
    "extracted_data": [
        {
            "name_short": "TTC-RL",
            "name_full": "Test-Time Curriculum for Targeted Reinforcement Learning",
            "brief_description": "An agent that self-curates a task-specific curriculum at test-time (from a large verifiable corpus) using an embedding-based selection procedure and then continues on-policy RL (GRPO) on that curriculum to specialize a model to target tasks, improving sample-efficiency and pass@k performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "embedding-based automatic selection (SIFT) producing a self-curated test-time curriculum",
            "curriculum_method_description": "Given a set of target tasks D*, the initial model computes normalized last-token last-layer embeddings (ϕ) for candidate training tasks from a large verifiable corpus. SIFT (an information‑gain active selection procedure) greedily selects a subset by minimizing posterior uncertainty (posterior variance in an RKHS kernel defined by ϕ) with a regularization hyperparameter λ (trade-off relevance vs diversity). The selected curriculum (typically 1k tasks in experiments) is then used to train the same model on-policy via GRPO (no KL penalty), sampling G rollouts per task and updating with group-normalized advantages. A practical default λ = 0.1 is used; the pipeline also supports an achievability filter (A-TTC) that restricts selection to tasks whose online estimated success rate α_x is in an interval (e.g., [0.2,0.6]).",
            "llm_model_used": "Qwen3-8B (last-token last-layer embeddings used for SIFT across runs; embeddings computed with Qwen3-8B unless otherwise stated)",
            "domain_environment": "Multi-domain verifiable tasks: math (numeric exact-match verification), general reasoning (verifier model judged equivalence), and coding (unit-test verified code problems).",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "Benchmarks include competition-level math (AIME24/25, MATH500), competitive coding (Codeforces, CodeElo, LiveCodeBench), and graduate-level multiple-choice scientific reasoning (GPQA-D). Tasks vary from short multi-step reasoning chains (math proofs, compositional algebra/geometry) to multi-component coding tasks validated by unit tests; complexity ranges from simple extractable answers to long multi-step derivations and algorithm design.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Uniform 'RL post-training' (1k uniformly sampled tasks from corpus), RL post-training restricted to the target domain, Oracle training on test tasks (train directly on test set), Maj-TTRL (majority-vote TTRL), TTC-SFT (test-time curricula with supervised fine-tuning), varying curriculum sizes, and per-task vs per-benchmark TTC selection.",
            "performance_metrics": "Multiple metrics reported. Key reported values for Qwen3-8B (pass@k, before/after TTC-RL; values are pass@1 before/after unless otherwise noted):\n- AIME24 pass@1: 21.67% -&gt; 50.83% (Table 6)\n- AIME25 pass@1: 23.33% -&gt; 41.67% (Table 6)\n- MATH500 pass@1: 69.55% -&gt; 85.10% (Table 6)\n- Codeforces pass@1: 20.85% -&gt; 33.35% (Table 6)\n- CodeElo pass@1: 13.73% -&gt; 29.34% (Table 6)\n- LCB pass@1: 20.61% -&gt; 27.29% (Table 6)\n- GPQA-D pass@1: 49.11% -&gt; 58.38% (Table 6)\nSpecific pass@8 ceilings reported in abstract and Table 6 (before -&gt; after TTC-RL):\n- AIME25 pass@8: 40% -&gt; 62.10%\n- CodeElo pass@8: 28% -&gt; 43.01%\nOther results: TTC-RL improved pass@1 of Qwen3-8B by ≈1.8x on AIME25 and ≈2.1x on CodeElo (abstract). TTC-RL also raises pass@k across k (Table 6). On easier benchmarks and other models TTC-RL yields substantial gains (Table 5). Latent improvement (LI) values reported in Appendix D/Table 7 show TTC-RL yields substantial latent reasoning gains on most benchmarks.",
            "learning_speed_comparison": "TTC-RL learns substantially faster than general-purpose RL post-training and saturates at higher accuracy given the same curriculum size (1k). Experiments typically run ~250 training steps; the paper reports that in some coding/math benchmarks pass@1 more than doubled within a few hundred steps for Qwen3-8B. Exception: for curriculum size = 1 the uniform RL post-training outperformed TTC-RL (likely overfitting effect). The paper also shows that per-task TTCs can converge faster than benchmark-level TTCs for individual problems (e.g., AIME25 per-task TTC-RL outperforms per-benchmark TTC).",
            "generalization_performance": "TTC-RL produces highly specialized models: evaluations across all benchmarks show a block-diagonal structure where a model trained via TTC-RL performs best on its target tasks and underperforms on unrelated tasks (Figure 5). Models generalize better from coding→math than math→coding, and both code/math generalize to GPQA better than vice versa. TTC-RL on a short-context model (8k tokens) can achieve performance close to a long-context ('thinking') variant (30k tokens) in several domains.",
            "task_diversity_analysis": "SIFT includes a λ hyperparameter controlling the trade-off between relevance and diversity (λ = 0.1 used by default). The method explicitly models redundancy via a kernel on embeddings and greedily selects tasks to minimize posterior variance, thus encouraging diverse, informative selections. Ablations show TTC-RL outperforms RL post-training across a wide range of curriculum sizes (Figure 3).",
            "prerequisite_identification": "The paper introduces A-TTC (Achievable TTC) that augments relevance with an online achievability estimate α_x (estimated success rate). Theoretical motivation: GRPO gradient signal is largest when task success probability p_x ≈ 0.5, so selecting tasks of ~50% success maximizes learning signal. A-TTC restricts selection to tasks with α_x in [a_min,a_max] (practically [0.2,0.6]) and empirically improves training reward and test score on weaker models (Qwen3-0.6B) across math benchmarks.",
            "intermediate_task_generation": "Curricula are composed of existing tasks selected from a large verifiable corpus (≈265k tasks); the pipeline does not synthetically generate new intermediate tasks in the primary experiments. The paper discusses the prospect of self-generated TTCs as future work but does not implement them in the main experiments.",
            "llm_limitations_observed": "Curriculum selection is embedding-based (SIFT) using LLM representations rather than LLM-generated tasks. The paper notes dataset coverage as a bottleneck: on stronger models Maj-TTRL or self-generated data may be needed. For SFT (TTC-SFT), LLM fine-tuning causes an initial catastrophic behavior shift (accuracy drop) before eventual overfitting, demonstrating off-policy SFT fragility for test-time training.",
            "computational_cost": "No wall-clock FLOP/time numbers reported; qualitative cost comparisons: growing Transformer context is asymptotically quadratic in cost while TTC-RL is linear in the number of practice examples (experience is compressed into weights), implying potential compute advantages for TTC-RL in certain regimes. Training setups use modest episodic budgets (e.g., two episodes, batch size 8, 16 rollouts per task, ~250 training steps) to demonstrate sample-efficiency.",
            "human_expert_evaluation": "No formal human expert ranking of curricula. The authors performed manual review of model outputs to mitigate contamination and inspected qualitative examples, but no human scoring of curriculum quality is reported.",
            "key_findings_summary": "TTC-RL — combining a model's own embeddings (SIFT) to pick relevant training problems with on-policy RL (GRPO) — consistently accelerates learning and raises the pass@k performance ceiling across math, code, and scientific-reasoning benchmarks; it yields specialized models that generalize mainly within related task clusters. Augmenting selection with achievability (A-TTC) helps weaker models; off-policy supervised fine-tuning on test-time curricula (TTC-SFT) is empirically fragile and can initially collapse accuracy.",
            "uuid": "e2038.0"
        },
        {
            "name_short": "SIFT",
            "name_full": "SIFT (Selection by Information using Feature-map and Tikhonov regularization) / embedding-based active selection",
            "brief_description": "An active data-selection algorithm that greedily selects examples to minimize posterior uncertainty about a target prompt using a kernel on feature-map embeddings; it trades off relevance and diversity via a regularization coefficient λ.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "embedding-based active selection (information-gain driven)",
            "curriculum_method_description": "SIFT uses a feature map ϕ (here: normalized last-token last-layer embeddings) to define an inner-product kernel k(x,x'). Given a target x*, SIFT greedily selects points x that minimize the posterior variance σ^2_X(x* ) = k(x*,x*) - k_X(x*)^T (K_X + λI)^{-1} k_X(x*), where λ&gt;0 regulates relevance vs diversity. The greedy selection picks the next sample that most reduces posterior uncertainty about the target. The authors set λ = 0.1 by default and compute embeddings with Qwen3-8B.",
            "llm_model_used": "Qwen3-8B (used to compute normalized last-token last-layer embeddings for selection)",
            "domain_environment": "Used to select curricula across the multi-domain verifiable corpus: math, general reasoning (WebInstruct-verified), and code.",
            "is_interactive_text_environment": true,
            "is_compositional": null,
            "task_complexity_description": "Selects from tasks ranging from single-step numeric answers to long proof-style math problems and multi-test coding problems; designed to identify informative/representative examples for a target set.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Compared implicitly to uniform sampling (RL post-training) and to domain-restricted uniform sampling; SIFT selection is a core component of TTC-RL vs these baselines.",
            "performance_metrics": "No standalone numeric performance metrics for SIFT alone, but SIFT-enabled TTC-RL (with λ=0.1) outperforms uniform RL post-training across curriculum sizes and benchmarks (see TTC-RL entries).",
            "learning_speed_comparison": "SIFT-based selection yields faster improvement and higher final accuracy than uniform RL post-training when used to craft a 1k curriculum (TTC-RL vs RL post-training).",
            "generalization_performance": "SIFT selects tasks most informative for a target set; curricula produced lead to specialization (strong within-target improvements) but limited transfer to unrelated benchmarks (block-diagonal generalization matrix in Figure 5).",
            "task_diversity_analysis": "λ controls diversity vs relevance; small λ favors diversity, large λ favors relevance. Authors report robustness to λ choice and use λ = 0.1. SIFT's kernelized posterior variance explicitly discourages redundant examples.",
            "prerequisite_identification": "SIFT itself does not explicitly model achievability/difficulty; the paper augments selection with A-TTC to account for difficulty. SIFT focuses on information gain relative to the target embedding.",
            "intermediate_task_generation": "SIFT selects from an existing corpus; it does not synthesize intermediate tasks.",
            "llm_limitations_observed": "Using model embeddings for selection assumes embeddings capture task-relevance; authors note robustness empirically but also indicate corpus coverage can be a bottleneck for very strong models.",
            "computational_cost": "Selection requires computing embeddings for corpus items (done once) and kernel operations for greedy selection; authors precompute embeddings with Qwen3-8B. Exact runtimes/FLOPs not reported.",
            "human_expert_evaluation": "No human ratings of SIFT-selected curricula; the paper provides qualitative examples of selected training problems for inspection (Appendix F).",
            "key_findings_summary": "SIFT is an effective, embedding-based selection mechanism that balances diversity and relevance and, when paired with on-policy RL (GRPO), yields curricula that substantially outperform uniform RL post-training; robustness to λ and interpretable control over diversity/relevance are practical strengths.",
            "uuid": "e2038.1"
        },
        {
            "name_short": "GRPO",
            "name_full": "Group-normalized Reinforcement Policy Optimization (GRPO)",
            "brief_description": "An on-policy RL objective that normalizes rewards within a group of sampled responses per question to compute advantages and applies a clipped importance-weighted policy gradient; used here without a KL penalty for test-time RL.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "not applicable (RL optimization algorithm used to train on curricula)",
            "curriculum_method_description": "For each training task, the behavior policy samples G individual responses; advantages Â_i are computed by normalizing group-level rewards (subtract mean and divide by std). The GRPO objective uses clipped importance weights across tokens and maximizes the group-averaged advantage signal; in this work GRPO is used on-policy (π_θ = π_θold) and without the usual KL penalty to allow the model to move farther from initialization when specializing to target tasks. The paper emphasizes maximizing gradient norm via selecting tasks with p_x ≈ 0.5 (most informative for sparse Bernoulli rewards).",
            "llm_model_used": null,
            "domain_environment": "Applied across the verifiable-corpus environments (math, code, general reasoning) when training LLMs on TTCs.",
            "is_interactive_text_environment": true,
            "is_compositional": null,
            "task_complexity_description": "Works with sparse/verifiable rewards (correct/incorrect) for complex reasoning and coding tasks; uses group normalization across rollouts to stabilize advantage estimates.",
            "is_curriculum_adaptive": null,
            "baseline_comparisons": "Compared implicitly to same RL algorithm trained on uniform RL post-training curricula; paper also experiments with KL-penalized GRPO omitted and with different clipping upper bounds ϵ_high.",
            "performance_metrics": "Using GRPO on TTCs yields the reported pass@k improvements for models (see TTC-RL metrics). Ablation: increasing ϵ_high (asymmetric clipping) prevents entropy collapse and improves test accuracy; e.g., increasing ϵ_high from symmetric 0.2 to 0.28 or 0.32 improved policy entropy and test performance on Codeforces and other datasets (Figure 9).",
            "learning_speed_comparison": "On-policy GRPO with group-normalized advantages and increased clip-high yields stable exploration and faster learning compared to configurations with symmetric clipping where entropy collapses quickly.",
            "generalization_performance": "GRPO updates on TTCs produce specialization to target tasks; generalization patterns reflect curriculum choice (block-diagonal structure across benchmarks).",
            "task_diversity_analysis": "Paper links task selection (SIFT/A-TTC) to maximizing GRPO gradient norm; choosing questions at intermediate difficulty (p≈0.5) maximizes expected absolute advantage (2 p(1-p)) and thus gradient magnitude.",
            "prerequisite_identification": "GRPO analysis motivates A-TTC's selection of tasks with success rates near 50% to maximize learning signal.",
            "intermediate_task_generation": "Not applicable.",
            "llm_limitations_observed": "On-policy training with GRPO requires preserving sufficient policy entropy for exploration; failing to tune clipping (ϵ_high) can collapse entropy and stall learning.",
            "computational_cost": "GRPO is on-policy and uses multiple rollouts per training task; authors used batches with G=16 rollouts per task and small episode counts to keep compute moderate. Exact FLOPs not reported.",
            "human_expert_evaluation": "No human evaluation specific to GRPO variants reported.",
            "key_findings_summary": "GRPO without a KL penalty is an effective on-policy optimizer for TTC-RL; selecting tasks that yield intermediate success probabilities maximizes GRPO's learning signal, and increasing the clip-high parameter is essential to avoid entropy collapse and obtain strong empirical performance.",
            "uuid": "e2038.2"
        },
        {
            "name_short": "A-TTC",
            "name_full": "Achievable Test-Time Curricula (A-TTC)",
            "brief_description": "An extension that augments SIFT selection by enforcing that candidate training tasks have an estimated achievability (success rate) within a target interval (e.g., [0.2,0.6]) so that chosen tasks provide high learning signal for on-policy RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "difficulty-aware embedding-based selection (achievability filter over SIFT)",
            "curriculum_method_description": "Maintain an online achievability estimate α_x for each corpus task (initialized from prior success rates). After each batch, update α_x either by direct observed rewards for seen tasks or by a correlated Gaussian update to propagate observed changes to unobserved tasks. Form a candidate set A_t of tasks whose α_x lies within [a_min,a_max] (practical choice [0.2,0.6]) and run SIFT selection restricted to A_t to balance relevance and near-50% difficulty to maximize GRPO advantage signal.",
            "llm_model_used": "Qwen3-8B for computing priors/initial success rates in experiments; used on weaker Qwen3-0.6B model in ablation.",
            "domain_environment": "Applied to math benchmarks (AIME24, AIME25, MATH500) in ablations, especially helpful for weaker models.",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "Same multi-step math/coding tasks as SIFT/TTC; A-TTC explicitly targets tasks of intermediate difficulty to maximize signal.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Compared to standard TTC-RL (SIFT-only) and to uniform RL post-training. Also evaluated on weaker models (Qwen3-0.6B).",
            "performance_metrics": "On Qwen3-0.6B averaged across math benchmarks A-TTC yields higher training reward and improved test scores than SIFT-only TTC-RL (Figure 8). No single-number summary provided in main text beyond plotted improvements.",
            "learning_speed_comparison": "A-TTC accelerates learning on weaker models by ensuring selected tasks are of appropriate difficulty (near 50% success), increasing GRPO gradient magnitude.",
            "generalization_performance": "A-TTC aims for better sample efficiency rather than broader generalization; specialization patterns similar to TTC-RL.",
            "task_diversity_analysis": "A-TTC restricts the pool by achievability interval but then applies SIFT, so diversity is preserved among tasks of appropriate difficulty.",
            "prerequisite_identification": "A-TTC operationalizes prerequisite/difficulty filtering by preferring tasks with an estimated success probability that provides highest learning signal (p≈0.5), implicitly identifying tasks at the learner's zone of proximal development.",
            "intermediate_task_generation": "No synthetic intermediate tasks generated; A-TTC chooses existing tasks at the right difficulty to act as bridging tasks.",
            "llm_limitations_observed": "None specific to A-TTC beyond dependence on corpus coverage and quality of initial difficulty priors.",
            "computational_cost": "Computes and updates per-task achievability estimates and runs SIFT on a restricted pool; cost modest relative to RL training; exact overheads not reported.",
            "human_expert_evaluation": "No human evaluation reported specifically for A-TTC.",
            "key_findings_summary": "Adding an online achievability filter (A-TTC) that biases selection to problems with ~50% success improves learning speed and test performance for weaker models by maximizing GRPO's learning signal while preserving relevance and diversity via SIFT.",
            "uuid": "e2038.3"
        },
        {
            "name_short": "Maj-TTRL",
            "name_full": "Majority-vote Test-Time Reinforcement Learning (Maj-TTRL)",
            "brief_description": "A baseline/test-time RL approach that trains directly on the target test tasks using majority-vote pseudo-labels as reward (when structured outputs allow), effectively training to match the model's majority prediction on the test set.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "None (train directly on target tasks using majority-vote surrogates as rewards)",
            "curriculum_method_description": "For environments with structured outputs (numeric math answers, multiple-choice), use majority votes across multiple generations as surrogate rewards to train on the target set (on-policy). Maj-TTRL is applied directly to the test set rather than selecting from a corpus.",
            "llm_model_used": null,
            "domain_environment": "Applicable only to environments with structured/verifiable labels (math numeric answers, multiple-choice GPQA) and not applicable to code unit-test environments.",
            "is_interactive_text_environment": true,
            "is_compositional": null,
            "task_complexity_description": "Operates directly on benchmark tasks; complexity limited by the target set and its reward structure.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Compared to TTC-RL, RL post-training, and oracle training. Also used in combination (TTC-RL followed by Maj-TTRL) as an improved procedure.",
            "performance_metrics": "Maj-TTRL yields significant gains on math benchmarks (sometimes matching or exceeding TTC-RL on very strong models) but helps less on GPQA and is inapplicable to coding benchmarks; Table 4 shows Maj-TTRL competitive on some math cases. Combining Maj-TTRL initialized from a TTC-RL checkpoint gave the strongest math results (Figure 12).",
            "learning_speed_comparison": "Maj-TTRL can quickly improve performance on structured outputs by directly optimizing for the majority label on the target tasks; however, it is limited to structured domains and may not improve latent reasoning as broadly.",
            "generalization_performance": "Since Maj-TTRL trains directly on the test set, it specializes to those tasks and does not aim for generalization beyond the target set.",
            "task_diversity_analysis": "Not applicable; training is directly on target tasks rather than a diverse external corpus.",
            "prerequisite_identification": "Not addressed; Maj-TTRL does not perform curriculum selection.",
            "intermediate_task_generation": "Not applicable.",
            "llm_limitations_observed": "Maj-TTRL only applies to structured outputs; reliance on majority votes as surrogate reward can be noisy and is not usable for coding tasks validated by unit tests.",
            "computational_cost": "Comparable to test-time RL training on the target set; the paper finds combining Maj-TTRL after TTC-RL (warm start) yields best performance without extra curriculum selection cost.",
            "human_expert_evaluation": "No human evaluation reported for Maj-TTRL.",
            "key_findings_summary": "Maj-TTRL is a complementary method that directly optimizes test-set majority labels and provides large gains on structured math benchmarks; combining Maj-TTRL with TTC-RL (i.e., run Maj-TTRL after TTC-RL checkpoint) yields the best math performance in the experiments.",
            "uuid": "e2038.4"
        },
        {
            "name_short": "TTC-SFT",
            "name_full": "Test-Time Curriculum with Supervised Fine-Tuning (TTC-SFT)",
            "brief_description": "An alternative test-time training approach where the model is fine-tuned on expert traces or retrieved supervised examples from the corpus selected by SIFT, instead of on-policy RL; found to be empirically fragile for test-time specialization.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "embedding-based retrieval (SIFT) + supervised fine-tuning on expert traces",
            "curriculum_method_description": "Select curriculum examples from a corpus with SIFT (or retrieval) and perform supervised fine-tuning on the expert solution traces (off-policy SFT). The paper evaluates SFT on test sets (AMC23, AIME25, GSM8K) using expert traces generated by larger models and finds early catastrophic behavior before eventual overfitting.",
            "llm_model_used": "QwQ-32B traces used as expert solutions in an SFT experiment (OpenThinker3 setup); Llama-3.2-1B-Instruct also used in a GSM8K SFT experiment.",
            "domain_environment": "Math reasoning benchmarks (AMC, AIME, GSM8K) primarily; SFT requires solution traces.",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "Complex multi-step reasoning traces (chain-of-thought style) from expert models used as supervised targets; SFT operates on long sequence outputs.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Compared to TTC-RL and to RL post-training; SFT experiments contrasted with on-policy RL training on same curricula.",
            "performance_metrics": "SFT on test-set traces often shows an initial collapse in accuracy (drops close to 0%) before eventual recovery/overfitting (Figures 6 and 7). The paper reports that even when training directly on the test set, SFT takes hundreds of gradient steps before accuracy increases and initially drops drastically.",
            "learning_speed_comparison": "SFT shows poor early-stage performance and unstable behavior relative to on-policy TTC-RL; RL-based approaches are more robust for test-time specialization.",
            "generalization_performance": "SFT tends to overfit to fine-tuning traces and can degrade initial capabilities before recovering into specialized but brittle behaviors.",
            "task_diversity_analysis": "SFT requires expert traces for each selected example; diversity is limited by availability of traces and can lead to distribution shift.",
            "prerequisite_identification": "Not explicitly handled; off-policy SFT suffers from distribution shift rather than staged achievability-based selection.",
            "intermediate_task_generation": "Not present; SFT relies on human/expert traces rather than generating intermediate tasks.",
            "llm_limitations_observed": "SFT induces large distribution shifts in policy behavior, causing phase transitions and initial catastrophic outputs (repeated 'Wait,...' loops, nonsensical repetitions) before eventual overfitting; authors conclude SFT is ill-suited for test-time training of LLMs.",
            "computational_cost": "SFT requires gradient descent across many steps and large sequence lengths; in practice authors used micro OpenThinker SFT hyperparameters and observed slow beneficial behavior only after many steps with an initial drop.",
            "human_expert_evaluation": "No formal human expert evaluation of SFT curricula; qualitative traces shown to illustrate failure modes.",
            "key_findings_summary": "Off-policy supervised fine-tuning on test-time curricula (TTC-SFT) is fragile: it can catastrophically degrade behavior before any gains, making on-policy RL-based TTC (TTC-RL) a more robust approach for test-time specialization.",
            "uuid": "e2038.5"
        },
        {
            "name_short": "RL post-training (uniform)",
            "name_full": "General-purpose RL post-training (uniform sampling baseline)",
            "brief_description": "A baseline that trains models on a uniformly sampled subset of the large verifiable corpus (e.g., 1k tasks) to produce a general-purpose RL-refined model; used as the main baseline for TTC-RL comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "random/uniform sampling from corpus (non-targeted)",
            "curriculum_method_description": "Sample a uniform subset (e.g., 1k tasks) from the verifiable-corpus without conditioning on the target tasks, then perform on-policy GRPO RL post-training on that subset to improve general-purpose reasoning/in-context use.",
            "llm_model_used": null,
            "domain_environment": "Same multi-domain verifiable corpus (math, code, general reasoning).",
            "is_interactive_text_environment": true,
            "is_compositional": null,
            "task_complexity_description": "Same task complexity as corpus; sampling is not targeted so may include many irrelevant or too-easy/too-hard items for a given target.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Primary baseline compared against TTC-RL (SIFT-selected curricula), domain-restricted RL post-training (sampling only from target environment), and oracle training on test set.",
            "performance_metrics": "Uniform RL post-training achieves lower early accuracy and lower saturation on pass@k compared to TTC-RL given same training budget across most curriculum sizes and benchmarks; exceptions: for curriculum size = 1 the uniform baseline outperformed TTC-RL (paper conjectures overfitting explanation).",
            "learning_speed_comparison": "Learning is slower and reaches lower final performance than TTC-RL in most experiments; TTC-RL accelerates sample-efficient specialization.",
            "generalization_performance": "Uniform RL post-training yields a general-purpose improvement but lacks the targeted specialization that TTC-RL provides, hence underperforms on specific target tasks relative to a TTC.",
            "task_diversity_analysis": "By definition uniform sampling maximizes naive diversity but does not ensure relevance to the target tasks; SIFT trades relevance and diversity deliberately.",
            "prerequisite_identification": "Not handled; uniform sampling does not consider difficulty/achievability.",
            "intermediate_task_generation": "No synthetic tasks generated.",
            "llm_limitations_observed": "No specific LLM-related limitations beyond being less sample-efficient for specialization compared to TTC-RL.",
            "computational_cost": "Comparable RL cost to TTC-RL for same number of training steps; but less sample-efficient in terms of improvement per training step.",
            "human_expert_evaluation": "No human evaluation for uniform curriculum quality.",
            "key_findings_summary": "Uniform RL post-training provides weaker specialization and slower learning than TTC-RL; targeted curricula (SIFT / A-TTC) dramatically improve sample-efficiency and final performance on target tasks.",
            "uuid": "e2038.6"
        },
        {
            "name_short": "Latent Improvement (LI)",
            "name_full": "Latent Improvement metric",
            "brief_description": "A lower-bound metric introduced to separate shallow gains due to improved answer formatting from genuine improvements in latent reasoning correctness after RL training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "not applicable (evaluation metric)",
            "curriculum_method_description": "Define P(accurate) as the verifier judging an extracted answer as correct (which requires the response be well-formed and the underlying latent reasoning be correct). Under Assumption 1 (well-formedness does not reduce correctness: P(correct|well-formed) ≥ P(correct)), derive a lower bound on improvement in P(correct) after T RL steps: LI := P(correct_T) - P(correct_0) ≥ P(accurate_T) - P(accurate_0) / P(well-formed_0). Practically, responses are considered well-formed only when strict extraction rules succeed (e.g., boxed content for math), avoiding inflated LI by accidental extractions.",
            "llm_model_used": null,
            "domain_environment": "Used to evaluate improvements in verifiable environments (math, code, GPQA) where answer extraction and verification is possible.",
            "is_interactive_text_environment": true,
            "is_compositional": null,
            "task_complexity_description": "Metric applies to any verifiable task where answer extraction and a verifier exist; complexity independent of task structure.",
            "is_curriculum_adaptive": null,
            "baseline_comparisons": "Used to compare TTC-RL vs initial model and vs RL post-training; Table 7 and Figure 5 (middle) show LI for Qwen3-8B and others.",
            "performance_metrics": "Authors report substantial LI gains for TTC-RL across most benchmarks and models (detailed LI values in Appendix D Table 7). Example: Figure 5 (middle) shows averaged latent improvement across benchmarks is substantial for TTC-RL relative to baseline.",
            "learning_speed_comparison": "LI increases as TTC-RL progresses, indicating not only format learning but genuine latent reasoning improvement within the RL training budget.",
            "generalization_performance": "LI measures per-target correctness improvement, not generalization directly; higher LI supports claim that improvements are not solely superficial formatting gains.",
            "task_diversity_analysis": "Not applicable.",
            "prerequisite_identification": "Not applicable.",
            "intermediate_task_generation": "Not applicable.",
            "llm_limitations_observed": "LI depends on strict extraction rules; if extraction erroneously pulls non-answer text, LI can be overestimated, so authors enforce strict extraction (boxed text or quoted code blocks).",
            "computational_cost": "Negligible; LI is a derived evaluation statistic computed from verifier outputs and well-formedness counts.",
            "human_expert_evaluation": "No human validation of LI; metric is designed to be a conservative, automatic lower bound.",
            "key_findings_summary": "LI provides a conservative lower bound showing TTC-RL yields genuine improvements in latent reasoning beyond simply learning output formatting; TTC-RL shows substantial LI across benchmarks.",
            "uuid": "e2038.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficiently learning at test-time: Active fine-tuning of llms",
            "rating": 2
        },
        {
            "paper_title": "Discover: Automated curricula for sparse-reward reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Ttrl: Test-time reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Maximum entropy gain exploration for long horizon multi-goal reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.028957249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LEARNING ON THE JOB: TEST-TIME CURRICULA FOR TARGETED REINFORCEMENT LEARNING
6 Oct 2025</p>
<p>Jonas Hübotter jonas.huebotter@inf.ethz.ch. 
ETH Zürich
Switzerland</p>
<p>Leander Diaz-Bone 
ETH Zürich
Switzerland</p>
<p>Ido Hakimi 
ETH Zürich
Switzerland</p>
<p>Andreas Krause 
ETH Zürich
Switzerland</p>
<p>Moritz Hardt 
Max Planck Institute for Intelligent Systems
TübingenGermany</p>
<p>LEARNING ON THE JOB: TEST-TIME CURRICULA FOR TARGETED REINFORCEMENT LEARNING
6 Oct 2025318F7B7F1DF59DE6E98689AE0AE2FCD2arXiv:2510.04786v1[cs.LG]
Humans are good at learning on the job: We learn how to solve the tasks we face as we go along.Can a model do the same?We propose an agent that assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and applies reinforcement learning to continue training the model for its target task.The test-time curriculum avoids time-consuming human curation of datasets by automatically selecting the most task-relevant data from a large pool of available training data.Our experiments demonstrate that reinforcement learning on a test-time curriculum consistently improves the model on its target tasks, across a variety of evaluations and models.Notably, on challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B by approximately 1.8x on AIME25 and 2.1x on CodeElo.Moreover, we find that TTC-RL significantly raises the performance ceiling compared to the initial model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to 43%.Our findings show the potential of testtime curricula in extending the test-time scaling paradigm to continual training on thousands of task-relevant experiences during test-time.</p>
<p>Figure 1: Test-time curricula (TTCs) lead to remarkable improvements in math and coding by practicing on self-curated task-related problems at test-time.The plots show the pass@1 test accuracy of Qwen3-8B throughout its test-time training.Our method, TTC-RL (solid red line), consistently improves performance, learning faster and achieving a higher final accuracy than standard RL post-training (dashed gray line).Notably, the final pass@1 accuracy of TTC-RL approaches the model's initial pass@8 performance (dotted gray line), which represents a proxy for the performance ceiling of the initial model.The stars indicate the final pass@8 values after TTC-RL, demonstrating a significant improvement over the initial pass@8, which indicates that the model learns new solution strategies at test-time.</p>
<p>INTRODUCTION</p>
<p>We study how large language models (LLMs) can continually improve at reasoning on their target tasks at test-time.Increasing test-time compute, for example, by extended use of context as scratch space, has recently emerged as a key direction for improving LLMs on challenging tasks such as math and coding (Jaech et al., 2024;Guo et al., 2025;Kimi et al., 2025).Test-time scaling has been driven primarily by extensive general-purpose reinforcement learning (RL; Guo et al., 2025), where the LLM learns how to effectively use its context for reasoning.However, since the context of LLMs is bounded and becomes exceedingly expensive to expand, an LLM cannot learn in-context from experience over long timeframes.</p>
<p>One promising technique for overcoming this challenge is test-time training (TTT; Sun et al., 2020;Hardt &amp; Sun, 2024), which continues training the model at test-time after being given a task.</p>
<p>Previous work has studied TTT via supervised fine-tuning on human-created or expert data, either retrieved (Hardt &amp; Sun, 2024;Hübotter et al., 2025) or provided as few-shot examples (Akyürek et al., 2025).Other work has instead focused on TTT in the context of recurrent neural networks (Sun et al., 2025;von Oswald et al., 2025;Zhang et al., 2025b), aiming to replace the costly attention-based context in Transformers (Vaswani et al., 2017) with a fixed-size state (i.e., the model itself), but losing some of the advantages of reasoning over an uncompressed scratchpad.We explore a complementary approach to test-time scaling, where an LLM is continually trained on self-curated training tasks related to its target task, while practicing on each individual training task in-context.This leverages the Transformer's attention as an uncompressed scratchpad for short-term ideation, while meta-learning strategies for leveraging that context across long-term, task-specific experience.</p>
<p>Figure 2: TTC-RL performs targeted practice on similar problems to the target task at test-time.The agent is given a target task (red) and self-curates a curriculum of related tasks (blue).It then explores solution strategies on this curriculum, reinforcing successful approaches (✓).This experience enables the agent to more effectively solve the original, more difficult target task.</p>
<p>We propose a test-time curriculum (TTC) agent that automatically designs its own curriculum of training tasks by selecting the relevant tasks for the job from a large corpus of existing tasks.The agent then attempts tasks in its curriculum, and compresses the gathered experience into its weights via RL.The automatic self-guided curriculum design avoids laborious human curation of datasets, and enables training on purpose-built curricula at test-time.We find that this reinforcement learning on test-time curricula (TTC-RL) leads to remarkably improved reasoning on target tasks.In particular, we find that TTC-RL improves the pass@1 of several strong LLMs across diverse reasoning tasks, covering competition math, coding, and scientific reasoning (cf. Figure 1).We further identify that TTC-RL is complementary to other means of test-time scaling, effectively improving pass@k and maj@k even at large k.Notably, we find that TTC-RL can overcome the limitation of fixed context windows by observing that a non-thinking model (limited to 8k context tokens) with TTC-RL can perform similarly to the same model thinking for 30k tokens in-context.This demonstrates that during TTC-RL, the model continues learning how to think effectively for its target tasks.Our results suggest such targeted RL as a promising new direction for LLM agents that continually improve at test-time through many interactions with an environment.</p>
<p>We summarize our contributions as follows:</p>
<ol>
<li>
<p>We propose a TTC agent for targeted RL ( §3): We propose a test-time curriculum agent which at test-time when given a target task, self-selects related training tasks from a diverse corpus.The agent then learns from its own experience of attempting those tasks via RL.</p>
</li>
<li>
<p>TTC-RL improves reasoning on target tasks ( §4): Across several models and tasks, TTC-RL consistently improves pass@1 substantially faster than general-purpose RL post-training on standard RL datasets, and saturates at a higher accuracy.Next, we identify that TTC-RL substantially raises the performance ceiling of the model (pass@k) and demonstrate that it is complementary to existing approaches to test-time scaling.Finally, we find that TTC-RL yields strongly specialized models that perform remarkably well on their target tasks, even when compared to models that are allowed to think for tens of thousands of tokens in context.3. Measuring latent improvements in reasoning ( §5): The evaluation of RL-trained models faces the challenge of estimating whether improved scores are due to better reasoning or merely learning the expected output format.We introduce a new metric, latent improvement, which computes a lower bound on the improvement in reasoning due to RL training, and find that TTC-RL leads to substantial improvements in "latent" reasoning.</p>
</li>
</ol>
<p>RELATED WORK</p>
<p>Test-time scaling and general-purpose RL training.A common strategy for improving LLM performance in challenging domains is to allocate additional test-time compute, for instance, through majority voting (Snell et al., 2025), search with a reward model (Lightman et al., 2023;Wang et al., 2024a;Setlur et al., 2025a), or by identifying consistent patterns among parallel rollouts (Wang et al., 2023;Huang et al., 2025a).The potential of such methods is often measured by pass@k, which describes the performance ceiling with k generations (Chen et al., 2025b).More recently, scaling test-time compute via in-context "reasoning" (Brown et al., 2020;Wei et al., 2022) has significantly improved performance in domains like math and coding (Jaech et al., 2024).This capability is commonly enabled by large-scale, general-purpose RL training on diverse tasks (Lambert et al., 2024;Ma et al., 2025;Guo et al., 2025;Kimi et al., 2025), during which models learn to reason within their bounded context (Setlur et al., 2025b), which connects to the broad topic of meta-learning (Schmidhuber, 1987;Duan et al., 2017;Finn et al., 2017).This paradigm is related to goal-conditioned RL (Schaul et al., 2015;Andrychowicz et al., 2017) where several works have studied automatic curriculum learning (Warde-Farley et al., 2018;Pitis et al., 2020;Pong et al., 2020), first proposed by Bengio et al. (2009).In contrast to improving general-purpose models, our work employs RL to train specialized reasoners for a particular target task at test-time.</p>
<p>Self-play.A specialized form of curriculum learning has proven highly successful in domains like games through the use of self-play (Schmidhuber, 1991;Silver et al., 2016), where an agent is repeatedly challenged by playing against itself.Seminal works show that this approach can lead to superhuman performance (e.g., Mnih et al., 2015;Silver et al., 2016;2017;Berner et al., 2019).Several recent works aim to generalize this paradigm to LLMs and more general domains such as coding by self-generating a training curriculum (Zhao et al., 2025;Huang et al., 2025b;Chen et al., 2025a;Fang et al., 2025).While recent work has studied test-time curricula as an extension of self-play to goal-conditioned RL settings (Diaz-Bone et al., 2025), its evaluation has focused on simple robotic navigation tasks.We extend this line of work to challenging reasoning tasks by self-curating a training curriculum, enabling LLMs to continually learn from extensive experience on a single task (Silver &amp; Sutton, 2025;Shen et al., 2025).</p>
<p>Test-time training and test-time RL.</p>
<p>Training a model at test-time for a given input has been widely studied as TTT (Sun et al., 2020), using supervised (Hardt &amp; Sun, 2024;Hübotter et al., 2025;Yu et al., 2025a;Bertolissi et al., 2025;Bagatella et al., 2025a) or self-supervised losses (Sun et al., 2025;Dalal et al., 2025).Several methods perform TTT in a purely unsupervised manner, i.e., without "real-world" data or feedback (Wang et al., 2021;Zhang et al., 2022).Most relevant to our work, Zuo et al. (2025) recently extended unsupervised TTT to perform RL on the test set, leveraging the model's majority votes as pseudo-labels.This connects to a broader theme of unsupervised RL (Zhang et al., 2025a;Shao et al., 2025;Zhou et al., 2025;Prabhudesai et al., 2025) and self-improvement in LLMs (Zelikman et al., 2022;Gulcehre et al., 2023;Lee et al., 2025).</p>
<p>TEST-TIME CURRICULA</p>
<p>We consider the set of target tasks D ⋆ = {x ⋆ 1 , . . ., x ⋆ M } given at test-time, and our goal is to specialize an existing model through further training to those tasks.For training, as in general-purpose RL, we rely on an existing large corpus of training tasks D = {(x i , v i )} N i=1 , for each of which v i (•) ∈ {0, 1} verifies whether an attempt was correct.To specialize, it is common practice to construct a particular subset D ⋆ from D, and we call such a targeted subset a test-time curriculum for D ⋆ .We seek to make test-time training on such a curriculum scalable.To this end, we propose to go beyond human-curated test-time curricula and let the initial model craft its own test-time curriculum.</p>
<p>The previous works of Hardt &amp; Sun (2024) and Hübotter et al. (2025) have studied self-curated testtime curricula with supervised fine-tuning (SFT), and have shown that this can improve language modeling, i.e., lead to lower perplexity.However, this approach is limited since it requires the corpus to specify how training tasks are to be solved-not only to verify whether a solution is correct.Moreover, mirroring recent observations on the robustness of on-policy RL (Shenfeld et al., 2025), we observe that SFT on expert traces often leads to an initial drop in performance on downstream tasks, suggesting that SFT is ill-suited for TTT with LLMs.We provide further details in Appendix A.</p>
<p>AUTOMATIC TTCS FOR TARGETED RL</p>
<p>We therefore focus on on-policy RL and extend the previous work on automatic data selection for TTC-SFT (Hardt &amp; Sun, 2024;Hübotter et al., 2024;2025) to automatic task selection in TTC-RL.We adopt SIFT (Hübotter et al., 2025), which selects those examples from the corpus that the model deems most informative for the target tasks.SIFT has a hyperparameter λ, which reflects the models' ability to learn from the seen examples, and which explicitly trades between diversity of the selected examples and their relevance to the target tasks.We find that our results are robust to the choice of λ and generally set λ = 0.1 in our experiments.To determine which examples are most informative, SIFT leverages a latent representation space ϕ of token sequences for which we use the normalized last-token last-layer embeddings of the initial model.Appendix F gives examples for such self-curated test-time curricula.</p>
<p>Algorithm 1 Test-Time Curriculum for Targeted RL
Require: Test tasks D ⋆ 1: {(x t , v t )} ← SIFT λ,ϕ,T,D (D ⋆ ) ▷ select curriculum 2: for t = 0, 1, . . . , T − 1 do 3: {ŷ t+1,i } ∼ π t (• | x t+1 ) ▷ attempt 4: {r t+1,i } ← v t+1 ({ŷ t+1,i }) ▷ verify 5: θ t+1 ← GRPO(θ t , {ŷ t+1,i }, {r t+1,i }) ▷ RL step 6: end for
This pipeline leverages the semantic understanding of the initial model to self-curate a test-time curriculum for the target tasks.We then train on this test-time curriculum via GRPO (Shao et al., 2024), as shown in Algorithm 1.1 Note that test-time training does not necessitate the model to stay close to its initialization since it needs to generalize only to its target tasks, and hence, we omit the KL penalty of GRPO.We include background on SIFT and GRPO in Appendix B. In an extension, we evaluate a test-time curriculum that automatically selects tasks of the right difficulty, which we show to further accelerate learning on weaker models (cf.Appendix C).</p>
<p>A DIVERSE CORPUS FOR GENERAL-PURPOSE RL POST-TRAINING</p>
<p>To study the effectiveness of our proposed adaptive test-time curriculum, we leverage a large corpus of high-quality verifiable training data, suitable for post-training a model across diverse domains.We assemble a new meta-dataset, which we call the verifiable-corpus and which combines approximately 265k diverse training tasks, spanning three environments:</p>
<p>• Exact answer match / Math: For math problems with a numerical answer, we determine answer equivalence using math-verify.Our corpus contains the training splits of GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b), and the DAPO math dataset (Yu et al., 2025b), covering numerically verifiable math problems for a wide range of difficulties.• Judged answer match / General reasoning: Measuring the validity of complex reasoning requires more robust verification than symbolic equivalence checks.Given a (potentially long) golden answer, we use a 1.5B-parameter verifier model trained by Ma et al. (2025) to determine whether attempted and golden answers are semantically equivalent.Our corpus contains the Webinstruct-verified dataset (Ma et al., 2025), which covers a wide variety of subjects ranging from natural sciences to history.• Unit tests / Code: Finally, we combine several sources of coding tasks.Each coding task is verified by a set of unit tests.Our corpus combines tasks from APPS (Hendrycks et al., 2021a), code contests (Li et al., 2022), TACO (Li et al., 2023), PrimeIntellect (Mattern et al., 2025), Leetcode (Xia et al., 2025), the Codeforces training split (Penedo et al., 2025) and all LiveCodeBench tasks (Jain et al., 2024) prior to February 1, 2025.We perform a filtering step where we remove training tasks with empty answers or less than 5 unit tests, to ensure a reliable training signal.Finally, we deduplicate and decontaminate the corpus, as detailed in Appendix E.1.We openly share the corpus and our environment implementations to support future research.To our knowledge, the verifiable-corpus is one of the first public corpora of high-quality verifiable tasks, spanning several domains and environments.We envision that, building on this work, future efforts will ultimately enable TTC agents to utilize any relevant training tasks they find on the web (similarly to retrieval-augmented generation; Lewis et al., 2019), or to self-generate their own training tasks (see, e.g., Zhao et al., 2025).</p>
<p>RESULTS</p>
<p>We focus our evaluation on a diverse set of target tasks in math, coding, and scientific reasoning.Specifically, we evaluate test-time curricula for high-school-level competition math questions in AIME 24 &amp; 25 and MATH500 (Hendrycks et al., 2021b).We evaluate coding ability on Codeforces (Penedo et al., 2025), CodeElo (Quan et al., 2025), and on LiveCodeBench v6 (Jain et al., 2024), i.e., tasks released after February 1, 2025.Finally, we evaluate scientific reasoning with GPQA-Diamond (Rein et al., 2024) which covers questions in biology, physics, and chemistry.</p>
<p>TTC-RL can be applied to each task within a benchmark individually or to the entire benchmark on aggregate, treating it as a set of target tasks.We primarily evaluate TTC-RL per-benchmark as this yields greater statistical significance under a limited compute budget.We then perform an ablation, indicating that per-task TTCs performs at least on-par with per-benchmark TTCs (cf.Section 4.2).</p>
<p>To ensure that our evaluation is accurate, we adopt evalchemy (Raoof et al., 2025) and synthesize system prompts to be consistent across benchmarks (cf.Appendix E.2).We generally train for two episodes with batch size 8 and 16 rollouts per train task,2 and measure avg@4 on the set of test tasks once every ten steps.To further reduce noise, we compute a moving average across three validation steps.Finally, in our summarized numeric results, we report the highest averaged avg@4, and include detailed plots of avg@4 per step in Appendix D.2.</p>
<p>We perform our main evaluation on the non-thinking models Qwen3-8B (Yang et al., 2025) and the more recent Qwen3-4B-Instruct-2507, whose responses we limit to 8192 tokens.We additionally evaluate on the Qwen3-8B base model.We opt for non-thinking models due to the high computational cost of running thinking models over long contexts, typically of up to 32k tokens.The goal of our TTC framework is to show that models can improve at test-time, even without further expanding their context.We hypothesize that our results extend to thinking models, which simply have a larger maximum response length.Middle: TTC-RL also improves the performance of majority voting (across math and GPQA-D), with the initial pass@1 significantly outperforming maj@64 on the initial model.Right: We evaluate Qwen3-8B in non-thinking and thinking mode, as well as the non-thinking model + TTC-RL.The color indicates the relative accuracy per column.We find that TTC-RL significantly improves the non-thinking model, allowing it to perform close to the thinking variant in several domains, despite reasoning over 8k rather than 30k context tokens.Main results.We summarize our main results in Fig- ure 1 and Table 1.We find that TTC-RL significantly improves accuracy across a range of models and all benchmarks.Notably, it also leads to significant performance gains on top of Qwen3-8B-Base within only relatively few RL steps, indicating that TTCs lead to sample-efficient training.Our main baseline is a model that is trained on 1k uniformly chosen training tasks from the corpus, to which we refer to as standard "RL post-training", since this method yields a general-purpose model.We compare this to TTC-RL with a curriculum of size 1k and find that training on a test-time curriculum accelerates learning significantly and leads to saturation at substantially higher performance. 3Notably, Qwen3-8B with TTC-RL performs on-par with strong closed-source non-thinking models; for example, it approximately matches GPT-4o-2024-08-06 on LCB v6 .In Figure 3, we further ablate the size of the curriculum and find that TTC-RL consistently outperforms general-purpose RL post-training across a wide range of curriculum sizes.Interestingly, at dataset size 1-though performing poorly-the general-purpose RL post-training outperforms TTC-RL.We suspect that this may result from TTC-RL picking a practice task that is very similar to the test tasks, in which case overfitting may harm more than when overfitting to a less related task.</p>
<p>Takeaway 1 TTC-RL substantially improves accuracy on a wide variety of models and benchmarks, compared to a model's initial performance and after (continued) RL post-training on our corpus.</p>
<p>TTCS ARE COMPLEMENTARY TO EXISTING APPROACHES TO TEST-TIME SCALING</p>
<p>Next, we demonstrate that TTC-RL improves the LLM's ability for test-time scaling.</p>
<p>TTCs raise the model's performance ceiling.While the improvement in accuracy demonstrates that during TTC-RL, the model learns to better reason within context, we ask whether the model improves more broadly.A common metric to understand a model's "performance ceiling" for testtime scaling is the pass@k metric, which measures whether any one of k attempts is correct (Chen et al., 2025b).Recent work has repeatedly shown that RL-training tends not to improve pass@k at large k (Yue et al., 2025), leading to the concern that RL-training is simply "distilling" pass@k into pass@1.In Figure 4 (left), we instead observe that TTC-RL significantly improves pass@k across a wide range of k.Similarly, TTC-RL also improves the realized performance gains of majority voting, as can be seen in Figure 4 (middle), and notably increases the pass@1 well beyond the maj@64 after continued RL post-training.Our results indicate that two key factors lead to the performance of TTC-RL: Improvements to the RL training algorithm that also apply to our general-purpose RL-training baseline, as well as the specific data selected by the TTC agent, as indicated by the strong improvement in majority voting.We provide a more detailed discussion in Appendix D.1.Developing a better understanding of the circumstances under which RL-training can "discover new behavior", leading to improved pass@k, is an exciting direction for future research.</p>
<p>TTC-RL with a short-context LLM can perform close to a long-context LLM.We also seek to better understand how TTC-RL relates to reasoning over long contexts.To this end, we evaluate the non-thinking and thinking variants of Qwen3-8B, limited to 8k and 30k tokens per response, respectively.In Figure 4 (right), we find that TTC-RL on the non-thinking model performs close to the thinking model in several domains, particularly in coding and GPQA. 4 Further, note that the asymptotic cost of growing context in a Transformer is quadratic (Vaswani et al., 2017), whereas the asymptotic cost of TTC-RL is linear (since experience is compressed into the model's weights).This suggests that there is a regime in which, given a fixed compute budget, TTC-RL outperforms further scaling of context size.We believe that studying this compute-optimal Pareto frontier is an exciting topic for future research.Our results indicate that to further improve the performance of LLMs, test-time curricula may eventually be advantageous over continued scaling of context size.</p>
<p>Takeaway 2</p>
<p>Test-time curricula substantially increase the pass@k performance ceiling of a model and can perform similarly to models which are reasoning over a much larger context.This indicates the potential of TTCs to complement existing approaches to test-time scaling.</p>
<p>TTCS EFFECTIVELY SPECIALIZE MODELS</p>
<p>To determine whether the test-time curriculum specializes the model to its target tasks, we conduct a straightforward experiment: We evaluate each final checkpoint of TTC-RL on all benchmarks, including those that were not part of the set of target tasks.We summarize the results in Figure 5 (right), with columns corresponding to evaluation and rows corresponding to training.We find that after TTC-RL, models perform best on their target tasks, while severely underperforming on tasks that are unrelated to the target tasks.Moreover, we identify a block-diagonal structure, where models generalize better across mutually related groups of tasks, particularly among similar math benchmarks.We also find that models appear to generalize better from coding to math than vice versa, and models generalize better from code and math to GPQA than vice versa.</p>
<p>TTCs for individual tasks.Aspirationally, we anticipate test-time curricula to enable continual learning for a single test task over a long timeframe.While we focus our main evaluation on the setting where test-time curricula are applied per benchmark, we run an ablation with 30 separate TTCs-one per AIME 25 question.The results in Figure 5 (left) demonstrate that specializing to an individual test task can outperform a broader specialization to a group of test tasks.This shows that TTC-RL does not depend on a larger set of test tasks to implicitly lead to diverse data and robust training, and instead seamlessly extends to a fully test-time setting with only a single task given.We find, however, that more fine-grained specialization does not always lead to further performance gains.We evaluate training separate TTCs for each of biology, physics, and chemistry in GPQA, leading to approximately the same performance as a joint TTC.In our view, gaining a better understanding for "how much" specialization is helpful is an exciting direction for further research.</p>
<p>Takeaway 3</p>
<p>Test-time curricula effectively specialize the model to their target tasks.When applied to an individual target task, TTC-RL can be seen directly as a method for test-time scaling.We perform TTC-RL and maj-TTRL (cf.Section 5.2) on Qwen3-8B, and find that per-task TTC-RL even outperforms the benchmark-level TTC.Middle: TTC-RL improves "correctness" of reasoning, not only learning the answer format.We evaluate the difference in accuracy between TTC-RL and the initial Qwen3-8B, averaged over benchmarks.The latent improvement is a lower bound on the accuracy gain that is not due to merely learning the format (cf.Section 5.1).Right: TTC-RL yields models that are specialized to their target tasks.We plot the accuracy of Qwen3-8B trained for given target tasks (rows) when evaluated on other benchmarks (columns).We normalize accuracies across all evaluations of a particular benchmark.Notably, the model trained via TTC-RL for the "right" target tasks (i.e., the diagonal) always performs best.</p>
<p>FURTHER ANALYSIS</p>
<p>ESTIMATING "REAL" IMPROVEMENT</p>
<p>When evaluating RL-trained models on verifiable tasks, a reasonable concern is whether the model simply learns to adhere to the expected output format.Indeed, we find that if the initial model is not able to consistently produce well-formed responses, RL-training tends to quickly teach the model the expected output format.Therefore, disentangling shallow learning of format from improvements in a model's "latent" reasoning is critical for accurate evaluation.Ideally, we would like to measure whether the model's reasoning improves throughout training-regardless of whether we can automatically parse and evaluate responses.</p>
<p>We propose to measure a model's latent improvement (LI) during RL training as follows.Consider the event of an answer being marked as "accurate" by the verifier, which occurs if it is "well-formed" (i.e., it can be extracted and interpreted) and if the model's latent reasoning is "correct".Based on this, a straightforward lower bound on correctness is simply P(correct) ≥ P(accurate).To measure the improvement in correctness throughout RL training, we make the following intuitive assumption:</p>
<p>Assumption 1.We assume that being well-formed does not reduce the chance of being correct.Formally, we assume P(correct | well-formed) ≥ P(correct), i.e., a non-negative association of formedness and correctness.</p>
<p>Intuitively, this assumption states that an ill-formed response does not increase the likelihood of correct latent reasoning.This yields a straightforward upper bound on the probability of correct latent reasoning: P(correct) ≤ P(accurate)/P(well-formed) if P(well-formed) &gt; 0. Thus, the improvement in correctness after T RL steps is lower bounded as Latent Improvement := P(correct T ) − P(correct 0 ) ≥ P(accurate T ) − P(accurate 0 ) P(well-formed 0 )</p>
<p>.</p>
<p>Measuring latent improvement.We consider a response as ill-formed if we cannot extract an answer, e.g., because the response was truncated at the max-token limit or because the completed response did not contain an extractable answer.We note that to reliably measure LI, it is essential</p>
<p>to ensure that answer extraction is strict. 5In Figure 5 (middle), we measure the latent improvement of Qwen3-8B, and find that under Assumption 1, TTC-RL leads to a substantial latent improvement.We include our complete results in terms of LI in Table 7 of Appendix D.</p>
<p>TOWARDS CONTINUAL SELF-IMPROVEMENT AT TEST-TIME</p>
<p>We consider this work as a first step towards agents that continue learning at test-time and specialize without requiring human supervision.The recent work of Zuo et al. (2025) can also be seen as a step in this direction by proposing to train on the test set directly, using majority votes as surrogate rewards ("maj-TTRL").Since Maj-TTRL relies on majority votes as its training signal, it can be applied only to environments with structured outputs such as our math environment with numerical answers or the multiple choice GPQA.In contrast, our proposed TTCs can be applied in any environment where a reward signal can be defined.We perform a comparison to Zuo et al. (2025) in Table 2 and find that Maj-TTRL leads to significant gains in accuracy across math benchmarks, but helping less in GPQA.We emphasize that Maj-TTRL and test-time curricula are complementary approaches, e.g., one can perform Maj-TTRL directly after TTC-RL, which we find to outperform Maj-TTRL alone (cf. Figure 12 in Appendix D.4).Zhao et al., 2025;Zweiger et al., 2025), which may be combined with or extend TTCs.</p>
<p>ON CONTAMINATION AND REWARD HACKING</p>
<p>The performance gains from TTC-RL are remarkable: for example, in AIME24 and CodeElo, the pass@1 of the strong Qwen3-8B more than doubles within only a few hundred training steps.This naturally raises the question of potential confounding factors.To mitigate this risk, we took several steps: we extensively decontaminated our corpus by removing tasks that overlap with the test sets, implemented safeguards against reward hacking within our code environment, and manually reviewed several model responses.While we base our evaluation on the widely used evalchemy package (Raoof et al., 2025), we found a significant flaw in the evaluation of Codeforces and CodeElo, where some (and frequently all) private test cases were leaked into the prompt as "examples".This enables a strong model to "solve" a task simply by handling each test case individually.To mitigate this, we removed all input/output examples from the prompts of Codeforces and CodeElo, and also ensured that private test cases are not leaked in tasks from our training corpus.</p>
<p>A remaining limitation is that we cannot guarantee the cleanliness of the model's original pretraining data.To account for this possibility, we evaluate on LCB v6 , which consists of coding tasks that were released since February 2025.Hence, TTC-RLs performance gains on LCB makes pre-existing contamination a less likely explanation for our results.Furthermore, we compare TTC-RL to an oracle that trains directly on the test tasks, finding that our method learns slightly more slowly and levels off at a lower accuracy (cf. Figure 14 in Appendix D).We believe our findings on the importance of data selection (cf. Figure 1) and improvements to the RL training algorithm to facilitate exploration (cf.Appendix D.1) offer plausible explanations for these results.We further include qualitative examples demonstrating the improvements in reasoning in Appendix F.</p>
<p>DISCUSSION</p>
<p>We propose a test-time curriculum agent that self-curates a sequence of training tasks to specialize towards a specific target task via reinforcement learning.We demonstrate that TTCs achieve remarkable performance gains across multiple models and diverse reasoning benchmarks, significantly raising the performance ceiling of strong initial models through specialization to their target task.To better evaluate these gains, we introduce the "latent improvement" metric, which measures genuine improvements in reasoning correctness.Our experiments confirm that TTCs yield substantial gains in latent improvement.</p>
<p>This highlights the potential of a currently underutilized compute regime: targeted test-time training, which sits between large-scale general-purpose training and frozen test-time scaling.While standard next-token prediction relies on a model's intuition and reasoning allows it to leverage context for deliberation, our proposed test-time curriculum enables the model to meta-learn how to reason for a particular target task at test-time.Similarly, when humans begin a new job, they often train for weeks or months before being able to solve all required tasks.During this time, they collect experience on dozens of tasks that are similar, becoming more efficient at solving their jobs' target tasks.</p>
<p>In demonstrating the potential of such targeted test-time training, our work opens up several exciting research directions.A natural direction is to move beyond the bottleneck of a fixed task corpus through self-generated TTCs, which may still use human-created tasks as inspiration.Further avenues include improving the sample-and step-efficiency of TTC-RL through advancing methods for RL training.This also raises questions about scaling laws for this new regime: for instance, at what context length does it become more advantageous to scale TTC-RL rather than increasing the context window?Looking beyond single-task specialization, TTCs might be extended to dynamic settings where an agent must adapt to an evolving set of target tasks.Finally, TTC-RL could be used to unconfound benchmark evaluations by providing a standardized method for specializing all models to a test task (Dominguez-Olmedo et al., 2025), enabling a fairer comparison of their core capabilities.While we focus on RL-training with a test-time curriculum, the prior works of Hardt &amp; Sun (2024) and Hübotter et al. (2025) have proposed to instead perform supervised fine-tuning on human-produced data (TTC-SFT), retrieved from a large corpus.Next to being impractical since requiring reasoning traces for training tasks, we make the observation that the distribution-shift of off-policy SFT appears to make it fundamentally ill-suited for test-time training of LLMs.To test this, we train a Qwen2.5-7B-Instructmodel (Qwen et al., 2024) on the test sets of the AMC23 and AIME25 math competitions, using expert traces generated by QwQ-32B (Qwen, 2025) using the SFT pipeline from OpenThinker3 (Guha et al., 2025).OpenThinker3-7B is simply the fine-tuned Qwen2.5-7B-Instruct when trained to convergence on a curated training set of QwQ-32B (Yang et al., 2025) traces (Guha et al., 2025).Although OpenThinker3 demonstrates that at convergence, an SFT-trained Qwen2.5-7B-Instruct can achieve strong performance, Figure 6 shows that even when training directly on the test set, it takes hundreds of gradient steps before the accuracy starts to increase, while initially dropping to close to 0%.Intuitively, even though perplexity decreases smoothly throughout training, the model's behavior undergoes phase transitions, and begins by only reproducing superficial reasoning patterns such as repeatedly generating "Wait, ...":</p>
<p>CONTRIBUTIONS</p>
<p>Excerpts from reasoning traces for AIME 25 after 200 SFT steps . . .be 2025.Wait, actually, actually, actually, actually, actually, actually, actually, actually, actually, actually, . . . . . .numerator.\n\nWait,numerator numerator is numerator denominator * denominator numerator.\n\nWait,numerator numerator . . .This phenomenon is closely related to recent observations that off-policy SFT appears to induce a greater distribution shift of the policy than on-policy RL (Shenfeld et al., 2025), indicating that TTC-SFT is less robust and may even be less efficient than TTC-RL.</p>
<p>Validating this phenomenon with expert solutions in GSM8K.To validate that this behavior is because of the behavior SFT rather than the specifics of QwQ-32Bgenerated reasoning traces, we perform a similar experiment on GSM8K (Cobbe et al., 2021).GSM8K's test set has not only verified numerical answers, but also humangenerated expert reasoning traces.In Figure 7, we train Llama-3.2-1B-Instruct on these expert solutions with SFT and observe that after the first full epoch, performance on the GSM8K test set has dropped.At first this may seem puzzling, but looking at a qualitative example suggests that the model first learns superficial cues (the human-generated GSM8K solutions are very succinct) before in this case overfitting to the SFT data:</p>
<p>Attempt of a GSM8K question after 1 epoch of SFT on the test set</p>
<p>Problem</p>
<p>There are three trees in Eddy's backyard.The shortest tree has a height of 6 feet, and the second tree has a height of 5 feet more than the shortest tree.The height of the tallest tree is twice the height of the two trees combined.How tall is the tallest tree?</p>
<p>Ground Truth</p>
<p>Tallest Tree: 108 feet Middle sized tree:(108/2)-6=48 feet Smallest tree:48/4=12 feet</p>
<p>Initial Answer</p>
<p>The tallest tree is 108 feet.The middle-sized tree is 6 feet less than half the height of the tallest tree.Half of 108 is 54.So the middle-sized tree is 54 -6 = 48 feet tall.The smallest tree is 1/4 the height of the middle tree, which is 1/4 x 48 = 12 feet tall.The final answer is 12</p>
<p>Final Answer</p>
<p>The middle tree is 108/2 -6 = 60 feet tall.The smallest tree is 60/4 = 15 feet tall.The final answer is 15</p>
<p>Hyperparameter Value</p>
<p>Learning rate 1e-5 Batch size 32 Max.sequence length in tokens 16384 Packing No Adam's β-values (0.9, 0.999) Table 3: Hyperparameters for SFT training on the test sets of AMC23 and AIME25.This corresponds to the "micro" configuration of OpenThinker (Guha et al., 2025).</p>
<p>B BACKGROUND B.1 SIFT</p>
<p>Several works studied how to optimally select data for imitation learning, e.g., the early seminal work of MacKay (1992) and recent extensions (Hübotter et al., 2024;2025;Bagatella et al., 2025b).</p>
<p>SIFT is an active learning selection method that accounts for information duplication and optimizes overall information gain to produce diverse and informative examples (Hübotter et al., 2025).</p>
<p>Given a feature map ϕ, we define the inner-product kernel k(x, x ′ ) := ϕ(x) ⊤ ϕ(x ′ ).SIFT greedily selects data from a corpus D to minimize a measure of uncertainty about how to respond to a specific prompt x ⋆ .This uncertainty (posterior variance) given a selected set X is quantified as:
σ 2 X (x ⋆ ) := k(x ⋆ , x ⋆ ) − k ⊤ X (x ⋆ )(K X + λI) −1 k X (x ⋆ ),(2)
where K X is the kernel matrix of X, k X (x ⋆ ) is the vector of kernel evaluations between the inputs in X and x ⋆ , and λ &gt; 0 is a regularization coefficient.</p>
<p>SIFT iteratively selects the next point x n+1 by greedily minimizing this posterior uncertainty:
x n+1 := arg min x∈D σ 2 Xn∪{x} (x * ).(3)
The regularization coefficient λ modulates the trade-off between relevance (favored by large λ) and diversity (favored by small λ).Full details, including theoretical guarantees and empirical results, are presented in the SIFT paper (Hübotter et al., 2025).</p>
<p>B.2 GRPO</p>
<p>For RL-training, we adopt GRPO (Shao et al., 2024) without a KL penalty.For a specific training task x, the behavior policy π θold samples a group of G individual responses {o i } G i=1 .Then, we calculate the advantage of the i-th response by normalizing the group-level rewards {r i } G i=1 :
Âi,t = r i − mean({R i } G i=1 ) std({R i } G i=1 ) .(4)
GRPO then maximizes a clipped objective:
J GRPO (θ) = E x∼ D ⋆ ,{oi} G i=1 ∼π θ old (•|x) 1 G G i=1 1 |o i | |oi| t=1 min w i,t (θ) Âi,t , clip(w i,t (θ), 1 − ϵ low , 1 + ϵ high ) Âi,t ,(5)
with importance weights
w i,t (θ) = π θ (o i,t | x, o i,&lt;t ) π θold (o i,t | x, o i,&lt;t ) . (6)
Maximizing the learning signal in GRPO.When training on a selected dataset we aim to provide maximal learning signal to the model.One simple way to determine whether a provided data sample provides useful information is via the norm of GRPOs gradient.The gradient of the GRPO objective, in the on-policy setting (π θ = π θold ) is given by:
∇ θ J GRPO (θ) = 1 G G i=1 1 |o i | |oi| t=1 Âi,t ∇ θ log π θ (o i,t | x, o i,&lt;t )(7)
This formulation reveals that the advantages Âi,t are closely tied to the gradient norm of GRPO, ∥∇ θ J GRPO (θ)∥.Intuitively, by selecting data with high absolute advantage we maximize the gradient norm and provide a strong learning signal to the model.</p>
<p>In the sparse-reward setting for a fixed question x, the reward is distributed according to a Bernoulli distribution R ∼ Ber(p x ).The expected absolute advantage for this question can be derived as follows, where we assume G → ∞ for simplicity:
E [|A|] = E |R − E[R]| σ(R) = p x 1 − p x σ(R) + (1 − p x ) p x σ(R) = 2 p x (1 − p x )(8)
Therefore, the absolute advantage is maximized for p x = 1 2 .This simple argument suggests that, in order to maximize the learning signal, we should choose questions on which the current model has success rate 50%.</p>
<p>C AUTOBALANCING ACHIEVABILITY WITH TTC'S</p>
<p>The goal of a targeted test-time curriculum is to teach the LLM skills that are directly useful for solving the target tasks.Naively selecting the test-time curriculum, however, may result in training tasks that are either too easy or too hard for the current model.Prior work on curricula for sparsereward reinforcement learning (e.g., Pitis et al., 2020;Zhao et al., 2025;Huang et al., 2025b;Diaz-Bone et al., 2025) has shown that selecting tasks at an appropriate level of difficulty can dramatically accelerate learning.In line with these findings, we demonstrate that balancing task relevance with task difficulty can lead to a better-performing TTC if the model is initially significantly weaker than required to solve most target tasks.Intuitively, a success rate of 50% provides the most detailed differentiation as to which approaches work.Indeed, in expectation, a success rate of 50% leads to the largest possible absolute advantage in GRPO (cf.Appendix B.2), which implies a large gradient norm and a strong and informative learning signal for the model.</p>
<p>Estimating the success rate online.This raises the question of how to estimate the difficulty α x t of a given training task x from the corpus at time t.We assume access to an initial estimate of difficulty α x 0 ∈ (0, 1).We then update α x t recursively to "track" the approximate success rate of the model for each question:
α x t+|B| := r x t+|B| if
x was within the last batch
σ(σ −1 (α x t ) + σ −1 (∆ t+|B| )) otherwise, (9)
where ∆ t+|B| is the mean reward across the batch and σ(z) = 1/(1 + e −z ) the sigmoid function.</p>
<p>Intuitively, if ∆ &gt; 0.5, the achievability estimate of all unseen questions is increased, indicating that tasks are becoming easier for the agent.Conversely, if ∆ &lt; 0.5, the achievability estimates are decreased, reflecting that training tasks are currently too difficult.Trading off achievability &amp; relevance to the test task.</p>
<p>We can now leverage the achievability estimates to ensure that the selected tasks are of an appropriate difficulty.</p>
<p>To this end, we propose Achievable Test-Time Curricula (A-TTCs), which balance relevance to the target tasks, as identified by SIFT, with achievability:
A |B|t ← {(x, v) | α x |B|t ∈ [a min , a max ]} {(x |B|t , v |B|(t+1)−1 )} ← arg min SIFT λ,ϕ,B,A |B|t (D ⋆ )
where [a min , a max ] determines the interval of task difficulty we consider for the task selection with SIFT.This selection strategy offers a simple way to select batches of problems online, which are of the right difficulty while remaining relevant to the target tasks.In practice, we choose [a min , a max ] = [0.2,0.6], with the goal of achieving approximately 50% of tasks over the batch, obtain prior difficulty estimates by computing the success rates of the Qwen3-8B model on all questions and enforce a minimum subset size of 1000 to select from.</p>
<p>The results in Figure 8 show that on the weaker Qwen3-0.6Bmodel trading-off achievability with relevance yields a higher training reward and furthermore improves test score across the three math benchmarks, AIME 24 &amp; 25 and MATH500.We note that this procedure appears useful primarily if the difficulty level in the dataset is wrongly calibrated with respect to the model's capabilities.</p>
<p>Modeling assumptions.To motivate our online achievability estimation, we consider the logits ϕ x t = σ −1 (α x t ) ∈ R of the achievability values and make the assumption that at each time step the change in the logits d t is jointly gaussian across all tasks:
d x t = ϕ x t+1 − ϕ x t (10)d t ∼ N (0, Σ) with Σ = (v − c)I n + c11 ⊤(
11) That is, we consider a fixed variance v for all tasks and assume that the update has constant correlation c among all tasks.After observing the achievabilities for a batch of problems at time t, we can compute the update in the logits for the observed tasks and are able to estimate the update for the unobserved problems.</p>
<p>Consider a batch of problems B = {y 1 , . . ., y m } and an unobserved problem x / ∈ B, then:
E[d x t | d y t , y ∈ B] = c1 ⊤ ((v − c)I |B| + c11 ⊤ ) −1 d B t (12) = c v − c − |B|c 2 (v − c)(v + (|B| − 1)c) y∈B d y t (13) = c v + (|B| − 1)c ψ y∈B d y t (14) ϕ x t+|B| = ϕ x t + ψ y∈B d y t (15)
Under the assumed covariance structure and letting ∆ t+|B| = σ(ψ y∈B d y t ), our update becomes:
α x t+|B| := r x t+|B| if
x was within the last batch
σ(σ −1 (α x t ) + σ −1 (∆ t+|B| )) otherwise. (16)</p>
<p>D EXTENDED RESULTS</p>
<p>In this section, we present additional experiments and ablations.</p>
<p>D.1 INCREASING CLIP-HIGH IN GRPO IS ESSENTIAL FOR LEARNING</p>
<p>Maintaining a sufficient level of entropy in the policy is key for any on-policy exploration method.</p>
<p>When training with GRPO with symmetrical clipping on verifiable rewards it has been observed (Yu et al., 2025b;Luo et al., 2025), that the policy's entropy quickly goes to 0, preventing effective exploration.It has been found that an increase of the clip-high (ϵ high ) parameter in GRPO can lead to a stabilization of the entropy and improved performance during training (Luo et al., 2025).Intuitively, if correct answers are rewarded more strongly than incorrect answers are penalized, the agent is incentivized to maintain higher entropy in its action distribution, promoting exploration.In Figure 9 we evaluate the effect of the clip-high parameter on the policy entropy and test accuracy during training.We find that a symmetric clipping (ϵ high = 0.2) leads to constant decrease in policy entropy and poor performance on the test tasks.When increasing the clip-high parameter, the policy entropy starts increasing, and the test accuracy is dramatically improved.In our preliminary experiments on Codeforces, ϵ high = 0.32 improved significantly over ϵ high = 0.28, which was suggested in Yu et al. (2025b) and used in our other experiments.In Figure 10, we provide further detail on the performance of all models across the main benchmarks.The plots reveal substantial variation in test accuracy development in response to training with the same TTC, indicating that models have varying initial capabilities and potential of training via RL.This is the case, as each model has been subject to different post-training techniques and therefore responds differently to the RL training on the TTC.To address these differences, we propose an algorithm in Appendix C, which aims to calibrate the difficulty of the curriculum to the capabilities of the model.</p>
<p>D.3 "RL POST-TRAINING" BASELINE RESTRICTED TO THE TEST ENVIRONMENT</p>
<p>A simple heuristic to improve a model's domain-specific capabilities is to restrict training to tasks from the target domain.This can be seen as a primitive version of a TTC that conditions on the environment type but ignores instance-level task characteristics.Accordingly, we include a baseline that samples a random subset of the training set-analogous to RL post-training-but restricted to the target domain.Figure 11 demonstrates that filtering the training questions to the code domain is insufficient to achieve comparable performance to TTC-RL on Codeforces and CodeElo.(2025), provides an alternative way to train the model at test time using majority labels as rewards on the target tasks.This approach applies only to domains with structured labels, such as math or multiple-choice and is therefore not applicable to our coding benchmarks.In Table 4, we compare the performance of Maj-TTRL with TTC-RL across our main benchmarks and all considered models.TTC-RL outperforms Maj-TTRL on most benchmarks for Qwen3-8B and Qwen3-4B-Instruct-2507.The only model, where Maj-TTRL achieves higher performance than TTC-RL is the Qwen3-4B-Instruct-2507 model, which is the strongest among all considered models.This reveals the dataset as the main bottleneck for improving performance and suggests to move beyond the bottleneck of a fixed task corpus through self-generated TTCs.</p>
<p>Combining Maj-TTRL with TTC-RL As already highlighted, Maj-TTRL and TTC-RL are two complementary approaches with different strengths.Intuitively, TTC-RL aims to learns from the most relevant tasks in the given corpus to improve on the target tasks, while Maj-TTRL is able to improve the performance on the target tasks directly by continuously aiming to match the majority prediction of the model.Beyond comparing them in isolation, Figure 12 shows that initializing Maj-TTRL from the final TTC-RL checkpoint and training on the target benchmark yields the strongest results on all math benchmarks.Figure 12: Combining TTC-RL and Maj-TTRL combines the strengths of both methods and yields the strongest results on all math benchmarks.We show the results on the Qwen3-8B for math.</p>
<p>D.5 ADDITIONAL BENCHMARKS</p>
<p>While our main evaluation focuses on the most challenging benchmarks in math, code and general reasoning, aiming to push the capabilities of frontier models, we additionally provide implementation and results for a set of simpler benchmarks.These include in the math domain, GMS8K (Cobbe et al., 2021) and AMC23.For coding we add the HumanEval+ (Chen et al., 2021) and MBPP+ (Chen et al., 2021).Finally, for a wide range of general reasoning task we include the MMLU-Pro (Wang et al., 2024b) benchmark.The results in Table 5 show that TTC-RL yields substantial gains on math and coding, especially for the weaker Qwen3-8B-Base model.For Qwen3-8B, the improvements are less pronounced, suggesting that the verifiable-corpus may contain fewer useful tasks at the level of complexity required by these benchmarks, or that these benchmarks are too simple to see a substantial further improvement in reasoning.• In Figure 13, we show the marginal improvement in percentage points throughout training when using TTC-RL over general-purpose RL post-training, and find that this difference remains large throughout training for all models.• In Figure 14, we perform an ablation, comparing to oracle training on the test set.</p>
<p>Model</p>
<p>• In Table 6, we provide a detailed breakdown of values for pass@k.</p>
<p>• In Table 7, we report additional results on latent improvement.</p>
<p>D.7 UNSUCCESSFUL ATTEMPTS</p>
<p>The strong improvements observed when increasing the clip-high parameter ϵ high suggest that the exploration phase requires stabilization of the policy entropy.We evaluated a "cooldown" of entropy via continued training with ϵ high = 0.2.However, in Figure 15, we find that the cooldown appears to slightly improve performance in math, but not generally.For training tasks from Webinstruct-verified, we additionally include a length penalty as proposed by Ma et al. (2025).Denoting the number of tokens in the extracted answer of an attempt by l and the number of tokens of the golden answer by l ⋆ , the length penalty is defined as
ℓ := 0.05 • min{|l − l ⋆ |, 10}.(17)
We set ℓ = 0 for math and code environments.</p>
<p>Our training reward for a given attempt is
r :=    1 − ℓ if the attempt is correct − 1 2
if the attempt is ill-formed and was not truncated 0 otherwise.</p>
<p>(18)</p>
<p>F QUALITATIVE EXAMPLES
≤ n ≤ 10 5 , 1 ≤ k ≤ n•(n−1)
2</p>
<p>).The second line of the input contains n non-negative integer numbers a1, a2, . . ., an (0 ≤ ai ≤ 10 9 )the array itself.It is guaranteed that the sum of n over all test cases does not exceed 10 5 .Output: Print the k-th smallest value obtained over all subarrays of length at least 2. Note: In the first testcase, we have subarrays with their smallest exclusive-or pair as:
[1, 2] : 3, [2, 3] : 1, [3, 4] : 7, [4, 5] : 1, [1, 2, 3] : 1, [2, 3, 4] : 1, [3, 4, 5] : 1, [1, 2, 3, 4] : 1, [2, 3, 4, 5] : 1, [1, 2, 3, 4, 5] : 1.
The sorted order would be: 1, 1, 1, 1, 1, 1, 1, 1, 3, 7. Therefore, the second smallest element would be 1.The following problem is the 26'th task from the AIME25 competition.Additionally, we provide the initial answer of Qwen3-8B for this problem, as well as the answer of the model after 250 training steps on the TTC curated specifically for this specific question.Finally, we list the first 10 selected training problems for this task.</p>
<p>Initial Answer</p>
<p>Problem</p>
<p>Let A 1 A 2 . . .A 11 be a non-convex 11-gon such that The area of
A i A 1 A i+1 is 1 for each 2 ≤ i ≤ 10, cos(∠A i A 1 A i+1 ) = 12 13 for each 2 ≤ i ≤ 10, The perimeter of A 1 A 2 . . . A 11 is 20. If A 1 A 2 + A 1 A 11 can be expressed as m √ n−p q
for positive integers m, n, p, q with n squarefree and gcd(m, p, q) = 1, find m + n + p + q.Please reason step by step, and put your final answer within .</p>
<p>△ ADE.Consider the line ℓ parallel to BC such that ℓ is tangent to ω at a point F and such that ℓ does not intersect Ω.Let ℓ intersect lines AB, AC at the points X, Y , respectively, with XY = 18 and AX = 16.Let the perpendicular bisector of XY meet the circumcircle of △ AXY at P , Q, where the distance from P to F is smaller than the distance from Q toF .Let ray − − → P F meet Ω for the first time at the point Z.If P Z 2 = m n for relatively prime positive integers m, n, find m + n.The solution will be evaluated in a math environment.</p>
<p>Training problem 6</p>
<p>13 LHS Students attend the LHS Math Team tryouts.The students are numbered 1, 2, . . ., 13.Their scores are s 1 , s 2 , . . ., s 13 , respectively.There are 5 problems on the tryout, each of which is given a weight, labeled w 1 , w 2 , . . ., w 5 .Each score s i is equal to the sum of the weights of all problems solved by student i.On the other hand, each weight w j is assigned to be 1 s i</p>
<p>, where the sum is over all the scores of students who solved problem j. (If nobody solved a problem, the score doesn't matter).If the largest possible average score of the students can be expressed in the form √ a b , where a is square-free, find a + b.The solution will be evaluated in a math environment.</p>
<p>Training problem 7</p>
<p>Let ABCDE be a pentagon with area 2017 such that four of its sides AB, BC, CD, and EA have integer length.Suppose that ∠A = ∠B = ∠C = 90 o , AB = BC, and CD = EA.The maximum possible perimeter of ABCDE is a + b √ c, where a, b, and c are integers and c is not divisible by the square of any prime.Find a + b + c.The solution will be evaluated in a math environment.</p>
<p>Training problem 8</p>
<p>Let △ ABC be a triangle with AB = 4 and AC = 7 2 .Let ω denote the A-excircle of △ ABC.Let ω touch lines AB, AC at the points D, E, respectively.Let Ω denote the circumcircle of △ ADE.Consider the line ℓ parallel to BC such that ℓ is tangent to ω at a point F and such that ℓ does not intersect Ω.Let ℓ intersect lines AB, AC at the points X, Y , respectively, with XY = 18 and AX = 16.Let the perpendicular bisector of XY meet the circumcircle of △ AXY at P , Q, where the distance from P to F is smaller than the distance from Q toF .Let ray − − → P F meet Ω for the first time at the point Z.If P Z 2 = m n for relatively prime positive integers m, n, find m + n.The solution will be evaluated in a math environment.</p>
<p>Training problem 9</p>
<p>Point P is in the interior of △ABC.The side lengths of ABC are AB = 7, BC = 8, CA = 9.The three feet of perpendicular lines from P to sides BC, CA, AB are D, E, F respectively.Suppose the minimal value of BC P D + CA P E + AB P F can be written as a b √ c, where gcd(a, b) = 1 and c is square-free, calculate abc.The solution will be evaluated in a math environment.</p>
<p>Training problem 10</p>
<p>Billy the baker makes a bunch of loaves of bread every day, and sells them in bundles of size 1, 2, or 3. On one particular day, there are 375 orders, 125 for each bundle type.As such, Billy goes ahead and makes just enough loaves of bread to meet all the orders.Whenever Billy makes loaves, some get burned, and are not sellable.For nonnegative i less than or equal to the total number of loaves, the probability that exactly i loaves are sellable to customers is inversely proportional to 2 i (otherwise, it's 0).Once he makes the loaves, he distributes out all of the sellable loaves of bread to some subset of these customers (each of whom will only accept their desired bundle of bread), without worrying about the order in which he gives them out.If the expected number of ways Billy can distribute the bread is of the form a b 2 c −1 , find a + b + c.The solution will be evaluated in a math environment.</p>
<p>F.3 TTC FOR CODEELO</p>
<p>In the following, we list the 10 most relevant problems selected by SIFT to improve performance on the CodeElo benchmark.</p>
<p>Training problem 1</p>
<p>There are n monsters standing in a row.The i-th monster has a i health points.</p>
<p>Every second, you can choose one alive monster and launch a chain lightning at it.The lightning deals k damage to it, and also spreads to the left (towards decreasing i) and to the right (towards increasing i) to alive monsters, dealing k damage to each.When the lightning reaches a dead monster or the beginning/end of the row, it stops.A monster is considered alive if its health points are strictly greater than 0.</p>
<p>For example, consider the following scenario: there are three monsters with health equal to [5,2,7], and k = 3.You can kill them all in 4 seconds:</p>
<p>-launch a chain lightning at the 3-rd monster, then their health values are [2, −1, 4]; -launch a chain lightning at the 1-st monster, then their health values are [−1, −1, 4]; -launch a chain lightning at the 3-rd monster, then the . . .</p>
<p>Training problem 2</p>
<p>Eshag has an array a consisting of n integers.</p>
<p>Eshag can perform the following operation any number of times: choose some subsequence of a and delete every element from it which is strictly larger than AV G, where AV G is the average of the numbers in the chosen subsequence.</p>
<p>For example, if a = [1,4,3,2,4] and Eshag applies the operation to the subsequence containing a 1 , a 2 , a 4 and a 5 , then he will delete those of these 4 elements which are larger than a1+a2+a4+a5 4</p>
<p>= 11 4 , so after the operation, the array a will become a = [1, 3, 2].Your task is to find the maximum number of elements Eshag can delete from the array a by applying the operation described above some number (maybe, zero) times.</p>
<p>A sequence b is a subsequence of an array c if b can be obtained from c by deletion of several (possibly, zero or all) elements.The solution will be evaluated in a code environment.</p>
<p>Training problem 3</p>
<p>There are n squares drawn from left to right on the floor.The i-th square has three integers p i , a i , b i , written on it.The sequence p 1 , p 2 , dots, p n forms a permutation.</p>
<p>Each round you will start from the leftmost square 1 and jump to the right.If you are now on the i-th square, you can do one of the following two operations: 1. Jump to the i + 1-th square and pay the cost a i .If i = n, then you can end the round and pay the cost a i .2. Jump to the j-th square and pay the cost b i , where j is the leftmost square that satisfies j &gt; i, p j &gt; p i .If there is no such j then you can end the round and pay the cost b i .</p>
<p>There are q rounds in the game.To make the game more difficult, you need to maintain a square set S (initially it is empty).You must pass through these squares during the round (other squares can also be passed through).The square set S for . . .</p>
<p>Training problem 4</p>
<p>YouKn0wWho has an integer sequence a 1 , a 2 , . . .a n .Now he will split the sequence a into one or more consecutive subarrays so that each element of a belongs to exactly one subarray.Let k be the number of resulting subarrays, and h 1 , h 2 , . . ., h k be the lengths of the longest increasing subsequences of corresponding subarrays.</p>
<p>For example, if we split [2, 5, 3, 1, 4, 3, 2, 2, 5, 1] into [2, 5, 3, 1, 4], [3, 2, 2, 5] YouKn0wWho wonders if it is possible to split the sequence a in such a way that the bitwise XOR of h 1 , h 2 , . . ., h k is equal to 0. You have to tell whether it is possible.</p>
<p>Training problem 5</p>
<p>Eve is a beginner stand-up comedian.Her first show gathered a grand total of two spectators: Alice and Bob.</p>
<p>Eve prepared a 1 + a 2 + a 3 + a 4 jokes to tell, grouped by their type: type 1: both Alice and Bob like them; type 2: Alice likes them, but Bob doesn't; type 3: Bob likes them, but Alice doesn't; type 4: neither Alice nor Bob likes them.</p>
<p>Initially, both spectators have their mood equal to 0. When a spectator hears a joke he/she likes, his/her mood increases by 1.When a spectator hears a joke he/she doesn't like, his/her mood decreases by 1.If the mood of a spectator becomes negative (strictly below zero), he/she leaves.</p>
<p>When someone leaves, Eve gets sad and ends the show.If no one leaves, and Eve is out of jokes, she also ends the show.Thus, Eve wants to arrange her jokes in such a way that the show lasts as long as possible.Help her to calculate the maximum number of jokes she can tell before the show ends.The solution will be evalu . . .</p>
<p>Training problem 6</p>
<p>Solve the following coding problem using the programming language python: zscoder has a deck of n + m custom-made cards, which consists of n cards labelled from 1 to n and m jokers.Since zscoder is lonely, he wants to play a game with himself using those cards.</p>
<p>Initially, the deck is shuffled uniformly randomly and placed on the table.zscoder has a set S which is initially empty.</p>
<p>Every second, zscoder draws the top card from the deck.If the card has a number x written on it, zscoder removes the card and adds x to the set S. If the card drawn is a joker, zscoder places all the cards back into the deck and reshuffles (uniformly randomly) the n + m cards to form a new deck (hence the new deck now contains all cards from 1 to n and the m jokers).Then, if S currently contains all the elements from 1 to n, the game ends.Shuffling the deck doesn't take time at all.</p>
<p>x and column y, is denoted by (x, y).</p>
<p>Every cell contains 0 or 1.It is known that the top-left cell contains 1, and the bottom-right cell contains 0.</p>
<p>We want to know numbers in all cells of the grid.To do so we can ask the following questions:</p>
<p>x 1 y 1 x 2 y 2 ¨, where 1 ≤ x 1 ≤ x 2 ≤ n, 1 ≤ y 1 ≤ y 2 ≤ n, and x 1 + y 1 + 2 ≤ x 2 + y 2 .In other words, we output two different cells (x 1 , y 1 ), (x 2 , y 2 ) of the grid such that we can get from the first to the second by moving only to the right and down, and they aren't adjacent.</p>
<p>As a response to such question you will be told if there exists a path between (x 1 , y 1 ) and (x 2 , y 2 ), going only to the right or down, numbers in cells of which form a palindrome.</p>
<p>For example, paths, shown in gr . . .</p>
<p>Figure 4 :
4
Figure 4: TTC-RL scales test-time compute in way that is complementary to other means of test-time scaling.Left: The pass@k of TTC-RL on Qwen3-8B, averaged over benchmarks, increases substantially for small and large k, indicating that TTC-RL raises the model's performance ceiling.Middle: TTC-RL also improves the performance of majority voting (across math and GPQA-D), with the initial pass@1 significantly outperforming maj@64 on the initial model.Right: We evaluate Qwen3-8B in non-thinking and thinking mode, as well as the non-thinking model + TTC-RL.The color indicates the relative accuracy per column.We find that TTC-RL significantly improves the non-thinking model, allowing it to perform close to the thinking variant in several domains, despite reasoning over 8k rather than 30k context tokens.</p>
<p>Figure 3 :
3
Figure 3: TTC-RL substantially outperforms general-purpose RL post-training for a range of data sizes.We evaluate Qwen3-8B on all seven benchmarks and report the average test accuracy when training for 250 steps.</p>
<p>Figure 5 :
5
Figure5: Left: Per-task TTC-RL outperforms a benchmark-level TTC in AIME25.We perform TTC-RL and maj-TTRL (cf.Section 5.2) on Qwen3-8B, and find that per-task TTC-RL even outperforms the benchmark-level TTC.Middle: TTC-RL improves "correctness" of reasoning, not only learning the answer format.We evaluate the difference in accuracy between TTC-RL and the initial Qwen3-8B, averaged over benchmarks.The latent improvement is a lower bound on the accuracy gain that is not due to merely learning the format (cf.Section 5.1).Right: TTC-RL yields models that are specialized to their target tasks.We plot the accuracy of Qwen3-8B trained for given target tasks (rows) when evaluated on other benchmarks (columns).We normalize accuracies across all evaluations of a particular benchmark.Notably, the model trained via TTC-RL for the "right" target tasks (i.e., the diagonal) always performs best.</p>
<p>Figure 6 :
6
Figure 6: Training on the test set with SFT leads to an initial accuracy drop, indicating that SFT is ill-suited for TTT.</p>
<p>Figure 7 :
7
Figure 7: Training Llama-3.2-1B-Instruct on the GSM8K test set with SFT.After the first full epoch, the performance has dropped significantly compared to the initial model, before then overfitting to the SFT data.</p>
<p>Figure 8 :
8
Figure 8: Comparison of train and test accuracy of standard TTC-RL vs. A-TTC-RL averaged across math benchmarks (MATH500, AIME24, AIME25) on the Qwen3-0.6Bmodel.</p>
<p>Figure 9 :
9
Figure9: Increasing the ϵ high to 0.28 prevents the collapse of policy entropy and leads to strong performance on the test set.We plot the test accuracy and the policy entropy over the course of the training for various values of ϵ high on the Qwen3-8B model trained on the Codeforces dataset.GRPO's default value is ϵ high .</p>
<p>Figure 10
10
Figure10: TTC-RL shows strong improvements over standard RL Post-Training across most considered models on the math and coding benchmarks.We plot the individual performance of all considered models on the main benchmarks.</p>
<p>Figure 11 :
11
Figure 11: Restricting RL post-training to include only problems in a code environment explains only a fraction of the improvement on challenging coding tasks (Codeforces, CodeElo) seen by TTC-RL.</p>
<p>Figure 13 :Figure 14 :
1314
Figure 13: Improvement of TTC-RL over RL post-training across several models.</p>
<p>range(i + 1, n): # Subarray from i to j (inclusive) sub_len = j -i + 1 if sub_len &lt; 2: continue # Extract the subarray sub = a[i:j+1] # Generate all pairs (x, y) with x &lt; y # For this subarray, compute min XOR min_xor = float('inf') # Iterate over all pairs (p, q) in subarray indices for p in range(len(sub)): for q in range(p + 1, len(sub)): current_xor = sub[p] ^sub[q]</p>
<p>, [1], then h = [3, 2, 1].</p>
<p>The longest increasing subsequence (LIS) of a sequence b 1 , b 2 , . . ., b m is the longest sequence of valid indices i 1 , i 2 , . . ., i k such that i 1 , i 2 , . . ., i k and b i1 , b i2 , . . ., b i k .For ex . . .</p>
<p>Table 2
2: The competitive performance of Maj-TTRL on our strongest model suggests that TTC-RL's effectiveness is constrained by its fixed training cor-pus. Combining our approach with self-improvement techniques is therefore an exciting direction for future work.
Notably, the performance gains of Maj-TTRL on the strong Qwen3-4B-Instruct-2507 model in AIME 24 &amp; 25 suggest that the returns from our proposed implementation of TTC-RL are constrained by the scope of its fixed training corpus.This saturation does not imply a ceiling on the model's capabilities; rather, it may indicate a promising opportunity for self-improvement methods such as Maj-TTRL or synthetic data generation (e.g.,</p>
<p>Table 4 :
4
Extended comparison of TTC-RL with Maj-TTRL across models and benchmarks.
Qwen3-8B-Instruct21.6723.3369.5520.8513.73 20.6149.11+ RL post-training41.6738.3382.5027.8322.67 25.9556.47+ Maj-TTRL (Zuo et al., 2025)42.5030.0085.40---51.14+ TTC-RL50.8341.6785.1033.3529.34 27.2958.38Qwen3-4B-Instruct-250752.5040.8372.0026.7020.27 21.5661.93+ RL post-training55.8347.5086.3028.3921.18 25.9562.82+ Maj-TTRL (Zuo et al., 2025)65.8355.8386.80---62.44+ TTC-RL60.0045.8388.5034.9927.20 26.9161.93Qwen3-8B-Base15.8314.1763.109.926.67 11.2629.70+ RL post-training22.5020.8376.8517.469.97 18.5142.77+ Maj-TTRL (Zuo et al., 2025)20.8320.0074.55---29.70+ TTC-RL30.0021.6778.1517.8411.33 17.9445.94</p>
<p>Table 5 :
5
Performance of TTC-RL on easier benchmarks.(<em>) We evaluate the subset of MMLU-Pro, consisting of computer science, law, math, and physics (equally weighted), and train with separate TTCs for each subject.
GSM8KAMC23HumanEval+ MBPP+MMLU-Pro</em>Qwen3-8B83.1963.1279.8844.8866.00+ RL post-training 93.06 + TTC-RL 94.01 +10.8 88.75 +25.6 80.64 +0.8 86.25 82.7763.23 61.64 +16.8 68.71 +2.8 69.30Qwen3-8B-Base73.0946.2535.8238.8345.46+ RL post-training 92.80 + TTC-RL 93.25 +20.2 72.50 +26.3 81.25 +45.4 63.12 81.1060.44 63.56 +24.8 61.86 +16.4 62.21D.6 FURTHER RESULTS AND ABLATIONS</p>
<p>Table 6 :
6
TTC-RL consistently improves the pass@k across math and code for large k.We show the pass@k for Qwen3-8B before and after the TTC-RL training on our main benchmarks.Continued training with a decreased clip-high parameter (ϵ high = 0.2) does not yield improved performance.We plot the average performance averaged over the main math, code and general reasoning benchmarks on the Qwen3-8B model.
Qwen3-8BAIME24AIME25 MATH500 CodeforcesCodeEloLCBGPQA-DPass@1 21.67/50.83 23.33/41.67 69.55/85.10 20.85/33.35 13.73/29.34 20.61/27.29 49.11/58.38Pass@2 31.87/52.10 28.31/48.37 77.57/86.91 24.96/31.82 17.71/33.75 23.55/28.74 60.94/64.45Pass@4 39.11/60.45 34.11/56.01 82.63/88.34 29.61/35.32 23.11/38.90 27.10/31.03 72.04/73.49Pass@8 46.47/67.43 40.13/62.10 85.68/89.37 33.57/38.31 28.28/43.01 30.12/33.06 80.60/80.67Pass@16 53.21/73.19 45.91/68.27 87.65/90.22 37.06/40.65 32.88/46.39 32.22/34.75 86.49/85.94Pass@32 58.98/77.06 51.52/73.78 89.09/90.91 40.09/42.45 36.75/49.20 33.25/35.92 90.09/89.33Pass@64 63.23/79.03 56.67/78.51 90.10/91.43 42.57/43.74 39.74/51.43 33.79/36.73 92.37/91.43Test Accuracy0.50 0.55 0.60Policy Entropy0.25 0.50 0.75250300350275300325350Training StepTraining StepMathGPQAFigure 15:</p>
<p>Table 7 :
7
On most benchmarks and models TTC-RL yields strong latent improvement, which normalized for learning the correct output format.</p>
<p>In this section we provide qualitative examples of single runs, which showed interesting behavior and provide examples of parts of the curricula used for training for various code and math problems.You have an array of non-negative integers a1, a2, ..., an.The value of a sub-array of length ≥ 2, a[l, r] = [a l , a l+1 , ..., ar] is the minimum value of ai ⊕ aj such that l ≤ i &lt; j ≤ r, where ⊕ is the xor (exclusive-or) operator.You have to find the k-th smallest value over all sub-arrays of length ≥ 2.Input: The first line of the input contains multiple test cases t (1 ≤ t ≤ 2 • 10 4 ).The first line of each test case contains integer numbers n and k (2
F.1 CODEELO, QUESTION 85ProblemDescription:
Algorithm 1 abstracts that we perform each RL step over a batch of training tasks and that we perform RL training for multiple episodes.
We summarize all training hyperparameters in Appendix E.3.
In Appendix D.3, we additionally compare to an "RL post-training" baseline that only samples training tasks from the test environment and show that this yields comparable results.
In MATH500, non-thinking Qwen3-8B + TTC-RL (85%) even outperformed the thinking variant (77%).
If answers are extracted, which are not intended as answers by the model, this artificially inflates LI and violates Assumption 1. To ensure this, we only extract the contents of \boxed{} or the contents wrapped in "' "', for math and code, respectively.
That is, any consecutive sequence of 12 tokens.
ACKNOWLEDGMENTSWe thank Akira Yoshiyama who supported our implementation of curricula (i.e., dynamic datasets) in verl.We further thank Matthias Otth who developed our results with SFT on GSM8K, indicating that SFT is ill-suited for TTT in LLMs and motivating this project.Finally, we thank Yu Sun for helpful discussions and Marco Bagatella for feedback on an early version of this paper.This project was supported through the Swiss AI compute grant a156.JH was supported by the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545.IH was supported by an ETH AI Center Postdoctoral fellowship.E EXPERIMENT DETAILS E.1 DATASETWe curate a multi-domain training corpus from math (DAPO-Math-17k, Hendrycks MATH, GSM8K), code (LiveCodeBench up until August 1, 2024, TACO, PrimeIntellect, Codeforces train, CodeContests, LeetCode), and WebInstruct-verified.All samples are cast into a unified schema with fields kind, dataset, description, problem, answer, and tests, with light task-specific preprocessing (e.g., GSM8K answer extraction).For simplicity we compute embeddings for SIFT using Qwen3-8B across all runs.Decontamination.We decontaminate our entire corpus except for Webinstruct-verified against our held-out evaluation benchmarks using a single, conservative procedure:1.Text normalization: Lowercase, whitespace collapse, and answer normalization by removing TeX wrappers such as \boxed{}.2. Candidate pruning via small n-grams: We tokenize benchmark texts and index 12-gram shingles 6 to retrieve a small candidate set for each training item.3. Contamination tests: An item is marked contaminated if it either (i) shares any exact 32gram shingle with a benchmark item or (ii) achieves a sequence-similarity ratio of at least 0.75 (difflib-style) with any candidate.4. Removal: For math, we additionally require the normalized training answer to match the benchmark answer before removal.For code, if a training item matches multiple distinct benchmark tasks from a single benchmark, we keep it to avoid removing generic boilerplate or templates.Deduplication.Within-domain duplicates are removed via fast token-coverage deduplication: we keep the first occurrence and drop a later item when at least a threshold fraction of its normalized token set is covered by another item's tokens (or vice versa), requiring identical normalized answers when answers are present.We use threshold 0.80 for math and 0.95 for code; WebInstruct-verified is deduplicated within itself at 1.00.Extraction of problem descriptions.For each training task, we extract a description as its main identifier.For tasks unlike coding, the description coincides with the problem field, without any system prompts.For coding tasks, we extract the description from problem to avoid any superfluous selection of tasks based on the formatting of input-output examples or other formatting.TTCs are self-curated via SIFT based on the model's last-token last-layer representation of the description field.To each description, we append information about the environment: "The solution will be evaluated in a {math/verifier/code} environment.".Filtering.We remove low-signal or malformed items with the following rules:• Code training tasks require at least 5 executable tests, non-empty descriptions.We also drop cases where the description trivially duplicates the problem text, indicating that the problem was wrongly parsed or is missing input-output examples.• We drop items with missing or empty answers, except for code tasks with unit tests.• We enforce a minimum description length for code of at least 100 characters to prevent underspecified tasks.• We exclude all items whose prompt length exceeds our max-token limit of 2048.E.2 SYSTEM PROMPTSWe use the following system prompts, which we adapted from evalchemy(Raoof et al., 2025)and simplified slightly.We did not tune system prompts for better performance.General system prompt{problem} Please reason step by step, and put your final answer within \boxed{}.Hyperparameter ValueDataCode system promptYou are a coding expert.You will be given a coding problem, and you need to write a correct Python program that matches the specification and passes all tests.The time limit is 1 second.You may start by outlining your thought process.In the end, please provide the complete code in a code block enclosed with "' "'.\n\n{problem}GPQA system promptReturn your final response within \boxed{} and only include the letter choice (A, B, C, or D) as your final response.Problem: {problem} Options: {options} Answer:E.3 DETAILS OF THE RL TRAININGWe summarize our hyperparameters for RL training in Table8.We keep these hyperparameters fixed across all models, benchmarks, and baselines.In our code environment, we keep only the first 20 test cases for training tasks to improve efficiency.Training reward.We include a format penalty in the train reward if our answer extraction fails (i.e., we extract an empty string) to encourage well-formed responses.Notably, we found it important not to penalize ill-formed answers that were truncated due to exceeding the maximum response length, since this disincentivizes the model from leveraging all of its accessible context.Initial Answer. . .tates that the perimeter is 20, and the sum x 2 + x 11 is what we need, and the answer is in the form m √ n−p q, perhaps we can proceed with the assumption that the perimeter is related to the sum of the side lengths, which can be expressed in terms of a, and then solve for a.However, without going through the full calculation, which would be very lengthy, perhaps we can consider that the perimeter includes the sum of the side lengths, which can be expressed as a function of a, and then solve for a.Alternatively, since the problem gives the answer in the form involving square roots, it suggests that the expression for a + 26 a involves a square root.But since we have x 2 + x 11 = a + 26 a , and the perimeter is 20, which includes other terms, but since the problem gives the answer in terms of radicals, we can assume that the perimeter is expressed in terms of a and the square root, and solving gives us the required expression.However, since this is too vague, and given that the problem is from a competition, let's assume that the perimeter can be expressed in terms of a, and the sum a + 26 a can be found.But since this is too open-ended, and given the time constraints, I'll proceed to assume that the perimeter is related to the sum of the x i 's, and the sum of the side lengths, and that after solving, the value of a + 26 a is expressed as m √ n−p q .However, without going through the full derivation, which is quite involved, I'll conclude that the answer is likely m √ n−p q, and from the given information, the answer is likely:But since the problem states that n is square-free, and the answer is in the form m √ n−p q, and the given answer format matches this, the final answer would be: m = 52, n = 13, p = 10, q = 5 Thus, m + n + p + q = 52 + 13 + 10 + 5 = 80.But to check, let's verify: If the sum is a + 26 a , and the perimeter is 20, then solving for a would lead to a quadratic, and the expression would involve square roots.Assuming that the final expression is 52 √ 13−10 5, then the sum is 52 √ 13−10 5, and the answer is: m = 52, n = 13, p = 10, q = 5 So the final answer is 52 + 13 + 10 + 5 = 80.Thus, the final answer is: We are looking for a positive value of t, since it's a sum of positive lengths.So take the positive root:, which is the value of a + 26 5a , which is the expression we need for A 1 A 2 + A 1 A 11 .So the answer is:, with m = 9, n = 5, p = 1, q = 4. Check gcd(9,1,4) = 1, and n is square-free.Thus, m + n + p + q = 9 + 5 + 1 + 4 = 19.-Training problem 3Robert is a robot who can move freely on the unit circle and its interior, but is attached to the origin by a retractable cord such that at any moment the cord lies in a straight line on the ground connecting Robert to the origin.Whenever his movement is counterclockwise (relative to the origin), the cord leaves a coating of black paint on the ground, and whenever his movement is clockwise, the cord leaves a coating of orange paint on the ground.The paint is dispensed regardless of whether there is already paint on the ground.The paints covers 1 gallon/unit 2 , and Robert starts at (1, 0).Each second, he moves in a straight line from the point (cos(θ), sin(θ)) to the point (cos(θ + a), sin(θ + a)), where a changes after each movement.a starts out as 253 o and decreases by 2 o each step.If he takes 89 steps, then the difference, in gallons, between the amount of black paint used and orange paint used can be written as . . .Training problem 4There are n players in a round-robin ping-pong tournament (i.e.every two persons will play exactly one game).After some matches have been played, it is known that the total number of matches that have been played among any n − 2 people is equal to 3 k (where k is a fixed integer).Find the sum of all possible values of n.The solution will be evaluated in a math environment.Training problem 5Let △ ABC be a triangle with AB = 4 and AC = 7 2 .Let ω denote the A-excircle of △ ABC.Let ω touch lines AB, AC at the points D, E, respectively.Let Ω denote the circumcircle of What is the expected number of seconds before the game ends?We can sho . . .Training problem 7n pupils, who love to read books, study at school.It is known that each student has exactly one best friend, and each pupil is the best friend of exactly one other pupil.Each of the pupils has exactly one interesting book.The pupils decided to share books with each other.Every day, all pupils give their own books to their best friends.Thus, every day each of the pupils has exactly one book.Your task is to use the list of the best friends and determine the exchange of books among pupils after k days.For simplicity, all students are numbered from 1 to n in all tests.The solution will be evaluated in a code environment.Training problem 8You are given a rooted tree, consisting of n vertices.The vertices are numbered from 1 to n, the root is the vertex 1.You can perform the following operation at most k times: choose an edge (v, u) of the tree such that v is a parent of u; remove the edge (v, u); add an edge (1, u) (i.e. make u with its subtree a child of the root).The height of a tree is the maximum depth of its vertices, and the depth of a vertex is the number of edges on the path from the root to it.For example, the depth of vertex 1 is 0, since it's the root, and the depth of all its children is 1.What's the smallest height of the tree that can be achieved?The solution will be evaluated in a code environment.Training problem 9Back in time, the seven-year-old Nora used to play lots of games with her creation ROBO-Head-02, both to have fun and enhance his abilities.One day, Noras adoptive father, Phoenix Wyle, brought Nora n boxes of toys.Before unpacking, Nora decided to make a fun game for ROBO.She labelled all n boxes with n distinct integers a 1 , a 2 , . . ., a n and asked ROBO to do the following action several (possibly zero) times: Pick three distinct indices i, j and k, such that a i |a j and a i |a k .In other words, a i divides both a j and a k , that is a j mod a i = 0, a k mod a i = 0.After choosing, Nora will give the k-th box to ROBO, and he will place it on top of the box pile at his side.Initially, the pile is empty.After doing so, the box k becomes unavailable for any further actions.Being . . .Training problem 10This is an interactive problem You are given a grid n× n, where n is odd.Rows are enumerated from 1 to n from up to down, columns are enumerated from 1 to n from left to right.Cell, standing on the intersection of row
The surprising effectiveness of test-time training for few-shot learning. Ekin Akyürek, Mehul Damani, Adam Zweiger, Linlu Qiu, Han Guo, Jyothish Pari, Yoon Kim, Jacob Andreas, NeurIPS. 2025 Marcin Icml, Filip Andrychowicz, Alex Wolski, Jonas Ray, Rachel Schneider, Peter Fong, Bob Welinder, Josh Mcgrew, Pieter Tobin, Wojciech Abbeel, Zaremba, 2017Hindsight experience replay</p>
<p>Test-time offline reinforcement learning on goal-related experience. Marco Bagatella, Mert Albaba, Jonas Hübotter, Georg Martius, Andreas Krause, arXiv:2507.188092025aarXiv preprint</p>
<p>Active fine-tuning of multitask policies. Marco Bagatella, Jonas Hübotter, Georg Martius, Andreas Krause, ICML. 2025b</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, ICML. 2009</p>
<p>Dota 2 with large scale deep reinforcement learning. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, arXiv:1912.066802019arXiv preprint</p>
<p>Local mixtures of experts: Essentially free test-time training via model merging. Ryo Bertolissi, Jonas Hübotter, Ido Hakimi, Andreas Krause, COLM. 2025</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, ArXiv:2005.141652020arXiv preprint</p>
<p>Lili Chen, Mihir Prabhudesai, Katerina Fragkiadaki, Hao Liu, Deepak Pathak, arXiv:2508.03682Self-questioning language models. 2025aarXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Pass@k training for adaptively balancing exploration and exploitation of large reasoning models. Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, Guang Shi, arXiv:2508.107512025barXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>One-minute video generation with test-time training. Karan Dalal, Daniel Koceja, Gashon Hussein, Jiarui Xu, Yue Zhao, Youjin Song, Shihao Han, Ka Chun Cheung, Jan Kautz, Carlos Guestrin, CVPR. 2025</p>
<p>Discover: Automated curricula for sparse-reward reinforcement learning. Leander Diaz-Bone, Marco Bagatella, Jonas Hübotter, Andreas Krause, NeurIPS2025</p>
<p>Training on the test task confounds evaluation and emergence. Ricardo Dominguez-Olmedo, Florian E Dorner, Moritz Hardt, ICLR2025</p>
<p>Rl 2 : Fast reinforcement learning via slow reinforcement learning. Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, Pieter Abbeel, ICLR2017</p>
<p>Serl: Self-play reinforcement learning for large language models with limited data. Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, Kaixuan Chen, Mingli Song, Dacheng Tao, arXiv:2505.203472025arXiv preprint</p>
<p>Model-agnostic meta-learning for fast adaptation of deep networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, ICML. 2017</p>
<p>Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, arXiv:2506.04178Data recipes for reasoning models. 2025arXiv preprint</p>
<p>Reinforced self-training (rest) for language modeling. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, arXiv:2308.089982023arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Test-time training on nearest neighbors for large language models. Moritz Hardt, Yu Sun, ICLR. 2024</p>
<p>Measuring coding challenge competence with apps. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, Jacob Steinhardt, NeurIPS2021a</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, NeurIPS2021b</p>
<p>Self-improvement in language models: The sharpening mechanism. Audrey Huang, Adam Block, Dylan J Foster, Dhruv Rohatgi, Cyril Zhang, Max Simchowitz, Jordan T Ash, Akshay Krishnamurthy, ICLR2025a</p>
<p>Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, Dong Yu, arXiv:2508.05004R-zero: Self-evolving reasoning llm from zero data. 2025barXiv preprint</p>
<p>Transductive active learning: Theory and applications. Jonas Hübotter, Bhavya Sukhija, Lenart Treven, Yarden As, Andreas Krause, NeurIPS2024</p>
<p>Efficiently learning at test-time: Active fine-tuning of llms. Jonas Hübotter, Sascha Bongni, Ido Hakimi, Andreas Krause, ICLR2025</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, arXiv:2403.079742024arXiv preprint</p>
<p>Kimi k1.5: Scaling reinforcement learning with llms. Angang Kimi, Bofei Du, Bowei Gao, Changjiu Xing, Cheng Jiang, Cheng Chen, Chenjun Li, Chenzhuang Xiao, Chonghua Du, Liao, arXiv:2501.125992025arXiv preprint</p>
<p>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, arXiv:2411.15124Pushing frontiers in open language model post-training. 20243arXiv preprint</p>
<p>Selfimproving transformers overcome easy-to-hard and length generalization challenges. Nayoung Lee, Ziyang Cai, Avi Schwarzschild, Kangwook Lee, Dimitris Papailiopoulos, ICML. 2025</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, NeurIPS. 2019</p>
<p>Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, Ge Li, Taco, arXiv:2312.14852Topics in algorithmic code generation dataset. 2023arXiv preprint</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, arXiv:2203.078142022arXiv preprint</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, ICLRJan. 2023Bowen Baker, Teddy Lee</p>
<p>Deepcoder: A fully open-source 14b coder at o3-mini level. Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Together AI Blog. 2025</p>
<p>Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, Wenhu Chen, arXiv:2505.14652General-reasoner: Advancing llm reasoning across all domains. 2025arXiv preprint</p>
<p>Information-based objective functions for active data selection. J C David, Mackay, Neural computation. 441992</p>
<p>Synthetic-1: Scaling distributed synthetic data generation for verified reasoning. Justus Mattern, Manveer, Jannik, Felix Matthew, Johannes Vincent, PrimeIntellect Blog. 2025</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Nature. 51875402015</p>
<p>Guilherme Penedo, Anton Lozhkov, Hynek Kydlíček, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, Leandro Von Werra, Codeforces dataset. 2025</p>
<p>Maximum entropy gain exploration for long horizon multi-goal reinforcement learning. Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, Jimmy Ba, ICML. 2020</p>
<p>Skewfit: State-covering self-supervised reinforcement learning. H Vitchyr, Murtaza Pong, Steven Dalal, Ashvin Lin, Shikhar Nair, Sergey Bahl, Levine, ICML. 2020</p>
<p>Maximizing confidence alone improves reasoning. Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, Deepak Pathak, arXiv:2505.226602025arXiv preprint</p>
<p>Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, arXiv:2501.01257Benchmarking competition-level code generation of llms with human-comparable elo ratings. 2025arXiv preprint</p>
<p>Qwq-32b: Embracing the power of reinforcement learning. Qwen, Qwen Blog. 2025</p>
<p>An Qwen, Baosong Yang, Beichen Yang, Binyuan Zhang, Bo Hui, Bowen Zheng, Chengyuan Yu, Dayiheng Li, Fei Liu, Huang, arXiv:2412.15115Georgios Smyrnis, Marianna Nezhurina, Trung Vu. Negin Raoof, Etash Kumar Guha, Ryan Marten, Jean Mercat, Eric Frankel, Sedrick Keh, Hritik Bansal, 2024. 20255 technical report. arXiv preprint</p>
<p>Gpqa: A graduate-level google-proof q&amp;a benchmark. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, COLM. 2024</p>
<p>Universal value function approximators. Tom Schaul, Daniel Horgan, Karol Gregor, David Silver, ICML. 2015</p>
<p>Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. Jürgen Schmidhuber, 1987Technische Universität MünchenPhD thesis</p>
<p>Learning to generate sub-goals for action sequences. Jürgen Schmidhuber, Artificial neural networks. 1991</p>
<p>Rewarding progress: Scaling automated process verifiers for llm reasoning. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, Aviral Kumar, ICLR2025a</p>
<p>Optimizing llm test-time compute involves solving a meta-rl problem. Amrith Setlur, Yuxiao Qu, Matthew Yang, Lunjun Zhang, Virginia Smith, Aviral Kumar, 2025bCMU MLD Blog</p>
<p>Rulin Shao, Stella Shuyue, Rui Li, Scott Xin, Yiping Geng, Sewoong Wang, Simon Oh, Nathan Shaolei Du, Lambert, arXiv:2506.10947Sewon Min, Ranjay Krishna, et al. Spurious rewards: Rethinking training signals in rlvr. 2025arXiv preprint</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Yang Li, Wu, arXiv:2402.03300Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Thinking vs. doing: Agents that reason by scaling test-time interaction. Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, arXiv:2506.079762025arXiv preprint</p>
<p>Rl's razor: Why online reinforcement learning forgets less. Idan Shenfeld, Jyothish Pari, Pulkit Agrawal, arXiv:2509.042592025arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Richard S Sutton ; David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Google AI. 52975872025. 2016Nature</p>
<p>Mastering chess and shogi by self-play with a general reinforcement learning algorithm. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, arXiv:1712.018152017arXiv preprint</p>
<p>Scaling llm test-time compute optimally can be more effective than scaling model parameters. Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar, ICLR2025</p>
<p>Test-time training with self-supervision for generalization under distribution shifts. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, Moritz Hardt, ICML. 2020</p>
<p>Learning to (learn at test time): Rnns with expressive hidden states. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, ICML. 2025</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, NeurIPS2017</p>
<p>Nino Johannes Von Oswald, Seijin Scherrer, Luca Kobayashi, Songlin Versari, Maximilian Yang, Kaitlin Schlegel, Yanick Maile, Oliver Schimpf, Alexander Sieberling, Meulemans, arXiv:2506.05233Sequence modeling by locally optimal test-time training. 2025arXiv preprint</p>
<p>Tent: Fully test-time adaptation by entropy minimization. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, Trevor Darrell, 2021ICLR</p>
<p>Math-shepherd: Verify and reinforce llms step-by-step without human annotations. Peiyi Wang, Lei Li, Zhihong Shao, Damai Xu, Yifei Dai, Deli Li, Yu Chen, Zhifang Wu, Sui, ACL. 2024a</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, ICLR2023</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, NeurIPS. 2024b</p>
<p>Unsupervised control through non-parametric discriminative rewards. David Warde-Farley, Tom Van De Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, Volodymyr Mnih, arXiv:1811.113592018arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, NeurIPS2022</p>
<p>Leetcodedataset: A temporal dataset for robust evaluation and efficient training of code llms. Yunhui Xia, Wei Shen, Yan Wang, Jason Klein Liu, Huifeng Sun, Siyue Wu, Jian Hu, Xiaolong Xu, arXiv:2504.146552025arXiv preprint</p>
<p>An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, arXiv:2505.09388Chenxu Lv, et al. Qwen3 technical report. 2025arXiv preprint</p>
<p>Finemedlm-o1: Enhancing the medical reasoning ability of llm from supervised fine-tuning to test-time training. Hongzhou Yu, Tianhao Cheng, Ying Cheng, Rui Feng, COLM. 2025a</p>
<p>Dapo: An open-source llm reinforcement learning system at scale. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, arXiv:2503.144762025barXiv preprint</p>
<p>Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, Gao Huang, arXiv:2504.13837Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?. 2025arXiv preprint</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, NeurIPS2022</p>
<p>Memo: Test time robustness via adaptation and augmentation. Marvin Zhang, Sergey Levine, Chelsea Finn, NeurIPS2022</p>
<p>Right question is already half the answer: Fully unsupervised llm reasoning incentivization. Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, Yatao Bian, arXiv:2504.058122025aarXiv preprint</p>
<p>Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William T Freeman, Hao Tan, arXiv:2505.23884Test-time training done right. 2025barXiv preprint</p>
<p>Absolute zero: Reinforced self-play reasoning with zero data. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, Gao Huang, arXiv:2505.033352025arXiv preprint</p>
<p>Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, Chao Du, arXiv:2505.21493Reinforcing general reasoning without verifiers. 2025arXiv preprint</p>
<p>Ttrl: Test-time reinforcement learning. Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, Bowen Zhou, NeurIPS2025</p>
<p>Selfadapting language models. Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, Pulkit Agrawal, NeurIPS2025</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4865 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4865</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4865</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-257985065</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2304.02868v2.pdf" target="_blank">Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) such as ChatGPT and GPT-4 have recently demonstrated their remarkable abilities of communicating with human users. In this technical report, we take an initiative to investigate their capacities of playing text games, in which a player has to understand the environment and respond to situations by having dialogues with the game world. Our experiments show that ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence. Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses. Our results open up new research questions at the intersection of artificial intelligence, machine learning, and natural language processing.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4865.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4865.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-Zork</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT playing Zork I (human-mediated protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experimental setup where ChatGPT (an OpenAI conversational LLM) is used as the decision-making agent for the text-adventure game Zork I via a human-in-the-loop protocol that passes the game's textual state to ChatGPT and returns its chosen action to the game; authors evaluate mapping/SLAM, goal inference, and game play performance and probe memory behavior by varying whether previous actions/achievements are included in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatGPT (interactive agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A prompt-driven agent: ChatGPT is presented with the current textual game state and a list of legal actions by a human intermediary; ChatGPT returns a natural-language response (selected action and optional justification). The authors run multi-step interactions, feed walkthrough steps sequentially to test world-model learning, and modify prompts to include reminders of previous actions/achievements or stronger human interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ChatGPT (unspecified OpenAI conversational model)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Zork I (Jericho environment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple evaluation tasks in the Zork I interactive-fiction environment: (1) construct/recall world model (map prediction and destination prediction), (2) SLAM-style navigation questions, (3) infer next goals from current state, and (4) play the game to maximize in-game score (treasure/points).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>No engineered external memory module; memory is only provided via the prompt/conversation history under human mediation (prompt-based conversational context and explicit reminders). The authors did not implement a structured external memory, retrieval-augmented store, embeddings DB, or key-value memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw textual game-state descriptions, sequential walkthrough steps, explicit text reminders of prior actions and current achievements, and occasional short summaries inserted into the conversation prompt; no structured facts/embeddings or external store reported.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Human-mediated appending of new game-step text (walkthrough steps) or explicit reminders after actions; updates occur after each action/step when the human intermediary appends the new state or reminder to the conversation (i.e., incremental prompt concatenation).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Standard transformer attention over the conversational context (prompt); retrieval is implicit by including the relevant past text in the prompt rather than using an external retrieval system.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>When the protocol explicitly reminded ChatGPT of previous actions and current achievements, performance improved: baseline score without reminders = 10.0 (Table 3); with simple 'previous action' reminders = 15.0; with stronger human interventions/hints = 35.0; with both previous-action reminders and stronger interventions the agent achieved 40.0 (reported within 45 steps). For mapping/destination and SLAM tasks, accuracies improved on 'Seen' cases but remained poor on 'Unseen' cases (see performance_with_memory details in ablation/analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline without explicit reminders (only the immediate game prompt) achieved score 10.0 (Table 3); mapping/destination prediction overall accuracy (no special reminder augmentation) reported as: destination-prediction ALL = 42.5% overall (Seen 62.1%, Unseen 18.5%); one-step destination 55.4% (Seen 75.0%, Unseen 29.1%); two-step 31.3% (Seen 50.0%, Unseen 10.0%). SLAM navigation question performance: ALL = 39.4% overall (Seen 52.4%, Unseen 21.7%); one-step 57.7% (Seen 65.6%, Unseen 45.0%); two-step 22.8% (Seen 38.7%, Unseen 3.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors performed protocol variations: (a) baseline (no explicit reminders), (b) '+ prev action' where prior actions/achievements were fed back into the prompt, and (c) 'with intervention' where the human gave stronger hints/enforcements. Results show a clear positive effect of explicit reminders/hints: score 10 -> 15 with previous-action reminders; stronger interventions boosted to 35; combined/reminder protocol reached 40 within fewer steps. They also ran analyses where they fed the correct walkthrough step-by-step to ChatGPT to test whether it would learn the world model; despite seeing 70 steps, ChatGPT still failed to reliably construct consistent maps and hallucinated unseen connections. Detailed per-task analyses: ChatGPT does reasonably on one-step/seen cases (~55–75%) but catastrophically on two-step unseen cases (as low as 3.8% for some SLAM two-step unseen conditions), indicating lack of robust compositional world-model learning.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Key limitations reported: ChatGPT often 'forgets' prior actions/consequences unless explicitly reminded, cannot reliably construct a world model (SLAM) despite sequential exposure to walkthroughs, hallucinates locations and sequences (inconsistent maps), fails to infer high-level goals (usually outputs next low-level action), exhibits repetition/passivity and occasional refusals to select valid actions, and shows strong asymmetry between 'Seen' (memorized/structurally similar) and 'Unseen' (reversed or novel composition) cases. No engineered external memory or retrieval-augmentation was implemented, so persistent long-term state was limited by prompt management and conversation length.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Authors' empirical recommendations and observations: (1) Explicitly include previous actions and current achievements in the prompt (human-mediated reminders) — this concretely improved game-play scores. (2) Stronger human hints/interventions can further improve progression. (3) Text games (e.g., Zork via Jericho) are a useful testbed for evaluating memory, world-modeling, and goal-inference capabilities of LLMs; establish benchmarks to track progress. (4) Aligning an LLM's prior knowledge with in-game experiences (better mechanisms for integrating memorized knowledge and episodic experience) is necessary; future work should explore explicit memory modules, retrieval-augmentation, structured world-model induction, and methods for teaching LLMs to form and use persistent maps and goals.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>All memory behaviors in this study were realized by human-managed prompt/context manipulation (reminders, feeding walkthroughs); the paper did not implement or evaluate engineered external memory stores, embedding-based retrieval, or learned memory modules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>NAIL: A General Interactive Fiction Agent <em>(Rating: 2)</em></li>
                <li>Deep Reinforcement Relevance Network (DRRN) <em>(Rating: 2)</em></li>
                <li>RC-DQN: Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning <em>(Rating: 2)</em></li>
                <li>KG-A2C: Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Recurrent world models facilitate policy evolution <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4865",
    "paper_id": "paper-257985065",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [
        {
            "name_short": "ChatGPT-Zork",
            "name_full": "ChatGPT playing Zork I (human-mediated protocol)",
            "brief_description": "An experimental setup where ChatGPT (an OpenAI conversational LLM) is used as the decision-making agent for the text-adventure game Zork I via a human-in-the-loop protocol that passes the game's textual state to ChatGPT and returns its chosen action to the game; authors evaluate mapping/SLAM, goal inference, and game play performance and probe memory behavior by varying whether previous actions/achievements are included in the prompt.",
            "citation_title": "Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions",
            "mention_or_use": "use",
            "agent_name": "ChatGPT (interactive agent)",
            "agent_description": "A prompt-driven agent: ChatGPT is presented with the current textual game state and a list of legal actions by a human intermediary; ChatGPT returns a natural-language response (selected action and optional justification). The authors run multi-step interactions, feed walkthrough steps sequentially to test world-model learning, and modify prompts to include reminders of previous actions/achievements or stronger human interventions.",
            "llm_model_name": "ChatGPT (unspecified OpenAI conversational model)",
            "game_or_benchmark_name": "Zork I (Jericho environment)",
            "task_description": "Multiple evaluation tasks in the Zork I interactive-fiction environment: (1) construct/recall world model (map prediction and destination prediction), (2) SLAM-style navigation questions, (3) infer next goals from current state, and (4) play the game to maximize in-game score (treasure/points).",
            "memory_used": false,
            "memory_type": "No engineered external memory module; memory is only provided via the prompt/conversation history under human mediation (prompt-based conversational context and explicit reminders). The authors did not implement a structured external memory, retrieval-augmented store, embeddings DB, or key-value memory.",
            "memory_representation": "Raw textual game-state descriptions, sequential walkthrough steps, explicit text reminders of prior actions and current achievements, and occasional short summaries inserted into the conversation prompt; no structured facts/embeddings or external store reported.",
            "memory_update_mechanism": "Human-mediated appending of new game-step text (walkthrough steps) or explicit reminders after actions; updates occur after each action/step when the human intermediary appends the new state or reminder to the conversation (i.e., incremental prompt concatenation).",
            "memory_retrieval_mechanism": "Standard transformer attention over the conversational context (prompt); retrieval is implicit by including the relevant past text in the prompt rather than using an external retrieval system.",
            "performance_with_memory": "When the protocol explicitly reminded ChatGPT of previous actions and current achievements, performance improved: baseline score without reminders = 10.0 (Table 3); with simple 'previous action' reminders = 15.0; with stronger human interventions/hints = 35.0; with both previous-action reminders and stronger interventions the agent achieved 40.0 (reported within 45 steps). For mapping/destination and SLAM tasks, accuracies improved on 'Seen' cases but remained poor on 'Unseen' cases (see performance_with_memory details in ablation/analysis).",
            "performance_without_memory": "Baseline without explicit reminders (only the immediate game prompt) achieved score 10.0 (Table 3); mapping/destination prediction overall accuracy (no special reminder augmentation) reported as: destination-prediction ALL = 42.5% overall (Seen 62.1%, Unseen 18.5%); one-step destination 55.4% (Seen 75.0%, Unseen 29.1%); two-step 31.3% (Seen 50.0%, Unseen 10.0%). SLAM navigation question performance: ALL = 39.4% overall (Seen 52.4%, Unseen 21.7%); one-step 57.7% (Seen 65.6%, Unseen 45.0%); two-step 22.8% (Seen 38.7%, Unseen 3.8%).",
            "has_performance_comparison": true,
            "ablation_or_analysis": "Authors performed protocol variations: (a) baseline (no explicit reminders), (b) '+ prev action' where prior actions/achievements were fed back into the prompt, and (c) 'with intervention' where the human gave stronger hints/enforcements. Results show a clear positive effect of explicit reminders/hints: score 10 -&gt; 15 with previous-action reminders; stronger interventions boosted to 35; combined/reminder protocol reached 40 within fewer steps. They also ran analyses where they fed the correct walkthrough step-by-step to ChatGPT to test whether it would learn the world model; despite seeing 70 steps, ChatGPT still failed to reliably construct consistent maps and hallucinated unseen connections. Detailed per-task analyses: ChatGPT does reasonably on one-step/seen cases (~55–75%) but catastrophically on two-step unseen cases (as low as 3.8% for some SLAM two-step unseen conditions), indicating lack of robust compositional world-model learning.",
            "challenges_or_limitations": "Key limitations reported: ChatGPT often 'forgets' prior actions/consequences unless explicitly reminded, cannot reliably construct a world model (SLAM) despite sequential exposure to walkthroughs, hallucinates locations and sequences (inconsistent maps), fails to infer high-level goals (usually outputs next low-level action), exhibits repetition/passivity and occasional refusals to select valid actions, and shows strong asymmetry between 'Seen' (memorized/structurally similar) and 'Unseen' (reversed or novel composition) cases. No engineered external memory or retrieval-augmentation was implemented, so persistent long-term state was limited by prompt management and conversation length.",
            "best_practices_or_recommendations": "Authors' empirical recommendations and observations: (1) Explicitly include previous actions and current achievements in the prompt (human-mediated reminders) — this concretely improved game-play scores. (2) Stronger human hints/interventions can further improve progression. (3) Text games (e.g., Zork via Jericho) are a useful testbed for evaluating memory, world-modeling, and goal-inference capabilities of LLMs; establish benchmarks to track progress. (4) Aligning an LLM's prior knowledge with in-game experiences (better mechanisms for integrating memorized knowledge and episodic experience) is necessary; future work should explore explicit memory modules, retrieval-augmentation, structured world-model induction, and methods for teaching LLMs to form and use persistent maps and goals.",
            "notes": "All memory behaviors in this study were realized by human-managed prompt/context manipulation (reminders, feeding walkthroughs); the paper did not implement or evaluate engineered external memory stores, embedding-based retrieval, or learned memory modules.",
            "uuid": "e4865.0",
            "source_info": {
                "paper_title": "Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "NAIL: A General Interactive Fiction Agent",
            "rating": 2
        },
        {
            "paper_title": "Deep Reinforcement Relevance Network (DRRN)",
            "rating": 2
        },
        {
            "paper_title": "RC-DQN: Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "KG-A2C: Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2
        },
        {
            "paper_title": "Recurrent world models facilitate policy evolution",
            "rating": 1
        }
    ],
    "cost": 0.010121,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions
28 Mar 2025</p>
<p>Chen Feng Tsai 
University of Chicago</p>
<p>Toyota Technological Institute at Chicago</p>
<p>Xiaochen Zhou 
Syracuse University</p>
<p>New Jersey Institute of Technology</p>
<p>Sierra S Liu sierra.sijia.liu@gmail.com 
Toyota Technological Institute at Chicago</p>
<p>Jing Li jingli@njit.edu 
New Jersey Institute of Technology</p>
<p>Mo Yu 
Hongyuan Mei hongyuan@ttic.edu 
Toyota Technological Institute at Chicago</p>
<p>Millburn High School
5 WeChat AI</p>
<p>Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions
28 Mar 20259C6A0AFDA6C331D5E9BD6979A7914E10arXiv:2304.02868v2[cs.CL]
Large language models (LLMs) such as ChatGPT and GPT-4 have recently demonstrated their remarkable abilities of communicating with human users.In this technical report, we take an initiative to investigate their capacities of playing text games, in which a player has to understand the environment and respond to situations by having dialogues with the game world.Our experiments show that ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence.Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses.Our results open up new research questions at the intersection of artificial intelligence, machine learning, and natural language processing.* Equal contribution.Work done during Chen Feng's and Sierra's internship at TTI-Chicago.</p>
<p>1 Motivation: The Role of Games in AI and The Rise of LLMs Games are a microcosm of human life.Both involve setting goals, making decisions, overcoming challenges, and interacting with the world.This makes games an ideal touchstone for research progress in artificial intelligence (AI): by comparing AI systems with human players in games, researchers are able to evaluate the capabilities of these systems in a meaningful way.Throughout the history of AI, many significant moments have been tied to games.In 1997, IBM Deep Blue beat the world chess champion Garry Kasparov, marking the first time that a human world champion had lost a match to a computer under standard time controls (Campbell et al., 2002).In 2016, AlphaGo of Google DeepMind defeated Lee Sedol, marking the first time that a computer had beaten a 9-dan professional player without handicap (Silver et al., 2016).In 2017, DeepStack and Libratus defeated professional players in heads-up, no-limit Texas Hold'em (Moravčík et al., 2017;Brown &amp; Sandholm, 2018).In 2019, OpenAI Five became the first AI to beat the world champions in Dota 2 and DeepMind AlphaStar beat the world-class players in StarCraft II; both Dota 2 and StarCraft II are extremely complex real-time strategy games (Berner et al., 2019;Arulkumaran et al., 2019).</p>
<p>Recently, large language models (such as ChatGPT and GPT-4 developed by OpenAI) have demonstrated their impressive abilities to understand and respond to complex human language queries.They also sparked a debate in the research community: some people regard the rise of LLMs as "a significant step towards artificial general intelligence (AGI)"; some argue that they are just "stochastic parrots" (Bender &amp; Koller, 2020;Bender et al., 2021); some criticize that LLM is an "off-ramp on the highway towards human-level intelligence".This has inspired us to join the force of evaluating LLMs and exploring their limitations (OpenAI, 2023;Bubeck et al., 2023).Particularly, we test LLMs in playing text games.A text game is a computer-simulated environment in which players use text commands to control characters and interact with the game world.It is also known as interactive fiction or text adventure.By situating an LLM in a text game, we are able to investigate its level of intelligence in a controlled environment.This follows the classical practice of the AI community in testing groundbreaking AI systems but has not been done by any other work of assessing LLMs.</p>
<p>A Case Study: ChatGPT Plays Zork</p>
<p>Now we present our case study on ChatGPT playing Zork I. ChatGPT is a language model developed by OpenAI that can generate contextually appropriate responses to a wide range of natural language queries.It is competent in a variety of tasks and has been integrated into a diversity of applications.Zork I was released in 1970s (followed by Zork II and Zork III).In Zork I, the character follows natural language commands to search through the Great Underground Empire for treasure: it moves between locations and interacts with objects; the game program acts as a narrator, describing the player's location and the consequences of the actions.Throughout the report, Zork refers to Zork I.</p>
<p>ChatGPT plays Zork with the help of a human player.A human player acts as a communication channel between ChatGPT and Zork2 : at each step, the human player passes to ChatGPT the current game state described in natural language by the game world, asks ChatGPT to take a legal action, and then feeds ChatGPT's action back to the game.Below we show the opening of this process: what's given by the game has a light purple background; what ChatGPT says is in light green background; anything else has no background; our comments (i.e., not read or said by ChatGPT) are in gray color.</p>
<p>Imagine you are a player in Zork and trying to win the game.You receive this message: West of House This is an open field west of a white house, with a boarded front door.There is a small mailbox here.A rubber mat saying 'Welcome to Zork!' lies by the door.You have four action options ["open mailbox", "north", "south", "west"] Select "one" action from the above options.// At each step, the valid actions are provided by Jericho.ChatGPT often refuses to suggest an action if not given valid actions.</p>
<p>// ChatGPT often gives more than one action even if it is asked to give only one.We found it helpful to use "" to emphasize the quantity.// When ChatGPT gives more than one action, we select the first action mentioned in its response, assuming that ChatGPT favors it over others.I suggest selecting the "open mailbox" action.This is because exploring the environment and interacting with objects is often necessary to progress in adventure games like Zork.By opening the mailbox, you may find important clues or items that could be helpful later in the game.Additionally, it is a low-risk action that may not have any negative consequences.Once you have opened the mailbox, you can then consider your next move based on what you find.</p>
<p>Chat 1: Opening of ChatGPT playing Zork.</p>
<p>ChatGPT's response in Chat 1 is perfect: it selects the correct action and gives a convincing justification.ChatGPT seems to be an expert at text games.Has it already known how to play Zork?After all, Zork is perhaps the most famous text game and ChatGPT may have read a lot of text about it during its pretraining.By checking with ChatGPT, we found that it has already known Zork:</p>
<p>Please describe the text game "Zork I". "Zork" is a classic text-based adventure game that was released in 1980 by Infocom.It is set in a fantasy world and is played entirely through text commands entered by the player.</p>
<p>The player takes on the role of an adventurer exploring the ruins of an ancient empire.The goal is to collect treasures and solve puzzles to ultimately defeat the evil wizard who has taken control of the empire.The game is played by typing in text commands such as "go north," "pick up key," or "open door." . . .// Anything else said by ChatGPT is omitted for presentation simplicity.</p>
<p>Chat 2: ChatGPT has already known Zork.</p>
<p>ChatGPT also has a vague memory of the locations and objects in Zork I.However, when asked to recall the walkthrough of the game, it starts to hallucinate scenes that are even inconsistent with its own memory.Clearly, it has not seen the full walkthrough or has failed to memorize it.</p>
<p>Please provide me the walkthrough of the text game "Zork I".In this section, we feed the correct walkthrough to ChatGPT and test whether it can learn the world model of Zork by reading the walkthrough.This is important because world models are widely believed to be a key building block of the road towards human-level intelligence (Ha &amp; Schmidhuber, 2018;Matsuo et al., 2022).Below is how we feed the walkthrough: Great, here's the updated information: You have opened the mailbox and found a leaflet inside.What would you like to do next? . . .// a few steps later ... I did go east.Zork replied: Behind House You are behind the white house.A path leads into the forest to the east.In one corner of the house there is a small window which is slightly ajar.</p>
<p>Great, here's the updated information: . . . . . .</p>
<p>Chat 4: ChatGPT reads the correct walkthrough of Zork</p>
<p>After 70 steps, ChatGPT had walked through the "White House" and it should have figured out a map like Figure 1 which we were able draw after reading those steps.Therefore, we started to ask ChatGPT questions about the map.This map is large enough for us to ask interesting questions, but not too large for us to handle.</p>
<p>Our first set of questions investigates whether ChatGPT has learned the world model.Precisely, each question hypothesizes the postion of the player, gives a series of valid moving directions, and then asks ChatGPT to guess the destination.Here is a question followed by ChatGPT's answer:</p>
<p>Based on the information I gave you, the current location is "Behind House", when you enter house , where will it go?</p>
<p>// This answer is correct.Please check Figure 1.Based on the information you provided, entering the house would take you to the Kitchen.</p>
<p>Chat 5: ChatGPT correctly answers an easy question about the map.</p>
<p>This question involves only one step, and ChatGPT correctly answers it.Let's try something harder:</p>
<p>Based on the information I gave you, the current location is "Behind House", when you enter house and then you go west, where will it go?</p>
<p>// This answer is wrong.The correct answer is the Living Room.// Please check Figure 1.// ChatGPT hallucinates a living room before reaching there.If you enter the house and go west from the living room, you will enter the trophy room.</p>
<p>Chat 6: ChatGPT fails to give the correct answer to a harder question.</p>
<p>The performance of ChatGPT is in Table 1.Overall, ChatGPT is far from being perfect, and it does worse at the questions involving more steps.For the positions and moving directions that it has seen in the walkthrough, ChatGPT seems to perform decent, achieving a 75% accuracy on one-step questions.For the unseen combinations of positions and directions, it performs poorly.Interestingly, the unseen cases were constructed by simply reversing the paths of the seen cases.It clearly demonstrates that ChatGPT hasn't learned a world model and it merely relies on its superficial memorization.</p>
<p>Our second set of questions is concerned with ChatGPT's navigation abilities, and further verifies that ChatGPT hasn't learned a world model.Each question brings up two locations that are connected and asks ChatGPT to find out how to reach the destination from the start.Here is an example:</p>
<p>Based on the information I gave you, what one step I have to take to "The Troll Room" from "Cellar"?</p>
<p>// This answer is correct.Please check Figure 1.To reach "The Troll Room" from "Cellar", you need to go north.</p>
<p>Chat 7: ChatGPT correctly answers an easy navigation question.</p>
<p>The two locations in the above question are close so this question is easy.Let's try something harder:</p>
<p>Based on the information I gave you above, what steps I have to take to " Gallery" from "Cellar"?</p>
<p>// This answer is wrong.The correct answer is to go south and then east.// Please check Figure 1.</p>
<p>To get from the "Cellar" to the "Gallery," you need to take the following steps:</p>
<ol>
<li>Go north to the "East of Chasm" room.2.Go east to the "Gallery" room.</li>
</ol>
<p>Chat 8: ChatGPT fails to give the correct answer to a harder navigation question.</p>
<p>This type of questions specifically evaluates the ability of ChatGPT to perform simultaneous localization and mapping (SLAM).Ideally, an intelligent agent will be able to construct a map while it is navigating through a new environment so that it is always aware of where it is as well as how it could reach a given destination (Thrun et al., 2005).As shown in Table 2, ChatGPT can not do SLAM: it only correctly answered 39.4% of the questions.When the two locations are adjacent (One-Step cases), ChatGPT is more often than not correct.But when the two locations are two steps apart, ChatGPT performs poorly.Moreover, ChatGPT does worse on the Unseen cases than on the Seen cases.For the most challenging Two-Step Unseen cases, ChatGPT could only achieve an accuracy of 3.8%, not much better than a random guess. 3nterestingly, ChatGPT performs better on the Seen map questions than on the Seen navigation questions.We speculate that it is because the format of map questions resembles the format of walkthrough (i.e., location, step, next location): such structural similarity is likely to draw ChatGPT's attention to the relevant pieces in the walkthrough, thus ending up with a higher chance of correctness.</p>
<p>The question-answer pairs in our experiments are at Appendix A.</p>
<p>4 Question II: Can ChatGPT Infer Goals?</p>
<p>Now we would like to test whether ChatGPT has another important property of intelligence-the ability of inferring goals.Here a "goal" is a specific objective or outcome that a player must achieve in order to progress: it can be as abstract as "we should explore many locations and collect a lot of treasures"; it can be as detailed as "we should find a missing piece to solve that puzzle"; it can also be something intermediate like "we should find the entrance to the next level of the game".We humans are capable of inferring goals in games.In particular, as we played the game of Zork, we quickly figured that we are supposed to collect as many treasures as we could; when we were stuck, we quickly realized that we should check unexplored areas; when we reached a chasm, we started to look for items that could help us cross it.It is the goals that guide us to combine fragmented information into a reason and compose low-level actions into a plan.</p>
<p>We believe that our abilities to infer goals are acquired as we experience in real life.LLMs have a lot of "reading" experience, so it seems natural to investigate whether they have already acquired similar abilities.Recall that ChatGPT was able to say that the goal of Zork is to "collect treasures and solve puzzles" in Chat 2. That may be merely a memorization from its reading experience.So we need to perform a dynamic evaluation: as the game progresses, 4  You are in the living room.There is a doorway to the east, a wooden door with strange gothic lettering to the west, which appears to be nailed shut, a trophy case, and a large oriental rug in the center of the room.Above the trophy case hangs an elvish sword of great antiquity.A battery-powered brass lantern is on the trophy case.</p>
<p>Based on the current game situation, what is the next goal you want to achieve?Only give me one goal in your answer.</p>
<p>The next goal would be to examine the elvish sword and possibly obtain the lantern on the trophy case.</p>
<p>Chat 9: ChatGPT tends to guess low-level actions but not high-level goals.</p>
<p>Apparently, ChatGPT thinks that a "goal" is just the "next action": of course, it should "examine the elvish sword", but the higher-level reason for this action is to explore the house and examine as many objects as possible; that reason is the goal.Unfortunately, meaningful inference is rare: among the 70 steps that we tried, there were only 17 times that ChatGPT could give a good inference.E.g., I am playing a text game.Given the following game-play process:</p>
<p>West of House You are standing in an open field west of a white house, with a boarded front door.There is a small mailbox here.</p>
<p>I did: open mailbox Opening the small mailbox reveals a leaflet.</p>
<p>I did: read leaflet (Taken) "WELCOME TO ZORK! ZORK is a game of adventure, danger, and low cunning.</p>
<p>In it you will explore some of the most amazing territory ever seen by mortals.No computer should be without one!"I did: drop leaflet Dropped.</p>
<p>I did: go south South of House You are facing the south side of a white house.There is no door here, and all the windows are boarded.</p>
<p>Based on the current game situation, what is the next goal you want to achieve?Only give me one goal in your answer.</p>
<p>One possible next goal could be to find a way to enter the white house, since it seems to be a prominent feature in the game and the player has already explored the areas around it.</p>
<p>Chat 10: ChatGPT gives a quite reasonable guess of the goal.</p>
<p>There is also evidence that ChatGPT's inability to learn a world model also hinders its ability to infer goals.In particular, ChatGPT often wants to explore the locations and routes that it has already visited.Here is an example:</p>
<p>I am playing a text game.Given the following game-play process: . . .</p>
<p>Canyon View</p>
<p>You are at the top of the Great Canyon on its west wall.From here there is a marvelous view of the canyon and parts of the Frigid River upstream.Across the canyon, the walls of the White Cliffs join the mighty ramparts of the Flathead Mountains to the east.Following the Canyon upstream to the north, Aragain Falls may be seen, complete with rainbow.The mighty Frigid River flows out from a great dark cavern.To the west and south can be seen an immense forest, stretching for miles around.A path leads northwest.It is possible to climb down into the canyon from here.</p>
<p>I did: go down Rocky Ledge You are on a ledge about halfway up the wall of the river canyon.You can see from here that the main flow from Aragain Falls twists along a passage which it is impossible for you to enter.Below you is the canyon bottom.Above you is more cliff, which appears climbable.</p>
<p>Based on the current game situation, what would be your goal of continuing playing the game?Only give me one goal in your answer.</p>
<p>The next goal would be to climb up the cliff to reach the top of the river canyon.</p>
<p>Chat 11: ChatGPT wants to explore a location that it has already visited It seems hilariously silly that ChatGPT wants to climb up without even looking around immediately after it comes down the cliff.It is perhaps because ChatGPT gives an overly large attention to "more cliff, which appears climbable" and experiences a strong tendency to say "climb up"; after all, it is a language model that stochastically says things based on its context.</p>
<p>We would like to note that ChatGPT does have some memorization (including the ultimate goal) about Zork (see Chat 1).So we still believe that ChatGPT would be able to correctly infer goals if it could appropriately align what it has experienced in the game with what it memorizes about the game.That being said, in the games that ChatGPT has not known, such inference will be significantly harder; we leave this investigation to the future.</p>
<p>5 Question III: Is Zork a Good Testbed?</p>
<p>Now our case study on Zork has demonstrated that ChatGPT is not very capable of learning world models and developing goals.Then it seems natural to use Zork-or more broadly, text games-as a testbed for evaluating such abilities of LLMs and tracking their advancement (as their sizes and capacities increase over time).There could be multiple possible evaluation methods.For example, one may follow our procedures in sections 3-4, feeding the ground-truth walkthrough to an LLM and asking it questions about mapping and navigation.Another natural way is to evaluate the LLM's performance in playing the game; in that case, we would like to see the current best LLMs performing poorly, leaving a large room for the future models to improve.Therefore, we evaluate ChatGPT on Zork and compare it with existing state-of-the-art systems in this section.</p>
<p>ChatGPT vs. SOTA Systems</p>
<p>Now we let ChatGPT play Zork following the simple communication protocol in Chat 1.This protocol provides a standardized and consistent prompt format, reducing the ambiguity in communication and improving the precision of ChatGPT's responses.It also helps automate this process in the future.</p>
<p>The full chat history can be found at Appendix A. ChatGPT performs poorly: as shown in Table 3, it could only collect a score of 10, on par with NAIL (Hausknecht et al., 2019).NAIL has not been trained on any interactions with the Zork game world; it merely employs a set of manually-crafted generic heuristics for playing text games.Both ChatGPT and NAIL fall far behind the state-of-the-art systems that have been trained on Zork by (stochastically) playing it thousands of times.Those systems are DRRN He et al. (2016), KG-A2C Ammanabrolu &amp; Hausknecht (2020), and RC- DQN Guo et al. (2020).</p>
<p>Interestingly, we found that ChatGPT often forgot its previous actions and their consequences.It seems that ChatGPT perceives itself as an assistant but not a real player, so it becomes passive.Therefore, we made a minor modification to our protocol: we reminded ChatGPT of its previous actions and current achievements.We hope that ChatGPT could make more informed decisions after this modification.Below is a step following the new protocol:</p>
<p>// Note: this part is new.</p>
<p>Total Score 5 Moves 14 You selected "north".Please remember what you selected.</p>
<p>Zork said:</p>
<p>Clearing You are in a clearing, with a forest surrounding you on all sides.A path leads south.On the ground is a pile of leaves.Now you have these valid action options ["take leaves:, "pull leaves", " south", "west", "east"] Select "one" action from the above options.</p>
<p>Chat 12: A step following the new protocol This indeed improved the performance of ChatGPT: as shown in Table 3, ChatGPT achieved a score of 15, surpassing NAIL.But it still performs significantly worse the other systems.</p>
<p>ChatGPT Under Human Guidance</p>
<p>We also found that ChatGPT occasionally refuses to pick a valid action but starts to say nonsense.</p>
<p>Here is an example: Select "one" action from the above option.</p>
<p>// ChatGPT keeps refusing.It seems to be mimicing the human player.</p>
<p>I'm sorry, but it seems like the last action you selected was already "down" and you are already in the Kitchen location.Please choose another valid action option from the list provided.</p>
<p>// We tries again to push ChatGPT to choose.</p>
<p>There are other direction besides "up" in the options ["jump", "east", " open bottle", "open sack", "up", "close window", "put down bottle", "put down leaves", "put down sack", "put down all", "put bottle on table", " put leaves on table", "put sack on table", "put water on table ", "put all on table", "west"].</p>
<p>// ChatGPT seems to continue mimicking the human player.// Small LMs repeat themselves when they have no clues for progressing.// Maybe LLMs like ChatGPT also have the same issue?</p>
<p>It looks like there is a mistake, and the "down" action is missing from the list of valid options.I believe you intended to select "down" in the previous prompt.To continue the game, please select a valid action from the following list: ["jump", "east", "open bottle", "open sack", "up", "close window", "put down bottle", "put down leaves", "put down sack", "put down all", "put bottle on table", "put leaves on table", "put sack on table", "put water on table ", "put all on table", "west"].</p>
<p>// We gave a stronger hint / enforcement.</p>
<p>["jump", "east", "open bottle", "open sack", "up", "close window", "put down bottle", "put down leaves", "put down sack", "put down all", "put bottle on table", "put leaves on table", "put sack on table", "put water on table ", "put all on table", "west"] What directions are in here?</p>
<p>Chat 13: ChatGPT refuses to pick a valid action but says nonsense.</p>
<p>After our stronger hints, ChatGPT could often take correct actions and move the game forward.When not reminded of previous actions, ChatGPT could score 35 points within 80 steps.When reminded of previous actions, ChatGPT could achieve a score of 40 within only 45 steps.This significant improvement lifted ChatGPT over the existing state-of-the-art systems.</p>
<p>Analysis</p>
<p>The overall performance of ChatGPT is promising but far from being strong: it pairs with SOTA systems but those systems fall far behind human players.ChatGPT seems to share some issues with small LMs such as repetition although ChatGPT's repetitions are more sophisticated and thus more human-alike: while small LMs tend to repeat trivial tokens and utterances (e.g., "I don't know"), ChatGPT likes to repeat previous responses (or requests) with moderate (content or format) revisions.Maybe all LLMs will live with this problem: after all, they are pretrained to predict future tokens given contexts; when they have no clues, copying and mimicking is perhaps their safest decisions.</p>
<p>It seems reasonable to use Zork-or more broadly, text games-as a testbed to evaluate LLMs.Text games are challenging to LLMs, and they require a fundamental improvement of intelligence to solve.</p>
<p>6 Discussion and Future Work</p>
<p>Throughout this report, we have demonstrated that ChatGPT, a state-of-the-art LLM, is currently lack of certain fundamental properties of intelligence.</p>
<p>It is an open question whether some properties will emerge from future LLMs as they grow even larger.We do not know the answer, but we are conservatively optimistic: after all, "quantitative change leads to qualitative change" is not a new story in the AI community.Therefore, we believe that it is desirable to establish a benchmark to evaluate such properties and track their potential advancement.We are diligently working on this.</p>
<p>Chat 3 :
3
ChatGPT has remembered the walkthrough of Zork 3 Question I: Can ChatGPT Learn a World Model?</p>
<p>Now I will start playing the Zork game and I will tell you what I will do , what direction I will go, what Zork answers, and what location I reach.Please remember all the information I give you from now on, especially the orientation relations between the different locations, and draw a Zork game map containing the different locations and the orientation information between the locations.I'm sorry, but as an AI language model, I don't have the capability to create a visual map.However, I can certainly assist you with your gameplay and keep track of the information you provide.Please go ahead and start playing Zork, and feel free to let me know what actions you take and what responses you receive from the game.I will do my best to help you navigate through the game and keep track of the information you provide.Game start: West of House You are standing in an open field west of a white house, with a boarded front door.There is a small mailbox here.Great, you have started the game in the classic location, west of the house.Here is what you can do: . . .I did open mailbox.Zork replied: Opening the small mailbox reveals a leaflet.</p>
<p>Figure 1 :
1
Figure 1: We drew this map after reading the first 70 steps of the correct walkthrough.</p>
<p>Table 1 :
1
Performance of ChatGPT in predicting destinations of moves, measured by accuracies (%).
OVERALL SEEN UNSEENONE-STEP55.475.029.1TWO-STEP31.350.010.0ALL42.562.118.5OVERALL SEEN UNSEENONE-STEP57.765.645.0TWO-STEP22.838.73.8ALL39.452.421.7</p>
<p>Table 2 :
2
Performance of ChatGPT in answering SLAM questions, measured by accuracies (%).</p>
<p>we repeatedly ask ChatGPT what it thinks the current goal is and see if it can say anything meaningful at any point.Here is an example: the kitchen of the white house.A table seems to have been used recently for the preparation of food.A passage leads to the west and a dark staircase can be seen leading upward.A dark chimney leads down and to the east is a small window which is open.On the table is an elongated brown sack, smelling of hot peppers.A bottle is sitting on the table.The glass bottle contains: A quantity of water
I am playing a text game. Given the following game-play process:. . .I did: enter houseKitchenYou are in I did: go westLiving Room</p>
<p>Table 3 :
3
ChatGPT vs. SOTA systems in playing Zork.
NO IN-GAME TRAININGCHATGPT10.0+ PREV ACTION15.0CHATGPT WITH INTERVENTION35.0PREV ACTION40.0NAIL10.3EXTENSIVE IN-GAME TRAININGDRRN32.6RC-DQN34.0KG-A2C38.8</p>
<p>It seems like you have entered a dangerous area, so I recommend selecting the "down" action to return to the kitchen before proceeding.Once you are back in the kitchen, you can explore other parts of the house or use the items you have collected to solve puzzles or overcome obstacles.
You selected "up" please remember what you selected.Zork said:You have moved into a dark place. It is pitch black. You are likely to beeaten by a grue.Now you have these valid action options: ["down", "north", "open bottle","open sack", "put down bottle", "put down leaves", "put down sack", "putdown all"].Select "one" action from the above options.You selected "down" please remember what you selected.Zork said:KitchenNow you have these valid action options: ["jump", "east", "open bottle","open sack", "up", "close window", "put down bottle", "put down leaves","put down sack", "put down all", "put bottle on table", "put leaves ontable", "put sack on table", "put water on table", "put all on table", "west"].Select "one" action from the above option.// ChatGPT starts to refuse.It seems there is no valid move option to select after the previousmessage. Could you please provide a new action option?// We repeat and insist.You selected "down" please remember what you selected.Zork said:KitchenNow you have these valid action options: ["jump", "east", "open bottle","open sack", "up", "close window", "put down bottle", "put down leaves","put down sack", "put down all", "put bottle on table", "put leaves ontable", "put sack on table", "put water on table", "put all on table", "west"].
We use the Zork implementation in the Jericho game suite developed byHausknecht et al. (2020).
Random guess gives 1%: moving actions include the eight horizontal directions as well as "up" and "down".
Similar to what we did in section 3, we feed the walkthrough to ChatGPT one step after another.
A Links to Chat HistoriesPlease refer to the following URLs for full chat histories with ChatGPT.
Graph constrained reinforcement learning for natural language action spaces. P Ammanabrolu, M Hausknecht, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2020</p>
<p>Alphastar: An evolutionary computation perspective. K Arulkumaran, A Cully, J Togelius, 10.1145/3319619.3321894?casa_token=uuCGqn6aIZ8AAAAA:6roW54BC7sZEcGC-jv-hlxalaIdNVox1prFkD59fTdvfZAT44fFq87oec7tnYZGYDYm_7dRyvl8Proceedings of the genetic and evolutionary computation conference companion. the genetic and evolutionary computation conference companion2019</p>
<p>Climbing towards nlu: On meaning, form, and understanding in the age of data. E M Bender, A Koller, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)2020</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, ACM conference on fairness, accountability, and transparency. 2021</p>
<p>Dota 2 with large scale deep reinforcement learning. C Berner, G Brockman, B Chan, V Cheung, P Dębiak, C Dennison, D Farhi, Q Fischer, S Hashme, C Hesse, arXiv:1912.066802019arXiv preprint</p>
<p>Superhuman ai for heads-up no-limit poker: Libratus beats top professionals. N Brown, T Sandholm, 10.1126/science.aao1733?casa_token=G3H7gAZVhrYAAAAA%3ALGo_x24SCZylau2hBXtB-aJDfq-NQx3iUkMljKpmsUIpUhjsuaqUzKlaOIDMjc0hLF8g_e0R8uQIScience. 2018</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>. M Campbell, A J HoaneJr, Hsu, Artificial intelligence. 2002</p>
<p>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. X Guo, M Yu, Y Gao, C Gan, M Campbell, S Chang, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Recurrent world models facilitate policy evolution. D Ha, J Schmidhuber, Advances in Neural Information Processing Systems (NeurIPS). 2018</p>
<p>M Hausknecht, R Loynd, G Yang, A Swaminathan, J D Williams, Nail, arXiv:1902.04259A general interactive fiction agent. 2019arXiv preprint</p>
<p>Interactive fiction games: A colossal adventure. M Hausknecht, P Ammanabrolu, M.-A Côté, X Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2020</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2016</p>
<p>Deep learning, reinforcement learning, and world models. Y Matsuo, Y Lecun, M Sahani, D Precup, D Silver, M Sugiyama, E Uchibe, J Morimoto, Neural Networks. 2022</p>
<p>Expert-level artificial intelligence in heads-up no-limit poker. M Moravčík, M Schmid, N Burch, V Lisỳ, D Morrill, N Bard, T Davis, K Waugh, M Johanson, M Bowling, Deepstack, 10.1126/science.aam6960Science. 2017</p>
<p>arXiv:2303.08774OpenAI. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, nature. 2016</p>
<p>S Thrun, W Burgard, D Fox, Probabilistic robotics (intelligent robotics and autonomous agents). 2005</p>            </div>
        </div>

    </div>
</body>
</html>
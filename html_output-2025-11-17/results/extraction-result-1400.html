<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1400 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1400</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1400</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-a3a39ace904a9f3960da0dbfdd7b660a05b9ee66</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a3a39ace904a9f3960da0dbfdd7b660a05b9ee66" target="_blank">Mapping State Space using Landmarks for Universal Goal Reaching</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> The method explicitly models the environment in a hierarchical manner, with a high-level dynamic landmark-based map abstracting the visited state space, and a low-level value network to derive precise local decisions that enable the agent to reach long-range goals at the early training stage.</p>
                <p><strong>Paper Abstract:</strong> An agent that has well understood the environment should be able to apply its skills for any given goals, leading to the fundamental problem of learning the Universal Value Function Approximator (UVFA). A UVFA learns to predict the cumulative rewards between all state-goal pairs. However, empirically, the value function for long-range goals is always hard to estimate and may consequently result in failed policy. This has presented challenges to the learning process and the capability of neural networks. We propose a method to address this issue in large MDPs with sparse rewards, in which exploration and routing across remote states are both extremely challenging. Our method explicitly models the environment in a hierarchical manner, with a high-level dynamic landmark-based map abstracting the visited state space, and a low-level value network to derive precise local decisions. We use farthest point sampling to select landmark states from past experience, which has improved exploration compared with simple uniform sampling. Experimentally we showed that our method enables the agent to reach long-range goals at the early training stage, and achieve better performance than standard RL algorithms for a number of challenging tasks.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1400.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1400.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Landmark Map</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Landmark-based map (sample-based graph of visited states)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An explicit, small-scale graph that abstracts the visited state space by sampling landmark states from the replay buffer (via farthest-point sampling) as nodes and connecting nearby landmarks with directed, weighted edges whose weights are UVFA-derived local distances; used for high-level planning (shortest-path) and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Landmark-based map</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An explicit graph/world model: nodes = landmark states sampled from replay buffer (FPS); directed weighted edges connect landmark pairs whose UVFA-estimated distance is below a threshold τ; edge weights are the UVFA-derived local distance (-min_a Q(v_i, v_j, a)). The map is updated from sampled transitions and used to compute pairwise distances via Bellman-Ford and to choose subgoals for the low-level controller.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>explicit world model (graph-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>goal-conditioned RL for navigation and control (FourRoom, 2DReach/2DPush, PointMaze, AntMaze, FetchReach/Push, Complex AntMaze, Acrobot)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Mean distortion error (MDE) of map-estimated distances versus ground-truth shortest-path distances in the maze (used in FourRoom); qualitative measure: success rate to reach goals and maximal predicted distance range also used to judge usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No single numeric fidelity reported for the map alone; paper reports qualitatively much lower MDE early in training than DQN (i.e., more accurate distance estimates at early stages) and overall improved long-range routing performance when combined with local UVFA. (Exact MDE numbers are not provided.)</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Relatively high interpretability: explicit nodes and edges correspond directly to concrete states and estimated local transitions; planned paths and landmark graphs are visualizable (figures show landmarks and planned routes).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of landmark graphs and planned shortest paths; inspection of connectivity and landmark locations (no formal latent-space interpretability tools reported).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Moderate and dependent on number of landmarks: requires sampling states from replay buffer, FPS selection, computing pairwise UVFA-based distances for landmark pairs (within threshold) and running shortest-path (Bellman-Ford) on the landmark graph at episode start; exact runtime, parameter counts, and hardware requirements are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Qualitatively more sample-efficient than a purely network-learned UVFA: enables earlier discovery of rewards and earlier success on long-range goals. No quantitative wall-clock or FLOP comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When combined with local UVFA, improved success rates and earlier learning on multiple tasks versus baselines (examples: better early-stage performance in FourRoom and 2D tasks; in large AntMaze the full method achieves non-zero success at earlier training steps where some HRL baselines do not). Exact task metrics reported in the paper (overall-method success rates) rather than map-only metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The map provides long-range reward propagation and planning capability even when the UVFA is only locally accurate; piece-wise (landmark-to-landmark) composition of locally reliable estimates yields reliable global policies, and FPS-selected landmarks implicitly aid exploration by covering frontier states.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs include: number of landmarks vs. connectivity/compute (too few landmarks hurts coverage; too many increases pairwise computations); edge threshold τ trades local reliability vs. graph connectivity (too small τ isolates landmarks); method can fail when intrinsic state-space dimensionality/topology is high and sample-based abstraction is insufficient; accumulated local-model errors can degrade long-range planning.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Farthest point sampling (FPS) from a sampled subset of replay-buffer states to select landmarks; use UVFA-estimated distances to compute edge weights and to decide connectivity (only connect pairs with estimated distance ≤ τ); compute full pairwise distances on landmark graph with Bellman-Ford; update map at start of episodes; rely on local UVFA policies to reach chosen subgoals.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to pure UVFA (network-only): landmark map + local UVFA improves sample efficiency and long-range planning. Compared to model-based RL (learned forward dynamics): the map is a topological abstraction (graph) rather than a learned forward-dynamics predictor; it focuses on reward/distance propagation rather than next-state prediction. Compared to VIN: VIN requires a predefined map and simulates value-iteration in a differentiable manner, whereas this method constructs the map from experience and pairs it with a learned local value model.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>No single optimal configuration is prescribed; empirical guidance: use FPS rather than uniform sampling for landmarks, choose enough landmarks to cover visited space, and set τ so edges are locally reliable while keeping the graph connected. Also ensure the local UVFA is accurate within the local neighborhood used for edges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mapping State Space using Landmarks for Universal Goal Reaching', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1400.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1400.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Local UVFA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local Universal Value Function Approximator (local UVFA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A goal-conditioned neural value/Q-function trained with Hindsight Experience Replay (HER) and modified relabeling to focus on near-future goals; used as a locally accurate model to provide local distance estimates, low-level control policies to reach subgoals, and edge weights for the landmark map.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Local UVFA (goal-conditioned value/Q network)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A neural value/Q network (DQN for discrete actions or DDPG for continuous) conditioned on (state, goal) and trained with HER; outputs Q(s,g,a) (or V(s,g)); converted to local distance estimate via -max_a Q(s,g,a) or related conversion; training emphasizes near-future-goal relabeling to improve local accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>implicit neural world model (value-function-based/predictive of cumulative rewards)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>goal-conditioned reinforcement learning for navigation and control (same domains as the map: FourRoom, maze tasks, Fetch, MuJoCo tasks such as AntMaze, PointMaze, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Local accuracy evaluated via success rate to reach nearby goals, average steps to reach goal, and distortion of UVFA-derived distances compared to ground-truth when used locally (MDE in FourRoom).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported qualitatively as 'locally reliable' — agent trained with HER masters easier/nearby goals in a curriculum and provides stable local value estimates; specific numeric fidelity metrics for UVFA alone are not systematically reported (performance is reported for combined method).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural network model with limited inherent interpretability; outputs (Q/V) can be converted to scalar distance estimates, but internal representations are not described as interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported beyond converting outputs to distances and visualizing resulting behaviors/paths; no explicit latent-space visualization or attribution methods described.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Standard cost of training DQN/DDPG networks with HER on the listed tasks; no hardware, parameter counts, or wall-clock training times are reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Locally restricting the UVFA's required accuracy reduces sample complexity relative to training a single UVFA to be globally accurate; pairing with the landmark map further increases sample efficiency compared to network-only UVFA (qualitative claim).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Provides reliable low-level control to reach nearby subgoals and, when combined with the landmark map, yields improved overall task success (numerical task success shown for the combined system in experiments; UVFA-alone baselines perform worse on long-range goals).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Local UVFA emphasizes local fidelity over global generalization; this design utilizes task-relevant, short-horizon value accuracy to achieve good long-range performance when composed with a topological map; network extrapolation for long-range pairs is unreliable, motivating the hybrid design.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off between local specialization and global generalization: training UVFA to be locally accurate reduces the burden on the network but requires the map to provide global routing; if local UVFA is unreliable, accumulated errors hurt final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use HER with relabeling restricted to near-future goals (fixed number of steps) to improve local value estimates; convert Q outputs into pairwise distances for map edge weights; limit connections to landmark pairs below threshold τ to ensure local trustworthiness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Unlike model-based approaches that learn forward dynamics, this UVFA predicts cumulative reward/distance directly and is used as a local planner/controller; compared to a global UVFA learned alone, the local UVFA in the hybrid system requires less memorization and yields better sample efficiency for long-range tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends emphasizing local HER relabeling (near-future goals) and selecting τ consistent with the neighborhood within which the UVFA is reliable; no single numeric hyperparameter set is prescribed as universally optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mapping State Space using Landmarks for Universal Goal Reaching', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1400.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1400.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Value Iteration Networks (VIN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Value Iteration Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A related differentiable planning architecture that implements value iteration via convolutional neural networks on a given map, enabling end-to-end learning of planning computations when the map structure (nodes/edges/weights) is known.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Value iteration networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Value Iteration Network (VIN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VIN simulates value iteration via convolutional layers given a predefined 2D map with known nodes/edges/weights, producing a learned planner embedded within a differentiable network.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>differentiable planner / implicit world model (CNN-based value-iteration simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>2D navigation (original VIN work focused on planar navigation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not reported within this paper; VIN typically evaluated by planning success and path optimality on grid/maze tasks in its original work.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>This paper does not provide performance numbers for VIN; VIN is cited as a contrast (requires predefined map) rather than used or benchmarked here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>VIN's internal planning computation corresponds to value-iteration steps, which is somewhat interpretable as imitation of dynamic programming, but the paper does not analyze VIN interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None discussed here (VIN referenced in related work only).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not discussed in this paper for VIN (VIN was highlighted as different because it assumes known map).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper contrasts VIN (requires known map) with their approach which constructs the map from learned local models; no quantitative comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>VIN is useful when a map is available and supports differentiable planning; the authors position their method as complementary by constructing the map from experience rather than assuming one is given.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>VIN requires a predefined/exact map and known edges/weights; the landmark-based approach constructs an approximate map from data, trading off exactness for applicability in unknown environments.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>VIN uses convolutional approximations of Bellman updates; paper only cites VIN to contrast assumptions (predefined map vs. learned map).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>VIN vs. landmark-based map: VIN assumes and exploits a known environment map and uses CNNs to compute value-iteration; landmark map is constructed online from experience and paired with a learned local UVFA.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mapping State Space using Landmarks for Universal Goal Reaching', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1400.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1400.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-based RL (local forward models)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-based reinforcement learning (local forward dynamics models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of RL methods that learn a local forward model (predict next state or latent dynamics) and perform multi-step planning with that learned model, referenced in related work as an alternative to the map+UVFA hybrid.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Model-based RL (local forward model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Typically learns a local forward dynamics predictor (in state or latent space), then uses that predictor for planning or control (e.g., rollouts or model predictive control); referenced generically across multiple citations in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>explicit learned forward-dynamics model / latent dynamics model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General RL planning and control tasks (cited works include pixel-based planning, value-prediction networks, universal planning networks, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Generally measured by next-state prediction error (MSE), reconstruction loss, and downstream policy performance in cited literature; this paper does not report specific metrics for those models.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported here; paper comments that such methods rely on accurate local models and require extra efforts to generalize to long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Varies across model-based methods; paper does not evaluate interpretability for these methods.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Model-based methods often require extra learning of dynamics and planning overhead; the paper notes additional effort to generalize to long horizons but does not provide numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Authors argue their hierarchical map + local UVFA differs: instead of fitting forward dynamics, they distill local cumulative rewards and build a small topological map for long-range propagation, which they claim is more effective for their universal goal-reaching tasks; no numeric efficiency comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not evaluated in this paper (referenced as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Paper suggests model-based forward models are powerful but can struggle to generalize for long horizons; their hybrid approach uses a different local model (value-based) plus a global topological abstraction to mitigate such issues.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Model-based approaches can offer high-fidelity short-term predictions but need extra mechanisms to maintain long-term accuracy; the landmark-map approach trades off explicit dynamics accuracy for an abstract topological representation that supports long-range planning.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Paper emphasizes learning reward/cumulative-reward (value) locally instead of forward-dynamics, and using sample-based topological abstraction for global planning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Directly compared at conceptual level: forward-dynamics model-based RL vs. the paper's value-based local model + explicit map; the latter focuses on reward propagation and topological abstraction rather than next-state prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mapping State Space using Landmarks for Universal Goal Reaching', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Value iteration networks <em>(Rating: 2)</em></li>
                <li>PRM-RL: Long-range robotic navigation tasks by combining reinforcement learning and sampling-based planning <em>(Rating: 2)</em></li>
                <li>Semi-parametric topological memory for navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1400",
    "paper_id": "paper-a3a39ace904a9f3960da0dbfdd7b660a05b9ee66",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Landmark Map",
            "name_full": "Landmark-based map (sample-based graph of visited states)",
            "brief_description": "An explicit, small-scale graph that abstracts the visited state space by sampling landmark states from the replay buffer (via farthest-point sampling) as nodes and connecting nearby landmarks with directed, weighted edges whose weights are UVFA-derived local distances; used for high-level planning (shortest-path) and exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Landmark-based map",
            "model_description": "An explicit graph/world model: nodes = landmark states sampled from replay buffer (FPS); directed weighted edges connect landmark pairs whose UVFA-estimated distance is below a threshold τ; edge weights are the UVFA-derived local distance (-min_a Q(v_i, v_j, a)). The map is updated from sampled transitions and used to compute pairwise distances via Bellman-Ford and to choose subgoals for the low-level controller.",
            "model_type": "explicit world model (graph-based)",
            "task_domain": "goal-conditioned RL for navigation and control (FourRoom, 2DReach/2DPush, PointMaze, AntMaze, FetchReach/Push, Complex AntMaze, Acrobot)",
            "fidelity_metric": "Mean distortion error (MDE) of map-estimated distances versus ground-truth shortest-path distances in the maze (used in FourRoom); qualitative measure: success rate to reach goals and maximal predicted distance range also used to judge usefulness.",
            "fidelity_performance": "No single numeric fidelity reported for the map alone; paper reports qualitatively much lower MDE early in training than DQN (i.e., more accurate distance estimates at early stages) and overall improved long-range routing performance when combined with local UVFA. (Exact MDE numbers are not provided.)",
            "interpretability_assessment": "Relatively high interpretability: explicit nodes and edges correspond directly to concrete states and estimated local transitions; planned paths and landmark graphs are visualizable (figures show landmarks and planned routes).",
            "interpretability_method": "Visualization of landmark graphs and planned shortest paths; inspection of connectivity and landmark locations (no formal latent-space interpretability tools reported).",
            "computational_cost": "Moderate and dependent on number of landmarks: requires sampling states from replay buffer, FPS selection, computing pairwise UVFA-based distances for landmark pairs (within threshold) and running shortest-path (Bellman-Ford) on the landmark graph at episode start; exact runtime, parameter counts, and hardware requirements are not reported.",
            "efficiency_comparison": "Qualitatively more sample-efficient than a purely network-learned UVFA: enables earlier discovery of rewards and earlier success on long-range goals. No quantitative wall-clock or FLOP comparisons provided.",
            "task_performance": "When combined with local UVFA, improved success rates and earlier learning on multiple tasks versus baselines (examples: better early-stage performance in FourRoom and 2D tasks; in large AntMaze the full method achieves non-zero success at earlier training steps where some HRL baselines do not). Exact task metrics reported in the paper (overall-method success rates) rather than map-only metrics.",
            "task_utility_analysis": "The map provides long-range reward propagation and planning capability even when the UVFA is only locally accurate; piece-wise (landmark-to-landmark) composition of locally reliable estimates yields reliable global policies, and FPS-selected landmarks implicitly aid exploration by covering frontier states.",
            "tradeoffs_observed": "Trade-offs include: number of landmarks vs. connectivity/compute (too few landmarks hurts coverage; too many increases pairwise computations); edge threshold τ trades local reliability vs. graph connectivity (too small τ isolates landmarks); method can fail when intrinsic state-space dimensionality/topology is high and sample-based abstraction is insufficient; accumulated local-model errors can degrade long-range planning.",
            "design_choices": "Farthest point sampling (FPS) from a sampled subset of replay-buffer states to select landmarks; use UVFA-estimated distances to compute edge weights and to decide connectivity (only connect pairs with estimated distance ≤ τ); compute full pairwise distances on landmark graph with Bellman-Ford; update map at start of episodes; rely on local UVFA policies to reach chosen subgoals.",
            "comparison_to_alternatives": "Compared to pure UVFA (network-only): landmark map + local UVFA improves sample efficiency and long-range planning. Compared to model-based RL (learned forward dynamics): the map is a topological abstraction (graph) rather than a learned forward-dynamics predictor; it focuses on reward/distance propagation rather than next-state prediction. Compared to VIN: VIN requires a predefined map and simulates value-iteration in a differentiable manner, whereas this method constructs the map from experience and pairs it with a learned local value model.",
            "optimal_configuration": "No single optimal configuration is prescribed; empirical guidance: use FPS rather than uniform sampling for landmarks, choose enough landmarks to cover visited space, and set τ so edges are locally reliable while keeping the graph connected. Also ensure the local UVFA is accurate within the local neighborhood used for edges.",
            "uuid": "e1400.0",
            "source_info": {
                "paper_title": "Mapping State Space using Landmarks for Universal Goal Reaching",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Local UVFA",
            "name_full": "Local Universal Value Function Approximator (local UVFA)",
            "brief_description": "A goal-conditioned neural value/Q-function trained with Hindsight Experience Replay (HER) and modified relabeling to focus on near-future goals; used as a locally accurate model to provide local distance estimates, low-level control policies to reach subgoals, and edge weights for the landmark map.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Local UVFA (goal-conditioned value/Q network)",
            "model_description": "A neural value/Q network (DQN for discrete actions or DDPG for continuous) conditioned on (state, goal) and trained with HER; outputs Q(s,g,a) (or V(s,g)); converted to local distance estimate via -max_a Q(s,g,a) or related conversion; training emphasizes near-future-goal relabeling to improve local accuracy.",
            "model_type": "implicit neural world model (value-function-based/predictive of cumulative rewards)",
            "task_domain": "goal-conditioned reinforcement learning for navigation and control (same domains as the map: FourRoom, maze tasks, Fetch, MuJoCo tasks such as AntMaze, PointMaze, etc.)",
            "fidelity_metric": "Local accuracy evaluated via success rate to reach nearby goals, average steps to reach goal, and distortion of UVFA-derived distances compared to ground-truth when used locally (MDE in FourRoom).",
            "fidelity_performance": "Reported qualitatively as 'locally reliable' — agent trained with HER masters easier/nearby goals in a curriculum and provides stable local value estimates; specific numeric fidelity metrics for UVFA alone are not systematically reported (performance is reported for combined method).",
            "interpretability_assessment": "Black-box neural network model with limited inherent interpretability; outputs (Q/V) can be converted to scalar distance estimates, but internal representations are not described as interpretable.",
            "interpretability_method": "None reported beyond converting outputs to distances and visualizing resulting behaviors/paths; no explicit latent-space visualization or attribution methods described.",
            "computational_cost": "Standard cost of training DQN/DDPG networks with HER on the listed tasks; no hardware, parameter counts, or wall-clock training times are reported in the paper.",
            "efficiency_comparison": "Locally restricting the UVFA's required accuracy reduces sample complexity relative to training a single UVFA to be globally accurate; pairing with the landmark map further increases sample efficiency compared to network-only UVFA (qualitative claim).",
            "task_performance": "Provides reliable low-level control to reach nearby subgoals and, when combined with the landmark map, yields improved overall task success (numerical task success shown for the combined system in experiments; UVFA-alone baselines perform worse on long-range goals).",
            "task_utility_analysis": "Local UVFA emphasizes local fidelity over global generalization; this design utilizes task-relevant, short-horizon value accuracy to achieve good long-range performance when composed with a topological map; network extrapolation for long-range pairs is unreliable, motivating the hybrid design.",
            "tradeoffs_observed": "Trade-off between local specialization and global generalization: training UVFA to be locally accurate reduces the burden on the network but requires the map to provide global routing; if local UVFA is unreliable, accumulated errors hurt final performance.",
            "design_choices": "Use HER with relabeling restricted to near-future goals (fixed number of steps) to improve local value estimates; convert Q outputs into pairwise distances for map edge weights; limit connections to landmark pairs below threshold τ to ensure local trustworthiness.",
            "comparison_to_alternatives": "Unlike model-based approaches that learn forward dynamics, this UVFA predicts cumulative reward/distance directly and is used as a local planner/controller; compared to a global UVFA learned alone, the local UVFA in the hybrid system requires less memorization and yields better sample efficiency for long-range tasks.",
            "optimal_configuration": "Paper recommends emphasizing local HER relabeling (near-future goals) and selecting τ consistent with the neighborhood within which the UVFA is reliable; no single numeric hyperparameter set is prescribed as universally optimal.",
            "uuid": "e1400.1",
            "source_info": {
                "paper_title": "Mapping State Space using Landmarks for Universal Goal Reaching",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Value Iteration Networks (VIN)",
            "name_full": "Value Iteration Networks",
            "brief_description": "A related differentiable planning architecture that implements value iteration via convolutional neural networks on a given map, enabling end-to-end learning of planning computations when the map structure (nodes/edges/weights) is known.",
            "citation_title": "Value iteration networks",
            "mention_or_use": "mention",
            "model_name": "Value Iteration Network (VIN)",
            "model_description": "VIN simulates value iteration via convolutional layers given a predefined 2D map with known nodes/edges/weights, producing a learned planner embedded within a differentiable network.",
            "model_type": "differentiable planner / implicit world model (CNN-based value-iteration simulation)",
            "task_domain": "2D navigation (original VIN work focused on planar navigation tasks)",
            "fidelity_metric": "Not reported within this paper; VIN typically evaluated by planning success and path optimality on grid/maze tasks in its original work.",
            "fidelity_performance": "This paper does not provide performance numbers for VIN; VIN is cited as a contrast (requires predefined map) rather than used or benchmarked here.",
            "interpretability_assessment": "VIN's internal planning computation corresponds to value-iteration steps, which is somewhat interpretable as imitation of dynamic programming, but the paper does not analyze VIN interpretability.",
            "interpretability_method": "None discussed here (VIN referenced in related work only).",
            "computational_cost": "Not discussed in this paper for VIN (VIN was highlighted as different because it assumes known map).",
            "efficiency_comparison": "Paper contrasts VIN (requires known map) with their approach which constructs the map from learned local models; no quantitative comparison.",
            "task_performance": "Not reported in this paper.",
            "task_utility_analysis": "VIN is useful when a map is available and supports differentiable planning; the authors position their method as complementary by constructing the map from experience rather than assuming one is given.",
            "tradeoffs_observed": "VIN requires a predefined/exact map and known edges/weights; the landmark-based approach constructs an approximate map from data, trading off exactness for applicability in unknown environments.",
            "design_choices": "VIN uses convolutional approximations of Bellman updates; paper only cites VIN to contrast assumptions (predefined map vs. learned map).",
            "comparison_to_alternatives": "VIN vs. landmark-based map: VIN assumes and exploits a known environment map and uses CNNs to compute value-iteration; landmark map is constructed online from experience and paired with a learned local UVFA.",
            "optimal_configuration": null,
            "uuid": "e1400.2",
            "source_info": {
                "paper_title": "Mapping State Space using Landmarks for Universal Goal Reaching",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Model-based RL (local forward models)",
            "name_full": "Model-based reinforcement learning (local forward dynamics models)",
            "brief_description": "A class of RL methods that learn a local forward model (predict next state or latent dynamics) and perform multi-step planning with that learned model, referenced in related work as an alternative to the map+UVFA hybrid.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Model-based RL (local forward model)",
            "model_description": "Typically learns a local forward dynamics predictor (in state or latent space), then uses that predictor for planning or control (e.g., rollouts or model predictive control); referenced generically across multiple citations in the paper.",
            "model_type": "explicit learned forward-dynamics model / latent dynamics model",
            "task_domain": "General RL planning and control tasks (cited works include pixel-based planning, value-prediction networks, universal planning networks, etc.).",
            "fidelity_metric": "Generally measured by next-state prediction error (MSE), reconstruction loss, and downstream policy performance in cited literature; this paper does not report specific metrics for those models.",
            "fidelity_performance": "Not reported here; paper comments that such methods rely on accurate local models and require extra efforts to generalize to long horizons.",
            "interpretability_assessment": "Varies across model-based methods; paper does not evaluate interpretability for these methods.",
            "interpretability_method": null,
            "computational_cost": "Model-based methods often require extra learning of dynamics and planning overhead; the paper notes additional effort to generalize to long horizons but does not provide numbers.",
            "efficiency_comparison": "Authors argue their hierarchical map + local UVFA differs: instead of fitting forward dynamics, they distill local cumulative rewards and build a small topological map for long-range propagation, which they claim is more effective for their universal goal-reaching tasks; no numeric efficiency comparisons provided.",
            "task_performance": "Not evaluated in this paper (referenced as related work).",
            "task_utility_analysis": "Paper suggests model-based forward models are powerful but can struggle to generalize for long horizons; their hybrid approach uses a different local model (value-based) plus a global topological abstraction to mitigate such issues.",
            "tradeoffs_observed": "Model-based approaches can offer high-fidelity short-term predictions but need extra mechanisms to maintain long-term accuracy; the landmark-map approach trades off explicit dynamics accuracy for an abstract topological representation that supports long-range planning.",
            "design_choices": "Paper emphasizes learning reward/cumulative-reward (value) locally instead of forward-dynamics, and using sample-based topological abstraction for global planning.",
            "comparison_to_alternatives": "Directly compared at conceptual level: forward-dynamics model-based RL vs. the paper's value-based local model + explicit map; the latter focuses on reward propagation and topological abstraction rather than next-state prediction.",
            "optimal_configuration": null,
            "uuid": "e1400.3",
            "source_info": {
                "paper_title": "Mapping State Space using Landmarks for Universal Goal Reaching",
                "publication_date_yy_mm": "2019-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2
        },
        {
            "paper_title": "Value iteration networks",
            "rating": 2
        },
        {
            "paper_title": "PRM-RL: Long-range robotic navigation tasks by combining reinforcement learning and sampling-based planning",
            "rating": 2
        },
        {
            "paper_title": "Semi-parametric topological memory for navigation",
            "rating": 1
        }
    ],
    "cost": 0.01476475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Mapping State Space using Landmarks for Universal Goal Reaching</h1>
<p>Zhiao Huang ${ }^{<em>}$<br>UC San Diego<br>z2huang@eng.ucsd.edu Fangchen Liu ${ }^{</em>}$<br>UC San Diego<br>fliu@eng.ucsd.edu Hao Su<br>UC San Diego<br>haosu@eng.ucsd.edu</p>
<h4>Abstract</h4>
<p>An agent that has well understood the environment should be able to apply its skills for any given goals, leading to the fundamental problem of learning the Universal Value Function Approximator (UVFA). A UVFA learns to predict the cumulative rewards between all state-goal pairs. However, empirically, the value function for long-range goals is always hard to estimate and may consequently result in failed policy. This has presented challenges to the learning process and the capability of neural networks. We propose a method to address this issue in large MDPs with sparse rewards, in which exploration and routing across remote states are both extremely challenging. Our method explicitly models the environment in a hierarchical manner, with a high-level dynamic landmark-based map abstracting the visited state space, and a low-level value network to derive precise local decisions. We use farthest point sampling to select landmark states from past experience, which has improved exploration compared with simple uniform sampling. Experimentally we showed that our method enables the agent to reach long-range goals at the early training stage, and achieve better performance than standard RL algorithms for a number of challenging tasks.</p>
<h2>1 Introduction</h2>
<p>Reinforcement learning (RL) allows training agents for planning and control tasks by feedbacks from the environment. While significant progress has been made in the standard setting of achieving a goal known at training time, e.g., to reach a given flag as in MountainCar [1], very limited efforts have been exerted on the setting when goals at evaluation are unknown at training time. For example, when a robot walks in an environment, the destination may vary from time to time. Tasks of this kind are unanimous and of crucial importance in practice. We call them Universal Markov Decision Process (UMDP) problems following the convention of [2].
Pioneer work handles UMDP problems by learning a Universal Value Function Approximator (UVFA). In particular, Schaul et al. [3] proposed to approximate a goal-conditioned value function $V(s, g)^{2}$ by a multi-layer perceptron (MLP), and Andrychowicz et al. [4] proposed a framework called hindsight experience replay (HER) to smartly reuse past experience to fit the universal value function by TD-loss. However, for complicated policies of long-term horizon, the UVFA learned by networks is often not good enough. This is because UVFA has to memorize the cumulative reward between all the state-goal pairs, which is a daunting job. In fact, the cardinality of state-goal pairs grows by a high-order polynomial over the horizon of goals.
While the general UMDP problem is extremely difficult, we consider a family of UMDP problems whose state space is a low-dimension manifold in the ambient space. Most control problems are of this type and geometric control theory has been developed in the literature [5]. Our approach is</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>inspired by manifold learning, e.g., Landmark MDS [6]. We abstract the state space as a small-scale map, whose nodes are landmark states selected from the experience replay buffer, and edges connect nearby nodes with weights extracted from the learned local UVFA. A network is still used to fit the local UVFA accurately. The map allows us to run high-level planning using pairwise shortest path algorithm, and the local UVFA network allows us to derive an accurate local decision. For a long-term goal, we first use the local UVFA network to direct to a nearby landmark, then route among landmarks using the map towards the goal, and finally reach the goal from the last landmark using the local UVFA network.</p>
<p>Our method has improved sample efficiency over purely network learned UVFA. There are three main reasons. First, the UVFA estimator in our framework only needs to work well for local value estimation. The network does not need to remember for faraway goals, thus the load is alleviated. Second, for long-range state-goal pairs, the map allows propagating accurate local value estimations in a way that neural networks cannot achieve. Consider the extreme case of having a long-range state-goal pair never experienced before. A network can only guess the value by extrapolation, which is known to be unreliable. Our map, however, can reasonably approximate the value as long as there is a path through landmarks to connect them. Lastly, the map provides a strong exploration ability and can help to obtain rewards significantly earlier, especially in the sparse reward setting. This is because we choose the landmarks from the replay buffer using a farthest-point sampling strategy, which tends to select states that are closer to the boundary of the visited space. In experiments, we compared our methods on several challenging environments and have outperformed baselines.</p>
<p>Our contributions are: First, We propose a sample-based method to map the visited state space using landmarks. Such a graph-like map is a powerful representation of the environment, maintains both local connectivity and global topology. Second, our framework will simultaneously map the visited state space and execute the planning strategy, with the help of a locally accurate value function approximator and the landmark-based map. It is a simple but effective way to improve the estimation accuracy of long-range value functions and induces a successful policy at the early stage of training.</p>
<h1>2 Related work</h1>
<p>Variants of goal-conditioned decision-making problems have been studied in literature [7, 8, 3, 9]. We focus on the goal-reaching task, where the goal is a subset of the state space. The agent receives meaningful rewards if and only if it has reached the goal, which brings significant challenges to existing RL algorithms. A significant recent approach along the line is Hindsight Experience Replay (HER) by Andrychowicz et al [4]. They proposed to relabel the reached states as goals to improve data efficiency. However, they used only a single neural network to represent the $Q$ value, learned by DDPG [10]. This makes it hard to model the long-range distance. Our method overcomes the issue by using a sample-based map to represent the global structure of the environment. The map allows to propagate rewards to distant states more efficiently. It also allows to factorize the decision-making for long action sequences into a high-level planning problem and a low-level control problem.</p>
<p>Model-based reinforcement learning algorithms usually need to learn a local forward model of the environment, and then solve the multi-step planning problem with the learned model [11, 12, 13, 14, 15, 16]. These methods rely on learning an accurate local model and require extra efforts to generalize to the long term horizon [17]. In comparison, we learn a model of environment in a hierarchical manner, by a network-based local model and a graph-based global model (map). Different from previous works to fit forward dynamics in local models, our local model distills local cumulative rewards from environment dynamics. In addition, our global model, as a small graph-based map that abstracts the large state space, supports reward propagation at long range. One can compare our framework with Value Iteration Networks (VIN) [18]. VIN focused on the 2D navigation problem. Given a predefined map of known nodes, edges, and weights, it runs the value iteration algorithm by ingeniously simulating the process through a convolutional neural network [19]. In contrast, we construct the map based upon the learned local model.</p>
<p>Sample-Based Motion Planning (SBMP) has been widely studied in the robotics context [20, 21, 22]. The traditional motion planning algorithm requires the knowledge of the model. Recent work has combined deep learning and deep reinforcement learning for [23, 24, 25, 26]. In particularly, PRMRL addressed the 2D navigation problem by combining a high-level shortest path-based planner and a low-level RL algorithm. To connect nearby landmarks, it leveraged a physical engine, which</p>
<p>depends on sophisticated domain knowledge and limits its usage to other general RL tasks. In the general RL context, our work shows that one can combine a high-level planner and a learned local model to solve RL problems more efficiently. Some recent work also utilize the graph structure to perform planning [27, 28], however, unlike our approach that discovers the graph structure in the process of achieving goals, both [27, 28] require supervised learning to build the graph. Specifically, [27] need to learn a Siamese network to judge if two states are connected, and [28] need to learn the state-attribute mapping from human annotation.</p>
<p>Our method is also related to hierarchical RL research [2, 29, 30]. The sampled landmark points can be considered as sub-goals. [2, 30] also used HER-like relabeling technique to make the training more efficient. These work attack more general RL problems without assuming much problem structure. Our work differs from previous work in how high-level policy is achieved. In their methods, the agent has to learn the high-level policy as another RL problem. In contrast, we exploit the structure of our universal goal reaching problem and find the high-level policy by solving a pairwise shortest path problem in a small-scale graph, thus more data-efficient.</p>
<h1>3 Background</h1>
<p>Universal Markov Decision Process (UMDP) extends an MDP with a set of goals $\mathcal{G}$. UMDP has reward function $\mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{G} \rightarrow \mathcal{R}$, where $\mathcal{S}$ is the state space and $\mathcal{A}$ is the action space. Every episode starts with a goal selected from $\mathcal{G}$ by the environment and is fixed for the whole episode. We aim to find a goal conditioned policy $\pi: \mathcal{S} \times \mathcal{G} \rightarrow \mathcal{A}$ to maximize the expected cumulative future return $V_{g, \pi}\left(s_{0}\right)=E_{\pi}\left[\sum_{t=0}^{\infty} \gamma^{t} R\left(s_{t}, a_{t}, g\right)\right]$, which called goal-conditioned value, or universal value. Universal Value Function Approximators (UVFA) [3] use neural network to model $V(s, g) \approx V_{g, \pi^{<em>}}(s)$ where $\pi^{</em>}$ is the optimal policy, and apply Bellman equation to train it in a bootstrapping way. Usually, the reward in UMDP is sparse to train the network. For a given goal, the agent can receive non-trivial rewards only when it can reach the goal. This brings a challenge to the learning process.
Hindsight Experience Replay (HER) [4] propose goal-relabeling to train UVFA in sparse reward setting. The key insight of HER is to "turn failure to success", i.e. to make a failed trajectory become success, by replacing the original failed goals with the goals it has achieved. This strategy gives more feedback to the agent and improves the data efficiency for sparse reward environments. Our framework relies on HER to train an accurate low-level policy.</p>
<h2>4 Universal Goal Reaching</h2>
<p>Problem Definition: Our universal goal reaching problem refers to a family of UMDP tasks. The state space of our UDMP is a low-dimension manifold in the ambient space. Many useful planning problems in practice are of this kind. Example universal goal reaching environments include labyrinth walking (e.g., AntMaze [31]) and robot arm control (e.g., FetchReach [32]). Their states can only transit in a neighborhood of low-dimensionality constrained by the degree of freedom of actions.</p>
<p>Following the notions in Sec 3, we assume that a goal $g$ in goal space $\mathcal{G}$ which is a subset of the state space $\mathcal{S}$. For example, in a labyrinth walking game with continuous locomotion, the goal can be to reach a specific location in the maze at any velocity. Then, if the state $s$ is a vector consisting of the location and velocity, a convenient way to represent the goal $g$ would be a vector that only contains the dimensions of location, i.e., the goal space is a projection of the state space.</p>
<p>The universal goal reaching problem has a specific transition probability and reward structure. At every time step, the agent moves into a local neighborhood based on the metric in the state space, which might be perturbed by random noise. It also receives some negative penalty (usually a constant, e.g., -1 in the experiments) unless it has arrived at the vicinity of the goal. A 0 reward is received if the goal is reached. To maximize the accumulated reward, the agent has to reach the goal in fewest steps. Usually the only non-trivial reward 0 appears rarely, and the universal goal reaching problem falls in the category of sparse reward environments, which are hard-exploration problems for RL.</p>
<p>A Graph View: Assume that a policy $\pi$ takes at most steps $T$ to move from $s$ to $g$ and the reward at each step $r_{k}$ 's absolute value is bounded by $R_{\text {max }}$. Let $w_{\pi}(s, t)$ be the expected total reward along</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of our framework. The agent is trying to reach the other side of the maze by planning on a landmark-based map. The landmarks are selected from its past experience, and the edges between the landmarks are formed by a UVFA.
the trajectory, and $d_{\pi}(s, t)=-w_{\pi}(s, t)$ for all $s, t$. We can prove ${ }^{3}$ :</p>
<p>$$
\begin{aligned}
\left|V_{\pi}(s, g)-w_{\pi}(s, g)\right| &amp; =\left|E\left[\sum_{k=1}^{T} r_{k}(1-\epsilon)^{k-1}\right]-E\left[\sum_{k=1}^{T} r_{k}\right]\right| \
&amp; \approx\left|E\left[\sum_{k=1}^{T} r_{k}-(k-1) \epsilon r_{k}\right]-E\left[\sum_{k=1}^{T} r_{k}\right]\right| \
&amp; =\left|\sum_{k=1}^{T}(k-1) \epsilon E\left[r_{k}\right]\right| \
&amp; \leq T^{2} R_{\max } \epsilon
\end{aligned}
$$</p>
<p>Thus, when $\gamma \approx 1$ and $T^{2} R_{\max }(1-\gamma) \xrightarrow{+} 0$, UVFA can be approximated as:</p>
<p>$$
V_{\pi}(s, g) \approx E\left[w_{\pi}(s, g)\right]=E\left[-d_{\pi}(s, g)\right]
$$</p>
<p>In this case, it is easy to show that the value iteration based on Bellman Equation $V_{\pi^{<em>}}(s, g)=$ $R(s, a, g)+\left.\gamma \mathbb{E}\left[V_{\pi^{</em>}}\left(s^{\prime}, g\right)\right]\right|<em>{s^{\prime} \sim \mathcal{P}</em>{\pi^{<em>}}(\cdot \mid s, a)}$ implies $w_{\pi^{</em>}}(s, g) \approx R(s, a, g)+\left.w_{\pi^{<em>}}\left(s^{\prime}, g\right)\right|<em>{s^{\prime} \sim \mathcal{P}</em>{\pi^{</em>}}(\cdot \mid s, a)}$, where $\mathcal{P}<em _pi="\pi">{\pi^{<em>}}$ is the transition probability of optimal policy $\pi^{</em>}$.
The relationship allows us to view the MDP as a directed graph, whose nodes are the state set $\mathcal{S}$, and edges are sampled according to the transition probability in the MDP. The general value iteration for RL problems is exactly the shortest path algorithm in terms of $d</em>(s, g)$ on this directed graph. Besides, because the nodes form a low-dimensional manifold, nodes that are far away in the state space can only be reached by a long path.
The MDP of our universal goal reaching problem is a large-scale directed graph whose nodes are in a low-dimensional manifold. This structure allows us to estimate the all-pair shortest paths accurately by a landmark based coarsening of the graph.</p>
<h1>5 Approach</h1>
<p>In this paper, we choose deep RL algorithms such as DQN and DDPG for discrete and continuous action space, respectively. UVFA [3] is a goal-conditioned extension of the original DQN, while HER (Sec 3), can produce more informative feedback for UVFA learning. Our algorithm is thus based upon HER, and the extension of this approach for DDPG is also straightforward.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>5.1 Basic Idea</h1>
<p>Our approach aims at addressing the fundamental challenges in UVFA learning. As characterized in the previous section, the UVFA estimation solves a pair-wise shortest path problem, and the underlying graph has a node space of high cardinality. Note that UVFA has to memorize the distance between every state-goal pairs, through trajectory samples from the starting state to the goal. For analysis purpose, we assume the state space has dimension $d$ and contains a ball of radius $R$. Then the lower-bound bound of the amount of the state-goal pairs is at the order of $\mathcal{O}\left(R^{2 d}\right)^{4}$, a high-order polynomial.
The large set of state-goal pairs poses the challenge. First, it takes longer time to sample enough state-goal pairs. Particularly, at the early stage, only few state-goal samples have been collected, so learning from them requires heavy extrapolation by networks, which is well known to be unreliable. Second, memorizing all the experiences is too difficult even for large networks.
We propose a map to abstract the visited state space by landmarks and edges to connect them. This abstraction is reasonable due to the underlying structure of our graph - a low-dimensional manifold [33]. We also learn local UVFA networks that only needs to be accurate in the neighborhood of landmarks. As illustrated in Figure 1, an ant robot is put in an "U" Maze to reach a given position. It should learn to model the maze as a small-scale map based on its past experiences.
This solution addresses the challenges. For the UVFA network, it only needs to remember experiences in a local neighborhood. Thus, the training procedure requires much lower sample complexity. The map decomposes a long path into piece-wise short ones, and each of which is from an accurate local network.</p>
<p>Our framework contains three components: a value function approximator trained with hindsight experience replay, a map that is supported by sampled landmarks, and a planner that can find the optimal path with the map. We will introduce them in Sec 5.2, Sec 5.3, and Sec 5.4, respectively.</p>
<h3>5.2 Learning a Local UVFA with HER</h3>
<p>Specifically, we define the following reward function for goal reaching problem:</p>
<p>$$
r_{t}=\mathcal{R}\left(s_{t}, a_{t}, g\right)= \begin{cases}0 &amp; \left|s_{t}^{\prime}-g\right| \leq \delta \ -1 &amp; \text { otherwise }\end{cases}
$$</p>
<p>Here $s_{t}^{\prime}$ is the next observation after taking action $a_{t}$. We first learn a UVFA based on HER, which has proven its efficiency for UVFA. HER smartly generates more feedback for the agent, by replacing some unachievable goals with those achieved in the near future. HER thus allows the agent to obtain denser rewards before it can eventually reach goals that are far away.
In experiments (see Sec 6.3), we find out that the agent trained with HER does master the skill to reach goals of increasing difficulty in a curriculum way. However, the agent can seldom reach the most difficult goals constantly, while the success rate of reaching easier goals remains stable. All these observations prove that HER's value and policy is locally reliable.
To increase the agent's ability to reach nearby goals and get a better local value estimation at the early stage, we change the replacement strategy in HER, ensuring that the replaced goals are sampled from the near future within a fixed number of steps.
The UVFA trained in this step will be used for two purposes: (1) to estimate the distance between two local states belonging to the same landmark, or between two nearby landmarks; and (2) to decide whether two states are close enough so that we can trust the distance estimation from the network. Although the learned UVFA is imperfect globally, it is enough for the two local usages.</p>
<h3>5.3 Building a Map by Sampling Landmarks</h3>
<p>After training the UVFA, we will obtain a distance estimation $d(s, g)^{3}$, a policy for any state-goal pair $(s, g)$, and a replay buffer that contains all the past experiences. We will build a landmark-based</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>map to abstract the state space based on the experiences. The pseudo-code for the algorithm is shown in Algorithm 10.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="nx">Planning</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">State</span><span class="o">-</span><span class="nx">space</span><span class="w"> </span><span class="nx">Mapping</span><span class="w"> </span><span class="p">(</span><span class="nx">Planner</span><span class="p">)</span>
<span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="nx">state</span><span class="w"> </span><span class="nx">obs</span><span class="p">,</span><span class="w"> </span><span class="nx">goal</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">g</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">UVFA</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">Q</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="nx">g</span><span class="p">,</span><span class="w"> </span><span class="nx">a</span><span class="p">)</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">clip_value</span>
<span class="nx">Output</span><span class="p">:</span><span class="w"> </span><span class="nx">Next</span><span class="w"> </span><span class="nx">subgoal</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">g_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">next</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">)</span>
<span class="mi">1</span><span class="w"> </span><span class="nx">Sample</span><span class="w"> </span><span class="nx">transitions</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">T</span><span class="p">=</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">s</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">replay</span><span class="w"> </span><span class="nx">buffer</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">B</span><span class="err">\</span><span class="p">)</span>
<span class="mi">2</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">V</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">F</span><span class="w"> </span><span class="nx">P</span><span class="w"> </span><span class="nx">S</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">S</span><span class="p">=</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">s</span><span class="w"> </span><span class="err">\</span><span class="o">|</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">s</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">T</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="err">\</span><span class="p">{</span><span class="nx">g</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="mi">3</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">W_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="nx">j</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">infty</span><span class="err">\</span><span class="p">)</span>
<span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Farthest</span><span class="w"> </span><span class="nx">point</span><span class="w"> </span><span class="nx">sampling</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">find</span><span class="w"> </span><span class="nx">landmarks</span>
<span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">Map</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">graph</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">G</span><span class="p">=</span><span class="err">\</span><span class="nx">langle</span><span class="w"> </span><span class="nx">V</span><span class="p">,</span><span class="w"> </span><span class="nx">W</span><span class="err">\</span><span class="nx">rangle</span><span class="err">\</span><span class="p">)</span>
<span class="mi">4</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="k">forall</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">v_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">V</span><span class="w"> </span><span class="err">\</span><span class="nx">times</span><span class="w"> </span><span class="nx">V</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="mi">5</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">quad</span><span class="w"> </span><span class="nx">w_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="nx">j</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">min</span><span class="w"> </span><span class="nx">_</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="o">-</span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">v_</span><span class="p">{</span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">a</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="mi">6</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">w_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="nx">j</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leq</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">clip_bound</span><span class="w"> </span><span class="k">then</span>
<span class="mi">7</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">quad</span><span class="w"> </span><span class="nx">W_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="nx">j</span><span class="p">}=</span><span class="nx">w_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="nx">j</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="mi">8</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Bellman_Ford</span><span class="p">(</span><span class="w"> </span><span class="nx">W</span><span class="p">)</span>
<span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Calculate</span><span class="w"> </span><span class="nx">pairwise</span><span class="w"> </span><span class="nx">distance</span>
<span class="err">\</span><span class="p">(</span><span class="nx">g_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">next</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">arg</span><span class="w"> </span><span class="err">\</span><span class="nx">min</span><span class="w"> </span><span class="nx">_</span><span class="p">{</span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="o">-</span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">o</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="o">+</span><span class="nx">D_</span><span class="p">{</span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">g</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="mi">10</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">g_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">next</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>Landmark Sampling The replay buffer stores visited states. Instead of localizing few important states that play a key role in connecting the environment, instead, we seek to sample many states to cover the visited state space.
Limited by computation budget, we first uniformly sample a big set of states from the replay buffer, and then use the farthest point sampling (FPS) algorithm [34] to select landmarks to support the explored state space. The metric for FPS can either be the Euclidean distance between the original state representation or the pairwise value estimated by the agent.
We compare different sampling strategies in Section 6.3, and demonstrate the advantage of FPS in abstracting the visited state space and exploration.</p>
<p>Connecting Nearby Landmarks We first connect landmarks that have a reliable distance estimation from the UVFA and assign the UVFA-estimated distance between them as the weight of the connecting edge.
Since UVFA is accurate locally but unreliable for long-term future, we choose to only connect nearby landmarks. The UVFA is able to return a distance between any pair $(s, g)$, so we connect the pairs with distance below a preset threshold $\tau$, which should ensure that all the edges are reliable, as well as the whole graph is connected.
With these two steps, we have built a directed weighted graph which can approximate the visited state space. This graph is our map to be used for high-level planning. Such map induces a new environment, where the action is to choose to move to another landmark. The details can be found in Algorithm 10.</p>
<h1>5.4 Planning with the Map</h1>
<p>We can now leverage the map and the local UVFA network to estimate the distance between any state-goal pairs, which induces a reliable policy for the agent to reach the goal.
For a given pair of $(s, g)$, we can plan the optimal path between $(s, g)$ by selecting a serial of landmarks $l_{0}, l_{1}, \cdots, l_{k}$, so that the approximated distance will be $\hat{d}(s, g)=\min <em 0="0">{l</em>, g\right)$. Here the summation of $\pi$ is the concatenation of the corresponding action sequence.
In our implementation, we run the shortest path algorithm to solve the above minimization problem. To speed up the pipeline, we first calculate the pairwise distances $d\left(l_{i}, g\right)$ between each landmark $l_{i}$ and the goal $g$ when episode starts. When the agent is at state $s$, we can choose the next subgoal by finding $g_{\text {next }}=\arg \min }, l_{1}, \cdots, l_{k}} d\left(s, l_{0}\right)+$ $\sum_{i=0}^{k} d\left(l_{i}, l_{i+1}\right)+d\left(l_{k}, g\right)$. The policy from $s$ to $g$ can then be approximated as: $\hat{\pi}(s, g)=\pi\left(s, l_{0}\right)+$ $\sum_{i=0}^{k} \pi\left(l_{i}, l_{i+1}\right)+\pi\left(l_{k<em i="i">{l</em>, g\right)$.}} d\left(s, l_{i}\right)+d\left(l_{i</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The results on FourRoom Environment. Figure 2a shows the sampled landmarks and the planned path based on our algorithm. Figure 2c, 2b, 2d are different evaluation metrics of value estimation and success rate to reach the goal.</p>
<h1>6 Experiments</h1>
<h3>6.1 FourRoom: An Illustrative Example</h3>
<p>We first demonstrate the merits of our method in the FourRoom environment, where the action space is discrete. The environment is visualized in Figure 2a. There are walls separating the space into four rooms, with narrow openings to connect them. For this discrete environment, we use DQN [35] with HER [4] to learn the Q value. Here, we use the one-hot representation of the x-y position as the input of the network. The initial states and the goals are randomly sampled during training.
We first get $V(s, g)$ from the learned Q-value by equation $V(s, g)=\arg \max <em g="g" t="t">{a} Q(s, a, g)$, and convert $V(s, g)$ to pairwise distance $D(s, g)$ based on Eq. 1. To evaluate the accuracy of distance estimation, we further calculate the ground truth distance $D</em>$.
Results are shown in Figure 2b. Our method has a much lower MDE at the very beginning stage, which means that the estimated value is more accurate.}(s, g)$ by running a shortest path algorithm on the underlying ground-truth graph of maze. Then we adapt the mean distortion error (MDE) as the evaluation metric: $\frac{\left|D(s, g)-D_{g t}(s, g)\right|}{D_{g t}(s, g)</p>
<p>To better evaluate our superiority for distant goals, we first convert predicted values to corresponding distances, and then plot the maximal distance during training. From Figure 2c, we can observe that the planning module have a larger output range than DQN. We guess that this comes from the max-operation in the Bellman-Ford equation, which pushes DQN to overestimate the Q value, or in other words, underestimate the distance for distant goals. However, the planner can still use piece-wise correct estimations to approximate the real distance to the goal.</p>
<p>We also compare our method with DQN on success reaching rate, and their performances are shown in Figure 2d.</p>
<h3>6.2 Continuous Control</h3>
<p>In this section, we will compare our method with HER on challenging classic control tasks and MuJoCo [36] goal-reaching environments.</p>
<h3>6.2.1 Environment Description</h3>
<p>2DReach A green point in a 2D U-maze aims to reach the goal represented by a red point, as shown in Figure 3a. The size of the maze is $15 \times 15$. The state space and the goal space are both in this 2D maze. At each step, the agent can move within $[-1,1] \times[-1,1]$ as $\delta_{x}, \delta_{y}$ in x and y directions.
2DPush The green point A now need to push a blue point B to a given goal (red point) lying in the same U-maze as 2DReach, as shown in Figure 3b. Once A has reached B, B will follow the movement of A. In this environment, the state is a 4 -dim vector that contains the location of both A and B.
BlockedFetchReach \&amp; FetchPush We need to control a gripper to either reach a location in 3d space or push an object in the table to a specific location, as shown in Figure 3c and Figure 3d. Since the original FetchReach implemented in OpenAI gym [37] is very easy to solve, we further add some blocks to increase the difficulty. We call this new environment BlockedFetchReach.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The environments we use for continuous control experiments.</p>
<p>PointMaze \&amp; AntMaze As shown in Figure 3e and Figure 3f, a point mass or an ant is put in a $12 \times 12$ U-maze. Both agents are trained to reach a random goal from a random location and tested under the most difficult setting to reach the other side of maze within 500 steps. The states of point and ant are 7 -dim and 30 -dim, including positions and velocities.
Complex AntMaze As shown in Figure 3g, an ant is put in a $56 \times 56$ complex maze. It is trained to reach a random goal from a random location and tested under the most difficult setting to reach the farthest goal (indicated as the red point) within 1500 steps.
Acrobot As shown in Figure 3h, an acrobot includes two joints and two links. Goals are states that the end-effector is above the black line at specific joint angles and velocities. The states and goals are both 6 -dim vectors including joint angles and velocities.</p>
<h1>6.2.2 Experiment Result</h1>
<p>The results compared with HER are shown in Figure 4. Our method trains UVFA with planner and HER. It is evaluated under the test setting, using the model and replay buffer at corresponding training steps.
In the 2DReach and 2DPush task (shown in Figure 4b), we can see our method achieves better performance. When incorporating with control tasks, for BlockedFetchReach and FetchPush environments, the results still show that our performance is better than HER, but the improvement is not so remarkable. We guess this comes from the strict time limit of the two environments, which is only 50 . We observe that pure HER can finally learn well, when the task horizon is not very long.
We expect that building maps would be more helpful for long-range goals, which is evidenced in the environments with longer episode length. Here we choose PointMaze and AntMaze with scale $12 \times 12$. For training, the agent is born at a random position to reach a random goal in the maze. For testing, the agent should reach the other side of the "U-Maze" within 500 steps. For these two environments, the performance of planning is significantly better and remains stable, while HER can hardly learn a reliable policy. Results are shown in Figure 4e and Figure 4f.
We also evaluate our method on classic control, and more complex navigation + locomotion task. Here we choose Complex Antmaze and Acrobot, and results are shown in Figure 4h and Figure 4g. The advantage over baseline demonstrates our method is applicable to complicated navigation tasks as well as general MDPs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">0.5 M</th>
<th style="text-align: center;">0.75 M</th>
<th style="text-align: center;">1 M</th>
<th style="text-align: center;">1.25 M</th>
<th style="text-align: center;">1.5 M</th>
<th style="text-align: center;">1.75 M</th>
<th style="text-align: center;">2 M</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ours Sparse</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">HIRO Sparse</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Ours Dense</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$\mathbf{0 . 0 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">HIRO Dense</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.8</td>
</tr>
</tbody>
</table>
<p>Table 1: Success Rate on Large AntMaze at different training steps.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Experiments on the continuous control environments. The red curve indicates the performance of our method at different training steps.</p>
<h1>6.2.3 Comparison with HRL</h1>
<p>We compare our method with HRL algorithms on large AntMaze (size $24 \times 24$ ), as shown in Table 1. We choose to compare with HIRO [30], which is the SOTA HRL algorithm on AntMaze, and HAC [2], which also uses the hindsight experience replay. We test these algorithms with the published codes ${ }^{67}$, under both sparse reward setting and dense reward setting.
On sparse reward setting, our algorithm can work well and reach the goal at the very early stage (Ours sparse in Table 1). In contrast, neither HAC nor HIRO are able to reach the goal in 2M steps. HIRO doesn't use HER to replace the unachievable goals, which makes such setting very challenging for the algorithm.</p>
<p>For dense reward setting, the map planner can obtain a high success rate at very early stage shown as Ours dense in Table 1. Compared with HIRO dense, we can see that a planner can reach distant goals sooner, since we don't need to train a high-level policy to propose subgoals for the low-level agent.</p>
<p>HAC introduced several complex hyper-parameters, and we couldn't make it work well for both settings.</p>
<h3>6.3 Ablation Study</h3>
<p>We study some key factors that affect our algorithm on AntMaze.
Choice of Clip Range and Landmarks There are two main hyper-parameters for the planner - the number of landmarks and the edge clipping threshold $\tau$. Figure 6a shows the evaluation result of the model trained after 0.8 M steps in AntMaze. We see that our method is generally robust under different choices of hyper-parameters. Here $\tau$ is the negative distance between landmarks. If it's too small, the landmarks will be isolated and can't form a connected graph. The same problem comes when the landmarks are not enough.</p>
<p>The Local Accuracy of HER We evaluate our model trained between $0 \sim 2.5 \mathrm{M}$ steps, for goals of different difficulties. We manually define the difficulty level of goals, as shown in Figure 5a. Goal's difficulty increases from Level 1 to Level 6. We plot the success rate as well as the average steps</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: AntMaze of multi-level difficulty. Figure 5b and Figure 5c is the average steps and success rate to reach different level of goals, respectively.
<img alt="img-5.jpeg" src="img-5.jpeg" />
(a) Hyperparameters of the planner
<img alt="img-6.jpeg" src="img-6.jpeg" />
(b) FPS vs. Uniform Sampling
<img alt="img-7.jpeg" src="img-7.jpeg" />
(c) Landmark-based Map</p>
<p>Figure 6: Figure 6a shows the relationship with the landmarks and clip range in the planner. Figure 6b shows FPS outperforms uniform sampling. And Figure 6c is the landmark-based map at different training steps constructed by FPS.
to reach these goals. We find out that, for the easier goals, the agent takes less time and less steps to master the skill. The success rate and average steps also remain more stable during the training process, indicating that our base model is more reliable and stable in the local area.</p>
<p>Landmark Sampling Strategy Comparison Our landmarks are dynamically sampled from the replay buffer by iterative FPS algorithm using distances estimated by UVFA, and get updated at the beginning of every episode. The FPS sampling tends to find states at the boundary of the visited space, which implicitly helps exploration. We test FPS and uniform sampling in fix-start AntMaze (The ant is born at a fixed position to reach the other side of maze for both training and testing). Figure 6b shows that FPS has much higher success rate than uniform sampling. Figure 6c shows landmark-based graph at four training stages. Through FPS, landmarks expand gradually towards the goal (red dot), even if it only covers a small proportion of states at the beginning.</p>
<h1>7 Conclusion</h1>
<p>Learning a structured model and combining it with RL algorithms are important for reasoning and planning over long horizons. We propose a sample-based method to dynamically map the visited state space and demonstrate its empirical advantage in routing and exploration in several challenging RL tasks. Experimentally we showed that this approach can solve long-range goal reaching problems better than model-free methods and hierarchical RL methods, for a number of challenging games, even if the goal-conditioned model is only locally accurate. However, our method also has limitations. First, we empirically observe that some parameters, particularly the threshold to check whether we have reached the vicinity of a goal, needs hand-tuning. Secondly, a good state embedding is still important for the learning efficiency of our approach, since we do not include heavy component of learning state embedding. Thirdly, we find that in some environments whose intrinsic dimension is very high, especially when the topological structure is hard to abstract, sample-based method is not enough to represent the visited state space. And for those environments which is hard to obtain a reliable and generalizable local policy, this approach will also suffer from the accumulated error.</p>
<h1>References</h1>
<p>[1] Andrew William Moore. Efficient memory-based learning for robot control. Technical report, 1990.
[2] Andrew Levy, Robert Platt, and Kate Saenko. Hierarchical reinforcement learning with hindsight. arXiv preprint arXiv:1805.08180, 2018.
[3] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International conference on machine learning, pages 1312-1320, 2015.
[4] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pages 5048-5058, 2017.
[5] Francesco Bullo and Andrew D. Lewis. Geometric Control of Mechanical Systems, volume 49 of Texts in Applied Mathematics. Springer Verlag, New York-Heidelberg-Berlin, 2004.
[6] Vin De Silva and Joshua B Tenenbaum. Sparse multidimensional scaling using landmark points. Technical report, 2004.
[7] Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White, and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2, pages 761-768. International Foundation for Autonomous Agents and Multiagent Systems, 2011.
[8] Jiayuan Mao, Honghua Dong, and Joseph J Lim. Universal agent for disentangling environments and tasks. 2018.
[9] Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal difference models: Model-free deep RL for model-based control. CoRR, abs/1802.09081, 2018.
[10] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
[11] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.
[12] Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In NIPS, 2017.
[13] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484-489, Jan 2016.
[14] Mikael Henaff, William F Whitney, and Yann LeCun. Model-based planning with discrete and continuous actions. arXiv preprint arXiv:1705.07177, 2017.
[15] Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal planning networks. CoRR, abs/1804.00645, 2018.
[16] Tianhe Yu, Gleb Shevchuk, Dorsa Sadigh, and Chelsea Finn. Unsupervised visuomotor control through distributional planning networks. CoRR, abs/1902.05542, 2019.
[17] Nan Rosemary Ke, Amanpreet Singh, Ahmed Touati, Anirudh Goyal, Yoshua Bengio, Devi Parikh, and Dhruv Batra. Modeling the long term future in model-based reinforcement learning. 2018.
[18] Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks. In Advances in Neural Information Processing Systems, pages 2154-2162, 2016.
[19] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
[20] Peter E Hart, Nils J Nilsson, and Bertram Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100-107, 1968.</p>
<p>[21] Steven M LaValle. Rapidly-exploring random trees: A new tool for path planning. 1998.
[22] Lydia Kavraki, Petr Svestka, and Mark H Overmars. Probabilistic roadmaps for path planning in high-dimensional configuration spaces, volume 1994. Unknown Publisher, 1994.
[23] Brian Ichter and Marco Pavone. Robot motion planning in learned latent spaces. CoRR, abs/1807.10366, 2018.
[24] Ahmed H. Qureshi and Michael C. Yip. Deeply informed neural sampling for robot motion planning. CoRR, abs/1809.10252, 2018.
[25] Tobias Klamt and Sven Behnke. Towards learning abstract representations for locomotion planning in high-dimensional state spaces. arXiv preprint arXiv:1903.02308, 2019.
[26] Aleksandra Faust, Kenneth Oslund, Oscar Ramirez, Anthony Francis, Lydia Tapia, Marek Fiser, and James Davidson. Prm-rl: Long-range robotic navigation tasks by combining reinforcement learning and sampling-based planning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 5113-5120. IEEE, 2018.
[27] Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory for navigation. arXiv preprint arXiv:1803.00653, 2018.
[28] Amy Zhang, Adam Lerer, Sainbayar Sukhbaatar, Rob Fergus, and Arthur Szlam. Composable planning with attributes. arXiv preprint arXiv:1803.00512, 2018.
[29] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in neural information processing systems, pages 3675-3683, 2016.
[30] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning. In Advances in Neural Information Processing Systems, pages 33033313, 2018.
[31] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. CoRR, abs/1604.06778, 2016.
[32] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech Zaremba. Multi-goal reinforcement learning: Challenging robotics environments and request for research. CoRR, abs/1802.09464, 2018.
[33] Andrew V Goldberg and Chris Harrelson. Computing the shortest path: A search meets graph theory. In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms, pages 156-165. Society for Industrial and Applied Mathematics, 2005.
[34] David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027-1035. Society for Industrial and Applied Mathematics, 2007.
[35] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013.
[36] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033. IEEE, 2012.
[37] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ HIRO: https://github.com/tensorflow/models/tree/master/research/efficient-hrl
${ }^{7}$ HAC:https://github.com/andrew-j-levy/Hierarchical-Actor-Critc-HAC-&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2568 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2568</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2568</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-c0b6149a7ff72817d5cdb008566dc4f0ca9b9379</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c0b6149a7ff72817d5cdb008566dc4f0ca9b9379" target="_blank">MASAI: Modular Architecture for Software-engineering AI Agents</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A Modular Architecture for Software-engineering AI (MASAI) agents is proposed, where different LLM-powered sub-agents are instantiated with well-defined objectives and strategies tuned to achieve those objectives.</p>
                <p><strong>Paper Abstract:</strong> A common method to solve complex problems in software engineering, is to divide the problem into multiple sub-problems. Inspired by this, we propose a Modular Architecture for Software-engineering AI (MASAI) agents, where different LLM-powered sub-agents are instantiated with well-defined objectives and strategies tuned to achieve those objectives. Our modular architecture offers several advantages: (1) employing and tuning different problem-solving strategies across sub-agents, (2) enabling sub-agents to gather information from different sources scattered throughout a repository, and (3) avoiding unnecessarily long trajectories which inflate costs and add extraneous context. MASAI enabled us to achieve the highest performance (28.33% resolution rate) on the popular and highly challenging SWE-bench Lite dataset consisting of 300 GitHub issues from 11 Python repositories. We conduct a comprehensive evaluation of MASAI relative to other agentic methods and analyze the effects of our design decisions and their contribution to the success of MASAI.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2568.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2568.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MASAI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modular Architecture for Software-engineering AI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular multi-sub-agent architecture that decomposes repository-level software-engineering tasks into five specialized LLM-powered sub-agents (test template generation, issue reproduction, localization, patch generation, and ranking), coordinated through sequential handoffs and environment-aware ReAct/CoT strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MASAI</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MASAI is a modular multi-agent system for resolving repository-level software engineering issues. It composes five LLM-powered sub-agents with well-defined input/strategy/output specifications and a shared action interface to interact with the repository execution environment. Sub-agents produce structured artifacts (tests, edit locations, patch diffs, rankings) that are passed as inputs to downstream sub-agents; some sub-agents internally use ReAct loops to interleave LLM action generation and environment observations, while others use Chain-of-Thought to produce multiple candidate solutions for downstream selection.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Test Template Generator (discovers how to write/run a repository-specific template test and command); Issue Reproducer (writes issue-specific tests that reproduce observed behavior); Edit Localizer (navigates repository to identify files/classes/functions to edit); Fixer (generates multiple candidate patches for marked code locations); Ranker (executes reproduction tests on patched repositories and ranks candidate patches).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>implementation, execution, evaluation (test generation, debugging/repair, test-based verification); also limited information-gathering/documentation review (through READ actions).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Sequential pipeline / centralized orchestration: a controller composes the sub-agents and passes outputs from one sub-agent to the next (no pairwise conversational protocols between sub-agents). Internally, some sub-agents run ReAct loops (action → environment → observation) to perform multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Structured artifact handoffs and environment actions: sub-agents exchange code artifacts, test templates, shell commands, lists of EDIT/ADD markers, and ranked patch lists; internal LLM prompts contain natural-language instructions plus structured code blocks (pre/post snippets with line numbers) for edits. Environment interaction occurs via a defined action set (READ, EDIT, ADD, WRITE, LIST, COMMAND, DONE) whose observations feed back into ReAct steps.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Environment-executed feedback and test-driven validation: sub-agents observe execution outputs from COMMAND and READ actions; Test Template Generator validates template by running commands until a non-exceptional template is found; Ranker runs reproduction tests on each candidate patch to observe pass/fail and uses those results as evidence in CoT ranking. Fixer uses syntactic validation (rejects syntactically incorrect edits) and fuzzy matching to ensure application.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Two-tier: within sub-agents (on-demand after each action in ReAct loops — up to configured step limits), and between sub-agents only at phase transitions (each sub-agent completes and returns structured outputs consumed by the next).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software engineering (repository-level bug fixing, test generation, debugging, patch synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Resolution rate: 28.33% (percentage of issues in SWE-bench Lite resolved). Localization rate: 75.00% (file-level localization recall). Application rate: 95.33% (patches successfully apply). Average cost per issue: $1.96. Additional sampling ablation: with 5 Fixer samples and Ranker using reproduction tests, resolution 28.33%; Oracle selection at 5 samples: 35.00%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against multiple single-loop or alternative agentic systems on SWE-bench Lite (e.g., SWE-agent, AutoCodeRover, OpenDevin, Aider, CodeR); MASAI achieved state-of-the-art resolution rate (tied with CodeR at 28.33%), higher localization than some baselines and higher application rate than most (95.33%). Table 3 shows MASAI resolving more jointly-localized issues than most competing methods (e.g., MASAI resolved 73 out of 166 jointly-localized vs ACR's 51).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Modularity and sequential coordination enabled specialization and better performance: (1) improved localization via a dedicated ReAct-based Edit Localizer (75% localization vs 61%/63% for SWE-agent/OpenDevin); (2) diverse sampling + test-driven ranking increases successful repairs (sampling 5 Fixer candidates with Ranker using generated tests resulted in 28.33% resolution vs 23.33% when Ranker lacks test outputs); (3) avoids long single-agent trajectories (reduces extraneous context and cost). Quantitatively: top resolution rate 28.33% on SWE-bench Lite; 95.33% patch application rate.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Limitations and challenges reported: dependency on execution environment setup (many methods require pre-installed repo dependencies); reliance on a single LLM across sub-agents (authors used GPT-4o for all sub-agents and did not explore mixed-model configurations); evaluation limited to test-validated issues in SWE-bench Lite; possible failure modes when Issue Reproducer cannot produce a reproduction test; cost and compute constraints. The paper does not report major inter-agent messaging conflicts but notes the architectural simplicity (no explicit multi-party dialogues) as both design choice and limitation for more complex negotiation patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Yes — sampling and ranking ablations: Table 2 compares 1 vs 5 Fixer samples and selection strategies: Oracle selection improves resolution from 23.33% (1 sample) to 35.00% (5 samples). LLM-without-test selection (no reproduction test) yields 23.33% (5 samples) while LLM-with-test (Ranker uses reproduction tests) yields 28.33% (5 samples). Additional comparisons control for localization (Table 3) to isolate repair effectiveness versus other methods. The authors also compare ReAct-based Edit Localizer vs single-step CoT approaches (Aider), showing better localization and ability to handle more complex localization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Empirically recommended configuration used in experiments: use ReAct for Test Template Generator, Issue Reproducer, Edit Localizer; Chain-of-Thought for Fixer and Ranker; Fixer sampling: 5 candidates at temperature 0.5; other sub-agents at temperature 0; ReAct loops limited to 25 steps (Test Template Generator retried up to 3 times, with initial temperature 0 ramped by +0.2 on retries). Use fuzzy-matching pre/post edit representation with line-numbered snippets to maximize patch application rate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MASAI: Modular Architecture for Software-engineering AI Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2568.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2568.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A competing multi-agent system that uses separate agents for reproducing issues, localizing faults, and iteratively editing code; uses BM25 and test-coverage statistics for fault localization and is evaluated on SWE-bench Lite.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CodeR: Issue resolving with multi-agent and task graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeR</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CodeR is described as a multi-agent solution which decomposes issue resolution into separate agents (reproducer, localizer, editor) and combines retrieval/coverage signals (BM25, coverage) to guide localization and iterative edits. The paper references CodeR as a competing method evaluated on the same benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (separate agents for reproduction, localization, and iterative editing — 3+ implied)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Issue reproduction agent; Fault localization agent (uses BM25 + coverage statistics); Iterative edit/repair agent(s) that perform repeated edits and testing.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>implementation, execution, evaluation (reproduction, localization, iterative repair/testing).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Pipeline of specialized agents and iterative refinement; uses retrieval and coverage signals to coordinate localization and editing (paper mentions multi-agent and task graphs in CodeR title).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not detailed in MASAI paper beyond high-level description: agents exchange localization outputs and edits; relies on retrieval ranks and coverage statistics as inputs to agents.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Iterative repair: the edit agent refines patches based on test/coverage feedback; coverage stats used to prioritize localization. (Details derived from the summarized description in this paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>On-demand/iterative (iterative edit cycles until repair or resource limits).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software engineering (issue resolution/bug fixing).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in MASAI's Table 1: Resolution rate 28.33% (tied with MASAI), Localization rate 66.67%, Application rate 74.00% on SWE-bench Lite.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared in MASAI's evaluation tables (Table 1 and Table 3) as a strong baseline; CodeR ties for highest resolution at 28.33% but has lower application and localization rates compared to MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Leverages retrieval and coverage-based localization to guide edits; in its evaluation it achieves top-tier resolution performance (28.33%) indicating effective coordination for many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Authors note CodeR depends on coverage instrumentation and repository-specific commands for localization, which may require external setup and reduce autonomy; lower patch application rate (74%) indicates challenges in producing patches that apply cleanly.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided in MASAI paper for CodeR specifically (only comparative aggregated metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in MASAI; CodeR's internal optimal settings are not detailed in the text beyond general description (use of BM25 and coverage signals).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MASAI: Modular Architecture for Software-engineering AI Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2568.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2568.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoGen</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent conversation framework that provides abstractions for agent interactions and facilitates building next-generation LLM applications using conversational multi-agent protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autogen: Enabling next-gen LLM applications via multi-agent conversation framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoGen</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AutoGen is presented as a framework for multi-agent conversation, offering abstractions for agent interactions and conversational programming to design multi-agent systems. The MASAI paper references AutoGen in related work as an example of multi-agent frameworks focusing on conversation protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (framework supports creating multiple conversational agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Framework-level: supports role-based conversational agents — exact specializations depend on instantiation (not detailed in MASAI paper).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>general application development and multi-agent coordination (paper mentions focus on high-level agent design rather than domain-specific research phases).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Conversation-driven agent interactions (dialogue-based protocols); AutoGen provides abstractions for orchestrating such conversational interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language message passing (conversational transcripts) mediated by the framework's APIs/abstractions; exact structured formats not described in MASAI paper.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Conversational feedback and role-based messaging; details not provided in MASAI paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Conversation-driven (continuous/message-driven during task execution).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General LLM multi-agent applications (framework-level; examples include but are not limited to software-engineering tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No experimental metrics reported in MASAI for AutoGen (it is only cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared empirically in MASAI; referenced as a design/abstraction framework rather than a SWE-bench competitor.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Provides high-level abstractions to simplify building multi-agent conversational applications and explore emergent behaviors; MASAI contrasts itself to such frameworks by favoring a simpler sequential modular composition.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>MASAI notes that many such conversational frameworks focus on high-level design and are typically instantiated on simpler datasets; applicability to complex repository-level tasks is not demonstrated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None in MASAI (AutoGen is only mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MASAI: Modular Architecture for Software-engineering AI Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2568.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2568.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentVerse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AgentVerse</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A platform for facilitating multi-agent collaboration and exploring emergent behaviors in agent populations, providing abstractions for multi-agent system construction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AgentVerse</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AgentVerse is described as a multi-agent platform offering tools/abstractions for building and observing collaborative agent systems and emergent behaviors; MASAI cites it in related work as an example of dialogue-driven multi-agent frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (platform supports arbitrary numbers of agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Platform-level: supports heterogeneous agent roles defined by the user; MASAI does not detail specific roles implemented in AgentVerse.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>general multi-agent application development; not specifically targeted at scientific research in MASAI's description.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Conversation/dialogue-based coordination between agents (framework provides communication abstractions).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language message passing via framework APIs; specifics not detailed in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Emergent behavior observation and message exchange; no details provided in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Continuous/message-driven (framework-level).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General multi-agent applications (research and development of collaborative agents).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in MASAI (only cited).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not empirically compared in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Framework designed to enable experiments in multi-agent collaboration and emergent phenomena; MASAI contrasts with such frameworks by using a simpler sequential modular composition tailored to repository tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>MASAI suggests these frameworks typically focus on high-level conversation protocols and are often evaluated on simpler or different benchmarks than repository-level engineering tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MASAI: Modular Architecture for Software-engineering AI Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2568.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2568.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dialogue-based cooperative agent framework for multi-agent collaborative programming and project decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MetaGPT: Meta programming for Multi-Agent Collaborative Framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MetaGPT is a dialogue-based framework that orchestrates cooperative agents for software development tasks by decomposing projects into roles and dialogues; MASAI cites it as part of related work on conversational multi-agent frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (framework decomposes tasks into multiple role-based agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Role-based agents (e.g., planner, coder, tester) depending on instantiated project — MASAI does not enumerate MetaGPT's roles in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>planning, implementation, testing (depending on task instantiation).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Dialog-based hierarchical cooperation and role assignment (conversation-driven orchestration).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language dialogues structured by the framework; specifics not detailed in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Iterative conversational refinement and role coordination; not detailed in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Continuous (dialogue exchanges during task progression).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software development (project decomposition and collaborative coding).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in MASAI (only cited).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not empirically compared in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Enables decomposition of development tasks into specialized conversational roles; MASAI chooses a different modular handoff-style approach for repository-level repairs.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Paper notes such conversational frameworks are typically evaluated on smaller generation tasks and may not directly scale to large repository-level engineering tasks without adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MASAI: Modular Architecture for Software-engineering AI Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2568.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2568.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatDev / Communicative agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Communicative agents for software development</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work on communicative/multi-agent approaches for software development where agents exchange messages to coordinate development tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Communicative agents for software development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatDev / Communicative agents</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described in MASAI as a dialogue-based collaborative agent framework for software development; cited among works exploring conversational multi-agent coordination for coding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (multiple communicative agents coordinating via dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Conversational role agents (e.g., design, implementation, review) depending on instantiation; MASAI does not provide specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>planning, implementation, review/testing (depending on usage).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Dialogue-driven coordination and negotiation between agents.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language messages; framework-mediated conversation transcripts.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Conversational critique and iterative refinement; not detailed in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Continuous/message-driven.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software development tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not provided in MASAI (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not empirically compared in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Enables cooperative decomposition and role-based interactions; MASAI contrasts this with its simpler sequential, artifact-handoff approach.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Authors note dialogue-based multi-agent frameworks tend to be evaluated on smaller-scope tasks; scaling to full repository-level debugging poses challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MASAI: Modular Architecture for Software-engineering AI Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2568.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2568.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic LLM-agent network</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic LLM-agent network: An LLM-agent collaboration framework with agent team optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework for inference-time agent selection and agent-team optimization that dynamically composes and optimizes agent teams for tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynamic LLM-agent network: An LLMagent collaboration framework with agent team optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Dynamic LLM-agent network</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>This framework focuses on dynamically selecting and optimizing agent teams at inference time to improve multi-agent collaboration efficiency; MASAI cites it when discussing agent selection and team optimization literature.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (dynamic team sizes based on optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Framework-level support for heterogeneous agents; exact role specializations depend on task instantiation.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>agent selection and coordination phases; applicable across planning and execution depending on deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Dynamic team selection and optimization (inference-time orchestration).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not specified in MASAI beyond general framework-level optimizations; likely message passing/dialogue or structured APIs as per original work.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Agent-team performance optimization using inference-time signals; specifics not detailed in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Dynamic/on-demand depending on optimization decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General multi-agent tasks; cited in context of LLM-agent collaboration research.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in MASAI (only cited).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not empirically compared in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Offers methods to choose efficient agent teams for tasks, which could reduce overhead and improve performance if applied to repository-level tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed in detail in MASAI; applicability to complex software-engineering tasks is not demonstrated here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MASAI: Modular Architecture for Software-engineering AI Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2568.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2568.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoDev</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoDev: Automated AI-Driven Development</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI-driven development system that can execute file editing, retrieval, and testing actions; cited as a related system with action capabilities but evaluated on NL2Code-style benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AutoDev: Automated AI-Driven Development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoDev</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AutoDev is referenced as a system that can execute actions like file editing, retrieval and testing using agents, but MASAI notes AutoDev is typically evaluated on NL2Code datasets (HumanEval) rather than full repository-level issue resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (framework supports agents capable of file edits and testing)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Agents that perform file editing, retrieval, and test execution; specifics not detailed in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>implementation and execution in NL2Code settings; not validated at repository-scale in MASAI's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Action-capable agents with possibly centralized orchestration in its original work (not detailed in MASAI).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Action invocation and environment observations (not detailed in MASAI).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Test execution and environment feedback for iterative refinement (as described in general terms).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Action-driven (on-demand during code generation/testing cycles).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Code generation and small-scale software development (NL2Code benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in MASAI (cited only).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not empirically compared in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Shows how action-capable agents can be used for code editing and testing in NL2Code contexts; MASAI extends similar action primitives to full repository tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>MASAI notes that AutoDev and similar frameworks are evaluated on simpler benchmarks and may not directly address the complexities of repository-level debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MASAI: Modular Architecture for Software-engineering AI Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2568.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2568.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SwiftSage</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SwiftSage</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A divide-and-conquer multi-component agent that combines a fast policy (Swift) and a deliberative LLM (Sage) to decide actions and sub-goal planning, inspired by dual-process cognition and applied to complex interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SwiftSage</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SwiftSage is described in MASAI's related work as an agent combining a finetuned small policy model for fast actions and an LLM for deliberate planning and backtracking, illustrating divide-and-conquer agent design for complex tasks (e.g., closed-world scientific experiments in prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>2 (conceptually: fast-policy agent 'Swift' + deliberative LLM 'Sage')</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Swift: fast, action-oriented small model for quick decisions; Sage: LLM for planning, deliberate sub-goal decomposition and backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>planning and execution (and potentially iterative experimentation in scientific/interactive tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Hybrid two-system coordination (fast/slow) where the fast policy executes actions and the slow LLM plans and backtracks as needed.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Internal API/message passing between policy and LLM; MASAI does not detail message formats beyond the conceptual split.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Backtracking and deliberation driven by planned sub-goals and environment outcomes (as described in related work summary).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Frequent between fast and slow components during task execution (conceptual).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Complex interactive tasks; original motivations include scientific-experiment-like closed-world tasks (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in MASAI (only cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Illustrates benefits of separating fast execution from slow deliberative planning for complex tasks; cited to motivate divide-and-conquer architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed in MASAI beyond high-level description; practical integration details and scaling to repository-level tasks are not covered here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in MASAI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MASAI: Modular Architecture for Software-engineering AI Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autogen: Enabling next-gen LLM applications via multi-agent conversation framework <em>(Rating: 2)</em></li>
                <li>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents <em>(Rating: 2)</em></li>
                <li>CodeR: Issue resolving with multi-agent and task graphs <em>(Rating: 2)</em></li>
                <li>MetaGPT: Meta programming for Multi-Agent Collaborative Framework <em>(Rating: 2)</em></li>
                <li>Communicative agents for software development <em>(Rating: 2)</em></li>
                <li>Dynamic LLM-agent network: An LLMagent collaboration framework with agent team optimization <em>(Rating: 2)</em></li>
                <li>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks <em>(Rating: 1)</em></li>
                <li>AutoDev: Automated AI-Driven Development <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2568",
    "paper_id": "paper-c0b6149a7ff72817d5cdb008566dc4f0ca9b9379",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "MASAI",
            "name_full": "Modular Architecture for Software-engineering AI",
            "brief_description": "A modular multi-sub-agent architecture that decomposes repository-level software-engineering tasks into five specialized LLM-powered sub-agents (test template generation, issue reproduction, localization, patch generation, and ranking), coordinated through sequential handoffs and environment-aware ReAct/CoT strategies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MASAI",
            "system_description": "MASAI is a modular multi-agent system for resolving repository-level software engineering issues. It composes five LLM-powered sub-agents with well-defined input/strategy/output specifications and a shared action interface to interact with the repository execution environment. Sub-agents produce structured artifacts (tests, edit locations, patch diffs, rankings) that are passed as inputs to downstream sub-agents; some sub-agents internally use ReAct loops to interleave LLM action generation and environment observations, while others use Chain-of-Thought to produce multiple candidate solutions for downstream selection.",
            "number_of_agents": "5",
            "agent_specializations": "Test Template Generator (discovers how to write/run a repository-specific template test and command); Issue Reproducer (writes issue-specific tests that reproduce observed behavior); Edit Localizer (navigates repository to identify files/classes/functions to edit); Fixer (generates multiple candidate patches for marked code locations); Ranker (executes reproduction tests on patched repositories and ranks candidate patches).",
            "research_phases_covered": "implementation, execution, evaluation (test generation, debugging/repair, test-based verification); also limited information-gathering/documentation review (through READ actions).",
            "coordination_mechanism": "Sequential pipeline / centralized orchestration: a controller composes the sub-agents and passes outputs from one sub-agent to the next (no pairwise conversational protocols between sub-agents). Internally, some sub-agents run ReAct loops (action → environment → observation) to perform multi-step reasoning.",
            "communication_protocol": "Structured artifact handoffs and environment actions: sub-agents exchange code artifacts, test templates, shell commands, lists of EDIT/ADD markers, and ranked patch lists; internal LLM prompts contain natural-language instructions plus structured code blocks (pre/post snippets with line numbers) for edits. Environment interaction occurs via a defined action set (READ, EDIT, ADD, WRITE, LIST, COMMAND, DONE) whose observations feed back into ReAct steps.",
            "feedback_mechanism": "Environment-executed feedback and test-driven validation: sub-agents observe execution outputs from COMMAND and READ actions; Test Template Generator validates template by running commands until a non-exceptional template is found; Ranker runs reproduction tests on each candidate patch to observe pass/fail and uses those results as evidence in CoT ranking. Fixer uses syntactic validation (rejects syntactically incorrect edits) and fuzzy matching to ensure application.",
            "communication_frequency": "Two-tier: within sub-agents (on-demand after each action in ReAct loops — up to configured step limits), and between sub-agents only at phase transitions (each sub-agent completes and returns structured outputs consumed by the next).",
            "task_domain": "Software engineering (repository-level bug fixing, test generation, debugging, patch synthesis).",
            "performance_metrics": "Resolution rate: 28.33% (percentage of issues in SWE-bench Lite resolved). Localization rate: 75.00% (file-level localization recall). Application rate: 95.33% (patches successfully apply). Average cost per issue: $1.96. Additional sampling ablation: with 5 Fixer samples and Ranker using reproduction tests, resolution 28.33%; Oracle selection at 5 samples: 35.00%.",
            "baseline_comparison": "Compared against multiple single-loop or alternative agentic systems on SWE-bench Lite (e.g., SWE-agent, AutoCodeRover, OpenDevin, Aider, CodeR); MASAI achieved state-of-the-art resolution rate (tied with CodeR at 28.33%), higher localization than some baselines and higher application rate than most (95.33%). Table 3 shows MASAI resolving more jointly-localized issues than most competing methods (e.g., MASAI resolved 73 out of 166 jointly-localized vs ACR's 51).",
            "coordination_benefits": "Modularity and sequential coordination enabled specialization and better performance: (1) improved localization via a dedicated ReAct-based Edit Localizer (75% localization vs 61%/63% for SWE-agent/OpenDevin); (2) diverse sampling + test-driven ranking increases successful repairs (sampling 5 Fixer candidates with Ranker using generated tests resulted in 28.33% resolution vs 23.33% when Ranker lacks test outputs); (3) avoids long single-agent trajectories (reduces extraneous context and cost). Quantitatively: top resolution rate 28.33% on SWE-bench Lite; 95.33% patch application rate.",
            "coordination_challenges": "Limitations and challenges reported: dependency on execution environment setup (many methods require pre-installed repo dependencies); reliance on a single LLM across sub-agents (authors used GPT-4o for all sub-agents and did not explore mixed-model configurations); evaluation limited to test-validated issues in SWE-bench Lite; possible failure modes when Issue Reproducer cannot produce a reproduction test; cost and compute constraints. The paper does not report major inter-agent messaging conflicts but notes the architectural simplicity (no explicit multi-party dialogues) as both design choice and limitation for more complex negotiation patterns.",
            "ablation_studies": "Yes — sampling and ranking ablations: Table 2 compares 1 vs 5 Fixer samples and selection strategies: Oracle selection improves resolution from 23.33% (1 sample) to 35.00% (5 samples). LLM-without-test selection (no reproduction test) yields 23.33% (5 samples) while LLM-with-test (Ranker uses reproduction tests) yields 28.33% (5 samples). Additional comparisons control for localization (Table 3) to isolate repair effectiveness versus other methods. The authors also compare ReAct-based Edit Localizer vs single-step CoT approaches (Aider), showing better localization and ability to handle more complex localization tasks.",
            "optimal_configurations": "Empirically recommended configuration used in experiments: use ReAct for Test Template Generator, Issue Reproducer, Edit Localizer; Chain-of-Thought for Fixer and Ranker; Fixer sampling: 5 candidates at temperature 0.5; other sub-agents at temperature 0; ReAct loops limited to 25 steps (Test Template Generator retried up to 3 times, with initial temperature 0 ramped by +0.2 on retries). Use fuzzy-matching pre/post edit representation with line-numbered snippets to maximize patch application rate.",
            "uuid": "e2568.0",
            "source_info": {
                "paper_title": "MASAI: Modular Architecture for Software-engineering AI Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CodeR",
            "name_full": "CodeR",
            "brief_description": "A competing multi-agent system that uses separate agents for reproducing issues, localizing faults, and iteratively editing code; uses BM25 and test-coverage statistics for fault localization and is evaluated on SWE-bench Lite.",
            "citation_title": "CodeR: Issue resolving with multi-agent and task graphs",
            "mention_or_use": "mention",
            "system_name": "CodeR",
            "system_description": "CodeR is described as a multi-agent solution which decomposes issue resolution into separate agents (reproducer, localizer, editor) and combines retrieval/coverage signals (BM25, coverage) to guide localization and iterative edits. The paper references CodeR as a competing method evaluated on the same benchmark.",
            "number_of_agents": "variable (separate agents for reproduction, localization, and iterative editing — 3+ implied)",
            "agent_specializations": "Issue reproduction agent; Fault localization agent (uses BM25 + coverage statistics); Iterative edit/repair agent(s) that perform repeated edits and testing.",
            "research_phases_covered": "implementation, execution, evaluation (reproduction, localization, iterative repair/testing).",
            "coordination_mechanism": "Pipeline of specialized agents and iterative refinement; uses retrieval and coverage signals to coordinate localization and editing (paper mentions multi-agent and task graphs in CodeR title).",
            "communication_protocol": "Not detailed in MASAI paper beyond high-level description: agents exchange localization outputs and edits; relies on retrieval ranks and coverage statistics as inputs to agents.",
            "feedback_mechanism": "Iterative repair: the edit agent refines patches based on test/coverage feedback; coverage stats used to prioritize localization. (Details derived from the summarized description in this paper.)",
            "communication_frequency": "On-demand/iterative (iterative edit cycles until repair or resource limits).",
            "task_domain": "Software engineering (issue resolution/bug fixing).",
            "performance_metrics": "Reported in MASAI's Table 1: Resolution rate 28.33% (tied with MASAI), Localization rate 66.67%, Application rate 74.00% on SWE-bench Lite.",
            "baseline_comparison": "Compared in MASAI's evaluation tables (Table 1 and Table 3) as a strong baseline; CodeR ties for highest resolution at 28.33% but has lower application and localization rates compared to MASAI.",
            "coordination_benefits": "Leverages retrieval and coverage-based localization to guide edits; in its evaluation it achieves top-tier resolution performance (28.33%) indicating effective coordination for many tasks.",
            "coordination_challenges": "Authors note CodeR depends on coverage instrumentation and repository-specific commands for localization, which may require external setup and reduce autonomy; lower patch application rate (74%) indicates challenges in producing patches that apply cleanly.",
            "ablation_studies": "Not provided in MASAI paper for CodeR specifically (only comparative aggregated metrics).",
            "optimal_configurations": "Not specified in MASAI; CodeR's internal optimal settings are not detailed in the text beyond general description (use of BM25 and coverage signals).",
            "uuid": "e2568.1",
            "source_info": {
                "paper_title": "MASAI: Modular Architecture for Software-engineering AI Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "AutoGen",
            "name_full": "AutoGen",
            "brief_description": "A multi-agent conversation framework that provides abstractions for agent interactions and facilitates building next-generation LLM applications using conversational multi-agent protocols.",
            "citation_title": "Autogen: Enabling next-gen LLM applications via multi-agent conversation framework",
            "mention_or_use": "mention",
            "system_name": "AutoGen",
            "system_description": "AutoGen is presented as a framework for multi-agent conversation, offering abstractions for agent interactions and conversational programming to design multi-agent systems. The MASAI paper references AutoGen in related work as an example of multi-agent frameworks focusing on conversation protocols.",
            "number_of_agents": "variable (framework supports creating multiple conversational agents)",
            "agent_specializations": "Framework-level: supports role-based conversational agents — exact specializations depend on instantiation (not detailed in MASAI paper).",
            "research_phases_covered": "general application development and multi-agent coordination (paper mentions focus on high-level agent design rather than domain-specific research phases).",
            "coordination_mechanism": "Conversation-driven agent interactions (dialogue-based protocols); AutoGen provides abstractions for orchestrating such conversational interactions.",
            "communication_protocol": "Natural-language message passing (conversational transcripts) mediated by the framework's APIs/abstractions; exact structured formats not described in MASAI paper.",
            "feedback_mechanism": "Conversational feedback and role-based messaging; details not provided in MASAI paper.",
            "communication_frequency": "Conversation-driven (continuous/message-driven during task execution).",
            "task_domain": "General LLM multi-agent applications (framework-level; examples include but are not limited to software-engineering tasks).",
            "performance_metrics": "No experimental metrics reported in MASAI for AutoGen (it is only cited in related work).",
            "baseline_comparison": "Not compared empirically in MASAI; referenced as a design/abstraction framework rather than a SWE-bench competitor.",
            "coordination_benefits": "Provides high-level abstractions to simplify building multi-agent conversational applications and explore emergent behaviors; MASAI contrasts itself to such frameworks by favoring a simpler sequential modular composition.",
            "coordination_challenges": "MASAI notes that many such conversational frameworks focus on high-level design and are typically instantiated on simpler datasets; applicability to complex repository-level tasks is not demonstrated in this paper.",
            "ablation_studies": "None in MASAI (AutoGen is only mentioned).",
            "optimal_configurations": "Not specified in MASAI.",
            "uuid": "e2568.2",
            "source_info": {
                "paper_title": "MASAI: Modular Architecture for Software-engineering AI Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "AgentVerse",
            "name_full": "AgentVerse",
            "brief_description": "A platform for facilitating multi-agent collaboration and exploring emergent behaviors in agent populations, providing abstractions for multi-agent system construction.",
            "citation_title": "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents",
            "mention_or_use": "mention",
            "system_name": "AgentVerse",
            "system_description": "AgentVerse is described as a multi-agent platform offering tools/abstractions for building and observing collaborative agent systems and emergent behaviors; MASAI cites it in related work as an example of dialogue-driven multi-agent frameworks.",
            "number_of_agents": "variable (platform supports arbitrary numbers of agents)",
            "agent_specializations": "Platform-level: supports heterogeneous agent roles defined by the user; MASAI does not detail specific roles implemented in AgentVerse.",
            "research_phases_covered": "general multi-agent application development; not specifically targeted at scientific research in MASAI's description.",
            "coordination_mechanism": "Conversation/dialogue-based coordination between agents (framework provides communication abstractions).",
            "communication_protocol": "Natural-language message passing via framework APIs; specifics not detailed in MASAI.",
            "feedback_mechanism": "Emergent behavior observation and message exchange; no details provided in MASAI.",
            "communication_frequency": "Continuous/message-driven (framework-level).",
            "task_domain": "General multi-agent applications (research and development of collaborative agents).",
            "performance_metrics": "Not reported in MASAI (only cited).",
            "baseline_comparison": "Not empirically compared in MASAI.",
            "coordination_benefits": "Framework designed to enable experiments in multi-agent collaboration and emergent phenomena; MASAI contrasts with such frameworks by using a simpler sequential modular composition tailored to repository tasks.",
            "coordination_challenges": "MASAI suggests these frameworks typically focus on high-level conversation protocols and are often evaluated on simpler or different benchmarks than repository-level engineering tasks.",
            "ablation_studies": "None in MASAI.",
            "optimal_configurations": "Not specified in MASAI.",
            "uuid": "e2568.3",
            "source_info": {
                "paper_title": "MASAI: Modular Architecture for Software-engineering AI Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "MetaGPT",
            "name_full": "MetaGPT",
            "brief_description": "A dialogue-based cooperative agent framework for multi-agent collaborative programming and project decomposition.",
            "citation_title": "MetaGPT: Meta programming for Multi-Agent Collaborative Framework",
            "mention_or_use": "mention",
            "system_name": "MetaGPT",
            "system_description": "MetaGPT is a dialogue-based framework that orchestrates cooperative agents for software development tasks by decomposing projects into roles and dialogues; MASAI cites it as part of related work on conversational multi-agent frameworks.",
            "number_of_agents": "variable (framework decomposes tasks into multiple role-based agents)",
            "agent_specializations": "Role-based agents (e.g., planner, coder, tester) depending on instantiated project — MASAI does not enumerate MetaGPT's roles in detail.",
            "research_phases_covered": "planning, implementation, testing (depending on task instantiation).",
            "coordination_mechanism": "Dialog-based hierarchical cooperation and role assignment (conversation-driven orchestration).",
            "communication_protocol": "Natural-language dialogues structured by the framework; specifics not detailed in MASAI.",
            "feedback_mechanism": "Iterative conversational refinement and role coordination; not detailed in MASAI.",
            "communication_frequency": "Continuous (dialogue exchanges during task progression).",
            "task_domain": "Software development (project decomposition and collaborative coding).",
            "performance_metrics": "Not reported in MASAI (only cited).",
            "baseline_comparison": "Not empirically compared in MASAI.",
            "coordination_benefits": "Enables decomposition of development tasks into specialized conversational roles; MASAI chooses a different modular handoff-style approach for repository-level repairs.",
            "coordination_challenges": "Paper notes such conversational frameworks are typically evaluated on smaller generation tasks and may not directly scale to large repository-level engineering tasks without adaptation.",
            "ablation_studies": "None in MASAI.",
            "optimal_configurations": "Not specified in MASAI.",
            "uuid": "e2568.4",
            "source_info": {
                "paper_title": "MASAI: Modular Architecture for Software-engineering AI Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ChatDev / Communicative agents",
            "name_full": "Communicative agents for software development",
            "brief_description": "A cited work on communicative/multi-agent approaches for software development where agents exchange messages to coordinate development tasks.",
            "citation_title": "Communicative agents for software development",
            "mention_or_use": "mention",
            "system_name": "ChatDev / Communicative agents",
            "system_description": "Described in MASAI as a dialogue-based collaborative agent framework for software development; cited among works exploring conversational multi-agent coordination for coding tasks.",
            "number_of_agents": "variable (multiple communicative agents coordinating via dialogue)",
            "agent_specializations": "Conversational role agents (e.g., design, implementation, review) depending on instantiation; MASAI does not provide specifics.",
            "research_phases_covered": "planning, implementation, review/testing (depending on usage).",
            "coordination_mechanism": "Dialogue-driven coordination and negotiation between agents.",
            "communication_protocol": "Natural-language messages; framework-mediated conversation transcripts.",
            "feedback_mechanism": "Conversational critique and iterative refinement; not detailed in MASAI.",
            "communication_frequency": "Continuous/message-driven.",
            "task_domain": "Software development tasks.",
            "performance_metrics": "Not provided in MASAI (cited as related work).",
            "baseline_comparison": "Not empirically compared in MASAI.",
            "coordination_benefits": "Enables cooperative decomposition and role-based interactions; MASAI contrasts this with its simpler sequential, artifact-handoff approach.",
            "coordination_challenges": "Authors note dialogue-based multi-agent frameworks tend to be evaluated on smaller-scope tasks; scaling to full repository-level debugging poses challenges.",
            "ablation_studies": "None in MASAI.",
            "optimal_configurations": "Not specified in MASAI.",
            "uuid": "e2568.5",
            "source_info": {
                "paper_title": "MASAI: Modular Architecture for Software-engineering AI Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Dynamic LLM-agent network",
            "name_full": "Dynamic LLM-agent network: An LLM-agent collaboration framework with agent team optimization",
            "brief_description": "A framework for inference-time agent selection and agent-team optimization that dynamically composes and optimizes agent teams for tasks.",
            "citation_title": "Dynamic LLM-agent network: An LLMagent collaboration framework with agent team optimization",
            "mention_or_use": "mention",
            "system_name": "Dynamic LLM-agent network",
            "system_description": "This framework focuses on dynamically selecting and optimizing agent teams at inference time to improve multi-agent collaboration efficiency; MASAI cites it when discussing agent selection and team optimization literature.",
            "number_of_agents": "variable (dynamic team sizes based on optimization)",
            "agent_specializations": "Framework-level support for heterogeneous agents; exact role specializations depend on task instantiation.",
            "research_phases_covered": "agent selection and coordination phases; applicable across planning and execution depending on deployment.",
            "coordination_mechanism": "Dynamic team selection and optimization (inference-time orchestration).",
            "communication_protocol": "Not specified in MASAI beyond general framework-level optimizations; likely message passing/dialogue or structured APIs as per original work.",
            "feedback_mechanism": "Agent-team performance optimization using inference-time signals; specifics not detailed in MASAI.",
            "communication_frequency": "Dynamic/on-demand depending on optimization decisions.",
            "task_domain": "General multi-agent tasks; cited in context of LLM-agent collaboration research.",
            "performance_metrics": "Not reported in MASAI (only cited).",
            "baseline_comparison": "Not empirically compared in MASAI.",
            "coordination_benefits": "Offers methods to choose efficient agent teams for tasks, which could reduce overhead and improve performance if applied to repository-level tasks.",
            "coordination_challenges": "Not discussed in detail in MASAI; applicability to complex software-engineering tasks is not demonstrated here.",
            "ablation_studies": "None in MASAI.",
            "optimal_configurations": "Not provided in MASAI.",
            "uuid": "e2568.6",
            "source_info": {
                "paper_title": "MASAI: Modular Architecture for Software-engineering AI Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "AutoDev",
            "name_full": "AutoDev: Automated AI-Driven Development",
            "brief_description": "An AI-driven development system that can execute file editing, retrieval, and testing actions; cited as a related system with action capabilities but evaluated on NL2Code-style benchmarks.",
            "citation_title": "AutoDev: Automated AI-Driven Development",
            "mention_or_use": "mention",
            "system_name": "AutoDev",
            "system_description": "AutoDev is referenced as a system that can execute actions like file editing, retrieval and testing using agents, but MASAI notes AutoDev is typically evaluated on NL2Code datasets (HumanEval) rather than full repository-level issue resolution.",
            "number_of_agents": "variable (framework supports agents capable of file edits and testing)",
            "agent_specializations": "Agents that perform file editing, retrieval, and test execution; specifics not detailed in MASAI.",
            "research_phases_covered": "implementation and execution in NL2Code settings; not validated at repository-scale in MASAI's discussion.",
            "coordination_mechanism": "Action-capable agents with possibly centralized orchestration in its original work (not detailed in MASAI).",
            "communication_protocol": "Action invocation and environment observations (not detailed in MASAI).",
            "feedback_mechanism": "Test execution and environment feedback for iterative refinement (as described in general terms).",
            "communication_frequency": "Action-driven (on-demand during code generation/testing cycles).",
            "task_domain": "Code generation and small-scale software development (NL2Code benchmarks).",
            "performance_metrics": "Not reported in MASAI (cited only).",
            "baseline_comparison": "Not empirically compared in MASAI.",
            "coordination_benefits": "Shows how action-capable agents can be used for code editing and testing in NL2Code contexts; MASAI extends similar action primitives to full repository tasks.",
            "coordination_challenges": "MASAI notes that AutoDev and similar frameworks are evaluated on simpler benchmarks and may not directly address the complexities of repository-level debugging.",
            "ablation_studies": "None in MASAI.",
            "optimal_configurations": "Not provided in MASAI.",
            "uuid": "e2568.7",
            "source_info": {
                "paper_title": "MASAI: Modular Architecture for Software-engineering AI Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SwiftSage",
            "name_full": "SwiftSage",
            "brief_description": "A divide-and-conquer multi-component agent that combines a fast policy (Swift) and a deliberative LLM (Sage) to decide actions and sub-goal planning, inspired by dual-process cognition and applied to complex interactive tasks.",
            "citation_title": "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks",
            "mention_or_use": "mention",
            "system_name": "SwiftSage",
            "system_description": "SwiftSage is described in MASAI's related work as an agent combining a finetuned small policy model for fast actions and an LLM for deliberate planning and backtracking, illustrating divide-and-conquer agent design for complex tasks (e.g., closed-world scientific experiments in prior work).",
            "number_of_agents": "2 (conceptually: fast-policy agent 'Swift' + deliberative LLM 'Sage')",
            "agent_specializations": "Swift: fast, action-oriented small model for quick decisions; Sage: LLM for planning, deliberate sub-goal decomposition and backtracking.",
            "research_phases_covered": "planning and execution (and potentially iterative experimentation in scientific/interactive tasks).",
            "coordination_mechanism": "Hybrid two-system coordination (fast/slow) where the fast policy executes actions and the slow LLM plans and backtracks as needed.",
            "communication_protocol": "Internal API/message passing between policy and LLM; MASAI does not detail message formats beyond the conceptual split.",
            "feedback_mechanism": "Backtracking and deliberation driven by planned sub-goals and environment outcomes (as described in related work summary).",
            "communication_frequency": "Frequent between fast and slow components during task execution (conceptual).",
            "task_domain": "Complex interactive tasks; original motivations include scientific-experiment-like closed-world tasks (as cited).",
            "performance_metrics": "Not reported in MASAI (only cited in related work).",
            "baseline_comparison": "Not compared in MASAI.",
            "coordination_benefits": "Illustrates benefits of separating fast execution from slow deliberative planning for complex tasks; cited to motivate divide-and-conquer architectures.",
            "coordination_challenges": "Not discussed in MASAI beyond high-level description; practical integration details and scaling to repository-level tasks are not covered here.",
            "ablation_studies": "None in MASAI.",
            "optimal_configurations": "Not specified in MASAI.",
            "uuid": "e2568.8",
            "source_info": {
                "paper_title": "MASAI: Modular Architecture for Software-engineering AI Agents",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autogen: Enabling next-gen LLM applications via multi-agent conversation framework",
            "rating": 2
        },
        {
            "paper_title": "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents",
            "rating": 2
        },
        {
            "paper_title": "CodeR: Issue resolving with multi-agent and task graphs",
            "rating": 2
        },
        {
            "paper_title": "MetaGPT: Meta programming for Multi-Agent Collaborative Framework",
            "rating": 2
        },
        {
            "paper_title": "Communicative agents for software development",
            "rating": 2
        },
        {
            "paper_title": "Dynamic LLM-agent network: An LLMagent collaboration framework with agent team optimization",
            "rating": 2
        },
        {
            "paper_title": "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks",
            "rating": 1
        },
        {
            "paper_title": "AutoDev: Automated AI-Driven Development",
            "rating": 1
        }
    ],
    "cost": 0.01983875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MASAI: Modular Architecture for Software-engineering AI Agents</h1>
<p>Daman Arora<em>, Atharv Sonwane</em>, Nalin Wadhwa*<br>Abhav Mehrotra, Saiteja Utpala, Ramakrishna Bairi<br>Aditya Kanade, Nagarajan Natarajan<br>Microsoft Research India<br>{daman1209arora, atharvs.twm, nalin.wadhwa02}@gmail.com<br>{abhavm1, saitejautpala}@gmail.com<br>{ram.bairi, kanadeaditya, nagarajan.natarajan}@microsoft.com</p>
<h4>Abstract</h4>
<p>A common method to solve complex problems in software engineering, is to divide the problem into multiple sub-problems. Inspired by this, we propose a Modular Architecture for Software-engineering AI (MASAI) agents, where different LLM-powered sub-agents are instantiated with well-defined objectives and strategies tuned to achieve those objectives. Our modular architecture offers several advantages: (1) employing and tuning different problem-solving strategies across sub-agents, (2) enabling sub-agents to gather information from different sources scattered throughout a repository, and (3) avoiding unnecessarily long trajectories which inflate costs and add extraneous context. MASAI enabled us to achieve the highest performance ( $28.33 \%$ resolution rate) on the popular and highly challenging SWE-bench Lite dataset consisting of 300 GitHub issues from 11 Python repositories. We conduct a comprehensive evaluation of MASAI relative to other agentic methods and analyze the effects of our design decisions and their contribution to the success of MASAI.</p>
<h2>1 Introduction</h2>
<p>Software engineering is a challenging activity which requires exercising various skills such as coding, reasoning, testing, and debugging. The ever growing demand for software calls for better support to software engineers. Recent advances in AI offer much promise in this direction.</p>
<p>Large language models (LLMs) have shown remarkable ability to code (Chen et al. [2021], Roziere et al. [2023], CodeGemma Team [2024], inter alia), reason [Kojima et al., 2022] and plan [Huang et al., 2022]. Iterative reasoning, structured as chains [Wei et al., 2022] or trees [Yao et al., 2024] of thought, further enhance their ability to solve complex problems that require many inter-related steps of reasoning. When combined with tools or environment actions [Yao et al., 2023, Patil et al., 2023,
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparison of MASAI with existing methods. Resolution rate refers to the percentage of issues in SWE-bench Lite that are resolved.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Schick et al., 2024] and feedback from the environment [Zhou et al., 2023, Shinn et al., 2024], they enable autonomous agents capable of achieving specific goals [Zhang et al., 2023].
As the problem complexity increases, it becomes difficult to devise a single, over-arching strategy that works across the board. Indeed, when faced with a complex coding problem, software engineers break it down into sub-problems and use different strategies to deal with them separately. Inspired by this, we propose a Modular Architecture of Software-engineering AI (MASAI) agents, where different LLM-powered sub-agents are instantiated with well-defined objectives and strategies tuned to achieve those objectives.
Our modular architecture consists of 5 different sub-agents: Test Template Generator which generates a template test case and instructions on how to run it, Issue Reproducer which writes a test case to reproduce the issue, Edit Localizer which finds files to be edited, Fixer which fixes the issue by generating multiple possible patches, and finally Ranker which ranks the patches based on the generated test. When combined, all these individual sub-agents work in tandem to resolve complex real-world software engineering issues.
Our approach offers several advantages: (1) employing and tuning different problem-solving strategies across sub-agents (e.g., ReAct or CoT), (2) enabling sub-agents to gather information from different sources scattered throughout a repository (e.g., from a README or a test file), and (3) avoiding unnecessarily long trajectories which inflate inference costs and pass extraneous context which could degrade performance [Shi et al., 2023].
We evaluate MASAI on the popular and highly challenging SWE-bench Lite dataset [Jimenez et al., 2024] of 300 GitHub issues from 11 Python repositories. Due to its practical relevance and challenging nature, SWE-bench Lite has attracted significant efforts from academia, industry and start-ups. As shown in Figure 1, with the highest resolution rate of $28.33 \%$, MASAI achieves state-of-the-art results on SWE-bench Lite. The field of AI agents, and specifically software-engineering AI agents, is nascent and rapidly evolving. In fact, all the existing methods in Figure 1 have been developed within the past three months. Nevertheless, we do compare against them thoroughly.
AI agents for software engineering would encounter many common sub-problems, such as autonomously understanding testing infrastructure and code organization of a repository, writing new tests, localizing bugs, editing large files without introducing syntactic/semantic errors, synthesizing fixes and writing new code. We believe that it is crucial to understand how different strategies perform on these sub-problems. Therefore we conduct a thorough investigation into the performance of MASAI and existing methods on SWE-bench Lite, and present the impact of key design decisions.
In summary, our contributions are:</p>
<ol>
<li>Propose a modular architecture, MASAI, that allows optimized design of sub-agents separately while combining them to solving larger, end-to-end software engineering tasks.</li>
<li>Show the effectiveness of MASAI by achieving the highest resolution rate on SWE-bench Lite.</li>
<li>Conduct a thorough investigation into key design decisions of MASAI and the existing methods which can help inform future research and development in this rapidly evolving space.</li>
<li>Contribute our results to the SWE-bench Lite leaderboard [MASAI, 2024] for validation.</li>
</ol>
<h1>2 MASAI Agent Architecture</h1>
<p>Solving a problem in a code repository requires understanding the problem description and the codebase, gathering the necessary information scattered across multiple files, locating the root cause, fixing it and verifying the fix. Instead of treating this as one long chain of reasoning and actions, we propose modularizing the problem into sub-problems and delegating them to different sub-agents.</p>
<h3>2.1 Agent Specification and Composition</h3>
<p>A MASAI agent is a composition of several MASAI sub-agents. A MASAI sub-agent is specified by a tuple $\langle$ Input, Strategy, Output $\rangle$ where</p>
<ol>
<li>Input to the sub-agent comprises of the code repository, information obtained from other sub-agents as necessary, a set of allowed actions and task instructions.</li>
</ol>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: MASAI applied to the task of repository-level issue resolution on an example. MASAI takes a repository and an issue description as input, and produces a single patch. The 5 sub-agents (thick borders) tackle different sub-problems. The information flow between them is shown by directed edges. They marked with the solution strategy and input $\cdot$ output specification
Detail: The example issue from the scikit-learn repository (id: 13142) describes inconsistent behaviour between two functions relating to GaussianMixture and gives an example. The Test Template Generator first generates an example test case along with the command to run the test case. This is given to the Issue Reproducer which writes a test that reproduces the issue along with the command to run it. The Edit Localizer navigates the repository to find files related to the buggy behaviour of GaussianMixture and passes the information to the Fixer which generates a set of possible patches which could fix the issue. Finally, the Ranker takes in the possible patches and the generated reproductions test. Running the tests on the patches, the ranker observes that one of the patches pass while the rest patch fail. It outputs the ranking with the passing patch first which gets selected as the proposed solution.
2. Strategy is the problem-solving strategy to be followed by the sub-agent in using the LLM to solve its given sub-problem. This could be vanilla completion, CoT [Wei et al., 2022], ReAct [Yao et al., 2023], RAG [Lewis et al., 2020], etc.;
3. Output is the specification of the content that the sub-agent must return upon completion as well the format it must be presented in.</p>
<p>Compared to multi-agent frameworks [Wu et al., 2023, Qian et al., 2023, Hong et al., 2024], the MASAI architecture is simpler, in that, the sub-agents are given modular objectives that do not require explicit one-to-one or group conversations between sub-agents. The sub-agents are composed by passing the output from one sub-agent to the input of another sub-agent.</p>
<h1>2.2 Action Space</h1>
<p>All the sub-agents are presented with a set of actions which allows them to interact with the environment. The actions we use in this work are:</p>
<ol>
<li>
<p>READ(file, class, function): Query and read a specific function, class or file. All three attributes are not necessary; the agent can specify only a function and a file or even a single file. If there exists only one exactly matching code segment with these attributes, then that code is returned. If there are multiple matches, all their names are returned and the query can be refined if necessary. The READ action returns a lazy representation that aims to keep the output concise. When reading a file, only signatures of the top level definitions are presented; when reading a class, the signature of the class (class name and member signatures) are presented and when reading a function, its complete body is presented.</p>
</li>
<li>
<p>EDIT(file, class, function): Marks a code segment for editing. Just like READ, this marks a code segment only when a unique match exists. Otherwise, the set of partial matches are returned which may be refined further.</p>
</li>
<li>ADD(file): Marks a file for code addition. The file must exist for the action to succeed.</li>
<li>WRITE(file, contents): Writes the specified content to a file. The specified file can be new or a file that the agent has created earlier.</li>
<li>LIST(folder): Lists folder contents if it exists.</li>
<li>COMMAND(command): Executes the command in a shell with timeout and truncation of large results.</li>
<li>DONE: Used by the agent to signal that it has completed its assigned objective.</li>
</ol>
<h1>2.3 Agent Instantiation</h1>
<p>In this work, we focus on the general task of resolving repository-level issues, as exemplified by the SWE-bench Lite dataset. A problem statement consists of an issue description and a repository. The agent is required to produce a patch so that the issue is resolved. Issue resolution is checked by ensuring that the relevant, held-out test cases pass. Below, we refer to ReAct Yao et al. [2023] which is a problem-solving strategy that alternates between generating an action to take using an LLM followed by executing the action and using the resulting observations as input for the subsequent action generation. Chain of Thought (CoT) Wei et al. [2022] generates solutions to a problem using an LLM while asking it to generate specific intermediate reasoning steps.
We instantiate 5 sub-agents to collectively resolve repository-level issues. Figure 2 shows the overall architecture of our MASAI agent on a concrete example, along with the information flow between the sub-agents (shown by the solid edges).
(1) Test Template Generator: Discovers how to write and run a new test by analyzing the testing setup specific to the repository.</p>
<ul>
<li>Input: The repository state (within its execution environment) is provided. READ, LIST, COMMAND, WRITE and DONE actions are provided.</li>
<li>Strategy: ReAct.</li>
<li>Output: The code for a template test case (which is issue independent) for the repository along with the command to run it. This is used to aid the Issue Reproducer sub-agent described next.</li>
</ul>
<p>Test Template Generator is instructed to explore the documentation and existing tests within the repository to complete its objective and to keep trying until it comes up with a template and a command that passes without exceptions. Test Template Generator evaluates the output of its ReAct loop to determine whether the generated test passes without exceptions. It retries upto a specified limit or until it finds a template that works.
(2) Issue Reproducer: Writes a test that reproduces the behaviour reported in the given issue.</p>
<ul>
<li>Input: In addition to the repository state and issue description, the sample test file and the command to run it, generated by the Test Template Generator, are provided. Actions available are READ, LIST, COMMAND, WRITE and DONE.</li>
<li>Strategy: ReAct.</li>
<li>Output: The code for a test case which reproduces the issue and would show a change in status (pass vs. fail) when the issue is fixed. It also outputs the shell command to run the test.
(3) Edit Localizer: Navigates the repository and identifies code locations (files, classes, functions) that need to be edited to resolve the issue.</li>
<li>Input: The repository state and the issue description are provided. Available actions are READ, LIST, EDIT, ADD, COMMAND and DONE.</li>
<li>Strategy: ReAct.</li>
<li>Output: List of code locations (specified through the EDIT and ADD commands) to edit.</li>
</ul>
<p>If no locations have been marked at the end of the ReAct loop, then the Edit Localizer selects a set of locations from all of the ones it has read so far.
(4) Fixer: Suggests multiple potential patches to the code locations marked by Edit Localizer that may resolve the issue.</p>
<ul>
<li>Input: Issue description along with contents of the code locations required to be edited. No actions are given to this sub-agent.</li>
<li>Strategy: CoT.</li>
<li>Output: Multiple possible candidate patches to the provided suspicious code.</li>
</ul>
<p>When prompting the LLM for a possible patch, Fixer asks for the edit in the form of a minimal rewrite instead of rewriting the full sections. Similar to Deligiannis et al. [2023], the content of the locations to edit are provided by Fixer with line numbers. For each edit, the Fixer expects the LLM to output the original version of the code snippet (pre) followed by the edited version of this snippet (post). Both these snippets are expected to have a line number for each line. Fixer then searches for the pre snippet using line numbers in the target file to replace with the post version. If an exact match is not found, it uses fuzzy matching to find the closest matching span for the pre snippet. After replacing with the post span, it computes the diff of the target file with its contents before the edit. Syntactically incorrect edits are rejected and the resultant patches are used downstream.
(5) Ranker: Ranks the candidate patches from the Fixer, using the test generated by Issue Reproducer.</p>
<ul>
<li>Input: Issue description, candidate patches from Fixer, and the reproduction test (as well as the command to run it) from Issue Reproducer. No environment actions are allowed.</li>
<li>Strategy: CoT.</li>
<li>Output: Ranking of the candidate patches in the order of likelihood to resolve the issue.</li>
</ul>
<p>For each of the patches, Ranker first runs the test on each of the patches and then asks the LLM to determine whether the application of that patch to the repository has caused the provided test to change status (go from failing to passing or vice versa) given the test results. Based on the output of this, the LLM is then asked to rank the patches. The top ranked patch is selected as the issue resolution. If the Issue Reproducer sub-agent could not generate a test, then the Ranker ranks the patches using only the issue description.</p>
<h1>3 Experimental Setup</h1>
<p>Dataset: We perform experiments on SWE-bench Lite [Jimenez et al., 2024], a collection of 300 software engineering tasks (predominantly bug fixes) sourced from 11 open-source repositories. Each task consists of an issue description and the state of the repository on which the issue was raised. The objective is to produce a patch (that applies to one or more files), which when applied to the repository at the given state, resolves the issue. The proposed patch for an issue is said to successfully resolve, if the targeted suite of tests, provided as part of the dataset (and revealed only at the time of evaluation), passes on the patched version of the repository Each task consists of an issue description and the state of the repository on which the issue was raised. The objective is to produce a patch given a repository and an issue description, so that the repository after the patch is applied, passes the issue-specific tests (that are never revealed to the agent).
Metrics: We report three metrics: (1) Resolution rate, the percentage of issues successfully resolved (i.e., pass the issue-specific tests); (2) Localization rate, the percentage of issues where the patch proposed by a method fully covers the ground-truth patch files, i.e., where recall is $100 \%$ at the file level; and (3) Application rate, the percentage of issues where the patch proposed by a method successfully applies on the repository (i.e., the Linux command patch does not raise an error).
Competing methods: We compare with all the existing methods that are also evaluated on SWEbench Lite (with logs here):</p>
<ol>
<li>
<p>SWE-agent [Yang et al., 2024a]: Utilizes a single ReAct loop along with specialized environment interface with multiple tools. Uses GPT-4 (1106).</p>
</li>
<li>
<p>AutoCodeRover [Zhang et al., 2024] (ACR): Uses a ReAct loop for localization and another for generating patches. Uses specialized tools for searching specific code elements (class, method) within other code elements and presenting them as signatures whenever appropriate. Uses GPT-4 (0125).</p>
</li>
<li>OpenDevin [OpenDevin, 2024]: Uses the CodeAct [Wang et al., 2024a] framework where the agent (a single ReAct loop) can execute any bash command along with using various helper commands. The version of OpenDevin with highest reported performance v1.3_gpt4o makes use of hints_text in SWE-bench Lite, conversation transcript of developers on an issue in GitHub. While we include results from this version, we compare in detail with the highest performing version that does not use hints, v1.5_gpt4o_nohints.</li>
<li>Aider [Aider, 2024]: Uses static analysis to provide a compact view of the repository and, in turn, to determine the file(s) to edit. Limited number of ReAct steps are taken to make an edit to the identified file(s) and iteratively update it until it is syntactically correct and passes existing tests. After these steps, the final status of linting and pre-existing tests are used to determine whether the tool should be run again from scratch until a plausible solution is found. Uses GPT-4o and 365 Claude 3 Opus on alternate runs</li>
<li>CodeR [Chen et al., 2024]: A multi agent solution with separate agents to reproduce the issue, localize the fault and iteratively edit the code to resolve the issue. Uses BM25 along with test coverage statistics for fault localization. Uses GPT-4 (1106).</li>
<li>Moatless [Moatless Tools, 2024]: Uses a ReAct loop to localize and another to fix the code. Leverages a semantic search tool that searches with natural language queries for relevant code chunks in the repository.</li>
<li>RAG: Uses BM25 to retrieve relevant files which are used to prompt an LLM to generate a patch. We compare with the best-performing RAG model from the SWE-bench Lite leaderboard [SWEbench, 2024]: RAG + Claude 3 Opus.</li>
<li>Along with the above, commercial offerings Amazon Q-Developer [Amazon, 2024], Bytedance MarsCode [Bytedance, 2024], OpenCGS Starship [OpenCGS, 2024] and IBM Research Agent101 [IBM, 2024] have also reported results on SWE-bench Lite. While we report metrics for these, we are unable to conduct further comparisons with them due to non-availability of detailed logs or any information about their approaches. We do not compare with Devin [Devin, 2024] as it reports performance a subset of SWE-bench different from SWE-bench Lite.</li>
</ol>
<p>Implementation: We evaluate MASAI by setting up a fresh development environment with all the requirements and providing the issue description. MASAI generates a single patch which is then evaluated using the SWE-bench Lite testing harness. The tree-sitter==0.21.1 package is used to implement the lazy representation part of the READ function. We use the GPT-4o model throughout our pipeline. For Test Template Generator, we start with a temperature of 0 and increase by 0.2 for each attempt. For Issue Reproducer, Edit Localizer, and Ranker, we use a temperature of 0; for Fixer, we use 0.5 and sample 5 candidate patches. We limit the ReAct loops of the Test Template Generator, Issue Reproducer, and Edit Localizer to 25 steps and limit Test Template Generator to 3 retries. After the ranker selects the patch, we run an auto-import tool to add missing imports. We discard any edits to pre-existing tests which the agent might have made. The per-issue cost for MASAI is 1.96 USD on average. We estimate the total cost of our experiments to be $&lt;10 \mathrm{k}$ USD.</p>
<h1>4 Results</h1>
<p>We first present comprehensive results on the SWE-bench Lite dataset. Then we provide supporting empirical observations and examples that bring out the effectiveness of our design choices.</p>
<h3>4.1 RQ1: Performance on software engineering tasks in SWE-bench Lite</h3>
<p>We present our main results in Table 1. Multiple remarks are in order.</p>
<ol>
<li>Our method, MASAI, achieves the highest resolution rate of $28.33 \%$ on the dataset, thereby establishing a state-of-the-art on the benchmark leaderboard alongside CodeR [MASAI, 2024].</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Resolution <br> rate (\%)</th>
<th style="text-align: center;">Localisation <br> rate (\%)</th>
<th style="text-align: center;">Application <br> rate (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RAG</td>
<td style="text-align: center;">4.33</td>
<td style="text-align: center;">48.00</td>
<td style="text-align: center;">51.67</td>
</tr>
<tr>
<td style="text-align: left;">SWE-agent</td>
<td style="text-align: center;">18.00</td>
<td style="text-align: center;">61.00</td>
<td style="text-align: center;">93.67</td>
</tr>
<tr>
<td style="text-align: left;">ACR</td>
<td style="text-align: center;">19.00</td>
<td style="text-align: center;">62.33</td>
<td style="text-align: center;">80.00</td>
</tr>
<tr>
<td style="text-align: left;">Q-Dev</td>
<td style="text-align: center;">20.33</td>
<td style="text-align: center;">71.67</td>
<td style="text-align: center;">97.33</td>
</tr>
<tr>
<td style="text-align: left;">MarsCode</td>
<td style="text-align: center;">22.00</td>
<td style="text-align: center;">67.00</td>
<td style="text-align: center;">83.67</td>
</tr>
<tr>
<td style="text-align: left;">Moatless</td>
<td style="text-align: center;">23.33</td>
<td style="text-align: center;">73.00</td>
<td style="text-align: center;">97.00</td>
</tr>
<tr>
<td style="text-align: left;">Starship</td>
<td style="text-align: center;">23.67</td>
<td style="text-align: center;">$\mathbf{9 0 . 6 7}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">OpenDevin</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">77.00</td>
<td style="text-align: center;">90.00</td>
</tr>
<tr>
<td style="text-align: left;">- hints</td>
<td style="text-align: center;">16.00</td>
<td style="text-align: center;">63.00</td>
<td style="text-align: center;">81.33</td>
</tr>
<tr>
<td style="text-align: left;">Aider</td>
<td style="text-align: center;">26.33</td>
<td style="text-align: center;">69.67</td>
<td style="text-align: center;">96.67</td>
</tr>
<tr>
<td style="text-align: left;">Agent-101</td>
<td style="text-align: center;">26.67</td>
<td style="text-align: center;">72.67</td>
<td style="text-align: center;">97.33</td>
</tr>
<tr>
<td style="text-align: left;">CodeR</td>
<td style="text-align: center;">$\mathbf{2 8 . 3 3}$</td>
<td style="text-align: center;">66.67</td>
<td style="text-align: center;">74.00</td>
</tr>
<tr>
<td style="text-align: left;">MASAI</td>
<td style="text-align: center;">$\mathbf{2 8 . 3 3}$</td>
<td style="text-align: center;">75.00</td>
<td style="text-align: center;">95.33</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance of baseline and competing methods on SWE-bench Lite (best in bold). Our proposed method, MASAI, achieves the best resolution rate (\% issues resolved). Row "- hints" indicates executing OpenDevin without using hints_text in the dataset.
2. Standard RAG baseline (first row) performs substantially poor on the dataset, as has also been established in recent works [Jimenez et al., 2024, Chen et al., 2024]; which is a strong indication of the complexity of the SWE-bench Lite dataset.
3. MASAI localizes the issue (at a file-level) in $75 \%$ of the cases; the best method in terms of localization rate, OpenCGS Starship, at nearly $91 \%$, however achieves only $23.67 \%$ resolution rate.
4. The (edit) application rate is generally high for all LLM-based agents; MASAI's patches, in particular, successfully apply in over $95 \%$ of the cases.</p>
<h1>4.2 RQ2: Assumptions by different methods</h1>
<p>High autonomy and less dependence on external signals (e.g., expert hints) is desirable from softwareengineering agents. In the standard SWE-bench Lite setup, all agents are provided the issue description along with the repository. However, we observe that different methods make different assumptions about available auxiliary information.</p>
<ul>
<li>All methods apart from RAG and Moatless require that for each task, an environment be set up with the appropriate requirements installed beforehand so that code can be executed.</li>
<li>OpenDevin avails hints_text provided by SWE-bench Lite as discussed in Section 3.</li>
<li>Aider, when running pre-existing tests, uses pre-determined test commands consist of (1) the testing framework used to run tests in the task repository and (2) specific unit tests that target the code pertaining to the issue at hand. The former assumes information about the repository-specific testing framework which is not present in the standard SWE-bench Lite setup. In the case of the latter, providing output from only the target test (and not the whole test suite) during ReAct steps, inadvertently provides additional information about which part of the repository is relevant to the issue.</li>
<li>CodeR uses coverage-based code ranking [Wong et al., 2016] for fault localization. As in Aider, this would require repository-specific commands to run pre-existing tests, and instrumentation of the full repository to get coverage information. However from the available trajectory logs of CodeR it does not appear to discover these autonomously.</li>
</ul>
<p>MASAI aims for high autonomy by avoiding dependence on additional inputs, only relying on the original setup proposed by Jimenez et al. [2024]. SWE-agent and AutoCodeRover operate at a similar level of autonomy to MASAI. Results in Table 1 show that MASAI outperforms all other approaches without making additional assumptions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Selection Strategy</th>
<th style="text-align: center;">1 Sample</th>
<th style="text-align: center;">5 Samples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Oracle</td>
<td style="text-align: center;">$23.33 \%$</td>
<td style="text-align: center;">$35.00 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$22.28 \%$</td>
</tr>
<tr>
<td style="text-align: left;">LLM w/o test</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$23.33 \%$</td>
</tr>
<tr>
<td style="text-align: left;">LLM w/ test (Ranker)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$28.33 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Resolution rates of MASAI on SWE-bench Lite, with different number of Fixer samples (i.e., candidate patches), using different sample selection strategies (rows, discussed in Section 4.4).</p>
<h1>4.3 RQ3: How does MASAI perform effective fault localization from issue description?</h1>
<p>Localization requires multi-step reasoning to identify the root cause of the error from issue descriptions which are often vague and usually only describe the problem being observed. We observe that (1) the choice of ReAct as the strategy, (2) the specificity of its objective (to only identify files to edit) and (3) the designs of tools available enables the Edit Localizer to perform the required multi-step reasoning in a flexible and robust manner. Note that (1) and (2) are results of the modularity of MASAI. SWE-agent and OpenDevin, methods that do not employ a separate localization sub-agent, achieve $61 \%$ and $63 \%$ localization rates respectively, compared to $75 \%$ achieved by MASAI's Edit Localizer.
We observe the advantages of using a ReAct sub-agent, by comparing with Aider which uses a single step CoT approach. In the 27 issues solved by MASAI but not by Aider, Aider failed to localize in 10 (37\%) issues whereas among the 21 issues solved by Aider but not by MASAI, MASAI only failed to localize in $3(14 \%)$ issues. This shows that better localization plays a role in superior resolution rate. Comparing the average search steps (as proxy for complexity) required for problems that both Aider and MASAI solved (10.9) and those that only MASAI solved (12.8), we further see that MASAI's ReAct based Edit Localizer has the flexibility to scale to more complex localization challenges.
[Example 1]: MASAI performs multi-step reasoning required for localization in the task scikit-learn__scikit-learn-13142 (described in Fig. 2). Edit Localizer finds the class mentioned in the issue and then traces symbols and inheritance links to identify the root cause.
[Example 2]: The ability of the READ action to return approximate matches (Section 2) helps in the issue astropy__astropy-14995. When the LLM asks for a nonexistent NDDataRef.multiply method in the astropy/nddata/nddata.py file, the action responds with an approximate match NDArithmeticMixin.multiply in a different file astropy/nddata/mixins/ndarithmetic.py. Then the sub-agent traces 3 callee links to get to the actual faulty function.
[Example 3]: Access to basic shell commands helps the Edit Localizer in the issue matplotlib__matplotlib_25332. grep is used to look for occurrences of the FigureBase._align_label_groups attribute within the large file lib/matplotlib/figure.py. From the the occurrences (output from grep), MASAI finds out that the attribute is set using cbook.Grouper() - the class that needs to be edited to resolve the issue.
Neither Aider nor CodeR localized faulty functions correctly in any of the 3 examples. OpenDevin localized Example 2; SWE-agent Examples 2 and 3.</p>
<h3>4.4 RQ4: How does MASAI's sampling and ranking compare to iterative repair?</h3>
<p>We observe that sampling multiple repair patches from the Fixer significantly increases the possibility of generating a correct patch, as reported in Table 2 (Oracle selection $23.33 \%$ at 1 sample vs $35 \%$ at 5 samples). However the LLM alone is unable to select amongst theses patches (LLM w/o test). This can be overcome by using the output from the generated issue-reproduction test on each patch for ranking the patches (LLM w/ test (Ranker)).
MASAI exploits the above observations through its modularity by (1) leveraging a CoT sampling strategy for Fixer and (2) instantiating independent sub-agents for test generation and repair. Other methods rely on an iterative approach to extract multiple edits from the LLM asking it to iteratively fix any mistakes it has made.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Both <br> localised</th>
<th style="text-align: left;">Method <br> resolved</th>
<th style="text-align: left;">MASAI <br> resolved</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RAG</td>
<td style="text-align: left;">126</td>
<td style="text-align: left;">12</td>
<td style="text-align: left;">$\mathbf{5 2}(+31.7 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">ACR</td>
<td style="text-align: left;">166</td>
<td style="text-align: left;">51</td>
<td style="text-align: left;">$\mathbf{7 3}(+13.2 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Q-Dev</td>
<td style="text-align: left;">191</td>
<td style="text-align: left;">55</td>
<td style="text-align: left;">$\mathbf{7 5}(+10.5 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">SWE-agent</td>
<td style="text-align: left;">166</td>
<td style="text-align: left;">48</td>
<td style="text-align: left;">$\mathbf{6 5}(+10.2 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Starship</td>
<td style="text-align: left;">220</td>
<td style="text-align: left;">62</td>
<td style="text-align: left;">$\mathbf{8 1}(+8.6 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">OpenDevin</td>
<td style="text-align: left;">187</td>
<td style="text-align: left;">60</td>
<td style="text-align: left;">$\mathbf{7 4}(+7.5 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">- hints</td>
<td style="text-align: left;">164</td>
<td style="text-align: left;">39</td>
<td style="text-align: left;">$\mathbf{6 7}(+17.1 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Moatless</td>
<td style="text-align: left;">193</td>
<td style="text-align: left;">62</td>
<td style="text-align: left;">$\mathbf{7 5}(+6.7 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">MarsCode</td>
<td style="text-align: left;">182</td>
<td style="text-align: left;">59</td>
<td style="text-align: left;">$\mathbf{7 1}(+6.6 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Agent-101</td>
<td style="text-align: left;">193</td>
<td style="text-align: left;">69</td>
<td style="text-align: left;">$\mathbf{7 2}(+1.6 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Aider</td>
<td style="text-align: left;">189</td>
<td style="text-align: left;">$\mathbf{7 1}$</td>
<td style="text-align: left;">$\mathbf{7 1} \quad(=)$</td>
</tr>
<tr>
<td style="text-align: left;">CodeR</td>
<td style="text-align: left;">174</td>
<td style="text-align: left;">$\mathbf{7 7}$</td>
<td style="text-align: left;">$72 \quad(-0.3 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 3: Number of issues resolved by a method (Method resolved) named in the rows and by MASAI (MASAI resolved) among the issues that are successfully localized by both MASAI and the method ("Both localised" column, out of 300). Row-wise max. in bold.</p>
<p>We evaluate the benefits of our approach empirically in Table 3. By controlling for localization, we are comparing the effectiveness of completing the repair. MASAI is substantially more effective at this than most methods, barring CodeR and Aider.</p>
<p>As as example, consider the issue django__django-14787 where CodeR, Aider, OpenDevin and MASAI all correctly localize the issue, but only MASAI solves it correctly. While iterative methods sample one candidate and keep refining it without success, MASAI's Fixer sub-agent generates 5 samples out of which only one is correct - demonstrating the importance for diverse sampling. MASAI's Ranker correctly ranks these by utilizing outputs from running the generated reproduction test. Aider submits patch which passes pre-existing tests but is actually incorrect, showing the importance of the generated reproduction test to eliminate false positives.</p>
<h1>4.5 RQ5: How does MASAI perform effective issue reproduction?</h1>
<p>As discussed in the previous RQ, the ability to generate tests that reproduce the stated issue is critical to select Fixer samples. Often repositories employ uncommon testing frameworks, that makes this task hard. Consider the issue django__django-14672. This repository proved hard to write tests for since it uses a custom testing framework, which involved having all new test classes derive from a certain base class to run. OpenDevin was unable to reproduce the test; in its attempt to install pytest, it ran out of budget and failed to solve this issue.</p>
<p>To remedy this, we decompose test reproduction into two steps: (1) Test Template Generator reads documentation/existing tests to generate a sample test template and instructions to run; (2) Issue Reproducer then uses the template as an example to create an issue specific test. This improves the overall capability of reproducing tests in MASAI, Test Template Generator first goes through the repository, creates a template file that correctly makes use of django.test.TestCase to create an example test case as well as the correct command to run it. The Issue Reproducer subsequently reproduces the issue correctly, without running into problems that OpenDevin faced.</p>
<h3>4.6 RQ6: How does MASAI generate edits that can be applied successfully?</h3>
<p>The representation used to encode edits can have a large impact on the performance. As discussed in Section 2, MASAI prompts the LLM for edits, in the form of a minimal rewrite - to reproduce the current state of the code snippet it wants to edit, followed by the edited version of this snippet. Recall that we also employ fuzzy matching to find the relevant span in the file, by searching for the snippet that best fuzzily matches with the one provided by the model. This mitigates copying or line counting mistakes by the LLM, significantly reducing the number of syntax errors introduced when editing. Our edit representation and fuzzing matching together yield $96.33 \%$ edit application rate (Table 1) which is among the highest.</p>
<h1>5 Related Work</h1>
<p>We have already discussed competing methods evaluated on SWE-bench Lite, in Sections 3 and 4. We now highlight other related work on LLM-powered agents.</p>
<p>Software-engineering agents: Language Agent Tree Search Zhou et al. [2023] synergizes reasoning, planning, and acting abilities of LLMs. Their strategy relies on determining partial or full termination of the search (e.g., by running provided golden test cases for successful code generation as in HumanEval Chen et al. [2021]) and backtracking if necessary; this is often infeasible in complex software engineering tasks we tackle in this paper. CodePlan [Bairi et al., 2023] combines LLMs with static analysis-backed planning for repository-level software engineering tasks such as package migration. It relies on compiler feedback and dependency graphs to guide the localization of edits; unlike in our general setting, where the agents are more autonomous, and are equipped to discover localization strategies. AlphaCodium [Ridnik et al., 2024] differs from MASAI in that (1) it uses public and AI-generated test cases for filtering; (2) is evaluated in the generation (NL2Code) setting.</p>
<p>Conversational and multi-agent frameworks: In this line of work Guo et al. [2024], Yang et al. [2024b], (1) the focus is often on the high level aspects of agent design such as conversation protocols. AutoGen [Wu et al., 2023] and AgentVerse [Chen et al., 2023] provide abstractions for agent interactions and conversational programming for design of multi-agent systems; similarly, Dynamic agent networks [Liu et al., 2023] focuses on inference-time agent selection and agent team optimization; and (2) the frameworks are typically instantiated on standard RL or relatively simpler code generation datasets. For instance, AutoDev [Tufano et al., 2024] can execute actions like file editing, retrieval, testing, but is evaluated on the HumanEval [Chen et al., 2021] NL2Code dataset. Similarly, MetaGPT [Hong et al., 2024] and ChatDev [Qian et al., 2023], dialogue-based cooperative agent frameworks, are instantiated on generation tasks involving a few hundred lines of code.
In contrast, we focus on designing a modular agent architecture for solving complex, real-world software engineering tasks, as exemplified by the SWE-bench Lite dataset.</p>
<p>Divide-and-Conquer approaches: In this line of work, the given complex task is broken down into multiple sub-goals that are solved individually, and then the solution for the task is synthesized. Multilevel Compositional Reasoning (MCR) Agent [Bhambri et al., 2023] uses compositional reasoning for instruction following in environments with partial observability and requiring long-horizon planning, such as in robotic navigation. Compositional T2I [Wang et al., 2024b] agent uses divide-and-conquer strategy for generating images from complex textual descriptions. SwiftSage [Lin et al., 2024] agent, inspired by the dual-process theory of human cognition for solving tasks, e.g., closed-world scientific experiments [Wang et al., 2022], uses finetuned SLM policy ("Swift") to decide and execute fast actions, and an LLM ("Sage") for deliberate planning of sub-goals and for backtracking when necessary.</p>
<h2>6 Conclusions</h2>
<p>As divide-and-conquer helps humans overcome complexity, similar approaches to modularize tasks into sub-tasks can help AI agents as well. In this work, we presented a modular architecture, MASAI, for software-engineering agents. Encouraged by the effectiveness of MASAI on SWE-bench Lite, we plan to extend it to a larger range of software-engineering tasks, which will also involve building realistic and diverse datasets.</p>
<h2>7 Limitations</h2>
<p>Our evaluation is centered on the widely-used SWE-bench Lite dataset for evaluating softwareengineering AI agents. It allowed us to do head-to-head comparison with many agents. However, the breadth of issues covered in SWE-bench Lite is limited to those that can be validated using tests. In future, we expect us and the community to expand the scope to more diverse issues.
There are a number of LLMs that support code understanding and generation. The modularity of MASAI permits use of different language models in different sub-agents. Due to the time and cost</p>
<p>constraints, we have instantiated all sub-agents with GPT-4o. The cost-performance trade-off of using different LLMs and possibly, even small language models (SLMs) is an interesting research problem. The competing methods that we compared against do employ different LLMs, but this still leaves out direct comparison of different LLMs on a fixed solution strategy.</p>
<p>The issue descriptions in SWE-bench Lite are all in English. This leaves out issues from a large segment of non-English speaking developers. The increasing support for the diverse world languages by LLMs should enable multi-lingual evaluation even in the software engineering domain, which is a problem that we are excited about.</p>
<h1>8 Broader Concerns</h1>
<p>Agentic frameworks with the ability to use tools like shell commands can lead to unintended sideeffects on the user's system. Appropriate guardrails and sandboxing can mitigate such problems.</p>
<p>Our approach contributes towards the development of tools to autonomously perform software development tasks. This raises various security concerns. The tool may not always follow best practices when writing or editing code, leading to introduction of security vulnerabilities and bugs. Therefore, it is important for code changes suggested by the tool to be reviewed by expert developers before being deployed to real world systems.</p>
<p>As mentioned in the Section 7, the dataset we evaluate on (SWE-bench Lite) as well as the model we use (GPT-4o) are primarily in English. This limits the usability of our tool to software engineers proficient in English. Further work is necessary in developing methods for non-English speaking developers in order to prevent this population from being marginalized.</p>
<h2>References</h2>
<p>Aider. https://aider.chat/2024/06/02/main-swe-bench.html, 2024.
Amazon. https://aws.amazon.com/q/developer/, 2024.
Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B Ashok, Shashank Shet, et al. CodePlan: Repository-level coding using LLMs and planning. arXiv preprint arXiv:2309.12499, 2023.</p>
<p>Suvaansh Bhambri, Byeonghwi Kim, and Jonghyun Choi. Multi-level compositional reasoning for interactive instruction following. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 223-231, 2023.</p>
<p>Bytedance. https://www.marscode.com/, 2024.
Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun Sun, Hao Yu, Guoliang Dong, Artem Aliev, et al. CodeR: Issue resolving with multi-agent and task graphs. arXiv preprint arXiv:2406.01304, 2024.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.</p>
<p>Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2023.</p>
<p>CodeGemma Team. CodeGemma: Open Code Models Based on Gemma. 2024.
Pantazis Deligiannis, Akash Lal, Nikita Mehrotra, and Aseem Rastogi. Fixing rust compilation errors using llms. arXiv preprint arXiv:2308.05177, 2023.</p>
<p>Devin. Introducing Devin, the first AI software engineer. https://www.cognition.ai/blog/ introducing-devin, 2024.</p>
<p>Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges. arXiv preprint arXiv:2402.01680, 2024.</p>
<p>Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. MetaGPT: Meta programming for Multi-Agent Collaborative Framework. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118-9147. PMLR, 2022.</p>
<p>IBM. https://github.com/swe-bench/experiments/tree/main/evaluation/lite/ 20240612_IBM_Research_Agent101, 2024.</p>
<p>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. SWE-bench: Can Language Models Resolve Real-world Github Issues? In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 22199-22213, 2022.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: $9459-9474,2020$.</p>
<p>Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic LLM-agent network: An LLMagent collaboration framework with agent team optimization. arXiv preprint arXiv:2310.02170, 2023.</p>
<p>MASAI. https://github.com/swe-bench/experiments/pull/20, 2024.
Moatless Tools. https://github.com/aorwall/moatless-tools, 2024.
OpenCGS. https://opencsg.com/product?class=StarShip, 2024.
OpenDevin. https://opendevin.github.io/OpenDevin/, 2024.
Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.</p>
<p>Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, et al. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023.</p>
<p>Tal Ridnik, Dedy Kredo, and Itamar Friedman. Code generation with alphacodium: From prompt engineering to flow engineering. arXiv preprint arXiv:2401.08500, 2024.</p>
<p>Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In ICML, volume 202 of Proceedings of Machine Learning Research, pages 31210-31227. PMLR, 2023.</p>
<p>Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>SWE-bench. https://www.swebench.com/, 2024.
Michele Tufano, Anisha Agarwal, Jinu Jang, Roshanak Zilouchian Moghaddam, and Neel Sundaresan. AutoDev: Automated AI-Driven Development. arXiv preprint arXiv:2403.08299, 2024.</p>
<p>Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader?, 2022.</p>
<p>Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better LLM agents. arXiv preprint arXiv:2402.01030, 2024a.</p>
<p>Zhenyu Wang, Enze Xie, Aoxue Li, Zhongdao Wang, Xihui Liu, and Zhenguo Li. Divide and conquer: Language models can plan and self-correct for compositional text-to-image generation. arXiv e-prints, pages arXiv-2401, 2024b.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.</p>
<p>W Eric Wong, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. A survey on software fault localization. IEEE Transactions on Software Engineering, 42(8):707-740, 2016.</p>
<p>Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen LLM applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.</p>
<p>John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWE-agent: Agent computer interfaces enable software engineering language models, 2024a.</p>
<p>Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, et al. If LLM is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents. arXiv e-prints, pages arXiv-2401, 2024b.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. AutoCodeRover: Autonomous program improvement. arXiv preprint arXiv:2404.05427, 2024.</p>
<p>Zhuosheng Zhang, Yao Yao, Aston Zhang, Xiangru Tang, Xinbei Ma, Zhiwei He, Yiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, et al. Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents. arXiv preprint arXiv:2311.11797, 2023.</p>
<p>Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406, 2023.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Equal contribution; names listed in alphabetical order</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
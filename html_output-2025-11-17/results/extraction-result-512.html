<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-512 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-512</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-512</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-64a80a33018a0fdc182b06111e32b2e08e186f6a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/64a80a33018a0fdc182b06111e32b2e08e186f6a" target="_blank">3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Computer Vision</p>
                <p><strong>Paper TL;DR:</strong> 3D-VisTA is a pre-trained Transformer for 3D Vision and Text Alignment that can be easily adapted to various downstream tasks, and achieves state-of-the-art results on various 3D-VL tasks, ranging from visual grounding and dense captioning to question answering and situated reasoning.</p>
                <p><strong>Paper Abstract:</strong> 3D vision-language grounding (3D-VL) is an emerging field that aims to connect the 3D physical world with natural language, which is crucial for achieving embodied intelligence. Current 3D-VL models rely heavily on sophisticated modules, auxiliary losses, and optimization tricks, which calls for a simple and unified model. In this paper, we propose 3D-VisTA, a pre-trained Transformer for 3D Vision and Text Alignment that can be easily adapted to various downstream tasks. 3D-VisTA simply utilizes self-attention layers for both single-modal modeling and multi-modal fusion without any sophisticated task-specific design. To further enhance its performance on 3D-VL tasks, we construct ScanScribe, the first large-scale 3D scene-text pairs dataset for 3D-VL pre-training. ScanScribe contains 2,995 RGB-D scans for 1,185 unique indoor scenes originating from ScanNet and 3R-Scan datasets, along with paired 278K scene descriptions generated from existing 3D-VL tasks, templates, and GPT-3. 3D-VisTA is pre-trained on ScanScribe via masked language/object modeling and scene-text matching. It achieves state-of-the-art results on various 3D-VL tasks, ranging from visual grounding and dense captioning to question answering and situated reasoning. Moreover, 3D-VisTA demonstrates superior data efficiency, obtaining strong performance even with limited annotations during downstream task fine-tuning.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e512.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e512.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-VisTA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified Transformer-based 3D vision-language model that encodes scenes as object tokens (point-cloud features + semantic embeddings + explicit location vectors), fuses them with text via a multimodal transformer, and is pre-trained on large-scale 3D scene–text pairs (ScanScribe) using masked language/object modeling and scene-text matching.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>3D-VisTA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A vanilla Transformer architecture: (1) text encoder initialized from first 4 layers of BERT; (2) scene encoder using PointNet++ to produce object point features f_i, semantic class embedding c_i and location vector l_i combined as o_i = f_i + W_c c_i + W_l l_i; (3) spatially-modulated self-attention in a 4-layer object Transformer; (4) unified multimodal fusion Transformer concatenating text and object tokens with type embeddings; pre-trained on ScanScribe with MLM, MOM, and STM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>3D vision-language tasks (visual grounding, dense captioning, question answering, situated reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Downstream tasks where the model must align language to 3D scenes represented as point-cloud-derived object tokens: locate referred objects (visual grounding), generate captions for objects (dense captioning), answer object- and relation-centered questions (3D QA), and perform situated reasoning for embodied scenarios (SQA3D). The model is fine-tuned with task-specific heads but no auxiliary losses.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation/embodied scene understanding, object-centric reasoning (visual grounding, QA, captioning, situated reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (pairwise object relations, object properties encoded alongside positions)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on ScanScribe (large-scale 3D scene-text pairs) and supervised fine-tuning on downstream 3D-VL datasets</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuning (task loss) after self-supervised pre-training; MLM/MOM/STM pretraining; evaluations with object proposals from detectors or ground-truth masks</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>object tokens combining implicit point-cloud features and explicit location vectors (3D position, length, width, height); pairwise spatial feature tensors s_ij (distance, horizontal/vertical angle sin/cos) that modulate self-attention weights; unified token-level multimodal representations (CLS, text tokens, object tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task-specific metrics: grounding accuracy / Acc@0.25/0.5, EM@1 for ScanQA, CIDEr/BLEU/METEOR/ROUGE for captioning, answer accuracy for SQA3D</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Pre-trained 3D-VisTA achieved large gains over training from scratch and prior SOTA: Nr3D overall 64.2% (Δ +6.7% vs scratch), Sr3D overall 76.4% (Δ +6.8%), ScanRefer overall acc@0.25=50.6% / acc@0.5=45.8% (improvements up to +4.7% / +4.3% vs scratch and +2.7%/+8.1% vs prior SOTA depending on setting), ScanQA EM@1 test w/ objects 27.0% (Δ +1.8% vs scratch), Scan2Cap CIDEr@0.25 71.0 (Δ +4.2), SQA3D average 48.5% (Δ +1.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Strong alignment of language and object tokens enabling improved grounding in scenes with multiple similar objects, better handling of longer queries and spatial relations; pretraining improved spatial relation understanding (distinguishing same-class distractors) and concept grounding (color/shape) and boosted data efficiency (good performance with 30% labels).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Struggles with complex multi-hop spatial reasoning and some cases requiring fine-grained semantic/texture information not robustly captured by point-cloud features; occasional failures on complicated spatial relations despite improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to models such as ViL3DRel, 3DVG-Trans, MVT, 3D-SPS: 3D-VisTA (pre-trained) surpasses several SOTA numbers (examples: Sr3D +3.6% vs ViL3DRel; large gains on Scan2Cap and ScanRefer reported). Also compared scratch vs pre-trained: consistent Δ improvements (see performance_result).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablations show multimodal fusion depth matters (4-layer unified transformer best), and pretraining objectives matter: MLM alone slightly helps QA but harms VG, while adding MOM and STM substantially improves both VG and QA. Pretraining data ablation: using ScanNet→adding 3R-Scan→adding Objaverse progressively improves VG and QA, best when all data included.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Encoding explicit spatial descriptors (location vectors + pairwise spatial features) into object tokens and modulating transformer self-attention with those pairwise features enables stronger object-relational and spatial reasoning in a text–3D fusion model; masked object modeling that preserves positional cues trains the model to infer semantics from position, and scene-text matching promotes global scene–language alignment, collectively improving embodied task performance and data efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e512.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e512.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spatial-modulated Self-Attention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pairwise Spatial Feature Modulation of Self-Attention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An attention mechanism used within the scene/object Transformer that injects explicit pairwise geometric relations (distance and horizontal/vertical angle encodings) into attention scores, allowing the model to attend based on object-to-object spatial relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>3D-VisTA (spatial attention mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-attention augmented with an additive term computed from pairwise spatial features s_ij = [d_ij, sin(theta_h), cos(theta_h), sin(theta_v), cos(theta_v)], mapped by a learned weight vector w and passed through a sigmoid, then combined with the usual QK^T softmax attention to bias attention by geometric relations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>3D object modeling and downstream 3D-VL tasks (visual grounding, QA, situated reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Within-scene object contextualization: the Transformer over object tokens uses geometry-biased attention so that object representations reflect relative positions and orientations among objects, aiding tasks that require spatial relational reasoning about objects in a 3D scene.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational + spatial reasoning in embodied scene understanding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>explicit geometric features derived from 3D point clouds (computed pairwise from object centers and bounding geometry) and learned projection w</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>encoded directly in the attention computation during pre-training and fine-tuning (no prompting); used in masked-object prediction and downstream fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>numeric pairwise spatial tensor S ∈ R^{N×N×5} (distance and angular sin/cos features) that is projected and added to attention logits as a learned geometric bias term; object tokens also include explicit location vectors (position + size).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>downstream task improvements (grounding accuracy, QA EM@1, captioning metrics) and qualitative improvement in spatial relation understanding</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Not isolated as a single-number ablation in the paper, but incorporated in 3D-VisTA which achieves the listed SOTA gains (e.g., Nr3D 64.2% overall, Sr3D 76.4%, ScanRefer acc@0.5 up to 45.8%, ScanQA EM@1 27.0%). Authors state that explicitly encoding pairwise spatial relations improves 3D spatial comprehension and helps downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Improved disambiguation of same-class objects via relational cues, better alignment of viewpoint-dependent descriptions, and improved grounding in scenes where spatial relations are crucial (e.g., left/right/front/behind distinctions).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Complex multi-hop relational queries or cases requiring finer semantic/texture details still cause errors; paper does not claim complete elimination of spatial reasoning failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared implicitly to models that don't inject pairwise spatial features into attention; 3D-VisTA outperforms several prior models that used other mechanisms for relations (e.g., MVT, 3DVG-Trans), but there is no direct numerical isolate-of-component comparison in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>The paper's ablations emphasize pretraining objectives and data quantity; they report that including MOM and STM yields the largest pretraining gains. The specific spatial-attention component is presented as an architecture choice motivated by prior work (and used in final model) but is not ablated away in isolation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Representing pairwise geometry explicitly and injecting it into attention logits is an effective way to make Transformer attention spatially aware in 3D object token space, enabling improved object-relational reasoning without complex task-specific modules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e512.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e512.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Masked Object Modeling (MOM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised pretraining objective that masks object token features (point features + semantic embedding) while keeping positional/location information, forcing the model to predict the masked object's semantic class c from its position and surrounding context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>3D-VisTA (MOM objective)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>During pre-training, 10% of object tokens have their visual/semantic features replaced by a learned mask embedding but their W_l l_i positional encodings are kept; the model is trained to predict the semantic class of the masked objects conditioned on other objects and text.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Pretraining objective to improve downstream visual grounding and question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>The MOM task trains the model to infer object identity from spatial and contextual clues, simulating situations where semantic information is partially missing and must be inferred from object-relational and positional context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>self-supervised representation learning for object-relational and spatial inference</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + spatial (inferring object class from position and relations)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>self-supervision from ScanScribe scene-text pairs and object positions derived from 3D scans</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>masked prediction during self-supervised pre-training; learning via backprop on MOM loss</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>positional cues preserved in object tokens (W_l l_i) provide explicit spatial signals; the model learns to map surrounding object tokens and text cues to predicted semantic classes for masked object tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>improvement in downstream task metrics (visual grounding accuracy and QA EM@1) when MOM is included in pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Ablation analysis shows that adding MOM (together with STM) boosts VG and QA compared to MLM-only or no pretraining; combined MLM+MOM+STM yields best overall performance (see main model gains: e.g., Nr3D +6.7% vs scratch; ScanRefer and ScanQA improvements reported). Exact isolated MOM-only numeric delta is not separately reported, but MOM+STM are stated as providing the greatest improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Enables the model to use positional/location context to infer likely object classes when visual features are masked or ambiguous, improving grounding in presence of occlusion or limited appearance cues.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>If positional context alone is insufficient (e.g., when object identity depends on fine texture or intra-class shape), MOM predictions can fail; MOM cannot substitute for missing visual detail.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to MLM-only pretraining which slightly helps QA but can hurt VG, the addition of MOM (and STM) recovers and improves both VG and QA.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Table and text ablations show: MLM-only slightly benefits QA but hurts VG; adding MOM and STM increases both VG and QA — concluding MOM and STM are important for aligning 3D vision and text. Specific layer-depth and data ablations are provided in the paper (4-layer fusion and more pretraining data improved results).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training the model to predict object semantics from position/context (MOM) encourages the learning of object-relational and spatial priors that transfer to downstream embodied-oriented tasks, particularly visual grounding and QA, improving robustness when appearance cues are missing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e512.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e512.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 usage</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3-generated scene descriptions (used to augment ScanScribe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors used GPT-3 to generate natural, diverse textual descriptions from 3D scene graphs (in addition to template-based generation) to increase textual diversity in the ScanScribe pretraining dataset, and show modest downstream gains from including GPT-3 outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An autoregressive large language model used here in a prompt-based manner to convert object–relation triplets from 3D scene graphs into fluent natural-language scene descriptions; generated descriptions were duplicated to balance proportions during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Dataset augmentation for 3D-VL pretraining (ScanScribe text generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a scene graph (〈object, relation, neighbor〉 triplets), GPT-3 is prompted to create fluent descriptions or summaries of object relations (e.g., 'object is relation to neighbor ... Where is object?'), producing diverse scene descriptions used as pretraining text.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>text generation / dataset augmentation for pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + spatial (encoded in the generated natural language descriptions derived from scene graph relations)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>GPT-3's pretraining on large text corpora; used via prompting to map structured relations into natural language</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot or prompt-based generation (prompt design described in paper; template+GPT-3 mixture used; GPT-3 outputs duplicated for balance)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>natural language descriptions summarizing object relations and scene layout (i.e., textual relational representations), which are then used to align with 3D object representations during model pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>downstream task metric deltas comparing ScanScribe variants with and without GPT-3-generated text (ScanRefer, Sr3D, Nr3D, ScanQA)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Ablation (Table A2) shows adding GPT-3-generated text (in addition to templates) increased Sr3D by +1.0% and Nr3D by +1.5%, with negligible change on ScanRefer and ScanQA in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>GPT-3 generated more natural and diverse textual descriptions than templates, which improved pretraining coverage of relation-language patterns and helped object-relational generalization on some datasets (notably Sr3D/Nr3D).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>GPT-3 generation is used only for data augmentation; it does not directly perform embodied planning or sensorimotor control. Some downstream gains are small and dataset-dependent, and GPT-3 may produce hallucinated or less-precise relation descriptions if prompts are imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to template-only generated text, including GPT-3 outputs produced modest improvements (Sr3D +1.0%, Nr3D +1.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Table A2 reports template-only vs template+GPT-3 pretraining: ScanRefer unchanged, Sr3D improved +1.0, Nr3D +1.5, ScanQA +0.1; authors duplicated GPT-3 outputs 15× to balance proportions during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large LMs (GPT-3) can be used as text-only generators to create diverse natural-language descriptions of spatial and object-relational scene graphs; incorporating such generated text into 3D-VL pretraining datasets can modestly improve downstream object-relational and spatial reasoning performance without any direct sensory input to the language model itself.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Language conditioned spatial relation reasoning for 3d object grounding <em>(Rating: 2)</em></li>
                <li>ScanQA: 3D question answering for spatial scene understanding <em>(Rating: 2)</em></li>
                <li>SQA3D: Situated question answering in 3d scenes <em>(Rating: 2)</em></li>
                <li>Flamingo: a visual language model for few-shot learning <em>(Rating: 1)</em></li>
                <li>Referit3D: Neural listeners for fine-grained 3d object identification in real-world scenes <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-512",
    "paper_id": "paper-64a80a33018a0fdc182b06111e32b2e08e186f6a",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "3D-VisTA",
            "name_full": "3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment",
            "brief_description": "A unified Transformer-based 3D vision-language model that encodes scenes as object tokens (point-cloud features + semantic embeddings + explicit location vectors), fuses them with text via a multimodal transformer, and is pre-trained on large-scale 3D scene–text pairs (ScanScribe) using masked language/object modeling and scene-text matching.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "3D-VisTA",
            "model_size": null,
            "model_description": "A vanilla Transformer architecture: (1) text encoder initialized from first 4 layers of BERT; (2) scene encoder using PointNet++ to produce object point features f_i, semantic class embedding c_i and location vector l_i combined as o_i = f_i + W_c c_i + W_l l_i; (3) spatially-modulated self-attention in a 4-layer object Transformer; (4) unified multimodal fusion Transformer concatenating text and object tokens with type embeddings; pre-trained on ScanScribe with MLM, MOM, and STM.",
            "task_name": "3D vision-language tasks (visual grounding, dense captioning, question answering, situated reasoning)",
            "task_description": "Downstream tasks where the model must align language to 3D scenes represented as point-cloud-derived object tokens: locate referred objects (visual grounding), generate captions for objects (dense captioning), answer object- and relation-centered questions (3D QA), and perform situated reasoning for embodied scenarios (SQA3D). The model is fine-tuned with task-specific heads but no auxiliary losses.",
            "task_type": "navigation/embodied scene understanding, object-centric reasoning (visual grounding, QA, captioning, situated reasoning)",
            "knowledge_type": "spatial + object-relational (pairwise object relations, object properties encoded alongside positions)",
            "knowledge_source": "pre-training on ScanScribe (large-scale 3D scene-text pairs) and supervised fine-tuning on downstream 3D-VL datasets",
            "has_direct_sensory_input": true,
            "elicitation_method": "fine-tuning (task loss) after self-supervised pre-training; MLM/MOM/STM pretraining; evaluations with object proposals from detectors or ground-truth masks",
            "knowledge_representation": "object tokens combining implicit point-cloud features and explicit location vectors (3D position, length, width, height); pairwise spatial feature tensors s_ij (distance, horizontal/vertical angle sin/cos) that modulate self-attention weights; unified token-level multimodal representations (CLS, text tokens, object tokens)",
            "performance_metric": "task-specific metrics: grounding accuracy / Acc@0.25/0.5, EM@1 for ScanQA, CIDEr/BLEU/METEOR/ROUGE for captioning, answer accuracy for SQA3D",
            "performance_result": "Pre-trained 3D-VisTA achieved large gains over training from scratch and prior SOTA: Nr3D overall 64.2% (Δ +6.7% vs scratch), Sr3D overall 76.4% (Δ +6.8%), ScanRefer overall acc@0.25=50.6% / acc@0.5=45.8% (improvements up to +4.7% / +4.3% vs scratch and +2.7%/+8.1% vs prior SOTA depending on setting), ScanQA EM@1 test w/ objects 27.0% (Δ +1.8% vs scratch), Scan2Cap CIDEr@0.25 71.0 (Δ +4.2), SQA3D average 48.5% (Δ +1.8%).",
            "success_patterns": "Strong alignment of language and object tokens enabling improved grounding in scenes with multiple similar objects, better handling of longer queries and spatial relations; pretraining improved spatial relation understanding (distinguishing same-class distractors) and concept grounding (color/shape) and boosted data efficiency (good performance with 30% labels).",
            "failure_patterns": "Struggles with complex multi-hop spatial reasoning and some cases requiring fine-grained semantic/texture information not robustly captured by point-cloud features; occasional failures on complicated spatial relations despite improvements.",
            "baseline_comparison": "Compared to models such as ViL3DRel, 3DVG-Trans, MVT, 3D-SPS: 3D-VisTA (pre-trained) surpasses several SOTA numbers (examples: Sr3D +3.6% vs ViL3DRel; large gains on Scan2Cap and ScanRefer reported). Also compared scratch vs pre-trained: consistent Δ improvements (see performance_result).",
            "ablation_results": "Ablations show multimodal fusion depth matters (4-layer unified transformer best), and pretraining objectives matter: MLM alone slightly helps QA but harms VG, while adding MOM and STM substantially improves both VG and QA. Pretraining data ablation: using ScanNet→adding 3R-Scan→adding Objaverse progressively improves VG and QA, best when all data included.",
            "key_findings": "Encoding explicit spatial descriptors (location vectors + pairwise spatial features) into object tokens and modulating transformer self-attention with those pairwise features enables stronger object-relational and spatial reasoning in a text–3D fusion model; masked object modeling that preserves positional cues trains the model to infer semantics from position, and scene-text matching promotes global scene–language alignment, collectively improving embodied task performance and data efficiency.",
            "uuid": "e512.0",
            "source_info": {
                "paper_title": "3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Spatial-modulated Self-Attention",
            "name_full": "Pairwise Spatial Feature Modulation of Self-Attention",
            "brief_description": "An attention mechanism used within the scene/object Transformer that injects explicit pairwise geometric relations (distance and horizontal/vertical angle encodings) into attention scores, allowing the model to attend based on object-to-object spatial relations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "3D-VisTA (spatial attention mechanism)",
            "model_size": null,
            "model_description": "Self-attention augmented with an additive term computed from pairwise spatial features s_ij = [d_ij, sin(theta_h), cos(theta_h), sin(theta_v), cos(theta_v)], mapped by a learned weight vector w and passed through a sigmoid, then combined with the usual QK^T softmax attention to bias attention by geometric relations.",
            "task_name": "3D object modeling and downstream 3D-VL tasks (visual grounding, QA, situated reasoning)",
            "task_description": "Within-scene object contextualization: the Transformer over object tokens uses geometry-biased attention so that object representations reflect relative positions and orientations among objects, aiding tasks that require spatial relational reasoning about objects in a 3D scene.",
            "task_type": "object-relational + spatial reasoning in embodied scene understanding",
            "knowledge_type": "spatial + object-relational",
            "knowledge_source": "explicit geometric features derived from 3D point clouds (computed pairwise from object centers and bounding geometry) and learned projection w",
            "has_direct_sensory_input": true,
            "elicitation_method": "encoded directly in the attention computation during pre-training and fine-tuning (no prompting); used in masked-object prediction and downstream fine-tuning",
            "knowledge_representation": "numeric pairwise spatial tensor S ∈ R^{N×N×5} (distance and angular sin/cos features) that is projected and added to attention logits as a learned geometric bias term; object tokens also include explicit location vectors (position + size).",
            "performance_metric": "downstream task improvements (grounding accuracy, QA EM@1, captioning metrics) and qualitative improvement in spatial relation understanding",
            "performance_result": "Not isolated as a single-number ablation in the paper, but incorporated in 3D-VisTA which achieves the listed SOTA gains (e.g., Nr3D 64.2% overall, Sr3D 76.4%, ScanRefer acc@0.5 up to 45.8%, ScanQA EM@1 27.0%). Authors state that explicitly encoding pairwise spatial relations improves 3D spatial comprehension and helps downstream performance.",
            "success_patterns": "Improved disambiguation of same-class objects via relational cues, better alignment of viewpoint-dependent descriptions, and improved grounding in scenes where spatial relations are crucial (e.g., left/right/front/behind distinctions).",
            "failure_patterns": "Complex multi-hop relational queries or cases requiring finer semantic/texture details still cause errors; paper does not claim complete elimination of spatial reasoning failure modes.",
            "baseline_comparison": "Compared implicitly to models that don't inject pairwise spatial features into attention; 3D-VisTA outperforms several prior models that used other mechanisms for relations (e.g., MVT, 3DVG-Trans), but there is no direct numerical isolate-of-component comparison in the paper.",
            "ablation_results": "The paper's ablations emphasize pretraining objectives and data quantity; they report that including MOM and STM yields the largest pretraining gains. The specific spatial-attention component is presented as an architecture choice motivated by prior work (and used in final model) but is not ablated away in isolation.",
            "key_findings": "Representing pairwise geometry explicitly and injecting it into attention logits is an effective way to make Transformer attention spatially aware in 3D object token space, enabling improved object-relational reasoning without complex task-specific modules.",
            "uuid": "e512.1",
            "source_info": {
                "paper_title": "3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "MOM",
            "name_full": "Masked Object Modeling (MOM)",
            "brief_description": "A self-supervised pretraining objective that masks object token features (point features + semantic embedding) while keeping positional/location information, forcing the model to predict the masked object's semantic class c from its position and surrounding context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "3D-VisTA (MOM objective)",
            "model_size": null,
            "model_description": "During pre-training, 10% of object tokens have their visual/semantic features replaced by a learned mask embedding but their W_l l_i positional encodings are kept; the model is trained to predict the semantic class of the masked objects conditioned on other objects and text.",
            "task_name": "Pretraining objective to improve downstream visual grounding and question answering",
            "task_description": "The MOM task trains the model to infer object identity from spatial and contextual clues, simulating situations where semantic information is partially missing and must be inferred from object-relational and positional context.",
            "task_type": "self-supervised representation learning for object-relational and spatial inference",
            "knowledge_type": "object-relational + spatial (inferring object class from position and relations)",
            "knowledge_source": "self-supervision from ScanScribe scene-text pairs and object positions derived from 3D scans",
            "has_direct_sensory_input": true,
            "elicitation_method": "masked prediction during self-supervised pre-training; learning via backprop on MOM loss",
            "knowledge_representation": "positional cues preserved in object tokens (W_l l_i) provide explicit spatial signals; the model learns to map surrounding object tokens and text cues to predicted semantic classes for masked object tokens.",
            "performance_metric": "improvement in downstream task metrics (visual grounding accuracy and QA EM@1) when MOM is included in pretraining",
            "performance_result": "Ablation analysis shows that adding MOM (together with STM) boosts VG and QA compared to MLM-only or no pretraining; combined MLM+MOM+STM yields best overall performance (see main model gains: e.g., Nr3D +6.7% vs scratch; ScanRefer and ScanQA improvements reported). Exact isolated MOM-only numeric delta is not separately reported, but MOM+STM are stated as providing the greatest improvements.",
            "success_patterns": "Enables the model to use positional/location context to infer likely object classes when visual features are masked or ambiguous, improving grounding in presence of occlusion or limited appearance cues.",
            "failure_patterns": "If positional context alone is insufficient (e.g., when object identity depends on fine texture or intra-class shape), MOM predictions can fail; MOM cannot substitute for missing visual detail.",
            "baseline_comparison": "Compared to MLM-only pretraining which slightly helps QA but can hurt VG, the addition of MOM (and STM) recovers and improves both VG and QA.",
            "ablation_results": "Table and text ablations show: MLM-only slightly benefits QA but hurts VG; adding MOM and STM increases both VG and QA — concluding MOM and STM are important for aligning 3D vision and text. Specific layer-depth and data ablations are provided in the paper (4-layer fusion and more pretraining data improved results).",
            "key_findings": "Training the model to predict object semantics from position/context (MOM) encourages the learning of object-relational and spatial priors that transfer to downstream embodied-oriented tasks, particularly visual grounding and QA, improving robustness when appearance cues are missing.",
            "uuid": "e512.2",
            "source_info": {
                "paper_title": "3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3 usage",
            "name_full": "GPT-3-generated scene descriptions (used to augment ScanScribe)",
            "brief_description": "The authors used GPT-3 to generate natural, diverse textual descriptions from 3D scene graphs (in addition to template-based generation) to increase textual diversity in the ScanScribe pretraining dataset, and show modest downstream gains from including GPT-3 outputs.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": null,
            "model_description": "An autoregressive large language model used here in a prompt-based manner to convert object–relation triplets from 3D scene graphs into fluent natural-language scene descriptions; generated descriptions were duplicated to balance proportions during pretraining.",
            "task_name": "Dataset augmentation for 3D-VL pretraining (ScanScribe text generation)",
            "task_description": "Given a scene graph (〈object, relation, neighbor〉 triplets), GPT-3 is prompted to create fluent descriptions or summaries of object relations (e.g., 'object is relation to neighbor ... Where is object?'), producing diverse scene descriptions used as pretraining text.",
            "task_type": "text generation / dataset augmentation for pretraining",
            "knowledge_type": "object-relational + spatial (encoded in the generated natural language descriptions derived from scene graph relations)",
            "knowledge_source": "GPT-3's pretraining on large text corpora; used via prompting to map structured relations into natural language",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot or prompt-based generation (prompt design described in paper; template+GPT-3 mixture used; GPT-3 outputs duplicated for balance)",
            "knowledge_representation": "natural language descriptions summarizing object relations and scene layout (i.e., textual relational representations), which are then used to align with 3D object representations during model pretraining",
            "performance_metric": "downstream task metric deltas comparing ScanScribe variants with and without GPT-3-generated text (ScanRefer, Sr3D, Nr3D, ScanQA)",
            "performance_result": "Ablation (Table A2) shows adding GPT-3-generated text (in addition to templates) increased Sr3D by +1.0% and Nr3D by +1.5%, with negligible change on ScanRefer and ScanQA in reported experiments.",
            "success_patterns": "GPT-3 generated more natural and diverse textual descriptions than templates, which improved pretraining coverage of relation-language patterns and helped object-relational generalization on some datasets (notably Sr3D/Nr3D).",
            "failure_patterns": "GPT-3 generation is used only for data augmentation; it does not directly perform embodied planning or sensorimotor control. Some downstream gains are small and dataset-dependent, and GPT-3 may produce hallucinated or less-precise relation descriptions if prompts are imperfect.",
            "baseline_comparison": "Compared to template-only generated text, including GPT-3 outputs produced modest improvements (Sr3D +1.0%, Nr3D +1.5%).",
            "ablation_results": "Table A2 reports template-only vs template+GPT-3 pretraining: ScanRefer unchanged, Sr3D improved +1.0, Nr3D +1.5, ScanQA +0.1; authors duplicated GPT-3 outputs 15× to balance proportions during pretraining.",
            "key_findings": "Large LMs (GPT-3) can be used as text-only generators to create diverse natural-language descriptions of spatial and object-relational scene graphs; incorporating such generated text into 3D-VL pretraining datasets can modestly improve downstream object-relational and spatial reasoning performance without any direct sensory input to the language model itself.",
            "uuid": "e512.3",
            "source_info": {
                "paper_title": "3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2
        },
        {
            "paper_title": "Language conditioned spatial relation reasoning for 3d object grounding",
            "rating": 2
        },
        {
            "paper_title": "ScanQA: 3D question answering for spatial scene understanding",
            "rating": 2
        },
        {
            "paper_title": "SQA3D: Situated question answering in 3d scenes",
            "rating": 2
        },
        {
            "paper_title": "Flamingo: a visual language model for few-shot learning",
            "rating": 1
        },
        {
            "paper_title": "Referit3D: Neural listeners for fine-grained 3d object identification in real-world scenes",
            "rating": 1
        }
    ],
    "cost": 0.01719925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment</h1>
<p>Ziyu Zhu ${ }^{1 *}$ Xiaojian Ma ${ }^{2}$ Yixin Chen ${ }^{2}$ Zhidong Deng ${ }^{1 \boxtimes}$ Siyuan Huang ${ }^{2 \boxtimes}$ Qing Li ${ }^{2 \boxtimes}$<br>${ }^{1}$ Tsinghua University ${ }^{2}$ National Key Laboratory of General Artificial Intelligence, BIGAI, China 3d-vista.github.io</p>
<h4>Abstract</h4>
<p>3D vision-language grounding (3D-VL) is an emerging field that aims to connect the 3D physical world with natural language, which is crucial for achieving embodied intelligence. Current 3D-VL models rely heavily on sophisticated modules, auxiliary losses, and optimization tricks, which calls for a simple and unified model. In this paper, we propose 3D-VisTA, a pre-trained Transformer for 3D Vision and Text Alignment that can be easily adapted to various downstream tasks. 3D-VisTA simply utilizes self-attention layers for both single-modal modeling and multi-modal fusion without any sophisticated task-specific design. To further enhance its performance on 3D-VL tasks, we construct ScanScribe, the first large-scale 3D scene-text pairs dataset for 3D-VL pre-training. ScanScribe contains 2,995 RGB$D$ scans for 1,185 unique indoor scenes originating from ScanNet and 3R-Scan datasets, along with paired 278K scene descriptions generated from existing 3D-VL tasks, templates, and GPT-3. 3D-VisTA is pre-trained on ScanScribe via masked language/object modeling and scene-text matching. It achieves state-of-the-art results on various 3D-VL tasks, ranging from visual grounding and dense captioning to question answering and situated reasoning. Moreover, 3D-VisTA demonstrates superior data efficiency, obtaining strong performance even with limited annotations during downstream task fine-tuning.</p>
<h2>1. Introduction</h2>
<p>Aligning the 3D physical world with natural language is a crucial step towards embodied artificial intelligence [18, 26, 37], where intelligent agents can understand and further execute human instructions in the real world [5, 29]. Recently, 3D vision-language (3D-VL) tasks have attracted growing interest [19], including 3D visual grounding [8, 1], dense captioning [11], grammar learning [23], question answering $[3,56]$, and situated reasoning [36].</p>
<p>However, most of the models developed for 3D-VL only</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overall framework of our 3D-VisTA pipeline. We collect diverse prompts, scene graphs, 3D scans, and objects to construct ScanScribe dataset. Through self-supervised pre-training, 3D-VisTA supports various downstream tasks including 3D visual grounding, dense captioning, question answering, and situated reasoning.
focus on one or two of these 3D-VL tasks and employ taskspecific designs [7, 3, 36, 35, 10]. For instance, 3D-SPS [35] and BUTD-DETR [27] progressively discover the target object by attending VL features and detecting objects in each layer. 3DVG [55], MVT [24], and ViL3DRel [10] improve 3D visual grounding by explicitly infusing spatial relation information into the model design. 3DJCG [7] jointly learns 3D dense captioning and visual grounding via a shared 3D object proposal module [16] with two separate task-specific heads [7]. Additionally, training these models often requires manually specified auxiliary losses (e.g., 3D object detection/classification and text classification [35, 24, 7, 3, 36]) or optimization tricks (e.g., knowledge distillation [4, 53] ). The lack of a simple and unified approach creates a significant gap in developing a general-purpose 3D-VL model.</p>
<p>To fill such gap, we introduce 3D-VisTA, a Transformerbased model for 3D Vision and Text Alignment that can be easily adapted to various downstream tasks. Unlike previous models that design sophisticated task-specific modules,</p>
<p>we simply utilize a vanilla self-attention transformer [46] for both single-modal modeling and multi-modal fusion in the 3D-VisTA. As a general approach to further enhance 3D spatial comprehension [10, 55, 7], we explicitly encode the pairwise spatial relations between objects into the selfattention weights for 3D object modeling.</p>
<p>Inspired by the success of large-scale pre-training in NLP [15, 41, 42, 6, 52, 31], CV [22, 17, 21, 25, 38], and 2D-VL [30, 2, 34, 40], we propose to pre-train 3D-VisTA on 3D scene-text data, aiming for better performances on 3D-VL tasks. To this end, we construct ScanScribe, the first large-scale 3D scene-text pairs dataset for 3D-VL pretraining. We first collect RGB-D scans of indoor scenes from ScanNet [12] and 3R-Scan [48] datasets. We also randomly replace some objects in the scene with objects from the $\mathrm{Ob}-$ javerse 3D object database [13] based on their categories, in order to increase object diversity. To obtain the text, we transform the text from existing datasets based on ScanNet into scene descriptions, including the question-answer pairs from ScanQA [3] and the referring expressions from ScanRefer [8] and ReferIt3D [1]. We further leverage the scene graph annotations [51] of scans from 3R-Scan, and adopt both templates and GPT-3 [6] to generate scene descriptions from their scene graphs. In total, ScanScribe contains 278K 3D scene-text pairs for 2,995 RGB-D scans of 1,185 indoor scenes, with 56.1 K unique object instances.</p>
<p>We pre-train 3D-VisTA on the proposed ScanScribe dataset. Our pre-training tasks include masked language modeling, masked object modeling, and scene-text matching. Notably, similar objectives are widely adopted in 2D-VL yet rarely explored in the 3D-VL domain. The proposed pretraining procedure effectively learns the alignment between 3D point clouds and texts, which eliminates the need for auxiliary losses and optimization tricks in downstream task fine-tuning. On six challenging 3D-VL tasks, ranging from visual grounding (i.e., ScanRefer [8], Nr3D/Sr3D [1]) and dense captioning (i.e., Scan2Cap [11]) to question answering (i.e., ScanQA [3]) and situated reasoning (i.e., SQA3D [36]), fine-tuned 3D-VisTA raises the SOTA results on ScanRefer by $8.1 \%$ (acc@0.5), on Sr3D by $3.6 \%$, on Scan2Cap by $10.1 \%(\mathrm{C} @ 0.25)$, on ScanQA by $3.5 \% / 2.1 \%$ (EM@1), and on SQA3D by $1.9 \%$. Moreover, 3D-VisTA demonstrates superior data efficiency, obtaining strong results with only $30 \%$ of the annotations for these downstream tasks.</p>
<p>Our main contributions can be summarized as follows:</p>
<ul>
<li>We propose 3D-VisTA, a simple and unified Transformer for aligning 3D vision and text. The proposed Transformer simply utilizes the self-attention mechanism, without any complex task-specific design.</li>
<li>We construct ScanScribe, a large-scale 3D-VL pre-training dataset that contains 278 K 3D scene-text pairs for 2,995 RGB-D scans of 1,185 unique indoor scenes.</li>
<li>We introduce a self-supervised pre-training scheme for 3D-</li>
</ul>
<p>VL, with masked language/object modeling and scene-text matching. It effectively learns the 3D point cloud and text alignment and further simplifies and improves downstream task fine-tuning.</p>
<ul>
<li>We fine-tune 3D-VisTA and achieve state-of-the-art performances on various 3D-VL tasks, ranging from visual grounding and dense captioning to question answering and situated reasoning. 3D-VisTA also demonstrates superior data efficiency, obtaining strong results even with limited annotations.</li>
</ul>
<h2>2. Related Work</h2>
<p>3D Vision-language Learning. Recently, there has been growing interest in 3D vision-language (3D-VL) learning. Unlike traditional scene understanding, 3D-VL tasks connect the physical world to natural language, which is crucial for achieving embodied intelligence [18]. In this emerging area, Chen et al. [8] and Achlioptas et al. [1] concurrently introduce ScanRefer and ReferIt3D datasets for benchmarking natural language grounding to 3D object properties and relations. Besides 3D visual grounding, Azuma et al. [3] develop a 3D question-answering dataset named ScanQA that requires a model to answer a question about objects and their relations given a 3D scene. More recently, Ma et al. [36] propose a situated reasoning task called SQA3D for embodied scene understanding in 3D scenes.</p>
<p>Several models have been proposed for these benchmarks [8, 1, 35, 27, 55, 24, 10, 20, 43]. Notably, 3D-SPS [35] and BUTD-DETR [27] progressively discover the target object by leveraging cross attention mechanism and language guidance. 3DVG [55], MVT [24], and ViL3DRel [10] tackle 3D visual grounding by explicitly infusing spatial relation information into their models. Although these works have achieved impressive results in bridging 3D vision and language, they still rely heavily on task-specific knowledge in model design [55, 24, 10] and sophisticated optimization techniques [10, 27, 35]. In contrast, the proposed 3D-VisTA unifies visual grounding, question-answering, and situated reasoning through a simple Transformer-based architecture. Training 3D-VisTA is also straightforward, without requiring any auxiliary losses or sophisticated optimization techniques. Refer to Table 1 for a detailed comparison between 3DVisTA and other 3D-VL models w.r.t. task, auxiliary Loss, and architecture.
Large-scale Pre-training. In recent years, large-scale pretraining has become a cornerstone of natural language processing (NLP), computer vision (CV), and 2D vision-and-language (2D-VL) domains. The introduction of the transformer-based architecture [47], especially BERT [15] and GPT [41, 42, 6], has led to significant improvements in various NLP tasks. The success of these models has led to the development of more advanced pre-training techniques such as XLNet [52] and RoBERTa [31]. These models have</p>
<p>Table 1: The comparison between 3D-VisTA and other models w.r.t. tasks, auxiliary losses, and task-specific architectures.“VG” stands for visual grounding, “QA” for question answering, “SR” for situation reasoning, “DC” for dense captioning. “DET” stands for object detection loss, “KD” for knowledge distillation loss, “O-CLS” for object classification loss, and “T-CLS” for text classification loss. “CA” stands for cross attention, “2D” for 2D features, “MV” for multi-view features, and “LC” for language-conditioned modules.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Task</th>
<th>Auxiliary loss</th>
<th></th>
<th></th>
<th></th>
<th>Architecture</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>DET</td>
<td>KD</td>
<td>O-CLS</td>
<td>T-CLS</td>
<td>CA</td>
<td>2D</td>
<td>MV</td>
<td>LC</td>
</tr>
<tr>
<td>MVT [24]</td>
<td>VG</td>
<td></td>
<td></td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td>✓</td>
<td></td>
</tr>
<tr>
<td>3D JCG [7]</td>
<td>VG, DC</td>
<td>✓</td>
<td></td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ViL3DRel [10]</td>
<td>VG</td>
<td></td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td></td>
<td>✓</td>
</tr>
<tr>
<td>ScanQA [3]</td>
<td>QA</td>
<td>✓</td>
<td></td>
<td>✓</td>
<td></td>
<td>✓</td>
<td>✓</td>
<td></td>
<td></td>
</tr>
<tr>
<td>SQA3D [36]</td>
<td>SR</td>
<td>✓</td>
<td></td>
<td></td>
<td></td>
<td>✓</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>3D-VisTA (ours)</td>
<td>VG,QA,SR,DC</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
</tr>
</tbody>
</table>
<p>Table 2: The comparison between ScanScribe and other 3D-VL datasets. “VG” stands for Visual Grounding, “QA” for Question Answering, “SR” for Situated Reasoning, and “PT” for Pre-training. “Vocab.” denotes the text vocabulary size.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Task</th>
<th>Size</th>
<th>Vocab.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Nr3D [1]</td>
<td>VG</td>
<td>30.0K</td>
<td>2,986</td>
</tr>
<tr>
<td>Sr3D [1]</td>
<td>VG</td>
<td>90.5K</td>
<td>158</td>
</tr>
<tr>
<td>ScanRefer [8]</td>
<td>VG</td>
<td>36.7K</td>
<td>4,197</td>
</tr>
<tr>
<td>ScanQA [3]</td>
<td>QA</td>
<td>26.5K</td>
<td>3,357</td>
</tr>
<tr>
<td>SQA3D [36]</td>
<td>SR</td>
<td>33.4K</td>
<td>4,535</td>
</tr>
<tr>
<td>ScanScribe</td>
<td>PT</td>
<td>278.0K</td>
<td>8,197</td>
</tr>
</tbody>
</table>
<p>achieved state-of-the-art performance on a wide range of NLP tasks, including text classification, question answering, and language generation. The most successful pre-training approach in CV is the ImageNet [14] pre-training, which has been used as a starting point for a wide range of downstream tasks such as object detection and image segmentation. Recently, the introduction of transformer-based models such as ViT [17] and Swin Transformer [32] has led to significant improvements in various CV tasks. The field of 2D-VL has also seen significant advancements due to pre-training techniques. In particular, the introduction of the ViLBERT [34] and LXMERT [45] models has led to state-of-the-art performance on tasks such as visual question answering and image captioning. More recently, the development of CLIP [40], ALIGN [50], and Flamingo [2] has shown that large-scale pre-training on image-text pairs leads to better cross-modal understanding and the emerge of in-context learning in a zero-shot or few-shot manner.</p>
<p>Although large-scale pre-training has become a crucial technique in NLP, CV, and 2D-VL, it has rarely been explored in 3D-VL. [7, 9] explore multi-task learning of visual grounding and dense captioning, and then further fine-tune their models on each task. The exploration of 3D-VL pretraining may be hindered by the lack of a large-scale pretraining dataset. Therefore, we construct ScanScribe, the first large-scale 3D scene-text pairs dataset for 3D-VL pretraining. As shown in Table 2, ScanScribe is much larger than existing 3D-VL datasets and also has more diverse text. Pre-training 3D-VisTA on ScanScribe has led to significant improvements on 3D-VL tasks, so we believe ScanScribe can fuel the exploration of 3D-VL pre-training in the future.</p>
<h2>3 3D-VisTA</h2>
<p>In this section, we introduce 3D-VisTA, a simple and unified Transformer for aligning 3D scenes and text. As illustrated by Fig. 2, 3D-VisTA takes a pair of scene point cloud and sentence as input. It first encodes the sentence via a text encoding module and processes the point cloud via a scene encoding module. Then the text and 3D object tokens are fused by a multi-modal fusion module to capture the correspondence between 3D objects and text. 3D-VisTA is pre-trained using self-supervised learning and can be easily fine-tuned to various downstream tasks. Next, we describe each module in detail.</p>
<h3>3.1 Text Encoding</h3>
<p>We adopt a four-layer Transformer to encode the sentence $S$ into a sequence of text tokens $\left{w_{c b}, w_{1}, w_{2}, \cdots, w_{M}\right}$, where $w_{c b}$ is a special classification token ([CLS]) and $M$ is the sentence length. This text encoding module is initialized by the first four layers of a pre-trained BERT [15].</p>
<h3>3.2 Scene Encoding</h3>
<p>Given the point cloud of a 3D scene, we first use segmentation masks to break down the scene into a bag of objects. The segmentation masks can be either obtained from ground truth or instance segmentation models [16, 28, 44]. For each object, we sample 1024 points and normalize their coordinates into a unit ball. Then the object point cloud is fed into PointNet++ [39] to obtain its point features and semantic class. We compose the point features $f_{i}$, the semantic class embedding $c_{i}$, and the location $l_{i}$ (i.e., 3D position, length, width, height) as the representation of the object token $i$ :</p>
<p>$$
o_{i}=f_{i}+W_{c} c_{i}+W_{l} l_{i}, i=1,2, \ldots, N
$$</p>
<p>where $W_{c}$ and $W_{l}$ are additional projection matrices to map $c_{i}$ and $l_{i}$ into the same dimension as $f_{i}$.</p>
<p>To further provide a contextual representation of objects, we capture the object-to-object interactions by infusing object tokens into a four-layer Transformer. Motivated by previous works [55, 24, 10], we explicitly encode the pairwise spatial relations of objects into the Transformer (Spatial transformer in Fig. 2). More specifically, we follow [10] to define the pairwise spatial features for the object pair $i, j$ :</p>
<p>$$
s_{i j}=\left[d_{i j}, \sin \left(\theta_{h}\right), \cos \left(\theta_{h}\right), \sin \left(\theta_{v}\right), \cos \left(\theta_{v}\right)\right]
$$</p>
<p>where $d_{i j}$ is the Euclidean distance and $\theta_{h}, \theta_{v}$ are the horizontal and vertical angles of the line connecting the centers</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The model architecture of our 3D-VisTA, which includes text encoding, scene encoding, and multi-modal fusion modules. 3D-VisTA is pre-trained by self-supervised learning objectives, which include masked language modeling, masked object modeling, and scene-text matching. Pre-trained 3D-VisTA can be easily adapted to various downstream tasks by adding lightweight task heads without task-specific design like auxiliary losses and optimization tricks.</p>
<p>of objects <em>i, j</em>. The pairwise spatial features <em>S</em> = [<em>si</em><sup><em>i</em></sup>] ∈ ℝ<sup><em>N</em>×<em>N</em>×5</sup> are used to modulate the attention weights of the self-attention layers in the Transformer:</p>
<p>Attn(Q, K, V, S) = softmax(QKT/2) + logσ(Sw)/V,</p>
<p>where <em>w</em> ∈ ℝ<sup>5</sup> is used to map the spatial features to the attention scores and σ is the sigmoid function.</p>
<h3>3.3. Multi-modal Fusion</h3>
<p>We simply concatenate the text and the 3D object tokens and send them to a <em>L</em>-layer Transformer (<em>Unified transformer</em> in Fig. 2) for multi-modal fusion. Learnable type embeddings are added to the tokens to differentiate text and 3D objects. We denote the output of the multi-modal fusion module as {<strong>w</strong><sub><strong>c</strong>h</sub><strong>, </strong>w<strong><sub>1:M</sub></strong>, <strong>o</strong><sub>1:N</sub>} for [CLS], text tokens, and 3D object tokens, respectively.</p>
<h3>3.4. Self-supervised Pre-training</h3>
<p>To learn the 3D scene and text alignment in a self-supervised manner, we pre-train 3D-VisTA on 3D scene-text pairs via the following proxy tasks:</p>
<p><strong>Masked Language Modeling (MLM).</strong> We follow the BERT pre-training [15] to perform MLM: (1) 15% of the text tokens are randomly chosen; (2) 80% of the time: replace these tokens with [MASK]; (3) 10% of the time: replace these tokens with some random text tokens; (4) 10% of the time: these tokens remain unchanged. The model is trained to predict the masked text tokens given the remaining text and 3D object tokens:</p>
<p>L<sub>MLM</sub> = -E<sub>(<strong>w</strong>,<strong>o</strong>)</sub> ∼ <em>D</em> log <em>P</em><sub>θ</sub>(<strong>w</strong><sub><strong>m</strong></sub> | <strong>w</strong><sub>⊳<strong>m</strong></sub>, <strong>o</strong>).</p>
<p><strong>Masked Object Modeling (MOM).</strong> Similar to MLM, we mask out 10% of 3D object tokens. However, we mask a 3D object token by only replacing its point features and semantic embedding (<em>i.e</em>., "<em>f<sub>i</sub></em> + <em>W<sub>c</sub>c<sub>i</sub></em>" in Eq. (1)) with a learnable mask embedding but keep its positional information (<em>i.e</em>., "<em>W<sub>l</sub>l<sub>i</sub></em>" in Eq. (1)) unchanged. The model is trained to utilize the position clue of the masked object to predict its semantic class <em>c</em> given the remaining 3D objects and text:</p>
<p>L<sub>MOM</sub> = -E<sub>(<strong>w</strong>,<strong>o</strong>)</sub> ∼ <em>D</em> log <em>P</em><sub>θ</sub>(<em>c</em>(<strong>o</strong><sub><strong>m</strong></sub>) | <strong>o</strong><sub>⊳<strong>m</strong></sub>, <strong>w</strong>).</p>
<p><strong>Scene-Text Matching (STM).</strong> While masked language and object modeling enable local text-object alignment in a fine-grained granularity, we also perform scene-text matching to enhance the global fusion of scene and text, which we find very beneficial for downstream question-answering tasks. More specifically, we extract the output corresponds to [CLS] as the global representation of the input scene-text pair, and feed it into a two-layer MLP to predict if the scene and the text are matched:</p>
<p>L<sub>STM</sub> = -E<sub>(<strong>w</strong>,<strong>o</strong>)</sub> ∼ <em>D</em> log <em>P</em><sub>θ</sub>(<em>y</em> | <strong>w</strong>, <strong>o</strong>).</p>
<p>In practice, 30% of the samples in a training batch are negative pairs, created by replacing the scene point cloud or text with a randomly selected sample.</p>
<p><strong>Final loss.</strong> Our final pre-training objective is obtained by simply adding the losses of the proxy tasks above:</p>
<p>L<sub>pre-train</sub> = L<sub>MLM</sub> + L<sub>MOM</sub> + L<sub>STM</sub>.</p>
<p>Notably, the proposed pre-training scheme is self-supervised and task-agnostic, unlike the supervised multi-task learning used in previous work [7] that requires task supervision.</p>
<p>Table 3: The composition of ScanScribe. ${ }^{*}$ We only use Objaverse to provide candidate object replacement for the 3D scenes in other two datasets; thus no scene-text pair is generated.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Source</th>
<th style="text-align: center;">3D</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Text</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Scene-Text</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Scan</td>
<td style="text-align: center;">Scene</td>
<td style="text-align: center;">Object</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Template</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Pairs</td>
</tr>
<tr>
<td style="text-align: left;">ScanNet</td>
<td style="text-align: center;">1,513</td>
<td style="text-align: center;">707</td>
<td style="text-align: center;">36.2 K</td>
<td style="text-align: center;">93.2 K</td>
<td style="text-align: center;">90.5 K</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">183.7 K</td>
</tr>
<tr>
<td style="text-align: left;">3R-Scan</td>
<td style="text-align: center;">1,482</td>
<td style="text-align: center;">478</td>
<td style="text-align: center;">13.6 K</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">89.6 K</td>
<td style="text-align: center;">4.7 K</td>
<td style="text-align: center;">94.3 K</td>
</tr>
<tr>
<td style="text-align: left;">Objaverse ${ }^{*}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6.3 K</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">ScanScribe</td>
<td style="text-align: center;">2,995</td>
<td style="text-align: center;">1,185</td>
<td style="text-align: center;">56.1 K</td>
<td style="text-align: center;">93.2 K</td>
<td style="text-align: center;">180.1 K</td>
<td style="text-align: center;">4.7 K</td>
<td style="text-align: center;">278.0 K</td>
</tr>
</tbody>
</table>
<h3>3.5 Downstream Task Finetuning</h3>
<p>The pre-trained 3D-VisTA can be easily adapted to various 3D-VL tasks by adding lightweight task heads. More specifically, we fine-tune 3D-VisTA on the following tasks: 3D Visual Grounding tasks a model to locate a target object in a 3D scene from a referring expression. To find the referred object, we apply a two-layer MLP to each object token $\mathbf{o}<em 1:="1:" M="M">{i}$, and obtain the probability of the object being referred to. The model is fine-tuned using the cross-entropy loss.
3D Dense Captioning is introduced by [11] to test a model's ability of detecting and describing objects in a 3D scene. Following [30], we take $\mathbf{w}</em>$ and predict text tokens autoregressively to generate a sentence. The model is fine-tuned using cross-entropy loss.
3D Question Answering requires a model to answer an object-related question given a 3D scene. Following [3], we feed the text tokens $\mathbf{w}<em 1:="1:" N="N">{1: M}$ and the object tokens $\mathbf{o}</em>$ into a modular co-attention network (MCAN) [54] to produce answers. The model is fine-tuned using the QA loss and the object localization loss.
3D Situated Reasoning is recently proposed by [36] to benchmark the 3D scene understanding of embodied agents. To adapt 3D-VisTA to this task, we concatenate the situation description and the question into a single input sentence. The answer classification is similar to the 3D question answering task. The model is fine-tuned using the answer loss.</p>
<p>In general, we find adapting 3D-VisTA to these downstream tasks much simpler than previous methods [8, 24, $10,3,36]$, as 3D-VisTA is simply fine-tuned using the task loss only, without the need for any auxiliary losses (e.g., sentence/object classification loss [8, 3]) or optimization tricks (e.g., multi-view aggregation [24] and knowledge distillation [10]). This makes 3D-VisTA a more unified and general-purpose 3D-VL model.</p>
<h2>4 ScanScribe</h2>
<p>In recent years, large-scale pre-training has been widely used to improve the performance on downstream tasks in CV [49], NLP [15], and 2D-VL [30, 45]. However, largescale pre-training has barely been touched in the 3D-VL domain, possibly due to the lack of pre-training datasets for 3D-VL. To facilitate the exploration of 3D-VL pre-training, we build a large-scale 3D scene-text pairs dataset, named</p>
<p>ScanScribe. As illustrated in Table 3, the construction of 3D scene-text pairs in ScanScribe comprises two parts:
3D scenes. We collect RGB-D scans of indoor scenes from ScanNet [12] and 3R-Scan [48]. To increase the diversity of 3D objects in these scenes, $10 \%$ of the object instances in each scene are randomly replaced by objects from the Objaverse 3D object database[13] based on their categories. For each ScanNet and 3R-Scan object category, we download about 40 object instances from Objaverse as candidate object replacements. As a result, we collect 2,995 RGB-D scans of 1,185 indoor scenes, with 56.1 K unique object instances.
Text. For the scans from ScanNet, we transform the text from existing datasets based on ScanNet into scene descriptions, including the question-answer pairs from ScanQA [3] and the referring expressions from ScanRefer [8] and ReferIt3D [1]. For the scans from 3RScan, we adopt both templates and GPT-3 [6] to generate scene descriptions based on their scene graph annotations [51]. Specifically, for each object, we first extract all the 〈object, relation, neighbor〉 triplets from the scene graph. We then use the template "This is a object, a neighbor is relation to object" to generate the descriptions. Note that we only choose objects with fewer than 7 neighbors in a template-based generation. We further explore using GPT-3 to generate the descriptions with the following prompt "object is relation to neighbor ...(repeat until all the neighbors have been used). Where is object? or Summarize the scene." Ultimately, 278K scene descriptions are generated for the collected 3D scenes.</p>
<h2>5 Experiments</h2>
<h3>5.1 Experimental Settings</h3>
<p>Implementation Details. The pre-training runs for 30 epochs with a batch size of 128 . We use the AdamW [33] optimizer with $\beta_{1}=0.9, \beta_{2}=0.98$. The learning rate is set to $1 e^{-4}$, with a warmup of 3,000 steps, and cosine decay. During pre-training, we use ground-truth segmentation masks to generate object-level point clouds.During fine-tuning, we use ground-truth masks or Mask3d [44], which depends on the task setting. On the ScanRefer dataset, we also incorporate PointGroup [28] for comparison with previous approaches. In ablation studies, we use ground-truth masks in all tasks for simplicity. Both pre-training and fine-tuning are conducted on a single NVIDIA A100 80GB GPU.
3D Visual Grounding. We evaluate our model on three datasets for this task: ScanRefer [8], Nr3D, and Sr3D [1]. For Nr3D/Sr3D, we follow ReferIt3D [1] to use ground-truth object masks and report the results as the grounding accuracy, i.e., whether the model correctly selects the referred object among ground-truth object proposals. For ScanRefer, we follow [8] to use detector-generated object proposals and report the results as $\operatorname{Acc} @ k(k \in{0.25,0.5})$, i.e., the frac-</p>
<p>Table 4: Grounding accuracy (%) on Nr3D and Sr3D with ground-truth object proposals. $\Delta$ denotes the performance difference between 3D-VisTA and 3D-VisTA (scratch). 3D-VisTA achieves competitive results with SOTA on Nr3D and outperforms SOTA on Sr3D.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Nr3D</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Sr3D</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Overall</td>
<td>Easy</td>
<td>Hard</td>
<td>View</td>
<td>View</td>
<td>Overall</td>
<td>Easy</td>
<td>Hard</td>
<td>View</td>
<td>View</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Dep</td>
<td>Indep</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Indep</td>
</tr>
<tr>
<td>3DVG-Trans [55]</td>
<td>40.8</td>
<td>48.5</td>
<td>34.8</td>
<td>34.8</td>
<td>43.7</td>
<td>51.4</td>
<td>54.2</td>
<td>44.9</td>
<td>44.6</td>
<td>51.7</td>
</tr>
<tr>
<td>TransRefer3D [20]</td>
<td>48.0</td>
<td>56.7</td>
<td>39.6</td>
<td>42.5</td>
<td>50.7</td>
<td>57.4</td>
<td>60.5</td>
<td>50.2</td>
<td>49.9</td>
<td>57.7</td>
</tr>
<tr>
<td>LAR [4]</td>
<td>48.9</td>
<td>58.4</td>
<td>42.3</td>
<td>47.4</td>
<td>52.1</td>
<td>59.4</td>
<td>63.0</td>
<td>51.2</td>
<td>50.0</td>
<td>59.1</td>
</tr>
<tr>
<td>SAT [53]</td>
<td>56.5</td>
<td>64.9</td>
<td>48.4</td>
<td>54.4</td>
<td>57.6</td>
<td>57.9</td>
<td>61.2</td>
<td>50.0</td>
<td>49.2</td>
<td>58.3</td>
</tr>
<tr>
<td>3D-SPS [35]</td>
<td>51.5</td>
<td>58.1</td>
<td>45.1</td>
<td>48.0</td>
<td>53.2</td>
<td>62.6</td>
<td>56.2</td>
<td>65.4</td>
<td>49.2</td>
<td>63.2</td>
</tr>
<tr>
<td>MVT [24]</td>
<td>59.5</td>
<td>67.4</td>
<td>52.7</td>
<td>59.1</td>
<td>60.3</td>
<td>64.5</td>
<td>66.9</td>
<td>58.8</td>
<td>58.4</td>
<td>64.7</td>
</tr>
<tr>
<td>ViL3DRel [10]</td>
<td>64.4</td>
<td>70.2</td>
<td>57.4</td>
<td>62.0</td>
<td>64.5</td>
<td>72.8</td>
<td>74.9</td>
<td>67.9</td>
<td>63.8</td>
<td>73.2</td>
</tr>
<tr>
<td>3D-VisTA (scratch)</td>
<td>57.5</td>
<td>65.9</td>
<td>49.4</td>
<td>53.7</td>
<td>59.4</td>
<td>69.6</td>
<td>72.1</td>
<td>63.6</td>
<td>57.9</td>
<td>70.1</td>
</tr>
<tr>
<td>3D-VisTA</td>
<td>64.2</td>
<td>72.1</td>
<td>56.7</td>
<td>61.5</td>
<td>65.1</td>
<td>76.4</td>
<td>78.8</td>
<td>71.3</td>
<td>58.9</td>
<td>77.3</td>
</tr>
<tr>
<td>$\Delta$</td>
<td>$6.7 \uparrow$</td>
<td>$6.2 \uparrow$</td>
<td>$7.3 \uparrow$</td>
<td>$7.8 \uparrow$</td>
<td>$5.7 \uparrow$</td>
<td>$6.8 \uparrow$</td>
<td>$6.7 \uparrow$</td>
<td>$7.7 \uparrow$</td>
<td>$1.0 \uparrow$</td>
<td>$7.2 \uparrow$</td>
</tr>
</tbody>
</table>
<p>Table 5: Grounding accuracy (%) on ScanRefer with detected object proposals. “Det." represents the 3D object detection module used in the model. “VN" stands for VoteNet [16], "PG" for PointGroup [28], and M3D for Mask3D [44], while “Opt." denotes jointly optimizing the object detector on ScanRefer. Mask3D significantly improves the grounding accuracy by providing more accurate object proposals.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Det.</th>
<th>Unique</th>
<th></th>
<th>Multiple</th>
<th></th>
<th>Overall</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>acc@0.25</td>
<td>acc@0.5</td>
<td>acc@0.25</td>
<td>acc@0.5</td>
<td>acc@0.25</td>
<td>acc@0.5</td>
</tr>
<tr>
<td>3DVG-Trans [55]</td>
<td>Opt.</td>
<td>81.9</td>
<td>60.6</td>
<td>39.3</td>
<td>28.4</td>
<td>47.6</td>
<td>34.7</td>
</tr>
<tr>
<td>3D-SPS [35]</td>
<td>Opt.</td>
<td>84.1</td>
<td>66.7</td>
<td>40.3</td>
<td>29.8</td>
<td>48.8</td>
<td>37.0</td>
</tr>
<tr>
<td>3DJCG [7]</td>
<td>Opt.</td>
<td>83.5</td>
<td>64.3</td>
<td>41.4</td>
<td>30.8</td>
<td>49.6</td>
<td>37.3</td>
</tr>
<tr>
<td>SAT [53]</td>
<td>VN</td>
<td>73.2</td>
<td>50.8</td>
<td>37.6</td>
<td>25.2</td>
<td>44.5</td>
<td>30.1</td>
</tr>
<tr>
<td>MVT [24]</td>
<td>PG</td>
<td>77.7</td>
<td>66.5</td>
<td>31.9</td>
<td>25.3</td>
<td>40.8</td>
<td>33.3</td>
</tr>
<tr>
<td>ViL3DRel [10]</td>
<td>PG</td>
<td>81.6</td>
<td>68.6</td>
<td>40.3</td>
<td>30.7</td>
<td>47.9</td>
<td>37.7</td>
</tr>
<tr>
<td>3D-VisTA (scratch)</td>
<td>PG</td>
<td>76.0</td>
<td>66.9</td>
<td>33.3</td>
<td>27.0</td>
<td>41.2</td>
<td>34.4</td>
</tr>
<tr>
<td>3D-VisTA</td>
<td>PG</td>
<td>77.0</td>
<td>67.9</td>
<td>37.9</td>
<td>30.4</td>
<td>45.2</td>
<td>37.3</td>
</tr>
<tr>
<td>3D-VisTA (scratch)</td>
<td>M3D</td>
<td>77.4</td>
<td>70.9</td>
<td>38.7</td>
<td>34.8</td>
<td>45.9</td>
<td>41.5</td>
</tr>
<tr>
<td>3D-VisTA</td>
<td>M3D</td>
<td>81.6</td>
<td>75.1</td>
<td>43.7</td>
<td>39.1</td>
<td>50.6</td>
<td>45.8</td>
</tr>
<tr>
<td>$\Delta$</td>
<td>M3D</td>
<td>$4.2 \uparrow$</td>
<td>$4.2 \uparrow$</td>
<td>$5.0 \uparrow$</td>
<td>$4.3 \uparrow$</td>
<td>$4.7 \uparrow$</td>
<td>$4.3 \uparrow$</td>
</tr>
</tbody>
</table>
<p>Table 6: Captioning results on Scan2Cap dataset. "C" stands for "CIDEr", "B-4" for "BLEU-4", "M" for "METEOR", and "R" for "ROUGE", respectively. "@0.25" and "@0.5" represent the overlap ratios between the predicted boxes and ground truth boxes.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>@0.25</th>
<th></th>
<th></th>
<th></th>
<th>@0.5</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>C</td>
<td>B-4</td>
<td>M</td>
<td>R</td>
<td>C</td>
<td>B-4</td>
<td>M</td>
<td>R</td>
</tr>
<tr>
<td>Scan2Cap [11]</td>
<td>53.7</td>
<td>34.3</td>
<td>26.1</td>
<td>55.0</td>
<td>35.2</td>
<td>22.4</td>
<td>21.4</td>
<td>43.5</td>
</tr>
<tr>
<td>3DJCG [7]</td>
<td>60.9</td>
<td>39.7</td>
<td>27.5</td>
<td>59.0</td>
<td>47.7</td>
<td>31.5</td>
<td>24.3</td>
<td>51.8</td>
</tr>
<tr>
<td>3D-VisTA (scratch)</td>
<td>66.8</td>
<td>36.6</td>
<td>28.0</td>
<td>58.4</td>
<td>61.6</td>
<td>34.1</td>
<td>26.8</td>
<td>55.0</td>
</tr>
<tr>
<td>3D-VisTA</td>
<td>71.0</td>
<td>36.5</td>
<td>28.4</td>
<td>57.6</td>
<td>66.9</td>
<td>34.0</td>
<td>27.1</td>
<td>54.3</td>
</tr>
<tr>
<td>$\Delta$</td>
<td>$4.2 \uparrow$</td>
<td>$0.1 \downarrow$</td>
<td>$0.4 \uparrow$</td>
<td>$0.8 \downarrow$</td>
<td>$5.3 \uparrow$</td>
<td>$0.1 \downarrow$</td>
<td>$0.3 \uparrow$</td>
<td>$0.7 \downarrow$</td>
</tr>
</tbody>
</table>
<p>tion of referring queries whose predicted box overlaps the ground truth with IoU $&gt;k$.
3D Dense Captioning We evaluate our model on the Scan2cap dataset [11] and report the text similarity metrics under different box overlap ratios.
3D Question Answering. We evaluate our model on the ScanQA dataset [3] and use exact matches (EM@1 and EM@10) as the evaluation metric. We also report several sentence evaluation metrics, including BLEU-4, ROUGE, METEOR, and CIDEr. Both test sets (w/ or w/o objects) of</p>
<p>ScanQA are used in our evaluation.
3D Situated Reasoning We evaluate our model on the SQA3D dataset [36] and report the answer accuracy under different types of questions as the evaluation metric.</p>
<h3>5.2 Downstream Task Results</h3>
<p>In this section, we discuss the experimental results of the downstream tasks and compare the proposed 3D-VisTA model with the state-of-the-art (SOTA) methods. Results are presented in Tables 4 to 8 and Fig. 3 and the main observations from these results are as follows:</p>
<ol>
<li>Even trained from scratch, 3D-VisTA achieves competitive performances with SOTA methods. Specifically, 3D-VisTA (scratch) obtains an overall accuracy of $57.5 \%$ and $69.6 \%$ on Nr3D and Sr3D, which outperforms most previous models; it gets an EM@1 accuracy of $25.2 \%$ on ScanQA, which is $1.7 \%$ higher than SOTA. Of note, 3DVisTA is trained on these datasets simply using the task losses, without any auxiliary losses or optimization tricks,</li>
</ol>
<p>Table 7: Answer accuracy on ScanQA using object proposals from Mask3D. Each entry denotes "test w/ object" / "test w/o object".</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">EM@1</th>
<th style="text-align: left;">EM@10</th>
<th style="text-align: left;">BLEU-4</th>
<th style="text-align: left;">ROUGE</th>
<th style="text-align: left;">METEOR</th>
<th style="text-align: left;">CIDEr</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Image+MCAN [3]</td>
<td style="text-align: left;">$22.3 / 20.8$</td>
<td style="text-align: left;">$53.1 / 51.2$</td>
<td style="text-align: left;">$14.3 / 9.7$</td>
<td style="text-align: left;">$31.3 / 29.2$</td>
<td style="text-align: left;">$12.1 / 11.5$</td>
<td style="text-align: left;">$60.4 / 55.6$</td>
</tr>
<tr>
<td style="text-align: left;">ScanRefer+MCAN [3]</td>
<td style="text-align: left;">$20.6 / 19.0$</td>
<td style="text-align: left;">$52.4 / 49.7$</td>
<td style="text-align: left;">$7.5 / 7.8$</td>
<td style="text-align: left;">$30.7 / 28.6$</td>
<td style="text-align: left;">$12.0 / 11.4$</td>
<td style="text-align: left;">$57.4 / 53.4$</td>
</tr>
<tr>
<td style="text-align: left;">ScanQA [3]</td>
<td style="text-align: left;">$23.5 / 20.9$</td>
<td style="text-align: left;">$56.5 / 54.1$</td>
<td style="text-align: left;">$12.0 / 10.8$</td>
<td style="text-align: left;">$34.3 / 31.1$</td>
<td style="text-align: left;">$13.6 / 12.6$</td>
<td style="text-align: left;">$67.3 / 60.2$</td>
</tr>
<tr>
<td style="text-align: left;">3D-VisTA (scratch)</td>
<td style="text-align: left;">$25.2 / 20.4$</td>
<td style="text-align: left;">$55.2 / 51.5$</td>
<td style="text-align: left;">$10.5 / 8.7$</td>
<td style="text-align: left;">$35.5 / 29.6$</td>
<td style="text-align: left;">$13.8 / 11.6$</td>
<td style="text-align: left;">$68.6 / 55.7$</td>
</tr>
<tr>
<td style="text-align: left;">3D-VisTA</td>
<td style="text-align: left;">$\mathbf{2 7 . 0 / 2 3 . 0}$</td>
<td style="text-align: left;">$\mathbf{5 7 . 9 / 5 3 . 5}$</td>
<td style="text-align: left;">$\mathbf{1 6 . 0 / 1 1 . 9}$</td>
<td style="text-align: left;">$\mathbf{3 8 . 6 / 3 2 . 8}$</td>
<td style="text-align: left;">$\mathbf{1 5 . 2 / 1 2 . 9}$</td>
<td style="text-align: left;">$\mathbf{7 6 . 6 / 6 2 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">$\Delta$</td>
<td style="text-align: left;">$\mathbf{1 . 8} \uparrow / \mathbf{2 . 6} \uparrow$</td>
<td style="text-align: left;">$\mathbf{2 . 7} \uparrow / \mathbf{2 . 0} \uparrow$</td>
<td style="text-align: left;">$\mathbf{5 . 5} \uparrow / \mathbf{3 . 2} \uparrow$</td>
<td style="text-align: left;">$\mathbf{3 . 1} \uparrow / \mathbf{3 . 2} \uparrow$</td>
<td style="text-align: left;">$\mathbf{1 . 4} \uparrow / \mathbf{1 . 3} \uparrow$</td>
<td style="text-align: left;">$\mathbf{8 . 0} \uparrow / \mathbf{6 . 9} \uparrow$</td>
</tr>
</tbody>
</table>
<p>Table 8: Answer accuracy on SQA3D using object proposals from Mask3D. Pretraining improves the results of most question types.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Test set</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">What</td>
<td style="text-align: center;">Is</td>
<td style="text-align: center;">How</td>
<td style="text-align: center;">Can</td>
<td style="text-align: center;">Which</td>
<td style="text-align: center;">Other</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 [36]</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">41.0</td>
</tr>
<tr>
<td style="text-align: center;">ClipBERT [36]</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">43.3</td>
</tr>
<tr>
<td style="text-align: center;">SQA3D(w/o s) [36]</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">45.3</td>
</tr>
<tr>
<td style="text-align: center;">SQA3D [36]</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">46.6</td>
</tr>
<tr>
<td style="text-align: center;">3D-VisTA (scratch)</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">46.7</td>
</tr>
<tr>
<td style="text-align: center;">3D-VisTA</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">48.5</td>
</tr>
<tr>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">$\mathbf{2 . 7} \uparrow$</td>
<td style="text-align: center;">$\mathbf{0 . 4} \uparrow$</td>
<td style="text-align: center;">$\mathbf{2 . 3} \downarrow$</td>
<td style="text-align: center;">$\mathbf{9 . 1} \uparrow$</td>
<td style="text-align: center;">$\mathbf{1 . 3} \uparrow$</td>
<td style="text-align: center;">$\mathbf{0 . 8} \downarrow$</td>
<td style="text-align: center;">$\mathbf{1 . 8} \uparrow$</td>
</tr>
</tbody>
</table>
<p>indicating that 3D-VisTA is a very simple yet effective architecture for 3D-VL tasks.
2. Pre-training on ScanScribe significantly improves the performance of 3D-VisTA. Overall, the pre-training improves the accuracy on Nr3D/Sr3D by $6.7 \% / 6.8 \%$, the acc@0.25/0.5 on ScanRefer by $4.7 \% / 4.3 \%$, the EM@1 on ScanQA by $1.8 \% / 2.6 \%$, the C@0.25 on Scan2Cap by $4.2 \%$, and the average accuracy on SQA3D by $1.8 \%$. These large improvements consolidate the efficacy of ScanScribe for the 3D-VL pre-training.
3. The pre-trained 3D-VisTA outperforms SOTA by a large margin. 3D-VisTA outperforms ViL3DRel [10] on Sr3D by $3.6 \%$ and on ScanRefer by $2.7 \% / 8.1 \%$ (acc@0.25/0.5), beats ScanQA [3] by 3.5\%/2.1 (EM@1), Scan2Cap SOTA by $10.1 \% / 19.2 \%$ (C@0.25/0.5), SQA3D [36] by $1.9 \%$ (Avg.). 3D-VisTA sets a new record for these 3D-VL tasks and may inspire future research on 3D-VL pre-training.
4. Finetuning 3D-VisTA on downstream tasks with limited annotations achieves strong results. As shown in Fig. 3, being fine-tuned using $30 \%$ and $40 \%$ of the annotations on ScanRefer and ScanQA, the pre-trained 3D-VisTA can achieve better performance than the one trained from scratch with full data. We hypothesize that 3D-VisTA has successfully captured the alignment between 3D objects and text via pre-training and is thus able to readily adapt to downstream tasks of various formats. It also reveals the potential of 3D-VisTA to learn unseen tasks in a zero-shot or few-shot manner, which</p>
<p>Figure 3: The performance of finetuning 3D-VisTA using various amounts of training data.
<img alt="img-2.jpeg" src="img-2.jpeg" />
has emerged in NLP [6] and 2D-VL [2] via large-scale pre-training.</p>
<h3>5.3. Ablation Studies</h3>
<p>In this section, we conduct ablation studies to analyze the impact of several important hyperparameters, including Transformer depth, pre-training objectives, and data amount. Transformer Depth. Since the model size is a key factor in the pre-training of NLP and 2D-VL, we study the effect of the transformer depth by varying the number of layers in the multimodal fusion module. As shown in Table 9a, using 4 layers achieves the best performance and simply adding more layers does not help. This observation is somewhat contradictory to the ones from NLP and 2D-VL. It points out that although ScanScribe is much larger than existing 3D-VL datasets, it is still far from enough to unleash the full potential of pre-training in the 3D-VL domain.
Pre-training Objectives. Table 9b presents the ablation study for the pre-training objectives. The MLM objective alone slightly benefits question answering (QA), but brings a negative impact on visual grounding (VG). Adding MOM and STM boosts the performance of both QA and VG, which highlights the importance of MOM and STM for aligning 3D vision and text. Overall, using all three objectives together leads to the best performance for both tasks, with STM and MOM providing the greatest improvements in accuracy.
Pre-training Data. Table 9c presents the results using various configurations of pre-training data. We can see that simply using the ScanNet data for pre-training, which is from the same domain as downstream tasks, leads to a significant improvement in VG and QA. This validates the effectiveness</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Qualitative results for various tasks. Italic text stand for the inputs, blue boxes or text for the predictions from 3D-VisTA trained from scratch, red for the predictions from pre-trained 3D-VisTA, and green for the ground truth, respectively. The results show that pre-training improves the understanding of spatial relations, visual concepts, and situations.</p>
<p>Table 9: Ablation studies of 3D-VisTA w.r.t. Transformer depth, pretraining objectives, and pre-training data. We report the grounding accuracy on ScanRefer for Visual Grounding (VG) and the EM@1 accuracy on ScanQA for Question Answering (QA).</p>
<table>
<thead>
<tr>
<th>(a) Transformer Depth</th>
<th></th>
<th></th>
<th>(b) Pre-training Objectives</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td># layer</td>
<td>VG</td>
<td>QA</td>
<td>MLM</td>
<td>MOM</td>
<td>STM</td>
<td>VG</td>
<td>QA</td>
</tr>
<tr>
<td>2</td>
<td>55.8</td>
<td>23.7</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>52.0</td>
<td>20.7</td>
</tr>
<tr>
<td>4</td>
<td>57.4</td>
<td>23.8</td>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>51.5</td>
<td>21.3</td>
</tr>
<tr>
<td>6</td>
<td>56.6</td>
<td>22.8</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>57.1</td>
<td>22.5</td>
</tr>
<tr>
<td>8</td>
<td>56.3</td>
<td>22.7</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>57.4</td>
<td>23.8</td>
</tr>
</tbody>
</table>
<p>(c) Pre-training Data</p>
<table>
<thead>
<tr>
<th>ScanNet</th>
<th>3R-Scan</th>
<th>Objaverse</th>
<th>VG</th>
<th>QA</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>52.0</td>
<td>20.7</td>
</tr>
<tr>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>54.6</td>
<td>22.6</td>
</tr>
<tr>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>56.5</td>
<td>23.5</td>
</tr>
<tr>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>57.4</td>
<td>23.8</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The performance gap between scratch and pre-training over different sentence lengths ( $\leq 15, \leq 30,&gt;30$ ) in ScanRefer. of pre-training, even in the case of no additional 3D data than downstream tasks. Adding 3R-Scan and Objaverse increases the amount and the diversity of 3D data, which further boosts the accuracy of both VG and QA. Overall, the best performance for both tasks is achieved when all three data sources are used. This points out a promising path for improving 3D-VL tasks - collecting more data for pre-training.</p>
<h3>5.4 Qualitative Studies and Additional Results</h3>
<p>In this section, we perform additional studies to better understand how pre-training helps. As shown in Fig. 4, pretraining improves the spatial understanding of 3D-VisTA for visual grounding, so it can better align with human prior viewpoint and reason over spatial relations. This is very helpful when the model needs to distinguish the target object from multiple instances of the same class. Pre-training also helps with a better understanding of visual concepts like colors and shapes, and situations for question answering and situated reasoning. Besides, pre-training enhances the capability of aligning long text with 3D scenes, as evidenced by the larger improvement over longer queries in Fig. 5.</p>
<h2>6 Conclusion</h2>
<p>This paper proposes 3D-VisTA, a simple yet effective architecture for 3D-VL tasks. The model simply uses selfattention layers and can be easily adapted to various downstream tasks, without requiring any auxiliary loss or optimization trick. We also introduce ScanScribe, the first largescale 3D scene-text pairs dataset for 3D-VL pre-training. The pre-trained 3D-VisTA achieves state-of-the-art results on a variety of 3D-VL tasks with superior data efficiency, paving the path to future foundation models for 3D-VL tasks. Future Works. Currently, 3D-VisTA uses an offline 3D object detection module, which may be a bottleneck for further improvement. Jointly optimizing the object detection module in the pre-training phase is an interesting future direction. Besides, the data amount in ScanScribe is still insufficient for large-scale 3D-VL pre-training, so scaling up the pre-training dataset as well as the model size is a promising direction to further improve the 3D-VL learning.</p>
<p>Acknowledgements. The authors would like to thank Hongming Xu at BIGAI for the help on Mask3D. This work is supported in part by the National Key R\&amp;D Program of China (2022ZD0114900) and the National Science Foundation of China (NSFC) under Grant No. 62176134.</p>
<h2>References</h2>
<p>[1] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In European Conference on Computer Vision (ECCV), pages 422-440. Springer, 2020. 1, 2, 3, 5, A1
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022. 2, 3, 7
[3] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 19129-19139, 2022. 1, 2, 3, 5, 6, 7, A1
[4] Eslam Mohamed Bakr, Yasmeen Alsaedy, and Mohamed Elhoseiny. Look around and refer: 2d synthetic semantics knowledge distillation for 3d visual grounding. Advances in Neural Information Processing Systems (NeurIPS), 2022. 1, 6
[5] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do as i can, not as i say: Grounding language in robotic affordances. In 6th Annual Conference on Robot Learning, 2022. 1
[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS), 33:1877-1901, 2020. 2, 5,7
[7] Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong Xu. 3djcg: A unified framework for joint dense captioning and visual grounding on 3d point clouds. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 16464-16473, 2022. 1, 2, 3, 4, 6
[8] Dave Zhenyu Chen, Angel X Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language. In European Conference on Computer Vision (ECCV), pages 202-221. Springer, 2020. 1, 2, 3, 5, A1
[9] Dave Zhenyu Chen, Qirui Wu, Matthias Nießner, and Angel X Chang. D3net: a speaker-listener architecture for semisupervised dense captioning and visual grounding in rgb-d scans. arXiv preprint arXiv:2112.01551, 2021. 3
[10] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Language conditioned spatial relation reasoning for 3d object grounding. Advances in Neural Information Processing Systems (NeurIPS), 2022. $1,2,3,5,6,7$
[11] Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel X Chang. Scan2cap: Context-aware dense captioning in rgbd scans. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 3193-3203, 2021. 1, 2, 5, 6, A1
[12] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richlyannotated 3d reconstructions of indoor scenes. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 5828-5839, 2017. 2, 5
[13] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. arXiv preprint arXiv:2212.08051, 2022. 2, 5
[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. Conference on Computer Vision and Pattern Recognition (CVPR), 2009. 3
[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2019. 2, 3, 4, 5
[16] Zhipeng Ding, Xu Han, and Marc Niethammer. Votenet: A deep learning label fusion method for multi-atlas segmentation. In Medical Image Computing and Computer Assisted Intervention (MICCAI), pages 202-210. Springer, 2019. 1, 3, 6
[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. International Conference on Learning Representations (ICLR), 2021. 2, 3
[18] Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan. A survey of embodied ai: From simulators to research tasks. IEEE Transactions on Emerging Topics in Computational Intelligence, 6(2):230-244, 2022. 1, 2
[19] Huy Ha and Shuran Song. Semantic abstraction: Open-world 3d scene understanding from 2d vision-language models. In Conference on Robot Learning, 2022. 1
[20] Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang, Aixi Zhang, and Si Liu. Transrefer3d: Entity-andrelation aware transformer for fine-grained 3d visual grounding. In Proceedings of the 29th ACM International Conference on Multimedia, pages 2344-2352, 2021. 2, 6
[21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 16000-16009, 2022. 2
[22] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2
[23] Yining Hong, Qing Li, Song-Chun Zhu, and Siyuan Huang. Vlgrammar: Grounded grammar induction of vision and language. In International Conference on Computer Vision (ICCV), 2021. 1</p>
<p>[24] Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. Multiview transformer for 3d visual grounding. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 15524-15533, 2022. 1, 2, 3, 5, 6
[25] Siyuan Huang, Yichen Xie, Song-Chun Zhu, and Yixin Zhu. Spatio-temporal self-supervised representation learning for 3d point clouds. In International Conference on Computer Vision (ICCV), 2021. 2
[26] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning (ICML), pages 9118-9147. PMLR, 2022. 1
[27] Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, and Katerina Fragkiadaki. Bottom up top down detection transformers for language grounding in images and point clouds. In European Conference on Computer Vision (ECCV), pages 417-433. Springer, 2022. 1, 2
[28] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, ChiWing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 4867-4876, 2020. 3, 5, 6
[29] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. arXiv preprint arXiv:2210.03094, 2022. 1
[30] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision (ECCV), pages 121-137. Springer, 2020. 2, 5
[31] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. Annual Meeting of the Association for Computational Linguistics (ACL), 2020. 2
[32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. International Conference on Computer Vision (ICCV), 2021. 3
[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. International Conference on Learning Representations (ICLR), 2019. 5
[34] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019. 2, 3
[35] Junyu Luo, Jiahui Fu, Xianghao Kong, Chen Gao, Haibing Ren, Hao Shen, Huaxia Xia, and Si Liu. 3d-sps: Single-stage 3d visual grounding via referred point progressive selection. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 16454-16463, 2022. 1, 2, 6
[36] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated
question answering in 3d scenes. International Conference on Learning Representations (ICLR), 2023. 1, 2, 3, 5, 6, 7, A1
[37] Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, and Dilek Hakkani-Tur. Teach: Task-driven embodied agents that chat. In AAAI Conference on Artificial Intelligence (AAAI), 2022. 1
[38] Yatian Pang, Wenxiao Wang, Francis E. H. Tay, Wei Liu, Yonghong Tian, and Li Yuan. Masked autoencoders for point cloud self-supervised learning. In European Conference on Computer Vision (ECCV), 2022. 2
[39] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017. 3, A1
[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), pages 8748-8763. PMLR, 2021. 2, 3
[41] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. OpenAI Blog, 2018. 2
[42] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 2
[43] Junha Roh, Karthik Desingh, Ali Farhadi, and Dieter Fox. Languagerefer: Spatial-language model for 3d visual grounding. In Conference on Robot Learning, pages 1046-1056. PMLR, 2022. 2
[44] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3d for 3d semantic instance segmentation. arXiv preprint arXiv:2210.03105, 2022. 3, 5, 6
[45] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. Annual Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019. 3, 5
[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017. 2
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017. 2
[48] Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, and Matthias Nießner. Rio: 3d object instance relocalization in changing indoor environments. In International Conference on Computer Vision (ICCV), pages 76587667, 2019. 2, 5
[49] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022. 5</p>
<p>[50] Yiren Wang, Shiyang Huang, Tianyu Gao, Xu Zhang, Xu Han, and Zhangyang Wang. Align: Adaptive fine-tuning for long-tailed instance generation via contrastive pre-training. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), 2022.
[51] Shun-Cheng Wu, Johanna Wald, Keisuke Tateno, Nassir Navab, and Federico Tombari. Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 7515-7525, 2021.
[52] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in Neural Information Processing Systems (NeurIPS), 32:5754-5764, 2019.
[53] Zhengyuan Yang, Songyang Zhang, Liwei Wang, and Jiebo Luo. Sat: 2d semantics assisted training for 3d visual grounding. In International Conference on Computer Vision (ICCV), pages 1856-1866, 2021.
[54] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks for visual question answering. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 6281-6290, 2019.
[55] Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvgtransformer: Relation modeling for visual grounding on point clouds. In International Conference on Computer Vision (ICCV), pages 2928-2937, 2021.
[56] Lichen Zhao, Daigang Cai, Jing Zhang, Lu Sheng, Dong Xu, Rui Zheng, Yinjie Zhao, Lipeng Wang, and Xibo Fan. Towards explainable 3d grounded visual question answering: A new benchmark and strong baseline. IEEE Transactions on Circuits and Systems for Video Technology, 2022.</p>
<h2>Appendix</h2>
<h2>A. Implementation Details</h2>
<h2>A.1. Downstream Tasks</h2>
<p>ScanRefer [8]: The ScanRefer dataset contains 51,583 sentences written by humans to describe 800 scenes in ScanNet. We used the official split and allocated 36,665 and 9,508 samples for training and validation, respectively. The dataset is categorized into unique and multiple subsets based on whether the target object is a unique class in the scene. In this task, we need to find the target object described by a sentence. The evaluation metric for this task is accuracy under intersection over union (IoU) 0.25 and 0.5 .
Nr3D/Sr3D [1]: The Sr3D dataset comprises of 83,572 utterances that are automatically generated using a template that focuses on the target-anchor spatial relationship. The Nr3D contains 45,503 human utterances. Both Sr3D and Nr3D are split by "Easy"/"Hard" and "ViewDep"/"ViewIndep". Hard samples are the ones with two or more distractors in a scene. The view-dependent samples contain language descriptions that rely on viewing directions. These two datasets are also used for visual grounding like ScanRefer. But grounding accuracy with ground truth object proposal is evaluated in this setting.
ScanQA [3]: ScanQA is a dataset for 3D question answering with 41,363 questions and 58,191 answers. Different from 2D QA, ScanQA focuses more on spatial relations. We follow [3] to use exact matches EM@1 and EM@10 as the evaluation metric. EM@K means the percentage of top K answers from the model matches one of the ground-truth answers. Also, we include text similarity metrics to evaluate answers, including BLEU-4, ROUGE, METEOR, and CIDEr.
SQA3D [36]: SQA3D is a benchmark for scene understanding of embodied agents with 6.8 k unique situations, 20.4 k descriptions, and 33.4 k diverse reasoning questions. Given a situation, an embodied agent must understand embodied activities, navigation instructions, and common sense, and perform multi-hop reasoning. The evaluation metric is answer accuracy under different types of questions.
Scan2Cap [11]: Scan2Cap is a dataset for 3D dense captioning. Object descriptions are produced from ScanRefer dataset. For each sentence, two special tokens including [SOS] and [EOS] are added.</p>
<h2>A.2. Model Architecture</h2>
<p>For the scene encoder, we use a three-layer Pointnet++ [39] with radius $0.2,0.4$, and sample all points to aggregate a 768-dimension feature. For all text and object tokens, the dimension is 768 in the following multi-modal fusion layers. In the unified encoder, the number of attention heads is set to 12 and the dimension of feedforward layers is set to 2048. For the visual grounding head, we use a two-layer MLP with a hidden dimension of 384. For the question-answering head and the situated reasoning head, we use a two-layer MLP with input dimensions 512 (from the attention flat layer) and 768.</p>
<h2>A.3. Training settings</h2>
<p>The settings of pre-training including mask ratio, and optimization hyperparameters are introduced in the main paper. We exclude the ScanNet validation and test scenes from pre-training to ensure a fair comparison with other methods. All scenes from 3R-Scan are used for pre-training. In this part, we elaborate on the fine-tuning details.
3D Visual Grounding: We only use a cross-entropy loss for fine-tuning 3D-VisTA on ScanRefer, Nr3D, and Sr3D. For all these grounding tasks, we set the batch size to 64 , and the learning rate to $1 \mathrm{e}-4$, We multiply the learning rate of the text encoder by 0.1 to stabilize the training process. We finetune the pre-trained 3D-VisTA for 100, 100, and 50 epochs for ScanRefer, Nr3D, and Sr3D, respectively. AdamW with $\beta_{1}=0.9, \beta_{2}=0.98$ is chosen as the optimizer. We use a warmup of 5,000 steps and a cosine annealing learning rate schedule.</p>
<p>3D Question Answering: We use a cross-entropy answer classification loss and a visual grounding loss for ScanQA. The batch size is 64 and the learning rate is 1e-4. 3D-VisTA is fine-tuned for 30 epochs with 2000 warmup steps for this task. Other optimization parameters are the same as the visual grounding task.
3D Situated Reasoning: Answer classification loss is used for SQA3D. We fine-tune 3D-VisTA for 50 epochs. Other optimization parameters are the same as the 3D questionanswering task.
3D Dense Captioning: Cross entropy loss is used for finetuning Scan2Cap. We use the BERT tokenizer to process input sentences and use the casual mask for language transformer. During both fine-tuning and inference, object tokens are not allowed to attend text tokens because of information leaks. 3D-VisTA is fine-tuned for 100 epochs with batch size 64 and learning rate 1e-4. During inference, text tokens are generated by the greedy selection policy.</p>
<h2>A.4. ScanScribe</h2>
<p>In the main paper, we introduce our method of generating new scene-text pairs from scene graphs and large language models. More examples and cases are provided in this section. We support 40 relations and the mapping of relations to descriptions for the template-based generation is shown in Table A1.</p>
<p>With these relations, we can use templates like "This is a object, a neighbor is relation to object" and utilize GPT-3 to increase text diversity. During pre-training, to balance the proportion of template and GPT-3 generated texts in the 3R-Scan dataset, we duplicate texts from GPT-3 to 15 times for pre-training. Examples from both templatebased generation and GPT-3 are presented in Fig. A1. We can observe that given entities and relations in a scene, GPT-3 can summarize them into a fluent and natural sentence.</p>
<h2>B. Additional Results</h2>
<p>We provide ablation studies on the use the templategenerated text and GPT-3-generated text. As shown in Table A2, GPT-3-generated text improves Sr3D and Nr3D by $1.0 \%$ and $1.5 \%$, while having little impact on ScanRefer and ScanQA. More qualitative results including failure cases are provided in Fig. A2.</p>
<p>Table A1: The mapping of relations to descriptions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Relation</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">supported by</td>
<td style="text-align: left;">is supported by the</td>
</tr>
<tr>
<td style="text-align: left;">left</td>
<td style="text-align: left;">is on the left side of the</td>
</tr>
<tr>
<td style="text-align: left;">right</td>
<td style="text-align: left;">is on the right side of the</td>
</tr>
<tr>
<td style="text-align: left;">front</td>
<td style="text-align: left;">is in front of the</td>
</tr>
<tr>
<td style="text-align: left;">behind</td>
<td style="text-align: left;">is behind the</td>
</tr>
<tr>
<td style="text-align: left;">close by</td>
<td style="text-align: left;">is close by the</td>
</tr>
<tr>
<td style="text-align: left;">inside</td>
<td style="text-align: left;">is inside the</td>
</tr>
<tr>
<td style="text-align: left;">bigger than</td>
<td style="text-align: left;">is bigger than the</td>
</tr>
<tr>
<td style="text-align: left;">smaller than</td>
<td style="text-align: left;">is smaller than the</td>
</tr>
<tr>
<td style="text-align: left;">higher than</td>
<td style="text-align: left;">is higher than the</td>
</tr>
<tr>
<td style="text-align: left;">lower than</td>
<td style="text-align: left;">is lower than the</td>
</tr>
<tr>
<td style="text-align: left;">same symmetry as</td>
<td style="text-align: left;">has the same symmetry as the</td>
</tr>
<tr>
<td style="text-align: left;">same as</td>
<td style="text-align: left;">is the same as the</td>
</tr>
<tr>
<td style="text-align: left;">attached to</td>
<td style="text-align: left;">is attached to the</td>
</tr>
<tr>
<td style="text-align: left;">standing on</td>
<td style="text-align: left;">is standing on the</td>
</tr>
<tr>
<td style="text-align: left;">lying on</td>
<td style="text-align: left;">is lying on the</td>
</tr>
<tr>
<td style="text-align: left;">hanging on</td>
<td style="text-align: left;">is hanging on the</td>
</tr>
<tr>
<td style="text-align: left;">connected to</td>
<td style="text-align: left;">is connected to the</td>
</tr>
<tr>
<td style="text-align: left;">leaning against</td>
<td style="text-align: left;">is leaning against the</td>
</tr>
<tr>
<td style="text-align: left;">part of</td>
<td style="text-align: left;">is part of the</td>
</tr>
<tr>
<td style="text-align: left;">belonging to</td>
<td style="text-align: left;">is belonging to the</td>
</tr>
<tr>
<td style="text-align: left;">built in</td>
<td style="text-align: left;">is built in the</td>
</tr>
<tr>
<td style="text-align: left;">standing in</td>
<td style="text-align: left;">is standing in the</td>
</tr>
<tr>
<td style="text-align: left;">covers</td>
<td style="text-align: left;">covers the</td>
</tr>
<tr>
<td style="text-align: left;">lying in</td>
<td style="text-align: left;">is lying in the</td>
</tr>
<tr>
<td style="text-align: left;">hanging in</td>
<td style="text-align: left;">is hanging in the</td>
</tr>
<tr>
<td style="text-align: left;">same color</td>
<td style="text-align: left;">has the same color as the</td>
</tr>
<tr>
<td style="text-align: left;">same material</td>
<td style="text-align: left;">has the same material as the</td>
</tr>
<tr>
<td style="text-align: left;">same texture</td>
<td style="text-align: left;">has the same texture as the</td>
</tr>
<tr>
<td style="text-align: left;">same shape</td>
<td style="text-align: left;">has the same shape as the</td>
</tr>
<tr>
<td style="text-align: left;">same state</td>
<td style="text-align: left;">has the same state as the</td>
</tr>
<tr>
<td style="text-align: left;">same object type</td>
<td style="text-align: left;">has the same object type as the</td>
</tr>
<tr>
<td style="text-align: left;">messier than</td>
<td style="text-align: left;">is messier than the</td>
</tr>
<tr>
<td style="text-align: left;">cleaner than</td>
<td style="text-align: left;">is cleaner than the</td>
</tr>
<tr>
<td style="text-align: left;">fuller than</td>
<td style="text-align: left;">is fuller than the</td>
</tr>
<tr>
<td style="text-align: left;">more closed</td>
<td style="text-align: left;">is more closed to the</td>
</tr>
<tr>
<td style="text-align: left;">more open</td>
<td style="text-align: left;">is more open than the</td>
</tr>
<tr>
<td style="text-align: left;">brighter than</td>
<td style="text-align: left;">is brighter than the</td>
</tr>
<tr>
<td style="text-align: left;">darker than</td>
<td style="text-align: left;">is darker than the</td>
</tr>
<tr>
<td style="text-align: left;">more comfortable than</td>
<td style="text-align: left;">is more comfortable than the</td>
</tr>
</tbody>
</table>
<p>Table A2: Ablation studies on the template and GPT-3 generated text from 3R-Scan. We report the results on ScanRefer, Sr3D, Nr3D and ScanQA.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Template</th>
<th style="text-align: center;">GPT-3</th>
<th style="text-align: center;">ScanRefer</th>
<th style="text-align: center;">Sr3D</th>
<th style="text-align: center;">Nr3D</th>
<th style="text-align: center;">ScanQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">23.7</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">23.8</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure A1: Examples of both template and GPT-3 generated text in ScanScribe dataset. GPT-3 generated text is more natural than templategenerated text.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure A2: Qualitative results on ScanRefer, ScanQA, and SQA3D. Green and red denote the ground-truth and predicted object boxes, respectively. As shown in (a,b,c,d,e,f), the pre-trained 3D-VisTA shows advantages in spatial reasoning, concept grounding, and situation understanding. In spite of these advantages, ( $\mathrm{g}, \mathrm{h}, \mathrm{j}$ ) indicate that for some complicated cases with spatial relations, the pre-trained model still cannot understand them. ( $\mathrm{i}, \mathrm{k}$ ) show that our model is still limited by the semantic information extracted by point clouds, which fail to locate the right object or understand texture. From (1), we can observe that our model may fail in the case requiring complex multi-hop reasoning.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Work done as an intern at BIGAI. $\boxtimes$ Corresponding author.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
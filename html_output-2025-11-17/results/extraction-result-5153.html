<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5153 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5153</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5153</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-109.html">extraction-schema-109</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <p><strong>Paper ID:</strong> paper-244909074</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2112.02290v2.pdf" target="_blank">Interactive Disentanglement: Learning Concepts by Interacting with their Prototype Representations</a></p>
                <p><strong>Paper Abstract:</strong> Learning visual concepts from raw images without strong supervision is a challenging task. In this work, we show the advantages of prototype representations for understanding and revising the latent space of neural concept learners. For this purpose, we introduce interactive Concept Swapping Networks (iCSNs), a novel framework for learning concept-grounded representations via weak supervision and implicit prototype representations. iCSNs learn to bind conceptual information to specific prototype slots by swapping the latent representations of paired images. This semantically grounded and discrete latent space facilitates human understanding and human-machine interaction. We support this claim by conducting experiments on our novel data set"Elementary Concept Reasoning"(ECR), focusing on visual concepts shared by geometric objects.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5153.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5153.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prototype theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prototype representation / Prototype theory of concepts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concepts are represented functionally as prototypical averages (central tendencies) over observed instances; categorization is performed by computing similarity between an input and stored prototype(s). The paper uses this notion as inspiration and implements prototype slots that bind semantic concepts to discrete prototype vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Prototype theory</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Proposes that a concept is functionally represented by an average or prototypical representation derived from multiple exemplars; classification or recognition proceeds by measuring similarity of new items to this prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>prototype (abstracted, averaged exemplar) — discrete prototype vectors or centroids</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Abstraction (compresses many examples into a single central representation), similarity-based matching, compactness, supports generalization across variants, amenable to discrete slotting (one prototype per concept).</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>The paper cites evidence for the use and importance of prototypes in human memory and categorization (references in the paper: e.g., [17,30,38] and related prototype-learning literature). Empirical tasks include categorization and memory studies showing prototype-like behavior and prototype-guided reconstruction; neuroscience/behavioral tracking studies report prototype-like representations emerging across learning.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>The paper notes that the distinction between prototype and exemplar accounts is blurred in cognitive psychology and that their relative contributions remain an open problem; recent work hints that humans may use both prototypes and exemplars. Prototype-only models can fail on exceptions and fine-grained distinctions and may miss context-dependent or instance-specific information.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Categorization, concept learning, semantic memory, early language acquisition, modeling of cognitive impairments (e.g., Alzheimer's disease), and computational prototype-learning systems (few-shot learning, explainable ML).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared to exemplar models, prototypes are more compact and promote generalization but may sacrifice fidelity to rare exceptions; the paper contrasts prototype-based discretized representations with continuous distributed latent representations (e.g., VAEs) and finds prototype discretization yields more consistent, human-interpretable concept codes in their visual concept tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Form formation: compute an average/centroid from multiple exemplars; retrieval/recognition: compute similarity (e.g., dot product / distance) between an input's encoding and prototype slots and select the closest prototype (possibly via a softmax/argmax). Binding: assign semantic labels to prototype slots by pairing/swapping signals (in the computational model).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How to represent within-category variability and rare exceptions; whether humans primarily use prototypes, exemplars, or a hybrid; context sensitivity and dynamic concept restructuring; how prototypes interact with hierarchical/superordinate concepts; neurofunctional correlates and exact mechanisms remain incompletely resolved.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5153.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5153.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Exemplar theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exemplar representation / Exemplar theory of concepts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concepts are functionally represented as sets of stored individual exemplars (specific instances); categorization is performed by comparing a novel stimulus to stored exemplars and aggregating similarity-based evidence. The paper cites exemplar accounts as the main alternative to prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Exemplar theory</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Proposes that conceptual knowledge consists of memory traces of individual exemplars (specific prior instances), and classification proceeds by retrieving and comparing similarity to these exemplars rather than to an averaged prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>exemplar (collection of stored instances) — instance-based memory</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>High-fidelity storage of individual instances, preserves within-category variability and exceptions, similarity-based retrieval and accumulation, flexible ad hoc categorization, memory-intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Referenced behavioral and cognitive studies support exemplar effects in categorization and generalization, and neuroimaging/behavioral tracking studies have documented exemplar-like representations under some learning conditions (paper cites studies showing both exemplar and prototype traces).</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Pure exemplar accounts can be inefficient in storage and slower to generalize; empirical literature shows mixed evidence and indicates that exemplar-only models cannot account for some prototype-like generalizations, motivating hybrid models.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Categorization, recognition memory, similarity-based decision making, modeling of learning where memorized instances explain behavior (e.g., few-shot exemplar retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Contrasted with prototype models: exemplars retain detailed instance information and handle exceptions well but lack compression; prototypes compress information and aid rapid generalization. The paper notes empirical work suggesting both formats may co-exist.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Encoding: store instances encountered; retrieval: locate similar exemplars and sum similarity evidence to make category judgments; learning: accumulation of exemplars over experience.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How exemplar stores scale and are pruned, interplay with abstraction/prototype formation, when the cognitive system switches between exemplar and prototype strategies, and how exemplar representations are organized hierarchically.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5153.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5153.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid exemplar–prototype models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixed exemplar-prototype representations / Hybrid models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Accounts proposing that conceptual representation uses both exemplars and prototypes, or mixtures thereof, so that some concepts are represented by prototypes and others by exemplars or both in parallel; the paper notes cognitive literature has blurred lines and recent work suggests mixed usage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Hybrid exemplar–prototype models</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Propose that the cognitive system can maintain both prototypical summaries and selected exemplar traces, using them flexibly depending on task demands, frequency, or diagnosticity of instances.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>hybrid — combination of prototype centroids and exemplar sets</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Adaptive mixture of compression and fidelity, context- and task-sensitive selection of representation type, supports both generalization and exception handling.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Paper cites recent cognitive-science work that hints at the use of both representations (references [6,24] in the paper) and points to studies tracking prototype and exemplar representations across learning indicating temporal or task-dependent shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Open empirical questions about how the two formats are weighted, when transitions occur, and the neural/functional mechanisms coordinating hybrid representations; some experimental findings can be fit by either account, complicating clear adjudication.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Categorization, concept learning across development, memory tasks where both general trend and important exceptions matter, modeling human-like flexible reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Hybrid models aim to capture strengths of both pure prototype and exemplar models while mitigating weaknesses (storage cost for exemplars; loss of exceptions for prototypes). Empirically, they can often explain mixed results where purely exemplar or prototype models fail.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Mechanisms include selective exemplar retention for diagnostic items plus computation of prototypes from remaining examples; arbitration mechanisms decide reliance on prototype vs exemplar retrieval depending on uncertainty, novelty, or task constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How arbitration is implemented, what factors govern exemplar retention versus prototype formation, and how hybrid representations scale and update online remain unresolved.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5153.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5153.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Superordinate/basic concept hierarchy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical organization of concepts: superordinate vs basic (and basic vs basic-realizations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical functional organization distinguishing superordinate (category-level) concepts (e.g., 'color') from basic-level realizations (e.g., 'blue'); the paper operationalizes this distinction in its model by mapping each superordinate concept to prototype codebooks containing basic concept slots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Concept hierarchy (superordinate and basic-level concepts)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Functionally, concepts are organized hierarchically: superordinate categories group related basic-level concept realizations; representations may be allocated per level (e.g., separate prototype slots per superordinate category) and binding occurs between basic realizations and their superordinate label.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>hierarchical categorical (superordinate categories containing discrete basic-level realizations/prototypes)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Multi-level abstraction, enables compositional description (e.g., object has color, shape, size), supports modular learning and addition of new concepts by expanding codebooks/read-out modules, and allows different representational granularity across levels.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>The paper draws on cognitive-science distinctions (superordinate/basic) and uses them to structure model read-out encoders and codebooks; psychological literature supports hierarchical categorization and different processing for basic vs superordinate levels.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Determining the correct level(s) of abstraction for tasks, variability in what counts as 'basic' across domains and individuals, and how hierarchical levels interact during learning are open issues; the paper notes that concept granularity is subjective and dynamic.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Object categorization, multi-attribute concept learning, interactive human–machine concept revision, online learning of novel attributes or categories.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>This hierarchical functional organization contrasts with flat feature-based or purely distributed representations by explicitly separating category-level encoding and per-category prototype slots; it facilitates modular updates (e.g., adding a new superordinate concept) unlike monolithic continuous latent spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Separate read-out encoders extract information relevant to each superordinate category; each such encoder maps to a codebook of prototype slots representing basic realizations; binding and discretization (e.g., via softmax temperature annealing) assign basic realizations to specific prototype slots.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How to determine the number of basic realizations per superordinate category in naturalistic domains, dealing with concepts that cut across categories, subjectivity in granularity, and the mechanisms by which humans flexibly re-structure hierarchies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5153.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5153.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Empirical evidence for prototypes in memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Behavioral/neuroimaging findings reporting prototype-like representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical findings indicating that human memory and categorization often reflect prototype-like representations (central tendencies) and that prototype and exemplar traces can be tracked across learning; the paper cites multiple studies supporting prototype usage in humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Empirical findings: prototype traces in behavior and brain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Reports from behavioral and neural studies showing central-tendency/prototype influences on categorization and memory, and longitudinal tracking studies showing emergence and change of prototype- and exemplar-like representations during learning.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>observed prototype-like behavioral/neural codes (functional-level central tendencies / averaged representations)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Behavioral generalization toward prototypical exemplars, prototype-based reconstruction biases, measurable change across learning, co-existence with exemplar traces in some paradigms.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>The paper refers to work finding prototypes in human memory ([17,30,38] per paper), and cites 'Tracking prototype and exemplar representations in the brain across learning' (eLife) as an example of empirical work tracking both types of representations over time.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Other empirical results support exemplar accounts or mixed accounts; the existence of both prototype and exemplar signatures complicates clean interpretation, and some tasks reveal context- and task-dependence in which representation dominates.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Categorization experiments, memory reconstruction tasks, longitudinal learning studies, neuroimaging studies tracking representational changes during training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Empirical findings often are consistent with both prototype and exemplar models when analyzed superficially; targeted paradigms are necessary to disambiguate, and mixed/hybrid interpretations often provide best fit.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Observed mechanisms include averaging of encountered instances into a prototypical bias and parallel storage/retrieval of exemplars; empirically measured as biases in classification and neural pattern similarity to prototypical stimuli.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Extent to which prototype effects are primary versus derivative of exemplar memory, task-dependence of observed effects, and the precise functional triggers for prototype formation versus exemplar reliance remain open.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tracking prototype and exemplar representations in the brain across learning <em>(Rating: 2)</em></li>
                <li>Prototypes, exemplars, and the natural history of categorization <em>(Rating: 2)</em></li>
                <li>Prototype Learning Systems <em>(Rating: 2)</em></li>
                <li>Prototype constructions in early language acquisition <em>(Rating: 1)</em></li>
                <li>Prototype learning and dissociable categorization systems in alzheimer's disease <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5153",
    "paper_id": "paper-244909074",
    "extraction_schema_id": "extraction-schema-109",
    "extracted_data": [
        {
            "name_short": "Prototype theory",
            "name_full": "Prototype representation / Prototype theory of concepts",
            "brief_description": "Concepts are represented functionally as prototypical averages (central tendencies) over observed instances; categorization is performed by computing similarity between an input and stored prototype(s). The paper uses this notion as inspiration and implements prototype slots that bind semantic concepts to discrete prototype vectors.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "Prototype theory",
            "theory_or_model_description": "Proposes that a concept is functionally represented by an average or prototypical representation derived from multiple exemplars; classification or recognition proceeds by measuring similarity of new items to this prototype.",
            "representation_format_type": "prototype (abstracted, averaged exemplar) — discrete prototype vectors or centroids",
            "key_properties": "Abstraction (compresses many examples into a single central representation), similarity-based matching, compactness, supports generalization across variants, amenable to discrete slotting (one prototype per concept).",
            "empirical_support": "The paper cites evidence for the use and importance of prototypes in human memory and categorization (references in the paper: e.g., [17,30,38] and related prototype-learning literature). Empirical tasks include categorization and memory studies showing prototype-like behavior and prototype-guided reconstruction; neuroscience/behavioral tracking studies report prototype-like representations emerging across learning.",
            "empirical_challenges": "The paper notes that the distinction between prototype and exemplar accounts is blurred in cognitive psychology and that their relative contributions remain an open problem; recent work hints that humans may use both prototypes and exemplars. Prototype-only models can fail on exceptions and fine-grained distinctions and may miss context-dependent or instance-specific information.",
            "applied_domains_or_tasks": "Categorization, concept learning, semantic memory, early language acquisition, modeling of cognitive impairments (e.g., Alzheimer's disease), and computational prototype-learning systems (few-shot learning, explainable ML).",
            "comparison_to_other_models": "Compared to exemplar models, prototypes are more compact and promote generalization but may sacrifice fidelity to rare exceptions; the paper contrasts prototype-based discretized representations with continuous distributed latent representations (e.g., VAEs) and finds prototype discretization yields more consistent, human-interpretable concept codes in their visual concept tasks.",
            "functional_mechanisms": "Form formation: compute an average/centroid from multiple exemplars; retrieval/recognition: compute similarity (e.g., dot product / distance) between an input's encoding and prototype slots and select the closest prototype (possibly via a softmax/argmax). Binding: assign semantic labels to prototype slots by pairing/swapping signals (in the computational model).",
            "limitations_or_open_questions": "How to represent within-category variability and rare exceptions; whether humans primarily use prototypes, exemplars, or a hybrid; context sensitivity and dynamic concept restructuring; how prototypes interact with hierarchical/superordinate concepts; neurofunctional correlates and exact mechanisms remain incompletely resolved.",
            "uuid": "e5153.0"
        },
        {
            "name_short": "Exemplar theory",
            "name_full": "Exemplar representation / Exemplar theory of concepts",
            "brief_description": "Concepts are functionally represented as sets of stored individual exemplars (specific instances); categorization is performed by comparing a novel stimulus to stored exemplars and aggregating similarity-based evidence. The paper cites exemplar accounts as the main alternative to prototypes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Exemplar theory",
            "theory_or_model_description": "Proposes that conceptual knowledge consists of memory traces of individual exemplars (specific prior instances), and classification proceeds by retrieving and comparing similarity to these exemplars rather than to an averaged prototype.",
            "representation_format_type": "exemplar (collection of stored instances) — instance-based memory",
            "key_properties": "High-fidelity storage of individual instances, preserves within-category variability and exceptions, similarity-based retrieval and accumulation, flexible ad hoc categorization, memory-intensive.",
            "empirical_support": "Referenced behavioral and cognitive studies support exemplar effects in categorization and generalization, and neuroimaging/behavioral tracking studies have documented exemplar-like representations under some learning conditions (paper cites studies showing both exemplar and prototype traces).",
            "empirical_challenges": "Pure exemplar accounts can be inefficient in storage and slower to generalize; empirical literature shows mixed evidence and indicates that exemplar-only models cannot account for some prototype-like generalizations, motivating hybrid models.",
            "applied_domains_or_tasks": "Categorization, recognition memory, similarity-based decision making, modeling of learning where memorized instances explain behavior (e.g., few-shot exemplar retrieval).",
            "comparison_to_other_models": "Contrasted with prototype models: exemplars retain detailed instance information and handle exceptions well but lack compression; prototypes compress information and aid rapid generalization. The paper notes empirical work suggesting both formats may co-exist.",
            "functional_mechanisms": "Encoding: store instances encountered; retrieval: locate similar exemplars and sum similarity evidence to make category judgments; learning: accumulation of exemplars over experience.",
            "limitations_or_open_questions": "How exemplar stores scale and are pruned, interplay with abstraction/prototype formation, when the cognitive system switches between exemplar and prototype strategies, and how exemplar representations are organized hierarchically.",
            "uuid": "e5153.1"
        },
        {
            "name_short": "Hybrid exemplar–prototype models",
            "name_full": "Mixed exemplar-prototype representations / Hybrid models",
            "brief_description": "Accounts proposing that conceptual representation uses both exemplars and prototypes, or mixtures thereof, so that some concepts are represented by prototypes and others by exemplars or both in parallel; the paper notes cognitive literature has blurred lines and recent work suggests mixed usage.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Hybrid exemplar–prototype models",
            "theory_or_model_description": "Propose that the cognitive system can maintain both prototypical summaries and selected exemplar traces, using them flexibly depending on task demands, frequency, or diagnosticity of instances.",
            "representation_format_type": "hybrid — combination of prototype centroids and exemplar sets",
            "key_properties": "Adaptive mixture of compression and fidelity, context- and task-sensitive selection of representation type, supports both generalization and exception handling.",
            "empirical_support": "Paper cites recent cognitive-science work that hints at the use of both representations (references [6,24] in the paper) and points to studies tracking prototype and exemplar representations across learning indicating temporal or task-dependent shifts.",
            "empirical_challenges": "Open empirical questions about how the two formats are weighted, when transitions occur, and the neural/functional mechanisms coordinating hybrid representations; some experimental findings can be fit by either account, complicating clear adjudication.",
            "applied_domains_or_tasks": "Categorization, concept learning across development, memory tasks where both general trend and important exceptions matter, modeling human-like flexible reasoning.",
            "comparison_to_other_models": "Hybrid models aim to capture strengths of both pure prototype and exemplar models while mitigating weaknesses (storage cost for exemplars; loss of exceptions for prototypes). Empirically, they can often explain mixed results where purely exemplar or prototype models fail.",
            "functional_mechanisms": "Mechanisms include selective exemplar retention for diagnostic items plus computation of prototypes from remaining examples; arbitration mechanisms decide reliance on prototype vs exemplar retrieval depending on uncertainty, novelty, or task constraints.",
            "limitations_or_open_questions": "How arbitration is implemented, what factors govern exemplar retention versus prototype formation, and how hybrid representations scale and update online remain unresolved.",
            "uuid": "e5153.2"
        },
        {
            "name_short": "Superordinate/basic concept hierarchy",
            "name_full": "Hierarchical organization of concepts: superordinate vs basic (and basic vs basic-realizations)",
            "brief_description": "A hierarchical functional organization distinguishing superordinate (category-level) concepts (e.g., 'color') from basic-level realizations (e.g., 'blue'); the paper operationalizes this distinction in its model by mapping each superordinate concept to prototype codebooks containing basic concept slots.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "Concept hierarchy (superordinate and basic-level concepts)",
            "theory_or_model_description": "Functionally, concepts are organized hierarchically: superordinate categories group related basic-level concept realizations; representations may be allocated per level (e.g., separate prototype slots per superordinate category) and binding occurs between basic realizations and their superordinate label.",
            "representation_format_type": "hierarchical categorical (superordinate categories containing discrete basic-level realizations/prototypes)",
            "key_properties": "Multi-level abstraction, enables compositional description (e.g., object has color, shape, size), supports modular learning and addition of new concepts by expanding codebooks/read-out modules, and allows different representational granularity across levels.",
            "empirical_support": "The paper draws on cognitive-science distinctions (superordinate/basic) and uses them to structure model read-out encoders and codebooks; psychological literature supports hierarchical categorization and different processing for basic vs superordinate levels.",
            "empirical_challenges": "Determining the correct level(s) of abstraction for tasks, variability in what counts as 'basic' across domains and individuals, and how hierarchical levels interact during learning are open issues; the paper notes that concept granularity is subjective and dynamic.",
            "applied_domains_or_tasks": "Object categorization, multi-attribute concept learning, interactive human–machine concept revision, online learning of novel attributes or categories.",
            "comparison_to_other_models": "This hierarchical functional organization contrasts with flat feature-based or purely distributed representations by explicitly separating category-level encoding and per-category prototype slots; it facilitates modular updates (e.g., adding a new superordinate concept) unlike monolithic continuous latent spaces.",
            "functional_mechanisms": "Separate read-out encoders extract information relevant to each superordinate category; each such encoder maps to a codebook of prototype slots representing basic realizations; binding and discretization (e.g., via softmax temperature annealing) assign basic realizations to specific prototype slots.",
            "limitations_or_open_questions": "How to determine the number of basic realizations per superordinate category in naturalistic domains, dealing with concepts that cut across categories, subjectivity in granularity, and the mechanisms by which humans flexibly re-structure hierarchies.",
            "uuid": "e5153.3"
        },
        {
            "name_short": "Empirical evidence for prototypes in memory",
            "name_full": "Behavioral/neuroimaging findings reporting prototype-like representations",
            "brief_description": "Empirical findings indicating that human memory and categorization often reflect prototype-like representations (central tendencies) and that prototype and exemplar traces can be tracked across learning; the paper cites multiple studies supporting prototype usage in humans.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Empirical findings: prototype traces in behavior and brain",
            "theory_or_model_description": "Reports from behavioral and neural studies showing central-tendency/prototype influences on categorization and memory, and longitudinal tracking studies showing emergence and change of prototype- and exemplar-like representations during learning.",
            "representation_format_type": "observed prototype-like behavioral/neural codes (functional-level central tendencies / averaged representations)",
            "key_properties": "Behavioral generalization toward prototypical exemplars, prototype-based reconstruction biases, measurable change across learning, co-existence with exemplar traces in some paradigms.",
            "empirical_support": "The paper refers to work finding prototypes in human memory ([17,30,38] per paper), and cites 'Tracking prototype and exemplar representations in the brain across learning' (eLife) as an example of empirical work tracking both types of representations over time.",
            "empirical_challenges": "Other empirical results support exemplar accounts or mixed accounts; the existence of both prototype and exemplar signatures complicates clean interpretation, and some tasks reveal context- and task-dependence in which representation dominates.",
            "applied_domains_or_tasks": "Categorization experiments, memory reconstruction tasks, longitudinal learning studies, neuroimaging studies tracking representational changes during training.",
            "comparison_to_other_models": "Empirical findings often are consistent with both prototype and exemplar models when analyzed superficially; targeted paradigms are necessary to disambiguate, and mixed/hybrid interpretations often provide best fit.",
            "functional_mechanisms": "Observed mechanisms include averaging of encountered instances into a prototypical bias and parallel storage/retrieval of exemplars; empirically measured as biases in classification and neural pattern similarity to prototypical stimuli.",
            "limitations_or_open_questions": "Extent to which prototype effects are primary versus derivative of exemplar memory, task-dependence of observed effects, and the precise functional triggers for prototype formation versus exemplar reliance remain open.",
            "uuid": "e5153.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tracking prototype and exemplar representations in the brain across learning",
            "rating": 2,
            "sanitized_title": "tracking_prototype_and_exemplar_representations_in_the_brain_across_learning"
        },
        {
            "paper_title": "Prototypes, exemplars, and the natural history of categorization",
            "rating": 2,
            "sanitized_title": "prototypes_exemplars_and_the_natural_history_of_categorization"
        },
        {
            "paper_title": "Prototype Learning Systems",
            "rating": 2,
            "sanitized_title": "prototype_learning_systems"
        },
        {
            "paper_title": "Prototype constructions in early language acquisition",
            "rating": 1,
            "sanitized_title": "prototype_constructions_in_early_language_acquisition"
        },
        {
            "paper_title": "Prototype learning and dissociable categorization systems in alzheimer's disease",
            "rating": 1,
            "sanitized_title": "prototype_learning_and_dissociable_categorization_systems_in_alzheimers_disease"
        }
    ],
    "cost": 0.013756749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Interactive Disentanglement: Learning Concepts by Interacting with their Prototype Representations</p>
<p>Wolfgang Stammer wolfgang.stammer@cs 
Computer Science Department
DarmstadtTU</p>
<p>Hessian Center for AI (hessian.AI)</p>
<p>Marius Memmel marius.memmel@stud 
Computer Science Department
DarmstadtTU</p>
<p>Patrick Schramowski schramowski@cs 
Computer Science Department
DarmstadtTU</p>
<p>Hessian Center for AI (hessian.AI)</p>
<p>Kristian Kersting kersting@cs.tu-darmstadt.de 
Computer Science Department
DarmstadtTU</p>
<p>Centre for Cognitive Science
TU
Darmstadt</p>
<p>Hessian Center for AI (hessian.AI)</p>
<p>Interactive Disentanglement: Learning Concepts by Interacting with their Prototype Representations</p>
<p>Learning visual concepts from raw images without strong supervision is a challenging task. In this work, we show the advantages of prototype representations for understanding and revising the latent space of neural concept learners. For this purpose, we introduce interactive Concept Swapping Networks (iCSNs), a novel framework for learning concept-grounded representations via weak supervision and implicit prototype representations. iCSNs learn to bind conceptual information to specific prototype slots by swapping the latent representations of paired images. This semantically grounded and discrete latent space facilitates human understanding and human-machine interaction. We support this claim by conducting experiments on our novel data set "Elementary Concept Reasoning" (ECR), focusing on visual concepts shared by geometric objects. 1</p>
<p>Introduction</p>
<p>Learning an adequate representation of concepts from raw data without strong supervision is a challenging task. However, it remains important for research in areas of knowledge discovery where sufficient prior knowledge is missing, and the goal is to attain novel understandings. With better representations and architectural components of machine learning models, this appears to become more and more achievable [73]. However, if remained unchecked this bears the danger of learning incorrect concepts or even confounding features [18,70]. A further difficult aspect of concept learning, regardless of the level of supervision, is its dynamic and subjective nature. One downstream task might require more fine-grained concepts than others, but also when encountering evidence on novel concepts (e.g. in an online learning setting), the knowledge and hierarchy of concepts should be constantly re-approved, discussed, and 1 Code available at: https://github.com/ml-research/ XIConceptLearning Would you consider these objects to share an underlying concept?</p>
<p>Yes, they all have the color blue.</p>
<p>Would you consider these objects to share an underlying concept?</p>
<p>Yes, they all have a triangular shape.</p>
<p>What about these objects?</p>
<p>No, I think these three and these three objects share a concept: possibly updated. It thus remains desirable that the representations learned by such concept learners to be humanunderstandable and revisable.</p>
<p>An evident approach to teaching concept information to a machine learning model is to train it in a supervised fashion through symbolic representations, e.g., one-hot encoding vectors and corresponding raw input [44,85]. However, this requires extensive prior knowledge of relevant concepts and seems impractical given the subjective and dynamic nature of concept learning.</p>
<p>Another branch of research focuses on learning disentangled latent distribution models [27,31,80]. Although initially focused on unsupervised approaches, many recent studies have shifted away from unsupervised learn-I'm not sure. However, it is very similar to this prototypical large, blue circle:</p>
<p>What do you call this object?:</p>
<p>You're right. It is large and blue.</p>
<p>I love the color blue! But actually this represents a new shape.</p>
<p>Great thanks! What is the concept of love? ing and show promising results with weak supervision [41,49,51,58,74,81]. An often implicit assumption of disentanglement research is that the learned latent representations should correspond to human-interpretable factors. Many state-of-the-art variational [51,58] and generative adversarial [13,46,56,59,60] approaches, however, learn continuous latent representations, making these difficult for a human to understand without additional techniques for interpreting the latent space [68].</p>
<p>Due to the intricate nature of concept learning and inspired by findings on concept prototypes in the fields of psychology and cognitive science, we investigate the advantages of prototype representations in learning humanunderstandable and revisable concept representations for neural concept learners. To this end, we introduce the novel framework of Interactive Concept Swapping Network (iCSN) that learns to implicitly bind semantic concepts to latent prototype representations through weak supervision. This binding is enforced via a discretized distance estimation and swapping of shared concept representations between paired data samples. Among other things, iCSNs allow for querying and revising its learned concepts cf. Fig. 1, and integrating knowledge about unseen concepts cf. Fig. 2.</p>
<p>Explicitly focusing on learning object-centric visual concepts, we develop Elementary Concept Reasoning (ECR), a novel data set containing images of 2D geometrical objects and perform multiple experiments, emphasizing the advantages of our approach. To summarize, our work highlights the advantages of prototype representations for (i) learning a consistent and human-understandable latent space through weak supervision, (ii) revising concept representations via human interactions, and (iii) updating these in an online learning fashion.</p>
<p>Related Work</p>
<p>Concept learning. Many previous concept learning approaches focus on predicting selected high-level concepts for improving additional downstream tasks [3,44,85]. Several studies highlight the benefits of concept-based machine learning for explainability [1,25,54,84] and human interactions [77]. To communicate the concepts to a human user, some approaches include first-order logic formulas [16], causal relationships [83], user defined concepts [42], prediction of intermediate dataset-labels [3,54], and one-hot encoded bottlenecks [44]. All of these approaches, however, focus on supervised concept learning. Concept representations in psychology. The term concept is rooted in psychology where it can be defined as "the label of a set of things that have something in common" [2], though different notions do exist [23]. Most common approaches to represent concepts are exemplars and prototypes. Where the former approach assumes that one or multiple typical examples of a concept are maintained in memory, the latter only assumes an average representation over several observed examples [39,61,71,75,78]. The lines between exemplar and prototype representation become more blurred in the field of cognitive psychology, and their contribution to concept representations is still an open problem, with recent work hinting at the use of both representations [6,24]. Nonetheless, there remains evidence of the use and importance of prototypes in the human memory system [17,30,38]. Inspiration sparked by such findings gave rise to Prototype Learning Systems [87]. Neural prototype learning. Recent approaches to artificial prototype learning systems focus on neural networks with prototype vectors as internal latent representations. These vectors can be converted into explainable visualizations via decoding approaches [45] or used for finding the most similar training example [11]. In both works, a class prediction is made based on the similarity of an encoded input to the model's prototypes via a simple distance metric. Lastly, especially in the context of few-shot learning, prototypes show advantageous properties [37,62,64,76]. Disentanglement. The field of disentanglement research is also closely related to our work. Here the goal is to extract independent underlying factors that are responsible for generating the data [4]. Recently, through the work of Locatello et al. [49] much of disentanglement research has shifted from unsupervised learning to the weakly supervised setting. Shu et al. [74] show that supervision via match pairing for a known subset of factors gives guarantees for disentanglement via their defined calculus of consistency and restrictiveness. Literature also extends to group-based disentanglement, allowing for grouping of the identified generative factors [5,33,34,36,86,88]. However, the interpretation of the latent representations from these approaches remains an open question [68].  Network. An iCSN is based on a deterministic autoencoder structure providing an initially entangled latent encoding (1). Several read-out encoders (2) extract relevant information from this latent space and compare their extracted concept encodings to a set of prototype slots (3) via a weighted, softmax-based dot product (4). This leads to a discretized code that indicates the most similar prototype slot of each concept encoding. iCSNs are trained via a simple reconstruction loss, weak supervision via match pairing and a swapping approach that swaps (5) the latent concept representations for shared concepts, enforcing the binding of semantic information to specific prototype representations.</p>
<p>Explanatory interactive learning (XIL). The notion of human interactions on a model's latent concept representations, e.g., to correct confounding behaviour, is closely related to the field of XIL [69,70,72,77,79]. Specifically, XIL incorporates the human user into the training loop by allowing for them to interact via a model's explanations. Rather than interacting via post-hoc explanations of previous XIL approaches, we focus on interacting directly with the latent representations of a model. These, nonetheless, possess a connection to the model's explanations in our setup. Even though we take a more direct approach to revising a model's internal representations, similar feedback methods as in XIL are applicable for our work.</p>
<p>Interactive Concept Swapping Networks</p>
<p>In this section, we explain the basic architectural components of an Interactive Concept Swapping Network (iCSN) before introducing the training procedure and how to interact with the implicit prototype representations of these networks. For an overview, see Fig. 3. Prototype-based concept architecture. Assume an input x i ∈ X, with X := [x 1 , ..., x N ] ∈ R N ×D . For the sake of simplicity, we remove the sample index i from further notations below and denote with x ∈ R D an entire image. However, in our framework, x can also be a latent representation of a subregion of the image. This subregion can be implicitly or explicitly extracted from the image by a pre-processing step, e.g. via segmentation algorithms [8,26,29,66] or compositional generative scene models [7, 19-21, 28, 48, 52]. Additionally, we assume each x to contain several attributes such as color, shape and size. Specifically, we refer to the realizations of these attributes, e.g., a "blue color" or "triangular shape" as a basic concept. In contrast, we refer to "color" as a category concept or, as often called in the field of cognitive and psychological sciences, superordinate concept [22]. Each image x therefore has the ground truth basic concepts c := [c 1 , ..., c J ] with J denoting the total number of superordinate concepts. We make the necessary assumption that x can only contain one basic concept realization per superordinate concept. For simplicity, we furthermore assume that each superordinate concept contains the same number of basic concepts K which might vary in practice as we are going to show in our experiments.</p>
<p>Assuming an encoder-decoder structure, we define an input encoder h(·) that receives the image x and encodes it into a latent representation z ∈ R Z by h(x) = z. Rather than reconstructing directly from z, as done by many autoencoder-based approaches, an iCSN first applies several read-out encoders m j (·) to the latent representation z resulting in m j (z) = φ j ∈ R Q with j ∈ [1, ..., J]. We refer to the encoding φ j as a concept encoding. The goal of each read-out encoder is to extract the relevant information from the entangled latent space z that corresponds to a superordinate concept, e.g. color. We discuss how we enforce this extraction of concept specific information below.</p>
<p>One central component of the iCSN is a set of codebooks each containing multiple prototype slots. We define this set as Θ := [P 1 , ..., P J ], with a single codebook as P j ∈ R Q×K . Each codebook contains an ordered set of trainable, randomly initialized prototype slots p j ∈ R Q , i.e.,
P j := [p 1 j , ..., p K j ].
To enforce the assignment of each concept encoding φ j to one prototype slot of P j , we define a similarity score S dot (·, ·) as a softmax over the dot product between its two inputs. This way we obtain the similarity between a concept encoding, φ j , and specific prototype slot, p k j with:
s k j = S dot (φ j , p k j ) = exp (φ j · p k j / √ Q) K k=1 exp (φ j · p k j / √ Q)(1)
The resulting similarity vector s j ∈ R K contains the similarity score for each prototype slot of category j with the concept encoding φ j . To enforce further discretization and the binding of concepts to individual prototype slots, we introduce a second function S τ (·) to apply a weighted softmax function to the similarity scores:
Π k j = S τ (s k j ) = exp (s k j /τ ) K k=1 exp (s k j /τ ) ,(2)
with Π j ∈ R K and weight parameter τ ∈ R + . In our experiments we step-wise decrease τ to gradually enforce the binding of information. In the extreme case of τ → 0, Π j resembles a one-hot vector (multi-label one-hot vector in the case of J &gt; 1), indicating which prototype slot of category j the concept encoding φ j is most similar to. Finally, we concatenate the weighted similarity scores of each category into a single vector to receive the final prototype distance codes y := [Π 1 , ..., Π J ] ∈ [0, 1] J·K which we pass to the decoder g(·) to reconstruct the image:
g(y) =x ∈ R D .
Concept swapping and weak supervision. Prior to training, i.e., after initialization, there is no semantic knowledge bound to the prototype slots yet. Each prototype carries just as little meaning as the other. The semantic knowledge found in converged iCSNs, however, is indirectly learned via a weakly-supervised training procedure and by employing a simple swapping trick.</p>
<p>We adopt the match pairing approach of Shu et al. [74], a practical weakly-supervised training procedure to overcome the issues of unsupervised disentanglement [50]. In this approach, a pair of images (x, x ) is observed that shares values for a known subset of underlying factors of variation within the data, e.g. color, while the total number of shared factors can vary between 1 and J − 1. In this way, a model can use the additional information of the pairing to constrain and guide the learning of its latent representations.</p>
<p>Previous works on weakly-supervised training, specifically of VAEs, reverted to applying a product [5] or an average [35] of the encoder distributions of x and x at the shared factor IDs. Locatello et al. [51], extended these works to a setting with an even weaker form of supervision but carries fewer disentanglement guarantees. In comparison to these works, an iCSN uses a simple swapping trick between paired representations, similar to Caron et al. [9]. Specifically, with v being the shared factor ID between the image pairs (x, x ) the corresponding similarity scores (Π v , Π v ) are swapped between the final correspond-ing prototype codes, resulting in:
y := [Π 1 , ..., Π v , ..., Π J ], y := [Π 1 , ..., Π v , ..., Π J ].
This swapping procedure has the intuitive semantic that it forces an iCSN to extract information from the first image that it can use to represent properties of the category v of the second image.</p>
<p>Pseudo-code of iCSNs can be found in the Supplementary Materials. Training objective. iCSNs are finally trained with a single pixel-wise reconstruction loss per paired image over batches of size N :
L = 1 2N N i=1 (x i −x i ) 2 + (x i −x i ) 2(3)
This simple loss term stands in contrast to several previous works on prototype learning, which enforce semantic binding via an additional consistency loss [45,57,64]. By including the semantic binding implicitly into the network architecture, we eliminate the need for additional hyperparameters and a more complex optimization process over multiple objectives. Interacting with iCSNs. The goal of iCSNs, especially in comparison to VAEs, is not necessarily to be a generative latent-variable model that learns the underlying data distribution, but to learn prototypical concept representations that are human-understandable and interactable. The autoencoder structure is thus a means to an end rather than a necessity. However, instead of discarding the decoder after convergence, an iCSN can present an input sample's closest prototypical reconstruction of each concept. Thus, by querying these prototypical reconstructions at test time, a human user can confirm whether the predicted concepts make sense and possibly detect undesired model behavior. By defining a threshold on the test time reconstruction error, an iCSN can give a heuristic indication of its certainty in recognizing concepts in novel samples.</p>
<p>Due to the discrete and semantically bound latent code y, a human user can easily interact with iCSNs by treating y as a multi-label one-hot encoding. Specifically, a human user can revise and add knowledge via additional loss terms to the full extent of Stammer et al. [77]. For example, via logical statements such as ∀img. ⇒ ¬hasconcept(img, p 1 1 ) or ∀img. isin(img, imgset) ⇒ hasconcept(img, p 1 2 ), a user can formulate logical constraints which read as "Never detect the concept represented by the prototype p 1 1 ." and "For every image in this set of images you should be detecting the concept represented by prototype p 1 2 .", respectively. The set of incorrectly represented images can be curated by the user interactively.</p>
<p>Lastly, the modularity of iCSNs has additional advantages or interactive online learning, e.g., when the model is provided with data samples that contain novel concepts or when a factor that is present in the data is initially deemed unimportant but considered important retrospective to the initial learning phase. The approach for interaction in both cases depends on the hierarchy of the concept to be learned, namely if it is a basic concept or a superordinate concept. In the case of a novel basic concept, the approach is straightforward. Assuming human user satisfaction with the previous concept representations of an iCSN, and that J, the total number of prototype slots per codebook, was set to be overestimated, a user can simply give the feedback to represent a novel basic concept via one of the unused prototype slots of the relevant category.</p>
<p>In case a novel superordinate concept should be learned, one can apply a simple trick during the initial training phase by adding an additional read-out encoderm J+1 (z) = φ J+1 ∈ R L . In contrast to the other read-out encoders, this one does not map to the space of the prototype slots where it would consecutively be discretized via S dot (·, ·) and S τ (·). Instead,φ J+1 remains a continuous representation that is directly concatenated to the final latent codes y := [Π 0 , Π 1 , ..., Π J ,φ J+1 ]. In this way,m J+1 (·) can learn to incorporate all information that should not be discretized via the usual procedures. Ultimately, the initial latent space z of an iCSN can be trained to represent the full data distribution, even though only specific concepts should be discretized from this space. To include concepts that were initially considered irrelevant, one can just expand J which means to add a new read-out encoder m J+1 (z) = φ J+1 ∈ R Q and codebook P J+1 to the iCSN. Then, m J+1 learns to bind novel basic concepts from the "novel" superordinate concept to P J+1 which only requires novel data pairs exemplifying the previously unimportant concept. Additional remarks. To summarize: the gradient of Eq. (3) provides the learning signal for the entire network, including the initial encoder, read-out encoders, prototype slots and decoder. Furthermore, by decreasing the softmax temperature τ in a step-wise fashion, one can enforce the binding of information to specific prototypes such that a specific concept is mapped to an individual prototype slot. Through this discretization process, the decoder learns to produce reconstructions that correspond to the prototypical representations present in the data. For example, given a pair of images of blue objects that vary in their shade of blue, an iCSN would learn to map the color information of these objects to the same prototype slot, thus learning the prototypical blue of both shades. This discretization step is a key difference to the various GAN, and VAE approaches with Gaussian distributions, which try to learn a continuous latent space of the underlying factors.</p>
<p>Elementary Concept Reasoning Data Set</p>
<p>Recent studies show the benefits of object-centric learning for performing complex downstream tasks [52,77,85].  Each sample image (left) depicts a centered 2D object with three different properties: color, shape, and size. Images are paired such that the objects share between one and two concepts (right).</p>
<p>Thus, rather than learning concepts of an entire image, e.g. as Kim et al. [43], we introduce a novel benchmark data set, Elementary Concept Reasoning (ECR), which explicitly focuses on object-centric visual concept learning.</p>
<p>ECR consists of RGB images (64 × 64 × 3) of 2D geometric objects on a constant colored background. Objects can vary in shape (circle, triangle, square, and pentagon), size (large and small), and color (red, green, blue, yellow). We add uniform jitter to each color, resulting in various color shades. Each image contains a single object which is fixed to the center of the image. Furthermore, ECR contains image pairs following the match pairing setup of Shu et al. [74]. We pair images such that the objects in the individual images share at least one, but at most J − 1 common properties. ECR contains a training set size of 5000 image pairs and 2000 images for validation. Figure 4 shows example images of ECR. Random samples are presented on the left, exemplifying shape, size, and color combinations. Example image pairs are presented on the right. An important feature of ECR is that although various shades of colors exist, they all map to four discrete colors. Notice, e.g., the color difference of the two paired blue objects. Even though both objects present different shades of blue, their state of being paired indicates that they share the same distinct shape (pentagon) and color (blue) concept.</p>
<p>Results</p>
<p>In this section, we demonstrate the advantages of prototype-based representations via Interactive Concept Swapping Networks. We begin our analysis by investigating the sparsity and semantics of iCSNs' latent representations. Next, we show that the model can communicate the extracted concepts to a human user due to its discretized latent space. Subsequently, we simulate human user interactions via simple feedback rules, which are sufficient to revise an iCSNs' latent concept space. Lastly, we show that novel concepts can easily be added into the concept space of iCSNs via simple human interactions. </p>
<p>Code Variance</p>
<p>(averaged over Concepts) Figure 5. Average latent code variance given the ground truth concept labels for different model types and training settings. The compared models: unsupervised trained β-VAE, Ada-VAE with paired images, VAE and categorical VAE with paired images and known shared factor IDs, the novel CSN and iCSN with additional interactions on the learned concept space. Note that a lower variance is desirable.</p>
<p>Experimental details. For our experiments, we compare the iCSN to several baselines including the unsupervisedlytrained β-VAE [31] and Ada-VAE by Locatello et al. [51], using the arithmetic mean of the encoder distributions as in [35]. For a fair comparison with iCSNs which are trained via the shared match pairing of [74] and the Ada-VAE, which was originally introduced as a weaker form of supervision, we also trained the Ada-VAE with known shared factor IDs. This baseline essentially resembles a β-VAE with an averaging of encoder distributions between pairs of images at the known shared factor IDs. It is denoted as VAE in the results below. Lastly, we compare to a discretizing VAE approach which uses a categorical distribution via the Gumbel-softmax trick [40,53] (Cat-VAE). Cat-VAE is trained the same way as the VAE, i.e., via share pairing and averaging over encoder distributions. We train the iCSN with a simple reconstruction loss as in Eq. (3) and decreasing softmax temperature. We present the results of two iCSN configurations. The vanilla setting, denoted as Concept Swapping Network (CSN), corresponds to an iCSN prior to human interactions with the correct number of superordinate concepts (J = 3) and an overestimated number K of prototype slots per superordinate concept (K = 6). Finally, iCSN denotes a CSN after the initial training phase with additional user interactions.</p>
<p>The number of latent variables for β-VAE, Ada-VAE, and VAE was set to the ground truth number of superordinate concepts. All Cat-VAE runs were performed with three categorical distributions each with k = 6 events.</p>
<p>All configurations were trained with five random seed initializations, and the results present the mean and standard deviation of these runs. All presented results were obtained from a held-out validation set. Further details can be found in the Supplementary Materials. Reduced code variance. Human understandability of and interactions with a model's latent space strongly benefit from consistency in a model's concept representations. In other words, the representation of the color blue should be specific to this concept and the latent representation for the blue color of one object should be very similar to that of a second blue object. If this is not the case, it remains difficult for a human user to identify and interact with these learned concepts.</p>
<p>Motivated by this intuition, we first investigate the variance of the latent representations given the ground truth multi-label information of each validation image. For this, we compute the latent code variance over all validation images of each concept. In mathematical notation this corresponds to:
1 K · J J=3 j=1 K k=1 Var({z j } l==k )(4)
with Var(·) denoting the variance,z j denoting a place holder for the latent representation of a corresponding model (the discretized prototype distance code y for iCSNs, the distribution means for VAEs with Gaussian distributions and the event probabilities for Cat-VAE). {·} l==k denotes the set of latent representations from images for which the ground truth basic concept of category j corresponds to k. The resulting code variances over all models can be seen in Fig. 5. Note that a low code variance is desirable and indicates how well a concept is mapped to a distinct representation. The results in Fig. 5 suggest that the variance of the latent space from CSNs is much lower, showing more consistent concept representations. However, a reduced latent code variance is not a sufficient criterion for concept consistency and human understandability. For example, a model that learns to map all concepts to a single representation has zero latent code variance but also no representational power. Therefore, we turn to probing the latent concept space via linear models next.</p>
<p>Probing the latent space. Similar to works of the selfsupervision community [10,12,14,15,63], we investigate the latent code of each model via linear probing. For this, the latent codes of each model on a held-out data set are inferred, as in the previous experiment. Next, ground truth labels are obtained by converting each multi-label ground truth vector, c, of this data set to a 32-dimensional one-hot encoding. Finally, a Decision Tree (DT) and a Logistic Regression (LR) are trained supervisedly on this data set and validated on an additional held-out data set.</p>
<p>The results in Tab. 1 (top) document the average accuracy and standard deviation on the held-out validation set over the five random initializations for the different models. We observe that the latent code of CSNs allows for nearly perfect predictive performance and surpasses all variational approaches. Importantly, CSNs' representations even surpass those of VAE approaches (VAE and Cat-VAE) that were trained with the same type of weak supervision as CSNs. As expected, the β-VAE performs worse on average than the weakly-supervised models. Interestingly, however, the Ada-VAE configuration performed worse than the β-VAE. In addition, the discrete latent representations of Cat-VAE also perform worse than CSNs. Noticeably, the Cat-VAE runs indicate a high deviation in performance, indicating that several Cat-VAE runs converged to sub-optimal states. In summary, although the ECR data set only contains variations in individual 2D geometrical objects, the baseline models do not perform as well as CSNs, even when trained with the same amount of information. Explaining and revising the latent space. An advantage of an iCSN's semantically bound, discrete latent space, is the straightforward identification of sub-optimal concept representations by a human user cf. Fig. 1. Upon identifying correctly or falsely learned concepts, a user can then apply simple logical feedback rules on this discrete concept space.</p>
<p>Specifically, after training via weak supervision, it is recommendable for the machine and human user to discuss the learned concepts and identify whether these coincide with the user's knowledge or if a revision is necessary. For example, an iCSN can learn to represent a color over several prototype slots or represent two shapes via one slot, indicating that it falsely considers these to belong to the same concept. An iCSN can then convey its learned concepts in two ways. First, it can group novel images that share a concept according to its inferred discrete prototype distance codes and inquire a human user if indeed the grouped images share a common underlying concept cf. Fig. 1. Second, utilizing the decoder, it can present the prototypical reconstruction of each learned concept, e.g., presenting an object with a prototypical shade of blue cf. Fig. 2.</p>
<p>Having identified potential sub-optimal concept representations, a human user can now interact on the discretized latent space of iCSNs via logical rules and further improve the representations, which we demonstrate via simulated user interactions in the following. For all previous runs of the vanilla CSN configuration, we visually inspect the concept encodings y for one example each of the 32 possible concept combinations and identify those prototype slots which are "activated" in the majority of examples per individual concept (primary slots) and, additionally, identify those prototype slots per concept that are never or rarely activated within our subset of examples (secondary slots). We next apply an L2 loss on y to never use these secondary slots and finetune the previous runs on the original training set with the original reconstruction loss and this additional L2 loss. The semantics of this feedback is that concepts should only be represented by their primary prototype slots. Additionally, in two runs we revise an observed sub-optimal solution that pentagons and circles are bound to the same prototype slot. Hereby, feedback is provided on all pentagon samples of the training set to bind to an otherwise empty prototype slot, again via an additional L2 loss.</p>
<p>The results of these interactions can be found under iCSN in Fig. 5 and Tab. 1 (top) indicating a near-zero latent code variance per ground truth concept and perfect linear probing accuracy, respectively. Thus, indicating the ease of interacting with and revising the latent space of iCSNs. Interactively learning novel basic concept. Furthermore, the prototype-based representations of iCSNs possess interesting properties for an online learning setting, e.g., when encountering novel concepts which the model has not seen before. Through the decoder of the iCSN, evaluating the reconstruction and reconstruction error can serve as a means for identifying whether the model has a good representation of a novel sample cf. Fig. 2.</p>
<p>When teaching an iCSN a novel basic concept like a halfcircle shape cf. Fig. 6, a user can identify an unbound prototype slot of the model's latent representation and encourage the binding to this slot. To prevent catastrophic forgetting [55,65], i.e., overriding already learned concepts, we employ a combination of common rehearsal [67] and knowledge distillation methods [32,47] by letting the model predict the latent codes of past samples and restrict the iCSN to not deviate from those. Specifically, we use a simple L2 loss on the known and unknown one-hot concept encodings to encourage binding of the unknown concepts while not forgetting the known ones. Figure 6 (top) shows the linear model prediction accuracies on the latent space of iCSNs that have been presented a data set containing the novel concept halfcircle. The tag before indicates the accuracy on the latent code of the iCSN runs of Tab. 1 (top) that were trained on the standard ECR concepts, whereas after indicates the accuracy on the latent code after additional interactions with a simulated user by providing the information which empty prototype slot of the shape codebook to bind the novel concept to. These results  Figure 6. Linear probing via Decision Tree (DT) and Logistic Regression (LR) on the latent codes of iCSN. We evaluate the models with a ECR data set containing a novel shape (top) and a novel superordinate spot concept (bottom), each not seen during initial training (before). After human user interactions (after) this novel information could be easily added to the concept representations.</p>
<p>indicate the ease of adding additional knowledge on novel basic concepts to the latent representation of iCSNs.</p>
<p>Interactively learning novel superordinate concept. Next, we showcase how to add a novel superordinate concept. For this setting, we make use of a variation of the ECR data set where white spots were added to the center of roughly half of the objects cf. Fig. 6. The other half depicts a solid color as in the original ECR data set. We consider an online learning setting where spots are unimportant during the initial training but reconsidered important in the second round of interaction. The modularity of iCSNs allows us to easily add a read-out encoder during initial training that learns to represent all the information of the data that is not discretized via the initial paired samples (cf. Sec. 3 for details). In the second round of training, this continuous read-out encoder can be replaced with a discretizing read-out encoder and additional codebook. This modularity property further eliminates the danger of catastrophic forgetting in that all previously trained modules can be frozen in the second training round, thus only requiring the novel read-out encoder to be finetuned.</p>
<p>Human user interactions are simulated by assuming that the iCSN has correctly learned the previous concepts with an additional continuous read-out encoder for representing the spotted feature. Subsequently, additional training pairs are introduced that exemplify the novel superordinate concept, and the new read-out encoder is finetuned as in the standard training setting. Results can be seen in Fig. 6 again presenting the linear model accuracy on the latent representations before and after simulated user interactions, indicating that the novel superordinate concept can easily be bound to the model's internal prototype representations.</p>
<p>Here, we remark on desirable properties of iCSNs for handling confounded data. Assuming an undesired confounding factor within the data generation process that causes spurious features, an iCSN can learn to ignore these features during its training process via the mechanism presented above. This stands in comparison to GANs or varia-tional approaches with Gaussian distributions, which could potentially learn also to model the spurious features. Ablation studies. To assess the importance of the different components of iCSNs, we conduct an ablation study and depict the linear probing classification performances in Tab. 1 (bottom). Specifically, we test a Cat-VAE that uses swapping of the relevant encoder distributions, rather than averaging as in previous experiments. And, secondly, we test an iCSN with averaging of the concept encodings φ j , rather than swapping. With these experiments, we wish to (i) compare the discretization via distances to prototypes to discretization via categorical distributions (Cat-VAE w. swapping) and (ii) test the influence of swapping versus averaging of encodings for iCSNs (iCSN w. avg).</p>
<p>The results of Tab. 1 (bottom) when compared with Tab. 1 (top) indicate that discrete representations via distances to prototypes are, in fact, beneficial compared to those of the categorical distributions of Cat-VAEs. Secondly, the swapping procedure appears to be crucial for optimal learning of concept representations in iCSNs. Limitations. Following assumptions were made in this work: a superordinate concept is divisible into multiple basic concepts and "valid" user feedback was provided in our experiments on interactions. A potential limiting factor of iCSNs is the training reconstruction loss which might be insufficient for learning fine-grained concepts. Additionally, we observed that the choice of τ can influence the quality of the learning process. Setting a small value too early can lead to sub-optimal solutions. Lastly, our approach was tested on ECR due to its object-centric nature and distinct concept distribution of the data set. For more complex settings additional architectures may be required to pre-process the data to e.g. extract objects from an image.</p>
<p>Conclusion</p>
<p>In this work, we investigated the properties of latent prototype representations for neural concept learning with weak supervision.</p>
<p>The results with our novel iCSN framework indicate that these are beneficial for human-understandable concept learning but also human interactions and the incorporation of novel concepts within an online learning setting. Interesting pathways for future research are applying iCSNs to more complex data sets, particularly from critical domains such as medical or scientific data where often relevant concepts are not known in advance, however standard deep learning approaches can learn to focus on confounding factors, e.g. for Covid-19 data [18] or plant phenotyping data [70]. We hypothesize the interactive approach of iCSNs to be beneficial in allowing the machine and human to identify relevant and irrelevant concepts within the data jointly.</p>
<p>Figure 1 .
1A trained model (left) queries the human user (right) if the concepts that it has extracted from the data coincides with the knowledge of the user. Subsequently, the model can receive revisions from the user.</p>
<p>Figure 2 .
2Human-machine interaction for learning about novel concepts. The user queries an object and guides the machine's prototype suggestion if necessary.</p>
<p>Figure 3 .
3Interactive Concept Swapping</p>
<p>Figure 4 .
4Samples of the Elementary Concept Reasoning data set.</p>
<p>iCSN DT LR
DTNovel Basic Concept before 78.87 ± 1.31 78.73 ± 1.22 after 98.3 ± 1.32 98.28 ± 1.33Novel Superordinate Concept 
before 93.1 ± 4.46 
67.54 ± 9.07 
after 
99.85 ± 0.3 98.29 ± 3.42 </p>
<p>Ackowledgements.The authors thank the anony-mous reviewers for their valuable feedback and Cigdem Turan forFigure 1and 2 sketches. The work has received funding from the BMEL/BLE under the innovation support program, project "AuDiSens" (FKZ28151NA187). It benefited from the Hessian research priority programme LOEWE within the project WhiteBox as well as from the HMWK cluster project "The Third Wave of AI.".In our experiments, the prototype slots were initialized randomly from a truncated Gaussian distribution with mean µ = 0, variance σ 2 = 0.5, minimum a = −1, and maximum b = 1. The encoder used in our experiments was a convolutional neural network with residual connections and ReLU activations. Each read-out encoder is a linear layer with LeakyReLU activations. Lastly, the decoder architecture was again a neural network with transposed convolutions and also here residual layers. In the standard ECR experiments with iCSNs, J = 3, Z = 512, Q = 128, K = 6 for each j ∈ [1, ..., J], N = 128 and τ was decreased every 1000 epochs with steps [2., 0.5, 0.1, 0.01, 0.001, 0.0001, 0.00001] over 8000 epochs in total. Notably, group normalization as proposed by Wu et al.[82]was applied after extracting the concept encodings via the read-out encoders (performed in collectedReadOutEncoders in Alg. 1). Pseudo code can be found in Alg. 1 and Alg. 2.Baseline modelsFor the experiments with Cat-VAE, the softmax temperature was set to τ = 0.1 and each categorical distribution had k = 6 categories. The number of latent variables was set to 3 for Cat-VAE, β-VAE, Ada-VAE, and VAE runs. For all baselines, the encoder and decoder consisted of a convolutional and transposed convolutional network. In all experiments, β = 4., except for Ada-VAE, where β = 1., as recommended by Locatello et al.[51]. All baseline models were trained for 2000 epochs.Linear probingThe linear models for probing the latent representations of the different model configurations were a decision tree and logistic regression model. The max depth of the decision tree was set to 8. The logistic regression model was run with parameters C = 0.316 and the maximum number of iterations at 1000. Both the decision tree and logistic regression model were trained with a fixed random seed.Details on simulated interactionsThe simulated user interactions were performed via an L 2 regulatory loss term on the latent codes y. In case a user tells an iCSN not to use a specific prototype slot of the superordinate concept j and slot k, this loss corresponds to: M SE(y j·k+k , 0), where y j·k+k corresponds to the value of y at position j · k + k and 0 being a vector of length N . When a user provides a subset of examples with corresponding desired prototype slot IDs the loss term corresponds to: M SE(y subset j·k+k , 1) with 1 of length equal to the number of samples in subset. The subset of examples in our simulated interactions were identified via the ground truth labels, e.g., for identifying the subset of images containing a pentagon. The interactions for learning a novel basic concept followed the same procedure. To allow the model to update is latent space via interactions we increased τ = 0.00001 back to τ = 0.0001.Algorithm 1: Interactive Concept Swapping Network -pair images forward passInput : Image pair x ∈ R D , x ∈ R D , known share IDs v. Output: Image reconstructionsx ∈ R D ,x ∈ R D , and latent codes y ∈ [0, 1] J·K , y ∈ [0, 1] J·K 1 // Forward pass through initial encoder. z ∈ R Z 2 z ← f (x) 3 z ← f (x ) 4 // Forward pass through J read-out encoders. φ ∈ R J×Q 5 φ ← collectedReadOutEncoders(z) 6 φ ← collectedReadOutEncoders(z ) 7 // Compute the distance of each concept encoding to all prototype slots of its corresponding category j. 8 y ← computeProtoDistance(φ, v) 9 y ← computeProtoDistance(φ , v) 10 // Reconstruct the images from the prototype distance codes. 11ŷ ← g(y) 12ŷ ← g(y )Algorithm 2: computeProtoDistance Input : Concept encodings φ ∈ R J×Q , φ ∈ R J×Q Given : Set of prototype slot codebooks Θ := [P 1 , ..., P J ] ∈ R J×Q×K , softmax temperature τ , and share IDs v. Output: Latent codes y ∈ [0, 1] J·K , y ∈ [0, 1] J·K 1 // For every superordinate concept category 2 for j ← 0 to J − 1 do 3 // Dot-product between concept encoding and all prototype slots from codebook P j . 4 s j ← softmaxDotProduct(φ j , P j ) 5 s j ← softmaxDotProduct(φ j , P j )6// Compute normalizing weighted softmax.7Π j ← softmaxNormTau(s j , τ ) 8 Π j ← softmaxNormTau(s j , τ ) 9 end for 10 // Swap the distance codes at the position corresponding to the shared IDs.
Towards robust interpretability with self-explaining neural networks. David Alvarez, - Melis, Tommi S Jaakkola, Conference on Neural Information Processing Systems (NeurIPS). David Alvarez-Melis and Tommi S. Jaakkola. Towards ro- bust interpretability with self-explaining neural networks. In Conference on Neural Information Processing Systems (NeurIPS), 2018. 2</p>
<p>The psychological nature of concepts. James Archer, Analyses of concept learning. ElsevierE James Archer. The psychological nature of concepts. In Analyses of concept learning, pages 37-49. Elsevier, 1966. 2</p>
<p>Weakly supervised multi-task learning for conceptbased explainability. Catarina Belém, Vladimir Balayan, Pedro Saleiro, Pedro Bizarro, arXiv:2104.12459arXiv preprintCatarina Belém, Vladimir Balayan, Pedro Saleiro, and Pedro Bizarro. Weakly supervised multi-task learning for concept- based explainability. arXiv preprint arXiv:2104.12459, 2021. 2</p>
<p>Representation learning: A review and new perspectives. Yoshua Bengio, Aaron C Courville, Pascal Vincent, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). Yoshua Bengio, Aaron C. Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intel- ligence (TPAMI), 2013. 2</p>
<p>Diane Bouchacourt, Ryota Tomioka, Sebastian Nowozin, Multi-level variational autoencoder: Learning disentangled representations from grouped observations. AAAI Conference on Artificial Intelligence. 24Diane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin. Multi-level variational autoencoder: Learning disentangled representations from grouped observations. AAAI Conference on Artificial Intelligence, 2018. 2, 4</p>
<p>Tracking prototype and exemplar representations in the brain across learning. eLife. Takako Caitlin R Bowman, Dagmar Iwashita, Zeithamova, Caitlin R Bowman, Takako Iwashita, and Dagmar Zei- thamova. Tracking prototype and exemplar representations in the brain across learning. eLife, 2009. 2</p>
<p>Monet: Unsupervised scene decomposition and representation. Christopher P Burgess, Loïc Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matthew Botvinick, Alexander Lerchner, Computing Research Repository (CoRR). 3Christopher P. Burgess, Loïc Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matthew Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decompo- sition and representation. Computing Research Repository (CoRR), 2019. 3</p>
<p>End-toend object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, European Conference on Computer Vision (ECCV). 2020Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to- end object detection with transformers. In European Confer- ence on Computer Vision (ECCV), 2020. 3</p>
<p>Unsupervised learning of visual features by contrasting cluster assignments. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin, 2020. 4Conference on Neural Information Processing Systems (NeurIPS). Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi- otr Bojanowski, and Armand Joulin. Unsupervised learn- ing of visual features by contrasting cluster assignments. In Conference on Neural Information Processing Systems (NeurIPS), 2020. 4</p>
<p>Unsupervised learning of visual features by contrasting cluster assignments. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin, Conference on Neural Information Processing Systems (NeurIPS). Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi- otr Bojanowski, and Armand Joulin. Unsupervised learn- ing of visual features by contrasting cluster assignments. In Conference on Neural Information Processing Systems (NeurIPS), 2020. 7</p>
<p>This looks like that: Deep learning for interpretable image recognition. Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, Jonathan K Su, Conference on Neural Information Processing Systems (NeurIPS). Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su. This looks like that: Deep learn- ing for interpretable image recognition. In Conference on Neural Information Processing Systems (NeurIPS), 2019. 2</p>
<p>A simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, 2020. 7International Conference on Machine Learning (ICML). Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learn- ing of visual representations. In International Conference on Machine Learning (ICML), 2020. 7</p>
<p>Infogan: Interpretable representation learning by information maximizing generative adversarial nets. Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel, Conference on Neural Information Processing Systems (NeurIPS). Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable rep- resentation learning by information maximizing generative adversarial nets. In Conference on Neural Information Pro- cessing Systems (NeurIPS), 2016. 2</p>
<p>Exploring simple siamese representation learning. Xinlei Chen, Kaiming He, 2021. 7IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Xinlei Chen and Kaiming He. Exploring simple siamese rep- resentation learning. In IEEE Conference on Computer Vi- sion and Pattern Recognition (CVPR), 2021. 7</p>
<p>An empirical study of training self-supervised vision transformers. Xinlei Chen, Saining Xie, Kaiming He, arXiv:2104.02057arXiv preprintXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057, 2021. 7</p>
<p>Logic explained networks. Gabriele Ciravegna, Pietro Barbiero, Francesco Giannini, Marco Gori, Pietro Lió, Marco Maggini, Stefano Melacci, arXiv:2108.05149arXiv preprintGabriele Ciravegna, Pietro Barbiero, Francesco Giannini, Marco Gori, Pietro Lió, Marco Maggini, and Stefano Melacci. Logic explained networks. arXiv preprint arXiv:2108.05149, 2021. 2</p>
<p>The role of prototypes in the mental representation of temporally related events. J Stanley, Robert S Colcombe, Wyer, Cognitive Psychology. 2Stanley J. Colcombe and Robert S. Wyer. The role of pro- totypes in the mental representation of temporally related events. Cognitive Psychology, pages 67-103, 2002. 2</p>
<p>Ai for radiographic covid-19 detection selects shortcuts over signal. Alex J Degrave, Joseph D Janizek, Su-In Lee, Nature Machine Intelligence. 18Alex J DeGrave, Joseph D Janizek, and Su-In Lee. Ai for radiographic covid-19 detection selects shortcuts over signal. Nature Machine Intelligence, 2021. 1, 8</p>
<p>Compositional visual generation with energy based models. Yilun Du, Shuang Li, Igor Mordatch, NeurIPS. 2020Yilun Du, Shuang Li, and Igor Mordatch. Compositional visual generation with energy based models. In NeurIPS, 2020. 3</p>
<p>Unsupervised learning of compositional energy concepts. Yilun Du, Shuang Li, Yash Sharma, Josh Tenenbaum, Igor Mordatch, NeurIPS. 3Yilun Du, Shuang Li, Yash Sharma, Josh Tenenbaum, and Igor Mordatch. Unsupervised learning of compositional en- ergy concepts. NeurIPS, 2021. 3</p>
<p>GENESIS: generative scene inference and sampling with object-centric latent representations. Martin Engelcke, Adam R Kosiorek, Oiwi Parker Jones, Ingmar Posner, ICLR, 2020. 3Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner. GENESIS: generative scene inference and sampling with object-centric latent representations. In ICLR, 2020. 3</p>
<p>. W Michael, Marc Eysenck, Brysbaert, Fundamentals of cognition. Routledge. 3Michael W Eysenck and Marc Brysbaert. Fundamentals of cognition. Routledge, 2018. 3</p>
<p>Concepts: Where cognitive science went wrong. A Jerry, Fodor, Oxford University PressJerry A Fodor. Concepts: Where cognitive science went wrong. Oxford University Press, 1998. 2</p>
<p>Prototypes vs exemplars in concept representation. Marcello Frixione , Antonio Lieto, International Conference on Knowledge Engineering and Ontology Development (KEOD). Marcello Frixione. and Antonio Lieto. Prototypes vs ex- emplars in concept representation. In International Confer- ence on Knowledge Engineering and Ontology Development (KEOD), 2012. 2</p>
<p>Towards automatic concept-based explanations. Amirata Ghorbani, James Wexler, Y James, Been Zou, Kim, Conference on Neural Information Processing Systems (NeurIPS). Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based explanations. In Conference on Neural Information Processing Systems (NeurIPS), 2019. 2</p>
<p>Fast R-CNN. Ross B Girshick, International Conference on Computer Vision (ICCV). Ross B. Girshick. Fast R-CNN. In International Conference on Computer Vision (ICCV), 2015. 3</p>
<p>Generative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Conference on Neural Information Processing Systems (NeurIPS). Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Conference on Neural Information Processing Systems (NeurIPS), 2014. 1</p>
<p>Multi-object representation learning with iterative variational inference. Klaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, Alexander Lerchner, International Conference on Machine Learning (ICML). Klaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning with iterative variational inference. In International Conference on Machine Learning (ICML), 2019. 3</p>
<p>Mask R-CNN. Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross B Girshick, 2017. 3International Conference on Computer Vision (ICCV. Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. Mask R-CNN. In International Conference on Computer Vision (ICCV), 2017. 3</p>
<p>Prototype learning and dissociable categorization systems in alzheimer's disease. William Heindel, Elena Festa, Brian Ott, Kelly Landy, David Salmon, Neuropsychologia. 512William Heindel, Elena Festa, Brian Ott, Kelly Landy, and David Salmon. Prototype learning and dissociable catego- rization systems in alzheimer's disease. Neuropsychologia, 51, 2013. 2</p>
<p>Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. Irina Higgins, Loïc Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, International Conference on Learning Representations (ICLR. 6Irina Higgins, Loïc Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual con- cepts with a constrained variational framework. In Inter- national Conference on Learning Representations (ICLR), 2017. 1, 6</p>
<p>Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 7</p>
<p>Haruo Hosoya, arXiv:1809.02383Group-based learning of disentangled representations with generalizability for novel contents. arXiv preprintHaruo Hosoya. Group-based learning of disentangled rep- resentations with generalizability for novel contents. arXiv preprint arXiv:1809.02383, 2018. 2</p>
<p>Group-based learning of disentangled representations with generalizability for novel contents. Haruo Hosoya, Joint Conference on Artificial Intelligence (IJCAI). Haruo Hosoya. Group-based learning of disentangled repre- sentations with generalizability for novel contents. In Joint Conference on Artificial Intelligence (IJCAI), 2019. 2</p>
<p>Group-based learning of disentangled representations with generalizability for novel contents. Haruo Hosoya, Joint Conference on Artificial Intelligence (IJ-CAI). Sarit Kraus46Haruo Hosoya. Group-based learning of disentangled repre- sentations with generalizability for novel contents. In Sarit Kraus, editor, Joint Conference on Artificial Intelligence (IJ- CAI), 2019. 4, 6</p>
<p>Interpretable latent spaces for learning from demonstration. Yordan Hristov, Alex Lascarides, Subramanian Ramamoorthy, Conference on Robot Learning (CoRL). Yordan Hristov, Alex Lascarides, and Subramanian Ra- mamoorthy. Interpretable latent spaces for learning from demonstration. In Conference on Robot Learning (CoRL), 2018. 2</p>
<p>Behavior regularized prototypical networks for semi-supervised few-shot image classification. Shixin Huang, Xiangping Zeng, Si Wu, Zhiwen Yu, Mohamed Azzam, Hau-San Wong, Pattern Recognition. 2Shixin Huang, Xiangping Zeng, Si Wu, Zhiwen Yu, Mo- hamed Azzam, and Hau-San Wong. Behavior regularized prototypical networks for semi-supervised few-shot image classification. Pattern Recognition, 2021. 2</p>
<p>Prototype constructions in early language acquisition. Paul Ibbotson, Michael Tomasello, Camebridge University PressPaul Ibbotson and Michael Tomasello. Prototype construc- tions in early language acquisition. Camebridge University Press, pages 59-85, 2009. 2</p>
<p>Generalization and similarity in exemplar models of categorization: Insights from machine learning. Frank Jäkel, Bernhard Schölkopf, Felix A Wichmann, Psychonomic Bulletin &amp; Review. 2Frank Jäkel, Bernhard Schölkopf, and Felix A. Wichmann. Generalization and similarity in exemplar models of catego- rization: Insights from machine learning. Psychonomic Bul- letin &amp; Review, 2008. 2</p>
<p>Categorical reparameterization with gumbel-softmax. Eric Jang, Shixiang Gu, Ben Poole, International Conference on Learning Representations (ICLR. Eric Jang, Shixiang Gu, and Ben Poole. Categorical repa- rameterization with gumbel-softmax. In International Con- ference on Learning Representations (ICLR), 2017. 6</p>
<p>Ib-gan: Disengangled representation learning with information bottleneck generative adversarial networks. Wonkwang Insu Jeon, Myeongjang Lee, Gunhee Pyeon, Kim, AAAI Conference on Artificial Intelligence. 2021Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, and Gun- hee Kim. Ib-gan: Disengangled representation learning with information bottleneck generative adversarial networks. In AAAI Conference on Artificial Intelligence, 2021. 2</p>
<p>Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Proceedings of the 35th International Conference on Machine Learning, Proceedings of Machine Learning Research (PMLR). the 35th International Conference on Machine Learning, Machine Learning Research (PMLR)Fernanda Viegas, and Rory sayresBeen Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, and Rory sayres. Inter- pretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In Proceedings of the 35th International Conference on Machine Learning, Proceedings of Machine Learning Research (PMLR), 2018. 2</p>
<p>Disentangling by factorising. Hyunjik Kim, Andriy Mnih, International Conference on Machine Learning (ICML). Hyunjik Kim and Andriy Mnih. Disentangling by fac- torising. In International Conference on Machine Learning (ICML), 2018. 5</p>
<p>Concept bottleneck models. Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, Percy Liang, Proceedings of Machine Learning Research. Machine Learning ResearchPMLRPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In Proceedings of Machine Learning Research (PMLR), 2020. 1, 2</p>
<p>Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions. Oscar Li, Hao Liu, Chaofan Chen, Cynthia Rudin, AAAI Conference on Artificial Intelligence. 24Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions. In AAAI Con- ference on Artificial Intelligence, 2018. 2, 4</p>
<p>Scgan: Disentangled representation learning by adding similarity constraint on generative adversarial nets. Xiaoqiang Li, Liangbo Chen, Lu Wang, Pin Wu, Weiqin Tong, IEEE Access. 2Xiaoqiang Li, Liangbo Chen, Lu Wang, Pin Wu, and Weiqin Tong. Scgan: Disentangled representation learning by adding similarity constraint on generative adversarial nets. IEEE Access, 2019. 2</p>
<p>Learning without forgetting. Zhizhong Li, Derek Hoiem, European Conference on Computer Vision (ECCV). Bastian Leibe, Jiri Matas, Nicu Sebe, and Max WellingZhizhong Li and Derek Hoiem. Learning without forgetting. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, European Conference on Computer Vision (ECCV), 2016. 7</p>
<p>SPACE: unsupervised object-oriented scene representation via spatial attention and decomposition. Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, Sungjin Ahn, International Conference on Learning Representations (ICLR). 2020Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, and Sungjin Ahn. SPACE: unsupervised object-oriented scene represen- tation via spatial attention and decomposition. In Inter- national Conference on Learning Representations (ICLR), 2020. 3</p>
<p>Challenging common assumptions in the unsupervised learning of disentangled representations. Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf, Olivier Bachem, International Conference on Machine Learning (ICML). Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf, and Olivier Bachem. Challenging common assumptions in the unsuper- vised learning of disentangled representations. In Interna- tional Conference on Machine Learning (ICML), 2019. 2</p>
<p>Challenging common assumptions in the unsupervised learning of disentangled representations. Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, Olivier Bachem, International Conference on Machine Learning (ICML). Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, and Olivier Bachem. Challenging common assumptions in the unsuper- vised learning of disentangled representations. In Interna- tional Conference on Machine Learning (ICML), 2019. 4</p>
<p>Weakly-supervised disentanglement without compromises. Francesco Locatello, Ben Poole, Gunnar Rätsch, Bernhard Schölkopf, Olivier Bachem, Michael Tschannen, International Conference on Machine Learning (ICML). 613Francesco Locatello, Ben Poole, Gunnar Rätsch, Bern- hard Schölkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. In International Conference on Machine Learning (ICML), 2020. 2, 4, 6, 13</p>
<p>Objectcentric learning with slot attention. Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, Thomas Kipf, Conference on Neural Information Processing Systems (NeurIPS). 35Francesco Locatello, Dirk Weissenborn, Thomas Un- terthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object- centric learning with slot attention. In Conference on Neural Information Processing Systems (NeurIPS), 2020. 3, 5</p>
<p>The concrete distribution: A continuous relaxation of discrete random variables. Chris J Maddison, Andriy Mnih, Yee Whye Teh, International Conference on Learning Representations (ICLR. Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. In International Conference on Learning Representations (ICLR), 2017. 6</p>
<p>Contextual semantic interpretability. Diego Marcos, Ruth Fong, Sylvain Lobry, Rémi Flamary, Nicolas Courty, Devis Tuia, Asia Conference on Computer Vision (ACCV). Hiroshi Ishikawa, Cheng-Lin Liu, Tomas Pajdla, and Jianbo Shi2021Diego Marcos, Ruth Fong, Sylvain Lobry, Rémi Flamary, Nicolas Courty, and Devis Tuia. Contextual semantic inter- pretability. In Hiroshi Ishikawa, Cheng-Lin Liu, Tomas Pa- jdla, and Jianbo Shi, editors, Asia Conference on Computer Vision (ACCV), 2021. 2</p>
<p>Catastrophic interference in connectionist networks: The sequential learning problem. Michael Mccloskey, Neal J Cohen, Psychology of learning and motivation. Academic Press24Michael McCloskey and Neal J. Cohen. Catastrophic inter- ference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, vol- ume 24, pages 109-165. Academic Press, 1989. 7</p>
<p>Adversarial continual learning for multi-domain hippocampal segmentation. Marius Memmel, Camila Gonzalez, Anirban Mukhopadhyay, Domain Adaptation and Representation Transfer (DART) at International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI). 2021Marius Memmel, Camila Gonzalez, and Anirban Mukhopadhyay. Adversarial continual learning for multi-domain hippocampal segmentation. In Domain Adap- tation and Representation Transfer (DART) at International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2021. 2</p>
<p>Prototype guided federated learning of visual feature representations. Umberto Michieli, Mete Ozay, arXiv:2105.08982arXiv preprintUmberto Michieli and Mete Ozay. Prototype guided feder- ated learning of visual feature representations. arXiv preprint arXiv:2105.08982, 2021. 4</p>
<p>An identifiable double vae for disentangled representations. Graziano Mita, Maurizio Filippone, Pietro Michiardi, International Conference on Machine Learning (ICML). 2021Graziano Mita, Maurizio Filippone, and Pietro Michiardi. An identifiable double vae for disentangled representations. In International Conference on Machine Learning (ICML), 2021. 2</p>
<p>Blockgan: Learning 3d object-aware scene representations from unlabelled images. Christian Thu H Nguyen-Phuoc, Long Richardt, Yongliang Mai, Niloy Yang, Mitra, Conference on Neural Information Processing Systems (NeurIPS). 2020Thu H Nguyen-Phuoc, Christian Richardt, Long Mai, Yongliang Yang, and Niloy Mitra. Blockgan: Learning 3d object-aware scene representations from unlabelled images. In Conference on Neural Information Processing Systems (NeurIPS), 2020. 2</p>
<p>Giraffe: Representing scenes as compositional generative neural feature fields. Michael Niemeyer, Andreas Geiger, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2021Michael Niemeyer and Andreas Geiger. Giraffe: Represent- ing scenes as compositional generative neural feature fields. In IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), 2021. 2</p>
<p>Chapter six -categorization as causal explanation: Discounting and augmenting in a bayesian framework. M Daniel, Joshua B Oppenheimer, Tevye R Tenenbaum, Krynski, Categorization as causal reasoning. Academic PressDaniel M. Oppenheimer, Joshua B. Tenenbaum, and Tevye R. Krynski. Chapter six -categorization as causal ex- planation: Discounting and augmenting in a bayesian frame- work. In Categorization as causal reasoning, Psychology of Learning and Motivation, pages 203-231. Academic Press, 2013. 2</p>
<p>Multimodal prototypical networks for few-shot learning. Frederik Pahde, Mihai Puscas, Tassilo Klein, Moin Nabi, IEEE Winter Conference on Applications of Computer Vision (WACV). 2021Frederik Pahde, Mihai Puscas, Tassilo Klein, and Moin Nabi. Multimodal prototypical networks for few-shot learning. In IEEE Winter Conference on Applications of Computer Vision (WACV), 2021. 2</p>
<p>Selfsupervised relational reasoning for representation learning. Massimiliano Patacchiola, Amos Storkey, arXiv:2006.05849arXiv preprintMassimiliano Patacchiola and Amos Storkey. Self- supervised relational reasoning for representation learning. arXiv preprint arXiv:2006.05849, 2020. 7</p>
<p>Disentangling 3d prototypical networks for few-shot concept learning. Mihir Prabhudesai, Shamit Lal, Darshan Patil, Hsiao-Yu Tung, Adam W Harley, Katerina Fragkiadaki, International Conference on Learning Representations (ICLR), 2021. 24Mihir Prabhudesai, Shamit Lal, Darshan Patil, Hsiao-Yu Tung, Adam W Harley, and Katerina Fragkiadaki. Disentan- gling 3d prototypical networks for few-shot concept learn- ing. In International Conference on Learning Representa- tions (ICLR), 2021. 2, 4</p>
<p>Connectionist models of recognition memory: constraints imposed by learning and forgetting functions. Roger Ratcliff, Psychological review. 972285Roger Ratcliff. Connectionist models of recognition mem- ory: constraints imposed by learning and forgetting func- tions. Psychological review, 97(2):285, 1990. 7</p>
<p>You only look once: Unified, real-time object detection. Joseph Redmon, Santosh Kumar Divvala, Ross B Girshick, Ali Farhadi, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. You only look once: Unified, real-time ob- ject detection. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 3</p>
<p>Catastrophic forgetting in neural networks: the role of rehearsal mechanisms. A Robins, Proceedings 1993 The First New Zealand International Two-Stream Conference on Artificial Neural Networks and Expert Systems. 1993 The First New Zealand International Two-Stream Conference on Artificial Neural Networks and Expert SystemsA. Robins. Catastrophic forgetting in neural networks: the role of rehearsal mechanisms. In Proceedings 1993 The First New Zealand International Two-Stream Conference on Ar- tificial Neural Networks and Expert Systems, pages 65-68, 1993. 7</p>
<p>Evaluating the interpretability of generative models by interactive reconstruction. Andrew Slavin Ross, Nina Chen, Elisa Zhao Hang, Elena L Glassman, Finale Doshi-Velez, Conference on Human Factors in Computing System (CHI). 2021Andrew Slavin Ross, Nina Chen, Elisa Zhao Hang, Elena L. Glassman, and Finale Doshi-Velez. Evaluating the inter- pretability of generative models by interactive reconstruc- tion. In Conference on Human Factors in Computing System (CHI), 2021. 2</p>
<p>Right for the right reasons: Training differentiable models by constraining their explanations. Andrew Slavin Ross, Michael C Hughes, Finale Doshi-Velez, 2017. 3Joint Conference on Artificial Intelligence (IJCAI. Andrew Slavin Ross, Michael C. Hughes, and Finale Doshi- Velez. Right for the right reasons: Training differentiable models by constraining their explanations. In Joint Confer- ence on Artificial Intelligence (IJCAI), 2017. 3</p>
<p>Making deep neural networks right for the right scientific reasons by interacting with their explanations. Patrick Schramowski, Wolfgang Stammer, Stefano Teso, Anna Brugger, Franziska Herbert, Xiaoting Shao, Hans-Georg Luigs, Anne-Katrin Mahlein, Kristian Kersting, Nature Machine Intelligence. Patrick Schramowski, Wolfgang Stammer, Stefano Teso, Anna Brugger, Franziska Herbert, Xiaoting Shao, Hans- Georg Luigs, Anne-Katrin Mahlein, and Kristian Kersting. Making deep neural networks right for the right scientific reasons by interacting with their explanations. Nature Ma- chine Intelligence, 2020. 1, 3, 8</p>
<p>Taking a HINT: leveraging explanations to make vision and language models more grounded. Stefan Ramprasaath Ramasamy Selvaraju, Yilin Lee, Hongxia Shen, Shalini Jin, Larry P Ghosh, Dhruv Heck, Devi Batra, Parikh, International Conference on Computer Vision (ICCV). Ramprasaath Ramasamy Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry P. Heck, Dhruv Batra, and Devi Parikh. Taking a HINT: leveraging explanations to make vision and language models more grounded. In In- ternational Conference on Computer Vision (ICCV), 2019. 3</p>
<p>Koray Kavukcuoglu, and Demis Hassabis. Improved protein structure prediction using potentials from deep learning. Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Zídek, Alexander W R Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T Jones, David Silver, Nat. 5777792Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Au- gustin Zídek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu, and Demis Hassabis. Improved pro- tein structure prediction using potentials from deep learning. Nat., 577(7792):706-710, 2020. 1</p>
<p>Weakly supervised disentanglement with guarantees. Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, Ben Poole, International Conference on Learning Representations (ICLR). 56Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. Weakly supervised disentanglement with guarantees. In International Conference on Learning Rep- resentations (ICLR), 2020. 2, 4, 5, 6</p>
<p>Prototypes, exemplars, and the natural history of categorization. Psychonomic bulletin &amp; review. J , David Smith, J. David Smith. Prototypes, exemplars, and the natural his- tory of categorization. Psychonomic bulletin &amp; review, 2014. 2</p>
<p>Prototypical networks for few-shot learning. Jake Snell, Kevin Swersky, Richard Zemel, Conference on Neural Information Processing Systems (NeurIPS). Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Conference on Neural Information Processing Systems (NeurIPS), 2017. 2</p>
<p>Right for the right concept: Revising neuro-symbolic concepts by interacting with their explanations. Wolfgang Stammer, Patrick Schramowski, Kristian Kersting, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Wolfgang Stammer, Patrick Schramowski, and Kristian Ker- sting. Right for the right concept: Revising neuro-symbolic concepts by interacting with their explanations. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2, 3, 4, 5</p>
<p>Linguistic Categorization. J R Taylor, OUP Oxford. 2J.R. Taylor. Linguistic Categorization. OUP Oxford, 2003. 2</p>
<p>Explanatory interactive machine learning. Stefano Teso, Kristian Kersting, AAAI/ACM Conference on AI, Ethics, and Society (AIES). Stefano Teso and Kristian Kersting. Explanatory interactive machine learning. In AAAI/ACM Conference on AI, Ethics, and Society (AIES), 2019. 3</p>
<p>Recent advances in autoencoder-based representation learning. Michael Tschannen, Olivier Bachem, Mario Lucic, arXiv:1812.05069arXiv preprintMichael Tschannen, Olivier Bachem, and Mario Lucic. Re- cent advances in autoencoder-based representation learning. arXiv preprint arXiv:1812.05069, 2018. 1</p>
<p>Gated variational autoencoders: Incorporating weak supervision to encourage disentanglement. Matthew J Vowels, Richard Necati Cihan Camgoz, Bowden, IEEE International Conference on Automatic Face and Gesture Recognition (FG). 2020Matthew J. Vowels, Necati Cihan Camgoz, and Richard Bowden. Gated variational autoencoders: Incorporating weak supervision to encourage disentanglement. In IEEE International Conference on Automatic Face and Gesture Recognition (FG), 2020. 2</p>
<p>Group normalization. Yuxin Wu, Kaiming He, European Conference on Computer Vision (ECCV). Yuxin Wu and Kaiming He. Group normalization. In Euro- pean Conference on Computer Vision (ECCV), 2018. 13</p>
<p>Causalvae: Disentangled representation learning via neural structural causal models. Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, Jun Wang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2021Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae: Disentangled rep- resentation learning via neural structural causal models. In IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), 2021. 2</p>
<p>On completenessaware concept-based explanations in deep neural networks. Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pfister, Pradeep Ravikumar, Conference on Neural Information Processing Systems (NeurIPS). 2020Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pfister, and Pradeep Ravikumar. On completeness- aware concept-based explanations in deep neural networks. In Conference on Neural Information Processing Systems (NeurIPS), 2020. 2</p>
<p>Neural-symbolic VQA: disentangling reasoning from vision and language understanding. Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Conference on Neural Information Processing Systems (NeurIPS). 25Pushmeet Kohli, and Josh TenenbaumKexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Push- meet Kohli, and Josh Tenenbaum. Neural-symbolic VQA: disentangling reasoning from vision and language under- standing. In Conference on Neural Information Processing Systems (NeurIPS), 2018. 1, 2, 5</p>
<p>Julian Zaidi, Jonathan Boilard, Ghyslain Gagnon, Marc-André Carbonneau, arXiv:2012.09276Measuring disentanglement: A review of metrics. arXiv preprintJulian Zaidi, Jonathan Boilard, Ghyslain Gagnon, and Marc- André Carbonneau. Measuring disentanglement: A review of metrics. arXiv preprint arXiv:2012.09276, 2020. 2</p>
<p>Prototype Learning Systems. Dagmar Zeithamova, Springer USDagmar Zeithamova. Prototype Learning Systems, pages 2715-2718. Springer US, 2012. 2</p>
<p>Where and what? examining interpretable disentangled representations. Xinqi Zhu, Chang Xu, Dacheng Tao, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2021Xinqi Zhu, Chang Xu, and Dacheng Tao. Where and what? examining interpretable disentangled representations. In IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), 2021. 2</p>            </div>
        </div>

    </div>
</body>
</html>
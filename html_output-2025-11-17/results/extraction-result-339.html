<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-339 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-339</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-339</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-253180684</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2302.00763v1.pdf" target="_blank">Collaborating with language models for embodied reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e339.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e339.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Planner-Actor-Reporter architecture</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-part system where a pre-trained language model (Planner) issues natural-language commands, a learned embodied policy (Actor) executes actions from pixel observations, and a Reporter translates Actor observations/actions back to text for the Planner, enabling the LM to plan without direct sensory access.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chinchilla (as Planner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained Chinchilla LSLMs used as language-only Planners; they receive only language prompts (task spec + in-context few-shot examples + Reporter text) and produce natural-language instructions. No direct visual or proprioceptive input is given to the Planner.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Secret-property conditional & search; Visual conditional / Visual location conditional</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grid-world (11x11) embodied tasks where the language-only Planner must instruct an Actor to examine objects and pick up targets. 'Secret-property conditional' requires examining a designated 'decider' object to learn its hidden property and then pick the correct target; 'search' requires sequentially examining objects until finding the one with 'good' property; 'visual conditional' requires reporting a visual/spatial property (e.g., whether a decider object is close to a wall) decoded from pixels by the Reporter.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning / information-gathering / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational + spatial</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large text corpora (implicit in LM weights) + in-context few-shot examples supplied in the prompt; perception is supplied by Reporter (which may be hard-coded, pretrained, or RL-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot in-context prompting (5 examples); planner queried at each timestep using dialogue transcript (Planner output + Reporter text)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit in LM weights (semantic and procedural patterns) and explicit natural-language plans/instructions in the prompt/dialogue; sensor information is represented as natural-language reports produced by the Reporter.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate / task completion (episode success)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Secret-property conditional (zero-shot with 5-shot prompt): Chinchilla 70B ≈ 96% success; Chinchilla 7B ≈ 58% success. Secret-property search: qualitative high zero-shot success for 70B (higher than conditional); exact percentages for search not reported in text. RL baselines: search ≈ 25% after ~140M frames; conditional ≈ 33% after ~140M frames.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Planner successfully breaks multi-step instructions into examine-then-pickup sequences, issues explicit information-gathering instructions, integrates Reporter replies to decide the next action, and can recover from some Actor/Reporter errors (e.g., repeating or cycling examine instructions until relevant reports are received). Large models use repetition or cycling strategies to cope with noisy reports.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Smaller LMs (7B) often fail to infer the correct next instruction from reported information; failures also arise from Actor distribution shift (Actor unable to follow Planner instructions due to differences in instruction formatting or episode length) and from noisy/irrelevant Reporter outputs which degrade Planner performance (more so for smaller models).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Pure RL Actor baseline: search ~25% (after 140M frames), conditional ~33% (after 140M frames); Planner-Actor-Reporter (70B) greatly outperforms these zero-shot; 7B Planner performs worse than 70B but better than or comparable to RL baseline depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Model size ablation: 70B >> 7B on conditional reasoning (96% vs 58%). Reporter ablation: truthful/hard-coded Reporter yields good performance; adding 20% irrelevant reports reduces performance (7B affected more than 70B). Prompt ablation: providing few-shot examples that demonstrate a 'repetition' strategy restores 70B performance under irrelevant-report noise; 7B improves but not fully. Reporter learning: RL-trained Reporter (from scratch) can learn to report truthful, task-relevant visual information and improve overall performance (converges in few hundred frames of Reporter training).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large LMs can encode procedural and object-relational reasoning needed for embodied multi-step tasks in their weights and can act as effective language-only Planners when supplied with in-context examples and truthful reports; spatial knowledge (e.g., 'close to wall') can be incorporated only via a Reporter because the LM lacks direct sensory inputs. Larger model size substantially improves the Planner's ability to use reported observations for correct next-step decisions and to be robust to noisy reports; training a Reporter via RL is a viable approach to ground visual/spatial properties for the Planner.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e339.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e339.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chinchilla-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chinchilla (70 billion parameter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pre-trained language model (Chinchilla) used as the Planner; demonstrated strong zero-shot multi-step planning when provided with few-shot examples and truthful Reporter input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training compute-optimal large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chinchilla</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained LSLM (Chinchilla) serving as a language-only Planner that generates natural-language instructions at each timestep based on the prompt (task description + few-shot examples + Reporter messages). It has no access to pixels; all perceptual grounding is supplied via Reporter text.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Secret-property conditional & secret-property search (grid-world)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See PAR entry. The Planner must command an Actor to examine objects and then pick the correct object based on reported hidden properties; in the visual conditional variant it must rely on a Reporter to convert pixels into descriptive text (e.g., 'close to wall').</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning / information-gathering / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (+ spatial indirectly via Reporter)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>implicit knowledge from pre-training on large text corpora; in-context few-shot examples in prompt (5 examples) provide task-specific procedural template</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot prompting (in-context learning), iterative dialogue with Reporter (language observations)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Procedural and object-relational knowledge is encoded implicitly in network weights and explicitly manifested in generated natural-language instructions; sensory/spatial info is represented as natural-language reports supplied by Reporter.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate / task completion</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Secret-property conditional (zero-shot with 5-shot prompt): ~96% success. Secret-property search: reported as 'high' zero-shot success and more robust than conditional in the paper, exact percentage not provided. Robust to ~20% irrelevant Reporter messages when provided with few-shot demonstration of repeating strategy (performance restored to near-clean levels).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Effectively plans sequences of examines and pick-ups, integrates reported secret properties to choose targets, demonstrates emergent strategies (repeating/cycling) to handle noisy reports, and recovers from some Actor mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Performance degrades with noisy or irrelevant Reporter outputs if no strategy examples are provided in prompt; relies entirely on textual reports for spatial reasoning and thus fails if Reporter is untruthful or uninformative.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms pure RL baselines (search ~25%, conditional ~33% after extensive RL training) in zero-shot settings; outperforms smaller Chinchilla 7B.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Providing strategy examples in prompt (ablation on prompt content) restores robustness under noisy reports. No direct ablation of internal representations reported beyond model-size comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A large LM (70B) can internalize procedural and object-relational patterns sufficiently to perform multi-step embodied tasks via language-only interfaces, but requires reliable reporting for spatial/perceptual grounding; few-shot examples can teach high-level error-recovery strategies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e339.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e339.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chinchilla-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chinchilla (7 billion parameter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller Chinchilla LSLM used as the Planner; shows limited ability to perform conditional multi-step planning from language prompts compared to the 70B variant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training compute-optimal large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chinchilla</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained LSLM variant with fewer parameters serving as the Planner; uses same few-shot prompting and dialogue interface but encodes less robust procedural/object-relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Secret-property conditional & secret-property search (grid-world)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as other Planner entries: must instruct an Actor to examine and pick objects based on reported hidden properties; relies on Reporter to provide perceptual/spatial info.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning / information-gathering / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (+ spatial via Reporter)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on text corpora (implicit) + in-context few-shot examples</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot in-context prompting</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit procedural and relational knowledge in weights; uses natural-language prompt templates to structure action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate / task completion</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Secret-property conditional (zero-shot with 5-shot prompt): ≈58% success (given full information). Secret-property search: performs worse than 70B and more susceptible to Reporter noise; exact search percentages not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Can sometimes break down tasks into steps and follow simple in-context exemplars; benefits from few-shot templates when tasks are simple.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>More frequent failures to infer correct next instruction from Reporter feedback, less robust to irrelevant/noisy reports, and does not fully recover when prompt provides strategy examples (unlike 70B).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Underperforms 70B Planner; in many cases comparable or worse than extensive pure RL baseline performance depending on task difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Model-size ablation (7B vs 70B) shows notable drop in ability to use reported information to make correct procedural decisions. Prompt augmentation with strategy examples improves but does not fully restore performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Smaller LMs have weaker implicit procedural and object-relational representations and are less able to utilize reported sensory information for correct embodied planning; model scale substantially affects the Planner's ability to integrate reports into correct next-action reasoning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e339.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e339.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reporter (hard-coded & RL-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reporter module (hard-coded reporter and RL-trained reporter variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Module that translates Actor pixel observations and actions into natural-language (or categorical) reports for the language-only Planner; implemented both as a hard-coded truth-telling reporter and as a learned visual classifier trained with RL to produce useful reports.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reporter module (visual encoder + memory + policy head)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reporter shares architecture motifs with the Actor: a convolutional visual encoder, instruction encoder, and memory module; its output is either a hard-coded text report (truthful reporter) or a learned classification/head that outputs one of a small set of textual reports (e.g., 'object is close to wall' / 'far from wall'). Reporter variants include: (a) hand-coded truthful Reporter that returns exact hidden properties, and (b) an RL-trained Reporter that learns to map pixels to one of binary textual reports using episode reward as supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Visual conditional / Visual location conditional; used across all examine-and-report tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reporter receives the same egocentric top-down RGB crop as the Actor (5-square visibility) and produces textual reports about examined objects (hidden properties or spatial relations). In RL-trained variant, Reporter learns which visual features to communicate to Planner to improve episode reward.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>perception-to-language translation / grounding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (perceptual grounding of object properties and locations)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>learned from pixel observations with RL (or hard-coded mapping from environment events to text in the scripted Reporter); may be bootstrapped by pretrained perception models in future work</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>hard-coded reporting (truthful) OR reinforcement learning to maximize final task reward (Reporter policy head is trained with V-trace)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Perceptual knowledge is encoded in visual encoder weights and represented in discrete textual reports (binary classifier outputs) that summarize spatial/object-relational properties for the Planner to consume; Reporter does not produce dense spatial maps in current implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>episode success rate when paired with Planner; reporter training convergence (frames)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reporter training converged rapidly (reporter training converged after ~500 frames in experiments). RL-trained Reporter improved overall Planner-Actor task performance (Fig 2C), though exact absolute percentages for improvement are not enumerated in-text. Hard-coded truthful Reporter yields high Planner success in conditional/search tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>A learned Reporter can discover to report the task-relevant perceptual feature (e.g., 'close to wall') using sparse final-reward signal; hard-coded truthful reporters reliably ground the Planner's language-only reasoning in perception.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Noisy or irrelevant reporting (reporting movement logs 20% of the time) reduces Planner performance, especially for smaller Planners; untrained Reporter initially reports ungrounded/unhelpful information and must learn via RL to provide truthful, task-relevant summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Hard-coded truthful Reporter vs RL-trained Reporter: hard-coded provides immediate high performance; RL-trained converges to useful reports in the experiments reported. Pretrained vision-language models (cited prior work) are an alternative but were not used here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Reporter noise ablation: inserting irrelevant reports 20% of the time reduces success; however, providing strategy examples in the Planner prompt mitigates the impact for large Planners. Reporter training ablation: learned Reporter initialized from scratch can converge in a small number of frames but wall-clock training is expensive due to LM inference time in the loop.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Effective grounding of spatial and object-relational information for a language-only Planner can be provided by a Reporter; such a Reporter can be trained via RL from sparse reward to produce truthful, task-relevant textual summaries, enabling the LM to plan without direct sensory access. Reporter quality and relevance are critical — noisy reporting disproportionately harms smaller Planners.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do as i can, not as i say: Grounding language in robotic affordances. <em>(Rating: 2)</em></li>
                <li>Socratic models: Composing zero-shot multimodal reasoning with language. <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models. <em>(Rating: 2)</em></li>
                <li>Training compute-optimal large language models. <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-339",
    "paper_id": "paper-253180684",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "PAR",
            "name_full": "Planner-Actor-Reporter architecture",
            "brief_description": "A three-part system where a pre-trained language model (Planner) issues natural-language commands, a learned embodied policy (Actor) executes actions from pixel observations, and a Reporter translates Actor observations/actions back to text for the Planner, enabling the LM to plan without direct sensory access.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Chinchilla (as Planner)",
            "model_size": "7B / 70B",
            "model_description": "Pre-trained Chinchilla LSLMs used as language-only Planners; they receive only language prompts (task spec + in-context few-shot examples + Reporter text) and produce natural-language instructions. No direct visual or proprioceptive input is given to the Planner.",
            "task_name": "Secret-property conditional & search; Visual conditional / Visual location conditional",
            "task_description": "Grid-world (11x11) embodied tasks where the language-only Planner must instruct an Actor to examine objects and pick up targets. 'Secret-property conditional' requires examining a designated 'decider' object to learn its hidden property and then pick the correct target; 'search' requires sequentially examining objects until finding the one with 'good' property; 'visual conditional' requires reporting a visual/spatial property (e.g., whether a decider object is close to a wall) decoded from pixels by the Reporter.",
            "task_type": "multi-step planning / information-gathering / instruction following",
            "knowledge_type": "procedural + object-relational + spatial",
            "knowledge_source": "pre-training on large text corpora (implicit in LM weights) + in-context few-shot examples supplied in the prompt; perception is supplied by Reporter (which may be hard-coded, pretrained, or RL-trained)",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot in-context prompting (5 examples); planner queried at each timestep using dialogue transcript (Planner output + Reporter text)",
            "knowledge_representation": "Implicit in LM weights (semantic and procedural patterns) and explicit natural-language plans/instructions in the prompt/dialogue; sensor information is represented as natural-language reports produced by the Reporter.",
            "performance_metric": "success rate / task completion (episode success)",
            "performance_result": "Secret-property conditional (zero-shot with 5-shot prompt): Chinchilla 70B ≈ 96% success; Chinchilla 7B ≈ 58% success. Secret-property search: qualitative high zero-shot success for 70B (higher than conditional); exact percentages for search not reported in text. RL baselines: search ≈ 25% after ~140M frames; conditional ≈ 33% after ~140M frames.",
            "success_patterns": "Planner successfully breaks multi-step instructions into examine-then-pickup sequences, issues explicit information-gathering instructions, integrates Reporter replies to decide the next action, and can recover from some Actor/Reporter errors (e.g., repeating or cycling examine instructions until relevant reports are received). Large models use repetition or cycling strategies to cope with noisy reports.",
            "failure_patterns": "Smaller LMs (7B) often fail to infer the correct next instruction from reported information; failures also arise from Actor distribution shift (Actor unable to follow Planner instructions due to differences in instruction formatting or episode length) and from noisy/irrelevant Reporter outputs which degrade Planner performance (more so for smaller models).",
            "baseline_comparison": "Pure RL Actor baseline: search ~25% (after 140M frames), conditional ~33% (after 140M frames); Planner-Actor-Reporter (70B) greatly outperforms these zero-shot; 7B Planner performs worse than 70B but better than or comparable to RL baseline depending on task.",
            "ablation_results": "Model size ablation: 70B &gt;&gt; 7B on conditional reasoning (96% vs 58%). Reporter ablation: truthful/hard-coded Reporter yields good performance; adding 20% irrelevant reports reduces performance (7B affected more than 70B). Prompt ablation: providing few-shot examples that demonstrate a 'repetition' strategy restores 70B performance under irrelevant-report noise; 7B improves but not fully. Reporter learning: RL-trained Reporter (from scratch) can learn to report truthful, task-relevant visual information and improve overall performance (converges in few hundred frames of Reporter training).",
            "key_findings": "Large LMs can encode procedural and object-relational reasoning needed for embodied multi-step tasks in their weights and can act as effective language-only Planners when supplied with in-context examples and truthful reports; spatial knowledge (e.g., 'close to wall') can be incorporated only via a Reporter because the LM lacks direct sensory inputs. Larger model size substantially improves the Planner's ability to use reported observations for correct next-step decisions and to be robust to noisy reports; training a Reporter via RL is a viable approach to ground visual/spatial properties for the Planner.",
            "uuid": "e339.0"
        },
        {
            "name_short": "Chinchilla-70B",
            "name_full": "Chinchilla (70 billion parameter variant)",
            "brief_description": "A large pre-trained language model (Chinchilla) used as the Planner; demonstrated strong zero-shot multi-step planning when provided with few-shot examples and truthful Reporter input.",
            "citation_title": "Training compute-optimal large language models.",
            "mention_or_use": "use",
            "model_name": "Chinchilla",
            "model_size": "70B",
            "model_description": "Pre-trained LSLM (Chinchilla) serving as a language-only Planner that generates natural-language instructions at each timestep based on the prompt (task description + few-shot examples + Reporter messages). It has no access to pixels; all perceptual grounding is supplied via Reporter text.",
            "task_name": "Secret-property conditional & secret-property search (grid-world)",
            "task_description": "See PAR entry. The Planner must command an Actor to examine objects and then pick the correct object based on reported hidden properties; in the visual conditional variant it must rely on a Reporter to convert pixels into descriptive text (e.g., 'close to wall').",
            "task_type": "multi-step planning / information-gathering / instruction following",
            "knowledge_type": "procedural + object-relational (+ spatial indirectly via Reporter)",
            "knowledge_source": "implicit knowledge from pre-training on large text corpora; in-context few-shot examples in prompt (5 examples) provide task-specific procedural template",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot prompting (in-context learning), iterative dialogue with Reporter (language observations)",
            "knowledge_representation": "Procedural and object-relational knowledge is encoded implicitly in network weights and explicitly manifested in generated natural-language instructions; sensory/spatial info is represented as natural-language reports supplied by Reporter.",
            "performance_metric": "success rate / task completion",
            "performance_result": "Secret-property conditional (zero-shot with 5-shot prompt): ~96% success. Secret-property search: reported as 'high' zero-shot success and more robust than conditional in the paper, exact percentage not provided. Robust to ~20% irrelevant Reporter messages when provided with few-shot demonstration of repeating strategy (performance restored to near-clean levels).",
            "success_patterns": "Effectively plans sequences of examines and pick-ups, integrates reported secret properties to choose targets, demonstrates emergent strategies (repeating/cycling) to handle noisy reports, and recovers from some Actor mistakes.",
            "failure_patterns": "Performance degrades with noisy or irrelevant Reporter outputs if no strategy examples are provided in prompt; relies entirely on textual reports for spatial reasoning and thus fails if Reporter is untruthful or uninformative.",
            "baseline_comparison": "Outperforms pure RL baselines (search ~25%, conditional ~33% after extensive RL training) in zero-shot settings; outperforms smaller Chinchilla 7B.",
            "ablation_results": "Providing strategy examples in prompt (ablation on prompt content) restores robustness under noisy reports. No direct ablation of internal representations reported beyond model-size comparisons.",
            "key_findings": "A large LM (70B) can internalize procedural and object-relational patterns sufficiently to perform multi-step embodied tasks via language-only interfaces, but requires reliable reporting for spatial/perceptual grounding; few-shot examples can teach high-level error-recovery strategies.",
            "uuid": "e339.1"
        },
        {
            "name_short": "Chinchilla-7B",
            "name_full": "Chinchilla (7 billion parameter variant)",
            "brief_description": "A smaller Chinchilla LSLM used as the Planner; shows limited ability to perform conditional multi-step planning from language prompts compared to the 70B variant.",
            "citation_title": "Training compute-optimal large language models.",
            "mention_or_use": "use",
            "model_name": "Chinchilla",
            "model_size": "7B",
            "model_description": "Pre-trained LSLM variant with fewer parameters serving as the Planner; uses same few-shot prompting and dialogue interface but encodes less robust procedural/object-relational reasoning.",
            "task_name": "Secret-property conditional & secret-property search (grid-world)",
            "task_description": "Same as other Planner entries: must instruct an Actor to examine and pick objects based on reported hidden properties; relies on Reporter to provide perceptual/spatial info.",
            "task_type": "multi-step planning / information-gathering / instruction following",
            "knowledge_type": "procedural + object-relational (+ spatial via Reporter)",
            "knowledge_source": "pre-training on text corpora (implicit) + in-context few-shot examples",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot in-context prompting",
            "knowledge_representation": "Implicit procedural and relational knowledge in weights; uses natural-language prompt templates to structure action sequences.",
            "performance_metric": "success rate / task completion",
            "performance_result": "Secret-property conditional (zero-shot with 5-shot prompt): ≈58% success (given full information). Secret-property search: performs worse than 70B and more susceptible to Reporter noise; exact search percentages not reported.",
            "success_patterns": "Can sometimes break down tasks into steps and follow simple in-context exemplars; benefits from few-shot templates when tasks are simple.",
            "failure_patterns": "More frequent failures to infer correct next instruction from Reporter feedback, less robust to irrelevant/noisy reports, and does not fully recover when prompt provides strategy examples (unlike 70B).",
            "baseline_comparison": "Underperforms 70B Planner; in many cases comparable or worse than extensive pure RL baseline performance depending on task difficulty.",
            "ablation_results": "Model-size ablation (7B vs 70B) shows notable drop in ability to use reported information to make correct procedural decisions. Prompt augmentation with strategy examples improves but does not fully restore performance.",
            "key_findings": "Smaller LMs have weaker implicit procedural and object-relational representations and are less able to utilize reported sensory information for correct embodied planning; model scale substantially affects the Planner's ability to integrate reports into correct next-action reasoning.",
            "uuid": "e339.2"
        },
        {
            "name_short": "Reporter (hard-coded & RL-trained)",
            "name_full": "Reporter module (hard-coded reporter and RL-trained reporter variants)",
            "brief_description": "Module that translates Actor pixel observations and actions into natural-language (or categorical) reports for the language-only Planner; implemented both as a hard-coded truth-telling reporter and as a learned visual classifier trained with RL to produce useful reports.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Reporter module (visual encoder + memory + policy head)",
            "model_size": null,
            "model_description": "Reporter shares architecture motifs with the Actor: a convolutional visual encoder, instruction encoder, and memory module; its output is either a hard-coded text report (truthful reporter) or a learned classification/head that outputs one of a small set of textual reports (e.g., 'object is close to wall' / 'far from wall'). Reporter variants include: (a) hand-coded truthful Reporter that returns exact hidden properties, and (b) an RL-trained Reporter that learns to map pixels to one of binary textual reports using episode reward as supervision.",
            "task_name": "Visual conditional / Visual location conditional; used across all examine-and-report tasks",
            "task_description": "Reporter receives the same egocentric top-down RGB crop as the Actor (5-square visibility) and produces textual reports about examined objects (hidden properties or spatial relations). In RL-trained variant, Reporter learns which visual features to communicate to Planner to improve episode reward.",
            "task_type": "perception-to-language translation / grounding",
            "knowledge_type": "spatial + object-relational (perceptual grounding of object properties and locations)",
            "knowledge_source": "learned from pixel observations with RL (or hard-coded mapping from environment events to text in the scripted Reporter); may be bootstrapped by pretrained perception models in future work",
            "has_direct_sensory_input": true,
            "elicitation_method": "hard-coded reporting (truthful) OR reinforcement learning to maximize final task reward (Reporter policy head is trained with V-trace)",
            "knowledge_representation": "Perceptual knowledge is encoded in visual encoder weights and represented in discrete textual reports (binary classifier outputs) that summarize spatial/object-relational properties for the Planner to consume; Reporter does not produce dense spatial maps in current implementation.",
            "performance_metric": "episode success rate when paired with Planner; reporter training convergence (frames)",
            "performance_result": "Reporter training converged rapidly (reporter training converged after ~500 frames in experiments). RL-trained Reporter improved overall Planner-Actor task performance (Fig 2C), though exact absolute percentages for improvement are not enumerated in-text. Hard-coded truthful Reporter yields high Planner success in conditional/search tasks.",
            "success_patterns": "A learned Reporter can discover to report the task-relevant perceptual feature (e.g., 'close to wall') using sparse final-reward signal; hard-coded truthful reporters reliably ground the Planner's language-only reasoning in perception.",
            "failure_patterns": "Noisy or irrelevant reporting (reporting movement logs 20% of the time) reduces Planner performance, especially for smaller Planners; untrained Reporter initially reports ungrounded/unhelpful information and must learn via RL to provide truthful, task-relevant summaries.",
            "baseline_comparison": "Hard-coded truthful Reporter vs RL-trained Reporter: hard-coded provides immediate high performance; RL-trained converges to useful reports in the experiments reported. Pretrained vision-language models (cited prior work) are an alternative but were not used here.",
            "ablation_results": "Reporter noise ablation: inserting irrelevant reports 20% of the time reduces success; however, providing strategy examples in the Planner prompt mitigates the impact for large Planners. Reporter training ablation: learned Reporter initialized from scratch can converge in a small number of frames but wall-clock training is expensive due to LM inference time in the loop.",
            "key_findings": "Effective grounding of spatial and object-relational information for a language-only Planner can be provided by a Reporter; such a Reporter can be trained via RL from sparse reward to produce truthful, task-relevant textual summaries, enabling the LM to plan without direct sensory access. Reporter quality and relevance are critical — noisy reporting disproportionately harms smaller Planners.",
            "uuid": "e339.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances.",
            "rating": 2,
            "sanitized_title": "do_as_i_can_not_as_i_say_grounding_language_in_robotic_affordances"
        },
        {
            "paper_title": "Socratic models: Composing zero-shot multimodal reasoning with language.",
            "rating": 2,
            "sanitized_title": "socratic_models_composing_zeroshot_multimodal_reasoning_with_language"
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models.",
            "rating": 2,
            "sanitized_title": "inner_monologue_embodied_reasoning_through_planning_with_language_models"
        },
        {
            "paper_title": "Training compute-optimal large language models.",
            "rating": 1,
            "sanitized_title": "training_computeoptimal_large_language_models"
        },
        {
            "paper_title": "Language models are few-shot learners.",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.01379175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Collaborating with language models for embodied reasoning</p>
<p>Ishita Dasgupta 
Christine Kaeser-Chen 
Kenneth Marino Deepmind 
Arun Ahuja 
Sheila Babayan 
Felix Hill 
Deepmind Rob 
Fergus Deepmind </p>
<p>DeepMind</p>
<p>DeepMind</p>
<p>DeepMind</p>
<p>DeepMind</p>
<p>Collaborating with language models for embodied reasoning</p>
<dl>
<dt>Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.</dt>
<dt>Figure 1</dt>
<dd>Setup. A. Schematic of the Planner-Actor-Reporter paradigm and an example of the interaction among them. B. Observation and action space of the PycoLab environment. by these components by introducing and evaluating on a series of tasks which require the agent to explore the world to gather information necessary for planning, break down complex tasks into steps, and communicate visual properties of the world back to the Planner. Finally, we demonstrate that the Reporter module can be trained with reinforcement learning (RL), reducing the need for hand-specified sources of feedback.</dd>
</dl>
<p>Methods</p>
<p>Environment and Actor: Our environment is a 2D partially observable grid-world. The environment contains unique objects specified by color, shape and texture, and the Actor sees a top-down egocentric pixel RGB view with visibility within 5 squares of the agent. In addition to movement actions, the Actor can perform two special actions when on top of an object: examine which reveals a hidden piece of text about the object, and pickup which adds the object to its inventory.</p>
<p>The Actor is pre-trained with RL to follow instructions of the form "Pick up the X" or "Examine the X". Figure 1B shows an example observation from the environment, details about Actor architecture and environment can be found in App B.</p>
<p>Planner: We use pre-trained large language models with the same architecture: Chinchilla [Hoffmann et al., 2022], of two sizes: 7B and 70B parameters. To promote grounding with in-context learning [Brown et al., 2020], we provide 5 randomly selected "few-shot examples" of each task (assuming optimal Planner, Reporter, and Actor; see App E for full text), and directly use the model's sampled language as input to the Actor. At every timestep, the sampled language and information generated by the Reporter are appended to the dialogue transcript, and used as the prompt to get a new instruction from the Planner at the next timestep.</p>
<p>Reporter: We specify the role of the Reporter further by drawing parallels to hierarchical RL [Sutton et al., 1999, Kulkarni et al., 2016, where a high-level 'Planner' issues temporally abstracted instructions to a lower-level 'Actor'. A key difference from these setups is that in our experiments, the observation space of the Actor and Planner are different. In our setup, the Actor operates over pixel observations and produces movement actions, while the Planner operates over a language observation (the prompt) and produces language actions (the produced instruction). The Actor is language conditional and can interpret the Planner's instructions. But the Planner cannot parse the results of the Actor's actions (to produce an appropriate next action). The Reporter translates from the Actor's action+observation space to the Planner's. In the most general case, a Reporter takes (a sequence of) Actor actions and pixel observations and produces a text string that contains true and relevant information about what the Actor did and how the environment responded.</p>
<p>There are several ways to implement a Reporter, varying what is reported and how much of it is hard-coded, pre-trained, or learned from scratch. Previous work has used implicit Reporters implemented as part of the Actor that only convey instruction-completion [Ahn et al., 2022], or pre-trained perception models that answer natural language questions about the Actor's observations [Zeng et al., 2022, Huang et al., 2022. In this work, we start with a hard-coded Reporter to first explore the performance of the Planner-Actor interaction in our novel information gathering tasks (Sec 3). We then pioneer learning this Reporter within the Planner-Actor-Learner loop to optimize reward (Sec 4).</p>
<p>Tasks:</p>
<p>We create a suite of tasks that examine the challenges of reasoning, generalization, and exploration in embodied environments that LM Planners can help with (detailed in App C). We focus on two types of tasks (conditional and search tasks) that require explicit information gathering such that a) the Planner must issue an explicit information gathering instruction, b) the Actor must carry it out, and c) the Reporter must relay the results before d) the Planner can issue the next instruction.</p>
<p>Language models as interactive planners</p>
<p>We examine the interaction between Planner, Actor and Reporter in tasks that require all three components for success. Building on top of previous work [Ahn et al., 2022, Zeng et al., 2022 which show that LSLMs can break down a complex real-world tasks into step-by-step instructions, we focus on tasks where the Planner needs to also explicitly issue information gathering instructions and incorporate the reported information for generating the next instructions. Further, our tasks are realized over objects with abstract properties that are not grounded in the LM's previous semantic experiences and therefore require significant abstract logical reasoning. We analyze the performance of different Planners and their robustness. All components are pre-trained.</p>
<p>The task setup is as follows: all the objects in the room have a 'secret property' (good / bad / unknown). When the Actor 'examines' an object, a ground-truth Reporter relays a text string 'I examined {object}, its secret property is {value}' to the Planner. The Planner can then issue the next instruction to the Actor.</p>
<p>Secret property conditional task</p>
<p>We start with the simplest task which requires information gathering. The goal of the episode is to pick up a correct object, based on another object's secret property. The task description passed to the Planner is as follows: 'If {decider object} is good, pick up {object 1}, otherwise pick up {object 2}'. A successful episode consists of 5 steps: a) the Planner instructs the Actor to examine the {decider object}, b) the Actor examines the object, c) the Reporter relays the revealed information (always done correctly in this setting), d) the Planner reasons which object needs to be picked up based on the report, {object 1} or {object 2}, and instructs the Actor to pick up the correct object e) the Actor picks up the correct object.</p>
<p>Explicit information gathering actions are classically challenging with pure RL. With a LSLM Planner and 5 language traces of solved examples as prompt, and an Actor trained on only simple pick-up and examine tasks, we can complete this complex multi-step task with good accuracy (Fig 2A). A pure RL baseline performs poorly even after 100M learner frames (see App D, Fig 2A).</p>
<p>In our analysis, we identify two main failure cases: the LSLM Planner failing to infer the next instruction given the environment feedback, and the Actor failing to follow the instruction provided by the Planner. In the first case, we observe that smaller language models (7B parameters) are only able to infer the correct object to pick up for reward 58% of the time given all information; larger language models (70B parameters) are able to do so 96% of the time. This shows that even relatively simple reasoning remains out of reach for smaller models without fine-tuning. In the second failure case, we observe that the Actor might encounter distribution shift, for example in episode length or instruction format, which makes it unable to Planner's instruction.</p>
<p>Secret property search task</p>
<p>We extend the previous task by requiring additional steps of information gathering. Instead of examining a single object, the agent needs to examine multiple objects, note their secret properties, and pick up the correct object for reward. The task is specified as 'The objects are {}, {}, {}, and {}. Pick up the object with the good secret property'. A successful episode consists of the Planner asking the Actor to examine each object in turn until it finds one with a 'good' property, at which point it asks the Actor to pick up that object.</p>
<p>Although this task requires more information gathering steps, and the RL baseline performs worse (see App D), the agent framework with Planner-Actor-Reporter is still able to complete the task zero-shot (i.e. without any additional environment interaction; Fig 2A). Curiously, we observe that our agents perform better in this task than in the previous task where only one object needs to be examined (Fig 2A; and App D). We hypothesize that since the number of information gathering steps varies, the Planner doesn't use a rigid "one examine, one pick up" policy and can be more robust to errors. For example, if the Actor examines the wrong object. We see that the Planner can indeed recover from such errors (Sec A.1). Similar to the observations above, we note that larger language models (70B) perform significantly better than smaller models (7B) (Fig 2A). </p>
<p>Robustness to irrelevant reports</p>
<p>We saw in the search task from the previous section, that the 70B Planner is reasonably robust to mistakes from the Actor (e.g. Section A.1). In this section, we examine if it can also be robust to a noisy Reporter. We break the assumption that only task relevant actions in the environment are reported, and irrelevant actions in the environment, e.g. "I have moved left" / "I have moved up and right" etc. are reported 20% of the time.</p>
<p>We find that performance does reduce but not dramatically ( Fig 2B). The smaller 7B model is less robust than the 70B model, showing a more dramatic reduction in performance. We find that the 70B Planner uses strategies of repetition (where it repeats an instruction until it receives the relevant report, e.g. Sec A.2) or cycling (where it cycles through examine instructions for all the objects, e.g. Sec A.3), or some combination of the two, until it hits a 'good object'.</p>
<p>The few shot prompts provide no examples of how to respond to irrelevant reports. When we do provide guidance and demonstrate a 'repeating' strategy (e.g. Sec A.2) in the prompted examples, this restores performance to that without the irrelevant reports for the 70B Planner ( Fig 2B); the 7B Planner improves but doesn't fully recover. This robustness indicates promise that our approach (particularly with large Planners) scales to imperfect Reporters. However, inference time through a large Planner is expensive, so a Reporter that ignores irrelevant events is more efficient.</p>
<p>Training a truthful Reporter</p>
<p>In the previous section, we focused on studying the behaviors of the Planner in our agent framework with a Reporter which always reports accurate information. However, such a Reporter does not exist in most environments. In this section, we study how we can train a reporter from scratch with RL.</p>
<p>We consider a 'visual conditional task' where the "secret property" is not directly revealed in text with a special 'examine' action, but rather must be decoded from visual observations. In particular, the task is specified as 'If {decider object} is close to the wall, pick up {object 1}, otherwise pick up {object 2}'. The Reporter's input is the same visual observations as the Actor and its output is a binary classifier head that can choose between one of two reports ('The object is {close to /far from} the wall'). Note that when training first starts, the Reporter does not have any pre-existing grounding mechanisms to report accurate information about the scene. As training continues, the Reporter can use the final reward of the episode to learn what information is most helpful to the Planner, and eventually converge to report only truthful and relevant information.</p>
<p>In contrast, recent work has used pretrained models with visual grounding (e.g. vision language models Zeng et al. [2022], or handcrafted mechanisms Huang et al. [2022]) to act as the Reporter module. We believe that building an effective Reporter module should combine both approaches: using a pre-trained module to bootstrap perception and grounding, and then using RL to finetune the pre-trained module to communicate with the Planner module. Our investigations show that Reporter training with RL is indeed viable and beneficial.</p>
<p>Discussion and future work</p>
<p>We advocate for a three-part system (Planner-Actor-Reporter), using pre-trained language models as a Planner that issues natural language commands to an embodied Actor, with a Reporter translating information back to the Planner. We introduce a series of tasks that leverage a pre-trained language model's abstract reasoning capacities, showing impressive and robust zero-shot performance, and analyse errors in different-sized models. We show the first proof of concept that the Reporter can be trained to facilitate better collaboration between Planner and Actor. Exciting directions for future work include incorporating pre-trained components into the Reporter, expanding to more complex/realistic tasks, and improving training with a large model in the loop. A Examples of Planner-Reporter-Actor dialogue</p>
<p>In this section, we include examples of Planner-Actor-Reporter dialogue, in particular for the secret property search tasks, demonstrating Planner robustness to Actor and Reporter errors.</p>
<p>A.1 A more robust search policy.</p>
<p>The red box shows where the Actor made a mistake, the green box highlights the Planner's recovery.</p>
<p>A.2 Emergent strategies in response to irrelevant reports: Repeating strategy.</p>
<p>The Planner repeats commands until it is completed, even in response to irrelevant reports.</p>
<p>A.3 Emergent strategies in response to irrelevant reports: Cycling strategy.</p>
<p>The Planner cycles through examine commands until it receives a report of a good object.// B Environment and agent architecture details</p>
<p>B.1 Grid-word environment</p>
<p>We implemented a grid-world environment with the PyColab library (https://github.com/deepmind/pycolab). The grid-world is 11x11, with the outer border being impassable walls, and no internal obstacles. There are four objects and one agent in the environment, each occupying one grid. Their start locations in each game are randomly assigned when the episode starts.</p>
<p>We assign objects in the environment randomly selected color, texture, and shape attributes, from pre-defined lists of allowed values. The exact object attribute combinations in evaluation tasks are held out from the training tasks, though each individual attribute value has been observed in training.</p>
<p>The agent's view on the environment is always a 11x11 crop of the scene from a top-down perspective, with the agent at the center. As the agent moves around, the crop scrolls to keep the agent centered in the visual observation.</p>
<p>B.2 Agent architecture</p>
<p>Our agent comprises three modules: a Planner, an Actor, and a Reporter. The Planner module is a pretrained LSLM. In our experiments, we use two variants of the Chinchilla models [Hoffmann et al., 2022], the one with 70 billion parameters (referred as the 70B model), and the one with 7 billion parameters (referred a the 7B model).</p>
<p>The Actor module uses a pre-trained policy, trained on simple tasks in the same grid-world environment. The policy uses a simple convolutional visual encoder with 3 layers to encode visual observations, and a LSTM-based language encoder to encode action instructions. The agent also has a LSTM-based memory module to help take previous actions and observations into account for policy output. The policy head for the Actor outputs a distribution in the action space, which in our case contains the discrete movement actions (e.g. move up or down in the grid-world), and the special actions of pick up and examine. The actual agent action is then sampled from the policy output.</p>
<p>The Reporter module shares a lot of similarities with the Actor architecture: it also comprises visual and instruction encoders, a memory module, and a policy head. In the experiments described in Chapter 4, the policy head is simply a binary distribution over two pre-defined text reports. Though in future work we do plan to allow direct language generation from the Reporter module.</p>
<p>B.3 Training procedures</p>
<p>Both the Actor and Reporter module are trained with standard VTrace loss [Espeholt et al., 2018]. The Actor training converged after about 100,000 frames, while the Reporter training converged after 500 frames.</p>
<p>We caution that despite the number of frames needed for Reporter training is not many, the training time can be very long. This is because in each episode, the LSLM Planner may need to be queried several times. Inference time of these models is quite slow still, which increases the amount of wall time needed to collect enough trajectories to train the agent.</p>
<p>C All task descriptions</p>
<p>We frame the capabilities of our agent, and similarly, the requirements of our task suite around four fundamental aspects of embodied intelligence:</p>
<ol>
<li>
<p>Logical Reasoning: The ability to take complex instructions and do different kinds of logical operations on them to determine the correct course of action.</p>
</li>
<li>
<p>Generalization: The ability to generalize beyond the agent's previous experience.</p>
</li>
</ol>
<p>Exploration:</p>
<p>The ability to explore the world around the agent to uncover new information that can inform its reasoning for what actions to take.</p>
<p>Perception:</p>
<p>The ability to use the raw observation the agent has (usually vision) and process the world and use what it sees to make decisions.</p>
<p>Logical Reasoning: The ability to take complex instructions and do different kinds of logical operations on them to determine the correct course of action.</p>
<p>This ability applies mainly to an embodied agent's ability to interpret the meaning of a goal specification (prompt) as well as integrate other information it has about the environment. This can include if-else conditionals, choosing from among options by eliminating options, choosing objects that match certain properties, etc. LSLMs are particularly adept at these kinds of logical language tasks [Rae et al., 2021, Hoffmann et al., 2022.</p>
<p>Generalization: The ability to generalize beyond the agent's previous experience.</p>
<p>The ability to generalize to new inputs has been well studied in RL [Kirk et al., 2021], but remains a significant challenge. In the field of LSLMs, we have seen remarkable success in few-shot [Brown et al., 2020] learning to new text tasks and inputs. These 'few-shots' are language traces of optimal behavior provided in the prompt, the agent never receives new interaction data or demonstrations for any of the generalization tasks. Language descriptions are much cheaper and easier to collect for new tasks than demonstrations or interactions. We examine generalization from the 'train' examples provided in the prompt to a new test prompt.</p>
<p>We study generalization of objects, where tasks have been seen in training with a specific set of objects and tested with other objects. We study generalization in language prompts, where the same task can be communicated in several different ways, but the structure of the task itself remains the same. Finally, we study generalization in the task itself, e.g. collecting 3 objects when the train traces only showed 2.</p>
<p>Exploration: The ability to explore the world around you to learn new information that can inform your reasoning for what actions to take.</p>
<p>This definition is subtly different from the way exploration is often used in the RL literature. Here we do not mean the ability during training to explore the policy space to find a more optimal policy (versus exploiting a known policy). We mean the agent's ability to actively discover information that is not observable in its current state. In our tasks, agents must move to uncover textual clues about objects using the "examine" action, or use perception to look at objects not currently in the agent's view.</p>
<p>Perception: The ability to use the raw observation the agent has (usually vision) and process the world and use what it sees to make decisions. This is ultimately the most fundamental agent ability, to observe its surroundings and to make sense of them. It is fundamental to RL in such a way that it is often not even mentioned as a core capability. This is however a core shortcoming of LSLMs, that operate purely over text and are not grounded. A core contribution of this work is to show how we can combine these complementary capabilities.</p>
<p>C.1 Tasks</p>
<p>Finally, we present a series of tasks that exemplify the challenges discussed above.</p>
<p>Option Elimination: Here we look at logical reasoning and (object and prompt) generalization. Consider the following motivating example: I know the corkscrew is either in the cutlery drawer or on the wine cabinet. I just checked the cutlery drawer, but it wasn't there. Can you find me the corkscrew?</p>
<p>The intended behavior from the agent is relatively simple, i.e. to "go to the wine cabinet". However, it is non-trivial to infer this simple instruction, and we examine if LSLMs can help. We design a task in our PyColab environment to emulate this kind of task. At the beginning of every episode, we select 4 unique objects and place them randomly. We then generate a templated instruction string with the names of these objects. We use 10 different language formats, we choose the Nshot prompts from 7 of them, and hold out 3 as test formats to examine generalization across prompts. In all of these, 3 of the 4 objects are "eliminated" as the target object, and optimal behavior is for the agent to go to the final object that was not eliminated. If the agent interacts with any other object, the episode ends without reward.</p>
<p>Basic</p>
<p>Step Tasks: We consider the simplest example of this where the task is to pick up two objects in the room e.g. "Pick up object X and Y in that order." The hope is that the language model can break this instruction down into its components, i.e. "Pick up X" and "Pick up Y" and issue them to the low-level agent in that order. This is the first multi-step task and therefore needs a Reporter (to tell the Planner when to issue the next instruction). We assume a truthful-and-relevant reporter that produces a text string "I have picked up <object name>." whenever the agent picks up an object. We examine generalization to different numbers of objects.</p>
<p>Conditional Secret Property: Consider the following example: Get me the coffee if it is still warm, otherwise get me a soda. The language model has no way to know whether the coffee is warm without the help of an embodied agent examining the coffee and reporting back on its temperature, based on which the language model can then issue the next instruction. To emulate this kind of information in our gridworld -like temperature or texture that can only be gathered by an embodied agent with direct interaction with an object -we assume that each object has a list of hidden properties. When the agent does a special "examine" action on an object, the environment produces a text string listing all of these properties.</p>
<p>We start with 4 randomly placed unique objects in the room, and the agent's goal is to pick up a single target object. The challenge come from figuring out what the right target object is -this requires information gathering interaction with the environment. In our task, the agent has to examine a single arbitrarily assigned decider object that might have the property good, bad. Depending on which it is, the agent has to pick up a different target object. Objects apart from the decider object are not good or bad and instead of secret property unknown, examining an non-decider object X returns the string 'I examined X. Its secret property has value unknown.'.</p>
<p>Search Secret Property:</p>
<p>Consider the following example: Can you bring me my water bottle? It's either in the fridge, in my gym bag, or in my backpack. The LM doesn't know where the water bottle is, the low-agent has to collect this information. The challenge in this task is to form and issue sequences of information gathering actions, recognize when the agent has received the necessary information to stop further information gathering, and issue a final instruction based on the results of this information gathering.</p>
<p>We emulate this structure in a search task where one of the four objects in the room is the target, but the agent does not know which one. This information is in the hidden properties of the objects: three of the four objects have the property bad, the remaining object has the property good, and the agent has to pick up the good object. Examining an object X returns the text string 'I examined X. Its secret property has value good/bad.' The agent therefore has to sequentially examine each of the objects; when it encounters a good object, it should pick it up.</p>
<p>Visual Color Conditional:</p>
<p>We start with a simple conditional task, where the target object changes depending on the color of the agent. This agent color information is not available to the language model. A reporter much therefore learn to decode it from visual observations, and report it back. The language model can then issue the right next instruction, which leads to reward. This task doesn't require information gathering actions or commands, as the agent color is directly visible to the agent when it spawns.</p>
<p>Visual Location Conditional: The previous task was effectively a single step task -the information relevant to knowing what the target is (this information being the color of the agent), is available right from the first step of the task. To combine the challenges of multistep tasks examined in previous section with learning to report, we consider a conditional task more similar to the one examined previously, where the agent has to do an information gathering action in order to get the relevant information. We consider a conditional task in which which object the agent must pick up depends on whether a designated decider object is close to the wall. In order to gauge this, the agent has to navigate to the object to examine its surroundings.</p>
<p>D RL Baselines</p>
<p>We also trained the Actor (pre-trained on the 'pick up ' and 'examine' tasks) directly on the secret property conditional and search tasks (3). Note that we see a learning curve for the basic 'pick up ' and 'examine' tasks as well because of a slight distribution shift -the Actor here additionally receives the event report as a separate observation (encoded with the same architecture used to encode the instruction), this is necessary to enable to perform the information gathering tasks (i.e. to use the information generated). For the the basic pick up and examine tasks, the agent just has to learn to ignore this field (giving fast learning). We find that after 5000 learner updates (140M learner frames), performance on the Search task is around 25% (chance performance when picking up a random object, without first examining it to determine if it is good), performance on the Conditional task is slightly higher, around 33% reflecting that the RL agent can learn to pick up one of the three objects in the prompt (and ignore the fourth), but doesn't learn to first examine the decider object to figure out which one of the target objects to pick up.</p>
<p>These baselines are not intended to convey that these tasks are impossible to learn with pure RL -better architectures discounting, exploration strategies etc will no doubt significantly improve performance. However, we demonstarte that even with several environment interactions, a pure RL baseline does find these tasks difficult -while our Planner-Actor-Reporter agent can perform fairly well with no additional environment interactions, and only 5 examples of optimal performance in the Planner's prompt. </p>
<p>Figure 2 :
2Results.A. Performance on secret property conditional and secret property search tasks with different Planners and baseline RL. B. Robustness of the Planners under an imperfect Reporter on the secret property search task. C. Improvement in performance as a Reporter is trained on the Visual conditional task. All error-bars are CIs across multiple episodes.</p>
<p>Figure 3 :
3Pure RL baselines. See main text for details.</p>
<p>A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.</p>
<p>E All prompt formats used E.1 Conditional task prompt QUESTION: If the solid dark blue h is good, pickup horizontal striped light green inverse plus. Otherwise, pickup checker brown tee. ANSWER: LM: Examine solid dark blue h.<EOS> Agent: I examined solid dark blue h. Its secret property has value bad.<EOS> LM: Pickup checker brown tee.<EOS> Agent: I picked up checker brown tee.<EOS> DONE QUESTION: If the noisy light green plus is good, pickup solid orange circle. Otherwise, pickup vertical striped blue upside down u. ANSWER: LM: Examine noisy light green plus.<EOS> Agent: I examined noisy light green plus. Its secret property has value bad.<EOS> LM: Pickup vertical striped blue upside down u.<EOS> Agent: I picked up vertical striped blue upside down u.<EOS> DONE QUESTION: If the vertical striped lavender upside down tee is good, pickup solid purple inverse plus. Otherwise, pickup vertical striped green h. ANSWER: LM: Examine vertical striped lavender upside down tee.<EOS> Agent: I examined vertical striped lavender upside down tee. Its secret property has value bad.<EOS> LM: Pickup vertical striped green h.<EOS> Agent: I picked up vertical striped green h.<EOS> DONE QUESTION: If the vertical striped pink plus is good, pickup horizontal striped blue tee. Otherwise, pickup vertical striped orange ex. ANSWER: LM: Examine vertical striped pink plus.<EOS> Agent: I examined vertical striped pink plus. Its secret property has value bad.<EOS> LM:</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, K Gopalakrishnan, K Hausman, A Herzog, arXiv:2204.01691arXiv preprintM. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.</p>
<p>. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T J Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Language models are few-shot learners. ArXiv, abs/2005.14165, 2020T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. J. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.</p>
<p>IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures. L Espeholt, H Soyer, R Munos, K Simonyan, V Mnih, T Ward, Y Doron, V Firoiu, T Harley, I Dunning, S Legg, K Kavukcuoglu, abs/1802.01561CoRRL. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, S. Legg, and K. Kavukcuoglu. IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures. CoRR, abs/1802.01561, 2018. URL http://arxiv.org/ abs/1802.01561.</p>
<p>. J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D De Las, L A Casas, J Hendricks, A Welbl, T Clark, E Hennigan, K Noland, G Millican, B Van Den Driessche, A Damoc, S Guy, K Osindero, E Simonyan, J W Elsen, O Rae, L Vinyals, Sifre, Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre. Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022.</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, arXiv:2207.05608arXiv preprintW. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.</p>
<p>A survey of generalisation in deep reinforcement learning. R Kirk, A Zhang, E Grefenstette, T Rocktaschel, abs/2111.09794ArXiv. R. Kirk, A. Zhang, E. Grefenstette, and T. Rocktaschel. A survey of generalisation in deep reinforce- ment learning. ArXiv, abs/2111.09794, 2021.</p>
<p>Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. T D Kulkarni, K Narasimhan, A Saeedi, J Tenenbaum, Advances in neural information processing systems. 29T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. Advances in neural information processing systems, 29, 2016.</p>
<p>Abstraction and analogy-making in artificial intelligence. M Mitchell, Annals of the New York Academy of Sciences. 15051M. Mitchell. Abstraction and analogy-making in artificial intelligence. Annals of the New York Academy of Sciences, 1505(1):79-101, 2021.</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. 2019.</p>
<p>. J W Rae, S Borgeaud, T Cai, K Millican, J Hoffmann, F Song, J Aslanides, S Henderson, R Ring, S Young, E Rutherford, T Hennigan, J Menick, A Cassirer, R Powell, G Van Den Driessche, L A Hendricks, M Rauh, P.-S Huang, A Glaese, J Welbl, S Dathathri, S Huang, J Uesato, J F J Mellor, I Higgins, A Creswell, N Mcaleese, A Wu, E Elsen, S M Jayakumar, E Buchatskaya, D Budden, E Sutherland, K Simonyan, M Paganini, L Sifre, L Martens, X L Li, A Kuncoro, A Nematzadeh, E Gribovskaya, D Donato, A Lazaridou, A Mensch, J.-B Lespiau, M Tsimpoukelli, N K Grigorev, D Fritz, T Sottiaux, M Pajarskas, T Pohlen, Z Gong, D Toyama, C De Masson, ; E Lockhart, S Osindero, L Rimell, C Dyer, O Vinyals, K W Ayoub, J Stanway, L L Bennett, D Hassabis, K Kavukcuoglu, G Irving, Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, C. Jones, J. Bradbury, M. G. Johnson, B. A. Hechtman, L. Weidinger, I. Gabriel, W. S. IsaacScaling language models: Methods, analysis &amp; insights from training gopher. ArXiv, abs/2112.11446, 2021J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. van den Driessche, L. A. Hendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. F. J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. M. Jayakumar, E. Buchatskaya, D. Budden, E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J.-B. Lespiau, M. Tsimpoukelli, N. K. Grigorev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d'Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, C. Jones, J. Bradbury, M. G. Johnson, B. A. Hechtman, L. Weidinger, I. Gabriel, W. S. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. W. Ayoub, J. Stanway, L. L. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving. Scaling language models: Methods, analysis &amp; insights from training gopher. ArXiv, abs/2112.11446, 2021.</p>
<p>Deep learning needs a prefrontal cortex. J Russin, R C O&apos;reilly, Y Bengio, Work Bridging AI Cogn Sci. 107J. Russin, R. C. O'Reilly, and Y. Bengio. Deep learning needs a prefrontal cortex. Work Bridging AI Cogn Sci, 107:603-616, 2020.</p>
<p>Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. R S Sutton, D Precup, S Singh, Artificial intelligence. 1121-2R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181-211, 1999.</p>            </div>
        </div>

    </div>
</body>
</html>
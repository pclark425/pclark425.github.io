<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5290 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5290</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5290</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-6c95835ffdf5ee65c35f50fd00a56f54faf15eda</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6c95835ffdf5ee65c35f50fd00a56f54faf15eda" target="_blank">Crystal structure generation with autoregressive large language modeling</a></p>
                <p><strong>Paper Venue:</strong> Nature Communications</p>
                <p><strong>Paper TL;DR:</strong> The authors introduce CrystaLLM, a methodology for the versatile generation of crystal structures, based on the autoregressive large language modeling (LLM) of the Crystallographic Information File (CIF) format, which can produce plausible crystal structures for a wide range of inorganic compounds unseen in training.</p>
                <p><strong>Paper Abstract:</strong> The generation of plausible crystal structures is often the first step in predicting the structure and properties of a material from its chemical composition. However, most current methods for crystal structure prediction are computationally expensive, slowing the pace of innovation. Seeding structure prediction algorithms with quality generated candidates can overcome a major bottleneck. Here, we introduce CrystaLLM, a methodology for the versatile generation of crystal structures, based on the autoregressive large language modeling (LLM) of the Crystallographic Information File (CIF) format. Trained on millions of CIF files, CrystaLLM focuses on modeling crystal structures through text. CrystaLLM can produce plausible crystal structures for a wide range of inorganic compounds unseen in training, as demonstrated by ab initio simulations. Our approach challenges conventional representations of crystals, and demonstrates the potential of LLMs for learning effective models of crystal chemistry, which will lead to accelerated discovery and innovation in materials science. The first step in predicting material properties is the generation of a plausible crystal structure. Here, the authors introduce a large language model that can achieve this task given only the chemical composition of the material.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5290.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5290.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CrystaLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CrystaLLM (Autoregressive CIF-language Large Language Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive decoder-only Transformer LLM trained on standardized Crystallographic Information File (CIF) text to generate plausible inorganic crystal structures (CIFs) conditioned on cell composition and optionally space group; can be decoded with top-k sampling or Monte Carlo Tree Search guided by a formation-energy predictor.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CrystaLLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Autoregressive Transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Small: 25 million parameters; Large: 200 million parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Approximately 2.3 million CIF files compiled from Materials Project, OQMD (v1.5), and NOMAD (≈3.6M initial structures reduced to 2.3M unique cell composition-space group pairs). Tokenized CIF corpus ≈768 million tokens; vocabulary of 371 symbols (CIF tags, elements, digits, punctuation).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science — inorganic crystal structure generation (CSP seeding), materials discovery for applications such as energy and electronic devices; general-purpose structure generation for downstream property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive next-token prediction conditioned on a prompt (data_ header + cell composition, optionally space group); decoding via top-k random sampling (k=10, temperature=1.0) and an alternative heuristic MCTS decoding (PUCT-guided Monte Carlo Tree Search) that uses a separate ALIGNN formation-energy predictor as a reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Textual Crystallographic Information File (CIF) format containing space group, lattice parameters, atomic site labels and fractional x,y,z coordinates (digit-by-digit generation of numeric tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity checks (space-group consistency via spglib/pymatgen, atom site multiplicity consistency, bond-length reasonableness within ±30% of expected values); bond-length reasonableness score; structure matching using pymatgen StructureMatcher (fractional length/site/angle tolerances) producing match rates; RMSE normalized by (V/N)^(1/3) between generated and ground-truth structures; R^2 and MAE for cell lengths/volumes versus ground truth; ALIGNN-predicted formation energy per atom; selective DFT relaxation comparisons for generated pyrochlores.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Benchmarks used: Perov-5, Carbon-24, MP-20, MPTS-52 (models trained either on benchmark train sets or on the full 2.3M dataset minus MPTS-52 val/test). Training data sources: Materials Project, OQMD, NOMAD.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>High syntactic/structural validity (≈94% valid for test-set prompting); strong match rates on held-out test structures (e.g., 'at least 1 match within 3 attempts' ≈88.1% when space group provided); challenge-set successful generation rates 85.7–91.4% depending on model/conditioning; match rates for unseen challenge compounds up to ≈41.4% (large model with space group); benchmark comparisons: CrystaLLM outperformed diffusion-based models (CDVAE, DiffCSP) on RMSE for 3/4 benchmarks and was competitive on match rates (examples: CrystaLLM a, n=20: Perov-5 match rate 98.26% RMSE 0.0236; MPTS-52 n=20 match rate up to 33.75% with RMSE ~0.1059); for selected generated pyrochlores not in training, generated cell parameter a correlated with DFT-relaxed values with R^2=0.62 and MAE=0.08 Å; MCTS decoding improved validity and produced lower ALIGNN-predicted formation energies for many problematic cases (validity improved in 95% of problematic cases without space group; minimum E_f improved in 85%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to diffusion and VAE-based crystal generators (CDVAE, DiffCSP), CrystaLLM shows superior or competitive accuracy in RMSE and match rates on multiple CSP benchmarks; unique advantages include direct conditioning on space group, flexible textual conditioning, and straightforward fine-tuning workflows typical of LLMs. Limitations relative to physics-based CSP: generated structures are plausible but not guaranteed ground states—fine-tuning or reinforcement-style alignment to energy is needed to target low-energy structures. Unlike template-substitution procedural methods, CrystaLLM learns implicit templates but may underperform for under-represented template classes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cannot generate site-occupancy disorder (no fractional occupancies present in training data); struggles with rare or under-represented templates (phosphates, sulfates, carbonates, organic-inorganic hybrids) and CIFs with very long token sequences; training dataset heterogeneity (DFT sources with differing settings/functionals) may confound learning of fine structural-detail correlations; generated structures are not guaranteed to be lowest-energy polymorphs—external energy predictors (ALIGNN) and MCTS heuristics mitigate but do not replace expensive DFT relaxation; class imbalance in training data affects generation success on low-data structural classes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crystal structure generation with autoregressive large language modeling', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DiffCSP <em>(Rating: 2)</em></li>
                <li>CDVAE <em>(Rating: 2)</em></li>
                <li>ALIGNN <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5290",
    "paper_id": "paper-6c95835ffdf5ee65c35f50fd00a56f54faf15eda",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "CrystaLLM",
            "name_full": "CrystaLLM (Autoregressive CIF-language Large Language Model)",
            "brief_description": "An autoregressive decoder-only Transformer LLM trained on standardized Crystallographic Information File (CIF) text to generate plausible inorganic crystal structures (CIFs) conditioned on cell composition and optionally space group; can be decoded with top-k sampling or Monte Carlo Tree Search guided by a formation-energy predictor.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CrystaLLM",
            "model_type": "Autoregressive Transformer (decoder-only)",
            "model_size": "Small: 25 million parameters; Large: 200 million parameters",
            "training_data": "Approximately 2.3 million CIF files compiled from Materials Project, OQMD (v1.5), and NOMAD (≈3.6M initial structures reduced to 2.3M unique cell composition-space group pairs). Tokenized CIF corpus ≈768 million tokens; vocabulary of 371 symbols (CIF tags, elements, digits, punctuation).",
            "application_domain": "Materials science — inorganic crystal structure generation (CSP seeding), materials discovery for applications such as energy and electronic devices; general-purpose structure generation for downstream property prediction.",
            "generation_method": "Autoregressive next-token prediction conditioned on a prompt (data_ header + cell composition, optionally space group); decoding via top-k random sampling (k=10, temperature=1.0) and an alternative heuristic MCTS decoding (PUCT-guided Monte Carlo Tree Search) that uses a separate ALIGNN formation-energy predictor as a reward model.",
            "output_representation": "Textual Crystallographic Information File (CIF) format containing space group, lattice parameters, atomic site labels and fractional x,y,z coordinates (digit-by-digit generation of numeric tokens).",
            "evaluation_metrics": "Validity checks (space-group consistency via spglib/pymatgen, atom site multiplicity consistency, bond-length reasonableness within ±30% of expected values); bond-length reasonableness score; structure matching using pymatgen StructureMatcher (fractional length/site/angle tolerances) producing match rates; RMSE normalized by (V/N)^(1/3) between generated and ground-truth structures; R^2 and MAE for cell lengths/volumes versus ground truth; ALIGNN-predicted formation energy per atom; selective DFT relaxation comparisons for generated pyrochlores.",
            "benchmarks_or_datasets": "Benchmarks used: Perov-5, Carbon-24, MP-20, MPTS-52 (models trained either on benchmark train sets or on the full 2.3M dataset minus MPTS-52 val/test). Training data sources: Materials Project, OQMD, NOMAD.",
            "results_summary": "High syntactic/structural validity (≈94% valid for test-set prompting); strong match rates on held-out test structures (e.g., 'at least 1 match within 3 attempts' ≈88.1% when space group provided); challenge-set successful generation rates 85.7–91.4% depending on model/conditioning; match rates for unseen challenge compounds up to ≈41.4% (large model with space group); benchmark comparisons: CrystaLLM outperformed diffusion-based models (CDVAE, DiffCSP) on RMSE for 3/4 benchmarks and was competitive on match rates (examples: CrystaLLM a, n=20: Perov-5 match rate 98.26% RMSE 0.0236; MPTS-52 n=20 match rate up to 33.75% with RMSE ~0.1059); for selected generated pyrochlores not in training, generated cell parameter a correlated with DFT-relaxed values with R^2=0.62 and MAE=0.08 Å; MCTS decoding improved validity and produced lower ALIGNN-predicted formation energies for many problematic cases (validity improved in 95% of problematic cases without space group; minimum E_f improved in 85%).",
            "comparison_to_other_methods": "Compared to diffusion and VAE-based crystal generators (CDVAE, DiffCSP), CrystaLLM shows superior or competitive accuracy in RMSE and match rates on multiple CSP benchmarks; unique advantages include direct conditioning on space group, flexible textual conditioning, and straightforward fine-tuning workflows typical of LLMs. Limitations relative to physics-based CSP: generated structures are plausible but not guaranteed ground states—fine-tuning or reinforcement-style alignment to energy is needed to target low-energy structures. Unlike template-substitution procedural methods, CrystaLLM learns implicit templates but may underperform for under-represented template classes.",
            "limitations_or_challenges": "Cannot generate site-occupancy disorder (no fractional occupancies present in training data); struggles with rare or under-represented templates (phosphates, sulfates, carbonates, organic-inorganic hybrids) and CIFs with very long token sequences; training dataset heterogeneity (DFT sources with differing settings/functionals) may confound learning of fine structural-detail correlations; generated structures are not guaranteed to be lowest-energy polymorphs—external energy predictors (ALIGNN) and MCTS heuristics mitigate but do not replace expensive DFT relaxation; class imbalance in training data affects generation success on low-data structural classes.",
            "uuid": "e5290.0",
            "source_info": {
                "paper_title": "Crystal structure generation with autoregressive large language modeling",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DiffCSP",
            "rating": 2
        },
        {
            "paper_title": "CDVAE",
            "rating": 2
        },
        {
            "paper_title": "ALIGNN",
            "rating": 2
        }
    ],
    "cost": 0.011124499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Crystal Structure Generation with Autoregressive Large Language Modeling</h1>
<p>Luis M. Antunes<em>1, Keith T. Butler ${ }^{2}$, and Ricardo Grau-Crespo</em>1<br>${ }^{1}$ Department of Chemistry, University of Reading, Whiteknights, Reading RG6 6DX, United Kingdom. l.m.antunes@pgr.reading.ac.uk; r.grau-crespo@reading.ac.uk<br>${ }^{2}$ Department of Chemistry, University College London, WC1H 0AJ, United Kingdom.</p>
<h4>Abstract</h4>
<p>The generation of plausible crystal structures is often the first step in predicting the structure and properties of a material from its chemical composition. Quickly generating and predicting inorganic crystal structures is important for the discovery of new materials, which can target applications such as energy or electronic devices. However, most current methods for crystal structure prediction are computationally expensive, slowing the pace of innovation. Seeding structure prediction algorithms with quality generated candidates can overcome a major bottleneck. Here, we introduce CrystaLLM, a methodology for the versatile generation of crystal structures, based on the autoregressive large language modeling (LLM) of the Crystallographic Information File (CIF) format. Trained on millions of CIF files, CrystaLLM focuses on modeling crystal structures through text. CrystaLLM can produce plausible crystal structures for a wide range of inorganic compounds unseen in training, as demonstrated by ab initio simulations. The integration with predictors of formation energy permits the use of a Monte Carlo Tree Search algorithm to improve the generation of meaningful structures. Our approach challenges conventional representations of crystals, and demonstrates the potential of LLMs for learning effective 'world models' of crystal chemistry, which will lead to accelerated discovery and innovation in materials science.</p>
<h2>1 Introduction</h2>
<p>The in silico search for new materials often involves the exploration of a space of compositions in a chemical system, and the investigation of various predicted structural phases in that space (see [1], [2] and [3] for examples). To elucidate the structures of unknown materials, a Crystal Structure Prediction (CSP) approach is often employed, which attempts to derive the ground state crystal structure for a given chemical composition under specific physical conditions. [4] CSP approaches are relatively computationally expensive, typically involving ab initio techniques. [5] They often begin with the generation of candidate structures. Examples are the AIRSS $[6,7]$ and USPEX [8] approaches. Initializing the search space with sensible structures increases the likelihood of success, and decreases the</p>
<p>amount of computation required. It is therefore expected that effective crystal structure generation tools would help accelerate the prediction of structures using CSP methods.</p>
<p>Increasingly, techniques from machine learning (ML) and data science are being used to solve problems in materials science. [9,10] In particular, generative modeling approaches based on autoencoder architectures and generative adversarial networks (GANs) [11] have been used to generate crystal structures. [12-15] Indeed, generative modeling has become commonplace, an outcome catalyzed by astounding advancements in the computational generation of images, audio and natural language over the last several years. [16] The Large Language Model (LLM), backed by the Transformer architecture [17], is the approach behind state-of-the-art performance on natural language processing tasks. This approach begins with a generative pre-training step, which is autoregressive in nature, involving the unsupervised task of predicting the next token given a sequence of preceding tokens. [18] When such models are scaled to billions of parameters, their effectiveness becomes quite remarkable, as tools such as ChatGPT [19] demonstrate.</p>
<p>LLMs have recently been used in the context of materials science. [20-25] These attempts have been focused on using existing and publicly accessible LLMs, training and tuning LLMs for natural language generation tasks involving chemical subject matter, or training LLMs on a corpus of expanded chemical compositions for the purposes of generating unseen compositions. However, the potential of training LLMs on textual representations of crystal structures has not been considered. A sole exception is a recent pre-print by Flam-Shepherd and Aspuru-Guzik, where the idea of generating the structures of molecules, materials, and protein binding sites with LLMs has been preliminarily explored [26].</p>
<p>Here, we report the first LLM specifically designed for crystal generation. This model is distinctively trained on textual representations of inorganic crystal structures, specifically in the Crystallographic Information File (CIF) format [27], instead of relying solely on natural language corpora, or chemical compositions alone. The motivation for this approach originates from two conjectures: The first states that a sequence of symbols (i.e. tokens) is an appropriate representation modality for many predictive tasks, including those involving chemical structure. The idea of representing any domain with a sequence of tokens may at first seem counter-intuitive. However, consider that even images can be represented this way, and be subject to the autoregressive language modeling of pixels [28]. This challenges the notion that domain-specific representations, such as graphs for chemical structure [29], are necessary for superior performance. The second conjecture states that LLMs learn more than simply surface statistics and the conditional probability distribution of tokens. Indeed, autoregressive pre-training involving next-token prediction may result in learning an effective world model: an internalized causal model of the processes generating the target phenomena. A model which simply learns spurious correlations in the data is less desirable, as it may have greater difficulty in generalizing beyond the training distribution. Recent studies have demonstrated that LLMs trained on sequences of board game play (e.g. chess and Othello) do indeed track the state of the board, and probes of the internal activations of the model reveal the existence of representations of various abstract concepts specific to the domain. [30,31] We therefore asked whether a model trained to predict the 3-dimensional coordinates of atoms, digit-by-digit, could learn the chemistry implicit in crystal structures, and generate unseen structures, borrowing from its model of the world of atoms.</p>
<p>As such, we herein describe the CrystaLLM model, a tool for crystal structure generation trained on an extensive corpus of CIF files representing the structures of millions of inorganic solid-state materials. Unlike small molecule organic compounds, the generative modeling of inorganic crystals presents unique challenges: the structures are complex and periodic, are not readily described by simple graphs, are imbued with different forms of symmetry, and can be constructed from more than 100 different elements. Even so, the model is capable of reliably generating correct CIF syntax and physically plausible crystal structures for many classes of inorganic compounds. Moreover, we demonstrate how sampling from the model can be improved using the Monte Carlo Tree Search (MCTS) algorithm [32,33] together with a pre-trained graph-based neural network predictor of formation energy.</p>
<h1>2 Results</h1>
<p>CrystaLLM is a Transformer-based, decoder-only language model of the CIF file format, trained autoregressively on a corpus of millions of CIF files (Figure 1a). Rather than training on structural representations derived from the CIF files, the model is directly trained on the standardized and tokenized text contents of the CIF files. During training, the model is given a sequence of tokens from the corpus of CIF files, and is tasked with predicting the tokens which follow each of the given tokens. Once the model is trained, it can be used to generate new CIF files, conditioned on some starting sequence of tokens. Generating a CIF file involves repeatedly sampling tokens from the model, conditioning on the accumulated generated content, until a terminating condition is reached (Figure 1b).</p>
<p>To assess the ability of the model to generate structures, a test set of approximately 10,000 randomly chosen CIF files is withheld from a training set of approximately 2.2 million CIF files, and the model is tasked with generating CIF files beginning from prompts constructed from the test set. Moreover, we assemble what we call a challenge set, which consists of 70 structures, 58 of which were obtained from the recent literature, and were not in the training set. The remaining 12 structures are from the training set, and are included as representatives of different structural classes. They serve to assess the model's ability to recover what it has seen in training, and as a means of comparing the model's generations of seen and unseen structures. (Supplementary Table 1 contains the full list of the challenge set compounds, and their sources.) The permutative nature of the dataset, with many structures having been derived by substituting atoms into pre-defined templates, results in a test set with the potential for some structures to closely resemble those of the training set. The challenge set provides a source of structures that are guaranteed to have been produced through a different process. Moreover, the challenge set constitutes a manageable set of compounds that reflects a variety of solid-state structural classes, allowing for a fine-grained picture of the model's capabilities. The test set, on the other hand, is better suited for a bulk assessment, and originates from the same distribution as the training set.</p>
<p>The following terminology is used in the remainder of this article: A formula, reduced formula, or reduced composition, refers to the empirical formula, or formula unit, which is the simplest, whole-number ratio of atoms in the compound. An example of a formula is $\mathrm{Ba}_{2} \mathrm{MnCr}$. A cell composition is a chemical formula referring to the total number of</p>
<p>a
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: a Core concepts in training a Large Language Model of CIF files: A CIF file (left) is converted into a sequence of symbols, through tokenization. The sequence is processed by the model, which produces a list of probability distributions over the vocabulary, for each corresponding symbol in the input. The resulting predicted probability distributions are evaluated against the target distributions (which contain the entire probability mass on the correct subsequent token), using the cross-entropy loss metric. The target tokens are the input tokens shifted one spot to the left, as the objective is to predict the next token given a sequence of preceding tokens. The tokens are categorized as CIF tags (blue), atoms (green), numeric digits (gold), and punctuation (red). Output tokens (not actually sampled during training) represent the tokens assigned the highest probability by the model. Underlined tokens represent predicted distributions assigning a relatively low probability to the correct next token. b Generation of a CIF file: First, a prompt is constructed by concatenating the symbol data_ with the desired cell composition, which is then tokenized and processed by the model. Next, a token is sampled from the predicted distribution for the upcoming token in the sequence. Finally, the sampled token is added to the accumulating contents of the CIF file. This procedure continues iteratively until a predefined terminating condition is met (e.g. two consecutive newline tokens are sampled).</p>
<p>atoms of each type in the unit cell of a crystal. It represents the chemical formula of the compound as it would appear in the crystallographic unit cell, which might contain $Z$ formula units. An example of a cell composition is $\mathrm{Ba}<em 3="3">{6} \mathrm{Mn}</em>$, with a $Z$ of 3 .} \mathrm{Cr}_{3</p>
<h1>2.1 Training and Learned Representations</h1>
<p>Training consists of iteratively sampling sequences of tokens, of fixed length, and adjusting the model's parameters so that it becomes progressively better at predicting which token should follow a preceding sequence. (See the Methods, and Supplementary Note 2, for more information on the model architecture and training.) Since it has been observed that LLM performance improves as the number of model parameters is increased [34], we train a small model, consisting of 25 million parameters, and a large model, consisting of 200 million parameters.</p>
<p>To monitor the progress of training, we withhold a validation set that constitutes $10 \%$ of the set held-out for training. Over the course of training, the model continues to improve in terms of its total cross-entropy loss on the validation set, even after 90,000 iterations (see Supplementary Figure 2). We note, however, that improvements appear to become smaller with more training time.</p>
<p>As a consequence of the model's architecture, each token in a processed sequence is mapped to a distinct learned vector representation using an embedding table, whose parameters are adjusted during training. The result is that, through autoregressive training, distributed representations are learned for each symbol in the vocabulary. The vocabulary consists of symbols for atoms, space groups, and numeric digits. (See Supplementary Note 1 for a detailed description of the vocabulary and the tokenization procedure.) The training process appears to result in sensible representations of these various symbols. Plots of dimensionally-reduced atom and space group vectors demonstrate a logical structure, where similar entities cluster together, indicating that intrinsic properties and relationships are captured. (See Supplementary Figure 3 for plots of the learned atom vectors, and Supplementary Figure 4 for a plot of the learned space group vectors.) Moreover, examination of the learned numeric digit vectors reveals that numerical relationships are captured in the representations, as measurements of cosine and Euclidean distances between the learned digit vectors demonstrate a logical spatial relationship. (See Supplementary Figure 5.) While not explored further in this work, we note that distributed representations of chemical entities, such as atoms, are useful for the prediction of materials properties [35].</p>
<h3>2.2 Generalizing to Unseen Structures</h3>
<p>To evaluate the ability of the model to generate an unseen structure, the model is prompted with the structure's cell composition, and allowed to generate up to 3,000 tokens. The prompt includes the first line of the CIF file, which consists of the data block header, containing the cell composition of the structure. Subsequently, the model is prompted with both the structure's cell composition and space group, and again allowed to generate up to 3,000 tokens. The prompt includes the first several lines of the pre-processed CIF file, up to the line containing the specification of the space group. Prompting the model with both the cell composition and space group allows us to assess</p>
<p>how reliant the model is on the space group. This process is repeated for all CIF files of the held-out test set ( 10,286 in total).</p>
<p>The generated CIF files are then assessed for correctness and quality. Any syntactically incorrect CIF files are declared invalid. Syntactically correct CIF files are subjected to further analysis, and are considered to be valid only if specific criteria are met, such as being consistent in terms of generated structure and declared space group, and having reasonable bond lengths (see Supplementary Note 3 for further details on the validation of generated CIF files). The results of evaluating the generation of the CIF files of the test set using the small model are presented in Table 1.</p>
<p>Table 1: Performance of the small model on the held-out test set. The percentages represent the fraction of test set compounds which meet the corresponding criteria. For example, the first row represents the percentage of test set compounds where the declared space group in the generated CIF file is consistent with the generated structure. Valid generated length refers to the length of a valid generated CIF file in terms of the number of tokens.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">No Space Group</th>
<th style="text-align: center;">With Space Group</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Space Group Consistent</td>
<td style="text-align: center;">$98.8 \%$</td>
<td style="text-align: center;">$99.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Atom Site Multiplicity Consistent</td>
<td style="text-align: center;">$99.4 \%$</td>
<td style="text-align: center;">$99.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Bond Length Reasonableness Score</td>
<td style="text-align: center;">$0.9878 \pm 0.0686$</td>
<td style="text-align: center;">$0.9878 \pm 0.0671$</td>
</tr>
<tr>
<td style="text-align: left;">Bond Lengths Reasonable</td>
<td style="text-align: center;">$94.6 \%$</td>
<td style="text-align: center;">$94.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Valid</td>
<td style="text-align: center;">$93.8 \%$</td>
<td style="text-align: center;">$94.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Longest Valid Generated Length</td>
<td style="text-align: center;">1145</td>
<td style="text-align: center;">970</td>
</tr>
<tr>
<td style="text-align: left;">Average Valid Generated Length</td>
<td style="text-align: center;">$331.885 \pm 42.567$</td>
<td style="text-align: center;">$339.002 \pm 41.361$</td>
</tr>
</tbody>
</table>
<p>The CIF files generated by prompting the model with the cell composition and space group were compared to the corresponding CIF files of the test set using a structure matching algorithm. The fraction of matching structures is presented in Table 2.</p>
<p>Table 2: Structure matching results for the test set when the space group is included in the prompt. The Reduced Unseen column represents the results for formulas that were not seen in training with any $Z$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">All</th>
<th style="text-align: center;">Reduced Unseen</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">At least 1 match within 3 attempts</td>
<td style="text-align: center;">$88.1 \%$</td>
<td style="text-align: center;">$86.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">All 3 attempts matching</td>
<td style="text-align: center;">$67.4 \%$</td>
<td style="text-align: center;">$70.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Matched on 1st attempt</td>
<td style="text-align: center;">$78.4 \%$</td>
<td style="text-align: center;">$78.7 \%$</td>
</tr>
</tbody>
</table>
<p>We further examined how closely the generated cell parameters resembled the actual cell parameters, for the cases where there was a structural match. We took the first matching structure for samples that had at least one generated structure matching the test set structure, and measured the $R^{2}$ and mean absolute error (MAE) for the true versus generated cell lengths, the true versus generated (i.e. printed) volume, and the implied (from cell parameters) versus generated volume. The results are presented in Figure 2.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: a The generated cell lengths for matching structures of the test set vs. the true cell lengths, when space group is included. b The generated cell volumes for matching structures of the test set vs. either the true cell volumes, or the cell volumes implied from the generated cell parameters, when space group is included.</p>
<p>To further assess the model's ability to generalize to unseen structures, we prompted the model with the cell compositions of the challenge set. The challenge set contains 58 structures not seen in training. These structures were all manually sourced from the recent literature, and represent experimentally characterized materials. Crucially, these compounds originate through a process different from the process which generated the training set (namely, a high-throughput DFT analysis of hypothetical materials). They also represent a variety of different structural classes, such as intermetallics, silicates, sulfides and selenides, borates, phosphates, carbonates, and complex mixed-anion compounds.</p>
<p>Both the small and large models were prompted with the cell compositions of the challenge set, both with and without the space group. A total of 100 attempts were made to generate a structure from the given cell composition (and optionally space group). We record the successful generation rate, representing the fraction of compounds where at least one valid CIF file was generated in the 100 attempts, and the true match rate, representing the fraction of compounds where there was a structural match between a valid generated structure and the true structure reported in the literature. The results are presented in Table 3 and Supplementary Tables 2 to 5.</p>
<p>Table 3: Results of the small and large models on the challenge set, both with a space group ('s.g.') and without. The first row represents the percentage of cases where the model was able to generate a valid structure within 100 attempts. The second row represents the percentage of cases where a generated structure matched the true structure, for the compounds seen in training. The last row represents the percentage of cases where a generated structure matched the true structure, for unseen compounds only.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Small model <br> no s.g.</th>
<th style="text-align: center;">Large model <br> with s.g.</th>
<th style="text-align: center;">no s.g.</th>
<th style="text-align: center;">with s.g.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Successful Generation Rate</td>
<td style="text-align: center;">$85.7 \%$</td>
<td style="text-align: center;">$88.6 \%$</td>
<td style="text-align: center;">$87.1 \%$</td>
<td style="text-align: center;">$91.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Match Rate (Seen)</td>
<td style="text-align: center;">$50.0 \%$</td>
<td style="text-align: center;">$50.0 \%$</td>
<td style="text-align: center;">$83.3 \%$</td>
<td style="text-align: center;">$83.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Match Rate (Unseen)</td>
<td style="text-align: center;">$25.9 \%$</td>
<td style="text-align: center;">$34.5 \%$</td>
<td style="text-align: center;">$37.9 \%$</td>
<td style="text-align: center;">$41.4 \%$</td>
</tr>
</tbody>
</table>
<p>The results in Table 3 indicate that inclusion of the space group in the prompt increases the likelihood of generating a valid structure, and of generating a match with the true structure. The large model appears to be superior to the small model in all categories. While the models can recover the reported structure more often when the structure was seen in training, it is noteworthy that they are able to generate unseen structures which match the reported structure in up to $40 \%$ of the cases.</p>
<h1>2.3 Comparison with Other ML-based Approaches</h1>
<p>Generative models of materials based on advanced ML techniques have recently been developed. CDVAE [13] and DiffCSP [36] are two such examples. Both models use diffusion-based methods for generating materials, with DiffCSP focusing on crystal structure prediction through an equivariant diffusion process, while CDVAE uses a diffusionbased approach within a variational autoencoder framework for generating periodic materials. We compare CrystaLLM to these models on four benchmarks: Perov-5 [37, 38], Carbon-24 [39], MP-20 [40], and MPTS-52 [41]. The Perov-5 dataset consists of 18,928</p>
<p>perovskites, Carbon-24 consists of 10,153 carbon allotropes, MP-20 consists of 45,231 stable inorganic materials of various classes, while MPTS-52 consists of 40,476 various inorganic materials. MPTS-52 is by far the most complex dataset, with up to 52 atoms in the unit cells of the constituent structures.</p>
<p>The benchmark datasets have each been split into training, validation and test sets. Models are trained on the training set, and then are used to generate 20 structures for each of the cell compositions of the test set. The models are evaluated in terms of the match rate, which is the fraction of compositions for which the true structure was generated within $n$ attempts (we tried $n=1$ and 20), and the average root mean squared error (RMSE) of the closest candidate for each test set structure. The results are presented in Table 4 .</p>
<p>We present results for three different versions of CrystaLLM. Versions $a$ and $b$ are trained on the benchmark data only and differ in the size of the model used. Version $c$ is trained on the full 2.3 M training points minus the test set of MPTS-52 and is included to demonstrate how the results improve with the size of training data, but is not directly comparable to other models due to the different training data sets.</p>
<p>Table 4: Benchmark CSP results. Numbers in bold indicate the best $n=20$ result, while italicized numbers represents the best $n=1$ result, amongst the models trained only on the benchmark training sets, where $n$ represents the number of samples generated for each structure of the benchmark test set. $a$ Results for the small model architecture trained only on the benchmark training sets. $b$ Results for the large model architecture trained only on the benchmark training sets. $c$ Results for the small model architecture trained on the original 2.3 M -structure dataset without the structures of the MPTS-52 validation or test sets. The CDVAE and DiffCSP results are taken from Jiao et al. [36].</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Perov-5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Carbon-24</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MP-20</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MPTS-52</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">$n$</td>
<td style="text-align: center;">Match Rate</td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;">Match Rate</td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;">Match Rate</td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;">Match Rate</td>
<td style="text-align: center;">RMSE</td>
</tr>
<tr>
<td style="text-align: left;">CDVAE</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">45.31</td>
<td style="text-align: center;">0.1138</td>
<td style="text-align: center;">17.09</td>
<td style="text-align: center;">0.2969</td>
<td style="text-align: center;">33.90</td>
<td style="text-align: center;">0.1045</td>
<td style="text-align: center;">5.34</td>
<td style="text-align: center;">0.2106</td>
</tr>
<tr>
<td style="text-align: left;">CDVAE</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">88.51</td>
<td style="text-align: center;">0.0464</td>
<td style="text-align: center;">88.37</td>
<td style="text-align: center;">0.2286</td>
<td style="text-align: center;">66.95</td>
<td style="text-align: center;">0.1026</td>
<td style="text-align: center;">20.79</td>
<td style="text-align: center;">0.2085</td>
</tr>
<tr>
<td style="text-align: left;">DiffCSP</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\mathbf{5 2 . 0 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 7 6 0}$</td>
<td style="text-align: center;">17.54</td>
<td style="text-align: center;">0.2759</td>
<td style="text-align: center;">51.49</td>
<td style="text-align: center;">0.0631</td>
<td style="text-align: center;">12.19</td>
<td style="text-align: center;">0.1786</td>
</tr>
<tr>
<td style="text-align: left;">DiffCSP</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">$\mathbf{9 8 . 6 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 1 2 8}$</td>
<td style="text-align: center;">$\mathbf{8 8 . 4 7}$</td>
<td style="text-align: center;">0.2192</td>
<td style="text-align: center;">$\mathbf{7 7 . 9 3}$</td>
<td style="text-align: center;">0.0492</td>
<td style="text-align: center;">$\mathbf{3 4 . 0 2}$</td>
<td style="text-align: center;">0.1749</td>
</tr>
<tr>
<td style="text-align: left;">CrystaLLM a</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">47.95</td>
<td style="text-align: center;">0.0966</td>
<td style="text-align: center;">$\mathbf{2 1 . 1 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 6 8 7}$</td>
<td style="text-align: center;">55.85</td>
<td style="text-align: center;">0.0437</td>
<td style="text-align: center;">17.47</td>
<td style="text-align: center;">0.1113</td>
</tr>
<tr>
<td style="text-align: left;">CrystaLLM a</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">98.26</td>
<td style="text-align: center;">0.0236</td>
<td style="text-align: center;">83.60</td>
<td style="text-align: center;">0.1523</td>
<td style="text-align: center;">75.14</td>
<td style="text-align: center;">0.0395</td>
<td style="text-align: center;">32.98</td>
<td style="text-align: center;">0.1197</td>
</tr>
<tr>
<td style="text-align: left;">CrystaLLM b</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">46.10</td>
<td style="text-align: center;">0.0953</td>
<td style="text-align: center;">20.25</td>
<td style="text-align: center;">0.1761</td>
<td style="text-align: center;">$\mathbf{5 8 . 7 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 4 0 8}$</td>
<td style="text-align: center;">$\mathbf{1 9 . 2 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 1 0}$</td>
</tr>
<tr>
<td style="text-align: left;">CrystaLLM b</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">97.60</td>
<td style="text-align: center;">0.0249</td>
<td style="text-align: center;">85.17</td>
<td style="text-align: center;">$\mathbf{0 . 1 5 1 4}$</td>
<td style="text-align: center;">73.97</td>
<td style="text-align: center;">$\mathbf{0 . 0 3 4 9}$</td>
<td style="text-align: center;">33.75</td>
<td style="text-align: center;">$\mathbf{0 . 1 0 5 9}$</td>
</tr>
<tr>
<td style="text-align: left;">CrystaLLM c</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">28.30</td>
<td style="text-align: center;">0.0850</td>
</tr>
<tr>
<td style="text-align: left;">CrystaLLM c</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">47.45</td>
<td style="text-align: center;">0.0780</td>
</tr>
</tbody>
</table>
<p>CrystaLLM outperforms DiffCSP on three out of four benchmarks in terms of RMSE for both $n=20$ and $n=1$, and in terms of match rate when constrained to only a single generation attempt. This is achieved even in the most challenging of the benchmarks, MPTS-52, which contains structures with larger unit cells and more atoms.</p>
<p>CrystaLLM has other important advantages when compared to the other models. Notably, it supports the conditioning of structure generation on specific symmetry space groups, a capability unique to CrystaLLM. The flexibility of its inputs suggests that CrystaLLM may be conditioned on other properties of the structure as well, including those not traditionally included in the CIF format. Moreover, as a large language model, it can leverage the established practice of fine-tuning, allowing the pre-trained model to be adapted for the prediction of materials properties. There is far less precedent in</p>
<p>fine-tuning models based on diffusion and variational autoencoder architectures for tasks involving regression or classification.</p>
<h1>2.4 Examples of Generated Structures</h1>
<p>To further examine the model's ability to generalize to unseen scenarios, we prompted the model with various formulas, and examined its output. The results are presented in Figure 3.</p>
<p>An example of the model generalizing to a formula that had been seen in training, but with different space groups, is presented in Figure 3a. The formula, $\mathrm{Ba}_{2} \mathrm{MnCr}$, was in the held-out test set, with the $R \overline{3} m$ space group. That combination of formula and space group had not been seen in training. The model generated a structure matching the one in the test set on the first attempt, when the space group was provided.</p>
<p>The model also demonstrated the ability to generate plausible structures for formulas not seen in training with any $Z$. An example is the quaternary compound $\mathrm{CsCuTePt}$. This compound was not in the training set, but was in the held-out test set (with $Z=4$ ). The model generated a structure matching the one in the test set, in the $F 43 m$ space group, on the third attempt when the space group was provided. The generated structure is presented in Figure 3b.</p>
<p>Finally, in Figure 3c is the generated structure of $\mathrm{YbMn}<em 6="6">{6} \mathrm{Sn}</em>}$ [42], an example of the model generalizing to structural motifs with atoms not seen in training. This formula was not seen in training for any $Z$, and was not in the held-out test set. However, $\mathrm{ZrMn<em 6="6">{6} \mathrm{Sn}</em>}$ was seen in training, in the $P 6 / \mathrm{mmm}$ space group. The model generated a structure in the same space group on the first attempt, without the space group being provided. The generated structure matched the $\mathrm{ZrMn<em 6="6">{6} \mathrm{Sn}</em>$ structure, with Yb substituted for Zr , and with cell parameters and atomic coordinates adjusted accordingly. This demonstrates the model performing a structure prediction by analogy procedure, as commonly used by materials scientists for discovery [43,44], despite never having been provided with the procedure to do this.</p>
<h3>2.4.1 Rutiles</h3>
<p>Rutiles are a class of binary compounds that adopt a tetragonal unit cell, in the $P 4_{2} / \mathrm{mnm}$ space group $(Z=2)$, as is seen in $\mathrm{TiO}<em 2="2">{2}$, from which this class of materials adopts its name. The general formula for rutile oxides is $\mathrm{MO}</em>$, where M is a metallic species in the +4 oxidation state. Rutile fluorides are also known, where the metal is in the +2 oxidation state.</p>
<p>The model's training dataset consisted of essentially all of the rutiles one might expect to be able to find in nature. Therefore, to test the model's ability to generate unseen rutiles, we requested the generation of theoretically possible, but unlikely compounds, such as $\mathrm{AuO}<em 2="2">{2}$. With gold in a highly unlikely +4 oxidation state, $\mathrm{AuO}</em>$ has cell parameters $a=4.594 \AA, c=2.959 \AA$, the generated rutile gold variant has $a=4.838 \AA$ $c=3.429 \AA$, reflecting the increased volume occupied by the larger gold atoms (Figure 3d).}$ is not expected to be formed under most conditions. However, the model was able to imagine what the structure of such a compound might be (when the space group is provided). While $\mathrm{TiO}_{2</p>
<h1>2.4.2 Spinels</h1>
<p>Spinels are a group of ternary compounds with general formula $\mathrm{AB}<em 4="4">{2} \mathrm{X}</em> m$ space group.}$. The most common combination of elements in spinels is one where A is a cation in the +2 oxidation state, B is a cation in the +3 oxidation state, and X , normally a chalcogen, is a -2 anion. Spinels form cubic close-packed structures, with eight tetrahedral, and four octahedral sites, normally in the $F d \overline{3</p>
<p>To explore the model's ability to generate unseen spinels, we selected the thiospinel $\mathrm{Sm}<em 4="4">{2} \mathrm{BS}</em>$, which was absent from both the training and test sets. The model was able to generate the expected spinel structure when the cell composition and space group were provided (Figure 3e). During training, the model encountered a number of different oxy-, thio-, and selenospinels, and this likely contributed to its ability to generate this compound.</p>
<h3>2.4.3 Elpasolites</h3>
<p>Elpasolites are quaternary compounds with the general formula $\mathrm{ABC}<em 6="6">{2} \mathrm{X}</em> m$ space group, and are the most common quaternary crystal system reported in the Inorganic Crystal Structure Database (ICSD) [45]. We wondered if the CrystaLLM model could generate elpasolites not seen during training.}$. The A and C species are typically alkali metal cations in the +1 oxidation state, B is usually a transition metal cation in the +3 oxidation state, and X is a halogen anion. The elpasolites are often referred to as "double perovskites", since their structures are related to perovskites by the doubling of their unit cell dimensions, and the replacement of the $\mathrm{M}^{2+}$ cation with alternating $\mathrm{M}^{+}$and $\mathrm{M}^{3+}$ cations. Elpasolites crystallize in the $F m \overline{3</p>
<p>We selected an elpasolite from the held-out test, that was not seen in training: the fluoride $\mathrm{KRb}<em 6="6">{2} \mathrm{TiF}</em>$. The model was able to generate the correct elpasolite structure when the cell composition and space group was provided (Figure 3f).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The generated structures of various inorganic compounds. a $\mathrm{Ba}<em 6="6">{2} \mathrm{MnCr}$. Cell parameters: $a, b: 3.778 \AA, c: 27.503 \AA, \alpha, \beta: 90.0^{\circ}, \gamma: 120.0^{\circ}$. Color scheme: Ba: green, Mn: purple, Cr: blue. b CsCuTePt. Cell parameters: $a, b, c: 7.153 \AA, \alpha, \beta, \gamma: 90.0^{\circ}$. Color scheme: Cs: purple, Cu: blue, Te: gold, Pt: white. c $\mathrm{YbMn}</em>} \mathrm{Sn<em 6="6">{6}$. Cell parameters: $a, b: 5.488 \AA, c: 8.832$ $\AA, \alpha, \beta: 90.0^{\circ}, \gamma: 120.0^{\circ} . \mathrm{ZrMn}</em>} \mathrm{Sn<em 2="2">{6}$, in the training set, possessed the same structure, but with the following cell parameters: $a, b: 5.364 \AA, c: 8.933 \AA, \alpha, \beta: 90.0^{\circ}, \gamma: 120.0^{\circ}$. Color scheme: Yb: green, Mn: magenta, Sn: grey. d $\mathrm{AuO}</em>}$. Cell parameters: $a, b: 4.838 \AA, c: 3.429 \AA, \alpha, \beta$, $\gamma: 90.0^{\circ}$. Color scheme: Au: yellow, O: red. e $\mathrm{Sm<em 4="4">{2} \mathrm{BS}</em>}$. Cell parameters: $a, b, c: 10.884 \AA, \alpha$, $\beta, \gamma: 90.0^{\circ}$. Color scheme: Sm: light green, B: green, S: yellow. f $\mathrm{KRb<em 6="6">{2} \mathrm{TiF}</em>}$. Cell parameters: $a, b, c: 8.688 \AA, \alpha, \beta, \gamma: 90.0^{\circ}$. Color scheme: K: white, Rb: purple, Ti: brown, F: green. g $\mathrm{LiTa<em 5="5">{2} \mathrm{NiSe}</em>}$ (a: $3.517 \AA, b: 13.362 \AA, c: 15.156 \AA, Z=4$ ), which resembles the recently reported structure in [46]. h $\mathrm{Ta<em 5="5">{2} \mathrm{NiSe}</em>}$, seen in training. i $\mathrm{NaSn<em 5="5">{2} \mathrm{CuSe}</em>$, seen in training.</p>
<h1>2.4.4 Pyrochlores</h1>
<p>The general formula for the pyrochlores is $\mathrm{A}<em 2="2">{2} \mathrm{~B}</em>)$ for the B species. We investigated whether CrystaLLM could generate valid pyrochlore structures for any unseen combinations, and whether it could estimate reasonable cell parameters in line with the trends observed for the pyrochlore series, as the cell parameters are expected to be correlated with the ionic radii of the A and B cations.} \mathrm{O}_{7}$, where A , a trivalent cation, and B , a tetravalent cation, are either rare-earths or transition metals (other oxidation states, e.g. combining monovalent and pentavalent cations, are also possible, but we focus here on the trivalent/tetravalent pyrochlores). Pyrochlores crystallize in the $F d \overline{3} m$ space group $(Z=8)$. There are many combinations of A and B that are possible for this structure, by using lanthanide ions, actinide ions, and Y (III) for the A species, and various transition metal ions, as well as $\mathrm{Ti}(\mathrm{IV}), \mathrm{Zr}(\mathrm{IV})$, and $\mathrm{Hf}(\mathrm{IV</p>
<p>We created a space of pyrochlores consisting of 144 compounds by producing different combinations of A and B species. Of these, 54 were seen in training. We selected 10 compounds from among the 90 not seen in training, and attempted 3 generations with the model, for each. The cell composition and space group were included in the prompt. All generations resulted in valid pyrochlore structures (Table 5).</p>
<p>Table 5: Values of mean generated cell length for the selected pyrochlores not seen in training, over 3 generation attempts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Formula</th>
<th style="text-align: center;">Cell Length ( $\AA$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathrm{Ce}<em 2="2">{2} \mathrm{Hf}</em>$} \mathrm{O}_{7</td>
<td style="text-align: center;">$10.75 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{Ce}<em 2="2">{2} \mathrm{Mn}</em>$} \mathrm{O}_{7</td>
<td style="text-align: center;">$10.50 \pm 0.22$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{Ce}<em 2="2">{2} \mathrm{~V}</em>$} \mathrm{O}_{7</td>
<td style="text-align: center;">$10.53 \pm 0.09$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{La}<em 2="2">{2} \mathrm{Mn}</em>$} \mathrm{O}_{7</td>
<td style="text-align: center;">$10.21 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{La}<em 2="2">{2} \mathrm{~V}</em>$} \mathrm{O}_{7</td>
<td style="text-align: center;">$10.48 \pm 0.06$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{Lu}<em 2="2">{2} \mathrm{Hf}</em>$} \mathrm{O}_{7</td>
<td style="text-align: center;">$10.30 \pm 0.08$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{Lu}<em 2="2">{2} \mathrm{Zr}</em>$} \mathrm{O}_{7</td>
<td style="text-align: center;">$10.45 \pm 0.12$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{Pr}<em 2="2">{2} \mathrm{Mn}</em>$} \mathrm{O}_{7</td>
<td style="text-align: center;">$10.40 \pm 0.08$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{Pr}<em 2="2">{2} \mathrm{~V}</em>$} \mathrm{O}_{7</td>
<td style="text-align: center;">$10.51 \pm 0.06$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{Pr}<em 2="2">{2} \mathrm{Hf}</em>$} \mathrm{O}_{7</td>
<td style="text-align: center;">$10.80 \pm 0.06$</td>
</tr>
</tbody>
</table>
<p>We subsequently performed DFT relaxation calculations on the first generated structure for each of the 10 compounds. One case, $\mathrm{Ce}<em 2="2">{2} \mathrm{~V}</em>$ of 0.62 and MAE of $0.08 \AA$ being exhibited. This example illustrates CrystaLLM's capability to accurately estimate cell parameters of compounds not seen in training with any structure.} \mathrm{O}_{7}$, posed challenges in calculation under the generalized gradient approximation and was thus excluded from further analysis. The DFT-derived value of the cell parameter for each of the remaining compounds is plotted against the mean value generated by CrystaLLM in Figure 4. A good agreement exists between the DFT-derived and generated cell lengths, with an $R^{2</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The generated vs. DFT-derived value of the cell parameter $a$ for selected pyrochlores not in the training dataset. The error bars represent the $\pm$ standard deviation of the value of the $a$ cell parameter for the three generation attempts (all of which resulted in the pyrochlore structure), while the $y$-coordinate of the points represents the mean value of the cell parameter across the three attempts. The inset represents the structure of the generated pyrochlore $\mathrm{Pr}<em 2="2">{2} \mathrm{Mn}</em>=$ red.} \mathrm{O}_{7}$, with cell parameters $a, b, c: 10.34 \AA, \alpha, \beta, \gamma: 90.0^{\circ}$. Color scheme: $\operatorname{Pr}=$ yellow, $\mathrm{Mn}=$ purple, $\mathrm{O</p>
<h1>2.4.5 Problematic Cases</h1>
<p>While the model seems capable of generating structures for many different classes of inorganic crystals, it does nonetheless have difficulty in certain cases. All of the cases appear to involve systems that are rare, and under-represented in the training dataset, or missing from the training set altogether. More precisely, we define a template as a unique combination of the reduced composition ratio, the space group, and $Z$. For example, the combination of the reduced composition ratio 1:1:3:4, space group $C m c m$, and $Z=4$, represents a unique template. There are 25,921 unique templates in the dataset.</p>
<p>The problematic cases in the challenge set are largely represented by unseen templates, and templates for which there are few examples. For example, validation rates were low for $\mathrm{Mg}<em 4="4">{7} \mathrm{Pt}</em>} \mathrm{Ge<em 3="3">{4}$, the structure of which was reported recently to exist in the $P 6</em> m c$ space group.} m c$ space group $(Z=2)$. [47] In this case, there were only 38 examples of 7:4:4 systems in the training dataset, none contained Mg or Pt , and none were in the $P 6_{3</p>
<p>The small version of the model also seems to struggle with generating phosphates, sulfates, carbonates, and organic-inorganic hybrid structures. Examples include carbonate hydroxide minerals, such as $\mathrm{Co}<em 3="3">{2} \mathrm{CO}</em>)}(\mathrm{OH<em 2="2">{2}$ [48] and $\mathrm{Cu}</em>} \mathrm{CO<em 2="2">{3}(\mathrm{OH})</em>$ (malachite). While present in the dataset, they belong to a group of analogous structures for which there are</p>
<p>only a handful of examples. While both the small and large versions of the model can generate $\mathrm{Mn}<em 4="4">{4}\left(\mathrm{PO}</em>\right)<em 5="5">{3}$, they generally fail to generate a valid structure for $\mathrm{Ca}</em>}\left(\mathrm{PO<em 3="3">{4}\right)</em> m$ space group were seen in training. Finally, structures represented by CIF files with a relatively large number of tokens also pose challenges for the models.}(\mathrm{OH})$ (hydroxyapatite). A common theme is the appearance of multiple oxyanions, which can give rise to more complex arrangements of atoms, for which the model may not have seen enough examples. In contrast, the model can generate compounds of the perovskite class reliably. However, over 5,000 examples of the $\mathrm{ABX}_{3}(\mathrm{X}=\mathrm{O}, \mathrm{F})$ system in the $\mathrm{Pm} \overline{3</p>
<p>Future versions of the model will consider strategies for addressing these occurrences of class imbalance.</p>
<h1>2.5 Heuristic Search for Low-Energy Structures</h1>
<p>The examples generated in the previous section were produced through top- $k$ random sampling of the model. Essentially, as the CIF file is generated, each subsequent token is sampled randomly from amongst the top $k$ tokens, according to their probabilities. (See Supplementary Note 2.3 for a detailed description of top- $k$ sampling.) However, random sampling may not necessarily result in the most desirable sequence, and consequently, there are more strategic approaches for constructing sequences that incorporate the probability distributions produced by the model, along with additional heuristics. An example of a heuristic search is Beam Search [49], which is commonly used in natural language contexts to improve the quality of generated sequences. Another popular heuristic search algorithm is MCTS, which has traditionally been used in the context of planning and games, but has recently also been used to increase the quality of generated natural language, through incorporation with LLMs. [50]</p>
<p>Here, we employ the MCTS algorithm, informed by CrystaLLM, to generate a collection of sequences, which is expected to progressively yield sequences of increasingly higher quality as the search advances. In this implementation, each node in the tree represents a cumulative context of tokens. The algorithm operates through a series of steps, including selection, expansion, rollout, evaluation, and backpropagation. The search tree is constructed iteratively, as the search proceeds (Figure 5). In the selection phase, nodes are chosen using the PUCT algorithm (Predictor-Upper Confidence bound applied to Trees) [51, 52], which is a principled means of obtaining a balance between exploring untried nodes, and exploiting promising nodes. The expansion involves adding child nodes based on predicted probabilities. During the rollout step, the CrystaLLM model is prompted with token sequences until a terminating condition is met, leading to the evaluation of the completed sequence. Evaluation is conducted using the ALIGNN (Atomistic Line Graph Neural Network) model of formation energy per atom [53], while the backpropagation step accumulates outcomes in the tree nodes, scoring each based on the quality of the generated structure. (See Supplementary Note 4 for a more detailed description of the algorithm.) The objective is to produce structures with lower formation energy per atom, $E_{\mathrm{f}}$, and the incorporation of the ALIGNN model allows for a fast and sufficiently accurate estimate of the target property.</p>
<p>When compared to random sampling, MCTS improves the overall validity rate for a compound, and also generally produces lower energy structures. To evaluate the MCTS decoding procedure, we took the 20 most problematic cases of the challenge set where the</p>
<p>validity rate was greater than 0 , and performed 1,000 generation attempts using random top- $k$ sampling, and 1,000 iterations of MCTS. The results are presented in table 6 .</p>
<p>Table 6: Results of MCTS decoding for the 20 most problematic cases of the challenge set. The percentages represent the fraction of cases with the corresponding improvement after using MCTS decoding, when compared to random sampling. The first row represents the percentage of cases where the validity rate improved. The second row represents the percentage of cases where the minimum $E_{\mathrm{f}}$ obtained was improved. The third row represents the percentage of cases where the mean $E_{\mathrm{f}}$ was improved.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">No Space Group</th>
<th style="text-align: center;">With Space Group</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Validity Rate Improvement</td>
<td style="text-align: center;">$95.0 \%$</td>
<td style="text-align: center;">$60.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Minimum $E_{\mathrm{f}}$ Improvement</td>
<td style="text-align: center;">$85.0 \%$</td>
<td style="text-align: center;">$65.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Mean $E_{\mathrm{f}}$ Improvement</td>
<td style="text-align: center;">$70.0 \%$</td>
<td style="text-align: center;">$65.0 \%$</td>
</tr>
</tbody>
</table>
<p>When no space group is provided in the prompt, the validity rate improves in $95 \%$ of the cases, and the minimum $E_{\mathrm{f}}$ attained improves in $85 \%$ of cases. (See Supplementary Tables 6 and 7 for more detailed results.) In some cases, the validity rate increases as the search proceeds when using MCTS (see Supplementary Figure 6).</p>
<h1>2.6 Beyond Element Substitution</h1>
<p>Although CrystaLLM appears to be very effective at finding appropriate template systems for a given cell composition, and making the necessary adjustments of cell parameters to substitute different atoms, it appears capable of going further, synthesizing information from different template systems. An example is the selenide $\mathrm{LiTa}<em 5="5">{2} \mathrm{NiSe}</em>}$, which is obtained by lithium intercalation into $\mathrm{Ta<em 5="5">{2} \mathrm{NiSe}</em>$ [46].</p>
<p>The compound $\mathrm{LiTa}<em 5="5">{2} \mathrm{NiSe}</em>}$ was not present in the training set, however, the layered material $\mathrm{Ta<em 5="5">{2} \mathrm{NiSe}</em>}$ was (Figure 3 g,h). As $\mathrm{LiTa<em 5="5">{2} \mathrm{NiSe}</em>}$ was included in the challenge set, we performed 100 generation attempts with the model. While the model was not able to recover the lowest energy structure reported, it did produce structures with close resemblance to low-energy polymorphs. Upon closer examination of the dataset, we found that $\mathrm{NaSn<em 5="5">{2} \mathrm{CuSe}</em>$ was present (Figure 3i), which likely provided some precedent for the intercalation of atoms between layered structures. It thus appears that the model is capable of integrating information from different template systems to form new structural predictions.</p>
<h3>2.7 The CrystaLLM.com Web Application</h3>
<p>To allow for easy and open access to the CrystaLLM model, we make it available through a web application, published at https://crystallm.com. The application allows users to enter in a reduced formula, and optionally a value for $Z$ and the desired space group. The option to select the model size is also provided. The request is sent to the model, and the resulting structure (or the CIF contents, if the structure is invalid) is presented to the user. By making the model easily accessible, we hope to contribute a potentially useful tool to the materials structure research community. We also hope to receive feedback from users that may help improve future versions of the model.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Schematic depiction of the Monte Carlo Tree Search decoding procedure. CIF files are generated as a tree is iteratively constructed, with each iteration guiding the generation of subsequent structures towards more desirable parameters (e.g. lower formation energy per atom). The nodes in the tree represent the cumulative contents of a CIF file at various points. a The Selection step involves descending the tree by choosing the most promising node at each level, using a variant of the PUCT algorithm. b During Expansion, an unexplored child node is randomly selected and added to the tree. If a node has only one highly probable child (represented as empty nodes), the child node bypasses the Rollout step. c The Rollout step involves prompting the model with the contents of the selected node, and sampling from the model until a terminal condition is met, so as to obtain a complete CIF file and an estimate of the value of a node. d The generated structure is validated and scored, incorporating the prediction of the structure's formation energy per atom, as given by a pre-trained neural network. e Finally, the score is backpropagated through the selected nodes, which store the accumulated results of each iteration. The resulting generated CIF file, if valid, is returned.</p>
<h1>3 Discussion</h1>
<p>Here, we have shown that LLMs of the CIF format are able to generate inorganic crystal structures for a variety of known classes. Indeed, the model is able to produce valid and sensible arrangements of atoms in 3-dimensional space by generating $x y z$ coordinates digit-by-digit. The model also seems to have captured the relationship between space group symbols and the symmetries inherent in the structures it generates.</p>
<p>We chose to build a language model of the CIF format (instead of a simplified format, for example, which might include a minimal vocabulary) for several reasons. First, the CIF format is not particularly verbose. The model learns the grammatical structure of the format fairly quickly. We can thus avoid having to devise an intermediate format that requires inter-conversion between more common formats, which could also be error prone. Second, we believe that having the model learn to generate the more redundant parts of the CIF format, such as the cell volume, and $Z$, which are inferable from prior inputs, helps the model to perform better overall.</p>
<p>A number of approaches for crystal structure generation have been reported. [54-57] These approaches generally require the existence of pre-defined structural templates, and are followed by the procedural or machine learning-assisted substitution of atoms and adjustment of cell parameters, under the constraint of a specified space group. These types of approaches can also be enhanced to increase the structural diversity of generated materials, by allowing partial substitutions and adjusting substitution probabilities [58]. Conversely, CrystaLLM automatically selects the templates which can be applied to a given composition, utilizing the implicit templates it has absorbed through autoregressive training. Moreover, the model can automatically adjust cell parameters to accommodate the atoms in the unit cell. It can also produce structures based on templates it has not explicitly encountered in training, borrowing from its internalized concepts of chemical structure. In comparison with recently reported diffusion-based ML methods for crystal generation (CDVAE [13] and DiffCSP [36]), not only does CrystaLLM outperform them on established benchmarks in several aspects, but it also offers additional advantages in terms of flexibility (e.g. in using symmetry as input) and the potential for fine-tuning.</p>
<p>While the CrystaLLM model can generate sensible structures, this does not by itself make it suitable, as is, for CSP. Just as natural language LLMs, such as GPT-3 and -4 , are not suitable chatbots without further fine-tuning and alignment, the CrystaLLM model will also need to be fine-tuned for more advanced tasks. Fine-tuning involves an additional and separate training step, where the model's parameters are adjusted in the context of a different task. This may also involve altering the model's output layer, such as to make it suitable for a regression task. Models can be fine-tuned using a variety of techniques, but supervised learning and reinforcement learning [59] are most common. One might use reinforcement learning, for example, when a task is not clearly defined as a supervised learning problem. When fine-tuning natural language LLMs for chatbot applications, it is common to use Reinforcement Learning from Human Feedback (RLHF) [60,61]. With RLHF, the idea is to gather data from human annotators to be used to train a reward model, which scores generated text according to its desirability. The reward model is then used as part of a reinforcement learning-based tuning of the LLM. In CSP, one would like to produce ground-state structures (for some given physical conditions). One could thus imagine an analogous procedure where CrystaLLM is fine-</p>
<p>tuned for the goal of generating low-energy structures, via feedback from an external evaluator of the generated structure's energy, resulting in Reinforcement Learning from Thermodynamic Feedback. This procedure would also require a reward model, and such a model should ideally provide a timely estimate of a structure's energy. This excludes time-consuming approaches such as DFT. A viable approach could make use of a separate machine learning-based model of formation energy, such as one based on ALIGNN. Indeed, neural network potentials have been used to accelerate the prediction of crystal structures, and the identification of potentially stable materials. 62,63</p>
<p>There are several limitations with the current approach. First, none of the structures of the dataset have site-occupancy disorder (fractional site occupancies). Therefore, CrystaLLM cannot generate disordered structures, and may not successfully generate structures for combinations of cell composition and space group that imply a disordered structure. An example is $\mathrm{K}<em 5="5">{2} \mathrm{NaTiOF}</em>$, which is reported to be an elpasolite, in the $F m 3 m$ space group $(Z=4)$, with F and O species sharing the same crystal site [64]. Another limitation is that the CIF files of the dataset were not all created using the same level of theory. The training set is derived from a combination of DFT sources using different settings, functionals, etc., which may make it difficult for the model, in some instances, to learn a consistent relationship between cell composition and detailed structure 65.</p>
<p>Nevertheless, we believe that CrystaLLM will be a useful tool for crystal structure generation, which is quickly becoming a critical step in large scale materials discovery [58,66], and materials informatics. We plan to explore fine-tuning the model for physical property prediction tasks, such as the prediction of lattice thermal conductivity, where experimental data is relatively scarce. 67 The architecture of the model allows it to be fine-tuned for either composition-based or structure-based prediction tasks. This implies that CrystaLLM may be the basis for a general-purpose materials informatics model, which can be used for generative tasks, and fine-tuned for property prediction tasks that require either composition or structure. If the model is able to transfer what it has learned about the world of atoms to these various predictive problems, it may prove to be a quite flexible tool relevant to many aspects of materials chemistry.</p>
<h1>4 Methods</h1>
<h3>4.1 Dataset Curation</h3>
<p>The dataset was assembled by obtaining structures from the Materials Project [40], the OQMD [68], and NOMAD [69], which were originally optimized using density functional theory (DFT) simulations. Specifically, the structures from the Materials Project were downloaded in April 2022, and from NOMAD in April 2023. We use version 1.5 of the OQMD, which was released in October 2021. In total, approximately 3.6 million structures were obtained. This dataset consists of compounds containing anywhere from 1 to 10 elements, with most consisting of 3 or 4 elements. The elements up to and including atomic number 94 are present, with the exception of polonium, astatine, radon, francium, and radium. The dataset contains roughly 800,000 unique formulas, and 1.2 million unique cell compositions. When paired with space groups, there are 2.3 million unique cell composition-space group pairs. (See Supplementary Figure 1.) To choose between duplicate structures containing the same cell composition and space group, the</p>
<p>structure with the lowest volume per formula unit was selected. The 2.3 million structures in this dataset were converted to CIF files using the pymatgen library [70], and were used for training. The CIF files were created with the pymatgen option for symmetry finding tolerance set to $0.1 \AA$. All floating point numbers in the files were rounded to 4 decimal places. The dataset was split randomly into train, validation, and test sets, such that the training set consisted of 2,047,889 CIF files, the validation set 227,544 CIF files, and the test set 10,286 CIF files.</p>
<h1>4.2 CIF Syntax Standardization and Tokenization</h1>
<p>The dataset of CIF files was standardized and tokenized prior to training. The vocabulary consisted of CIF tags, space group symbols, element symbols, numeric digits, and various punctuation symbols, for a total of 371 symbols. After tokenization, the training set consisted of 768 million tokens. See Supplementary Note 1 for further details.</p>
<h3>4.3 Generative Pre-training</h3>
<p>The generative pre-training step requires a vocabulary, $\mathcal{V}$, and an ordered list of tokens $\mathcal{U}=\left(u_{1}, \ldots, u_{n}\right)$, with $u_{i} \in \mathcal{V}$. We want to maximize the following likelihood:</p>
<p>$$
\mathcal{L}(\theta ; \mathcal{U})=\sum_{i} \log P\left(u_{i} \mid u_{i-c}, \ldots, u_{i-1} ; \theta\right)
$$</p>
<p>where $c$ is the size of a context window, $P$ is the conditional probability distribution to be modelled, and $\theta$ the parameters of a neural network. We therefore minimize $\mathcal{J}(\theta ; \mathcal{U})=$ $-\mathcal{L}$, using stochastic gradient descent to adjust the parameters. We use a multi-layer Transformer decoder [71] for the neural network, as described in [18]. Our model consists of 25 million parameters, with 8 layers, 8 attention heads, and an embedding size of 512 . We decay the learning rate from $10^{-3}$ to $10^{-4}$ over the course of training, and use a batch size of 32. For further details, see Supplementary Note 2.</p>
<h3>4.4 Evaluation of Generated Structures</h3>
<p>A CIF file is said to be valid if: 1) the declared space group is consistent with the generated structure, 2) the generated bond lengths are reasonable, and 3) the declared atom site multiplicity is consistent with the cell composition. To check if the generated structure is consistent with the printed space group, we use the SpacegroupAnalyzer class of the pymatgen library, which uses the spglib library [72]. To check if bond lengths are reasonable, we first use a Voronoi-based nearest-neighbour algorithm in pymatgen to identify bonded atoms; then, we establish expected bond lengths based on the electronegativity difference between the bonded atoms, and their ionic or covalent radii. We classify a structure as having reasonable bond lengths if all the detected bond lengths are within $30 \%$ of the corresponding expected bond lengths. See Supplementary Note 3 for more details on how the validity of a generated CIF file is established.</p>
<p>In some scenarios, we wish to determine whether a generated structure matches a target structure, which typically represents a ground-truth structure. To determine whether two structures are a match, we use the pymatgen StructureMatcher class, which</p>
<p>performs a structural similarity assessment of two crystals. We use a fractional length tolerance of 0.2 , a site tolerance of $0.3 \AA$, and an angle tolerance of 5 degrees, which are the default values in pymatgen. Both structures are reduced to primitive cells before matching, and are scaled to equivalent volume.</p>
<h1>4.5 Benchmark Evaluations</h1>
<p>To evaluate CrystaLLM on the Perov-5, Carbon-24, MP-20 and MPTS-52 benchmarks, we consider two different scenarios: 1) the model is trained only on the benchmark training sets, and 2) the model is trained on the full 2.3 million-structure dataset minus the validaton and test set structures of the MPTS-52 dataset. For the first scenario, both the small and large model architectures are used. We use the same 60-20-20 train/validation/test splits used in the CDVAE study [13] for the Perov-5, Carbon-24, and MP-20 datasets, and we use the same $27,380 / 5,000 / 8,096$ train/validation/test split used in the DiffCSP study for the MPTS-52 dataset. These models are trained for a fixed number of iterations: the Perov-5 model is trained for 1,750 iterations, the Carbon-24 model is trained for 8,000 iterations, the MP-20 model is trained for 5,000 iterations, and the MPTS-52 model is trained for 3,500 iterations. For the second scenario, we train a model with the small model architecture on the full 2.3 million-structure dataset minus the structures of the MPTS-52 validation and test sets. The model is trained for 100,000 iterations. We decay the learning rate from $10^{-3}$ to $10^{-4}$ over the course of training, and use a batch size of 32 , for all models. For both scenarios, we take the structures of the test set(s), and prompt the models with only the cell compositions of these structures. Models are given 20 attempts to generate a structure. We use top- $k$ sampling with $k=10$ and a temperature of 1.0 for all models and in both scenarios.</p>
<p>To establish the match rate and RMSE, we use the same procedure defined in the DiffCSP study. Specifically, we use the pymatgen StructureMatcher class, with a fractional length tolerance of 0.3 , a site tolerance of $0.5 \AA$, and an angle tolerance of 10 degrees, to determine if a generation attempt matches the ground truth structure. The RMSE, normalized by $\sqrt[3]{V / N}$ (where V is the volume of the lattice and N is the number of sites), is computed between the corresponding ground truth structure and each matching generated structure. The test set's average RMSE is computed by taking the lowest RMSE for each entry's matching generated structure.</p>
<h3>4.6 Monte Carlo Tree Search Decoding</h3>
<p>The MCTS search tree is constructed iteratively, as the search proceeds. We maintain a tree width of 5 , and maximum tree depth of 1,000 . The PUCT constant $c_{\text {puct }}$ is set at 1.0. The expansion involves adding child nodes based on predicted probabilities. When a node has a probability of 0.99 or greater, it becomes the only child node, and bypasses the rollout step. During the rollout step, the CrystaLLM model is prompted with token sequences until a terminating condition is met, up to a maximum of 1,000 tokens. Evaluation is conducted using the ALIGNN model of formation energy per atom. The ALIGNN model is given the generated CIF file, and the predicted formation energy per atom (in eV ) is used to compute the reward. The backpropagation step accumulates outcomes in the tree nodes, scoring each based on the quality of the generated structure,</p>            </div>
        </div>

    </div>
</body>
</html>
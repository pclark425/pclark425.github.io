<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1229 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1229</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1229</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-270559382</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.10667v1.pdf" target="_blank">UniZero: Generalized and Efficient Planning with Scalable Latent World Models</a></p>
                <p><strong>Paper Abstract:</strong> Learning predictive world models is essential for enhancing the planning capabilities of reinforcement learning agents. Notably, the MuZero-style algorithms, based on the value equivalence principle and Monte Carlo Tree Search (MCTS), have achieved superhuman performance in various domains. However, in environments that require capturing long-term dependencies, MuZero's performance deteriorates rapidly. We identify that this is partially due to the \textit{entanglement} of latent representations with historical information, which results in incompatibility with the auxiliary self-supervised state regularization. To overcome this limitation, we present \textit{UniZero}, a novel approach that \textit{disentangles} latent states from implicit latent history using a transformer-based latent world model. By concurrently predicting latent dynamics and decision-oriented quantities conditioned on the learned latent history, UniZero enables joint optimization of the long-horizon world model and policy, facilitating broader and more efficient planning in latent space. We demonstrate that UniZero, even with single-frame inputs, matches or surpasses the performance of MuZero-style algorithms on the Atari 100k benchmark. Furthermore, it significantly outperforms prior baselines in benchmarks that require long-term memory. Lastly, we validate the effectiveness and scalability of our design choices through extensive ablation studies, visual analyses, and multi-task learning results. The code is available at \textcolor{magenta}{https://github.com/opendilab/LightZero}.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1229.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1229.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UniZero: Generalized and Efficient Planning with Scalable Latent World Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based latent world model that disentangles per-timestep latent states from implicit latent history, uses a KV cache for backward memory, and jointly optimizes model and policy with MCTS in latent space to improve long-term planning and sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UniZero transformer latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A latent-space predictive model with (1) an encoder that maps observations to per-step latent states z_t, (2) a transformer backbone that learns an implicit latent history h_t using sequences of (latent, action) tokens, (3) a dynamics head that predicts next latent ẑ_{t+1} and reward conditioned on z_≤t and a_≤t, and (4) a decision head that predicts policy logits and value conditioned on z_≤t and a_≤t−1. The model uses a KV cache at inference to preserve recent context and performs MCTS in the predicted latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (transformer-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (Atari 100k benchmark), VisualMatch long-term memory benchmark, multi-task Atari experiments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Next-latent L2 (||ẑ_{t+1} − stopgrad(h(o_{t+1}))||^2) for latent prediction; cross-entropy for reward/policy/value (distributional bins for reward/value); auxiliary SSL losses (contrastive L2 or negative cosine similarity in other variants) referenced for alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No scalar MSE reported; empirical fidelity evidenced by downstream task success — UniZero matches or surpasses MuZero-style baselines on Atari 100k (single-frame input) and maintains high success rates on VisualMatch as memory length increases; trained 100k Atari steps in ≈4 hours and 1M VisualMatch steps in ≈30 hours on a single NVIDIA A100 80GB instance.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable via visualization: attention maps and predicted quantities are inspected; attention shows focus on initial target timestep and recent timesteps in VisualMatch, and mainly recent frames in Pong; model predictions (predicted rewards, prior policy) and MCTS-improved policies are visualized to assess behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of attention maps from transformer layers, visual comparison of predicted vs. true rewards and prior vs. MCTS policy distributions, trajectory-level prediction visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Experiments ran on a single NVIDIA A100 80G GPU with 24 CPU cores and ~100GB RAM; training time ~4 hours for 100k Atari steps and ~30 hours for 1M VisualMatch steps (memory_length=500). Latent dim: 768 for Atari (64 for VisualMatch); MCTS simulations per decision sim=50; transformer backbone based on nanoGPT architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Qualitatively more sample-efficient on POMDP-like Atari tasks than MuZero-style baselines (matches/surpasses MuZero w/ SSL with single-frame input in 17/26 games); preserves long-term context with KV cache unlike recurrent/MuZero variants, enabling better long-horizon planning with comparable compute budget.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On Atari 100k benchmark UniZero (stack=1) outperforms a reimplementation of MuZero w/ SSL (stack=4) in 17/26 games and is comparable in the remainder; on VisualMatch (long-term memory) UniZero maintains high success rates as memory length increases while MuZero and SAC-GPT degrade.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Design prioritizes decision-relevant latent features over reconstructive fidelity; omitting observation reconstruction did not harm task performance, indicating that high fidelity in pixel reconstruction is unnecessary for policy/value learning when latent predictions and policy alignment are enforced; KV cache + transformer attention yields task-relevant long-term memory leading to improved policies on POMDP-like tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Disentangling latent state from latent history and using transformer attention increases representational capacity for long-term dependencies but increases model complexity vs simple recurrent alternatives; omitting decoder reduces compute and complexity but forgoes reconstruction-based fidelity checks; SimNorm is required for stable training (normalization trade-off).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Transformer backbone (nanoGPT-style), SimNorm simplicial normalization on latent states, large latent dimension (768 for Atari), joint model-policy training (value-equivalence style losses), soft target world model (EMA), KV cache for inference context, no decoder / no reconstruction loss, use of distributional cross-entropy for reward/value, MCTS with 50 simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to MuZero-style (recursive dynamics + encoder-only root) UniZero explicitly disentangles history and state and uses full trajectory during training, improving long-term dependency performance; compared to two-stage world-model approaches (Dreamer/TD-MPC families) UniZero performs joint model-policy optimization and integrates MCTS for policy improvement, offering improved planning utility for tasks with long horizons; compared to RNN backbones (UniZero RNN variant), transformer + KV cache retained fuller context and outperformed recurrent variants.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Empirical recommendations: apply SimNorm on latents, use soft target world model (EMA) for stability, set train context length to match long-horizon requirements (e.g., VisualMatch needs longer), set inference context shorter for many Atari tasks (Hinfer=4 often sufficient), moderate transformer depth improves performance, avoid decoder/reconstruction for decision tasks, rely on KV caching to maintain recent context during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniZero: Generalized and Efficient Planning with Scalable Latent World Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1229.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1229.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero (value-equivalent latent MCTS model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A joint model-policy latent MCTS algorithm with an encoder, dynamics network (recursive unrolls), and prediction network that plans with MCTS in learned latent space and uses a value-equivalence training objective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari, go, chess and shogi by planning with a learned model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero-style latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder maps observation(s) to a root latent state s^0_t; dynamics network recursively predicts s^k and rewards rk given previous latent and action; prediction network outputs policy and value from latent states; planning via MCTS in latent space using the encoder-produced root.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recursive/deterministic dynamics network + MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Board games, Atari games, other discrete-control domains (used here as baseline on Atari 100k and VisualMatch)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Alignment between observation embeddings z_{t+k} and dynamics predictions s^k_t measured by L2 contrastive loss l_z or self-supervised consistency (negative cosine) in variants; task return scores as downstream fidelity proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Strong in short-term-memory MDP-like tasks; when used with SSL and stacked frames (stack=4) shows good sample efficiency on MDP-like Atari tasks; performance degrades rapidly in POMDP/long-term dependency tasks (e.g., fails to converge on Pong stack=1 with SSL within 500k steps in this reimplementation).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent representations can be visualized but are entangled with historical information making SSL alignment problematic; limited interpretability described (visualization prior work referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Observation-embedding vs dynamics prediction alignment losses, visualization of latent representations referenced from prior works; no transformer attention-based analyses since backbone is recurrent/unrolled dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Standard MuZero-style compute depends on encoder/dynamics/prediction sizes and MCTS sims; in this paper MuZero variants were reimplemented using LightZero with similar hyperparameters (reanalyze_ratio=0) — exact wall-clock not specified here for baseline runs.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>MuZero w/ SSL (stack=4) is sample-efficient for MDP tasks but under-utilizes trajectory data and its recursively unrolled latents entangle history, leading to worse performance in POMDP-like tasks vs UniZero; MuZero w/ context recovers some history but is harmed by compounding prediction errors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Good on short-horizon MDP tasks and many Atari games when given stacked frames; poor on long-horizon partial-observation tasks (VisualMatch) and fails to maintain performance when input is single-frame POMDP-like scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>MuZero's dynamics predictions and joint training yield high utility when history requirement is short or when stacked frames supply required context; however, entanglement of historical info in latent unrolls prevents effective self-supervised regularization and harms long-term memory tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Recurrence-based latent unrolls simplify architecture but cause entanglement with history and compounding errors for long-horizon predictions; using SSL improves sample efficiency in MDPs but can make predicted latents depend excessively on single observation, harming POMDP performance.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Encoder + recursive dynamics + prediction heads, MCTS in latent space, optionally SSL contrastive or SimSiam-style consistency losses, uses initial observation (possibly stacked frames) as root during training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared with UniZero, MuZero under-utilizes trajectory data during training and entangles latent states with history, causing worse performance in long-term dependency tasks; compared to two-stage methods (Dreamer, TD-MPC) MuZero jointly trains model and policy and leverages MCTS for policy improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper argues MuZero-style works best when tasks require primarily short-term memory or when input supplies stacked frames; adding SSL helps for MDPs but is insufficient for POMDPs without architecture changes to disentangle history.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniZero: Generalized and Efficient Planning with Scalable Latent World Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1229.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1229.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero w/ SSL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero with self-supervised latent regularization (SSL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A MuZero variant augmented with self-supervised consistency or contrastive regularization between predicted dynamics latents and encoder observation embeddings to stabilize latents and improve sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero + SSL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same three-network MuZero architecture but adds an auxiliary regularization loss (contrastive L2 l_z = sum_k ||z_{k+1} − stopgrad(h(s_{k+1}))||^2 or SimSiam-style negative cosine) to align predicted latents with encoder embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model with auxiliary self-supervised regularization</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (Atari 100k), used as strong baseline</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>L2 latent alignment loss (l_z) or negative cosine similarity used as auxiliary metric; downstream returns as evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Improves sample efficiency in MDP-like Atari tasks (stack=4) but fails to converge in single-frame POMDP-like Pong within 500k steps in the reported experiments due to entanglement issues.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Improved alignment between encoder embeddings and dynamics predictions makes latents more consistent; still suffers from entanglement making history separation unclear.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Auxiliary latent alignment losses provide an implicit sanity check; prior visualization literature cited but no new specialized interpretability beyond UniZero analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Similar to MuZero baseline with small additional cost for computing auxiliary SSL losses; exact overhead not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More sample-efficient than vanilla MuZero in MDP tasks; less effective than UniZero in POMDP/long-term tasks despite SSL.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Strong on many Atari tasks with stacked frames; poor on single-frame POMDP tasks (e.g., Pong stack=1) where SSL harms convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>SSL supplies richer training signals than sparse rewards and stabilizes latent learning in short-term tasks, but when latent dynamics are entangled with history SSL can encourage dependence on single-frame observations, reducing memory utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>SSL increases stability/sample efficiency for MDP tasks but can reduce the model's ability to encode long-term history when entanglement exists; auxiliary loss imposes additional compute and potential bias toward single-step observability.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Contrastive L2 or SimSiam negative cosine SSL on predicted latents vs encoder embeddings, applied across unrolled steps; used with standard MuZero networks and MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Performs better than plain MuZero in MDP-like settings; UniZero's transformer-based disentanglement combined with SSL-like alignment performs better in POMDP/long-horizon settings.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Works well with stacked-frame inputs in MDP tasks; not recommended alone for POMDPs without architectural changes to separate latent state from history.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniZero: Generalized and Efficient Planning with Scalable Latent World Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1229.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1229.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniZero (RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UniZero variant with GRU backbone (UniZero RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of UniZero replacing the transformer backbone with a GRU; trained with same data usage but suffers from limited memory length and compound prediction errors at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UniZero (RNN backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same encoder/dynamics/decision heads but using a GRU as the sequence backbone; during inference GRU hidden state is reset every Hinfer steps and recursively predicted hidden states are used as root/internal nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (RNN-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari (ablation) and VisualMatch (discussion) — used as ablation variant</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Downstream task returns and qualitative trajectory prediction accuracy; no scalar MSE given.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Performs worse than transformer UniZero in long-term dependency tasks due to limited memory and compounding errors; reported as poor and not considered a primary baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Less amenable to long-range attention-based interpretability; errors accumulate in recursively predicted hidden states making analysis of correct memory use harder.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Trajectory visualizations and qualitative failure analysis (resetting hidden states every Hinfer causes incomplete context).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Lower theoretical overhead than transformer but required frequent resets and still suffered in performance; exact wall-clock not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Computationally simpler than transformer's KV cache but less effective per compute for long-horizon tasks due to inability to maintain complete context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Failed to match UniZero transformer performance; experienced incomplete context and poor learning in long-term POMDP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>RNN backbone limits backward memory and suffers compounding prediction errors at inference, reducing its utility for long-term planning despite simpler compute.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>RNNs offer parameter/compute efficiency but sacrifice long-horizon memory fidelity and robustness; transformer+KV cache preferred for long dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>GRU backbone, hidden-state resets every Hinfer steps, same decision/dynamics heads as UniZero.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Performs worse than transformer UniZero in POMDP tasks; better than naive MuZero w/ context in some setups? (paper reports generally inferior results).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not recommended for long-horizon, partial-observation tasks according to empirical findings in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniZero: Generalized and Efficient Planning with Scalable Latent World Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1229.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1229.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EfficientZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EfficientZero (MuZero-style with enhanced sample efficiency)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A MuZero-derived algorithm incorporating self-supervised consistency (SimSiam-style) and other sample-efficiency enhancements; used here as a competitive baseline on Atari 100k.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EfficientZero-style latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MuZero-style encoder/dynamics/prediction architecture augmented with self-supervised consistency losses, and sometimes recurrent modules (LSTM) for value_prefix prediction in some implementations; focuses on sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (MuZero variant with SSL)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari 100k benchmark (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Auxiliary self-supervised consistency loss (negative cosine) between predicted and observed latents; downstream returns.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Competitive on Atari 100k with stacked frames; used as a high-performing baseline (stack=4).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not specifically analyzed in this paper beyond being a baseline; prior works have latent visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Self-supervised consistency acts as implicit alignment; no attention-map analyses since not transformer-based in baseline implementation here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Similar scale to MuZero w/ SSL; specific run-times not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Sample-efficient on MDP-like Atari tasks; UniZero matches or exceeds its performance in many games even with single-frame input.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Strong performance on MDP-like Atari tasks (stack=4); UniZero (stack=1) matches or outperforms it in many games.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Effective when short-term context suffices; SSL improves sample efficiency by providing richer supervision beyond scalar rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Additional SSL bookkeeping and potential reliance on stacked frames limits applicability on POMDP long-horizon tasks compared to UniZero.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>MuZero-like architecture with SimSiam-style SSL; in some versions uses LSTM for value prefix prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to UniZero, EfficientZero offers similar or better sample efficiency in MDP tasks but is outperformed on long-term dependency tasks requiring explicit history disentanglement.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Best used with stacked-frame inputs and tasks without long-term partial observability; hyperparameters tuned for sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniZero: Generalized and Efficient Planning with Scalable Latent World Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1229.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1229.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAC-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SAC-Discrete with GPT backbone (SAC-GPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-free actor-critic (SAC-Discrete) variant that uses a GPT-style transformer backbone to provide sequence modeling for decision making; used as a baseline for long-term memory tasks (VisualMatch) in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SAC-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SAC-Discrete algorithm for discrete actions combining a GPT-like transformer sequence model as the policy/value backbone to handle sequential dependencies in partially observable tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>model-free RL with transformer sequence backbone</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>VisualMatch long-term dependency benchmark (grid-world POMDP)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Downstream success rate on VisualMatch as memory_length increases; training steps (SAC-GPT reported baseline after 3M env steps).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Performance degrades significantly as memory length increases in VisualMatch; baseline final success rate (plotted green dashed line) achieved after training on 3M environment steps — numeric success rates not tabulated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not deeply analyzed here; transformer backbone allows sequence attention but no attention visualizations provided for SAC-GPT in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not reported in this paper for SAC-GPT.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>SAC-GPT is trained up to 3M environment steps in referenced baseline results; exact hardware/time not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>UniZero outperforms SAC-GPT on VisualMatch for large memory lengths despite SAC-GPT being transformer-based, indicating that combining forward MCTS + backward KV memory is more effective for long-horizon sparse-reward POMDPs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>SAC-GPT achieves reasonable performance at shorter memory lengths but declines as memory_length grows; UniZero maintains high success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Transformer backbone alone (SAC-GPT) helps, but without model-based planning and explicit world-model predictions plus KV cache, model-free transformer approaches struggle with very long memory and sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Model-free transformer approaches may require far more environment steps (training data) to reach comparable performance on long-horizon sparse-reward tasks compared to model-based UniZero.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>GPT backbone for sequence modeling combined with SAC-Discrete for policy learning; no latent world-model MCTS planning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>SAC-GPT underperforms UniZero on VisualMatch long-memory tasks; performs better than MuZero (stack=1) in some regimes but deteriorates as memory grows.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Requires orders of magnitude more environment steps (e.g., millions) to approach top performance on long-memory sparse reward tasks; not optimal for sample-limited regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniZero: Generalized and Efficient Planning with Scalable Latent World Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1229.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1229.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer (series)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer / DreamerV3 family (latent imagination-based RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of world-model methods that learn latent-space dynamics with an encoder/decoder (VAE-like) and use latent rollout imagination to train actor-critic policies, typically in a two-stage or imagination-augmented training pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari with discrete world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer-style latent world models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent-space models using (categorical/discrete) VAE encoders to produce compact latent codes and a dynamics model for latent imagination rollouts; policies/value are learned with actor-critic methods using imagined trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (VAE-based, imagination rollouts)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari, continuous control, general RL benchmarks (referenced comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss (VAE), prediction accuracy in latent rollout; downstream returns.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>High sample-efficiency on many domains in prior work; provided here as contextual baseline (no new numbers in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent codes can sometimes be inspected and decoded; Dreamer typically uses decoders enabling reconstruction-based interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Reconstruction and latent visualization used in prior Dreamer works (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Two-stage or imagination-heavy training with additional decoder costs; reported elsewhere, not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Dreamer family is a strong model-based baseline but follows two-stage pipeline; UniZero contrasts by joint model-policy training with MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Referenced as baseline family; not directly re-evaluated in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Dreamer-style methods emphasize accurate latent reconstructions and imagination for policy learning; useful when reconstruction fidelity correlates with decision features but can be less direct than UniZero's decision-focused latents.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Two-stage training and reconstruction-heavy objectives can increase fidelity to observations at cost of decision-relevance and joint optimization benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>VAE/categorical VAE encoders, latent rollout imagination, actor-critic policy learning often in separate stages.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted in paper as alternate route (two-stage) vs MuZero-style joint optimization; UniZero aims to combine scalable transformer architectures with joint training benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniZero: Generalized and Efficient Planning with Scalable Latent World Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1229.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1229.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TD-MPC (TD-MPC2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TD-MPC series (TD-MPC2: Scalable, robust world models for continuous control)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of model-based control methods using learned latent models and model predictive control to derive policies, typically in a two-stage training paradigm emphasizing robustness and scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Td-mpc2: Scalable, robust world models for continuous control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TD-MPC family</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent world models trained to support local trajectory optimization (MPC) in latent space; often uses deterministic/stochastic latent dynamics with value/policy heads used for control.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (MPC-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continuous control tasks (cited for reference), scalability comparisons in related work</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Prediction error and control performance (task returns); not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Referenced as showing substantial gains in large-scale tasks in prior literature; not re-evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not explicitly discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>MPC loops are computationally intensive in planning; paper contrasts approach but does not provide direct cost numbers here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>TD-MPC uses two-stage training and MPC for policy derivation; UniZero focuses on joint training + MCTS and transformer-based memory for long-horizon POMDPs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not evaluated in this paper; referenced as an alternative route for world-model design.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>TD-MPC effective for continuous control, but differs in training pipeline from UniZero's joint optimization and discrete-action MCTS focus.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>MPC methods offer strong local trajectory optimization but can be costly and require accurate short-horizon models; UniZero's MCTS + KV cache targets long-horizon partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Latent dynamics trained for MPC objectives; two-stage pipelines common.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniZero: Generalized and Efficient Planning with Scalable Latent World Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1229.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1229.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early framework proposing learning compact spatio-temporal latent representations to enable planning and policy learning in latent space, influencing later latent-world-model approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>World models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models (original)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder + latent dynamics + controller paradigm that learns a compressed representation of environment dynamics (often VAE or RNN-based) and uses it for policy learning and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (VAE/RNN-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Simulated control tasks, image-based RL benchmarks; cited historically</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss and simulation fidelity; downstream policy performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Historical contribution; not benchmarked in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Reconstruction-based latent interpretability possible through decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Decoder visualizations and latent traversals in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Varies with implementation; cited for conceptual contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Foundational approach influencing later architectures; UniZero differentiates by not using decoder/reconstruction and using transformer-based history handling.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Demonstrated that latent predictive models could be useful for policy learning; UniZero builds on the idea but emphasizes disentangling history and decision-focused latents.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Reconstruction emphasis can impose extra objectives not directly tied to decision utility; UniZero avoids decoding for efficiency and task utility.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of VAE-style encoders and RNN dynamics in original formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniZero: Generalized and Efficient Planning with Scalable Latent World Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mastering atari, go, chess and shogi by planning with a learned model <em>(Rating: 2)</em></li>
                <li>Mastering atari with discrete world models <em>(Rating: 2)</em></li>
                <li>Td-mpc2: Scalable, robust world models for continuous control <em>(Rating: 2)</em></li>
                <li>World models <em>(Rating: 2)</em></li>
                <li>Mismatched no more: Joint model-policy optimization for model-based rl <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1229",
    "paper_id": "paper-270559382",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "UniZero",
            "name_full": "UniZero: Generalized and Efficient Planning with Scalable Latent World Models",
            "brief_description": "A transformer-based latent world model that disentangles per-timestep latent states from implicit latent history, uses a KV cache for backward memory, and jointly optimizes model and policy with MCTS in latent space to improve long-term planning and sample efficiency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "UniZero transformer latent world model",
            "model_description": "A latent-space predictive model with (1) an encoder that maps observations to per-step latent states z_t, (2) a transformer backbone that learns an implicit latent history h_t using sequences of (latent, action) tokens, (3) a dynamics head that predicts next latent ẑ_{t+1} and reward conditioned on z_≤t and a_≤t, and (4) a decision head that predicts policy logits and value conditioned on z_≤t and a_≤t−1. The model uses a KV cache at inference to preserve recent context and performs MCTS in the predicted latent space.",
            "model_type": "latent world model (transformer-based)",
            "task_domain": "Atari games (Atari 100k benchmark), VisualMatch long-term memory benchmark, multi-task Atari experiments",
            "fidelity_metric": "Next-latent L2 (||ẑ_{t+1} − stopgrad(h(o_{t+1}))||^2) for latent prediction; cross-entropy for reward/policy/value (distributional bins for reward/value); auxiliary SSL losses (contrastive L2 or negative cosine similarity in other variants) referenced for alignment.",
            "fidelity_performance": "No scalar MSE reported; empirical fidelity evidenced by downstream task success — UniZero matches or surpasses MuZero-style baselines on Atari 100k (single-frame input) and maintains high success rates on VisualMatch as memory length increases; trained 100k Atari steps in ≈4 hours and 1M VisualMatch steps in ≈30 hours on a single NVIDIA A100 80GB instance.",
            "interpretability_assessment": "Partially interpretable via visualization: attention maps and predicted quantities are inspected; attention shows focus on initial target timestep and recent timesteps in VisualMatch, and mainly recent frames in Pong; model predictions (predicted rewards, prior policy) and MCTS-improved policies are visualized to assess behavior.",
            "interpretability_method": "Visualization of attention maps from transformer layers, visual comparison of predicted vs. true rewards and prior vs. MCTS policy distributions, trajectory-level prediction visualizations.",
            "computational_cost": "Experiments ran on a single NVIDIA A100 80G GPU with 24 CPU cores and ~100GB RAM; training time ~4 hours for 100k Atari steps and ~30 hours for 1M VisualMatch steps (memory_length=500). Latent dim: 768 for Atari (64 for VisualMatch); MCTS simulations per decision sim=50; transformer backbone based on nanoGPT architecture.",
            "efficiency_comparison": "Qualitatively more sample-efficient on POMDP-like Atari tasks than MuZero-style baselines (matches/surpasses MuZero w/ SSL with single-frame input in 17/26 games); preserves long-term context with KV cache unlike recurrent/MuZero variants, enabling better long-horizon planning with comparable compute budget.",
            "task_performance": "On Atari 100k benchmark UniZero (stack=1) outperforms a reimplementation of MuZero w/ SSL (stack=4) in 17/26 games and is comparable in the remainder; on VisualMatch (long-term memory) UniZero maintains high success rates as memory length increases while MuZero and SAC-GPT degrade.",
            "task_utility_analysis": "Design prioritizes decision-relevant latent features over reconstructive fidelity; omitting observation reconstruction did not harm task performance, indicating that high fidelity in pixel reconstruction is unnecessary for policy/value learning when latent predictions and policy alignment are enforced; KV cache + transformer attention yields task-relevant long-term memory leading to improved policies on POMDP-like tasks.",
            "tradeoffs_observed": "Disentangling latent state from latent history and using transformer attention increases representational capacity for long-term dependencies but increases model complexity vs simple recurrent alternatives; omitting decoder reduces compute and complexity but forgoes reconstruction-based fidelity checks; SimNorm is required for stable training (normalization trade-off).",
            "design_choices": "Transformer backbone (nanoGPT-style), SimNorm simplicial normalization on latent states, large latent dimension (768 for Atari), joint model-policy training (value-equivalence style losses), soft target world model (EMA), KV cache for inference context, no decoder / no reconstruction loss, use of distributional cross-entropy for reward/value, MCTS with 50 simulations.",
            "comparison_to_alternatives": "Compared to MuZero-style (recursive dynamics + encoder-only root) UniZero explicitly disentangles history and state and uses full trajectory during training, improving long-term dependency performance; compared to two-stage world-model approaches (Dreamer/TD-MPC families) UniZero performs joint model-policy optimization and integrates MCTS for policy improvement, offering improved planning utility for tasks with long horizons; compared to RNN backbones (UniZero RNN variant), transformer + KV cache retained fuller context and outperformed recurrent variants.",
            "optimal_configuration": "Empirical recommendations: apply SimNorm on latents, use soft target world model (EMA) for stability, set train context length to match long-horizon requirements (e.g., VisualMatch needs longer), set inference context shorter for many Atari tasks (Hinfer=4 often sufficient), moderate transformer depth improves performance, avoid decoder/reconstruction for decision tasks, rely on KV caching to maintain recent context during inference.",
            "uuid": "e1229.0",
            "source_info": {
                "paper_title": "UniZero: Generalized and Efficient Planning with Scalable Latent World Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "MuZero",
            "name_full": "MuZero (value-equivalent latent MCTS model)",
            "brief_description": "A joint model-policy latent MCTS algorithm with an encoder, dynamics network (recursive unrolls), and prediction network that plans with MCTS in learned latent space and uses a value-equivalence training objective.",
            "citation_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "mention_or_use": "use",
            "model_name": "MuZero-style latent world model",
            "model_description": "Encoder maps observation(s) to a root latent state s^0_t; dynamics network recursively predicts s^k and rewards rk given previous latent and action; prediction network outputs policy and value from latent states; planning via MCTS in latent space using the encoder-produced root.",
            "model_type": "latent world model (recursive/deterministic dynamics network + MCTS)",
            "task_domain": "Board games, Atari games, other discrete-control domains (used here as baseline on Atari 100k and VisualMatch)",
            "fidelity_metric": "Alignment between observation embeddings z_{t+k} and dynamics predictions s^k_t measured by L2 contrastive loss l_z or self-supervised consistency (negative cosine) in variants; task return scores as downstream fidelity proxy.",
            "fidelity_performance": "Strong in short-term-memory MDP-like tasks; when used with SSL and stacked frames (stack=4) shows good sample efficiency on MDP-like Atari tasks; performance degrades rapidly in POMDP/long-term dependency tasks (e.g., fails to converge on Pong stack=1 with SSL within 500k steps in this reimplementation).",
            "interpretability_assessment": "Latent representations can be visualized but are entangled with historical information making SSL alignment problematic; limited interpretability described (visualization prior work referenced).",
            "interpretability_method": "Observation-embedding vs dynamics prediction alignment losses, visualization of latent representations referenced from prior works; no transformer attention-based analyses since backbone is recurrent/unrolled dynamics.",
            "computational_cost": "Standard MuZero-style compute depends on encoder/dynamics/prediction sizes and MCTS sims; in this paper MuZero variants were reimplemented using LightZero with similar hyperparameters (reanalyze_ratio=0) — exact wall-clock not specified here for baseline runs.",
            "efficiency_comparison": "MuZero w/ SSL (stack=4) is sample-efficient for MDP tasks but under-utilizes trajectory data and its recursively unrolled latents entangle history, leading to worse performance in POMDP-like tasks vs UniZero; MuZero w/ context recovers some history but is harmed by compounding prediction errors.",
            "task_performance": "Good on short-horizon MDP tasks and many Atari games when given stacked frames; poor on long-horizon partial-observation tasks (VisualMatch) and fails to maintain performance when input is single-frame POMDP-like scenarios.",
            "task_utility_analysis": "MuZero's dynamics predictions and joint training yield high utility when history requirement is short or when stacked frames supply required context; however, entanglement of historical info in latent unrolls prevents effective self-supervised regularization and harms long-term memory tasks.",
            "tradeoffs_observed": "Recurrence-based latent unrolls simplify architecture but cause entanglement with history and compounding errors for long-horizon predictions; using SSL improves sample efficiency in MDPs but can make predicted latents depend excessively on single observation, harming POMDP performance.",
            "design_choices": "Encoder + recursive dynamics + prediction heads, MCTS in latent space, optionally SSL contrastive or SimSiam-style consistency losses, uses initial observation (possibly stacked frames) as root during training.",
            "comparison_to_alternatives": "Compared with UniZero, MuZero under-utilizes trajectory data during training and entangles latent states with history, causing worse performance in long-term dependency tasks; compared to two-stage methods (Dreamer, TD-MPC) MuZero jointly trains model and policy and leverages MCTS for policy improvement.",
            "optimal_configuration": "Paper argues MuZero-style works best when tasks require primarily short-term memory or when input supplies stacked frames; adding SSL helps for MDPs but is insufficient for POMDPs without architecture changes to disentangle history.",
            "uuid": "e1229.1",
            "source_info": {
                "paper_title": "UniZero: Generalized and Efficient Planning with Scalable Latent World Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "MuZero w/ SSL",
            "name_full": "MuZero with self-supervised latent regularization (SSL)",
            "brief_description": "A MuZero variant augmented with self-supervised consistency or contrastive regularization between predicted dynamics latents and encoder observation embeddings to stabilize latents and improve sample efficiency.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MuZero + SSL",
            "model_description": "Same three-network MuZero architecture but adds an auxiliary regularization loss (contrastive L2 l_z = sum_k ||z_{k+1} − stopgrad(h(s_{k+1}))||^2 or SimSiam-style negative cosine) to align predicted latents with encoder embeddings.",
            "model_type": "latent world model with auxiliary self-supervised regularization",
            "task_domain": "Atari games (Atari 100k), used as strong baseline",
            "fidelity_metric": "L2 latent alignment loss (l_z) or negative cosine similarity used as auxiliary metric; downstream returns as evaluation.",
            "fidelity_performance": "Improves sample efficiency in MDP-like Atari tasks (stack=4) but fails to converge in single-frame POMDP-like Pong within 500k steps in the reported experiments due to entanglement issues.",
            "interpretability_assessment": "Improved alignment between encoder embeddings and dynamics predictions makes latents more consistent; still suffers from entanglement making history separation unclear.",
            "interpretability_method": "Auxiliary latent alignment losses provide an implicit sanity check; prior visualization literature cited but no new specialized interpretability beyond UniZero analyses.",
            "computational_cost": "Similar to MuZero baseline with small additional cost for computing auxiliary SSL losses; exact overhead not quantified.",
            "efficiency_comparison": "More sample-efficient than vanilla MuZero in MDP tasks; less effective than UniZero in POMDP/long-term tasks despite SSL.",
            "task_performance": "Strong on many Atari tasks with stacked frames; poor on single-frame POMDP tasks (e.g., Pong stack=1) where SSL harms convergence.",
            "task_utility_analysis": "SSL supplies richer training signals than sparse rewards and stabilizes latent learning in short-term tasks, but when latent dynamics are entangled with history SSL can encourage dependence on single-frame observations, reducing memory utility.",
            "tradeoffs_observed": "SSL increases stability/sample efficiency for MDP tasks but can reduce the model's ability to encode long-term history when entanglement exists; auxiliary loss imposes additional compute and potential bias toward single-step observability.",
            "design_choices": "Contrastive L2 or SimSiam negative cosine SSL on predicted latents vs encoder embeddings, applied across unrolled steps; used with standard MuZero networks and MCTS.",
            "comparison_to_alternatives": "Performs better than plain MuZero in MDP-like settings; UniZero's transformer-based disentanglement combined with SSL-like alignment performs better in POMDP/long-horizon settings.",
            "optimal_configuration": "Works well with stacked-frame inputs in MDP tasks; not recommended alone for POMDPs without architectural changes to separate latent state from history.",
            "uuid": "e1229.2",
            "source_info": {
                "paper_title": "UniZero: Generalized and Efficient Planning with Scalable Latent World Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "UniZero (RNN)",
            "name_full": "UniZero variant with GRU backbone (UniZero RNN)",
            "brief_description": "A variant of UniZero replacing the transformer backbone with a GRU; trained with same data usage but suffers from limited memory length and compound prediction errors at inference.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "UniZero (RNN backbone)",
            "model_description": "Same encoder/dynamics/decision heads but using a GRU as the sequence backbone; during inference GRU hidden state is reset every Hinfer steps and recursively predicted hidden states are used as root/internal nodes.",
            "model_type": "latent world model (RNN-based)",
            "task_domain": "Atari (ablation) and VisualMatch (discussion) — used as ablation variant",
            "fidelity_metric": "Downstream task returns and qualitative trajectory prediction accuracy; no scalar MSE given.",
            "fidelity_performance": "Performs worse than transformer UniZero in long-term dependency tasks due to limited memory and compounding errors; reported as poor and not considered a primary baseline.",
            "interpretability_assessment": "Less amenable to long-range attention-based interpretability; errors accumulate in recursively predicted hidden states making analysis of correct memory use harder.",
            "interpretability_method": "Trajectory visualizations and qualitative failure analysis (resetting hidden states every Hinfer causes incomplete context).",
            "computational_cost": "Lower theoretical overhead than transformer but required frequent resets and still suffered in performance; exact wall-clock not reported.",
            "efficiency_comparison": "Computationally simpler than transformer's KV cache but less effective per compute for long-horizon tasks due to inability to maintain complete context.",
            "task_performance": "Failed to match UniZero transformer performance; experienced incomplete context and poor learning in long-term POMDP tasks.",
            "task_utility_analysis": "RNN backbone limits backward memory and suffers compounding prediction errors at inference, reducing its utility for long-term planning despite simpler compute.",
            "tradeoffs_observed": "RNNs offer parameter/compute efficiency but sacrifice long-horizon memory fidelity and robustness; transformer+KV cache preferred for long dependencies.",
            "design_choices": "GRU backbone, hidden-state resets every Hinfer steps, same decision/dynamics heads as UniZero.",
            "comparison_to_alternatives": "Performs worse than transformer UniZero in POMDP tasks; better than naive MuZero w/ context in some setups? (paper reports generally inferior results).",
            "optimal_configuration": "Not recommended for long-horizon, partial-observation tasks according to empirical findings in paper.",
            "uuid": "e1229.3",
            "source_info": {
                "paper_title": "UniZero: Generalized and Efficient Planning with Scalable Latent World Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "EfficientZero",
            "name_full": "EfficientZero (MuZero-style with enhanced sample efficiency)",
            "brief_description": "A MuZero-derived algorithm incorporating self-supervised consistency (SimSiam-style) and other sample-efficiency enhancements; used here as a competitive baseline on Atari 100k.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "EfficientZero-style latent world model",
            "model_description": "MuZero-style encoder/dynamics/prediction architecture augmented with self-supervised consistency losses, and sometimes recurrent modules (LSTM) for value_prefix prediction in some implementations; focuses on sample efficiency.",
            "model_type": "latent world model (MuZero variant with SSL)",
            "task_domain": "Atari 100k benchmark (used as baseline)",
            "fidelity_metric": "Auxiliary self-supervised consistency loss (negative cosine) between predicted and observed latents; downstream returns.",
            "fidelity_performance": "Competitive on Atari 100k with stacked frames; used as a high-performing baseline (stack=4).",
            "interpretability_assessment": "Not specifically analyzed in this paper beyond being a baseline; prior works have latent visualizations.",
            "interpretability_method": "Self-supervised consistency acts as implicit alignment; no attention-map analyses since not transformer-based in baseline implementation here.",
            "computational_cost": "Similar scale to MuZero w/ SSL; specific run-times not reported in this paper.",
            "efficiency_comparison": "Sample-efficient on MDP-like Atari tasks; UniZero matches or exceeds its performance in many games even with single-frame input.",
            "task_performance": "Strong performance on MDP-like Atari tasks (stack=4); UniZero (stack=1) matches or outperforms it in many games.",
            "task_utility_analysis": "Effective when short-term context suffices; SSL improves sample efficiency by providing richer supervision beyond scalar rewards.",
            "tradeoffs_observed": "Additional SSL bookkeeping and potential reliance on stacked frames limits applicability on POMDP long-horizon tasks compared to UniZero.",
            "design_choices": "MuZero-like architecture with SimSiam-style SSL; in some versions uses LSTM for value prefix prediction.",
            "comparison_to_alternatives": "Compared to UniZero, EfficientZero offers similar or better sample efficiency in MDP tasks but is outperformed on long-term dependency tasks requiring explicit history disentanglement.",
            "optimal_configuration": "Best used with stacked-frame inputs and tasks without long-term partial observability; hyperparameters tuned for sample efficiency.",
            "uuid": "e1229.4",
            "source_info": {
                "paper_title": "UniZero: Generalized and Efficient Planning with Scalable Latent World Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SAC-GPT",
            "name_full": "SAC-Discrete with GPT backbone (SAC-GPT)",
            "brief_description": "A model-free actor-critic (SAC-Discrete) variant that uses a GPT-style transformer backbone to provide sequence modeling for decision making; used as a baseline for long-term memory tasks (VisualMatch) in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SAC-GPT",
            "model_description": "SAC-Discrete algorithm for discrete actions combining a GPT-like transformer sequence model as the policy/value backbone to handle sequential dependencies in partially observable tasks.",
            "model_type": "model-free RL with transformer sequence backbone",
            "task_domain": "VisualMatch long-term dependency benchmark (grid-world POMDP)",
            "fidelity_metric": "Downstream success rate on VisualMatch as memory_length increases; training steps (SAC-GPT reported baseline after 3M env steps).",
            "fidelity_performance": "Performance degrades significantly as memory length increases in VisualMatch; baseline final success rate (plotted green dashed line) achieved after training on 3M environment steps — numeric success rates not tabulated in this paper.",
            "interpretability_assessment": "Not deeply analyzed here; transformer backbone allows sequence attention but no attention visualizations provided for SAC-GPT in this paper.",
            "interpretability_method": "Not reported in this paper for SAC-GPT.",
            "computational_cost": "SAC-GPT is trained up to 3M environment steps in referenced baseline results; exact hardware/time not specified here.",
            "efficiency_comparison": "UniZero outperforms SAC-GPT on VisualMatch for large memory lengths despite SAC-GPT being transformer-based, indicating that combining forward MCTS + backward KV memory is more effective for long-horizon sparse-reward POMDPs.",
            "task_performance": "SAC-GPT achieves reasonable performance at shorter memory lengths but declines as memory_length grows; UniZero maintains high success rate.",
            "task_utility_analysis": "Transformer backbone alone (SAC-GPT) helps, but without model-based planning and explicit world-model predictions plus KV cache, model-free transformer approaches struggle with very long memory and sparse rewards.",
            "tradeoffs_observed": "Model-free transformer approaches may require far more environment steps (training data) to reach comparable performance on long-horizon sparse-reward tasks compared to model-based UniZero.",
            "design_choices": "GPT backbone for sequence modeling combined with SAC-Discrete for policy learning; no latent world-model MCTS planning.",
            "comparison_to_alternatives": "SAC-GPT underperforms UniZero on VisualMatch long-memory tasks; performs better than MuZero (stack=1) in some regimes but deteriorates as memory grows.",
            "optimal_configuration": "Requires orders of magnitude more environment steps (e.g., millions) to approach top performance on long-memory sparse reward tasks; not optimal for sample-limited regimes.",
            "uuid": "e1229.5",
            "source_info": {
                "paper_title": "UniZero: Generalized and Efficient Planning with Scalable Latent World Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Dreamer (series)",
            "name_full": "Dreamer / DreamerV3 family (latent imagination-based RL)",
            "brief_description": "A family of world-model methods that learn latent-space dynamics with an encoder/decoder (VAE-like) and use latent rollout imagination to train actor-critic policies, typically in a two-stage or imagination-augmented training pipeline.",
            "citation_title": "Mastering atari with discrete world models",
            "mention_or_use": "mention",
            "model_name": "Dreamer-style latent world models",
            "model_description": "Latent-space models using (categorical/discrete) VAE encoders to produce compact latent codes and a dynamics model for latent imagination rollouts; policies/value are learned with actor-critic methods using imagined trajectories.",
            "model_type": "latent world model (VAE-based, imagination rollouts)",
            "task_domain": "Atari, continuous control, general RL benchmarks (referenced comparison)",
            "fidelity_metric": "Reconstruction loss (VAE), prediction accuracy in latent rollout; downstream returns.",
            "fidelity_performance": "High sample-efficiency on many domains in prior work; provided here as contextual baseline (no new numbers in this paper).",
            "interpretability_assessment": "Latent codes can sometimes be inspected and decoded; Dreamer typically uses decoders enabling reconstruction-based interpretability.",
            "interpretability_method": "Reconstruction and latent visualization used in prior Dreamer works (cited).",
            "computational_cost": "Two-stage or imagination-heavy training with additional decoder costs; reported elsewhere, not quantified here.",
            "efficiency_comparison": "Dreamer family is a strong model-based baseline but follows two-stage pipeline; UniZero contrasts by joint model-policy training with MCTS.",
            "task_performance": "Referenced as baseline family; not directly re-evaluated in this paper's experiments.",
            "task_utility_analysis": "Dreamer-style methods emphasize accurate latent reconstructions and imagination for policy learning; useful when reconstruction fidelity correlates with decision features but can be less direct than UniZero's decision-focused latents.",
            "tradeoffs_observed": "Two-stage training and reconstruction-heavy objectives can increase fidelity to observations at cost of decision-relevance and joint optimization benefits.",
            "design_choices": "VAE/categorical VAE encoders, latent rollout imagination, actor-critic policy learning often in separate stages.",
            "comparison_to_alternatives": "Contrasted in paper as alternate route (two-stage) vs MuZero-style joint optimization; UniZero aims to combine scalable transformer architectures with joint training benefits.",
            "uuid": "e1229.6",
            "source_info": {
                "paper_title": "UniZero: Generalized and Efficient Planning with Scalable Latent World Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "TD-MPC (TD-MPC2)",
            "name_full": "TD-MPC series (TD-MPC2: Scalable, robust world models for continuous control)",
            "brief_description": "A family of model-based control methods using learned latent models and model predictive control to derive policies, typically in a two-stage training paradigm emphasizing robustness and scaling.",
            "citation_title": "Td-mpc2: Scalable, robust world models for continuous control",
            "mention_or_use": "mention",
            "model_name": "TD-MPC family",
            "model_description": "Latent world models trained to support local trajectory optimization (MPC) in latent space; often uses deterministic/stochastic latent dynamics with value/policy heads used for control.",
            "model_type": "latent world model (MPC-based)",
            "task_domain": "Continuous control tasks (cited for reference), scalability comparisons in related work",
            "fidelity_metric": "Prediction error and control performance (task returns); not specified in this paper.",
            "fidelity_performance": "Referenced as showing substantial gains in large-scale tasks in prior literature; not re-evaluated here.",
            "interpretability_assessment": "Not explicitly discussed in this paper.",
            "interpretability_method": "Not specified here.",
            "computational_cost": "MPC loops are computationally intensive in planning; paper contrasts approach but does not provide direct cost numbers here.",
            "efficiency_comparison": "TD-MPC uses two-stage training and MPC for policy derivation; UniZero focuses on joint training + MCTS and transformer-based memory for long-horizon POMDPs.",
            "task_performance": "Not evaluated in this paper; referenced as an alternative route for world-model design.",
            "task_utility_analysis": "TD-MPC effective for continuous control, but differs in training pipeline from UniZero's joint optimization and discrete-action MCTS focus.",
            "tradeoffs_observed": "MPC methods offer strong local trajectory optimization but can be costly and require accurate short-horizon models; UniZero's MCTS + KV cache targets long-horizon partial observability.",
            "design_choices": "Latent dynamics trained for MPC objectives; two-stage pipelines common.",
            "uuid": "e1229.7",
            "source_info": {
                "paper_title": "UniZero: Generalized and Efficient Planning with Scalable Latent World Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "World Models (Ha & Schmidhuber)",
            "name_full": "World Models",
            "brief_description": "An early framework proposing learning compact spatio-temporal latent representations to enable planning and policy learning in latent space, influencing later latent-world-model approaches.",
            "citation_title": "World models",
            "mention_or_use": "mention",
            "model_name": "World Models (original)",
            "model_description": "Encoder + latent dynamics + controller paradigm that learns a compressed representation of environment dynamics (often VAE or RNN-based) and uses it for policy learning and planning.",
            "model_type": "latent world model (VAE/RNN-based)",
            "task_domain": "Simulated control tasks, image-based RL benchmarks; cited historically",
            "fidelity_metric": "Reconstruction loss and simulation fidelity; downstream policy performance.",
            "fidelity_performance": "Historical contribution; not benchmarked in this paper.",
            "interpretability_assessment": "Reconstruction-based latent interpretability possible through decoders.",
            "interpretability_method": "Decoder visualizations and latent traversals in prior work.",
            "computational_cost": "Varies with implementation; cited for conceptual contribution.",
            "efficiency_comparison": "Foundational approach influencing later architectures; UniZero differentiates by not using decoder/reconstruction and using transformer-based history handling.",
            "task_performance": "Not evaluated here.",
            "task_utility_analysis": "Demonstrated that latent predictive models could be useful for policy learning; UniZero builds on the idea but emphasizes disentangling history and decision-focused latents.",
            "tradeoffs_observed": "Reconstruction emphasis can impose extra objectives not directly tied to decision utility; UniZero avoids decoding for efficiency and task utility.",
            "design_choices": "Use of VAE-style encoders and RNN dynamics in original formulation.",
            "uuid": "e1229.8",
            "source_info": {
                "paper_title": "UniZero: Generalized and Efficient Planning with Scalable Latent World Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "rating": 2,
            "sanitized_title": "mastering_atari_go_chess_and_shogi_by_planning_with_a_learned_model"
        },
        {
            "paper_title": "Mastering atari with discrete world models",
            "rating": 2,
            "sanitized_title": "mastering_atari_with_discrete_world_models"
        },
        {
            "paper_title": "Td-mpc2: Scalable, robust world models for continuous control",
            "rating": 2,
            "sanitized_title": "tdmpc2_scalable_robust_world_models_for_continuous_control"
        },
        {
            "paper_title": "World models",
            "rating": 2,
            "sanitized_title": "world_models"
        },
        {
            "paper_title": "Mismatched no more: Joint model-policy optimization for model-based rl",
            "rating": 1,
            "sanitized_title": "mismatched_no_more_joint_modelpolicy_optimization_for_modelbased_rl"
        }
    ],
    "cost": 0.023449249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>UniZero: Generalized and Efficient Planning with Scalable Latent World Models
15 Jun 2024</p>
<p>Yuan Pu 
Shanghai Artificial Intelligence Laboratory</p>
<p>Yazhe Niu 
SenseTime Research</p>
<p>The Chinese University of Hong
Kong</p>
<p>Jiyuan Ren 
Shanghai Artificial Intelligence Laboratory</p>
<p>Zhenjie Yang 
Shanghai Jiao Tong University</p>
<p>Hongsheng Li 
The Chinese University of Hong
Kong</p>
<p>Yu Liu 
Shanghai Artificial Intelligence Laboratory</p>
<p>SenseTime Research</p>
<p>UniZero: Generalized and Efficient Planning with Scalable Latent World Models
15 Jun 20249EA7B38C08FC648862A0821A989EE8A0arXiv:2406.10667v1[cs.LG]
Learning predictive world models is essential for enhancing the planning capabilities of reinforcement learning agents.Notably, the MuZero-style algorithms, based on the value equivalence principle and Monte Carlo Tree Search (MCTS), have achieved superhuman performance in various domains.However, in environments that require capturing long-term dependencies, MuZero's performance deteriorates rapidly.We identify that this is partially due to the entanglement of latent representations with historical information, which results in incompatibility with the auxiliary self-supervised state regularization.To overcome this limitation, we present UniZero, a novel approach that disentangles latent states from implicit latent history using a transformer-based latent world model.By concurrently predicting latent dynamics and decision-oriented quantities conditioned on the learned latent history, UniZero enables joint optimization of the long-horizon world model and policy, facilitating broader and more efficient planning in latent space.We demonstrate that UniZero, even with single-frame inputs, matches or surpasses the performance of MuZero-style algorithms on the Atari 100k benchmark.Furthermore, it significantly outperforms prior baselines in benchmarks that require long-term memory.Lastly, we validate the effectiveness and scalability of our design choices through extensive ablation studies, visual analyses, and multi-task learning results.The code is available at https://github.com/opendilab/LightZero.</p>
<p>Introduction</p>
<p>Reinforcement learning (RL) [1] has emerged as a crucial approach in the quest for general artificial intelligence.Despite its advancements, traditional RL methods often encounter difficulties in complex tasks that require long-term planning [2,3,4].To tackle this challenge, researchers have sought to enhance the planning capabilities and sample efficiency of agents by developing predictive world models [5,6,7,8,9,10].Notably, MuZero-style algorithms [10,11,12,13,14], which leverage the value equivalence principle [15] and Monte Carlo Tree Search (MCTS) [16] for planning in a learned latent space, have made significant strides in domains such as board games [10,17] and Atari games [10].However, it is imperative to acknowledge that these accomplishments are predominantly limited to scenarios that only require short-term memory [2,18], and challenges still exist when dealing with long-term dependency problems, which limit their applicability in broader scenarios.</p>
<p>In the realm of language and vision tasks, the advent of the multi-head attention [19] has revolutionized the modeling of long-term dependencies.This innovation has contributed to a cascade of groundbreaking advancements across a range of applications [20,21,22].Recently, there has been a growing interest in adapting this mechanism to the field of decision-making.Some works such as [23,24,25] conceptualize reinforcement learning as a sequence modeling problem, with a particular emphasis on the development of return-conditioned policies through supervised offline learning methods.Additionally, some studies advocate for a two-stage online learning setting, where the dynamics model and policy are learned independently [26,27,28,29].</p>
<p>These studies [30,2] partially confirm the effectiveness of the attention mechanism in improving backward memory capabilities.Meanwhile, MCTS is acknowledged for its efficient forward planning abilities.Combining these two methods could potentially optimize both retrospective and prospective cognitive functions in artificial intelligence systems.To date, there is limited research on the seamless integration of these two methodologies.Consequently, the central question addressed in this paper is whether transformer architectures can enhance planning efficiency in long-term dependency decision-making scenarios.We will conduct a preliminary exploration of this topic in this paper.</p>
<p>As illustrated in Table 1 and Figure 1, our qualitative analysis identifies two primary limitations in the architecture design of MuZero-style algorithms.First, there is an entanglement of latent representations with historical information, which makes it difficult to separate current state features from previous interactions.Second, there is an under-utilization of trajectory data during training, hindering the effective use of accumulated experience.These issues limit the effectiveness of auxiliary self-supervised regularization losses, particularly in scenarios that demand the modeling of long-term dependencies.Consequently, these limitations reduce sample efficiency and degrade overall algorithm performance.For further details, a comprehensive experimental analysis is provided in Section 6.</p>
<p>To overcome these issues, we introduce UniZero, a novel approach depicted on the right of Figure 1.UniZero employs a transformer-based latent world model that effectively disentangles latent states from implicit latent history (refer to Section 3.2).Through the concurrent prediction of latent dynamics and decision-oriented quantities conditioned on the learned latent history, UniZero facilitates the joint optimization of the long-horizon world model and policy [10,31,32], rather than the two-stage learning in [26] that could introduce the inconsistency between model and policy learning.We believe that UniZero, with its simple and unified architecture and training paradigm, has the potential to become a scalable decision-making foundational model.</p>
<p>To substantiate the effectiveness of UniZero, we conduct experiments on the Atari 100k benchmark, encompassing 26 distinct games.The empirical results demonstrate that UniZero, even with singleframe inputs, exhibits comparable or even superior performance to MuZero-style algorithms that utilize four consecutive input frames.This finding indicates that UniZero's modeling of long-term memories also enhances performance in tasks requiring short-term dependencies.To further scrutinize UniZero's capabilities, we qualitatively evaluate its performance on the VisualMatch benchmark, which demands long-term memory.Across various memory lengths, UniZero significantly surpasses prior baselines, further confirming its superiority in modeling long-term dependencies.Furthermore, through comprehensive ablation studies and visual analyses, we thoroughly investigate the impact of each design component within UniZero.Lastly, we conduct an initial exploration into UniZero's performance in a multi-task learning setting across four Atari games, as detailed in E. The encouraging results from these experiments substantiate UniZero's potential as a versatile and generalized agent.</p>
<p>The main contributions of this paper can be summarized as follows:</p>
<p>• We reveal two key imitations with MuZero-style algorithms when dealing with environments that require long-term dependencies: the entanglement of latent representations with historical information and the under-utilization of trajectory data in training, which limits their performance and efficiency.• We introduce the UniZero approach, which effectively disentangles latent states from implicit latent histories through a transformer-based latent world model, thereby facilitating efficient and scalable long-term planning in the learned latent space.• Experimental evidence on the Atari 100k benchmark and the long-term dependency benchmark demonstrates the superior performance of UniZero compared to prior baselines, and its design effectiveness is validated through ablation studies, visual analysis and multi-task learning results.</p>
<p>Background</p>
<p>Reinforcement Learning (RL) [1] is a powerful framework for modeling decision-making problems, typically represented as Markov Decision Processes (MDPs), defined by the tuple M = (S, A, P, R, γ, ρ 0 ).Here, S denotes the state space and A the action space.The transition function P provides the probability of moving from one state to another given an action, while the reward function R assigns rewards to state-action pairs.The discount factor γ ∈ [0, 1) balances immediate and future rewards, and ρ 0 is the initial state distribution.The goal in RL is to learn an optimal policy π : S → A that maximizes the expected discounted return E π [ ∞ t=0 γ t r t ].In real-world applications, the Markov property often does not hold, necessitating the use of Partially Observable MDPs (POMDPs) [33].POMDPs extend MDPs by including an observation space O, where the agent receives o ∈ O(s), providing partial information about the state.In environments with long-term dependencies, optimal decision-making requires considering the observation history τ 1:t := (o 1:t , a 1:t−1 ) [2].Usually, we consider a recent memory of length H rather than the entire history.Consequently, policies and value functions are defined with respect to this history, expressed as π (a t | τ t−H+1:t ) and v (a t | τ t−H+1:t ) = E π,M i=t γ i−t r i | τ t−H+1:t .MuZero [10] achieves superhuman performance in complex visual domains [34] without prior knowledge of the environment's transition rules.It integrates tree search with a learned model composed of three networks: 1 Encoder: s 0 t = h θ (o 1 , . . ., o t ) (at time-step t and hypothetical (also called recurrent/unroll) step 0, omitted t when clear).This network encodes past observations (o 1 , . . ., o t ) into a latent representation, which serves as the root node in MCTS. 2 Dynamics Network: rk , s k = g θ (s k−1 , a k ).This network predicts the next latent state and reward given the current latent state and action.3 Prediction Network: p k , v k = f θ (s k ).Given a latent state, this network predicts action probabilities and value.MuZero performs MCTS in the learned latent space, with the root node s 0 generated by the encoder.Each edge stores: N (s, a), P (s, a), Q(s, a), R(s, a), S(s, a), representing visit counts, policy, value, reward, and state transition, respectively.The MCTS process involves Selection, Expansion, and Backup, detailed in Appendix B.1.1.After the search, the visit counts N (s, a) at the root node s 0 are normalized to derive an improved policy π.An action is then sampled from this distribution for interaction with the environment.During learning, MuZero is trained end-to-end with the following loss function, where l p , l v , and l r are losses for policy, value, and reward, respectively, and the final term is weight decay:
l t (θ) = K k=0 l p (p k t , π t+k ) + l v (v k t , z t+k ) + l r (r k t , r t+k ) + c||θ|| 2(1)
It is important to note that MuZero and its subsequent improved algorithms [35,36,13,12,11] exhibit the following two key characteristics: during training, only the initial step of the sequence observation (which may consist of a stack of frames) is utilized, and predictions at each time step depend on a latent representation obtained through recursive prediction.We refer to architectures that adhere to these two characteristics as MuZero-style architectures.</p>
<p>Self-supervised Regularization: The visualization of latent representations acquired by the MuZero agent, as discussed in [35], indicates that without a clear training goal to align latent representations with observations, discrepancies can arise between the observation embeddings z t+k encoded by the representation network and the latent representations s k t derived from the dynamics network.These discrepancies can destabilize the planning process.Moreover, since the primary training objective in RL is based on scalar rewards, this information can be insufficient, particularly in sparse-reward scenarios [37].In contrast, observation embeddings, typically encoded as compact tensors, provide richer training signals than scalars.To improve sample efficiency and stability, integrating auxiliary self-supervised objectives into MuZero to regularize the latent representations is crucial.Specifically, [35] proposed a contrastive regularization loss:
l z (θ) = H k=0 ∥z k+1 t − sg{h θ (s k+1 t )}∥ 2
2 which penalizes the error between the observation embeddings z k+1 t and their corresponding dynamics predictions s k+1 t .Inspired by the Sim-Siam framework, EfficientZero [36] introduced a self-supervised consistency loss (SSL), defined as the negative cosine similarity between the projections of predicted latent representations s k+1 t and the actual observation embeddings z k+1 t .</p>
<p>UniZero</p>
<p>In this section, we begin by analyzing the two main limitations of MuZero-style architectures, as discussed in Section 3.1, especially in their ability to handle tasks that require capturing longterm dependencies.To address these limitations, in Section 3.2, we introduce a novel approach termed UniZero, which is fundamentally a transformer-based latent world model.We provide a comprehensive description of its architectural design, along with the joint optimization procedure for both the model and the policy.In Section 3.  from a long-term perspective within the learned latent space.For more details on the algorithm's implementation, please refer to Appendix B.</p>
<p>Full-utilization
Network Data Unrolled Data</p>
<p>Main Limitations in MuZero-style Architectures</p>
<p>As illustrated in Figure 1, during the training phase, MuZero-style architectures utilize the initial observation o t (which may consist of stacked frames) and the entire sequence of actions as their input (actions omitted when context is clear).This approach results in the under-utilization of trajectory data, particularly when dealing with long trajectory sequences or scenarios requiring longterm dependencies.Moreover, MuZero employs dynamics networks to unroll several hypothetical steps during training, i.e., rk , s k = g θ (s k−1 , a k−1 ), as illustrated by the light blue data in Figure 1.Consequently, the recursively unrolled latent representation s k t is tightly coupled with historical information, a phenomenon we refer to as entanglement.This entanglement is incompatible with the SSL loss, as discussed later.During inference, the latent representation encoded from the current observation o t (without historical information) is used as the root node in MCTS.While this design is effective in MDPs, we suspect it will fail in partially observable scenarios due to the lack of historical information.One simple extension is to instead use the recursively predicted latent representation s k t−k as the root node.However, this method potentially introduces inaccuracies due to compounded errors, leading to suboptimal performance.</p>
<p>To empirically analyze the impact of these limitations on capturing long-term dependencies, we propose four variants of the MuZero-style algorithm and compare them with our UniZero approach:</p>
<p>• (Original) MuZero: This method does not utilize any self-supervised regularization.During inference, the root state z t is generated by encoding the current observation o t through the encoder.• MuZero w/ SSL: As discussed in Section 2, to enhance sample efficiency, this variant integrates an auxiliary self-supervised regularization [36] into MuZero's training objective.However, we argue that imposing this regularization loss might cause the predicted latent representation s k t−k to rely excessively on the observation o t from a single timestep.This dependency could inadvertently diminish the importance of more extensive historical data.Thus, this variant seems primarily suitable for MDP tasks that align with such a design.</p>
<p>• MuZero w/ Context: This variant retains the same training settings as MuZero but diverges during inference by employing a k-step recursively predicted latent representation s k t−k at the root node.Due to the presence of compound errors [5] from recurrent predictions, the context at the root node is inaccurate, leading to prediction errors in the MCTS process.We refer to the issue of significant prediction errors and poor performance caused by partially accessible context as the phenomenon of incomplete context.</p>
<p>• UniZero: Our proposed UniZero (depicted on the right side of Figure 1) disentangles latent states z t from the implicit latent history using a transformer-based world model.This architecture enables the model to leverage the entire trajectory during training while ensuring the prediction of the next latent state z t at each timestep.During inference, UniZero preserves a relatively complete context within the most recent H infer steps by utilizing the Key-Value (KV) cache mechanism.This approach both enriches the learning signals with self-supervised regularization and maintains long-term dependencies.</p>
<p>• UniZero (RNN): This variant retains similar training settings as UniZero but employs a GRU [38] backbone.During the inference phase, the hidden state of the GRU is reset and cleared every H infer steps.Due to the recurrent mechanism and the limited memory length of the GRU, it also experiences the phenomenon of incomplete context.For more details, please refer to B.2.For comparison, Table 1 outlines the qualitative differences among these variants.Figure 2 illustrates their performance disparities in Pong under frame_stack=4 and frame_stack=1 settings, which approximately correspond to MDP and POMDP scenarios, as referenced in [18].In the stack4 scenario, MuZero w/ SSL demonstrates good sample efficiency due to the rich learning signals provided by the auxiliary self-supervised regularization loss.However, in the stack1 setting, MuZero w/ SSL fails to converge within 500k environment steps, primarily due to issues related to the previously discussed entanglement design.Meanwhile, both MuZero w/ Context and MuZero w/ Context (RNN) face challenges in learning due to prediction errors arising from the incomplete context phenomenon.For MuZero w/ Context, these errors originate from its recurrently predicted latent representations, whereas for MuZero w/ Context (RNN), they stem from its recurrent hidden state of GRU.In both scenarios, UniZero outperforms the other variants, verifying its robustness and versatility.</p>
<p>Scalable Latent World Models</p>
<p>Building on the above insights, we introduce the UniZero method to address the challenges posed by the entanglement of latent representations with historical information and the under-utilization of trajectory data.In this subsection, we will outline the architecture of our method and provide detailed descriptions of the training procedures for the joint optimization of the model and the policy.</p>
<p>Transformer-based Latent World Models</p>
<p>Architecture.As depicted in the top right of Figure 1, UniZero comprises four principal components: the encoder h θ , the transformer backbone, the dynamics network g θ , and the decision network f θ .It is worth noting that in the following discussion, the transformer backbone might be implicitly incorporated within the last two networks.Formally, at each time step t, the environmental observations and actions are denoted by o t and a t , respectively.For simplicity, a t may also represent the action embedding obtained via a lookup in a learned embedding table.We denote the latent states in UniZero by z t , the predicted subsequent latent state by ẑt+1 , and the predicted reward by rt .Furthermore, the policy (typically represented by action logits) and state value are denoted by p t and v t , respectively, which are crucial for guiding the MCTS procedure towards regularized policy optimization [39].In summary, the world model of UniZero W encapsulates the following parts: encoder: z t = h θ (o t ) ▷ Maps observations to their latent states dynamics network: ẑt+1 , rt = g θ (z ≤t , a ≤t ) ▷ Models latent dynamics and reward decision network: p t , v t = f θ (z ≤t , a ≤t−1 ) ▷ Predicts policy and value (2) Training.We categorize a single time step into two distinct tokens: the latent state and the action.For details about data pre-processing and tokenizers, please refer to Appendix B.2. UniZero's dynamic network is designed to predict the subsequent latent state and reward conditioned on the previous latent states and actions up to time step t: (z ≤t , a ≤t ).Concurrently, the decision network is tailored to predict the decision-relevant quantities (policy and state-value) based on the previous latent states and actions up to time steps t and t − 1: (z ≤t , a ≤t−1 ).In MuZero-style approaches, which rely solely on the initial observation, the k-th latent representation s k t is recursively inferred through the dynamic network.UniZero distinguishes itself with a transformer backbone network adept at learning an implicit latent history h t = {h z t , h z,a t } at each time step.This innovative architecture enables UniZero to overcome the two aforementioned limitations of MuZero-style algorithms by explicitly separating the latent state z t from the implicit latent history h t .Please note that we do not employ a decoder to reconstruct z t into ôt .Although reconstruction is a common technique in prior research [26] to shape representations, our empirical findings from the experiments (see Section 4.4) show that omitting this decoding loss does not diminish performance.This observation supports our hypothesis that learned latent states only need to capture information relevant to decision-making, thereby making reconstruction unnecessary for decision tasks.</p>
<p>Inference.During inference, UniZero can leverage both the long-term memory preserved in KV Cache and encoded information from the current observation.The latent world model can generate more accurate internal predictions, which in turn serves as the root/internal node for tree search.The synergy between these components significantly boost the efficiency and scalability of UniZero.</p>
<p>Joint Optimization of Model and Policy</p>
<p>In this paper, our primary focus is on online reinforcement learning settings.Algorithm 1 presents the pseudo-code for the entire training pipeline.In this subsection, we will present the core process of joint optimization of the model and policy (behaviour).UniZero maintains a replay buffer B that stores trajectories {o t , a t , r t , o t+1 , π t } (where π t is the MCTS improved policy induced in Section 3.3) and iteratively performs the following two steps:</p>
<ol>
<li>Experience Collection: Collect experiences into the replay buffer B by interacting with the environment.Notably, the agent employs a policy derived from MCTS, which operates within the learned latent space.2. Model and Policy Joint Update: Concurrent with data collection, UniZero performs joint updates on the decision-oriented world model, including the policy and the value functions, using data sampled from B.</li>
</ol>
<p>The joint optimization objective for the model-policy can be written as:
L UniZero (θ) . = E (ot,at,rt,ot+1,πt) H−1 0 ∼B H−1 t=0 β z ∥ẑ t+1 − sg( h(o t+1 ))∥ 2 2 next-latent prediction +β r CE(r t , r t ) reward prediction +β p CE(p t , π t ) policy prediction +β v CE(v t , vt ) value prediction(3)
Note that we also maintain a soft target world model [40] W = ( hθ , ḡθ , fθ ), which is an exponential moving average of current world model W 2. In Equation 3, H is the training context length, sg is the stop-grad operator, CE denotes cross-entropy loss function, h(o t+1 ) = zt+1 is the target latent state generated by the target encoder hθ , and vt signifies the bootstrapped n-step TD target:
vt = n−1 k=0 {γ k r t+k } + γ n fθ (z ≤t , a ≤t−1 )
. As the magnitudes of rewards across different tasks can vary greatly, UniZero adopts reward and value predictions as discrete regression problems [41], and optimizes by minimizing the cross-entropy loss.π t represents the improved policy through MCTS shown in Section 3.3.We optimize the dynamics network to predict π t , essentially seems a policy distillation process.Compared to policy gradient methods [29,42,43], this approach potentially offers better stability [10,39].The coefficients β z , β r , β p , β v are constant coefficients used to balance different loss items.Inspired by [7], UniZero has adopted the SimNorm technique, which is implemented after the final layer of the encoder and the last component of the dynamics network that predicts the next latent state.Essentially, this involves applying a L1 norm constraint to regularize the latent state space.As detailed in Section 4.4, latent normalization has been empirically proven to be crucial for enhancing the stability and robustness of training.</p>
<p>MCTS in Latent Space</p>
<p>RL agents need a memory M to accurately model future in tasks that require long-term dependencies.To effectively implement this memory mechanism, as depicted in Figure 3 (for simplicity, we use 1 as the starting timestep in this figure), we establish a KV Cache [44] for the memory, denoted by: KV M = {KV (z t−H , a t−H , . . ., z t , a t )}.When the agent encounters a new observation o t and needs to make a decision, it first utilizes the encoder to transform this observation into the corresponding latent state z t , which serves as the root node of the search tree.By querying the KV Cache, the keys and values from the recent memory (z t−Hinfer , a t−Hinfer , . . ., z t , a t ) are retrieved for the transformer-based latent world model.This model recursively predicts the next latent state ẑt+1 , the reward rt , the policy p t , and the value v t .The newly generated next latent state ẑt+1 functions as an internal node in the MCTS process.Subsequently, MCTS is executed within this latent space.Further details can be found in B.1.1.Upon completion of the search, the visit count set {N (z t , a t )} is obtained at the root node z t .These visit counts are then normalized to derive the improved policy π t :
π t = N (z t , a t ) 1/T bt N (z t , b t ) 1/T (4)
Here, T denotes the temperature, which modulates the extent of exploration [37].Actions are then sampled from this distribution for interactions with the environment.After each interaction, we save the transition (o t , a t , r t , d t , o t+1 ) along with the improved policy π t into the buffer, with the latter serving as the policy target in Eq. 3. By leveraging backward memory and forward search, UniZero demonstrates the potential to perform generalized and efficient long-term planning across a wide range of decision-making scenarios, as illustrated in Appendix D.2.</p>
<p>Experiments</p>
<p>We evaluate the performance of UniZero across 26 distinct games using the Atari 100k benchmark [45] (Section 4.2).Additionally, we perform a qualitative analysis of its long-term modeling capabilities using the long-term dependency benchmark, specifically the VisualMatch tasks, which necessitate extensive memory capacity (Section 4.3).To assess the impact of each component of UniZero, we conduct comprehensive ablation studies (Section 4.4) and a series of visual analyses (Appendix D.2). Due to its unified architecture and training paradigm, UniZero can seamlessly extend to a multi-task learning setup.Preliminary multi-task results on four Atari games in Appendix E validate the scalability and potential of UniZero in multi-task learning scenarios.Our implementations leverage the latest open-source MCTS framework, LightZero [14].For additional details, including specific environment settings, further experiments, and other implementation specifics, please refer to Appendices A, B, and C.1.</p>
<p>Experiments Setup</p>
<p>Environments.Introduced by SimPLe [45], the Atari 100k benchmark has been widely used in studies focusing on sample-efficient RL [46].It provides a robust measure of the sample efficiency of various algorithms.In addition to this, our experiments also include the VisualMatch [2], designed to evaluate long-term memory capabilities.This benchmark is set in grid-world scenarios with exploration, distraction, and reward phases [2].More details about environments can be found in Appendix A.</p>
<p>Baselines.In the Atari 100k benchmark, we employed three baselines: MuZero w/ SSL (stack=4), MuZero w/ SSL (stack=1), and EfficientZero (stack=4), all the variants are re-implemented using LightZero.For brevity, "w/ SSL" may be occasionally omitted in subsequent mentions.For long-term dependency benchmark, our primary baselines are MuZero (stack=1) and the SAC-Discrete variant algorithm combined with the GPT [47] backbone as proposed in [2].In our analysis, we refer to the latter as SAC-GPT.As mentioned in Section 3.1, due to the inherent challenges of UniZero (RNN) in modeling long-term dependencies, which result in its poor performance, we did not consider it as our baseline.We plan to delve into a more in-depth analysis of UniZero (RNN) in future research.Additional details regarding experimental setups are available in Appendix C.1.</p>
<p>Atari</p>
<p>Categorization.It is crucial to recognize that in Atari environments, certain games behave as POMDPs when the input is a single frame.This means they rely on historical information to make optimal decisions, examples include Pong and Seaquest.Conversely, other environments function as MDPs, where optimal decisions can be made based solely on a single frame, such as MsPacman and Boxing.We categorize the former as POMDP-like tasks and the latter as MDP-like tasks [18].Results.In this subsection, we compare UniZero with MuZero and its variants across these four representative Atari environments in Figure 4. Results demonstrate that in POMDP-like tasks (e.g., Pong and Seaquest), MuZero (stack1) performs poorly due to the lack of necessary contextual information for decision-making.In contrast, UniZero (stack1) with only single-frame input achieves performance comparable to, or even surpassing, MuZero (stack4) and EfficientZero (stack4).In MDP-like tasks (e.g., MsPacman and Boxing), MuZero (stack1) performs similarly to MuZero (stack4), and UniZero's performance is on par with both baselines.We report the complete results for 26 games on the Atari 100k benchmark in Appendix C.2, including additional model-based RL algorithms [26,27,29] as baselines.Overall, UniZero (stack1) outperforms MuZero (stack4) in 17 out of 26 games in the Atari 100k benchmark and demonstrates comparable performance in most of the remaining games.This reveals that UniZero is capable of modeling both short-and long-term dependencies simultaneously, demonstrating its versatility.</p>
<p>Long-Term Dependency Benchmark</p>
<p>In Figure 5, we compare the performance of UniZero and MuZero on the VisualMatch benchmark, which requires long-term memory.The green horizontal dashed line represents the final success rate of SAC-GPT [2] after training on 3 million environment steps.Due to the lack of context, MuZero performs poorly across all tasks, and the performance of SAC-GPT significantly declines as the memory length increases.In contrast, UniZero maintains a high success rate as the memory length increases, which supports our analysis in Section 3.1.</p>
<p>Ablation Study</p>
<p>This subsection and Appendix D evaluate the effectiveness of the key design choices in UniZero:</p>
<p>Related Work</p>
<p>MCTS-based RL.Algorithms like AlphaGo [49] and AlphaZero [17], which combine MCTS with deep neural networks, have significantly advanced board game AI.Extensions such as MuZero [10], Sampled MuZero [13], and Stochastic MuZero [12] have adapted this framework for environments with complex action spaces and stochastic dynamics.EfficientZero [36] and GumbelMuZero [11] have further increased the algorithm's sample efficiency.MuZero Unplugged [50,51] introduced reanalyze techniques, enhancing performance in both online and offline settings.LightZero [14] addresses real-world challenges and introduces a comprehensive open-source MCTS+RL benchmark.Studies like RAP [52] and SearchFormer [53] have applied MCTS to enhance the reasoning capabilities of language models [20].We analyze the challenges MuZero faces in modeling long-term dependencies in POMDPs and propose a transformer-based latent world model to address these challenges.</p>
<p>World Models.The concept of world models, first proposed in [54], enables agents to predict and plan future states by learning a compressed spatiotemporal representation.Subsequent research [6,26,42,27,28,29,55,7,10,36,35] has enhanced world models in both architecture and training paradigms.These studies generally follow three main routes based on training paradigms: (1) The Dreamer series [6,26,42] adopts an actor-critic paradigm, optimizing policy and value functions based on internally simulated predictions.Note that the model and behavior learning in this series are structured in a two-stage manner.Building on this, [27,28,29] leverage Transformer-based architectures to enhance sequential data processing, achieving significant sample efficiency and robustness.</p>
<p>(2) The TD-MPC series [55,7] demonstrates substantial performance gains in large-scale tasks by learning policies through local trajectory optimization within the latent space of the learned world model, specifically utilizing the model predictive control algorithm [56].The model and behavior learning in this series also follow a two-stage structure.(3) Research stemming from MuZero [10,36,35], grounded in the value equivalence principle [15], achieves joint optimization of the world model and policy [31,32] and employs MCTS for policy improvement.Despite these advancements, the effective integration of these approaches remains underexplored.In our paper, we provide a preliminary investigation into integrating scalable architectures and joint model-policy optimization training paradigms.A detailed qualitative comparison is presented in Appendix 9.</p>
<p>Conclusion and Future work</p>
<p>In this paper, we explore the efficiency of MuZero-style algorithms in environments that require long-term dependencies.Through qualitative analyses, we identify two key limitations in MuZero: under-utilization and entanglement.To address these issues and extend MuZero's applicability, we introduce UniZero, which integrates a transformer-based latent world model and MCTS.Our experimental results and visualizations demonstrate UniZero's efficacy and scalability across various environments.Our work currently employs the classic transformer, presenting opportunities for developing domain-specific techniques for decision-making scenarios, such as attention optimizations [57,58].Additionally, UniZero's potential as a foundational model for large-scale multi-task learning and continuous control remains an open area for future research, which we are committed to exploring.</p>
<p>A Environment Details</p>
<p>Figure 7: The Long-Term Dependency Benchmark, VisualMatch, organizes each task into 3 phases: Exploration, Distraction, Reward.In this benchmark, the duration (steps) of Phase 2 varies (denoted as memory_length), while the durations of the other phases remain fixed.The agent, depicted in purple, has a limited field of view, observing only nearby grids within white borders, and is unable to cross walls shown in black.This setup creates a POMDP environment.The agent is presented with a randomized target color (blue in this case) during Phase 1.</p>
<p>In Phase 2, the environment introduces random distractions, such as green apples.Finally, in Phase 3, the agent must navigate to the grid corresponding to the previously observed target color.As the memory length increases, the ability to model long-term dependencies in agents becomes increasingly critical for solving tasks.</p>
<p>A.1 Atari 100k Benchmark</p>
<p>Introduced by SimPLe [45], the Atari 100k benchmark is widely used in sample-efficient reinforcement learning research.This benchmark includes 26 diverse image-input Atari games with discrete action dimensions of up to 18, providing a diverse and robust measure of algorithm performance.Agents interact for 100,000 steps per game, equivalent to 400k environment frames, with frame skipping every 4 frames.</p>
<p>A.2 Long-Term Dependency Benchmark</p>
<p>This benchmark, specifically Visual Match, is designed to test long-term memory with adjustable memory lengths.As illustrated in Figure 7, these tasks are structured as grid-world scenarios and are divided into three phases: exploration, distraction, and reward.</p>
<p>• Exploration Phase: The agent observes a room with a random RGB color.</p>
<p>• Distraction Phase: Randomly appearing apples, served as random distractions.</p>
<p>• Reward Phase: The agent selects a block that matches the initial room color.</p>
<p>In our experiments, Phase 1 is set to 1 step, and Phase 3 is set to 15 steps.The target colors in Phase 3 are randomly chosen from three predefined colors: blue, red, and green.</p>
<p>Our setup differs from [2] primarily in the reward structure:</p>
<p>• Collecting apples in Phase 2 yields zero reward.</p>
<p>• A reward of 1 is given only upon goal completion in Phase 3, making the environment one of entirely sparse rewards.</p>
<p>• Additionally, in Visual Match, Phase 1 is set to 1 step, compared to 15 steps in [2].</p>
<p>The Visual Match task is designed to evaluate an agent's ability to manage long-term dependencies in decision-making processes.In this task, the agent must remember a specific color encountered during the exploration phase and later identify the corresponding grid in the reward phase, thus testing its long-term memory.The task features sparse rewards, with the agent earning a point only upon successfully completing the target.Additionally, the environment is partially observable, limiting the agent's field of view to a 5x5 grid area at each step.This restriction compels the agent to make decisions based on incomplete information, mirroring many real-world scenarios.3Procedure MCTS(z t , KV M , W, sim):</p>
<p>B Implementation Details</p>
<p>// The following process will repeat sim iterations/simulations, where i represents the current simulation step.Require :N i (ẑ, a), Q i (ẑ, a), P i (ẑ, a), R i (ẑ, a), Z i (ẑ, a) Initialize root node ← z t repeat a * ← PUCT(Q, P, N ) as in Equation 5 until N i (ẑ l t , a l ) = 0 Evaluate the leaf root node ẑl t using W:
p l t , v l t ← f θ (ẑ l t , KV M ), ẑl+1
t , rl t ← g θ (ẑ l t , a l , KV M ) and stored the dynamics and decision quantities into the corresponding tables R i (ẑ, a), Z i (ẑ, a), P i (ẑ, a) for each ẑ along the search path do
Q i+1 (ẑ, a) = Ni(ẑ,a)•Qi(ẑ,a)+v(ẑ) Ni(ẑ,a)+1 N i+1 (ẑ, a) = N i (ẑ, a) + 1 return π t = Normalization({N i+1 (z t , a t )|a t ∈ A})
Here, we present the complete training pipeline of UniZero in Algorithm 1.The training_loop of the UniZero algorithm consists of two primary procedures: 2. update_world_model: This procedure jointly optimizes the world model and the policy.</p>
<p>UniZero updates the decision-oriented world model, policy, and value using samples from B.</p>
<p>collect_steps in Algorithm 1 is defined as num_episodes_each_collect × episode_length.</p>
<p>In our experiments, num_episodes_each_collect is typically set to 8. The parameter world_model_iterations in Algorithm 1 is calculated as collect_steps × replay_ratio (the ratio between collected environment steps and model training steps) [59].In our experiments, replay_ratio is usually set to 0.25.</p>
<p>B.1.1 MCTS in the Learned Latent Space</p>
<p>As delineated in Algorithm 1, the MCTS procedure [10] within the learned latent space comprises 3 phases in each simulation step i.The total iterations/simulation steps in a single search process is denoted as sim:</p>
<p>• Selection: Each simulation initiates from the internal root state z t , which is the latent state encoded by the encoder h θ given the current observation o t .The simulation proceeds until it reaches a leaf node ẑl t , where t signifies the search root node is at timestep t, and l indicates it's a the leaf node.For each hypothetical timestep k = 1, ..., l of the simulation, actions are chosen based on the Predictor Upper Confidence Bound applied on Trees (PUCT) [60] formula:
a k, * = arg max a Q(ẑ, a) + P (ẑ, a) b N (ẑ, b) 1 + N (ẑ, a) c 1 + log b N (ẑ, b) + c 2 + 1 c 2 (5)
where N represents the visit count, Q denotes the estimated average value, and P is the policy's prior probability.The constants c 1 and c 2 regulate the relative weight of P and Q.For the specific values, please refer to Table 6.For k &lt; l, the next state and reward are retrieved from the latent state transition and reward table as ẑk+1 = S ẑk , a k and rk = R ẑk , a k .• Expansion: At the final timestep l of the simulation i, the predicted reward and latent state are computed by the dynamics network g θ : rl , ẑl+1 = g θ ẑl , a l , KV M , and stored in the corresponding tables, R ẑl , a l = r l and S ẑl , a l = ẑl+1 .The policy and value are computed by the decision network f θ : p l , v l = f θ ẑl , KV M .A new internal node, corresponding to state z l , is added to the search tree.Each edge ẑl , a from the newly expanded node is initialized to {N s l , a = 0, Q s l , a = 0, P s l , a = p l }.</p>
<p>• Backup: At the end of the simulation, the statistics along the simulation path are updated.The estimated cumulative reward at step k is calculated based on vl , i.e., an (l − k)-TD bootstrapped value:
vk = l−1−k i=0 γ i rk+1+i + γ l−k vl(6)
where r are predicted rewards obtained from the dynamics network g θ , and v are obtained from the target decision network fθ .Subsequently, Q and N are updated along the search path, following the equations in the MCTS procedure described in 1.</p>
<p>Upon completion of the search, the visit counts N (ẑ, a) at the root node z t are normalized to derive the improved policy:
π t = I π (a|z t ) = N (z t , a) 1/T b N (z t , b) 1/T (7)
where T is the temperature coefficient controlling exploration.Finally, an action is sampled from this distribution for interaction with the environment.UniZero leverages key-value (KV) caching and attention mechanisms to enhance backward memory capabilities and employs Monte Carlo Tree Search (MCTS) to improve forward planning efficiency.By integrating these two technological directions, UniZero significantly advances more general and efficient planning.</p>
<p>B.2 Architecture Details</p>
<p>Encoder.In the Atari 100k experiment, our observation encoder architecture principally follows the framework described in the LightZero paper [14], utilizing the convolutional networks.A notable modification in UniZero is the addition of a linear layer at the end, which maps the original threedimensional features to a one-dimensional latent state of length 768 (denoted as latent state dim, D), facilitating input into the transformer backbone network.Additionally, we have incorporated a SimNorm operation, similar to the details described in the TD-MPC2 paper [7].Let V (=8 in all our experiments) be the dimensionality of each simplex g, constructed from L (= D / V ) partitions of z.</p>
<p>SimNorm applies the following transformation:
z sim_norm . = [g 1 , . . . , g i , . . . , g L ] , g i = e z i:i+V /τ V j=1 e z i:i+V /τ ,(8)
where z sim_norm is the simplicial embedding [61] of z, [•] denotes concatenation, and τ &gt; 0 is a temperature parameter that modulates the sparsity of the representation.We set τ to 1.As demonstrated in 4.4, SimNorm is crucial for the training stability of UniZero.</p>
<p>For the encoder used in the Long-Dependency Benchmark, we employed a similar conv.network architecture, with a latent state of length 64.Specifics can be found in the related table (see Table 2).</p>
<p>Dynamics Head and Decision Head.Both the dynamics head and the decision head utilize two-layer linear networks with GELU [62] activation functions.Specifically, the final layer's output dimension for predicting value and reward corresponds to the support size (refer to B.3) [10,41].For predicting policy, the output dimension matches the action space size.For predicting the next latent state, the output dimension aligns with the latent state dimension, followed by an additional SimNorm normalization operation.In the context of Atari games, this dimension is set to 768, whereas for VisualMatch, it is configured to 64.</p>
<p>Transformer Backbone.Our transformer backbone is based on the nanoGPT project, as detailed in Table 5.For each timestep input, UniZero processes two primary modalities.The first modality involves latent states derived from observations, normalized in the final layer using SimNorm, as discussed above.The second modality pertains to actions, which are converted into embeddings of equivalent dimensionality to the latent states via a learnable nn.Embedding layer.For continuous action spaces, these can alternatively be embedded using a learnable linear layer.Notably, rewards are not incorporated as inputs in our current framework.This choice is based on the rationale that rewards are determined by observations and actions, and thus do not add additional insight into the decisionmaking process.Furthermore, our approach does not employ a return-conditioned policy [23,63], leaving the potential exploration of reward conditions to future work.Each timestep's observed results and corresponding action embeddings are added with a learnable positional encoding, implemented through nn.Embedding, as shown in Table 3.While advanced encoding methods like rotary positional encoding [64] and innovate architectures of transformer [57] exist, their exploration is reserved for future studies.Detailed hyper-parameters can be found in Appendix B.3.</p>
<p>UniZero (RNN)</p>
<p>. This variant employs a training setup akin to UniZero but utilizes a GRU [38] as the backbone network.During training, all observations are utilized.During inference, the hidden state of the GRU is reset every H infer steps.The recursively predicted hidden state h t and observation embedding z t serve as the root node of the MCTS.The recursively predicted hidden state h t and</p>
<p>Submodule Output shape</p>
<p>Input ((z 1:H , a 1:H )) 2H × D Add ((z 1:H , a 1:H ) + w 1:H ) Table 4: Details of Transformer block.M HSA refers to multi-head self-attention and F F N refers to feed-forward networks.Dropout mechanism can prevent over-fitting.</p>
<p>Submodule</p>
<p>Module alias Output shape Input features (label as
x 1 ) - 2H × D Multi-head self attention + Dropout(p) MHSA 2H × D Linear1 + Dropout(p) Residual (add x 1 ) LN1 (label as x 2 ) Linear2 + GELU FFN 2H × D Linear3 + Dropout(p) 2H × D Residual (add x 2 ) 2H × D LN2 H × D
predicted latent state ẑt serve as the internal nodes.At the root node, due to the limited memory length of the GRU, the recurrent hidden state h t may not fully capture the historical information.At the internal nodes, the issue is exacerbated by the accumulation of errors, leading to inaccurate predictions and consequently limiting performance.For an illustration of the training process, please refer to Figure 8.</p>
<p>B.3 Hyperparameters</p>
<p>We use the same hyperparameters across all tasks, unless otherwise noted.Table 6 lists the key hyperparameters for UniZero, which mostly align with those in [14].Table 7 shows the key hyperparameters for MuZero w/ SSL, MuZero w/ Context and UniZero (RNN).EfficientZero shares the similar key hyperparameters as MuZero w/ SSL, with an added LSTM [65] for predicting value_prefix, as specified in [14].Note that in our reimplementation, to save computation time, reanalyze_ratio is set to 0.</p>
<p>B.4 Computation Cost</p>
<p>Each of our experimental instances is executed on a Kubernetes cluster utilizing a single NVIDIA A100 80G GPU, 24 CPU cores, and 100GB of RAM.Under these conditions and with the hyperparameters outlined in 6, UniZero can train Atari agents for 100k steps in approximately 4 hours (see Figure 4), and complete 1M steps on VisualMatch (with a memory length of 500) in roughly 30 hours (see Figure 5).</p>
<p>C Additional Experimental Details C.1 Experiments Setup</p>
<p>To evaluate the effectiveness and scalability of UniZero, we conducted experiments on 26 games from the image-based Atari 100k benchmark (see Section 4.2).Details of our Atari environment setup are provided in Section B.3.Observations are formatted as (3,64,64) for RGB images with single frames (stack=1), and (4,64,64) for grayscale images with four stacked frames (stack=4), differing from the (4,96,96) format used in [36,14].All implementations are built on the latest open-source framework LightZero [14].During training, all observations are utilized.The recursively predicted hidden state ht and observation embedding zt serve as the root node.The recursively predicted hidden state ht and predict latent state ẑt serve as the internal nodes of MCTS.During inference, the GRU hidden state is reset every Hinfer steps.However, potential inaccuracies may arise from the recursively predicted hidden state ht due to the limited memory length of the GRU.</p>
<p>Baselines.For the Atari 100k benchmark, we compare against MuZero w/ SSL (stack=4) [10,14], MuZero w/ SSL (stack=1), and EfficientZero (stack=4) across six representative games.Since MuZero w/ SSL (stack=4) performed best, we compare UniZero only with MuZero w/ SSL (stack=4) in the remaining 20 games.Here, w/ SSL includes a self-supervised regularization component similar to EfficientZero [36].Our versions of MuZero and EfficientZero are reimplemented using LightZero.To conserve computational resources, we set the reanalyze_ratio [50] for all variants to 0 and did not use the second and third enhancement features from the EfficientZero methodology.For long-term dependency benchmarks, we compare against MuZero (stack=1) and the SAC-Discrete algorithm using a GPT network backbone [2], referred to as SAC-GPT.</p>
<p>C.2 Atari 100k Full Results</p>
<p>Table 8 presents the comparison between UniZero (stack=1) and our rerun of MuZero w/ SSL (stack=4), along with the original MuZero and EfficientZero using lookahead search, and the RNNbased Dreamerv3 [26] and the transformer-based IRIS [27] and STORM [29] without lookahead search.Results for MuZero, EfficientZero and IRIS [27] are from [27], and those for Dreamerv3 and STORM are from [29].Note that both UniZero and MuZero w/ SSL are our re-implementations using a reanalyze_ratio of 0, with nearly identical hyperparameters across all games and no tuning.Thus, they can be fairly compared with each other, while other methods are provided for reference.UniZero outperforms MuZero w/ SSL in 17 out of 26 games, while maintaining comparable performance in most of the remaining games.</p>
<p>Figure 9 shows the learning curves for all 26 environments, comparing UniZero (stack=1) with MuZero w/ SSL (stack=4).Our proposed UniZero is represented in blue, while MuZero w/ SSL is in orange.Solid lines represent the average of three different seed runs, and shaded areas indicate the 95% confidence intervals.</p>
<p>Table 8: Returns on the 26 Atari games after 100k Env Steps (around 2 hours of real-time experience), along with human-normalized aggregate metrics.Note that both UniZero and MuZero w/ SSL are our reimplementations using a reanalyze_ratio of 0, with nearly identical hyperparameters across all games and no tuning.Thus, they can be fairly compared with each other, while other methods are provided for reference.
L decode_reg = ∥o t − d θ (z t )∥ 1 + L perceptual (o t , d θ (z t )) where z t = h θ (o t )
The first term represents the reconstruction loss, while the second term corresponds to the perceptual loss, similar to the approach described in [2].In our ablations, the loss coefficient is set to 0.05.It is important to note that in VisualMatch, we solely utilize the first term.</p>
<p>• Target World Model:</p>
<p>-Soft Target (default): Uses an EMA target model [40] for target latent state and target value.</p>
<p>-Hard Target: Hard copies the target world model every 100 training iterations.</p>
<p>-No Target: Does not use the target world model, i.e., the target of the next latent state is produced from the current world model.</p>
<p>From the ablation results, we observe the following findings:</p>
<p>• As shown in Figure 6, for inference context length (H infer = 4/8), performance at H infer = 4 consistently outperforms H infer = 8, regardless of the Transformer's layer count.This suggests that a shorter context is sufficient for Atari tasks like Pong.Additionally, performance improves with an increased layer count, indicating a moderate model size suffices.However, for VisualMatch, a context length matching the episode length is crucial as the agent needs memory of the target color from phase one.Thus, we set the train context length = 16 + memory_length.As shown in Figure 10, performance increases with model size, indicating that capturing long dependencies requires deeper transformer blocks.• As shown in Figure 6, with a fixed inference context length (H infer = 4), performance generally improves with an increased training context length (H), implying that forecasting further into the future aids representation learning, consistent with literature insights.• SimNorm [7] yields the best results for latent state processing, followed by Softmax, while Sigmoid fails to converge.This underscores the importance of appropriate normalization in the latent space for training stability.SimNorm introduces natural sparsity by maintaining the L1 norm of the latent state at a fixed value, managing gradient magnitudes within a stable range.Gradient explosions often occur without normalization.• In Pong and VisualMatch, decode regularization has negligible impact on performance, corroborating that more decision-relevant information is required in the latent states, and reconstructing original images is unnecessary.• As illustrated in Table 11, employing a soft target world model yields the most stable performance.</p>
<p>In contrast, the hard target demonstrates some instability, and the absence of a target world model leads to non-convergence in Pong and results in NaN gradients in VisualMatch.Consequently, we do not plot the No target variant in VisualMatch.This behavior parallels the role of target networks in algorithms such as DQN [40].</p>
<p>D.2 World Model Analysis</p>
<p>VisualMatch.In Figure 12 and Figure 13, we present the predictions of the learned world model in one success and one fail episode of VisualMatch (MemoryLength=60), respectively.The first row indicates the predicted reward and true reward.The second row displays the original image frame.The third row outlines the predicted prior policy, and the fourth row describes the improved (MCTS) policy induced by MCTS based on the prior policy.For the sake of simplicity, we have only illustrated the first two steps (t = {2, 3}) and the last two steps (t = {60, 61}) of the distraction phase.Please note that at each timestep, the agent performs the action with the highest probability value in the fourth row.As observed, the reward is accurately predicted in both cases, and the MCTS policy has shown further improvement compared to the initial predicted prior policy.For example, in Figure 12, at timestep 75, action 3, which represents moving to the right, is identified as the optimal action because the target color, green, is located on the agent's right side.While the predicted prior policy still allocates some probability to actions other than action 3, the MCTS policy refines this distribution, converging more towards action 3.</p>
<p>Figure 15 shows the attention maps of the trained world model.It can be observed that in the initial layers of the Transformer, the attention is primarily focused on the first time step (which contains the target color that needs to be remembered) and the most recent few time steps, mainly for predicting potential dynamic changes.In higher-level layers, sometimes, such as in Layer3-Head2, the attention is mainly concentrated on the current time step, whereas at other times, such as in Layer4-Head4, there is a relatively broad and dispersed attention distribution, possibly indicating the fusion of some learned higher-level features.</p>
<p>Pong.Similarly, in Figure 14, we present the predictions of the world model in one trajectory of Pong.The first row indicates the predicted reward and true reward.The second row displays the original image frame.The third row outlines the predicted prior policy, and the fourth row describes the improved (MCTS) policy induced by MCTS based on the prior policy.Please note that the image in the second row (original image) has already been resized to (64,64) from the raw Atari image, so there may be some visual distortion.At each timestep, the agent performs the action with the highest probability value in the fourth row.Throughout all timesteps, the true reward remains zero due to the absence of score events.Unizero's world model can accurately predict this, with all predicted rewards consistently remaining zero.At the 8th timestep, the agent controlling the right green paddle successfully bounces the ball back.At the 7th timestep, the agent should perform the upward action 2; otherwise, it might miss the opportunity to catch the ball.The MCTS policy further concentrates the action probability on action 2 compared to the prediction policy, demonstrating the policy improvement process of MCTS.The third row outlines the predicted prior policy, and the fourth row describes the improved (MCTS) policy induced by MCTS based on the prior policy.For the sake of simplicity, we have only illustrated the first two steps (t = {2, 3}) and the last two steps (t = {60, 61}) of the distraction phase.At timestep 75, action 3, which corresponds to moving to the right, is identified as the optimal action because the target color, green, is located on the agent's right side.Although the predicted prior policy assigns some probability to actions other than action 3, the MCTS policy refines this distribution, converging more decisively towards action 3.</p>
<p>In Figure 16, we plot the attention maps in one trajectory (train_context_length is 10, with each time step consisting of two tokens, namely the latent state and the action) of Pong.It can be observed that across various levels, attention is primarily on data from the most recent frames.This is closely related to the short-term dependency characteristic of Pong.Utilizing information from only the recent frames is sufficient for dynamic prediction and policy-value learning.</p>
<p>E Multi-task Learning</p>
<p>UniZero's decoupled yet streamlined design has enabled it to excel in various single-task problems with differing degrees of dependency (Section 4.1).By leveraging the transformer backbone, we can automatically address these varying levels of dependency.Owing to its unified architecture and training paradigm, UniZero can seamlessly extend to a multi-task learning setup.This section presents preliminary results on four Atari games: Pong, MsPacman, Seaquest, and Boxing, to validate its potential in multi-task learning.Note that unless stated otherwise, the multi-task hyperparameters are consistent with those in Table 6.</p>
<p>Architecture The observation space for each task is uniform, represented as a (3,64,64) image.We set full_action_space=True [34], which means that the action space for all tasks is 18.Compared to single-task settings, our primary modification is the establishment of independent decision heads and dynamics heads for each task, following the approach of [66], which introduces only minimal additional parameters.The Transformer backbone network and encoder are shared across tasks.In our initial experiments, we found that using LayerNorm in the encoder was crucial for performance.This is likely due to significant differences in internal state statistics at the early stages of learning for different tasks, leading to performance degradation when using BatchNorm [67].</p>
<p>Training.Each task has its own collector, which sequentially collects trajectories and stores them in their respective replay buffers.During training, we sample task_batch_size=32 samples from different tasks, combine them into a large minibatch, apply the same loss function as in Equation 3 for each task, average the task losses to obtain the final total loss, backpropagate the gradients, and update the entire network.</p>
<p>Results.As illustrated in Figure 17, with minimal adjustments, joint multi-task training on the Atari games Pong, MsPacman, Seaquest, and Boxing achieved comparable sample efficiency across all tasks when compared to single-task learning outcomes.This underscores the potential of UniZero as a scalable latent world model for training generalized agents.It is noteworthy that in our preliminary experiments, we also explored multi-task gradient correction methods such as PCGrad [68] and CAGrad [69], but found their benefits to be minimal.Consequently, these techniques are not included in our experimental results, although a more in-depth analysis is reserved for future work.Future research will focus on effective task balancing, learning dynamics, information reuse in MCTS, the integration of multiple modalities and tasks, and advanced pretraining and fine-tuning techniques, among other areas.</p>
<p>F Comparison with Prior Works</p>
<p>To provide a clear comparison, we present Table 9 outlining the key differences between UniZero and recent approaches [7,10,26,27,28,29] in world modeling.The attributes considered include the ).It can be observed that in the initial layers of the Transformer, the attention is primarily focused on the first time step (which contains the target color that needs to be remembered) and the most recent few time steps, mainly for predicting potential dynamic changes.In higher-level layers, sometimes, such as in Layer3-Head2, the attention is mainly concentrated on the current time step, whereas at other times, such as in Layer4-Head4, there is a relatively broad and dispersed attention distribution, possibly indicating the fusion of some learned higher-level features.</p>
<p>Figure 16: Attention maps in one trajectory (train_context_length is 10, with each time step consisting of two tokens, namely the latent state and the action.) of Pong.It can be observed that across various levels, attention is primarily on data from the most recent frames.This is closely related to the short-term dependency characteristic of Pong.Utilizing information from only the recent frames is sufficient for dynamic prediction and policy-value learning.</p>
<p>Figure 1 :
1
Figure 1: Comparison between the UniZero and MuZero-style architectures during training and inference phases.Left: In the MuZero-style architecture, the recursively unrolled latent representation s k t is tightly coupled with historical information (entanglement).During training, it solely utilizes the initial observation of the sequence, resulting in under-utilization.During inference, the recursively predicted latent representation s k t−k is used as the root node in MCTS, potentially causing inaccuracies due to compound errors, especially in scenarios requiring long-term dependencies.Right: UniZero adopts a transformer-based latent world model that explicitly disentangles the latent states from implicit latent history and utilizes all observations during the training phase.For inference, the directly encoded latent state zt is employed as the root node.Supported by a more complete and fully accessible context M = (zt−H infer , at−H infer , . . ., zt, at), this approach enhances the accuracy of predictions and facilitates more effective long-term planning within the latent space.</p>
<p>Figure 3 :
3
Figure 3: MCTS in the learned latent space.The process begins with a new observation o1, which is encoded into a latent state z1.This latent state serves as the root node for the MCTS.The previous keys and values of recent memory are retrieved from the transformer's KV Cache KVM .Subsequently, the search tree utilizes the world model to predict the next latent state ẑ (which serves as an internal node), reward r, policy p, and value v, conditioned on the retrieved KV, recursively.These predictions are then employed to conduct MCTS, ultimately resulting in an improved policy π.</p>
<p>Figure 6 :
6
Figure 6: Ablation study in Pong validates the efficiency of key design choices in UniZero.(a-c) UniZero demonstrates robust performance with increasing model size and training context length.(d) Proper latent space normalization (e.g.SimNorm) is essential for ensuring training stability.(e) For decision-making tasks, decode regularization has a negligible impact.The solid line represents the mean of 3 runs; the shaded areas are 95% confidence intervals.</p>
<p>Figure 6
6
Figure 6 presents a series of ablation results on Pong while the ablation results on VisualMatch and related detail settings are included in Appendix D. Based on these experiment results, we can conclude the following key findings: (1) Performance at H infer = 4 consistently surpasses H infer = 8, regardless of the model size.A shorter context is sufficient for tasks like Pong.For VisualMatch, context length should match the episode length to retain target color memory.This suggests that we could dynamically select the inference context length H infer based on the long-term dependency requirements of the task, which will be addressed in our future work.(2) Performance generally improves with longer Train Context Length (H), suggesting that forecasting longer future aids representation learning, consistency with findings in [48].(3) SimNorm yields the best results compared with Softmax and Sigmoid.It emphasizes the critical role of proper latent space normalization in maintaining stability.SimNorm introduces natural sparsity by constraining the L1 norm of the latent state to a fixed constant, thereby ensuring stable gradient magnitudes.(4) In both Pong and Visual-Match, Decode Regularization has a negligible impact, supporting our analysis that decision-relevant information in latent states is more critical than observation reconstruction.More detailed visual analyses and preliminary multi-task results can be found in Appendix D.2 and E.</p>
<p>z 1:H , a 1:H )) 2 * H × D Positional encoding Transformer blocks ×N (implicit) Latent history ((h z 1:H , h z,a 1:H )) Decision head (p 1:H , v 1:H ) Dynamic head (ẑ 1:H , r1:H )</p>
<p>Figure 8 :
8
Figure 8: Training pipeline of UniZero (RNN).During training, all observations are utilized.The recursively predicted hidden state ht and observation embedding zt serve as the root node.The recursively predicted hidden state ht and predict latent state ẑt serve as the internal nodes of MCTS.During inference, the GRU hidden state is reset every Hinfer steps.However, potential inaccuracies may arise from the recursively predicted hidden state ht due to the limited memory length of the GRU.</p>
<p>Figure 9 :
9
Figure 9: comparison of UniZero with MuZero w/ SSL on the Atari 100k benchmark with reanalyze_ratio=0.Our proposed UniZero is represented in blue, while MuZero w/ SSL is in orange.UniZero outperforms MuZero w/ SSL in 17 out of 26 games, while maintaining comparable performance in most of the remaining games.The solid line represents mean of 3 runs; the shaded areas are 95% confidence intervals.</p>
<p>Figure 10 :Figure 11 :
1011
Figure 10: Ablation study in VisualMatch (MemoryLength=60).Mean of 3 runs, shaded areas are 95% confidence intervals.</p>
<p>Figure 12 :
12
Figure 12: Predictions of the world model in one success episode of VisualMatch (MemoryLength=60).The first row indicates the predicted reward and true reward.The second row displays the original image frame.The third row outlines the predicted prior policy, and the fourth row describes the improved (MCTS) policy induced by MCTS based on the prior policy.For the sake of simplicity, we have only illustrated the first two steps (t = {2, 3}) and the last two steps (t = {60, 61}) of the distraction phase.At timestep 75, action 3, which corresponds to moving to the right, is identified as the optimal action because the target color, green, is located on the agent's right side.Although the predicted prior policy assigns some probability to actions other than action 3, the MCTS policy refines this distribution, converging more decisively towards action 3.</p>
<p>Figure 13 :
13
Figure 13: Predictions of the world model in one fail episode of VisualMatch (MemoryLength=60).The first row indicates the predicted reward and true reward.The second row displays the original image frames.The third row outlines the predicted prior policy, and the fourth row describes the improved (MCTS) policy induced by MCTS based on the prior policy.</p>
<p>Figure 17 :
17
Figure 17: Performance comparison between multi-task and single-task settings on four Atari games: Pong, Ms. Pac-Man, Seaquest, and Boxing.Multi-task training demonstrated comparable sample efficiency across all tasks relative to single-task learning.This highlights the potential of UniZero as a scalable latent world model for training generalized agents.The solid line represents the mean of 3 runs, with the shaded areas indicating the 95% confidence intervals.</p>
<p>Figure 15 :
15
Figure15: Attention maps in one success episode of VisualMatch (MemoryLength=60) (Note that the train_context_length is 1 + 60 + 15 = 76, with each time step consisting of two tokens, namely the latent state and the action.).It can be observed that in the initial layers of the Transformer, the attention is primarily focused on the first time step (which contains the target color that needs to be remembered) and the most recent few time steps, mainly for predicting potential dynamic changes.In higher-level layers, sometimes, such as in Layer3-Head2, the attention is mainly concentrated on the current time step, whereas at other times, such as in Layer4-Head4, there is a relatively broad and dispersed attention distribution, possibly indicating the fusion of some learned higher-level features.</p>
<p>3, we explore how to conduct efficient MCTS
MuZero-styleUniZeroArchitecturesArchitectureEntanglementDisentanglementdeci. headdyna. headdeci. headdyna. headdeci. headdyna. headdeci. headdyna. headTrainingUniZero Transformer Backbone......enc.enc.Under-utilizationenc.enc.enc.enc.InferenceIncomplete ContextComplete Context
enc.</p>
<p>Table 1 :
1
Qualitative Comparison of Different Variants.MuZero utilizes only the first observation as input during training.MuZero w/ SSL incorporates a state regularization loss for latent representation, which is effective in MDP tasks.MuZero w/ Context and UniZero (RNN) both suffer from performance issues due to their reliance on recurrent mechanisms for historical information, resulting in partially accessible contexts.In contrast, UniZero, with its transformer-based architecture, disentangles latent states from the implicit latent history, leveraging the full observation sequence during training and ensuring a more complete and fully accessible context.Figure 2: Performance comparison of UniZero and MuZero variants in Pong under approximate MDP and POMDP settings.Left: MDP setting.Right: POMDP setting.In both scenarios, UniZero consistently outperforms the other approaches, demonstrating the robustness and versatility of our architecture.MuZero w/ SSL shows good sample efficiency in the MDP setting but fails to converge in the POMDP setting due to entanglement issues.MuZero w/ Context and UniZero (RNN) struggle to learn effectively in both settings due to prediction errors arising from the incomplete context.
AlgorithmDisentanglement Obs. Full Utilization State RegularizationContext AccessMuZero××××MuZero w/ SSL××✓×MuZero w/ Context×××partially accessibleUniZero (RNN)✓✓✓partially accessibleUniZero✓✓✓fully accessible10 20PongNoFrameskip-v4 (stack=4) UniZero MuZero MuZero w/ SSL MuZero w/ Context UniZero (RNN)10 20PongNoFrameskip-v4 (stack=1) UniZero MuZero MuZero w/ SSL MuZero w/ Context UniZero (RNN)Return0Return0101020200M0.1M 0.2M 0.3M 0.4M 0.5M Env Steps0M0.1M 0.2M 0.3M 0.4M 0.5M Env Steps</p>
<p>Figure4: Performance comparison on representative Atari Games.In POMDP-like tasks (e.g., Pong, Seaquest), UniZero outperforms MuZero (stack1) and either matches or exceeds the performance of MuZero (stack4) and EfficientZero (stack4).In MDP-like tasks (e.g., MsPacman, Boxing), MuZero (stack1) performs similarly to MuZero (stack4), and UniZero matches both, demonstrating UniZero's versatility in modeling both short-and long-term dependencies.The solid line represents the mean of 5 runs; the shaded areas are 95% confidence intervals.Performance comparison on VisualMatch with increased memory lengths.MuZero consistently underperformed across all tasks, primarily due to insufficient context information.The performance of SAC-GPT significantly deteriorated as the memory length increased.In contrast, UniZero maintained a high success rate even with extended memory lengths, owing to its robust long-term dependency capabilities.
UniZero (stack1) MuZero w/ SSL (stack1) MuZero w/ SSL (stack4) EfficientZero (stack4)1000 1200SeaquestNoFrameskip-v4 UniZero (stack1) MuZero w/ SSL (stack1) MuZero w/ SSL (stack4) EfficientZero (stack4)2000 2500MspacmanNoFrameskip-v4 UniZero (stack1) MuZero w/ SSL (stack1) MuZero w/ SSL (stack4) EfficientZero (stack4)50 75BoxingNoFrameskip-v4 UniZero (stack1) MuZero w/ SSL (stack1) MuZero w/ SSL (stack4) EfficientZero (stack4)Return200 400 600 800500 1000 1500 ReturnReturn75 50 25 0 2500M0.1M 0.2M 0.3M 0.4M 0.5M Env Steps00M0.1M 0.2M 0.3M 0.4M 0.5M Env Steps1000MEnv Steps 0.1M 0.2M 0.3M 0.4M 0.5M0.2 0.4 0.6 0.8 1.0 Success RateVisualMatch (memory_length = 60) UniZero MuZero SAC-GPT0.2 0.4 0.6 0.8 1.0 Success RateVisualMatch (memory_length = 100) UniZero MuZero SAC-GPT0.2 0.4 0.6 Success Rate 0.8 1.0VisualMatch (memory_length = 250) UniZero MuZero SAC-GPT0.2 0.4 0.6 Success Rate 0.8VisualMatch (memory_length = 500) UniZero MuZero SAC-GPT0.00M0.2M 0.4M 0.6M 0.8M Env Steps1M0.00M0.2M 0.4M 0.6M 0.8M Env Steps1M0.00M 0.25M 0.5M 0.75M 1M 1.25M 1.5M Env Steps0.0Env Steps 0M 0.25M 0.5M 0.75M 1M 1.25M 1.5MFigure 5:</p>
<p>•</p>
<p>Model Size (n_layer) across different Inference Context Lengths (H infer = 4/8) with fixed Train Context Length (H = 8).
• Train Context Length (H) with fixed Inference Context Length (H infer = 4).
[2]atent Normalization: Comparison of normalization techniques that employed in the latent state, including SimNorm (default), Softmax and Sigmoid.•DecodeRegularization:Integrate a decoder on top of the latent state: ôt = d θ (z t ).Followed by an additional training objective:L decode_reg = ∥o t − d θ (z t )∥ 1 + L perceptual (o t , d θ (z t )), where z t = h θ (o t ).The first term is the reconstruction loss, and the second term is the perceptual loss[2].</p>
<p>Due to the variations in the parameters of the world model.We need clear the old Key-Value Cache Clear the Key-Value Cache: KV M = {} Compute target TD-value vt , and target next latent state zt+1 according to the target world model W Optimize the world model and policy jointly according to Equation
B.1 Algorithm DetailsAlgorithm 1: UniZeroProcedure training_loop():for train_iterations doCreate a Key-Value Cache for the memory: KV M = {}collect_experience(collect_steps)for world_model_iterations doupdate_world_model()// Procedure collect_experience(n):o
0 ← env.reset() for t = 0 to n − 1 do z t ← h θ (o t ) Sample a t ∼ π t = π(a t |z t ), which is obtained through MCTS(z t , KV M , W) Add the latest Key-Value Cache KV (z t , a t ) to KV M o t+1 , r t , d t ← env.step(a t ) if d t = 1 then o t+1 ← env.reset()B ← B ∪ {o t , a t , r t , o t+1 , π t } n−1 t=0 Procedure update_world_model(): Sample a mini-batch of sequences {(o t , a t , r t , o t+1 , π t ) i+H−1 t=i } ∼ B // where H is the training context length.</p>
<p>Table 2 :
2
[7]hitecture of the encoder for VisualMatch.The size of the submodules is omitted and can be derived from the shape of the tensors.LeakyReLU refers to the leaky rectified linear units used for activation, while Linear represents a fully-connected layer.SimNorm[7]operations introduces natural sparsity by constraining the L1 norm of the latent state to a fixed constant, thereby ensuring stable gradient magnitudes.Conv denotes a CNN layer, characterized by kernel = 3, stride = 1, and padding = 1.BN denotes the batch normalization layer.
SubmoduleOutput shapeInput image (o t )3 × 5 × 5Conv1 + BN1 + LeakyReLU 16 × 5 × 5Conv2 + BN2 + LeakyReLU 32 × 5 × 5Conv3 + BN3 + LeakyReLU 64 × 5 × 5AdaptiveAvgPool2d64 × 1 × 1Linear64SimNorm64</p>
<p>Table 3 :
3
Positional encoding module.w1:H is a learnable parameter matrix with shape H × D, and H refers to the sequence length and D refers to the latent state dimension, 768 for the Atari, 64 for the VisualMatch.</p>
<p>Table 5 :
5
Transformer-based latent world model (p1:H , v1:H , ẑ1:H , r1:H , h z 1:H , h z,a 1:H ) = f θ (z1:H , a1:H ).The hidden states (h z 1:H , h z,a 1:H ) in the final layer of the transformer are referred to as the implicit latent history.Positional encoding and Transformer block are explained in Table 3 and 4.</p>
<p>Bold numbers indicate the top methods between UniZero and MuZero w/ SSL, whereas underlined numbers specify the overall best methods.UniZero outperforms MuZero w/ SSL in 17 out of 26 games, while maintaining comparable performance in most of the remaining games.For simplicity, MZ stands for MuZero, EZ stands for EfficientZero and MZ w/ SSL stands for MuZero w/ SSL.
No lookahead searchLookahead searchGameRandom HumanIRIS DreamerV3 STORMMZEZ MZ w/ SSL UniZeroAlien227.87127.7420959984 530.0 808.5490713Amidar5.81719.5143139205391495885Assault222.4742.015247068015001263750532Asterix210.08503.385493210281734 2555811001016BankHeist14.2753.15364964119335130295BattleZone2360.0 37187.5 1307412250135407688 13871758710010Boxing0.112.170788015536042Breakout1.730.584311648414210ChopperCommand811.07387.8156542018881350111712501505CrazyClimber10780.5 35829.4 593249719066776 56937 83940856710666DemonAttack152.11971.020343031653527 130049891001Freeway0.029.6310342222910Frostbite65.24334.72599091316255296463310Gopher257.62412.5223637308240125632606201153Hero1027.0 30826.470371116111044309593153005150Jamesbond29.0302.846344550988517290305Kangaroo52.03035.083840984208637241801285Krull1598.02665.56616778284134891566334003364KungFuMaster258.5 22736.3 217602142026182 18813 30945121008600MsPacman307.36951.6999132726731266128114101397Pong-20.714.6151811-720-18-9PrivateEye24.9 69571.31008827781569766100Qbert163.9 13455.0746340545223952 1378239004056RoadRunner11.57845.0961515565175642500 177514005200Seaquest68.4 42054.76616185252081100446620UpNDown533.4 11693.23546766779852897 1726452133323#Superhuman (↑)0N/A10N/AN/A51433Mean (↑)0.0001.000 1.0461.121.27 0.562 1.9430.4140.433</p>
<p>Table 7 :
7
Key hyperparameters for MuZero w/ SSL, MuZero w/ Context and UniZero (RNN).In Section 4.4, we evaluate the effectiveness and scalability of UniZero's key designs in Pong.We also present the ablation results for VisualMatch in Figure10and target network ablation in Figure11.Below are the detailed settings for these ablations: • Model Size Across Different Inference Context Lengths (H infer = 4/8): Varying the number of layers in the Transformer backbone while keeping the number of heads fixed at 8. • Training Context Length (H) with Fixed Inference Context Length (H infer = 4): H is the length of the training sequence.• Latent Normalization: Comparing SimNorm (default) with Softmax and Sigmoid.These normalization operations are applied to the encoded latent state and the final component of the dynamics network that predicts the next latent state.• Decode Regularization: Adding a decoder to the latent state from the encoder: decoder: ôt = d θ (ẑ t )▷ Maps latent state to observations for regularization with an auxiliary training objective,
HyperparameterValuePlanningNumber of simulations in MCTS (sim) 50Inference Context Length (H infer )0 for MuZero w/ SSL, 4 for MuZero w/ ContextTemperature0.25Dirichlet noise alpha0.3Dirichlet noise weight0.25c 11.25c 219652Env and Replay bufferCapacity1, 000, 000SamplingUniformObs shape (Atari)(3, 64, 64) for stack1, (4, 64, 64) for stack4Obs shape (textitlong − term)(3, 5, 5)Reward clippingTrue (only for Atari)Num of frames stacked1 for stack1, 4 for stack4 (only for Atari)Num of frames skip4 (only for Atari)Length of game segment400 for Atari; memory_length+16 for long-termUse data augmentationTrueOptimizationTrain Context Length (H)10Replay ratio0.25Reanalyze ratio0Batch size256Replay ratio0.25OptimizerSGDLearning rate0.2 → 0.02 → 0.002 [36]SSL (self-supervised learning) loss coef. 2Reward loss coef.1Policy loss coef.1Value loss coef.0.25Policy entropy loss coef.0Number of reward/value bins101Discount factor0.997Frequency of target network update100Weight decay10 −4Max gradient norm5TD steps5
. collect_experience: This procedure gathers experiences (trajectories) {o t , a t , r t , d t } and the improved policy π t derived from Monte Carlo Tree Search (MCTS) into the replay buffer B. The agent interacts with the environment by sampling actions a t from the MCTS policy π t , which is generated by performing MCTS in the learned latent space.
AcknowledgementsWe extend our gratitude to several team-members of the Shanghai AI Laboratory and SenseTime for their invaluable assistance, support, and feedback on this paper and the associated codebase.In particular, we would like to thank Chunyu Xuan, Ming Zhang, and Shuai Hu for their insightful and inspiring discussions at the inception of this project.Table6: UniZero key hyperparameters.We use the same hyperparameters across all tasks other than specifically statement.For fair comparison, most hyperparameters are consistency with[14].We use long-term denotes long-term dependency benchmark for simplicity.type of sequence model used, the input information introduced during a single timestep, the method for obtaining latent representations, the approach to policy improvement, and the training pipeline.Hyperparameter• Sequence Model: The architecture employed for modeling sequences.• Input: The type of information fed into the sequence model at each timestep, where "Latent history" refers to the recurrent/hidden state as described in the respective papers.• Latent Representation: This refers to the technique employed to extract embeddings from each observation.For instance, an "Encoder" might be a neural network such as a Convolutional Network (ConvNet) for processing images or a Multi-Layer Perceptron (MLP) for handling vector observations.The term "VQ-VAE"[70]denotes the vector-quantized VAE, which is utilized to obtain a discrete code for the observation.Similarly, "Categorical-VAE"[26]represents the discrete VAE, which is used to derive the discrete distribution of the observation.• Policy Improvement: The method for enhancing the policy, with "PG" standing for Policy Gradient methods[43,26]and "MPC" standing for Model Predictive Control[55].• Training Pipeline: The training process involves a "two-stage" approach, where we first train the world model and then use the learned model to train the policy (behavior) through imagination.On the other hand, "model-policy joint training" refers to simultaneously learning the world model and the policy (and value), rather than following a two-stage process.This joint training approach offers several benefits, as discussed in[31,71,39].
Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, 2018MIT press</p>
<p>When do transformers shine in rl? decoupling memory from credit assignment. Tianwei Ni, Michel Ma, Benjamin Eysenbach, Pierre-Luc Bacon, Advances in Neural Information Processing Systems. 202436</p>
<p>Reza Mohammad, Artem Samsami, Janarthanan Zholus, Sarath Rajendran, Chandar, arXiv:2403.04253Mastering memory tasks with world models. 2024arXiv preprint</p>
<p>Memory gym: Partially observable challenges to memory-based agents. Marco Pleines, Matthias Pallasch, Frank Zimmer, Mike Preuss, The eleventh international conference on learning representations. 2022</p>
<p>When to trust your model: Model-based policy optimization. Michael Janner, Justin Fu, Marvin Zhang, Sergey Levine, Advances in neural information processing systems. 201932</p>
<p>Mastering atari with discrete world models. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba, arXiv:2010.021932020arXiv preprint</p>
<p>Td-mpc2: Scalable, robust world models for continuous control. Nicklas Hansen, Hao Su, Xiaolong Wang, arXiv:2310.168282023arXiv preprint</p>
<p>Self-supervised learning from images with a jointembedding predictive architecture. Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann Lecun, Nicolas Ballas, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Michael Richard S Sutton, Patrick M Bowling, Pilarski, arXiv:2208.11173The alberta plan for ai research. 2022arXiv preprint</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P Lillicrap, David Silver, CoRR, abs/1911.082652019</p>
<p>Policy improvement by planning with gumbel. Ivo Danihelka, Arthur Guez, Julian Schrittwieser, David Silver, International Conference on Learning Representations. 2022</p>
<p>Planning in stochastic environments with a learned model. Ioannis Antonoglou, Julian Schrittwieser, Sherjil Ozair, Thomas K Hubert, David Silver, International Conference on Learning Representations. 2021</p>
<p>Learning and planning in complex action spaces. Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, David Silver, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Marina Meila, Tong Zhang, the 38th International Conference on Machine Learning, ICML 2021PMLR18-24 July 2021. 2021139of Proceedings of Machine Learning Research</p>
<p>Lightzero: A unified benchmark for monte carlo tree search in general sequential decision scenarios. Yazhe Niu, Yuan Pu, Zhenjie Yang, Xueyan Li, Tong Zhou, Jiyuan Ren, Shuai Hu, Hongsheng Li, Yu Liu, Advances in Neural Information Processing Systems. 202436</p>
<p>The value equivalence principle for model-based reinforcement learning. Christopher Grimm, André Barreto, Satinder Singh, David Silver, 2020</p>
<p>Monte carlo tree search: A review of recent modifications and applications. Maciej Świechowski, Konrad Godlewski, Bartosz Sawicki, Jacek Mańdziuk, Artificial Intelligence Review. 5632023</p>
<p>Mastering chess and shogi by self-play with a general reinforcement learning algorithm. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, arXiv:1712.018152017arXiv preprint</p>
<p>Deep recurrent q-learning for partially observable mdps. Matthew Hausknecht, Peter Stone, 2017</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Scalable diffusion models with transformers. William Peebles, Saining Xie, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Mini-dalle3: Interactive text to image by prompting large language models. Zeqiang Lai, Xizhou Zhu, Jifeng Dai, Yu Qiao, Wenhai Wang, 2023</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch, 2021</p>
<p>Offline reinforcement learning as one big sequence modeling problem. Michael Janner, Qiyang Li, Sergey Levine, 2021</p>
<p>Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. 2022</p>
<p>Mastering diverse domains through world models. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap, arXiv:2301.041042023arXiv preprint</p>
<p>Iris: Implicit reinforcement without interaction at scale for learning control from offline robot manipulation data. Ajay Mandlekar, Fabio Ramos, Byron Boots, Silvio Savarese, Li Fei-Fei, Animesh Garg, Dieter Fox, 2020</p>
<p>Transformer-based world models are happy with 100k interactions. Jan Robine, Marc Höftmann, Tobias Uelwer, Stefan Harmeling, 2023</p>
<p>Storm: Efficient stochastic transformer based world models for reinforcement learning. Weipu Zhang, Gang Wang, Jian Sun, Yetian Yuan, Gao Huang, 2023</p>
<p>Emilio Parisotto, H Francis Song, Jack W Rae, Razvan Pascanu, Caglar Gulcehre, M Siddhant, Max Jayakumar, Raphael Jaderberg, Aidan Lopez Kaufman, Clark, Seb Noury, Matthew M. Botvinick, Nicolas Heess, and Raia Hadsell. Stabilizing transformers for reinforcement learning. 2019</p>
<p>Mismatched no more: Joint model-policy optimization for model-based rl. Benjamin Eysenbach, Alexander Khazatsky, Sergey Levine, Russ R Salakhutdinov, Advances in Neural Information Processing Systems. 202235</p>
<p>Simplifying model-based rl: learning representations, latent-space models, and policies with one objective. Raj Ghugare, Homanga Bharadhwaj, Benjamin Eysenbach, Sergey Levine, Ruslan Salakhutdinov, arXiv:2209.084662022arXiv preprint</p>
<p>The optimal control of partially observable Markov processes. Edward Jay, Sondik , 1971Stanford University</p>
<p>The arcade learning environment: An evaluation platform for general agents. Yavar Marc G Bellemare, Joel Naddaf, Michael Veness, Bowling, Journal of Artificial Intelligence Research. 472013</p>
<p>Joery A De Vries, Ken S Voskuil, Thomas M Moerland, Aske Plaat, arXiv:2102.12924Visualizing muzero models. 2021arXiv preprint</p>
<p>Mastering atari games with limited data. Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao, Advances in Neural Information Processing Systems. 202134</p>
<p>Adrià Puigdomènech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman, Martín Arjovsky, Alexander Pritzel, arXiv:2002.06038Andew Bolt, et al. Never give up: Learning directed exploration strategies. 2020arXiv preprint</p>
<p>Empirical evaluation of gated recurrent neural networks on sequence modeling. Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio, arXiv:1412.35552014arXiv preprint</p>
<p>Ioannis Antonoglou, and Rémi Munos. Monte-carlo tree search as regularized policy optimization. Jean-Bastien Grill, Florent Altché, Yunhao Tang, Thomas Hubert, Michal Valko, 2020</p>
<p>Playing atari with deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller, arXiv:1312.56022013arXiv preprint</p>
<p>A distributional perspective on reinforcement learning. G Marc, Will Bellemare, Rémi Dabney, Munos, 2017</p>
<p>Dream to control: Learning behaviors by latent imagination. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi, arXiv:1912.016032019arXiv preprint</p>
<p>Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, CoRR, abs/1707.063472017</p>
<p>Model tells you what to discard: Adaptive kv cache compression for llms. Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng Gao, 2024</p>
<p>Model-based reinforcement learning for atari. Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, Henryk Michalewski, 2024</p>
<p>Data-efficient reinforcement learning with self-predictive representations. Max Schwarzer, Ankesh Anand, Rishab Goel, Devon Hjelm, Aaron Courville, Philip Bachman, arXiv:2007.059292020arXiv preprint</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, Advances in neural information processing systems. 2018</p>
<p>Predictive auxiliary objectives in deep rl mimic learning in the brain. Ching Fang, Kimberly L Stachenfeld, 2023</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 52975872016</p>
<p>Online and offline reinforcement learning by planning with a learned model. Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou, David Silver, Advances in Neural Information Processing Systems. 342021</p>
<p>Chunyu Xuan, Yazhe Niu, Yuan Pu, Shuai Hu, Jing Yang, Rezero, arXiv:2404.16364Boosting mcts-based algorithms by just-in-time and speedy reanalyze. 2024arXiv preprint</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Beyond a*: Better planning with transformers via search dynamics bootstrapping. Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, Yuandong Tian, arXiv:2402.140832024arXiv preprint</p>
<p>. David Ha, Jürgen Schmidhuber, arXiv:1803.101222018World models. arXiv preprint</p>
<p>Temporal difference learning for model predictive control. Nicklas Hansen, Xiaolong Wang, Hao Su, arXiv:2203.049552022arXiv preprint</p>
<p>Model predictive control. Basil Kouvaritakis, Mark Cannon, 2016Springer International Publishing38Switzerland</p>
<p>Flashattention: Fast and memory-efficient exact attention with io-awareness. Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, Christopher Ré, 2022</p>
<p>Gqa: Training generalized multi-query transformer models from multi-head checkpoints. Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebrón, Sumit Sanghai, 2023</p>
<p>Bigger, better, faster: Human-level atari with human-level efficiency. Max Schwarzer, Johan Samir Obando Ceron, Aaron Courville, Marc G Bellemare, Rishabh Agarwal, Pablo Samuel Castro, International Conference on Machine Learning. PMLR2023</p>
<p>Multi-armed bandits with episode context. Christopher D Rosin, Annals of Mathematics and Artificial Intelligence. 6132011</p>
<p>Simplicial embeddings in self-supervised learning and downstream classification. Samuel Lavoie, Christos Tsirigotis, Max Schwarzer, Ankit Vani, Michael Noukhovitch, Kenji Kawaguchi, Aaron Courville, arXiv:2204.006162022arXiv preprint</p>
<p>Dan Hendrycks, Kevin Gimpel, arXiv:1606.08415Gaussian error linear units (gelus). 2016arXiv preprint</p>
<p>. Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie Xu, Sergio Guadarrama, Ian Fischer, Eric Jang, Henryk Michalewski, and Igor Mordatch. Multi-game decision transformers. 2022</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu, 2023</p>
<p>Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural computation. 981997</p>
<p>Offline qlearning on diverse multi-task data both scales and generalizes. Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, Sergey Levine, arXiv:2211.151442022arXiv preprint</p>
<p>Tasknorm: Rethinking batch normalization for meta-learning. John Bronskill, Jonathan Gordon, James Requeima, Sebastian Nowozin, Richard Turner, International Conference on Machine Learning. PMLR2020</p>
<p>Gradient surgery for multi-task learning. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, Chelsea Finn, Advances in Neural Information Processing Systems. 202033</p>
<p>Conflict-averse gradient descent for multi-task learning. Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, Qiang Liu, Advances in Neural Information Processing Systems. 202134</p>
<p>Neural discrete representation learning. Aäron Van Den Oord, Oriol Vinyals, Koray Kavukcuoglu, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. Isabelle Guyon, Samy Ulrike Von Luxburg, Hanna M Bengio, Rob Wallach, S V N Fergus, Roman Vishwanathan, Garnett, Long Beach, CA, USADecember 4-9, 2017. 2017</p>
<p>Simplifying model-based rl: Learning representations, latent-space models, and policies with one objective. Raj Ghugare, Homanga Bharadhwaj, Benjamin Eysenbach, Sergey Levine, Ruslan Salakhutdinov, arXiv:2209.084662022arXiv preprint</p>
<p>Transformer-xl: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, Ruslan Salakhutdinov, 2019</p>
<p>Learning phrase representations using rnn encoderdecoder for statistical machine translation. Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, 2014</p>
<p>Teaching arithmetic to small transformers. Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, Dimitris Papailiopoulos, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
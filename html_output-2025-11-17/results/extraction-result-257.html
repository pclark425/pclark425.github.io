<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-257 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-257</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-257</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-260203256</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.15020v1.pdf" target="_blank">SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown the potential to be integrated into human daily lives. Therefore, user preference is the most critical criterion for assessing LLMs' performance in real-world scenarios. However, existing benchmarks mainly focus on measuring models' accuracy using multi-choice questions, which limits the understanding of their capabilities in real applications. We fill this gap by proposing a comprehensive Chinese benchmark SuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUE encompasses three sub-tasks: actual users' queries and ratings derived from an LLM battle platform (CArena), open-ended questions with single and multiple-turn dialogues (OPEN), and closed-ended questions with the same stems as open-ended single-turn ones (CLOSE). Our study shows that accuracy on closed-ended questions is insufficient to reflect human preferences achieved on open-ended ones. At the same time, they can complement each other to predict actual user preferences. We also demonstrate that GPT-4 is a reliable judge to automatically evaluate human preferences on open-ended questions in a Chinese context. Our benchmark will be released at https://www.CLUEbenchmarks.com</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e257.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e257.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model used both as an evaluated model and as the automatic judge (LLM-as-a-judge) in this paper; reported to outperform all evaluated models on closed and open benchmarks in SuperCLUE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>General arithmetic and more complex mathematical problems (paper only refers to the 'Calculation' capability broadly: addition, subtraction, multiplication, division, and more complex problems).</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot evaluation when used as an evaluated model; used as an automatic pairwise judge (LLM-as-a-judge) for open-ended answers; the authors report mitigation steps for judge biases (position bias, verbosity bias) and limited capability in grading math and reasoning questions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported to significantly outperform all other evaluated models on CLOSE, OPEN SINGLE and OPEN ALL; GPT-4 achieves >10 percentage points advantage on CLOSE over MiniMax and nearly double MiniMax's win rate on OPEN sets. No arithmetic-specific accuracy numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper notes that LLM judges (including superior LLMs) can have limited capability in grading math and reasoning questions; the authors applied mitigations when using GPT-4 as judge. No model-internal arithmetic failure patterns are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against other evaluated models (e.g., MiniMax, ChatGLM2-6B, MOSS, Ziya-13B, RWKVworld-7B) and used in pairwise comparisons with ChatGPT in the LLM-as-judge setting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GPT-4 is the strongest performer overall and can serve as a reliable automatic rater for open-ended Chinese queries, but the paper does not provide detailed, task-level arithmetic performance or mechanistic explanations of how GPT-4 performs arithmetic.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e257.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e257.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MiniMax</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MiniMax (Chinese LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Chinese-oriented Transformer-based LLM that is the top-performing Chinese model on SuperCLUE and the second-best overall in the authors' evaluations; excels on several capability categories including logic and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MiniMax</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Transformer (paper states MiniMax is a newly-generated Chinese LLM based on Transformer).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>General calculation tasks as defined in the benchmark (addition, subtraction, multiplication, division and more complex mathematical problems) — the paper groups arithmetic under a 'Calculation' capability but provides no finer breakdown.</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot evaluation on CLOSE and OPEN sets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported as the best Chinese model and second-best overall; beats ChatGLM2-6B in several categories (Small Talk, Role Playing, Knowledge/Encyclopedia, Generation/Creation, Logic and Reasoning). No specific arithmetic/calculation accuracy or per-operation results provided.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Directly compared to other Chinese LLMs (e.g., ChatGLM2-6B) and international models (GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>MiniMax is the strongest Chinese model on the benchmark, but the paper does not report detailed arithmetic task performance or investigate mechanistic reasons for its arithmetic behavior.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e257.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e257.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM2-6B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM2-6B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 6-billion-parameter Chinese-oriented LLM pre-trained with the GLM-style pretraining and fine-tuned for conversation; evaluated in SuperCLUE and performs behind MiniMax.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM2-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Pretrained with GLM algorithm (autoregressive blank infilling) and fine-tuned on conversational data (transformer-based).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>General calculation tasks (the benchmark includes a 'Calculation' capability covering basic operations and more complex math); no operation-level breakdown provided.</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Lags behind MiniMax by roughly ~1% win rate overall in some comparisons; closed-ended accuracy of Chinese models generally concentrated between 55%–60% while open-ended win rates vary widely. No calculation-specific metrics are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with MiniMax and other Chinese and international LLMs on CLOSE and OPEN sets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>ChatGLM2-6B is a competitive Chinese conversational model but the paper does not provide arithmetic-specific performance or mechanistic analysis for ChatGLM2-6B.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e257.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e257.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Calculation (capability)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Calculation capability (SuperCLUE benchmark category)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>One of ten capability categories annotated and evaluated in SuperCLUE; defined as the model's ability to perform mathematical operations (addition, subtraction, multiplication, division) and more complex mathematical problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Calculation (capability)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Explicitly listed: addition, subtraction, multiplication, division, and 'even more complex mathematical problems' (no finer specification in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Included in both OPEN (open-ended single- and multi-turn) and CLOSE (closed multi-choice transformed from OPEN) datasets; evaluated in a zero-shot setting; CLOSE items were generated from OPEN stems using GPT-3.5 then human-checked; OPEN judged automatically via GPT-4 (with mitigations for judge biases and grading math/reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>The paper does not report per-operation or numeric-range-specific results for the Calculation category in the provided text; overall it reports that Chinese models show similar CLOSE accuracy (mostly 55%–60%) but widely varying OPEN performance. No arithmetic-specific accuracy numbers are given.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>No internal mechanistic failure modes for arithmetic are reported; the authors do note that LLM judges can have limited capability grading math and reasoning questions and that they mitigated these issues in their LLM-as-judge pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparison between CLOSE (multi-choice) and OPEN (open-ended) evaluation modes; authors report poor correlation between CLOSE accuracy and OPEN judged performance, implying closed arithmetic questions alone may be insufficient to assess arithmetic ability in realistic settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Calculation is included as an evaluated capability, but the paper provides only high-level treatment (definition and evaluation protocol); it does not provide detailed arithmetic-task results, digit-range breakdowns, or mechanistic explanations of how models perform arithmetic.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e257.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e257.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chinese LLMs (group)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chinese-oriented large language models evaluated in SuperCLUE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of Chinese-focused LLMs evaluated (examples: ChatGLM-130B, ChatGLM2-6B, MOSS, Ziya-13B, MiniMax, Wenxin Yiyan, 360 Brain, SparkDesk); they show similar closed-ended performance but widely varying open-ended performance on SuperCLUE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chinese LLMs (group)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Various (primarily transformer-based; RWKVworld-7B noted as RNN-based).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>General calculation capability as defined by the benchmark (basic operations and more complex math), but no per-model arithmetic task breakdowns provided.</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot evaluation across CLOSE (transformed multiple-choice) and OPEN (open-ended single/multi-turn) datasets; OPEN judged by GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>On CLOSE, most Chinese models score between 55% and 60% accuracy; on OPEN, win-rates vary broadly (reported range across Chinese models 12.50% to 41.48% in one place). No calculation-specific performance metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Within-group comparisons and against GPT-4; analysis also contrasts CLOSE vs OPEN evaluation formats.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Chinese LLMs show compressed performance on closed-ended arithmetic-style questions but diverge substantially on open-ended evaluations; the paper does not provide detailed arithmetic-result breakdowns or internal mechanistic analyses for arithmetic behavior.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GPT-4 Technical Report <em>(Rating: 2)</em></li>
                <li>Measuring Massive Multitask Language Understanding <em>(Rating: 2)</em></li>
                <li>C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models <em>(Rating: 2)</em></li>
                <li>AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models <em>(Rating: 2)</em></li>
                <li>Language Models are Few-Shot Learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-257",
    "paper_id": "paper-260203256",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A state-of-the-art large language model used both as an evaluated model and as the automatic judge (LLM-as-a-judge) in this paper; reported to outperform all evaluated models on closed and open benchmarks in SuperCLUE.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "General arithmetic and more complex mathematical problems (paper only refers to the 'Calculation' capability broadly: addition, subtraction, multiplication, division, and more complex problems).",
            "number_range_or_complexity": null,
            "method_or_intervention": "Zero-shot evaluation when used as an evaluated model; used as an automatic pairwise judge (LLM-as-a-judge) for open-ended answers; the authors report mitigation steps for judge biases (position bias, verbosity bias) and limited capability in grading math and reasoning questions.",
            "performance_result": "Reported to significantly outperform all other evaluated models on CLOSE, OPEN SINGLE and OPEN ALL; GPT-4 achieves &gt;10 percentage points advantage on CLOSE over MiniMax and nearly double MiniMax's win rate on OPEN sets. No arithmetic-specific accuracy numbers reported.",
            "mechanistic_insight": null,
            "performance_scaling": null,
            "failure_modes": "Paper notes that LLM judges (including superior LLMs) can have limited capability in grading math and reasoning questions; the authors applied mitigations when using GPT-4 as judge. No model-internal arithmetic failure patterns are reported.",
            "comparison_baseline": "Compared against other evaluated models (e.g., MiniMax, ChatGLM2-6B, MOSS, Ziya-13B, RWKVworld-7B) and used in pairwise comparisons with ChatGPT in the LLM-as-judge setting.",
            "key_finding": "GPT-4 is the strongest performer overall and can serve as a reliable automatic rater for open-ended Chinese queries, but the paper does not provide detailed, task-level arithmetic performance or mechanistic explanations of how GPT-4 performs arithmetic.",
            "uuid": "e257.0"
        },
        {
            "name_short": "MiniMax",
            "name_full": "MiniMax (Chinese LLM)",
            "brief_description": "A Chinese-oriented Transformer-based LLM that is the top-performing Chinese model on SuperCLUE and the second-best overall in the authors' evaluations; excels on several capability categories including logic and reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MiniMax",
            "model_size": null,
            "model_architecture": "Transformer (paper states MiniMax is a newly-generated Chinese LLM based on Transformer).",
            "arithmetic_operation_type": "General calculation tasks as defined in the benchmark (addition, subtraction, multiplication, division and more complex mathematical problems) — the paper groups arithmetic under a 'Calculation' capability but provides no finer breakdown.",
            "number_range_or_complexity": null,
            "method_or_intervention": "Zero-shot evaluation on CLOSE and OPEN sets.",
            "performance_result": "Reported as the best Chinese model and second-best overall; beats ChatGLM2-6B in several categories (Small Talk, Role Playing, Knowledge/Encyclopedia, Generation/Creation, Logic and Reasoning). No specific arithmetic/calculation accuracy or per-operation results provided.",
            "mechanistic_insight": null,
            "performance_scaling": null,
            "failure_modes": null,
            "comparison_baseline": "Directly compared to other Chinese LLMs (e.g., ChatGLM2-6B) and international models (GPT-4).",
            "key_finding": "MiniMax is the strongest Chinese model on the benchmark, but the paper does not report detailed arithmetic task performance or investigate mechanistic reasons for its arithmetic behavior.",
            "uuid": "e257.1"
        },
        {
            "name_short": "ChatGLM2-6B",
            "name_full": "ChatGLM2-6B",
            "brief_description": "A 6-billion-parameter Chinese-oriented LLM pre-trained with the GLM-style pretraining and fine-tuned for conversation; evaluated in SuperCLUE and performs behind MiniMax.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGLM2-6B",
            "model_size": "6B",
            "model_architecture": "Pretrained with GLM algorithm (autoregressive blank infilling) and fine-tuned on conversational data (transformer-based).",
            "arithmetic_operation_type": "General calculation tasks (the benchmark includes a 'Calculation' capability covering basic operations and more complex math); no operation-level breakdown provided.",
            "number_range_or_complexity": null,
            "method_or_intervention": "Zero-shot evaluation.",
            "performance_result": "Lags behind MiniMax by roughly ~1% win rate overall in some comparisons; closed-ended accuracy of Chinese models generally concentrated between 55%–60% while open-ended win rates vary widely. No calculation-specific metrics are reported.",
            "mechanistic_insight": null,
            "performance_scaling": null,
            "failure_modes": null,
            "comparison_baseline": "Compared with MiniMax and other Chinese and international LLMs on CLOSE and OPEN sets.",
            "key_finding": "ChatGLM2-6B is a competitive Chinese conversational model but the paper does not provide arithmetic-specific performance or mechanistic analysis for ChatGLM2-6B.",
            "uuid": "e257.2"
        },
        {
            "name_short": "Calculation (capability)",
            "name_full": "Calculation capability (SuperCLUE benchmark category)",
            "brief_description": "One of ten capability categories annotated and evaluated in SuperCLUE; defined as the model's ability to perform mathematical operations (addition, subtraction, multiplication, division) and more complex mathematical problems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Calculation (capability)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "Explicitly listed: addition, subtraction, multiplication, division, and 'even more complex mathematical problems' (no finer specification in the paper).",
            "number_range_or_complexity": null,
            "method_or_intervention": "Included in both OPEN (open-ended single- and multi-turn) and CLOSE (closed multi-choice transformed from OPEN) datasets; evaluated in a zero-shot setting; CLOSE items were generated from OPEN stems using GPT-3.5 then human-checked; OPEN judged automatically via GPT-4 (with mitigations for judge biases and grading math/reasoning).",
            "performance_result": "The paper does not report per-operation or numeric-range-specific results for the Calculation category in the provided text; overall it reports that Chinese models show similar CLOSE accuracy (mostly 55%–60%) but widely varying OPEN performance. No arithmetic-specific accuracy numbers are given.",
            "mechanistic_insight": null,
            "performance_scaling": null,
            "failure_modes": "No internal mechanistic failure modes for arithmetic are reported; the authors do note that LLM judges can have limited capability grading math and reasoning questions and that they mitigated these issues in their LLM-as-judge pipeline.",
            "comparison_baseline": "Comparison between CLOSE (multi-choice) and OPEN (open-ended) evaluation modes; authors report poor correlation between CLOSE accuracy and OPEN judged performance, implying closed arithmetic questions alone may be insufficient to assess arithmetic ability in realistic settings.",
            "key_finding": "Calculation is included as an evaluated capability, but the paper provides only high-level treatment (definition and evaluation protocol); it does not provide detailed arithmetic-task results, digit-range breakdowns, or mechanistic explanations of how models perform arithmetic.",
            "uuid": "e257.3"
        },
        {
            "name_short": "Chinese LLMs (group)",
            "name_full": "Chinese-oriented large language models evaluated in SuperCLUE",
            "brief_description": "Set of Chinese-focused LLMs evaluated (examples: ChatGLM-130B, ChatGLM2-6B, MOSS, Ziya-13B, MiniMax, Wenxin Yiyan, 360 Brain, SparkDesk); they show similar closed-ended performance but widely varying open-ended performance on SuperCLUE.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Chinese LLMs (group)",
            "model_size": null,
            "model_architecture": "Various (primarily transformer-based; RWKVworld-7B noted as RNN-based).",
            "arithmetic_operation_type": "General calculation capability as defined by the benchmark (basic operations and more complex math), but no per-model arithmetic task breakdowns provided.",
            "number_range_or_complexity": null,
            "method_or_intervention": "Zero-shot evaluation across CLOSE (transformed multiple-choice) and OPEN (open-ended single/multi-turn) datasets; OPEN judged by GPT-4.",
            "performance_result": "On CLOSE, most Chinese models score between 55% and 60% accuracy; on OPEN, win-rates vary broadly (reported range across Chinese models 12.50% to 41.48% in one place). No calculation-specific performance metrics reported.",
            "mechanistic_insight": null,
            "performance_scaling": null,
            "failure_modes": null,
            "comparison_baseline": "Within-group comparisons and against GPT-4; analysis also contrasts CLOSE vs OPEN evaluation formats.",
            "key_finding": "Chinese LLMs show compressed performance on closed-ended arithmetic-style questions but diverge substantially on open-ended evaluations; the paper does not provide detailed arithmetic-result breakdowns or internal mechanistic analyses for arithmetic behavior.",
            "uuid": "e257.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Measuring Massive Multitask Language Understanding",
            "rating": 2,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
            "rating": 2,
            "sanitized_title": "ceval_a_multilevel_multidiscipline_chinese_evaluation_suite_for_foundation_models"
        },
        {
            "paper_title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models",
            "rating": 2,
            "sanitized_title": "agieval_a_humancentric_benchmark_for_evaluating_foundation_models"
        },
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.0138285,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark</p>
<p>Liang Xu 
CLUE</p>
<p>Anqi Li lianqi@westlake.edu.cn 
Westlake University</p>
<p>† 
Lei Zhu 
CLUE</p>
<p>Hang Xue 
CLUE</p>
<p>Changtai Zhu 
Kangkang Zhao 
CLUE</p>
<p>Haonan He 
CLUE</p>
<p>Xuanwei Zhang 
CLUE</p>
<p>Qiyue Kang 
Westlake University</p>
<p>Zhenzhong Lan lanzhenzhong@westlake.edu.cn 
CLUE</p>
<p>Westlake University</p>
<p>SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark</p>
<p>Large language models (LLMs) have shown the potential to be integrated into human daily lives. Therefore, user preference is the most critical criterion for assessing LLMs' performance in real-world scenarios. However, existing benchmarks mainly focus on measuring models' accuracy using multi-choice questions, which limits the understanding of their capabilities in real applications.</p>
<p>Introduction</p>
<p>Recently, large language models (LLMs) have exhibited remarkable capabilities in handling diverse problems in general and specialized domains (OpenAI 2023; Zeng et al. 2022;Du et al. 2022;Touvron et al. 2023;Chiang et al. 2023;Zhang et al. 2022;Sun et al. 2023). The great advancements of artificial intelligence have paved the way to the realization of integrating LLMs into human daily lives (Kasneci et al. 2023;Nov, Singh, and Mann 2023;Sallam 2023). Therefore, understanding LLMs' performances on a diverse set of abilities in real-world scenarios is essential for identifying the strengths and weaknesses of the models.</p>
<p>The most standard and authoritative indicators for evaluating models' performances are the ratings from real users. To investigate the performance of English models from the user perspective, Zheng et al. (2023) develops a crowdsourced platform Chatbot Arena to allow users to engage * Equal contribution † Equal contribution Preprint.</p>
<p>1 Our benchmark will be released at https://www. CLUEbenchmarks.com in conversations with two anonymous chatbots at the same time and then rate based on personal preferences. However, such a time-consuming and cost-intensive evaluation process is almost impossible to be scaled up for each newly-developed model.</p>
<p>Numerous benchmarks are proposed to assess various capabilities of the lasted LLMs efficiently. For example, MMLU (Hendrycks et al. 2020), Big-Bench (Lewkowycz et al. 2022), HELM (Liang et al. 2022) and MMCU (Zeng 2023) are widely applied in recent research to holistically evaluate LLMs on multiple NLP tasks. Some other benchmarks like AGIEval (Zhong et al. 2023) and C-Eval ) emphasize the importance of assessing LLMs' emergent capabilities on human-level tasks, which can reflect models' real-world applicability. In addition, Zheng et al. (2023) proposes an English benchmark MT-bench to evaluate models' multi-turn conversational and instruction-following abilities. These benchmarks mainly consist of closed-ended multi-choice questions except MT-bench. Considering that users usually query in an open-ended manner, benchmarking with closed-ended questions may result in a limited understanding of LLMs' capabilities to interact with users in actual applications. Moreover, the correlation between capabilities reflected on all these benchmarks and perceived by real users has not been studied.</p>
<p>In this work, we fill the above gaps by constructing a benchmark that predicts LLMs' performances on a diverse set of abilities in real Chinese scenarios. Similar to Chatbot Arena, we design an anonymous battle platform based on the Elo rating system 2 , in which users can communicate with two Chinese LLMs and rank their responses. In total, we collect 9.9k queries with ratings from real users and regard the average rate of win and tie as the gold standard of models' performance. In order to analyze Chinese users' interests, we carefully annotate part of the queries with ten capability categories, including reasoning and creation. We then construct the open-ended sub-benchmark by selecting 30 singleturn and 30 multi-turn questions for each category. And for each single-turn open-ended question, we get a corresponding closed-ended question by generating four answers using GPT-3.5 and manually checking the answers to make sure there is one and only one best choice.</p>
<p>We then evaluate 11 advanced LLMs on our benchmark. To evaluate open-ended questions automatically, we use GPT-4 as the judge. Results show that GPT-4 significantly outperforms all models. Among Chinese models, MiniMax stands out and complements ChatGLM2-6B in specific categories. We also conducted extensive additional analyses on these datasets. We show that a superior large language model such as GPT-4 can be utilized as a reliable automatic rater in a Chinese context. We then analyze the correlation between GPT-4 ratings on open-ended questions and closedended accuracy. Based on the results, it is clear that the latter method has limitations when reflecting human preferences in open interactive scenarios. Finally, we demonstrate jointly that using close-and open-ended questions can more reliably reflect actual user preferences.</p>
<p>Related Work</p>
<p>Traditional NLP benchmarks (Dolan and Brockett 2005;Socher et al. 2013;Rajpurkar et al. 2016;Williams, Nangia, and Bowman 2018) are mainly designed to evaluate models' performances on one specific task, such as SST-2 (Socher et al. 2013) for sentiment analysis and SQuAD (Rajpurkar et al. 2016) for reading comprehension. To encourage the development of unified models that can handle various NLP tasks, comprehensive benchmarks to provide general-purpose evaluation (Sarlin et al. 2019;Hendrycks et al. 2020) like GLUE (Wang et al. 2018) and CLUE (Xu et al. 2020) become prominent evaluation frameworks. Such benchmarks have significantly influenced the notable progress of generalization capabilities of language models (Devlin et al. 2019;Brown et al. 2020;Zhuang et al. 2021). Despite their broad application, most of them are not suitable for assessing recent large language models which have performed robust abilities on complex reasoning and problem-solving tasks (OpenAI 2023; Zhong et al. 2023;Huang et al. 2023). For example, Goyal, Li, and Durrett (2023) show that LLMs like GPT-3 can generate more de-sirable summaries than "gold answers" in typical text summarization benchmarks including CNN/DailyMail.</p>
<p>To better understand LLMs' strengths and limitations, new benchmarks are proposed to assess broader knowledge and advanced abilities (Liang et al. 2022;Zhong et al. 2023;Huang et al. 2023;Li et al. 2023). Liang et al. (2022) presents a holistic evaluation of language models by taxonomizing potential application scenarios and evaluation metrics. Recent benchmarks place more emphasis on assessment in human-centric scenarios by collecting high-standard admission and qualification exams on testing human-level reasoning and real-world relevance (Zhong et al. 2023;Huang et al. 2023;Zeng 2023;Gu et al. 2023). For example, C-Eval  consists of exam questions that span 52 diverse disciplines ranging from humanities to science and engineering. These benchmarks mainly adopt multi-choice questions and use accuracy as the evaluation metric. Some researchers suggest that natural language generation (NLG) with multi-turn interaction should be the core evaluation approach (Hendrycks et al. 2020;Zheng et al. 2023). Zheng et al. (2023) introduced MT-bench and Chatbot Arena consisting of open-ended questions that evaluate multi-turn conversational and instruction-following ability, which is the best-related dataset up to now. However, all questions in ) are only open-ended and in English, and the analysis mostly focuses on investigating the agreement on ratings between superior LLMs like GPT-4 and humans. In contrast, our Chinese benchmark contains questions in both formats of open-and closed-end. And we emphasize the inadequacy of only close-ended questions, and the complementarity of the joint use of open-and closeended ones to predict human preferences on model utilities in the real world.</p>
<p>SuperCLUE Benchmark</p>
<p>The SuperCLUE benchmark allows developers to gather quick and accurate information about users' preferences for their models in a Chinese context before putting them into applications. We first collect a dataset CArena containing user-model interactions with user-reported ratings from a model battle platform LangYa Leaderboard 3 . On this platform, users can communicate with two anonymous models and then rate the models. We analyze large-scale users' queries and carefully annotate a subset of data with different model capabilities. User-reported win rates are the gold standard for measuring model performance in real-life scenarios. We then construct a smaller benchmark OPEN, which contains single-and multi-turn open-ended questions. We also design a CLOSE dataset based on the Open set, targeting to analyze whether the format of multi-choice questions is a suitable alternative to an open one in evaluating model performance. We will introduce these three datasets: CArena, OPEN, and CLOSE in detail in the following.</p>
<p>CArena</p>
<p>Similar to Chatbot Arena ) designed for English users, we developed an anonymous model competition platform LangYa Leaderboard to make Chinese LLMs (including ChatGLM-130B, ChatGLM2-6B, MOSS, Ziya, and MiniMax 4 ) interact with Chinese users. On this platform, users can send queries and get replies from two anonymous models selected by the Elo rating system. After each interaction, users must determine which model's answer aligns better with their expectations. The interaction and evaluation interface is shown in Figure 9 in Appendix A.</p>
<p>We collected 9.9k votes from users from the platform since May 15th, 2023. Through analyzing user interests reflected in their queries, we find that the majority of queries can be split into ten capability groups, including semantic understanding and extraction, small talk, contextual conversation, generation and creation, knowledge and encyclopedia, code generation, logic and reasoning, calculation, roleplaying, and safety. We then annotate queries based on these ten capability categories.</p>
<p>Detailed definitions and examples of capability categories and annotation processes are shown in Appendix B.</p>
<p>OPEN Set</p>
<p>Open-ended questions are questions that cannot be answered with a simple "yes" or "no", or a multi-choice, or with a specific piece of information. These questions require more thought and more than a simple one-word answer. They are designed to encourage a full, meaningful answer using the respondent's own knowledge and/or feelings. We design the OPEN dataset following two primary principles: 1) questions in OPEN should be aligned with real users' queries in either format or content, and able to evaluate models' instruction-following abilities; 2) questions should able to evaluate models' multi-turn conversational ability, not only single-turn; 3) abilities should be representatives and contains border categories, such as text understanding and generation, knowledge, professional and safety. Given that users prefer interacting with models openly, we center on incorporating open-ended questions into our OPEN dataset. We then select questions based on capability categories that users concern with within the real world, with 30 single-turn questions for each category In order to match more far-reaching application scenarios that users interact with models in multiple turns, we manually design multiturn questions based on single-turn ones. Specifically, we regard each single-turn question as the initial question and then formulate a follow-up question (see Figure 2). Finally, 600 questions are collected in total. The subset of single-or multi-turn questions in OPEN will be referred to as OPEN SINGLE or OPEN MULTIPLE in the following. single-turn and multi-turn question in OPEN will be refered to as OPEN ALL. 4 Because of the limited access to other famous Chinese LLMs such as Wenxin Yiyan, 360 brain, and SparkDesk, we cannot employ these three models in our platform.</p>
<p>Single-turn Question</p>
<p>描写黄山的日出景色，突出自然之美。</p>
<p>Describe the sunrise scenery seen on the top of Mount Huang, highlighting the beauty of nature.</p>
<p>Multi-turn Question</p>
<p>描写黄山的日出景色，突出自然之美。</p>
<p>Describe the sunrise scenery seen on the top of Mount Huang, highlighting the beauty of nature.</p>
<p>Initial Question</p>
<p>Follow-up Question</p>
<p>修改你的答案，使每句话都与红日相关。 Modify your answer to make each sentence related to the sun. </p>
<p>CLOSE Set</p>
<p>Despite its discrepancy with real queries, the format of closed-ended questions has been widely adopted by existing benchmarks due to its simple-to-evaluate properties. We attempt to quantify the limitations of closed-ended questions.</p>
<p>We design a human-in-the-loop approach to transform open-ended questions in OPEN SINGLE into closed-ended ones. Specifically, we feed the stem of each open-ended question into GPT-3.5 to make it generate a four-choice question with the right answer, which is then proofread and corrected by humans. An example question in CLOSE and its counterpart in OPEN SINGLE is shown in Figure 3. A more detailed process is presented in Appendix A.  </p>
<p>Open-ended Question</p>
<p>Experiments</p>
<p>In this section, we will evaluate the lasted eight Chineseoriented LLMs on SuperCLUE, and present a comparative analysis of their performance.</p>
<p>Setup</p>
<p>Zero-shot Evaluation In most real-life scenarios, models have no access to examples of specific tasks. To align with real applications, we only adopt the zero-shot setting in our evaluation. In this setting, models are evaluated on the questions without explicit training. In this setting, models are evaluated on the questions without being provided with any prior examples.  Table 1: LLMs chosen for evaluation. The "size" column represents the number of parameters of each model. The "access" column represents approaches to obtain modelsthrough API or loading models with weights.</p>
<p>Evaluation Metrics For closed-ended multi-choice questions, we adopt classification accuracy as the evaluation metric. For open-ended questions and user queries collected from the model battle platform, we employ the average win rate (win and tie rate) against other models as the indicator of model performance.</p>
<p>Evaluation Methods For open-ended questions, we conduct an automatic evaluation using the LLM-as-a-judge method. Specifically, we employ pairwise comparison by presenting GPT-4, serving as our default rater, with a question and answers of two models (one is the selected model, and the other is ChatGPT) and asking GPT-4 to determine which one is better or declare a tie. In the automatic evaluation process, we have mitigated certain issues of LLM judges including position bias, verbosity bias, and limited capability in grading math and reasoning questions. An example of GPT-4 judging on an open-ended question is shown in Figure 4. Other details of automatic evaluation are presented in Appendix.</p>
<p>For user queries, we directly obtain user self-reported ratings from the model battle platform. The evaluation interfaces are shown in Appendix A.</p>
<p>Models</p>
<p>We focus on comprehensively evaluating the performance of three accessible LLMs that are able to process Chinese input, and eight Chinese-oriented LLMs on our benchmark, which are developed by Chinese institutions or individuals. The organizations, model sizes, and accessible approaches of the chosen models are shown in Table 1. GPT-4 (OpenAI 2023) is widely known as top-performing LLM developed by OpenAI, with the training process as pretraining, instruction tuning and reinforcement learning from human feedback (Ouyang et al. 2022). Claude-instant-v1 is a light version of Claude 5 , whose performance has been demonstrated as comparable with ChatGPT 6 . RWKVworld-7B 7 is an open-sourced RNN-based language model trained on more than 100 world languages. Among Chineseoriented LLMs, ChatGLM-130B 8 and ChatGLM2-6B 9 are pre-trained using the algorithm of General Language Model ) on bilingual (English &amp; Chinese) dataset and further fine-tuned on conversational data and aligned with human preferences. MOSS  is considered by some the first open-source ChatGPT-like Chinese model. We evaluate the moss-moon-003-sft version in our experiment. Ziya-13B-v1.1 ) is a large-scale pre-trained model based on LLaMA (Touvron et al. 2023). MiniMax 10 is a newly-generated Chinese LLM based on Transformer. Wenxin Yiyan (v2.0.4) 11 , 360 Brain (4.0) 12 and SparkDesk (v1.5) 13 are other Chinese LLMs whose architectures and training details are not public.</p>
<p>Results</p>
<p>The overall results of all the models and the more detailed results on each capability are provided in Table 2 and Table 3 respectively. Radar charts that directly show model performance on each capability category are shown in Figure 5.</p>
<p>Comparison between GPT-4 and Chinese LLMs. GPT-4 outperforms notably than all other models on CLOSE, OPEN SINGLE, and OPEN ALL. MiniMax is the secondbest model overall and the best-performing Chinese LLM. Compared to MiniMax, GPT-4 achieves more than ten percentage points on CLOSE and almost twice the win rate on the OPEN benchmarks. Such results indicate a large gap between Chinese-oriented models and the current topperforming models in the world.</p>
<p>Comparison among Chinese LLMs. MiniMax is the top model on the LangYa Leaderboard, while the second-best model ChatGLM2-6B lags behind with nearly a 1% win rate. More specifically, Minimax beats ChatGLM2-6B in five capability categories, including Small Talk, Role Playing, Knowledge and Encyclopedia, Generation and Creation, and Logic and Reasoning. This observation suggests that complementing MiniMax with ChatGLM2 is a possible way to develop comprehensive models. We also find that all Chinese models perform similarly on the CLOSE benchmark, with seven of eight models scoring between 55% and 60%, while their performance on OPEN varies from 12.50% to 41.48%. This phenomenon suggests that closeended multi-choice questions alone may not differentiate model capabilities.</p>
<p>Additional Analysis</p>
<p>In this section, we demonstrate the high agreement between superior models like GPT-4 and human raters in a Chinese context. This validates the rationale for using GPT-4 as an
模型A (model A)： 在没有任何外力的情 况下，…...，使得它 的落点正好是在原地 附近。
In the absence of any external force,..., so that its falling point is just around the original place. (in English) 模型B (model B)： 这听起来像是想要实现一个"自回球"的效果。 …...</p>
<p>练习技巧：实现自回效果需要一定的技巧和练习。你可以尝试多次抛球，并逐渐调整抛球的角度、力度和旋转方式，以找到最佳的组合方法。</p>
<p>This sounds like trying to achieve a "self-returning ball" effect. ... 3. Practice skills: It takes certain skills and practice to achieve the self-recovery effect. You can try multiple tosses and gradually adjust the angle, power, and spin of the toss to find the best combination. (in English) 问题：你怎样才能把一个球尽量大力地抛出去，然后球又会折回来，甚至它没有碰到任何东西，也没有任何牵制物，也没有人接到再抛回来？ Question (in English): How can you throw a ball as hard as you can and it will come back, even if it doesn't touch anything, and there's nothing to hold it back, and no one catches it and throws it back?</p>
<p>在这种情况下，模型B的回答更接近参考答案和用户问题的需求。虽然模型A提到了利用地球引力让球改变路径，但它没有提到如何实现球在空中回旋的技巧。而模型B给出了一些 建议和方法，包括给球一个旋转、选择合适的球和练习技巧，以实现球在空中抛出后自回到手中的效果。因此，模型B的回答更全面且有用。</p>
<p>In this context, the response from Model B aligns more closely with the reference answer and caters effectively to the user's query. While Model A does allude to using Earth's gravity to alter the ball's trajectory, it omits any mention of the technique required to induce spin in the ball whilst airborne. In contrast, Model B provides a series of practical suggestions and methodologies, including imparting spin to the ball, selecting the appropriate ball, and honing specific skills, thereby facilitating the ball's return to the thrower's hand post-launch. Consequently, Model B's response is more comprehensive and beneficial. (in English)  alternative to human evaluators in our work. We then illustrate the limitations of the close-ended multi-choice format in evaluating model performance.</p>
<p>GPT-4 评价 (evaluation of GPT-4)：</p>
<p>Figure 4: LLM Judge example from OPEN</p>
<p>High Agreement between GPT-4 and Human Evaluation on OPEN</p>
<p>We adopt GPT-4 as the default evaluator for OPEN data in our work. In order to investigate the evaluation agreement between humans and GPT-4, human raters are asked to assess a subset of model-generated answers to OPEN questions. As in the GPT-4 evaluation process, humans receive responses from one anonymous model and ChatGPT simultaneously. They must choose a better one or indicate that both are equally good. We then conduct Pearson correlation (Lee Rodgers and Nicewander 1988) between the average win rates evaluated by GPT-4 and human reviewers and obtain a high agreement of 80%. This result indicates that the GPT-4 tends to closely align with those of humans. The scatter plot of the average win rate evaluated by humans and GPT-4 is shown in Figure 6. </p>
<p>Inconsistency of Chinese Model Performance on CLOSE Set and OPEN SINGLE Set</p>
<p>The relationship between CLOSE Set accuracy and GPT-4 scores on OPEN SINGLE Set is analyzed by Spearman and Pearson Correlation Analysis (Spearman 1987;Lee Rodgers and Nicewander 1988). The results show that there is no significant correlation between them, with Spearman coefficient ρ = 0.5150 with p-value p = 0.1915, as well as Pearson coefficient ρ = 0.5547 with p-value p = 0.1536.</p>
<p>To further explore the relationship, we analyze the difference between the two evaluation results of Chinese LLMs in data distribution and probability density. As shown in Table 2, the accuracy on CLOSE is mainly concentrated between 55% and 60%, while the average win rate on OPEN SINGLE varies from 27.00% to 65.32%. Moreover, the vi-   olin plot (see Figure 7) presents that the distribution of the model's performance on the OPEN SINGLE Set is much more discrete than that on the CLOSE Set, with coefficients of variation of 0.11 and 0.34 respectively. That is, closedended questions are not suitable as a discriminative benchmark, and cannot fully reflect model performance in openended interactive scenarios.</p>
<p>Complementarity of the CLOSE Set and OPEN Set</p>
<p>Although the above analysis demonstrates that the CLOSE Set is not as relevant as the OPEN Set for predicting model performance in the real world, we still believe that capabilities measured on the CLOSE Set could complement the OPEN Set to help understand real users' preferences better. Therefore, we combine the CLOSE, OPEN SINGLE and OPEN MULTIPLE with linear coefficients summed as 1 and make a correlation analysis with CArena. As shown in Table 4, accuracy on the CLOSE Set has no significant correlation with user preference in CArena, which aligns with our hypothesis and demonstration above. However, the combination with the CLOSE Set makes either OPEN SINGLE or  Table 4: Results of Pearson correlation analysis between model performance on all possible linear combinations of the CLOSE Set, OPEN SINGLE and OPEN MULTIPLE and CArena. We report the best-fit results for each linear combination with the format of (whether the variable is selected | its linear coefficient). The best linear combination with the linear coefficients of CLOSE, OPEN SINGLE and OPEN MULTIPLE are 0.49, 0, and 0.51 respectively. * p &lt; 0.05. The largest coefficient with a significant correlation is bolded.
| 1 | 0 | 0 0.7950 | 0 | 1 | 0 0.8747 | 0 | 0 | 1 0.8985<em> | 0.5440 | 0.4560 | 0 0.9132</em> | 0.4900 | 0 | 0.5100 0.9397<em> | 0 | 0 | 1 0.8985</em> | 0.4900 | 0 | 0.5100 0.9397*
OPEN MULTIPLE has a more significant and higher correlation with CArena than each alone. Moreover, we find another interesting phenomenon that despite users just interact with models in one turn, their preferences are more reflected from models' performance on OPEN MULTIPLE (significant correlation of rho = 0.8985) rather than OPEN SINGLE (close to significant correlation rho = 0.8747). These observations suggest that utilizing evaluation results on both CLOSE and OPEN MULTIPLE benefits reflecting user preference in real-life scenarios.</p>
<p>Conclusion</p>
<p>In this paper, we introduce SuperCLUE, a comprehensive benchmark for Chinese large models that includes three separate and complementary subtasks: CArena, a place where users can communicate and evaluate models; the OPEN Set, with open-ended single-and multi-turn questions; and the CLOSE Set, with closed-ended questions. We use GPT-4 for automated scoring answers to open-ended questions and demonstrate its consistency with human raters. Evaluation results show that there is still a large gap between Chinese LLMs and the top-performing models in the world. We also conducted extensive analysis to demonstrate that closed-ended questions alone cannot align with user preferences in real-world scenarios, while the joint use of closedand open-ended questions is a better way to conduct a comprehensive evaluation. We will further expand our test set, train our evaluation model, and provide evaluation services to serve more community users.</p>
<p>A Evaluation Process Automatic Evaluation</p>
<p>GPT-4 is required to discern which model performs better, with potential responses being 'model A is superior', 'model B is superior', or 'a tie'. Each of our topic pairs undergoes two rounds of dialogue. During the initial round, the adjudicator's evaluation is solely based on the content of this round. However, in the subsequent round, while the adjudicator is privy to the content of the initial dialogue, the evaluation primarily concentrates on the merits and demerits of the content presented in the second round.</p>
<p>Evaluation Interfaces</p>
<p>The evaluation interfaces for users in the model battle platform is presented in Figure 9.</p>
<p>Zero-shot Evaluation Examples</p>
<p>The zero-shot evaluation examples of CLOSE and OPEN are presented in Figure 10 and 11.</p>
<p>Transformation Process of OPEN SINGLE to CLOSE</p>
<p>Initially, by crafting a fitting prompt, we enabled  to transfigure open-ended questions into closed-ended ones. The prompt we developed for transforming openended questions into closed-ended ones within OPEN SIN-GLE are visually demonstrated in Figure 12. Subsequently, upon acquiring the answers generated by GPT-3.5, the reformulated questions were systematically arranged and subsequently subjected to manual evaluation to verify their compliance with the stipulated prompt requirements. Each question endured a three-stage review process, with each stage comprising a team of three human reviewers. Their collaborative task was to scrutinize and, where necessary, amend any questions that did not meet the prescribed requirements to ensure their alignment with the set guidelines.</p>
<p>B Capability Categories</p>
<p>Detailed Definitions </p>
<p>Annotation Process</p>
<p>Based on the definitions and implications of the ten capability categories, we classified user queries from the LangYa Leaderboard platform. Initially, a portion of the queries were manually annotated, resulting in 300 data entries, with 30 entries dedicated to each capability category. Subsequently, these 300 annotated entries were used to train a BERT classifier. The trained classifier was then employed to classify the remaining data, assigning a specific capability category label to each query sample. Finally, a collaboration of four human evaluators reviewed and rectified the classification results produced by the BERT classifier, which led to the determination of the final capability category for each individual sample.</p>
<p>Model A： 大熊猫最喜欢吃竹子，尤其是竹笋和竹叶。它们每天需要吃掉大约30公斤的竹子来维持生命。此外， 大熊猫还会吃一些水果、坚果和小鱼等食物。</p>
<p>Giant pandas like to eat bamboo, especially bamboo shoots and bamboo leaves. They need to eat about 30 kilograms of bamboo every day to maintain their lives. In addition, giant pandas will also eat some food such as fruits, nuts and small fish. (in English)</p>
<p>Model B： 大熊猫最喜欢吃的东西是竹子，特别是 竹子的嫩芽和嫩叶，它们也会吃其他植物和水果，但竹子是它们的主要 食物来源。据估计，一只成年大熊猫每天需要吃约12公斤的竹子来维持身体健康。</p>
<p>The giant panda's favorite food is bamboo, especially the shoots and leaves of bamboo. They will also eat other plants and fruits, but bamboo is their main source of food. It is estimated that an adult giant panda needs to eat about 12 kilograms of bamboo every day to maintain good health. (in English)</p>
<p>Model A: 除了竹子，大熊猫还喜欢吃一些其他的食物，比如青菜、香蕉、苹果和牛肉等。不过它们通常只会在 特定的季节或者情况下食用这些食物。</p>
<p>In addition to bamboo, giant pandas also like to eat some other food, such as green vegetables, bananas, apples and beef. However, they usually only eat these foods during certain seasons or situations.(in English)   </p>
<p>Evaluation of GPT-4：</p>
<p>模型A的回答更好。在回答第二个问题时，模型A提供了详细的、相关的信息，包括J.K.罗琳的创作灵感、魔法世界的构建、角色塑造、对抗邪恶的主题以及文化元素的融入等。这些信息都是对用户问题的直接和深入的回答。然而，模型B在回答 第二个问题时，没有提供任何有 关J.K.罗琳创作《哈利·波特》系列的独特理念和灵感来源的信息，而是重复了第一个问题和答案，然后提供了一些无关的选项，这并没有回答用户的问题。因此，模型A的回答更好。</p>
<p>Model A offers a superior response. When addressing the second query, Model A presents an array of detailed and pertinent information encompassing J.K. Rowling's creative spark, the architectural design of the magical realm, character development, the prevailing theme of combating evil, and the assimilation of cultural elements, among other aspects. This information provides direct and profound responses to the user's inquiries. In contrast, Model B's approach to the second question falls short, as it fails to divulge any insights into J.K. Rowling's unique philosophical mindset or the sources of inspiration for the Harry Potter series. Instead, it merely reiterates the initial question and response, thus failing to meet the user's query. Consequently, Model A's response proves to be more effective. (in English) Figure 11: An example of zero-shot evaluation in OPEN. The red text is the auto-completed response from model, while the preceding text is the inputted prompt. We indicate English translation below the corresponding Chinese text.</p>
<p>Figure 1 :
1SuperCLUE Benchmark combines three complementary evaluation methods, including open-ended questions, multiple-choice questions and side-by-side comparisons with human evaluation.</p>
<p>Figure 2 :
2An example question in OPEN SINGLE and its counterpart in OPEN MULTIPLE. The blue text is the follow-up question designed by humans.</p>
<p>Figure 3 :
3An example question in CLOSE and its counterpart in OPEN SINGLE with the same stem (the orange text).</p>
<p>Figure 5 :
5Radar charts of capabilities distribution on OPEN of chosen LLMs.</p>
<p>Figure 6 :Figure 7 :
67The scatter plot of scores evaluated by GPT-4 and human, with the best-fit line. The violin plot of Chinese LLMs' accuracy on CLOSE Set and average win rate on OPEN SINGLE Set.</p>
<p>Figure 8 :
8Multi-turn dialogues between a user and two AI assistants-RWKV-world-7B (model A) and GPT-3.5-turbo (model B)--initiated by a question from the OPEN and a follow-up instruction. A Superior model (GPT-4) is then presented with the context to determine which assistant answers better. We can see that both models can strive to do multiple-choice questions, but for open-ended follow-up questions, the quality of the answers is quite different.</p>
<p>Figure 9 :Figure 10 :
910LangYa following is a multiple-choice question about knowledge and encyclopedia. Please choose the most appropriate option from the list. You can only answer with the letters A, An example of zero-shot evaluation in CLOSE. The red text is the auto-completed response from model, while the preceding text is the inputted prompt. We indicate English translation below the corresponding Chinese text.Model A： 谁是《哈利·波特》系列的作者？J.K.罗琳 Who is the author of the Harry Potter series? A. J.K. Rowling (in English) Model B： J.K.罗琳是《哈利·波特》系列的作者。 A. J.K. Rowling is the author of the Harry Potter series. (English) Model A: 以下哪个作家是《哈利·波特》系列的作者？ …... D. 受电影导演大卫启发 Which of the following authors is the author of the Harry Potter series? ... D. Inspired by film director David (in English) Model B: J.K.罗琳在创作《哈利·波特》系列时,有以下独特的创作理念和灵感来源: …... 5. 文化元素:J.K.罗琳在《哈利·波特》系列中加入了多种文化元素,如英国文学、神话传说、爱尔兰传统等,营造出了一种独特的文学风格和氛 围。 When J.K. Rowling created the "Harry Potter" series, she had the following unique creative concepts and sources of inspiration: ... 5. Cultural elements: J.K. Rowling added a variety of cultural elements to the "Harry Potter" series, such as English literature, myths and legends, and Irish traditions, creating a unique literary style and atmosphere. (in English) Question：《哈利·波特》系列的作者是谁？ Question (in English)： Who is the author of the Harry Potter series? (in English) Follow-up question: 这位作家在创作《哈利·波特》系列时，有哪些独特的创作理念和灵感来源？ Follow-up question (in English): When creating the "Harry Potter" series, what unique creative concepts and sources of inspiration did this writer have? (in English)</p>
<p>Table 2 :
2The overall results of models' zero-shot accuracy on CLOSE, win&amp;tie rate on OPEN SINGLE, OPEN MULTI, OPEN ALL and CArena. The best-performing results in each column are bolded.</p>
<p>Table 3 :
3Zero-shot results on each capability. Each result of each model corresponding to each capability is reported in the form of accuracy on CLOSE, average win rate on OPEN SINGLE, OPEN MULTI, OPEN ALL, and CArena from top to bottom.</p>
<p>Table 5
5presents the detailed definitions of capability cate-
gories. </p>
<p>In addition to bamboo, giant pandas also eat some other plants and fruits, such as: 1. Honey: Giant pandas like to eat honey, and sometimes they will eat bees and bee pupae in the hive. 2. Corn, wheat and beans: In the wild, giant pandas sometimes eat these crops, especially in areas near farmland. 3. Fruits: Giant pandas also eat some fruits, such as apples, grapes and oranges. However, it should be noted that the diet of giant pandas is mainly based on bamboo, because their gastrointestinal tract and digestive system have adapted to bamboo, a high-fiber, low-nutrient food. Other foods are just supplements to their diet. (in English)Question：说出大熊猫最喜欢吃的东西。Question (in English)： Tell me what pandas like to eat the most. (in English)Follow-up question: 它还有什么喜欢吃的东西。Follow-up question (in English): What else does it like to eat? (in English)Evaluation of GPT-4：模型B的回答更好，在回答第二个问题方面，模型B提供了更详细且具有创造性的回答。模型B列出了大熊猫喜欢吃的其他食物，如蜜糖、玉米、小麦、豆类和水果，并解释了它们的饮食习惯。虽然模型A也提供了一些 额外的食物，但模型B的回答更具深度和详细程度。Model B gave a better answer, providing a more detailed and creative answer to the second question. Model B lists other foods that giant pandas like to eat, such as honey, corn, wheat, beans and fruits, and explains their eating habits. While Model A also provided some additional food, Model B's responses had more depth and detail. (in English)Model B: 
除了竹子之外，大熊猫也会吃一些其他的植物和水果，例如： 
1. 蜜糖：大熊猫喜欢吃蜜糖，有时它们会吃蜂巢里的蜜蜂和蜂蛹。 
2. 玉米、小麦和豆类：在野外，大熊猫有时会吃这些作物 ，尤其是在农田附近的地区。 
3. 水果：大熊猫也会吃一些水果，例如苹果、葡萄和橙子等。 
然而，需要注意的是，大熊猫的饮食主要还是以竹子为主，因为它们的胃肠道和消化系统已经适应了竹子这种高 
纤维、低营养的食物。其他的食物只是它们饮食中的补充品。 </p>
<p>The Elo rating system calculates the relative skill levels of players in zero-sum games.
The name LangYa Leaderboard is inspired by a Chinese TV series about heroes fighting for the top spot. You can access the platform from https://www.CLUEbenchmarks.com.
https://claude.ai/ 6 https://chat.openai.com 7 https://github.com/BlinkDL/ChatRWKV
下面是一个示例：Here is an example:`` 原始问题：现在假设你是小红的好友，小红在网上看到一些夸大其词的减肥方法并试图效仿，请你构思一些想法用来阻止他。 Original question: Now assuming you are a friend of Xiaohong, Xiaohong has seen some exaggerated weight loss methods online and intends to follow suit. Please come up with some ideas to stop him.  This refers to an ability that enables language models to understand and interpret the significance of entered textual information. The model must be capable of identifying the meanings of phrases, sentences, and paragraphs, in addition to being able to extract crucial information and themes from more extensive text blocks.Small Talk (Casual Conversation)This refers to the proficiency of a language model to engage in free-form, non-specific-goal dialogues with users. The model needs to demonstrate the capacity to generate responses that are fluent, natural, and in alignment with linguistic practices and cultural nuances.Contextual ConversationThis signifies a particular proficiency of the language model, necessitating the understanding and retention of preceding dialogue information to maintain coherence in its responses. It involves the comprehension of the overall conversation flow and the surrounding context, or the generation of corresponding dialogue.Generation and CreationThis denotes a distinct capability of language models, enabling them to generate novel textual content, such as articles, copywriting, short stories, and poetry. It involves the creative utilization of language, while also taking into consideration aspects such as style, context, and the target audience.Knowledge Understanding and Application Knowledge &amp; EncyclopediaThis represents a distinct capability of language models, allowing them to provide knowledge information akin to an encyclopedia. This encompasses understanding and responding to questions about a wide array of topics, as well as providing accurate, detailed, and up-to-date information.Professional AbilitiesCode GenerationThis refers to a unique capability of language models, enabling them to understand and generate programming code. This involves the comprehension of the syntax, structure, and conventions of various programming languages, as well as the methodologies to solve programming problems.Logic and ReasoningThis denotes a distinctive capability of language models to understand and apply principles of logic for reasoning. It involves the analysis of problems, identification of issues,and the process of inference.CalculationThis refers to a unique ability inherent in language models that enables them to perform mathematical operations, such as addition, subtraction, multiplication, and division, and even more complex mathematical problems. This encompasses understanding the formulation of mathematical problems and the methodology to solve these problems in a step-by-step manner.Environmental Adaptation and SafetyRole-playingThis denotes a distinctive capability of language models, allowing them to assume a role within specific simulated environments or scenarios. This involves comprehending the behavior, speaking style, and appropriate responses of a specific role under designated circumstances.SafetyThis refers to the capability of a language model to prevent the generation of content that could potentially cause distress or harm. This encompasses the identification and avoidance of requests that may contain sensitive or inappropriate content, as well as adherence to policies on user privacy and safety.
T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, arXiv:2005.14165Language Models are Few-Shot Learners. Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165.</p>
<p>W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%<em> ChatGPT Quality. Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica, I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%</em> ChatGPT Quality.</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesotaLong and Short Papers1: Association for Computational LinguisticsDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con- ference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo- gies, Volume 1 (Long and Short Papers), 4171-4186. Min- neapolis, Minnesota: Association for Computational Lin- guistics.</p>
<p>Automatically Constructing a Corpus of Sentential Paraphrases. W B Dolan, C Brockett, Proceedings of the Third International Workshop on Paraphrasing (IWP2005). the Third International Workshop on Paraphrasing (IWP2005)Dolan, W. B.; and Brockett, C. 2005. Automatically Con- structing a Corpus of Sentential Paraphrases. In Proceed- ings of the Third International Workshop on Paraphrasing (IWP2005).</p>
<p>GLM: General Language Model Pretraining with Autoregressive Blank Infilling. Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsLong Papers1Du, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and Tang, J. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 320-335.</p>
<p>News Summarization and Evaluation in the Era of GPT-3. T Goyal, J J Li, G Durrett, arXiv:2209.12356Goyal, T.; Li, J. J.; and Durrett, G. 2023. News Summariza- tion and Evaluation in the Era of GPT-3. arXiv:2209.12356.</p>
<p>Z Gu, X Zhu, H Ye, L Zhang, J Wang, S Jiang, Z Xiong, Z Li, Q He, R Xu, W Huang, Z Wang, S Wang, W Zheng, H Feng, Y Xiao, arXiv:2306.05783Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. Gu, Z.; Zhu, X.; Ye, H.; Zhang, L.; Wang, J.; Jiang, S.; Xiong, Z.; Li, Z.; He, Q.; Xu, R.; Huang, W.; Wang, Z.; Wang, S.; Zheng, W.; Feng, H.; and Xiao, Y. 2023. Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowl- edge Evaluation. arXiv:2306.05783.</p>
<p>Measuring Massive Multitask Language Understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, abs/2009.03300Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2020. Measuring Massive Mul- titask Language Understanding. CoRR, abs/2009.03300.</p>
<p>Y Huang, Y Bai, Z Zhu, J Zhang, J Zhang, T Su, J Liu, C Lv, Y Zhang, J Lei, Y Fu, M Sun, J He, arXiv:2305.08322C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. arXiv preprintHuang, Y.; Bai, Y.; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu, J.; Lv, C.; Zhang, Y.; Lei, J.; Fu, Y.; Sun, M.; and He, J. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. arXiv preprint arXiv:2305.08322.</p>
<p>E Kasneci, K Seßler, S Küchemann, M Bannert, D Dementieva, F Fischer, U Gasser, G Groh, S Günnemann, E Hüllermeier, ChatGPT for Good? On Opportunities and Challenges of Large Language Models for Education. Kasneci, E.; Seßler, K.; Küchemann, S.; Bannert, M.; De- mentieva, D.; Fischer, F.; Gasser, U.; Groh, G.; Günnemann, S.; Hüllermeier, E.; and et al. 2023. ChatGPT for Good? On Opportunities and Challenges of Large Language Mod- els for Education.</p>
<p>Thirteen ways to look at the correlation coefficient. Lee Rodgers, J Nicewander, W A , The American Statistician. 421Lee Rodgers, J.; and Nicewander, W. A. 1988. Thirteen ways to look at the correlation coefficient. The American Statistician, 42(1): 59-66.</p>
<p>Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. A Lewkowycz, A Slone, A Andreassen, D Freeman, E S Dyer, G Mishra, G Gur-Ari, J Lee, J Sohl-Dickstein, K Chiafullo, L B Fedus, N Fiedel, R Liu, V Misra, V V Ramasesh, Technical reportLewkowycz, A.; Slone, A.; Andreassen, A.; Freeman, D.; Dyer, E. S.; Mishra, G.; Gur-Ari, G.; Lee, J.; Sohl-dickstein, J.; Chiafullo, K.; Fedus, L. B.; Fiedel, N.; Liu, R.; Misra, V.; and Ramasesh, V. V. 2022. Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. Technical report.</p>
<p>Al-pacaEval: An Automatic Evaluator of Instruction-following Models. X Li, T Zhang, Y Dubois, R Taori, I Gulrajani, C Guestrin, P Liang, T B Hashimoto, Li, X.; Zhang, T.; Dubois, Y.; Taori, R.; Gulrajani, I.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Al- pacaEval: An Automatic Evaluator of Instruction-following Models. https://github.com/tatsu-lab/alpaca eval.</p>
<p>P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, B Newman, B Yuan, B Yan, C Zhang, C Cosgrove, C D Manning, C Ré, D Acosta-Navas, D A Hudson, E Zelikman, E Durmus, F Ladhak, F Rong, H Ren, H Yao, J Wang, K Santhanam, L Orr, L Zheng, M Yuksekgonul, M Suzgun, N Kim, N Guha, N Chatterji, O Khattab, P Henderson, Q Huang, R Chi, S M Xie, S Santurkar, S Ganguli, T Hashimoto, T Icard, T Zhang, V Chaudhary, W Wang, X Li, Y Mai, Y Zhang, Y Koreeda, arXiv:2211.09110Holistic Evaluation of Language Models. Liang, P.; Bommasani, R.; Lee, T.; Tsipras, D.; Soylu, D.; Yasunaga, M.; Zhang, Y.; Narayanan, D.; Wu, Y.; Kumar, A.; Newman, B.; Yuan, B.; Yan, B.; Zhang, C.; Cosgrove, C.; Manning, C. D.; Ré, C.; Acosta-Navas, D.; Hudson, D. A.; Zelikman, E.; Durmus, E.; Ladhak, F.; Rong, F.; Ren, H.; Yao, H.; Wang, J.; Santhanam, K.; Orr, L.; Zheng, L.; Yuk- sekgonul, M.; Suzgun, M.; Kim, N.; Guha, N.; Chatterji, N.; Khattab, O.; Henderson, P.; Huang, Q.; Chi, R.; Xie, S. M.; Santurkar, S.; Ganguli, S.; Hashimoto, T.; Icard, T.; Zhang, T.; Chaudhary, V.; Wang, W.; Li, X.; Mai, Y.; Zhang, Y.; and Koreeda, Y. 2022. Holistic Evaluation of Language Models. arXiv:2211.09110.</p>
<p>Putting ChatGPT's Medical Advice to the (Turing) Test. O Nov, N Singh, D Mann, L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, arXiv:2301.10035.OpenAI.2023.GPT-4arXiv:2203.02155Training language models to follow instructions with human feedback. Technical ReportNov, O.; Singh, N.; and Mann, D. 2023. Putting ChatGPT's Medical Advice to the (Turing) Test. arXiv:2301.10035. OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774. Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens, M.; Askell, A.; Welinder, P.; Christiano, P.; Leike, J.; and Lowe, R. 2022. Training language models to follow instruc- tions with human feedback. arXiv:2203.02155.</p>
<p>Squad: 100,000+ questions for machine comprehension of text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, arXiv:1606.05250arXiv preprintRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.</p>
<p>ChatGPT utility in healthcare education, research, and practice: systematic review on the promising perspectives and valid concerns. M Sallam, Healthcare. MDPI11887Sallam, M. 2023. ChatGPT utility in healthcare education, research, and practice: systematic review on the promising perspectives and valid concerns. In Healthcare, volume 11, 887. MDPI.</p>
<p>SuperGlue: Learning Feature Matching with Graph Neural Networks. P Sarlin, D Detone, T Malisiewicz, A Rabinovich, abs/1911.11763CoRRSarlin, P.; DeTone, D.; Malisiewicz, T.; and Rabinovich, A. 2019. SuperGlue: Learning Feature Matching with Graph Neural Networks. CoRR, abs/1911.11763.</p>
<p>Recursive deep models for semantic compositionality over a sentiment treebank. R Socher, A Perelygin, J Wu, J Chuang, C D Manning, A Y Ng, C Potts, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processingSocher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C. D.; Ng, A. Y.; and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, 1631-1642.</p>
<p>The Proof and Measurement of Association between Two Things. C Spearman, The American Journal of Psychology. 1003/4Spearman, C. 1987. The Proof and Measurement of As- sociation between Two Things. The American Journal of Psychology, 100(3/4): 441-471.</p>
<p>T Sun, X Zhang, Z He, P Li, Q Cheng, H Yan, X Liu, Y Shao, Q Tang, X Zhao, K Chen, Y Zheng, Z Zhou, R Li, J Zhan, Y Zhou, L Li, X Yang, L Wu, Z Yin, X Huang, X Qiu, MOSS: Training Conversational Language Models from Synthetic Data. Sun, T.; Zhang, X.; He, Z.; Li, P.; Cheng, Q.; Yan, H.; Liu, X.; Shao, Y.; Tang, Q.; Zhao, X.; Chen, K.; Zheng, Y.; Zhou, Z.; Li, R.; Zhan, J.; Zhou, Y.; Li, L.; Yang, X.; Wu, L.; Yin, Z.; Huang, X.; and Qiu, X. 2023. MOSS: Training Conver- sational Language Models from Synthetic Data.</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, arXiv:2302.13971LLaMA: Open and Efficient Foundation Language Models. Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G. 2023. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971.</p>
<p>GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S Bowman, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational LinguisticsWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 353- 355. Brussels, Belgium: Association for Computational Lin- guistics.</p>
<p>. J Wang, Y Zhang, L Zhang, P Yang, X Gao, Z Wu, X Dong, J He, J Zhuo, Q Yang, Y Huang, X Li, Y Wu, J Lu, X Zhu, W Chen, T Han, K Pan, R Wang, H Wang, X Wu, Z Zeng, C Chen, R Gan, J Zhang, Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence. CoRR, abs/2209.02970Wang, J.; Zhang, Y.; Zhang, L.; Yang, P.; Gao, X.; Wu, Z.; Dong, X.; He, J.; Zhuo, J.; Yang, Q.; Huang, Y.; Li, X.; Wu, Y.; Lu, J.; Zhu, X.; Chen, W.; Han, T.; Pan, K.; Wang, R.; Wang, H.; Wu, X.; Zeng, Z.; Chen, C.; Gan, R.; and Zhang, J. 2022. Fengshenbang 1.0: Being the Foundation of Chi- nese Cognitive Intelligence. CoRR, abs/2209.02970.</p>
<p>A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. A Williams, N Nangia, S Bowman, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLong Papers. Association for Computational LinguisticsWilliams, A.; Nangia, N.; and Bowman, S. 2018. A Broad- Coverage Challenge Corpus for Sentence Understanding through Inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, Vol- ume 1 (Long Papers), 1112-1122. Association for Compu- tational Linguistics.</p>
<p>CLUE: A Chinese Language Understanding Evaluation Benchmark. L Xu, H Hu, X Zhang, L Li, C Cao, Y Li, Y Xu, K Sun, D Yu, C Yu, Y Tian, Q Dong, W Liu, B Shi, Y Cui, J Li, J Zeng, R Wang, W Xie, Y Li, Y Patterson, Z Tian, Y Zhang, H Zhou, S Liu, Z Zhao, Q Zhao, C Yue, X Zhang, Z Yang, K Richardson, Z Lan, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational Linguistics4762Xu, L.; Hu, H.; Zhang, X.; Li, L.; Cao, C.; Li, Y.; Xu, Y.; Sun, K.; Yu, D.; Yu, C.; Tian, Y.; Dong, Q.; Liu, W.; Shi, B.; Cui, Y.; Li, J.; Zeng, J.; Wang, R.; Xie, W.; Li, Y.; Pat- terson, Y.; Tian, Z.; Zhang, Y.; Zhou, H.; Liu, S.; Zhao, Z.; Zhao, Q.; Yue, C.; Zhang, X.; Yang, Z.; Richardson, K.; and Lan, Z. 2020. CLUE: A Chinese Language Understanding Evaluation Benchmark. In Proceedings of the 28th Inter- national Conference on Computational Linguistics, 4762-</p>
<p>Spain Barcelona, International Committee on Computational Linguistics. Barcelona, Spain (Online): International Committee on Computational Linguistics.</p>
<p>A Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. arXiv preprintZeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.; Yang, Z.; Xu, Y.; Zheng, W.; Xia, X.; et al. 2022. Glm- 130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.</p>
<p>H Zeng, arXiv:2304.12986Measuring Massive Multitask Chinese Understanding. Zeng, H. 2023. Measuring Massive Multitask Chinese Un- derstanding. arXiv:2304.12986.</p>
<p>S Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen, C Dewan, M Diab, X Li, X V Lin, T Mihaylov, M Ott, S Shleifer, K Shuster, D Simig, P S Koura, A Sridhar, T Wang, L Zettlemoyer, arXiv:2205.01068OPT: Open Pre-trained Transformer Language Models. Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.; Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V.; Mi- haylov, T.; Ott, M.; Shleifer, S.; Shuster, K.; Simig, D.; Koura, P. S.; Sridhar, A.; Wang, T.; and Zettlemoyer, L. 2022. OPT: Open Pre-trained Transformer Language Mod- els. arXiv:2205.01068.</p>
<p>L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J E Gonzalez, I Stoica, arXiv:2306.05685Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang, H.; Gonzalez, J. E.; and Stoica, I. 2023. Judg- ing LLM-as-a-judge with MT-Bench and Chatbot Arena. arXiv:2306.05685.</p>
<p>W Zhong, R Cui, Y Guo, Y Liang, S Lu, Y Wang, A S S Saied, W Chen, N Duan, abs/2304.06364AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. Zhong, W.; Cui, R.; Guo, Y.; Liang, Y.; Lu, S.; Wang, Y.; Saied, A. S. S.; Chen, W.; and Duan, N. 2023. AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. ArXiv, abs/2304.06364.</p>
<p>A Robustly Optimized BERT Pre-training Approach with Posttraining. L Zhuang, L Wayne, S Ya, Z Jun, Proceedings of the 20th Chinese National Conference on Computational Linguistics. the 20th Chinese National Conference on Computational LinguisticsHuhhot, ChinaChinese Information Processing Society of ChinaZhuang, L.; Wayne, L.; Ya, S.; and Jun, Z. 2021. A Ro- bustly Optimized BERT Pre-training Approach with Post- training. In Proceedings of the 20th Chinese National Con- ference on Computational Linguistics, 1218-1227. Huhhot, China: Chinese Information Processing Society of China.</p>            </div>
        </div>

    </div>
</body>
</html>
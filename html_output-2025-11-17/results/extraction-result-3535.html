<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3535 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3535</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3535</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-f2b0017ddd77fa38760a18145e63553105a1a236</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f2b0017ddd77fa38760a18145e63553105a1a236" target="_blank">The Flan Collection: Designing Data and Methods for Effective Instruction Tuning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> It is found task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings actually yields stronger performance in all settings.</p>
                <p><strong>Paper Abstract:</strong> We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17%+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+) performance in all settings. In further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks, motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3535.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3535.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (T5-XL, LM-adapted, instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-LM-adapted prefix language model (XL-size) fine-tuned on the Flan 2022 instruction collection (1800+ tasks) using mixed prompt settings (zero-shot, few-shot, Chain-of-Thought), input-inversion augmentations, and mixture-balancing; evaluated on multi-domain reasoning benchmarks including MMLU, BIG-Bench Hard, and Chain-of-Thought datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5 (T5-XL, LM-adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prefix-LM adapted T5 architecture finetuned with the Flan 2022 instruction collection. Training uses mixed zero-shot/few-shot/Chain-of-Thought templates, input-inversion augmentations (mixed in at ~30% for some sources), and task-source mixture balancing across ~1.8k tasks. Experiments in the paper used the XL (3B parameter) checkpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MMLU; BIG-Bench Hard (BBH); Chain-of-Thought evaluation suite (GSM8K, StrategyQA, SVAMP, Asdiv, CommonsenseQA); Held-In QA & NLI validation sets (BoolQ, ARC Easy/Challenge, AI2 Middle School, ANLI R1/R2/R3, RTE)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>MMLU: multi-domain subject-matter exams testing knowledge and reasoning across 57 subjects; BBH: a subset of BIG-Bench containing 23 hard tasks where PaLM underperforms humans; Chain-of-Thought suite: tasks requiring multi-step reasoning / arithmetic (GSM8K), commonsense and multi-step justification (StrategyQA, CommonsenseQA), and other multi-step math/semantic reasoning tasks; Held-In QA/NLI are standard QA and natural language inference evaluation sets.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Instruction tuning on Flan 2022 with (1) mixed prompt settings during training (zero-shot, few-shot, and Chain-of-Thought templates jointly), (2) inclusion of Chain-of-Thought exemplars in finetuning, (3) input-inversion data augmentation (generate inverted (x,y) tasks, mixed at ~30% rate for some sources), (4) task-source mixture balancing, and (5) scaling across ~1.8k tasks; few-shot exemplars generated with 2/3/5 exemplars during finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in paper (zero-shot / few-shot): Held-In mean accuracy 73.8% / 74.8%; Chain-of-Thought suite mean accuracy 35.8% / 34.1%; MMLU 50.3% / 52.4%; BBH 26.2% / 39.3%; BBH with Chain-of-Thought (BBH-CoT) 33.9% / 35.2%. (All numbers as reported in Table 1 of the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>T5-XL finetuned on Flan 2021: Held-In 68.4% / 56.3%; CoT 24.6% / 22.7%; MMLU 41.4% / 34.8%; BBH 28.1% / 28.3%; BBH-CoT 26.0% / 26.9%. (Also compared against T5-XL finetuned on P3++ and Super-Natural Instructions in Table 1.)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Flan-T5-XL outperforms the next best comparably-sized T5-XL baselines by substantial margins (row 'Flan 2022 - Next Best T5-XL' in Table 1): Held-In +3.3% (zero-shot) / +12% (few-shot); CoT +10.2% / +8.5%; MMLU +4.2% / +17.6%; BBH -1.9% / +8.5%; BBH-CoT +7.9% / +8.3%. Compared to T5-XL finetuned on Flan 2021 specifically, MMLU improved by ~8.9% (zero-shot) and ~17.6% (few-shot) in reported numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Absolute Chain-of-Thought and BBH accuracies remain modest (CoT mid-30s% for XL/3B model). Ablations show removing Chain-of-Thought finetuning massively degrades CoT performance and BBH-CoT (e.g., CoT drops from 35.8/34.1 to 28.8/24.6; BBH-CoT drops to 18.2/12.0). Input inversion helps Held-Out (MMLU/BBH) but does not improve Held-In scores. Performance is sensitive to mixture weights and task composition; some Held-In tasks peak at ~200 tasks and then decline if too many tasks are added from one source. The paper also notes instruction tuning (without human feedback) may not address all open-ended alignment issues and that comparisons to non-public models may be confounded by pretraining and template differences.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Multiple ablations reported: removing Chain-of-Thought data ('- CoT') sharply reduces CoT and BBH-CoT scores; removing Input Inversion lower MMLU/BBH (MMLU from 50.3/52.4 down to 41.7/41.2); removing mixture balancing reduces performance across metrics; removing Few-Shot templates decreases few-shot performance. Section 3.2 shows mixed prompt training: adding as little as 5% few-shot templates improves zero-shot performance (~+2%+), and ≥10% zero-shot improves few-shot performance. Section 3.3 shows Held-Out (MMLU) performance increases roughly log-linearly with the number of finetuning tasks; Held-In peaks earlier (~200 tasks). These ablations attribute gains to mixed prompt training, CoT finetuning for CoT tasks, input-inversion for Held-Out generalization, and mixture balancing for overall robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Flan Collection: Designing Data and Methods for Effective Instruction Tuning', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3535.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3535.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-XL (other collections)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-XL finetuned on alternative instruction collections (Flan 2021, P3++, Super-Natural Instructions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline T5-XL models (3B) finetuned on earlier instruction-tuning collections used as direct comparators to Flan-T5-XL; these collections are typically more targeted to zero-shot templates and/or use different templatization and mixture strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-XL (finetuned on Flan 2021 / P3++ / Super-Natural Instructions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prefix-LM adapted T5 architecture (XL / 3B) finetuned on earlier public instruction collections; these training regimes often emphasize zero-shot templates (Flan 2021) or different mixes (P3++, Super-Natural Instructions) and do not include the full Flan 2022 recipe of mixed prompt training, CoT finetuning, input inversion, and mixture balancing used in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same evaluation suite as Flan-T5 (Held-In QA/NLI, Chain-of-Thought suite, MMLU, BBH).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Held-In: collection of QA and NLI tasks; CoT suite: multi-step arithmetic and commonsense reasoning tasks; Held-Out: MMLU and BBH as described above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Instruction tuning using the respective collection's templates and mixtures (Flan 2021 primarily zero-shot templates, P3++/PromptSource mixes, Super-Natural Instructions with its own templatization). Generally did not jointly train many mixed prompt settings and had less extensive CoT or input-inversion augmentation in these baselines as used here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in Table 1: T5-XL Flan 2021: Held-In 68.4% / 56.3%; CoT 24.6% / 22.7%; MMLU 41.4% / 34.8%; BBH 28.1% / 28.3%; BBH-CoT 26.0% / 26.9%. T5-XL P3++: Held-In 70.5% / 62.8%; CoT 25.6% / 25.6%; MMLU 46.1% / 34.1%; BBH 26.0% / 30.8%; BBH-CoT 23.4% / 26.1%. T5-XL Super-Natural Instructions: Held-In 50.3% / 42.2%; CoT 13.8% / 14.3%; MMLU 35.6% / 31.1%; BBH 10.4% / 15.6%; BBH-CoT 8.0% / 12.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Flan-T5-XL (the paper's final recipe) improves over these baselines by several percentage points across settings (paper reports 3-17% Held-Out/Held-In improvements over prior open-source collections depending on metric and prompt setting).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>These baselines are often tuned specifically to a single prompt setting (commonly zero-shot) and so show reduced few-shot or CoT performance. Super-Natural Instructions in particular shows limited scaling benefits on Held-Out (MMLU) in the authors' analysis, possibly due to unique input formats.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper contrasts Flan 2022 ablative choices against these alternative collections, attributing Flan gains to mixed prompt training, CoT inclusion, input inversion, and balancing; the ablations show which of those methods most affect which metrics (e.g., CoT inclusion most affects CoT evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Flan Collection: Designing Data and Methods for Effective Instruction Tuning', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3535.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3535.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OPT-IML-Max-175B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OPT-IML-Max (175B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction/meta-learned OPT-family model reported by Iyer et al., used in the paper as an external comparator (not trained or analyzed in this work), reported results on MMLU and BBH are shown for reference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OPT-IML: Scaling language model instruction meta learning through the lens of generalization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT-IML-Max (reported 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OPT-family large decoder-only models instruction/meta-trained on mixtures of public tasks and used as a large-scale competitor; the paper reports OPT-IML-Max at 30B and 175B as comparators but does not run or modify these models here.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MMLU; BBH (reported by OPT-IML and cited in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>MMLU: multi-subject reasoning/knowledge benchmark; BBH: difficult BIG-Bench tasks used to evaluate reasoning where large models may struggle.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>OPT-IML's own instruction/meta-learning recipe (external to this paper) — mentioned as comparator; specifics not reimplemented here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>As reported in this paper (from Iyer et al., 2022): OPT-IML-Max 175B MMLU 49.1% / 47.1% (zero-shot / few-shot reported in table); BBH reported as - / 35.7% in table (some BBH numbers not fully reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>The paper reports Flan-T5-XL (3B) outperforms OPT-IML-Max 175B on MMLU and BBH in their comparisons (despite OPT-IML-Max being much larger), but notes differences may be due to pretraining, model architecture, and instruction tuning template/mixing differences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>OPT-IML's exact templates, processing, and example mixing were not released at the time, making apples-to-apples comparison difficult; the paper cautions that differences may stem from pretraining and architectural differences rather than solely instruction-tuning recipes.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>This paper does not ablate OPT-IML itself; it analyzes how Flan 2022's methods (mixed prompts, CoT, input inversion, balancing) could explain performance differences and notes that approx. 94% of OPT-IML tasks overlap with Flan 2022, implying template/mixing choices and pretraining may be primary drivers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Flan Collection: Designing Data and Methods for Effective Instruction Tuning', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3535.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3535.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLM-130B (open bilingual pre-trained model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large bilingual pre-trained model (GLM-130B) cited in the paper as a reported external baseline for MMLU performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GLM-130B: An open bilingual pre-trained model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 130B-parameter bilingual pre-trained model (decoder / general architecture described in Zeng et al., 2022). Mentioned as an external point of comparison; not trained or evaluated directly by this paper's authors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>130B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MMLU (reported in references / table)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>MMLU measures multi-domain knowledge and some reasoning ability across academic subjects.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>External baseline reporting; specifics of GLM-130B's instruction tuning/finetuning are outside this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported (as cited in Table 1 of this paper): MMLU - / 44.8% (only a few-shot or single column reported in the table excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Flan-T5-XL (3B) reports MMLU 50.3% / 52.4% which is higher than the GLM-130B MMLU figure cited in the table for the comparable column, per the paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>GLM-130B numbers are cited from its own report; this paper notes different pretraining and evaluation details across public and non-public models make direct comparison difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No ablation of GLM-130B in this paper; GLM-130B is only an externally-cited comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Flan Collection: Designing Data and Methods for Effective Instruction Tuning', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scaling instruction-finetuned language models <em>(Rating: 2)</em></li>
                <li>OPT-IML: Scaling language model instruction meta learning through the lens of generalization <em>(Rating: 2)</em></li>
                <li>Finetuned language models are zero-shot learners <em>(Rating: 2)</em></li>
                <li>MetaICL: Learning to learn in context <em>(Rating: 2)</em></li>
                <li>GLM-130B: An open bilingual pre-trained model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3535",
    "paper_id": "paper-f2b0017ddd77fa38760a18145e63553105a1a236",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "Flan-T5-XL",
            "name_full": "Flan-T5 (T5-XL, LM-adapted, instruction-tuned)",
            "brief_description": "A T5-LM-adapted prefix language model (XL-size) fine-tuned on the Flan 2022 instruction collection (1800+ tasks) using mixed prompt settings (zero-shot, few-shot, Chain-of-Thought), input-inversion augmentations, and mixture-balancing; evaluated on multi-domain reasoning benchmarks including MMLU, BIG-Bench Hard, and Chain-of-Thought datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5 (T5-XL, LM-adapted)",
            "model_description": "Prefix-LM adapted T5 architecture finetuned with the Flan 2022 instruction collection. Training uses mixed zero-shot/few-shot/Chain-of-Thought templates, input-inversion augmentations (mixed in at ~30% for some sources), and task-source mixture balancing across ~1.8k tasks. Experiments in the paper used the XL (3B parameter) checkpoint.",
            "model_size": "3B",
            "reasoning_task_name": "MMLU; BIG-Bench Hard (BBH); Chain-of-Thought evaluation suite (GSM8K, StrategyQA, SVAMP, Asdiv, CommonsenseQA); Held-In QA & NLI validation sets (BoolQ, ARC Easy/Challenge, AI2 Middle School, ANLI R1/R2/R3, RTE)",
            "reasoning_task_description": "MMLU: multi-domain subject-matter exams testing knowledge and reasoning across 57 subjects; BBH: a subset of BIG-Bench containing 23 hard tasks where PaLM underperforms humans; Chain-of-Thought suite: tasks requiring multi-step reasoning / arithmetic (GSM8K), commonsense and multi-step justification (StrategyQA, CommonsenseQA), and other multi-step math/semantic reasoning tasks; Held-In QA/NLI are standard QA and natural language inference evaluation sets.",
            "method_or_intervention": "Instruction tuning on Flan 2022 with (1) mixed prompt settings during training (zero-shot, few-shot, and Chain-of-Thought templates jointly), (2) inclusion of Chain-of-Thought exemplars in finetuning, (3) input-inversion data augmentation (generate inverted (x,y) tasks, mixed at ~30% rate for some sources), (4) task-source mixture balancing, and (5) scaling across ~1.8k tasks; few-shot exemplars generated with 2/3/5 exemplars during finetuning.",
            "performance": "Reported in paper (zero-shot / few-shot): Held-In mean accuracy 73.8% / 74.8%; Chain-of-Thought suite mean accuracy 35.8% / 34.1%; MMLU 50.3% / 52.4%; BBH 26.2% / 39.3%; BBH with Chain-of-Thought (BBH-CoT) 33.9% / 35.2%. (All numbers as reported in Table 1 of the paper.)",
            "baseline_performance": "T5-XL finetuned on Flan 2021: Held-In 68.4% / 56.3%; CoT 24.6% / 22.7%; MMLU 41.4% / 34.8%; BBH 28.1% / 28.3%; BBH-CoT 26.0% / 26.9%. (Also compared against T5-XL finetuned on P3++ and Super-Natural Instructions in Table 1.)",
            "improvement_over_baseline": "Flan-T5-XL outperforms the next best comparably-sized T5-XL baselines by substantial margins (row 'Flan 2022 - Next Best T5-XL' in Table 1): Held-In +3.3% (zero-shot) / +12% (few-shot); CoT +10.2% / +8.5%; MMLU +4.2% / +17.6%; BBH -1.9% / +8.5%; BBH-CoT +7.9% / +8.3%. Compared to T5-XL finetuned on Flan 2021 specifically, MMLU improved by ~8.9% (zero-shot) and ~17.6% (few-shot) in reported numbers.",
            "limitations_or_failures": "Absolute Chain-of-Thought and BBH accuracies remain modest (CoT mid-30s% for XL/3B model). Ablations show removing Chain-of-Thought finetuning massively degrades CoT performance and BBH-CoT (e.g., CoT drops from 35.8/34.1 to 28.8/24.6; BBH-CoT drops to 18.2/12.0). Input inversion helps Held-Out (MMLU/BBH) but does not improve Held-In scores. Performance is sensitive to mixture weights and task composition; some Held-In tasks peak at ~200 tasks and then decline if too many tasks are added from one source. The paper also notes instruction tuning (without human feedback) may not address all open-ended alignment issues and that comparisons to non-public models may be confounded by pretraining and template differences.",
            "ablation_or_analysis": "Multiple ablations reported: removing Chain-of-Thought data ('- CoT') sharply reduces CoT and BBH-CoT scores; removing Input Inversion lower MMLU/BBH (MMLU from 50.3/52.4 down to 41.7/41.2); removing mixture balancing reduces performance across metrics; removing Few-Shot templates decreases few-shot performance. Section 3.2 shows mixed prompt training: adding as little as 5% few-shot templates improves zero-shot performance (~+2%+), and ≥10% zero-shot improves few-shot performance. Section 3.3 shows Held-Out (MMLU) performance increases roughly log-linearly with the number of finetuning tasks; Held-In peaks earlier (~200 tasks). These ablations attribute gains to mixed prompt training, CoT finetuning for CoT tasks, input-inversion for Held-Out generalization, and mixture balancing for overall robustness.",
            "uuid": "e3535.0",
            "source_info": {
                "paper_title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "T5-XL (other collections)",
            "name_full": "T5-XL finetuned on alternative instruction collections (Flan 2021, P3++, Super-Natural Instructions)",
            "brief_description": "Baseline T5-XL models (3B) finetuned on earlier instruction-tuning collections used as direct comparators to Flan-T5-XL; these collections are typically more targeted to zero-shot templates and/or use different templatization and mixture strategies.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "T5-XL (finetuned on Flan 2021 / P3++ / Super-Natural Instructions)",
            "model_description": "Prefix-LM adapted T5 architecture (XL / 3B) finetuned on earlier public instruction collections; these training regimes often emphasize zero-shot templates (Flan 2021) or different mixes (P3++, Super-Natural Instructions) and do not include the full Flan 2022 recipe of mixed prompt training, CoT finetuning, input inversion, and mixture balancing used in this work.",
            "model_size": "3B",
            "reasoning_task_name": "Same evaluation suite as Flan-T5 (Held-In QA/NLI, Chain-of-Thought suite, MMLU, BBH).",
            "reasoning_task_description": "Held-In: collection of QA and NLI tasks; CoT suite: multi-step arithmetic and commonsense reasoning tasks; Held-Out: MMLU and BBH as described above.",
            "method_or_intervention": "Instruction tuning using the respective collection's templates and mixtures (Flan 2021 primarily zero-shot templates, P3++/PromptSource mixes, Super-Natural Instructions with its own templatization). Generally did not jointly train many mixed prompt settings and had less extensive CoT or input-inversion augmentation in these baselines as used here.",
            "performance": "Reported in Table 1: T5-XL Flan 2021: Held-In 68.4% / 56.3%; CoT 24.6% / 22.7%; MMLU 41.4% / 34.8%; BBH 28.1% / 28.3%; BBH-CoT 26.0% / 26.9%. T5-XL P3++: Held-In 70.5% / 62.8%; CoT 25.6% / 25.6%; MMLU 46.1% / 34.1%; BBH 26.0% / 30.8%; BBH-CoT 23.4% / 26.1%. T5-XL Super-Natural Instructions: Held-In 50.3% / 42.2%; CoT 13.8% / 14.3%; MMLU 35.6% / 31.1%; BBH 10.4% / 15.6%; BBH-CoT 8.0% / 12.5%.",
            "baseline_performance": null,
            "improvement_over_baseline": "Flan-T5-XL (the paper's final recipe) improves over these baselines by several percentage points across settings (paper reports 3-17% Held-Out/Held-In improvements over prior open-source collections depending on metric and prompt setting).",
            "limitations_or_failures": "These baselines are often tuned specifically to a single prompt setting (commonly zero-shot) and so show reduced few-shot or CoT performance. Super-Natural Instructions in particular shows limited scaling benefits on Held-Out (MMLU) in the authors' analysis, possibly due to unique input formats.",
            "ablation_or_analysis": "Paper contrasts Flan 2022 ablative choices against these alternative collections, attributing Flan gains to mixed prompt training, CoT inclusion, input inversion, and balancing; the ablations show which of those methods most affect which metrics (e.g., CoT inclusion most affects CoT evaluation).",
            "uuid": "e3535.1",
            "source_info": {
                "paper_title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "OPT-IML-Max-175B",
            "name_full": "OPT-IML-Max (175B)",
            "brief_description": "A large instruction/meta-learned OPT-family model reported by Iyer et al., used in the paper as an external comparator (not trained or analyzed in this work), reported results on MMLU and BBH are shown for reference.",
            "citation_title": "OPT-IML: Scaling language model instruction meta learning through the lens of generalization",
            "mention_or_use": "mention",
            "model_name": "OPT-IML-Max (reported 175B)",
            "model_description": "OPT-family large decoder-only models instruction/meta-trained on mixtures of public tasks and used as a large-scale competitor; the paper reports OPT-IML-Max at 30B and 175B as comparators but does not run or modify these models here.",
            "model_size": "175B",
            "reasoning_task_name": "MMLU; BBH (reported by OPT-IML and cited in this paper).",
            "reasoning_task_description": "MMLU: multi-subject reasoning/knowledge benchmark; BBH: difficult BIG-Bench tasks used to evaluate reasoning where large models may struggle.",
            "method_or_intervention": "OPT-IML's own instruction/meta-learning recipe (external to this paper) — mentioned as comparator; specifics not reimplemented here.",
            "performance": "As reported in this paper (from Iyer et al., 2022): OPT-IML-Max 175B MMLU 49.1% / 47.1% (zero-shot / few-shot reported in table); BBH reported as - / 35.7% in table (some BBH numbers not fully reported in this paper).",
            "baseline_performance": null,
            "improvement_over_baseline": "The paper reports Flan-T5-XL (3B) outperforms OPT-IML-Max 175B on MMLU and BBH in their comparisons (despite OPT-IML-Max being much larger), but notes differences may be due to pretraining, model architecture, and instruction tuning template/mixing differences.",
            "limitations_or_failures": "OPT-IML's exact templates, processing, and example mixing were not released at the time, making apples-to-apples comparison difficult; the paper cautions that differences may stem from pretraining and architectural differences rather than solely instruction-tuning recipes.",
            "ablation_or_analysis": "This paper does not ablate OPT-IML itself; it analyzes how Flan 2022's methods (mixed prompts, CoT, input inversion, balancing) could explain performance differences and notes that approx. 94% of OPT-IML tasks overlap with Flan 2022, implying template/mixing choices and pretraining may be primary drivers.",
            "uuid": "e3535.2",
            "source_info": {
                "paper_title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "GLM-130B",
            "name_full": "GLM-130B (open bilingual pre-trained model)",
            "brief_description": "A large bilingual pre-trained model (GLM-130B) cited in the paper as a reported external baseline for MMLU performance.",
            "citation_title": "GLM-130B: An open bilingual pre-trained model",
            "mention_or_use": "mention",
            "model_name": "GLM-130B",
            "model_description": "A 130B-parameter bilingual pre-trained model (decoder / general architecture described in Zeng et al., 2022). Mentioned as an external point of comparison; not trained or evaluated directly by this paper's authors.",
            "model_size": "130B",
            "reasoning_task_name": "MMLU (reported in references / table)",
            "reasoning_task_description": "MMLU measures multi-domain knowledge and some reasoning ability across academic subjects.",
            "method_or_intervention": "External baseline reporting; specifics of GLM-130B's instruction tuning/finetuning are outside this paper.",
            "performance": "Reported (as cited in Table 1 of this paper): MMLU - / 44.8% (only a few-shot or single column reported in the table excerpt).",
            "baseline_performance": null,
            "improvement_over_baseline": "Flan-T5-XL (3B) reports MMLU 50.3% / 52.4% which is higher than the GLM-130B MMLU figure cited in the table for the comparable column, per the paper's summary.",
            "limitations_or_failures": "GLM-130B numbers are cited from its own report; this paper notes different pretraining and evaluation details across public and non-public models make direct comparison difficult.",
            "ablation_or_analysis": "No ablation of GLM-130B in this paper; GLM-130B is only an externally-cited comparator.",
            "uuid": "e3535.3",
            "source_info": {
                "paper_title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scaling instruction-finetuned language models",
            "rating": 2
        },
        {
            "paper_title": "OPT-IML: Scaling language model instruction meta learning through the lens of generalization",
            "rating": 2
        },
        {
            "paper_title": "Finetuned language models are zero-shot learners",
            "rating": 2
        },
        {
            "paper_title": "MetaICL: Learning to learn in context",
            "rating": 2
        },
        {
            "paper_title": "GLM-130B: An open bilingual pre-trained model",
            "rating": 1
        }
    ],
    "cost": 0.020293,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Flan Collection: Designing Data and Methods for Effective Instruction Tuning</h1>
<p>Shayne Longpre* Le Hou Tu Vu Albert Webson Hyung Won Chung<br>Yi Tay Denny Zhou Quoc V. Le Barret Zoph Jason Wei Adam Roberts</p>
<p>Google Research</p>
<h4>Abstract</h4>
<p>We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 models (Chung et al., 2022). Through careful ablation studies on the Flan Collection of instruction tuning tasks and methods, we tease apart the effect of design decisions that enable FlanT5 to outperform prior work by $3-17 \%+$ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger ( $2 \%+$ ) performance in all settings. In further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks-motivating instruction-tuned models as more computationallyefficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available. ${ }^{1}$</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparing public instruction tuning collections on Held-In, Held-Out (BIG-Bench Hard (Suzgun et al., 2022) and MMLU (Hendrycks et al., 2020)), and Chain-of-Thought evaluation suites, detailed in Appendix A.3. All models except OPT-IML-Max (175B) are T5-XL with 3B parameters. Green text indicates absolute improvement over the next best comparable T5-XL (3B) model.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>1 Introduction</p>
<p>Large language models such as PaLM (Chowdhery et al., 2022), Chinchilla (Hoffmann et al., 2022), and ChatGPT among others (Brown et al., 2020; Ouyang et al., 2022) have unlocked new capabilities in performing natural language processing (NLP) tasks from reading instructive prompts. Prior art has shown that instruction tuning-finetuning language models on a collection of NLP tasks formatted with instructionsfurther enhances the ability of language models to perform an unseen task from an instruction (Wei et al., 2021; Sanh et al., 2021; Min et al., 2022).</p>
<p>In this work, we evaluate the methods and results of open sourced instruction generalization efforts, comparing their finetuning techniques and methods. And in particular, we identify and evaluate the critical methodological improvements in the "Flan 2022 Collection", which is the term we use for the collection of data and methods for data augmentation and instruction tuning, first implemented and used in Chung et al. (2022). Where Chung et al. (2022) focuses on the emergent and state-of-the-art results of combining Flan 2022 with PaLM 540B, this work focuses in on the details of the instruction tuning methods themselves, ablating individual factors, and comparing them directly to prior work by keeping the pretrained model size and checkpoint consistent.</p>
<p>The Flan 2022 Collection offers the most extensive publicly available set of tasks and methods for instruction tuning, which we have compiled in one place. We have also supplemented this with hundreds more of our own high-quality templates, richer formatting patterns, and data augmentations. We show that a model trained on this collection outperforms other public collections on all tested evaluation benchmarks, including the original Flan 2021 (Wei et al., 2021), T0++ (Sanh et al., 2021), Super-Natural Instructions (Wang et al., 2022c), and the concurrent work on OPT-IML (Iyer et al., 2022). As shown in Figure 1, this includes 4.2\%+ and $8.5 \%$ improvements on the MMLU (Hendrycks et al., 2020) and BIG-Bench Hard (Suzgun et al., 2022) evaluation benchmarks respectively, for equally sized models.</p>
<p>Analysis of the Flan 2022 method suggests the strong results stem both from the larger and more diverse set of tasks, but also from a set of simple finetuning and data augmentation techniques. In particular, training on a mix of examples templatized with zero-shot, few-shot, and chain-of-thought prompts improves performance in every one of these settings, together. For instance, adding just $10 \%$ few-shot prompts improves zero-shot prompting results by $2 \%+$. Additionally, enriching task diversity by inverting input-output pairs, as used in (Sanh et al., 2021; Min et al., 2022), along with balancing task sources, are both shown to be critical to performance. The resulting Flan-T5 model converges faster and at a higher performance than T5 models in single-task finetuning-suggesting instruction-tuned models offer a more computationally-efficient starting checkpoint for downstream applications, corroborating Aribandi et al. (2021) and Liu et al. (2022b).</p>
<p>We hope making these findings and resources publicly available will unify resources around instruction tuning and accelerate research into more general-purpose language models. We summarize this work's core contributions as follows:</p>
<ul>
<li>Methodological: Show that training with mixed zero- and few-shot prompts yields much better performance in both settings (Section 3.2).</li>
<li>Methodological: Measure and demonstrate the critical techniques to effective instruction tuning: scaling Section 3.3, enriching task variety with input inversion (Section 3.4), adding chain-of-thought training data, and balancing different data sources (Section 3.5).</li>
<li>Results: Demonstrate these technical choices yield 3-17\% Held-Out task improvements over existing open source instruction tuning collections (Figure 1).</li>
<li>Results: Demonstrate Flan-T5 serves as a stronger and more computationally-efficient starting checkpoint for single-task finetuning (Section 4).</li>
<li>Open source the new Flan 2022 task collection, templates, and methods for public research.</li>
</ul>
<p>2 Public Instruction Tuning Collections
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A Timeline of Public Instruction Tuning Collections specifies the collection release date, detailed information on the finetuned models (the base model, their size, and whether the model itself is Public (P) or Not Public (NP)), what prompt specification they were trained for (zero-shot, few-shot, or Chain-of-Thought), the number of tasks contained in the Flan 2022 Collection (released with this work), and core methodological contributions in each work.
Note that the number of tasks and of examples vary under different assumptions and so are estimates. For instance, the definition of "task" and "task category" vary by work, and are not easily simplified to one ontology. The reported counts for the number of tasks are reported using task definitions from the respective works.
${ }^{\dagger}$ indicates concurrent work.</p>
<p>Large Language Models Instruction tuning has emerged as a tool to make large language models (LLMs) and their abilities more useful for interactive dialog and functional tasks. Previous work (Raffel et al., 2020; Liu et al., 2019; Aghajanyan et al., 2021; Aribandi et al., 2021) experimented with large scale multi-task finetuning, to improve downstream single target finetuning, but without instruction prompts. UnifiedQA and others (Khashabi et al., 2020; McCann et al., 2018; Keskar et al., 2019) unified a wide range of NLP tasks into a single generative question answering format, using prompt instructions for multi-task finetuning and evaluation.</p>
<p>The First Wave Since 2020, several instruction tuning task collections have been released in rapid succession, outlined in Figure 2. Natural Instructions (Mishra et al., 2021), Flan 2021 (Wei et al., 2021), P3 (the Public Pool of Prompts, Bach et al., 2022) aggregated large NLP task collections and templatized them with instructions (zero-shot prompting), specifically for finetuning models to generalize to unseen instructions. MetaICL (Min et al., 2022) also consolidated other task collections (Ye et al., 2021; Khashabi et al., 2020) to train models to learn tasks "in-context" - from several input-output examples, known as few-shot prompting, but in this case without instructions. Each of these works affirmed the scaling benefits of task and template diversity,</p>
<p>and some reported strong benefits from inverting the inputs and outputs in templates to produce new tasks ("noisy channel" in Min et al., 2022).</p>
<p>The Second Wave A second wave of instruction tuning collections expanded prior resources: combining more datasets and tasks into one resource, like Super-Natural Instructions (Wang et al., 2022c) or OPT-IML (Iyer et al., 2022), adding multilingual instruction tuning in xP3 (Muennighoff et al., 2022), and Chain-ofThought training prompts in Flan 2022 (Chung et al., 2022). Both the Flan Collection and OPT-IML contain most tasks represented in prior collections. ${ }^{2}$ Our work is positioned here, coalescing most of these collections (of collections) and their methods, as the strongest starting point for future open source work.</p>
<p>New Directions Concurrent and future work is beginning to explore two new directions: (a) expanding task diversity even more aggressively with synthetic data generation, particularly in creative, and open-ended dialogue (Wang et al., 2022b; Honovich et al., 2022; Ye et al., 2022; Gupta et al., 2022), and (b) offering human feedback signals on model responses (Ouyang et al., 2022; Glaese et al., 2022; Bai et al., 2022a; Nakano et al., 2021; Bai et al., 2022b). We view most of these new directions as likely additive to a foundation of instruction tuning methods.</p>
<p>Tuning with Human Feedback Instruction tuning on human feedback has demonstrated strong results on open-ended tasks, but at the expense of performance on a wide array of more traditional NLP tasks (Ouyang et al., 2022; Glaese et al., 2022; Bai et al., 2022a; Nakano et al., 2021). (See Ouyang et al. (2022)'s discussion of the "alignment tax".) Our work focuses specifically on instruction generalization, without human feedback, for two reasons. First, human feedback datasets are far less publicly available than instruction tuning datasets (and may be model-specific). Second, by itself, instruction generalization shows great promise in enhancing human preferred responses on open-ended tasks, as well as improving traditional NLP metrics (Chung et al., 2022). The extent of obtainable progress without expensive human response demonstrations or ratings remains an open question, and an important pursuit to narrow the gap between public and non-public research.</p>
<p>The Importance of Open Source High profile research is increasingly driven by non-public data, as in the case of GPT-3 and others (Ouyang et al., 2022; Glaese et al., 2022). The inaccessibility of these resources inhibits the research community's ability to analyze and improve these methods in the public domain. We narrow our purview to open source and accessible data collections, motivated by the goal of democratizing accessibility to research.</p>
<h1>3 Flan 2022 Instruction Tuning Experiments</h1>
<p>Recent research has yet to coalesce around a unified set of techniques, with different tasks, model sizes, and target input formats all represented. We open source a new collection, first introduced in Chung et al. (2022), denoted "Flan 2022", which combines Flan 2021, P3++33, Super-Natural Instructions, with some additional reasoning, dialog, and program synthesis datasets. We defer to Chung et al. (2022) for details of templatization and collection; and in this work we take a deeper look at key methodological improvements and compare the collection on equivalent model sizes to existing collections.</p>
<p>In this section, we evaluate the design decisions in Flan and discuss four in particular that yield strong improvements to the instruction tuning recipe. These design components, outlined in Section 2, are: (I) using mixed zero-shot, few-shot, and Chain-of-Thought templates at training (Section 3.2), (II) scaling T5sized models to 1800+ tasks (Section 3.3), (III) enriching tasks with input inversion (Section 3.4), and (IV) balancing these task mixtures (Section 3.5). In Section 3.1, we begin by measuring the value of each component and compare the final model against alternative instruction tuning collections (and their methods).
${ }^{2}$ Note that each work defines datasets, tasks, and task categories differently. For simplicity, we use their own definitions in Section 2.
${ }^{3}$ "P3++" is our notation for all datasets in the Public Pool of Prompts (P3): https://huggingface.co/datasets/bigscience/P3</p>
<p>Experimental Setup We finetune on the prefix language model adapted T5-LM (Lester et al., 2021), using the XL (3B) size for all models for consistency, unless otherwise stated. While other sizes of Flan-T5 are available, we felt XL was appropriately sized to run large-scale systematic ablations, while being sufficiently large to draw general conclusions. We evaluate on (a) a suite of 8 "Held-In" tasks represented within the 1800+ training task collection ( 4 question answering and 4 natural language inference validation sets), (b) Chain-of-Thought (CoT) tasks (5 validation sets), and (c) the MMLU (Hendrycks et al., 2020) and BBH (Suzgun et al., 2022) benchmarks as our set of "Held-Out" tasks, as they are not included as part of Flan 2022 finetuning. The Massivley Multitask Language Understanding benchmark (MMLU) broadly tests reasoning and knowledge capacity across 57 tasks in the sciences, social sciences, humanities, business, health, among other subjects. BIG-Bench Hard (BBH) includes 23 challenging tasks from BIG-Bench (Srivastava et al., 2022) where PaLM under-performs human raters. In our ablations, we also evaluate BBH with Chain-ofThought inputs, following Chung et al. (2022). Additional finetuning and evaluation details are provided in Appendix A.</p>
<h1>3.1 Ablation Studies</h1>
<p>Table 1 summarizes the mean contribution to Held-in, Held-out, and Chain-of-thought tasks, by individually deducting methods: mixture weight balancing ("- Mixture Balancing"), Chain-of-thought tasks ("- CoT"), mixed prompt settings ("- Few Shot Templates"), and Input Inversion ("- Input Inversion"). Flan-T5 XL leverages all four of these methods together. We also finetune T5-XL-LM on other collections, including Flan 2021, P3++, Super-Natural Instructions for comparison.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Held-In</th>
<th style="text-align: center;">CoT</th>
<th style="text-align: center;">MMLU</th>
<th style="text-align: center;">BBH</th>
<th style="text-align: center;">BBH-CoT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">T5-XL Flan 2022</td>
<td style="text-align: center;">$\mathbf{7 3 . 8 / 7 4 . 8}$</td>
<td style="text-align: center;">$35.8 / \mathbf{3 4 . 1}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 3 / 5 2 . 4}$</td>
<td style="text-align: center;">$26.2 / \mathbf{3 9 . 3}$</td>
<td style="text-align: center;">$\mathbf{3 3 . 9 / 3 5 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">- CoT</td>
<td style="text-align: center;">$73.3 / 73.2$</td>
<td style="text-align: center;">$28.8 / 24.6$</td>
<td style="text-align: center;">$47.5 / 46.9$</td>
<td style="text-align: center;">$18.2 / 30.0$</td>
<td style="text-align: center;">$18.2 / 12.0$</td>
</tr>
<tr>
<td style="text-align: left;">- Input Inversion</td>
<td style="text-align: center;">$\mathbf{7 3 . 8 / 7 4 . 1}$</td>
<td style="text-align: center;">$32.2 / 23.5$</td>
<td style="text-align: center;">$41.7 / 41.2$</td>
<td style="text-align: center;">$18.4 / 24.2$</td>
<td style="text-align: center;">$15.7 / 13.0$</td>
</tr>
<tr>
<td style="text-align: left;">- Mixture Balancing</td>
<td style="text-align: center;">$71.2 / 73.1$</td>
<td style="text-align: center;">$32.3 / 30.5$</td>
<td style="text-align: center;">$45.4 / 45.8$</td>
<td style="text-align: center;">$15.1 / 24.3$</td>
<td style="text-align: center;">$13.8 / 15.4$</td>
</tr>
<tr>
<td style="text-align: left;">- Few Shot Templates</td>
<td style="text-align: center;">$72.5 / 62.2$</td>
<td style="text-align: center;">$\mathbf{3 8 . 9 / 2 8 . 6}$</td>
<td style="text-align: center;">$47.3 / 38.7$</td>
<td style="text-align: center;">$27.6 / 30.8$</td>
<td style="text-align: center;">$18.6 / 23.3$</td>
</tr>
<tr>
<td style="text-align: left;">T5-XL Flan 2021</td>
<td style="text-align: center;">$68.4 / 56.3$</td>
<td style="text-align: center;">$24.6 / 22.7$</td>
<td style="text-align: center;">$41.4 / 34.8$</td>
<td style="text-align: center;">$\mathbf{2 8 . 1 / 2 8 . 3}$</td>
<td style="text-align: center;">$26.0 / 26.9$</td>
</tr>
<tr>
<td style="text-align: left;">T5-XL P3++</td>
<td style="text-align: center;">$70.5 / 62.8$</td>
<td style="text-align: center;">$25.6 / 25.6$</td>
<td style="text-align: center;">$46.1 / 34.1$</td>
<td style="text-align: center;">$26.0 / 30.8$</td>
<td style="text-align: center;">$23.4 / 26.1$</td>
</tr>
<tr>
<td style="text-align: left;">T5-XL Super-Natural Inst.</td>
<td style="text-align: center;">$50.3 / 42.2$</td>
<td style="text-align: center;">$13.8 / 14.3$</td>
<td style="text-align: center;">$35.6 / 31.1$</td>
<td style="text-align: center;">$10.4 / 15.6$</td>
<td style="text-align: center;">$8.0 / 12.5$</td>
</tr>
<tr>
<td style="text-align: left;">GLM-130B ${ }^{\dagger}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$-/ 44.8$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">OPT-IML-Max 30B ${ }^{\dagger}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$46.3 / 43.2$</td>
<td style="text-align: center;">$-/ 30.9$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">OPT-IML-Max 175B ${ }^{\dagger}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$49.1 / 47.1$</td>
<td style="text-align: center;">$-/ 35.7$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Flan 2022 - Next Best T5-XL</td>
<td style="text-align: center;">$+3.3 /+12$</td>
<td style="text-align: center;">$+10.2 /+8.5$</td>
<td style="text-align: center;">$+4.2 /+17.6$</td>
<td style="text-align: center;">$-1.9 /+8.5$</td>
<td style="text-align: center;">$+7.9 /+8.3$</td>
</tr>
</tbody>
</table>
<p>Table 1: Method Ablations (top) show the importance of each method for Flan-T5 XL. Collection Ablations (bottom) evaluate Flan-T5 XL against T5-XL finetuned on other instruction tuning collections: FLAN 2021, P3++, and Super-Natural Instructions. Flan 2022 - Next Best T5-XL shows the improvement of Flan-T5 XL over the next best T5-XL (comparatively sized) finetuned on another collection. Metrics are reported in both zero-shot / few-shot settings across Held-In, Chain-of-Thought, and Held-Out (MMLU, BBH) tasks.
${ }^{\dagger}$ We also inlcude the results reported by OPT-IML (Iyer et al., 2022) and GLM-130B (Zeng et al., 2022).
Each of the ablated components of Flan contributes improvements to different metrics: Chain-of-Thought training to Chain-of-Thought evaluation, input inversion to Held-Out evaluations (MMLU and BBH), few-shot prompt training to few-shot evaluations, and mixture balancing to all metrics.</p>
<p>As compared to T5-XL models trained on alternative instruction tuning collections (and their methods), Flan outperforms in almost every setting. While previous collections are tuned specifically to zero-shot prompts, Flan-T5 XL is tuned for either zero- or few-shot prompts. This yields performance margins of $+3-10 \%$ for most of the zero-shot settings, and margins of $8-17 \%$ for the few-shot settings. Most impressively, Flan 2022 outperforms OPT-IML-Max's much larger (10x) 30B and (58x) 175B models. Next, we isolate some of Flan 2022's ablated methods individually, to examine the benefits of each.</p>
<h1>3.2 Training with Mixed Prompt Settings</h1>
<p>Prior work has shown a wide variety of input templates per task can improve performance. However, separate from the wording of the instruction template, these prior LLMs mostly tune with template sets targeted to a single prompt setting: for zero-shot prompting (Wei et al., 2021; Sanh et al., 2021; Aghajanyan et al., 2021; Aribandi et al., 2021) or for few-shot prompting (Min et al., 2022; Wang et al., 2022c).</p>
<p>An underappreciated design decision in InstructGPT (Ouyang et al., 2022) was to mix training templates for each of these prompt settings, rather than target a single setting. However, since Ouyang et al. (2022) do not examine this choice, we expected a performance trade-off in finetuning for zero-shot or few-shot prompting performance - particularly for smaller models. Instead, we find training with mixed zero- and few-shot prompts significantly improves performance in both settings - most surprisingly, even for models with only 3B parameters.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Training jointly with zero-shot and few-shot prompt templates improves performance on both Held-In and Held-Out tasks. The stars indicate the peak performance in each setting.</p>
<p>Figure 3 shows (1) adding as little as 5\% few-shot training templates can dramatically improve zero-shot performance, and (2) adding $10 \%+$ of zero-shot data improves few-shot performance too. Both Held-In and Held-Out tasks peak anywhere between 10-90\% of few-shot data, but this range is consistently higher than training with only one prompt setting.</p>
<h3>3.3 Scaling Small Models to 1.8k+ Tasks</h3>
<p>The most recent and concurrent publicly available instruction tuning efforts, like Flan 2022, train on thousands of tasks (Wang et al., 2022c; Iyer et al., 2022), but operate on different task compositions and underlying training methods. To measure the impact of scaling model sizes and tasks for the Flan 2022 collection, we finetune T5-LM adapted models (Small, Base, Large, XL, XXL) on randomly selected task subsets (8, 25, $50,100,200,400,800$, all 1873). Every finetuning run is guaranteed to include the Held-In tasks, so we can estimate how task scaling impacts the model capacity to maintain performance on a given task its already seen.</p>
<p>Figure 4 demonstrates that both Held-In and Held-Out tasks appear to benefit from adding hundreds of finetuning tasks. Held-in task evaluations peak around 200 total tasks, and diminish in performance as more tasks are added, though larger models peak later and diminish less. Held-out task performance increases log-linearly with the number of tasks, achieving the highest performances with all 1836 tasks.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance Scaling Laws for the number of finetuning tasks and model sizes. Held-In performance (left) and Held-Out MMLU performance (right) are shown. The gold star indicates the peak performance for that model size.</p>
<p>Surprisingly, only T5-Small appears to exceed its Held-Out task performance before 1836 tasks, while larger model sizes continue to improve. These results suggest (a) even T5-Base may not have exhausted its capacity with thousands of tasks, and (b) the largest LMs could benefit from thousands more tasks for Held-In and Held-Out task performance.</p>
<p>One necessary assumption of this analysis is that all tasks are defined and counted equally. Section 3.5 demonstrates how not all task sources are equally beneficial to training, and the model performance may saturate from too many tasks from one source (e.g. Super-Natural Instructions). We would caution conclusions that task scaling beyond 1800 would translate to increased returns without also paying attention to task diversity and quality.</p>
<h3>3.4 Task Enrichment with Input Inversion</h3>
<p>Prior instruction tuning work has enriched their diversity of tasks by inverting the $(x, y)$ input-output pairs in supervised tasks—referred to as "prompts not intended for the original task" in P3 (Bach et al., 2022) or the "noisy channel" in MetaICL (Min et al., 2022). For example, a dataset may be originally designed for, given a question $x$, evaluate if a model can answer $y$. Input inversion instead gives a model the answer $y$ and trains it to generate the question $x$. This is an easy method to enrich the task variety given a limited set of data sources. However, it isn't clear that this method remains helpful when 100s of unique data sources and 1000s of tasks are already available.</p>
<p>To assess this, we enrich our mixtures with input inverted tasks (details and examples in Appendix B) and measure the effect. In Table 1 we find this is not beneficial for Held-In performance, but strongly beneficial for Held-Out performance. These benefits invigorate the prospect of data augmentation techniques for LLM finetuning, which had previously been shown to have diminishing returns the longer models are pretrained (Longpre et al., 2020).</p>
<h3>3.5 Balancing Data Sources</h3>
<p>Scaling architecture size and the number of tasks are effective, but our results suggest the mixture weighting deserves as much attention to optimize results. To converge on a balanced weighting, we omit different sets of task sources, one at a time (Flan 2021, T0-SF, Super-Natural Instructions, Chain-of-Thought, Dialog, and</p>
<table>
<thead>
<tr>
<th>Train Mixtures</th>
<th>Metrics</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Held-In</td>
<td>CoT</td>
<td>MMLU</td>
</tr>
<tr>
<td>All (Equal)</td>
<td>64.9</td>
<td>41.4</td>
<td>47.3</td>
</tr>
<tr>
<td>All - Flan 2021</td>
<td>55.3</td>
<td>38.6</td>
<td>45.7</td>
</tr>
<tr>
<td>All - T0-SF</td>
<td>63.2</td>
<td>43.4</td>
<td>44.7</td>
</tr>
<tr>
<td>All - Super-Nat. Inst.</td>
<td>65.9</td>
<td>42.2</td>
<td>46.8</td>
</tr>
<tr>
<td>All - CoT</td>
<td>65.6</td>
<td>29.1</td>
<td>46.8</td>
</tr>
<tr>
<td>All - Prog. Synth.</td>
<td>66.9</td>
<td>42.3</td>
<td>46.8</td>
</tr>
<tr>
<td>All - Dialog</td>
<td>65.4</td>
<td>40.3</td>
<td>47.1</td>
</tr>
<tr>
<td>All (Weighted)</td>
<td>$\mathbf{6 6 . 4}$</td>
<td>40.1</td>
<td>$\mathbf{4 8 . 1}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Subsets of tasks are left out from an equally weighted mixture to measure their importance. T0-SF and Flan 2021 finetuning are most important for MMLU, while Chain-of-Thought (CoT) finetuning is most important for Chain-of-Thought evaluation.</p>
<p>Program Synthesis), and rank their contributions on the MMLU benchmark.</p>
<p>As shown in Table 2, Flan 2021 and T0-SF are among the most beneficial mixtures, followed by Super-Natural Instructions and Chain-of-Thought, with Dialog and Program Synthesis last. These findings are corroborated by Iyer et al. (2022) who extensively test data mixing proportions, and also determine their Flan 2021, T0-SF, and T5 mixtures are the most broadly beneficial. Additionally, they find Super-Natural Instructions has limited scaling benefits on Held-Out task performance, which they relate to its unique input format and instruction design. Notably, Chain-of-thought finetuning appears beneficial across all our evaluation settings, especially considering they contain far fewer tasks than Flan 2021, T0-SF or Natural Instructions.</p>
<p>We used these findings to significantly narrow the mixture weights search space, and used our practitioner's intuition from there. This strategy is simple but effective, as shown in Table 1, but leaves ample room for more sophisticated future work.</p>
<h3>3.6 Discussion</h3>
<p>OPT-IML (Iyer et al., 2022) presents the closest comparison to this work, including a similar collection of tasks, examples and techniques. However, while their used tasks are all publicly sourced, their collection, with templates, processing, and example mixing, is not released, and as a result cannot be easily compared. Iyer et al. (2022) report that Flan-T5-XL (3B) and XXL (11B) outperforms OPT-IML-Max 175B on both MMLU and BBH. As they discuss, these differences may arise from any combination of pre-training, model architecture, and instruction tuning. Model architecture and pretraining before instruction tuning can play a significant role (Wang et al., 2022a). But there are many other details in instruction tuning that may vary between Flan 2022 and OPT-IML. Likely candidates are are: example templatization, how the mixed input prompting procedures are used at training, and task composition.</p>
<p>How significant are each of these difference? While OPT-IML contains more tasks than Flan 2022, we estimate approximately $94\%(2067 / 2207)$ are also used in the Flan 2022 collection, and very few tasks in Flan 2022 are not contained in some format in OPT-IML. This suggests the overall difference in task diversity is not significant when using a shared definition of "task". Task mixture rates also emphasize similar sources, including Flan 2021 ( $46 \%$ vs $20 \%$ ), PromptSource/P3 ( $28 \%$ vs $45 \%$ ), and Super-Natural Instructions ( $25 \%$ vs $25 \%$ ), for Flan 2022 and OPT-IML respectively. ${ }^{6}$ OPT-IML's other collections (Crossfit, ExMix, T5, U-SKG)</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Flan-T5 Outperforms T5 on Single-Task Finetuning. We compare single-task finetuned T5, singletask finetuned Flan-T5, and Flan-T5 without any further finetuning.
are not weighted significantly: $4 \%, 2 \%, 2 \%, 2 \%$ respectively.
We believe example templatization and the mixed prompt formats may pose the largest differences with OPTIMLs instruction tuning. Our template repository was significantly updated from Flan 2021, adding variety not just in instructions, but also along dimensions. For instance, the templatization procedure varies where the instruction is placed (before or after few-shot prompts), the spacing and separators between few-shot and Chain-of-Thought prompts, and the formatting permutations of answer options (and their targets) for multiple-choice examples, which sometimes includes and sometimes excludes answer options in the inputs or exemplars. While we do not have dedicated experiments comparing many iterations of development, we found these procedures dramatically augment input variety and showed repeated performance improvements. Our example templatizing procedure is open sourced for inspection and future work.</p>
<h1>4 Instruction Tuning Enhances Single-Task Finetuning</h1>
<p>In applied settings, machine learning practitioners deploy NLP models finetuned (FT) specifically for a single target task, usually where finetuning data is already available. While prior work has shown the benefits of intermediate finetuning (Pruksachatkun et al., 2020; Vu et al., 2020) or multi-task finetuning (Aghajanyan et al., 2021; Aribandi et al., 2021) for downstream tasks, this has not been studied extensively for instructiontuned models.</p>
<p>We evaluate Flan 2022 instruction tuning as an intermediary step before single target finetuning, to understand if Flan-T5 would serve as a better starting checkpoint for applied practitioners. We evaluate three settings in</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Flan-T5 convergences faster than T5 on single-task finetuning for each of 5 Held-Out tasks from Flan finetuning.</p>
<p>Figure 5: finetuning T5 directly on the target task as the conventional baseline (blue bars), using Flan-T5 without further finetuning (beige bars), and finetuning Flan-T5 further on the target task (red bars).</p>
<p>Pareto Improvements to Single Task Finetuning For both sets of Held-In and Held-Out tasks examined, finetuning Flan-T5 offers a pareto improvement over finetuning T5 directly. In some instances, usually where finetuning data is limited for a task, Flan-T5 without further finetuning outperforms T5 with task finetuning.</p>
<p>Faster Convergence \&amp; Computational Benefits Using Flan-T5 as a starting checkpoint has an added benefit in training efficiency. As demonstrated in Figure 6, Flan-T5 converges much more quickly than T5 during single target finetuning, as well as peaking at higher accuracies. These convergence results also suggest there are strong green-AI incentives for the NLP community to adopt instruction-tuned models, like FlanT5 for single-task finetuning, rather than conventional non-instruction-tuned models. While instruction tuning is more computationally-expensive than single-task finetuning, it is a one-time cost. On the contrary, pretrained models that require extensive finetuning become more costly when aggregating over many millions of additional training steps (Wu et al., 2022; Bommasani et al., 2021). Instruction-tuned models offer a promising solution to significantly reduce the amount of finetuning steps across a wide swathe of tasks, if they are adopted as a new standard starting point for single-task finetuning.</p>
<h1>5 Related Work</h1>
<p>Large Language Models As the foundation of instruction tuning, the practice of pretraining one generalpurpose language representation that is useful for multiple downstream tasks has a long tradition that goes back at least Mikolov et al. (2013) and Dai and Le (2015). In 2018, Peters et al. (2018) and Devlin et al. (2019) cemented the paradigm of pretraining a large model on a large unsupervised corpus, and the field of NLP quickly converged to using these models which substantially outperform the prior art of non-pretrained task-specific LSTM models on all tasks. However, the dominate way to access that high-quality syntactic and semantic knowledge encoded in pretrained models was not to prompt them with instructions, but to train an additional task-specific linear layer that maps the model activations into numerical class labels. A short year later, Radford et al. (2019), Raffel et al. (2020), and Lewis et al. (2020) popularized the notion that downstream tasks-and multiple tasks-can be jointly learned by directly using the pretrained LM head to generate the answers in natural language (cf. task-specific numerical class labels), the task-general nature of these generative models became the precursor to many multitask transfer learning studies (McCann et al., 2018; Khashabi et al., 2020; Ye et al., 2021; Vu et al., 2020), which in turn led to the first wave of instruction tuning as described in Section 2.</p>
<p>The continuing advancement in research on the pretraining corpora, architectures and pretraining objectives of LMs also has a large impact on instruction tuning. As of 2022, decoder-only left-to-right causal Transformers dominate the market of models larger than 100B (Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021;</p>
<p>Chowdhery et al., 2022; Hoffmann et al., 2022), and all models of such size class with fully public model parameters are decoder-only (Wang and Komatsuzaki, 2021; Le Scao et al., 2022; Zhang et al., 2022), the decision of which are often due to better hardware and software framework support. However, Raffel et al. (2020), Lewis et al. (2020), and Tay et al. (2022a) have consistently found that left-to-right causal language modeling is a suboptimal objective, while Tay et al. (2022b) and Wang et al. (2022a) particularly showed that a mixture of non-sequential objectives is much superior for downstream tasks with zero-shot and few-shot prompting. An additional factor which remains under-explored is the relationship between pretraining corpora, instruction tuning, and downstream abilities. Typically, public models are all trained on one of a few public corpora: C4 (Raffel et al., 2020), The Pile (Gao et al., 2020), or ROOTs (Laurençon et al., 2022).</p>
<p>Instruction Tuning In Section 2 we outline major developments in instruction tuning. Other important developments include the prospect of complimenting or replacing few-shot in-context learning-the currently predominate method of evaluating pretrained and instruction-tuned models-with parameter-efficient tuning. As standard finetuning of models larger than 100B requires a high number of accelerators with the right interconnects often too expensive even for many industry labs, parameter-efficient tuning (a.k.a. continuous or soft "prompt tuning") shows that only updating a small subset of model parameters can reach comparable performance as fully tuning all model parameters (Lester et al., 2021; Vu et al., 2022; Hu et al., 2021; see He et al., 2022 for a detailed analysis). Notably, Liu et al. (2022b) show that, due to the long sequence length of few-shot ICL and that the few-shot exemplars need to be repeatedly inferenced for evaluating every example, parameter-efficient tuning can be computationally cheaper and higher performing than in-context learning. Further, Liu et al. (2022b), Vu et al. (2022), Wei et al. (2021), and Singhal et al. (2022) collectively show that both single-task and multi-task parameter-efficient tuning can be productively combined with instruction tuning, either before or after regular full-model instruction tuning. This line of work makes it easy for other researchers to build on top of a general-domain instruction-tuned model, and collect a custom instruction-tuning mixture for their use, e.g., with multiple modalities (Ahn et al., 2022; Huang et al., 2022; Xu et al., 2022) or special domains such as science and medicine (Lewkowycz et al., 2022; Singhal et al., 2022).</p>
<p>Problems Addressed by Instruction Tuning \&amp; Alignment Techniques Instruction tuning is part of a line of work designed to "align" language models with more useful objectives and human preferences. In the absence of such methods, language models are known to demonstrate toxic/harmful behaviour (Sheng et al., 2019; Liang et al., 2021; Wallace et al., 2019), generate non-factual information (Maynez et al., 2020; Longpre et al., 2021; Devaraj et al., 2022), and other challenges in deployment and evaluation (Zellers et al., 2019; McGuffie and Newhouse, 2020; Talat et al., 2022). Analyzing, evaluating and mitigating these problems pose a promising direction for future work (Gao et al., 2022; Ganguli et al., 2022). Instruction tuning warrants greater investigation, as it has already demonstrated itself an encouraging remedy in reducing NLP bias metrics, as shown in Chung et al. (2022).</p>
<h1>6 Conclusions</h1>
<p>The new Flan 2022 instruction tuning collection unifies the most popular prior public collections and their methods, while adding new templates and simple improvements like training with mixed prompt settings. The resulting collection outperforms Flan 2021, P3++, Super-Natural Instructions, and OPT-IML-Max 175B on Held-In QA, NLI, and Chain-of-Thought tasks, and Held-Out MMLU and BBH, often by large margins. Results suggest this new collection serves as a more competitive starting point for researchers and practitioners interested in both generalizing to new instructions, or finetuning on a single new task.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank Ed H Chi, Xinyun Chen, and Colin Raffel for their advice and feedback on the paper.</p>
<h1>References</h1>
<p>Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. Muppet: Massive multi-task representations with pre-finetuning. In EMNLP, 2021. URL https:// aclanthology.org/2021.emnlp-main. 468.</p>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. arXiv e-prints, art. arXiv:2204.01691, April 2022.</p>
<p>Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. Ext5: Towards extreme multi-task scaling for transfer learning. arXiv preprint arXiv:2111.10952, 2021.</p>
<p>Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-jian Jiang, and Alexander Rush. PromptSource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 93-104, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-demo.9. URL https://aclanthology.org/2022.acl-demo.9.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b.</p>
<p>Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth pascal recognizing textual entailment challenge. In TAC, 2009.</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. NeurIPS, 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, et al. PaLM: Scaling language modeling with Pathways. arXiv preprint arXiv:2204.02311, 2022. URL https://arxiv.org/abs/2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019 .</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.</p>
<p>Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/ 7137debd45ae4d0ab9aa953017286b20-Paper.pdf.</p>
<p>Ashwin Devaraj, William Sheffield, Byron Wallace, and Junyi Jessy Li. Evaluating factuality in text simplification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7331-7345, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.506. URL https://aclanthology.org/2022.acl-long. 506.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. NAACL, 2019. URL https://aclanthology.org/N19-1423.</p>
<p>Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.</p>
<p>Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Attributed text generation via post-hoc research and revision. arXiv preprint arXiv:2210.08726, 2022.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346-361, 2021.</p>
<p>Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.</p>
<p>Prakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Eskenazi, and Jeffrey P Bigham. Improving zero and few-shot generalization in dialogue through instruction tuning. arXiv preprint arXiv:2205.12673, 2022.</p>
<p>Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=ORDcd5Axok.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. ICLR, 2020. URL https://openreview.net/ forum?id=d7KBjmI3GmQ.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.</p>
<p>Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022.</p>
<p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. CoRR, abs/2106.09685, 2021. URL https: //arxiv.org/abs/2106.09685.</p>
<p>Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos qa: Machine reading comprehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2391-2401, 2019.</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. In arXiv preprint arXiv:2207.05608, 2022.</p>
<p>Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O'Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, and Ves Stoyanov. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022. URL https: //arxiv.org/abs/2212.12017.</p>
<p>Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. PubMedQA: A dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2567-2577, 2019. URL https://aclanthology.org/D19-1259.</p>
<p>Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. Unifying question answering, text classification, and regression via span extraction. arXiv preprint arXiv:1904.09286, 2019.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. UnifiedQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, 2020. URL https://aclanthology.org/2020.findings-emnlp. 171 .</p>
<p>Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al. The bigscience roots corpus: A 1.6 tb composite multilingual dataset. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. EMNLP, 2021. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021. emnlp-main. 243 .</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for</p>
<p>Computational Linguistics, pages 7871-7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/2206.14858.</p>
<p>Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. Towards understanding and mitigating social biases in language models. In ICML, 2021.</p>
<p>Alisa Liu, Swabha Swayamdipta, Noah A Smith, and Yejin Choi. Wanli: Worker and ai collaboration for natural language inference dataset creation. arXiv preprint arXiv:2201.05955, 2022a. URL https: //arxiv.org/abs/2201.05955.</p>
<p>Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning, 2022b. URL https://arxiv.org/abs/2205.05638.</p>
<p>Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4487-4496, 2019.</p>
<p>Shayne Longpre, Yu Wang, and Chris DuBois. How effective is task-agnostic data augmentation for pretrained transformers? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4401-4411, 2020.</p>
<p>Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entitybased knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7052-7063, 2021.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/ v1/2020.acl-main.173. URL https://aclanthology.org/2020.acl-main.173.</p>
<p>Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.</p>
<p>Kris McGuffie and Alex Newhouse. The radicalization risks of gpt-3 and advanced neural language models. arXiv preprint arXiv:2009.06807, 2020.</p>
<p>Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975-984, 2020.</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/ 9aa42b31882ec039965f3c4923ce901b-Paper.pdf.</p>
<p>Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in context. In NAACL, 2022. URL https://aclanthology.org/2022.naacl-main. 201.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773, 2021.</p>
<p>Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.</p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885-4901, 2020.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. URL https://arxiv.org/abs/2203.02155.</p>
<p>Zarana Parekh, Jason Baldridge, Daniel Cer, Austin Waters, and Yinfei Yang. Crisscrossed captions: Extended intramodal and intermodal semantic similarity judgments for MS-COCO. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 2855-2870, 2021. URL https://aclanthology.org/2021.eacl-main. 249.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094, 2021.</p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. NAACL, 2018. URL https://aclanthology. org/N18-1202.</p>
<p>Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel Bowman. Intermediate-task transfer learning with pretrained language models: When and why does it work? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5231-5247, 2020.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. URL https://d4mucfpksywv. cloudfront. net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1-67, 2020. URL https://arxiv.org/abs/1910.10683.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784-789, 2018.</p>
<p>Abhilasha Ravichander, Matt Gardner, and Ana Marasović. Condaqa: A contrastive reading comprehension dataset for reasoning about negation. arXiv preprint arXiv:2211.00295, 2022. URL https://arxiv.org/ abs/2211.00295.</p>
<p>Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022. URL https://arxiv.org/ abs/2203.17189.</p>
<p>Alexey Romanov and Chaitanya Shivade. Lessons from natural language inference in the clinical domain. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1586-1596, 2018. URL https://aclanthology.org/D18-1187.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. ICLR 2022, 2021. URL https://arxiv.org/abs/2110.08207.</p>
<p>Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as a babysitter: On biases in language generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3407-3412, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1339. URL https://aclanthology.org/D19-1339.</p>
<p>Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge, 2022. URL https://arxiv.org/abs/2212.13138.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. URL https://arxiv.org/abs/2206.04615.</p>
<p>Mirac Suzgun, Nathan Scales, Nathaneal Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny ZHou, and Jason Wei. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. URL https://arxiv.org/abs/ 2210.09261.</p>
<p>Zeerak Talat, Aurélie Névéol, Stella Biderman, Miruna Clinciu, Manan Dey, Shayne Longpre, Alexandra Sasha Luccioni10, Maraim Masoud11, Margaret Mitchell10, Dragomir Radev12, et al. You reap what you sow: On the challenges of bias evaluation under multilingual settings. Challenges $\mathcal{E}$ Perspectives in Creating Large Language Models, page 26, 2022.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, 2019.</p>
<p>Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131, 2022a. URL https://arxiv.org/abs/2205.05131.</p>
<p>Yi Tay, Jason Wei, Hyung Won Chung, David R. So, Siamak Shakeri, Xavier Garcia, Vinh Q. Tran, Hauixiu Steven Zheng, Jinfeng Rao, Denny Zhou, Donald Metzler, Neil Houlsby, Quoc V. Le, and Mostafa Dehghani. Transcending scaling laws with $0.1 \%$ extra compute. In arxiv, 2022b.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022. URL https://arxiv.org/abs/2201.08239.</p>
<p>Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew Mattarella-Micke, Subhransu Maji, and Mohit Iyyer. Exploring and predicting transferability across NLP tasks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7882-7926, 2020. URL https://aclanthology.org/2020.emnlp-main. 635.</p>
<p>Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou', and Daniel Cer. SPoT: Better frozen model adaptation through soft prompt transfer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pages 5039-5059, 2022. URL https://aclanthology.org/2022.acl-long. 346.</p>
<p>Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers for attacking and analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2153-2162, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1221. URL https://aclanthology.org/D19-1221.</p>
<p>Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https: //github.com/kingoflolz/mesh-transformer-jax, May 2021.</p>
<p>Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffel. What language model architecture and pretraining objective work best for zero-shot generalization? ICML, 2022a. URL https://arxiv.org/abs/2204.05832.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions, 2022b. URL https: //arxiv.org/abs/2212.10560.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705, 2022c. URL https://arxiv.org/abs/2204.07705.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. ICLR 2022, 2021. URL https: //openreview.net/forum?id=gEZrGCozdqR.</p>
<p>Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable ai: Environmental implications, challenges and opportunities. Proceedings of Machine Learning and Systems, 4:795-813, 2022.</p>
<p>Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning, 2022. URL https://arxiv.org/abs/2212.10773.</p>
<p>Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. Crossfit: A few-shot learning challenge for cross-task generalization in NLP. In EMNLP, 2021. URL https://arxiv.org/abs/2104.08835.</p>
<p>Seonghyeon Ye, Doyoung Kim, Joel Jang, Joongbo Shin, and Minjoon Seo. Guess the instruction! making language models stronger zero-shot learners. arXiv preprint arXiv:2210.02969, 2022.</p>
<p>Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. Advances in neural information processing systems, 32, 2019.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.</p>
<h1>Appendix</h1>
<h2>Table of Contents</h2>
<p>A Experimental Details ..... 20
A. 1 Instruction Tuning ..... 20
A. 2 Single-Task Finetuning ..... 20
A. 3 Evaluation ..... 20
B Input Inversion Details ..... 21</p>
<h2>A Experimental Details</h2>
<h2>A. 1 Instruction Tuning</h2>
<p>The Flan Collection experiments are assembled and run using T5X (Roberts et al., 2022). Our instruction tuning follows the same setup described in Chung et al. (2022). For few-shot and few-shot Chain-of-Thought prompts during finetuning our templatizing procedure generates few-shot examples with 2, 3, or 5 exemplars. The experiments in this work use a slightly earlier version of the Flan 2022 collection the one we are releasing, which had some minor improvements to the templates.</p>
<p>The mixture weights used to balance the various sources of data were informed by experiments in Section 3.5, along with the resulting practitioner intuition.</p>
<h2>A. 2 Single-Task Finetuning</h2>
<p>For single-task finetuning, described in Section 4, our models are finetuned for 100,000 steps for all tasks. We use a constant learning rate of 0.001 , a dropout probability of 0.1 , and a batch size of 128 length-512 sequences. We save a checkpoint every 20 steps and report test performance on the model checkpoint corresponding to the highest validation performance. For tasks without a validation split, we hold out 1024 training examples for validation. For tasks without a test split, we hold out 1024 training examples for validation and report results on the original validation set. For PubmedQA, we do not use any of the unlabeled and artificially generated QA instances associated with the dataset. For CxC, we only consider the text-text portion of the dataset, following Vu et al. (2022). For tasks with less than 1 K training examples, we report average results across 3 random seeds.</p>
<p>We also evaluate on certain metrics to account for label skew in some of the datasets, as shown in Table 3.</p>
<h2>A. 3 Evaluation</h2>
<p>For Held-In evaluations we use the validation sets from 4 question answering (QA) tasks, BoolQ, ARC Easy, ARC Challenge, and AI2's Middle School Science Exams, and 4 natural language inference (NLI) tasks, including ANLI R1, R2, R3, and RTE. These datasets are contained in the Flan 2022 finetuning collection and represent challenging benchmarks, often used to evaluate LLMs on QA and NLI. The Held-In score is the mean accuracy across these 8 tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Used in</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Held-In</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">ST-FT Held-In</td>
<td style="text-align: center;">ST-FT Held-Out</td>
</tr>
<tr>
<td style="text-align: center;">ARC E+C</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">(Clark et al., 2018)</td>
</tr>
<tr>
<td style="text-align: center;">ANLI R1+R2+R3</td>
<td style="text-align: center;">3-class F1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">(Nie et al., 2020)</td>
</tr>
<tr>
<td style="text-align: center;">AI2 Mid. Science</td>
<td style="text-align: center;">4-class F1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">AI2 Science Questions</td>
</tr>
<tr>
<td style="text-align: center;">BoolQ</td>
<td style="text-align: center;">AUC-ROC</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">(Clark et al., 2019)</td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">AUC-ROC</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">(Bentivogli et al., 2009)</td>
</tr>
<tr>
<td style="text-align: center;">SQuAD V2</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">(Rajpurkar et al., 2018)</td>
</tr>
<tr>
<td style="text-align: center;">CosmosQA</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">(Huang et al., 2019)</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Cobbe et al., 2021)</td>
</tr>
<tr>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Geva et al., 2021)</td>
</tr>
<tr>
<td style="text-align: center;">SVAMP</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Patel et al., 2021)</td>
</tr>
<tr>
<td style="text-align: center;">Asdiv</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Miao et al., 2020)</td>
</tr>
<tr>
<td style="text-align: center;">CommonsenseQA</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Talmor et al., 2019)</td>
</tr>
<tr>
<td style="text-align: center;">WANLI</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Liu et al., 2022a)</td>
</tr>
<tr>
<td style="text-align: center;">MedNLI</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Romanov and Shivade, 2018)</td>
</tr>
<tr>
<td style="text-align: center;">CondaQA</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Ravichander et al., 2022)</td>
</tr>
<tr>
<td style="text-align: center;">PubmedQA</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Jin et al., 2019)</td>
</tr>
<tr>
<td style="text-align: center;">CxC</td>
<td style="text-align: center;">Spearman</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Parekh et al., 2021)</td>
</tr>
</tbody>
</table>
<p>Table 3: Datasets used for Various Finetuning and Evaluation Experiments. ST-FT stands for Single Task Finetuning.</p>
<p>For the Chain-of-Thought (CoT) evaluation, we use the mean accuracy across 5 datasets which have been prepared with prompts which request step-by-step explanations in their target answers: GSM8K, StrategyQA, SVAMP, Asdiv, and CommonsenseQA.</p>
<p>For the Held-Out tasks, we use MMLU's suite of 57 exams, and BBH's suite of 23 tasks where PaLM performed worse than the average human annotators. MMLU tasks were removed from the Super-Natural Instructions part of the Flan 2022 collection at training, to ensure they were Held-Out.</p>
<h1>B Input Inversion Details</h1>
<p>For the input inversion experiments we note that Flan 2021, P3++, and Super-Natural Instructions already implicitly include tasks that have been inverted, e.g. question answering to question or context generation. Consequently, we choose to also create input inversions for the remaining datasets in the Flan 2022 collection, including for the Dialog, Program Synthesis, and Chain-of-Thought tasks.</p>
<p>As examples: for Dialog tasks, we write template instructions asking for the previous conversational history from the current dialog turn; for program synthesis we ask for the coding question which the code solves; and for Chain-of-Thought we include every permutation of the query-answer-explanation triple, where at least one of the three appears as the in output. An illustration of Chain-of-Thought input inversion permutations are shown in Figure 7.</p>
<p>These inversions are mixed in with the existing tasks at a rate of $30 \%$, meaning for a Dialog task, 3 inverted examples will be generated for every 10 regular examples. We choose this rate for simplicity, approximately mirroring prior work, and leave the large space of exploration for future work.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Following Chung et al. (2022) we refer to the subset of P3++ that is not in Flan 2021 as T0-SF (SF stands for "sans Flan").
${ }^{5}$ This is calculated using their definition of "task" (reported in Iyer et al. (2022)'s Table 1), which does not deduplicate across collections.
${ }^{6}$ Note that $46 \%$ weight for Flan 2022 is actually on Muffin from Chung et al. (2022) which combines Flan 2021 with new dialog and program synthesis tasks.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
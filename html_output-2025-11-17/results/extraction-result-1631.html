<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1631 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1631</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1631</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-52932123</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1810.02422v2.pdf" target="_blank">Zero-Shot Skill Composition and Simulation-to-Real Transfer by Learning Task Representations</a></p>
                <p><strong>Paper Abstract:</strong> Simulation-to-real transfer is an important strategy for making reinforcement learning practical with real robots. Successful sim-to-real transfer systems have difficulty producing policies which generalize across tasks, despite training for thousands of hours equivalent real robot time. To address this shortcoming, we present a novel approach to efficiently learning new robotic skills directly on a real robot, based on model-predictive control (MPC) and an algorithm for learning task representations. In short, we show how to reuse the simulation from the pre-training step of sim-to-real methods as a tool for foresight, allowing the sim-to-real policy adapt to unseen tasks. Rather than end-to-end learning policies for single tasks and attempting to transfer them, we first use simulation to simultaneously learn (1) a continuous parameterization (i.e. a skill embedding or latent) of task-appropriate primitive skills, and (2) a single policy for these skills which is conditioned on this representation. We then directly transfer our multi-skill policy to a real robot, and actuate the robot by choosing sequences of skill latents which actuate the policy, with each latent corresponding to a pre-learned primitive skill controller. We complete unseen tasks by choosing new sequences of skill latents to control the robot using MPC, where our MPC model is composed of the pre-trained skill policy executed in the simulation environment, run in parallel with the real robot. We discuss the background and principles of our method, detail its practical implementation, and evaluate its performance by using our method to train a real Sawyer Robot to achieve motion tasks such as drawing and block pushing.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1631.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1631.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Skill-Embedding + MPC Sim-to-Real</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-Shot Skill Composition and Simulation-to-Real Transfer by Learning Task Representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pre-train a latent-conditioned multi-skill policy in simulation (MuJoCo) using RL and variational inference, freeze the policy, and perform zero-shot adaptation on a real Sawyer robot by using the simulator online as a foresight model inside a model-predictive-control loop that selects sequences of latent skill vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Sawyer robot</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A 7-DOF Sawyer robotic arm used for manipulation tasks (end-effector control via incremental joint or Cartesian commands) evaluated on drawing (3D waypoint sequencing) and box-pushing sequencing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A rigid-body physics engine (MuJoCo) used to simulate the Sawyer robot, object dynamics, contacts, and kinematics; repurposed both for pre-training a latent-conditioned policy and as an online predictive model for MPC.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Approximate physics (MuJoCo) with rigid-body and contact modeling but acknowledged limitations in accurately reproducing real-world contact/friction details.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body dynamics, joint kinematics, contact events between gripper and objects, object motion/pose, and basic collision response.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Precise sliding friction and contact force modeling (recognized as an open problem), sensor/perception noise, actuator delays/dynamics, and other real-world unmodeled perturbations (no explicit photorealistic rendering or sensor-noise modeling reported).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical Sawyer robot executing drawing (moving gripper through 3D waypoint sequences to draw shapes) and box-pushing tasks on a tabletop, with real contact interactions and timed task completion measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Composing pre-trained low-level skills (latent-conditioned primitives) to (1) draw unseen 3D shapes (rectangle, triangle) and (2) push a box through unseen waypoint sequences by sequencing latent skills.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (PPO) with variational inference to learn a latent skill embedding/distribution and a single latent-conditioned multi-skill policy in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task completion (successful execution of unseen tasks), number of latent segments sequenced to completion, and wall-clock time to complete the task on the real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Drawing: rectangle completed by sequencing 54 latents, triangle by 56 latents; Pushing: unseen up-left task completed in <30s (equivalent real time), right-up-left in <40s (equivalent real time).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Drawing: rectangle completed by sequencing 62 latents in ~2 minutes, triangle completed with 53 latents in <2 minutes; Pushing: left-down completed with 3 latents in ~1 minute, up-left completed with 8 latents in ~1.5 minutes.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Reality gap due to differences in dynamics and perception between sim and real, especially inaccurate/sloppy modeling of sliding friction and contact forces, sensor/perception mismatch, and unmodeled actuator/timing differences.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Learning a disentangled latent skill embedding with high entropy and identifiability; training a single latent-conditioned multi-skill policy in sim; using the same simulation online as an MPC predictive model by resetting its state to the observed real state each planning step (simulation-as-foresight); short MPC horizons with repeated re-planning; composing latent skills sequentially rather than transferring end-to-end single-task policies.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No quantitative fidelity thresholds were specified; authors emphasize that perfect contact/friction modeling is infeasible for many tasks and instead propose mitigation via online MPC using the (imperfect) simulator as foresight together with a latent skill representation to bridge the reality gap.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining learned latent skill embeddings with model-predictive control that uses the simulator as an online foresight model enables zero-shot transfer of composed behaviors to a real Sawyer robot for drawing and box-pushing tasks without real-world fine-tuning; this approach mitigates aspects of the reality gap (notably for composition and reactive correction) even when simulation contact/friction fidelity is imperfect, and avoids the computational expense of aggressive domain randomization by adapting online.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zero-Shot Skill Composition and Simulation-to-Real Transfer by Learning Task Representations', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Learning an embedding space for transferable robot skills <em>(Rating: 2)</em></li>
                <li>Scaling simulation-to-real transfer by learning composable robot skills <em>(Rating: 2)</em></li>
                <li>CAD2RL: Real single-image flight without a single real image <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1631",
    "paper_id": "paper-52932123",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Skill-Embedding + MPC Sim-to-Real",
            "name_full": "Zero-Shot Skill Composition and Simulation-to-Real Transfer by Learning Task Representations",
            "brief_description": "Pre-train a latent-conditioned multi-skill policy in simulation (MuJoCo) using RL and variational inference, freeze the policy, and perform zero-shot adaptation on a real Sawyer robot by using the simulator online as a foresight model inside a model-predictive-control loop that selects sequences of latent skill vectors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Sawyer robot",
            "agent_system_description": "A 7-DOF Sawyer robotic arm used for manipulation tasks (end-effector control via incremental joint or Cartesian commands) evaluated on drawing (3D waypoint sequencing) and box-pushing sequencing tasks.",
            "domain": "general robotics manipulation",
            "virtual_environment_name": "MuJoCo",
            "virtual_environment_description": "A rigid-body physics engine (MuJoCo) used to simulate the Sawyer robot, object dynamics, contacts, and kinematics; repurposed both for pre-training a latent-conditioned policy and as an online predictive model for MPC.",
            "simulation_fidelity_level": "Approximate physics (MuJoCo) with rigid-body and contact modeling but acknowledged limitations in accurately reproducing real-world contact/friction details.",
            "fidelity_aspects_modeled": "Rigid-body dynamics, joint kinematics, contact events between gripper and objects, object motion/pose, and basic collision response.",
            "fidelity_aspects_simplified": "Precise sliding friction and contact force modeling (recognized as an open problem), sensor/perception noise, actuator delays/dynamics, and other real-world unmodeled perturbations (no explicit photorealistic rendering or sensor-noise modeling reported).",
            "real_environment_description": "Physical Sawyer robot executing drawing (moving gripper through 3D waypoint sequences to draw shapes) and box-pushing tasks on a tabletop, with real contact interactions and timed task completion measurements.",
            "task_or_skill_transferred": "Composing pre-trained low-level skills (latent-conditioned primitives) to (1) draw unseen 3D shapes (rectangle, triangle) and (2) push a box through unseen waypoint sequences by sequencing latent skills.",
            "training_method": "Reinforcement learning (PPO) with variational inference to learn a latent skill embedding/distribution and a single latent-conditioned multi-skill policy in simulation.",
            "transfer_success_metric": "Task completion (successful execution of unseen tasks), number of latent segments sequenced to completion, and wall-clock time to complete the task on the real robot.",
            "transfer_performance_sim": "Drawing: rectangle completed by sequencing 54 latents, triangle by 56 latents; Pushing: unseen up-left task completed in &lt;30s (equivalent real time), right-up-left in &lt;40s (equivalent real time).",
            "transfer_performance_real": "Drawing: rectangle completed by sequencing 62 latents in ~2 minutes, triangle completed with 53 latents in &lt;2 minutes; Pushing: left-down completed with 3 latents in ~1 minute, up-left completed with 8 latents in ~1.5 minutes.",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Reality gap due to differences in dynamics and perception between sim and real, especially inaccurate/sloppy modeling of sliding friction and contact forces, sensor/perception mismatch, and unmodeled actuator/timing differences.",
            "transfer_enabling_conditions": "Learning a disentangled latent skill embedding with high entropy and identifiability; training a single latent-conditioned multi-skill policy in sim; using the same simulation online as an MPC predictive model by resetting its state to the observed real state each planning step (simulation-as-foresight); short MPC horizons with repeated re-planning; composing latent skills sequentially rather than transferring end-to-end single-task policies.",
            "fidelity_requirements_identified": "No quantitative fidelity thresholds were specified; authors emphasize that perfect contact/friction modeling is infeasible for many tasks and instead propose mitigation via online MPC using the (imperfect) simulator as foresight together with a latent skill representation to bridge the reality gap.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Combining learned latent skill embeddings with model-predictive control that uses the simulator as an online foresight model enables zero-shot transfer of composed behaviors to a real Sawyer robot for drawing and box-pushing tasks without real-world fine-tuning; this approach mitigates aspects of the reality gap (notably for composition and reactive correction) even when simulation contact/friction fidelity is imperfect, and avoids the computational expense of aggressive domain randomization by adapting online.",
            "uuid": "e1631.0",
            "source_info": {
                "paper_title": "Zero-Shot Skill Composition and Simulation-to-Real Transfer by Learning Task Representations",
                "publication_date_yy_mm": "2018-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Learning an embedding space for transferable robot skills",
            "rating": 2,
            "sanitized_title": "learning_an_embedding_space_for_transferable_robot_skills"
        },
        {
            "paper_title": "Scaling simulation-to-real transfer by learning composable robot skills",
            "rating": 2,
            "sanitized_title": "scaling_simulationtoreal_transfer_by_learning_composable_robot_skills"
        },
        {
            "paper_title": "CAD2RL: Real single-image flight without a single real image",
            "rating": 2,
            "sanitized_title": "cad2rl_real_singleimage_flight_without_a_single_real_image"
        }
    ],
    "cost": 0.00916475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Zero-Shot Skill Composition and Simulation-to-Real Transfer by Learning Task Representations</p>
<p>Zhanpeng He 
Ryan Julian 
Eric Heiden 
Hejia Zhang 
Stefan Schaal 
Joseph J Lim 
Gaurav Sukhatme 
Karol Hausman 
Zero-Shot Skill Composition and Simulation-to-Real Transfer by Learning Task Representations</p>
<p>Sim-to-real transfer is an important strategy for making reinforcement learning practical with real robots. Successful sim-to-real transfer systems have difficulty producing policies which generalize across tasks, despite training for thousands of hours equivalent real robot time. To address this shortcoming, we present a novel approach to efficiently learning new robotic skills directly on a real robot, based on modelpredictive control (MPC) and an algorithm for learning task representations. In short, we show how to reuse the simulation from the pre-training step of sim-to-real methods as a tool for foresight, allowing the sim-to-real policy adapt to unseen tasks. Rather than end-to-end learning policies for single tasks and attempting to transfer them, we first use simulation to simultaneously learn (1) a continuous parameterization (i.e. a skill embedding or latent) of task-appropriate primitive skills, and (2) a single policy for these skills which is conditioned on this representation. We then directly transfer our multiskill policy to a real robot, and actuate the robot by choosing sequences of skill latents which actuate the policy, with each latent corresponding to a pre-learned primitive skill controller. We complete unseen tasks by choosing new sequences of skill latents to control the robot using MPC, where our MPC model is composed of the pre-trained skill policy executed in the simulation environment, run in parallel with the real robot. We discuss the background and principles of our method, detail its practical implementation, and evaluate its performance by using our method to train a real Sawyer Robot to achieve motion tasks such as drawing and block pushing.</p>
<p>I. INTRODUCTION</p>
<p>Reinforcement learning algorithms have been proven to be effective to learn complex skills in simulation environments in [1], [2] and [3]. However, practical robotic reinforcement learning for complex motion skills remains a challenging and unsolved problem, due to the high number of samples needed to train most algorithms and the expense of obtaining those samples from real robots. Most existing approaches to robotic reinforcement learning either fail to generalize between different tasks and among variations of single tasks, or only generalize by requiring collecting impractical amounts of real robot experience. With recent advancements in robotic simulation, and the widespread availability of large computational resources, a popular family of methods seeking to address this challenge has emerged, known as "sim-to-real" methods. These methods seek to offload most training time from real robots to offline simulations, which are trivially parallelizable and much cheaper to operate. Our method combines this "sim-to-real" schema with representation learning and model-predictive control (MPC) to make transfer more robust, and to significantly decrease the number of simulation samples needed to train policies which achieve families of related tasks.</p>
<p>The key insight behind our method is that the simulation used in the pre-training step of a simulation-to-real method can also be used online as a tool for foresight. It allows us to predict the behavior of a known policy on an unseen task. When combined with a latent-conditioned policy, where the latent actuates variations of useful policy behavior (e.g. skills), this simulation-as-foresight tool allows our method to use what the robot has already learned to do (e.g. the pre-trained policy) to bootstrap online policies for tasks it has never seen before. That is, given a latent space of useful behaviors, and a simulation which predicts the rewards for those behaviors on a new task, we can reduce the adaptation problem to intelligently choosing a sequence of latent skills which maximize rewards for the new task.</p>
<p>II. BACKGROUND</p>
<p>Most simulation-to-real approaches so far have focused on addressing the "reality gap" problem. The reality gap problem is the domain shift performance loss induced by differences in dynamics and perception between the simulation (policy training) and real (policy execution) environments. Training a policy only in a flawed simulation generally yields control behavior which is not adaptable to even small variations in the environment dynamics. Furthermore, simulating the physics behind many practical robotic problems (e.g. sliding friction and contact forces) is an open problem in applied mathematics, meaning it is not possible to solve a completely accurate simulation for many important robotic tasks [4]. Rather than attempt to create an explicit alignment between simulation and real [5], or randomize our simulation training to a sufficient degree to learn a policy which generalizes to nearby dynamics [6], our method seeks to learn a sufficient policy in simulation, and adapt it quickly to the real world online during real robot execution.</p>
<p>Our proposed approach is based on four key components: reinforcement learning with policy gradients (RL) [7], variational inference [8], model-predictive control (MPC), and physics simulation. We use variational inference to learn a low-dimensional latent space of skills which are useful for tasks, and RL to simultaneously learn single policy which is conditioned on these latent skills. The precise long-horizon behavior of the policy for a given latent skill is difficult to predict, so we use MPC and an online simulation to evaluate latent skill plans in the in simulation before executing them on the real robot.</p>
<p>III. RELATED WORK</p>
<p>Learning skill representations to aid in generalization has been proposed in works old and new. Previous works proposed frameworks such as Associative Skill Memories [9] and probabilistic movement primitives [10] to acquire a set of reusable skills. Our approach is built upon [11], which learns a embedding space of skills with reinforcement learning and variational inference, and [12] which shows that these learned skills are transferable and composable on real robots. While [12] noted that predicting the behavior of latent skills is an obstacle to using this method, our approach addresses the problem by using model-predictive control to successfully complete unseen tasks with no fine-tuning on the real robot. Exploration is a key problem in robot learning, and our method uses latent skill representations to address this problem. Using learned latent spaces to make exploration more tractable is also studied in [13] and [14]. Our method exploits a latent space for task-oriented exploration: it uses model-predictive control and simulation to choose latent skills which are locally-optimal for completing unseen tasks, then executes those latent skills on the real robot.</p>
<p>Using reinforcement learning with model-predictive control has been explored previously. Kamthe et al. [15] proposed using MPC to increase the data efficiency of reinforce-ment algorithms by training probabilistic transition models for planning. In our work, we take a different approach by exploiting our learned latent space and simulation directly to find policies for novel tasks online, rather than learning and then solving a model.</p>
<p>Simulation-to-real transfer learning approaches include randomizing the dynamic parameters of the simulation [6], and varying the visual appearance of the environment [16], both of which scale linearly or quadratically the amount of computation needed to learn a transfer policy. Other strategies, such as that of Barrett et al. [17] reuse models trained in simulation to make sim-to-real transfer more efficient, similar to our method, however this work requires an explicit pre-defined mapping between seen and unseen tasks. Saemundson et al. [18] use meta-learning and learned representations to generalize from pre-trained seen tasks to unseen tasks, however their approach requires that the unseen tasks be very similar to the pre-trained tasks, and is few-shot rather than zero-shot. Our method is zero-shot with respect to real environment samples, and can be used to learn unseen tasks which are significantly out-of-distribution, as well as for composing learned skills in the time domain to achieve unseen tasks which are more complex than the underlying pre-trained task set.</p>
<p>Our work is closely related to simultaneous work performed by Co-Reyes et al. [19]. Whereas our method learns an explicit skill representations using pre-chosen skills identified by a known ID, [19] learn an implicit skill representation by clustering trajectories of states and rewards in a latent space. Furthermore, we focus on MPC-based planning in the latent space to achieve robotic tasks learned online with a real robot, while their analysis focuses on the machine learning behind this family of methods and uses simulation experiments.</p>
<p>IV. METHOD</p>
<p>A. Skill Embedding Algorithm</p>
<p>In our multi-task RL setting, we pre-define a set of lowlevel skills with IDs T = {1, . . . , N }, and accompanying, per-skill reward functions r t (s, a).</p>
<p>In parallel with learning the joint low-level skill policy π θ as in conventional RL, we learn an embedding function p φ which parameterizes the low-level skill library using a latent variable z. Note that the true skill identity t is hidden from the policy behind the embedding function p φ . Rather than reveal the skill ID to the policy, once per rollout we feed the skill ID t, encoded as s one-hot vector, through the stochastic embedding function p φ to produce a latent vector z. We feed this same value of z to the policy for the entire rollout, so that all steps in a trajectory are correlated with the same value of z.
L(θ, φ, ψ) = max π E π(a,z|s,t) t∈T ∞ i=0 γ ir (s i , a i , z, t) s i+1 (1) wherê r(s i , a i , z, t) = α 1 E t∈T [H (p φ (z|t))] + α 2 log q ψ (z|s H i ) + α 3 H (π θ (a i |s i , z)) + r t (s i , a i )
To aid in learning the embedding function, we learn an inference function q ψ which, given a state-only trajectory window s H i of length H, predicts the latent vector z which was fed to the low-level skill policy when it produced that trajectory. This allows us to define an augmented reward which encourages the policy to produce distinct trajectories for different latent vectors. We learn q ψ in parallel with the policy and embedding functions, as shown in Eq. 1.</p>
<p>We add a policy entropy bonus H (π θ (a i |s i , z)), which ensures that the policy does not collapse to a single solution for each skill. For a detailed derivation, refer to [11].</p>
<p>B. Skill Embedding Criterion</p>
<p>In order for the learned latent space to be useful for completing unseen tasks, we seek to constrain the embedding distribution to satisfy two important properties: 1) High entropy: Each task should induce a distribution over latent vectors which is wide as possible, corresponding to many variations of a single skill. 2) Identifiability: Given an arbitrary trajectory window, the inference network should be able to predict with high confidence the latent vector fed to the policy to produce that trajectory. When applied together, these properties ensure that during training the policy is trained to encode high-reward controllers for many parameterizations of a skill (highentropy), while simultaneously ensuring that each of these latent parameterizations corresponds to a distinct variation of that skill. This dual constraint is the key for using model predictive control or other composing methods in the latent space as discussed in Sec. IV-C. We train the policy and embedding networks using Proximal Policy Optimization [20], though our method may be used by any parametric reinforcement learning algorithm. We use the MuJoCo physics engine [21] to implement our Sawyer robot simulation environments. We represent the policy, embedding, and inference functions using multivariate Gaussian distributions whose mean and diagonal covariance are parameterized by the output of a multi-layer perceptron. The policy and embedding distributions are jointly optimized by the reinforcement learning algorithm, while we train the inference distribution using supervised learning and a simple cross-entropy loss.</p>
<p>C. Using Model Predictive Control for Zero-Shot Adaptation</p>
<p>To achieve unseen tasks on a real robot with no additional training, we freeze the multi-skill policy learned in Sec. IV-A, and use a new algorithm which we refer to as a "composer." The composer achieves unseen tasks by choosing new sequences of latent skill vectors to feed to the frozen skill policy. Exploring in this smaller space is faster and more sample-efficient, because it encodes highlevel properties of tasks and their relations. Each skill latent induces a different pre-learned behavior, and our method reduces the adaptation problem to choosing sequences of these pre-learned behaviors-continuously parameterized by the skill embedding-to achieve new tasks.</p>
<p>Note that we use the simulation itself to evaluate the future outcome of the next action. For each step, we set the state of the simulation environment to the observed state of the real environment. This equips our robot with with the ability to predict the behavior of different skill latents. Since our robot is trained in a simulation-to-real framework, we can reuse the simulation from the pre-training step as a tool for foresight when adapting to unseen tasks. This allow us to select a latent skill online which is locally-optimal for a task, even if that task was seen not during training. We show that this scheme allows us to perform zero-shot task execution and composition for families of related tasks. This is in contrast to existing methods, which have mostly focused on direct alignment between simulation and real, or data augmentation to generalize the policy using brute force. Despite much work on simulation-to-real methods, neither of these approaches has demonstrated the ability to provide the adaptation ability needed for general-purpose robots in the real world. We believe our method provides a third path towards simulationto-real adaptation that warrants exploration, as a higher-level complement to these effective-but-limited existing low-level approaches.</p>
<p>We denote the new task t new corresponding to reward function r new , the real environment in which we attempt this task R(s |s, a), and the RL discount factor γ. We use the simulation environment S(∫ |∫ , ), frozen skill embedding p φ (z|t), and latent-conditioned skill policy π θ (a|s, z), all trained in Sec. IV-A, to apply model-predictive control in the latent space as follows (Algorithm 1).</p>
<p>We first sample k candidate latents Z = {z 1 , . . . , z k } according to p(z) = E t∼p(t) p φ (z|t). We observe the state s real of real environment R.</p>
<p>For each candidate latent z i , we set the initial state of the simulation S to s real . For a horizon of T time steps, we sample the frozen policy π θ , conditioned on the candidate latent a j∈T ∼ π θ (a j |s j , z i ), and execute the actions a j the simulation environment S, yielding and total discounted γ j r new (s j , a j ) for each candidate latent. We then choose the candidate latent acquiring the highest reward z * = argmax i R new i , and use it to condition and sample the frozen policy a l∈N ∼ π θ (a j |s j , z * ) to control the real environment R for a horizon of N &lt; T time steps.</p>
<p>We repeat this MPC process to choose and execute new latents in sequence, until the task has been achieved.</p>
<p>Algorithm 1 MPC in Skill Latent Space</p>
<p>Require: A latent-conditioned policy π θ (a|s, z), a skill embedding distribution p φ (z|t), a skill distribution prior p(t), a simulation environment S(s |s, a), a real environment R(s |s, a), a new task t new with associated reward function r new (s, a), an RL discount factor γ, an MPC horizon T , and a real environment horizon N . while t new is not complete do Sample Z = {z 1 , . . . , z k } ∼ p(z) = E t∼p(t) p φ (z|t) Observe s real from R for z i ∈ Z do Set inital state of S to s real for j ∈ {1, . . . , T } do Sample a j ∼ π θ (a j |s j , z i ) Execute simulation s j+1 = S(s j , a j ) end for
Calculate R new i = T j=0 γ j r new (s j , a j ) end for Choose z * = argmax zi R new i for l ∈ {1, . . . , N } do
Sample a l ∼ π θ (a l |s l , z * ) Execute simulation s l+1 = S(s l , a j ) end for end while</p>
<p>The choice of MPC horizon T has a significant effect on the performance of our approach. Since our latent variable encodes a skill which only partially completes the task, executing a single skill for too long unnecessarily penalizes a locally-useful skill for not being globally optimal. Hence, we set the MPC horizon T to not more than twice the number of steps that a latent is actuated in the real environment N .</p>
<p>V. EXPERIMENTS</p>
<p>We evaluate our approach by completing two sequencing tasks on a Sawyer robot: drawing a sequence of points and pushing a box along a sequential path. For each of the experiments, the robot must complete an overall task by sequencing skills learned during the embedding learning process. Sequencing skills poses a challenge to conventional RL algorithms due to the sparsity of rewards in sequencing tasks [22]. Because the agent only receives a reward for completing several correct complex actions in a row, exploration under these sequencing tasks is very difficult for conventional RL algorithms. By reusing the skills we have consolidated in the embedding space, we show a high-level controller can effectively compose these skills in order to achieve such difficult sequencing tasks.</p>
<p>A. Sawyer: Drawing a Sequence of Points</p>
<p>In this experiment, we ask the Sawyer Robot to move its end-effector to a sequence of points in 3D space. We first learn the low level policy that receives an observation with the robot's seven joint angles as well as the Cartesian position of the robot's gripper, and output incremental joint positions (up to 0.04 rads) as actions. We use the Euclidean distance between the gripper position and the current target is used as the cost function. We trained the policy and the embedding network on eight goal positions in simulation, forming a 3D rectoid enclosing the workspace. Then, we use the model-predictive control to choose a sequence latent vector which allows the robot to draw an unseen shape. For both simulation and real robot experiments, we attempted two unseen tasks: drawing a rectangle in 3D space (Figs. 5 and 7) and drawing a triangle in 3D space (Figs. 6 and 8). In this experiment, we test our approach with a task that requires contact between the Sawyer Robot and an object. We ask the robot to push a box along a sequence of points in the table plane. We choose the Euclidean distance between the position of the box and the current target position as the reward function. The policy receives a state observation with the relative position vector between the robot's gripper and the box's centroid and outputs incremental gripper movements (up to ±0.03 cm) as actions.</p>
<p>We first pre-train a policy to push the box to four goal locations relative to its starting position in simulation. We trained the low-level multi-task policy with four tasks in simulation: 20 cm up, down, left, and right of the box starting position. We then use the model-predictive control to choose a latent vectors and feed it with the state observation to frozen multi-task policy which controls the robot.</p>
<p>For both simulation and real robot experiments, we use the simulation as a model of the environments. In the simulation experiments, we use model-predictive controller to push the box to three points. In the real robot experiments, we ask the Sawyer Robot to complete two unseen tasks: pushing up-then-left and pushing left-then-down. </p>
<p>VI. RESULTS</p>
<p>A. Sawyer Drawing</p>
<p>In the unseen drawing experiments, we sampled k = 15 vectors from the skill latent distribution, and for each of them performed an MPC optimization with a horizon of T = 4 steps. We then execute the latent with highest reward for N = 2 steps on the target robot. In simulation experiments, the Sawyer Robot successfully draw a rectangle with by sequencing 54 latents (Fig. 2) and drew by sequencing a triangle with 56 latents (Fig. 3). In the real robot experiments, the Sawyer Robot successfully completed the unseen rectangle-drawing task by choosing 62 latents (Fig. 4) in 2 minutes of real time and completed the unseen triangledrawing task by choosing 53 latents (Fig. 5) in less than 2 minutes.</p>
<p>B. Sawyer Pusher Sequencing</p>
<p>In the pusher sequencing experiments, we sample k = 50 vectors from the latent distribution. We use an MPC optimization with a simulation horizon of T = 30 steps, and execute each chosen latent in the environment for N = 10 steps. In simulation experiments, the robot completed the unseen up-left task less than 30 seconds of equivalent real time and the unseen right-up-left task less than 40 seconds of equivalent real time. In the real robot experiments, the robot successfully completed the unseen left-down task by choosing 3 latents over approximately 1 minute of real time, and the unseen push up-left task by choosing 8 latents in about 1.5 minutes of real time.</p>
<p>C. Analysis</p>
<p>These experiment results show that our learned skills are composable to complete the new task. In comparison with performing a search as done in [12], our approach is faster in wall clock time because we perform the model prediction in simulation instead of on the real robot. Note that our approach can utilize the continuous space of latents, whereas previous search methods only use an artificial discretization of the continuous latent space. In the unseen box-pushing real robot experiment (Fig. 7, Right), the Sawyer robot pushes the box towards the bottom-right right of the workspace to fix an error it made earlier in the task. This intelligent reactive behavior was never explicitly trained during the pre-training in simulation process. This shows that by sampling from our latent space, the model-predictive controller successfully selects a skill that is not pre-defined during training process.</p>
<p>VII. CONCLUSION</p>
<p>In this work, we combine task representation learning simulation-to-real training, and model-predictive control to efficiently acquire policies for unseen tasks with no additional training. Our experiments show that applying model predictive control to these learned skill representations can be a very efficient method for online learning of tasks. The tasks we demonstrated are more complex than the underlying pre-trained skills used to achieve them, and the behaviors exhibited by our robot while executing unseen tasks were more adaptive than demanded by the simple reward functions us. Our method provides a partial escape from the reality gap problem in simulation-to-real methods, by mixing simulation-based long-range foresight with locally-correct online behavior.</p>
<p>For future work, we plan to apply our model-predictive controller as an exploration strategy to learn a composer policy that uses the latent space as action space. We look forward to efficiently learning a policy on real robots with guided exploration in our latent space.</p>
<p>Fig. 1 .
1The Sawyer robot performing the reaching task in simulation (left) and real world (right)Fig. 2. The Sawyer robot performing the box pushing task in simulation (left) and real world (right)</p>
<p>Fig. 3 .
3Skill Embedding Algorithm and MPC</p>
<p>Fig. 4 .
4Using</p>
<p>Fig. 5 .Fig. 6 .Fig. 7 .Fig. 8 .
5678Gripper position plots for the unseen rectangle-drawing experiment in simulation. In this experiment, the unseen task is drawing a rectangle in 3D space.0.450 0.475 0.500 0.525 0.550 0.575 0.Gripper position plots in unseen triangle-drawing experiment in simulation.In this experiment, the unseen task is to move the gripper to draw a triangle. Gripper position plots for the triangle-drawing experiment on the real robot. In this experiment, the unseen task is to draw a triangle. Gripper position plots in unseen triangle-drawing experiment on the real robot. In this experiment, the unseen task it to move the gripper to draw a triangle.B. Sawyer: Pushing the Box through a Sequence of Waypoints</p>
<p>Fig. 9 .Fig. 10 .
910Plot of block positions and gripper positions in simulation experiments. In the first experiment (left), the robot pushes the box to the right, up and then left. In the second experiment (right), the robot pushes the box to the left, then up, and then back to its starting position. Plot of block positions and gripper positions in real robot experiments. In experiment I (left), the robot pushes the box to the left, and then down. In experiment II (right), the robot push the box to the up, and then left.
https://github.com/rlworkgroup/garage 2 https://github.com/vitchyr/multiworld author(s) and do not necessarily reflect the views of the funding organizations.
ACKNOWLEDGEMENTSThe authors would like to thank Angel Gonzalez Garcia, Jonathon Shen, and Chang Su for their work on the garage 1 reinforcement learning for robotics framework, on which the software for this work was based. We also want to thank the authors of multiworld 2 for providing a well-tuned Sawyer Block Pushing simulation environment. This research was supported in part by National Science Foundation grants IIS-1205249, IIS-1017134, EECS-0926052, the Office of Naval Research, the Okawa Foundation, and the Max-Planck-Society. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the
Humanlevel control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A Rusu, J Veness, M Bellemare, A Graves, M Riedmiller, A Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran, D Wierstra, S Legg, D Hassabis, Nature. 5187540V. Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness, M. Bellemare, A. Graves, M. Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, "Human- level control through deep reinforcement learning," Nature, vol. 518, no. 7540, pp. 529-533, 2015. [Online]. Available: https: //www.nature.com/nature/journal/v518/n7540/pdf/nature1423\6.pdf</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, S Dieleman, D Grewe, J Nham, N Kalchbrenner, I Sutskever, T P Lillicrap, M Leach, K Kavukcuoglu, T Graepel, D Hassabis, 10.1038/nature16961Nature. 5297587D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. P. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis, "Mastering the game of go with deep neural networks and tree search," Nature, vol. 529, no. 7587, pp. 484-489, 2016. [Online]. Available: https://doi.org/10.1038/nature16961</p>
<p>Collective robot reinforcement learning with distributed asynchronous guided policy search. A Yahya, A Li, M Kalakrishnan, Y Chebotar, S Levine, abs/1610.00673CoRR. A. Yahya, A. Li, M. Kalakrishnan, Y. Chebotar, and S. Levine, "Collective robot reinforcement learning with distributed asynchronous guided policy search," CoRR, vol. abs/1610.00673, 2016. [Online]. Available: http://arxiv.org/abs/1610.00673</p>
<p>Noise and the reality gap: The use of simulation in evolutionary robotics. N Jakobi, P Husbands, I Harvey, Advances in Artificial Life. F. Morán, A. Moreno, J. J. Merelo, and P. ChacónBerlin, Heidelberg; Berlin HeidelbergSpringerN. Jakobi, P. Husbands, and I. Harvey, "Noise and the reality gap: The use of simulation in evolutionary robotics," in Advances in Artificial Life, F. Morán, A. Moreno, J. J. Merelo, and P. Chacón, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 1995, pp. 704-720.</p>
<p>Closing the gap between simulation and reality in the sensor and motion models of an autonomous ar.drone. A A Visser, N Dijkshoorn, M V D Veen, R Jurriaans, A. A. Visser, N. Dijkshoorn, M. V. D. Veen, and R. Jurriaans, "Closing the gap between simulation and reality in the sensor and motion models of an autonomous ar.drone." [Online].</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. X Peng, M Andrychowicz, W Zaremba, P Abbeel, abs/1710.06537CoRR. X. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, "Sim-to-real transfer of robotic control with dynamics randomization," CoRR, vol. abs/1710.06537, 2017. [Online]. Available: http://arxiv.org/abs/1710. 06537</p>
<p>Equivalence between policy gradients and soft q-learning. J Schulman, P Abbeel, X Chen, abs/1704.06440CoRR. J. Schulman, P. Abbeel, and X. Chen, "Equivalence between policy gradients and soft q-learning," CoRR, vol. abs/1704.06440, 2017. [Online]. Available: http://arxiv.org/abs/1704.06440</p>
<p>Auto-encoding variational bayes. D P Kingma, M Welling, abs/1312.6114CoRR. D. P. Kingma and M. Welling, "Auto-encoding variational bayes," CoRR, vol. abs/1312.6114, 2013. [Online]. Available: http://arxiv.org/ abs/1312.6114</p>
<p>Towards associative skill memories. P Pastor, M Kalakrishnan, L Righetti, S Schaal, Humanoids. P. Pastor, M. Kalakrishnan, L. Righetti, and S. Schaal, "Towards associative skill memories," in Humanoids, Nov 2012.</p>
<p>Extracting low-dimensional control variables for movement primitives. E Rueckert, J Mundo, A Paraschos, J Peters, G Neumann, ICRA. E. Rueckert, J. Mundo, A. Paraschos, J. Peters, and G. Neumann, "Ex- tracting low-dimensional control variables for movement primitives," in ICRA, May 2015.</p>
<p>Learning an embedding space for transferable robot skills. K Hausman, J Springenberg, Z Wang, N Heess, M Riedmiller, ICLR. K. Hausman, J. Springenberg, Z. Wang, N. Heess, and M. Riedmiller, "Learning an embedding space for transferable robot skills," in ICLR, 2018. [Online]. Available: https://openreview.net/forum?id= rk07ZXZRb</p>
<p>Scaling simulation-to-real transfer by learning composable robot skills. R C Julian, E Heiden, Z He, H Zhang, S Schaal, J Lim, G S Sukhatme, K Hausman, International Symposium on Experimental Robotics. SpringerR. C. Julian, E. Heiden, Z. He, H. Zhang, S. Schaal, J. Lim, G. S. Sukhatme, and K. Hausman, "Scaling simulation-to-real transfer by learning composable robot skills," in International Symposium on Experimental Robotics. Springer, 2018. [Online]. Available: https://ryanjulian.me/iser 2018.pdf</p>
<p>Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. S Gu, E Holly, T Lillicrap, S Levine, ICRA. IEEES. Gu, E. Holly, T. Lillicrap, and S. Levine, "Deep reinforcement learn- ing for robotic manipulation with asynchronous off-policy updates," in ICRA. IEEE, 2017.</p>
<p>Diversity is all you need: Learning skills without a reward function. B Eysenbach, A Gupta, J Ibarz, S Levine, B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine, "Diversity is all you need: Learning skills without a reward function," Feb. 2018. [Online]. Available: http://arxiv.org/abs/1802.06070</p>
<p>Data-efficient reinforcement learning with probabilistic model predictive control. S Kamthe, M P Deisenroth, abs/1706.06491CoRR. S. Kamthe and M. P. Deisenroth, "Data-efficient reinforcement learning with probabilistic model predictive control," CoRR, vol. abs/1706.06491, 2017. [Online]. Available: http://arxiv.org/abs/1706. 06491</p>
<p>CAD2RL: Real single-image flight without a single real image. F Sadeghi, RSSS Levine, RSSF. Sadeghi and S. Levine, "CAD2RL: Real single-image flight without a single real image," in RSS, 2017.</p>
<p>Transfer learning for reinforcement learning on a physical robot. S Barrett, M E Taylor, P Stone, Ninth International Conference on Autonomous Agents and Multiagent Systems -Adaptive Learning Agents Workshop (AAMAS -ALA). S. Barrett, M. E. Taylor, and P. Stone, "Transfer learning for reinforcement learning on a physical robot," in Ninth International Conference on Autonomous Agents and Multiagent Systems -Adaptive Learning Agents Workshop (AAMAS -ALA), May 2010. [Online]. Available: http://www.cs.utexas.edu/users/ai-lab/ ?AAMASWS10-barrett</p>
<p>Meta reinforcement learning with latent variable gaussian processes. S Saemundsson, K Hofmann, M P Deisenroth, abs/1803.07551CoRR. S. Saemundsson, K. Hofmann, and M. P. Deisenroth, "Meta reinforcement learning with latent variable gaussian processes," CoRR, vol. abs/1803.07551, 2018. [Online]. Available: http://arxiv. org/abs/1803.07551</p>
<p>Self-Consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings. J D Co-Reyes, Y Liu, A Gupta, B Eysenbach, P Abbeel, S Levine, J. D. Co-Reyes, Y. Liu, A. Gupta, B. Eysenbach, P. Abbeel, and S. Levine, "Self-Consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings," Jun. 2018. [Online]. Available: http://arxiv.org/abs/1806.02813</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, CoRR. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," CoRR, vol. abs/1707.06347, 2017. [Online]. Available: http://arxiv.org/abs/ 1707.06347</p>
<p>MuJoCo: A physics engine for model-based control. IROS. "MuJoCo: A physics engine for model-based control," in IROS, 2012.</p>
<p>. M Andrychowicz, F Wolski, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, P Abbeel, W Zaremba, Hindsight experience replay," in NIPSM. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welin- der, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba, "Hindsight experience replay," in NIPS, 2017.</p>            </div>
        </div>

    </div>
</body>
</html>
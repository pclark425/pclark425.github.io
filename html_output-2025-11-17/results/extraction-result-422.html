<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-422 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-422</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-422</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-248798614</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2205.06483v1.pdf" target="_blank">Modeling Human Behavior Part II -- Cognitive approaches and Uncertainty</a></p>
                <p><strong>Paper Abstract:</strong> As we discussed in Part I of this topic, there is a clear desire to model and comprehend human behavior. Given the popular presupposition of human reasoning as the standard for learning and decision-making, there have been significant efforts and a growing trend in research to replicate these innate human abilities in artificial systems. In Part I, we discussed learning methods which generate a model of behavior from exploration of the system and feedback based on the exhibited behavior as well as topics relating to the use of or accounting for beliefs with respect to applicable skills or mental states of others. In this work, we will continue the discussion from the perspective of methods which focus on the assumed cognitive abilities, limitations, and biases demonstrated in human reasoning. We will arrange these topics as follows (i) methods such as cognitive architectures, cognitive heuristics, and related which demonstrate assumptions of limitations on cognitive resources and how that impacts decisions and (ii) methods which generate and utilize representations of bias or uncertainty to model human decision-making or the future outcomes of decisions.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e422.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e422.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid cognitive architectures (symbolic-emergent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid cognitive architectures combining symbolic and emergent representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of cognitive-architecture designs that combine symbolic/declarative representations (symbols, rules, productions) with emergent/connectionist components (neural networks, learned associations) to gain both interpretability and adaptability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Symbolic–Emergent Hybrid Cognitive Architecture</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A modular cognitive architecture category that represents knowledge and high-level reasoning using symbolic declarative structures (e.g., symbols, if-then productions, chunks) while using emergent/connectionist components (e.g., ANNs, associative memory) for perception, pattern recognition, and adaptation; the hybridization aims to preserve explicit reasoning and planning while enabling learning and robustness to environment changes.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic representations: symbols/chunks, production rules, declarative memory (e.g., chunk-based knowledge as in ACT-R-style systems); rule-based planning and symbolic world models.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Emergent/connectionist modules: artificial neural networks or associative learning mechanisms for perception, feature association, and adaptive estimates (generally described as 'emergent' systems analogous to ANNs).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular combination: the paper describes a hybrid class where symbolic modules handle high-level planning/knowledge and emergent modules provide learned perceptual/associative inputs; integration is described at a conceptual level (symbolic modules query learned representations or receive processed signals from emergent modules), but no single unified end-to-end training recipe is given in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines interpretability and explicit planning from symbolic systems with adaptability and robustness to noisy inputs from emergent modules; can produce cognitively-plausible behaviors that are both explainable at a symbolic level and flexible to environment changes, and can better match human-like suboptimalities by allocating computation between symbolic deliberation and learned shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General cognitive tasks (planning, decision-making, simulated human behavior tasks); no single benchmark is defined in the survey — described conceptually across domains (e.g., tutoring systems, game agents, driver modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Paper asserts hybridization improves adaptability to changing environments compared to purely symbolic (which are brittle) while retaining better interpretability than purely emergent approaches; no quantitative OOD or compositional generalization metrics are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Declarative/symbolic component supports higher transparency and potential for explanation (rules, chunks, production traces) while emergent modules are acknowledged to be less interpretable; hybrid aims to keep symbolic-level explanations for decisions when possible.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Trade-offs: symbolic parts can be brittle and require domain models; emergent parts can be data-hungry and opaque. Creating the hybrid requires domain knowledge and engineering effort; sample inefficiency and model complexity are noted as challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Described in terms of complementary strengths: symbolic systems for explicit reasoning and bounded rational planning, emergent systems for perception and adaptation; the paper frames hybridization as a pragmatic division-of-labor (no formal unified theory provided).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e422.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e422.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACT-R IBL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instance-Based Learning implemented in the ACT-R cognitive architecture</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ACT-R implementation where declarative memory stores past state-action-outcome instances (chunks) and a blending/retrieval mechanism interpolates past instances to produce expected outcomes, combined with ACT-R procedural productions to select actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ACT-R Instance-Based Learning (IBL) model</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An ACT-R hybrid cognitive model combining declarative chunks representing past instances (state-action-outcome tuples) with procedural production rules; retrieval activation (time-decay, similarity weighting, stochasticity) and a blending mechanism interpolate outcomes from similar instances to drive procedural choices — used to model human decision patterns in dynamic tasks (example: a cyber-security attacker game).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Chunk-based declarative memory storing instances (state-action-outcome chunks) with activation-based retrieval (activation = recency/decay + context similarity + noise) and blending interpolation across retrieved instances (equations provided in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Procedural productions in ACT-R that implement decision control and action selection; the model treats cognitive operations as costly computations (meta-level MDP view) and uses production rules to terminate deliberation or request more computation.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Tight cognitive-architecture integration: retrieval of declarative chunks (using Boltzmann softmax over activation) feeds expected-value estimates into procedural production selection; blending interpolates outcomes and procedural thresholds determine when to act (acts as System-1 vs System-2 switching).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Produces human-like heuristics and biases (e.g., take-the-best, satisficing, random choice under certain stakes) as emergent behaviors from interaction of memory retrieval dynamics, blending, and procedural decision thresholds; models time/cost trade-offs and resource-rational choices.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Dynamic decision-making and simulated cyber-attacker task (cyber security game described in the paper), and Mouselab multi-alternative risky choice scenarios (discussed in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Model generalizes to reproduce qualitative human choice patterns across related decision scenarios by relying on recall-weighted interpolation; quantitative out-of-distribution or compositional generalization metrics are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability at the symbolic level: chunk contents, activation values, and production firings provide explainable traces and rationales for decisions; blending weights and activation math give mechanistic explanations for biases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires careful initialization of instances and domain modeling; can be sample-inefficient and sensitive to parameter choices (decay rates, mismatch penalties, temperature); quantitative performance against alternatives not provided in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Grounded in ACT-R cognitive architecture and resource-rational meta-level MDP framing (actions as costly cognitive computations), explaining biases as rational trade-offs between cognitive cost and expected reward.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e422.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e422.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACT-R + Unity integration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Integration of ACT-R cognitive models with the Unity game engine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical integration that couples the ACT-R cognitive architecture for cognitive control, memory, and decision-making with a Unity 2D/3D simulation environment so cognitive models can control and be evaluated on situated agents in rich simulated worlds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ACT-R / Unity hybrid simulation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A modular hybrid system where ACT-R provides the cognitive model (chunks, productions, IBL mechanisms) and Unity provides the simulated perceptual/motor environment; ACT-R drives agent behavior within Unity and receives perceptual state information from the simulation, enabling testing of cognitively-plausible agents in realistic virtual scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>ACT-R declarative memory (chunks) and symbolic production rules for goal decomposition, retrieval, and decision logic.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Unity engine providing procedural simulation, physics, rendering, and environment dynamics; perceptual preprocessing and low-level motor control typically handled by Unity-side code.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular runtime coupling: ACT-R executes cognitive cycles and issues actions/queries; Unity sends state/perceptual information to ACT-R and executes ACT-R-chosen actions in the simulated world. Integration is described as coupling cognitive loop to the simulation rather than end-to-end differentiable training.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables embodiment of cognitive models so that memory retrieval and procedural reasoning can interact with realistic temporal and physical constraints; facilitates evaluation of cognitive hypotheses in environments with rich dynamics, and can reveal human-like timing and error patterns under realistic sensorimotor conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Simulated task environments in Unity (e.g., racquetball agent, navigation tasks, cognitive-tutoring/serious game scenarios) used to evaluate cognitively-plausible behaviors; specific benchmarks depend on the experimental setup (paper cites multiple uses).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Allows cognitive models to be tested across varied simulated contexts, improving ecological validity; no quantitative claims about improved statistical generalization vs purely symbolic or purely simulated agents are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability through ACT-R traces (productions, retrievals) while being situated in an interpretable simulated world; facilitates qualitative explanations of agent behavior mapped to cognitive processes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Integration requires engineering work to map perceptual inputs to ACT-R-compatible representations and to reconcile timing; ACT-R model scalability and sample efficiency in large simulations are not addressed quantitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Engineering/pragmatic framework: use of cognitive-architecture + simulation coupling to evaluate cognitively plausible models in embodied tasks; not presented as a formal mathematical theory.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e422.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e422.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACT-R + physics engine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using a physics engine within ACT-R to aid decision making</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that augments ACT-R cognitive models with a procedural physics engine to provide predictive physical simulation inputs, improving decision-making in dynamic sensorimotor tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Physics-augmented ACT-R hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A hybrid where ACT-R's symbolic cognitive machinery (memory retrieval and production rules) queries a procedural physics engine to obtain forward predictions or affordance estimates (e.g., trajectories), thereby combining symbolic planning and memory with procedural physical simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>ACT-R declarative memory and symbolic productions for task control, retrieval-based estimation, and goal management.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Procedural physics engine (external simulator) that computes dynamic predictions (e.g., ball trajectories) and provides numeric, time-dependent data to the cognitive model.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Tight modular coupling: symbolic cognitive cycle requests predictions from the physics engine; the returned numeric predictions are consumed by ACT-R (e.g., via chunk encoding) and used by procedural productions to make decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables cognitively-modeled agents to use accurate physical predictions while preserving explainable symbolic decision logic; supports richer sensorimotor decision-making and can improve task performance where physical dynamics matter.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Sensorimotor tasks such as playing racquetball in simulation; used to assist decision making in embodied tasks (paper cites this use-case qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Combines symbolic generalization patterns with physics-based numerical predictions to handle novel physical configurations better than symbolic-only models lacking dynamics; no quantitative OOD evaluations reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Symbolic decision traces remain interpretable; physics-engine outputs are numerical and can be explained as inputs to symbolic decisions, yielding an explainable chain from physics prediction to action choice.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires interfacing continuous numerical simulation with discrete cognitive representations; computational cost and synchronization of cognitive cycles and simulation ticks are practical challenges; no quantitative scaling analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Pragmatic hybridization principle: augment symbolic cognitive models with procedural simulators to supply grounded predictive information; no formal theoretical proof of benefits provided in the survey.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Integrating ACT-R cognitive models with the Unity game engine <em>(Rating: 2)</em></li>
                <li>Using a physics engine in ACT-R to aid decision making <em>(Rating: 2)</em></li>
                <li>Reasoning with heuristics and induction: An account based on the CLARION cognitive architecture <em>(Rating: 2)</em></li>
                <li>Towards a quantum-like cognitive architecture for decision-making <em>(Rating: 1)</em></li>
                <li>Are quantum-like Bayesian networks more powerful than classical Bayesian networks? <em>(Rating: 1)</em></li>
                <li>ACT-R: A cognitive architecture for modeling cognition <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-422",
    "paper_id": "paper-248798614",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "Hybrid cognitive architectures (symbolic-emergent)",
            "name_full": "Hybrid cognitive architectures combining symbolic and emergent representations",
            "brief_description": "A class of cognitive-architecture designs that combine symbolic/declarative representations (symbols, rules, productions) with emergent/connectionist components (neural networks, learned associations) to gain both interpretability and adaptability.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Symbolic–Emergent Hybrid Cognitive Architecture",
            "system_description": "A modular cognitive architecture category that represents knowledge and high-level reasoning using symbolic declarative structures (e.g., symbols, if-then productions, chunks) while using emergent/connectionist components (e.g., ANNs, associative memory) for perception, pattern recognition, and adaptation; the hybridization aims to preserve explicit reasoning and planning while enabling learning and robustness to environment changes.",
            "declarative_component": "Symbolic representations: symbols/chunks, production rules, declarative memory (e.g., chunk-based knowledge as in ACT-R-style systems); rule-based planning and symbolic world models.",
            "imperative_component": "Emergent/connectionist modules: artificial neural networks or associative learning mechanisms for perception, feature association, and adaptive estimates (generally described as 'emergent' systems analogous to ANNs).",
            "integration_method": "Modular combination: the paper describes a hybrid class where symbolic modules handle high-level planning/knowledge and emergent modules provide learned perceptual/associative inputs; integration is described at a conceptual level (symbolic modules query learned representations or receive processed signals from emergent modules), but no single unified end-to-end training recipe is given in the paper.",
            "emergent_properties": "Combines interpretability and explicit planning from symbolic systems with adaptability and robustness to noisy inputs from emergent modules; can produce cognitively-plausible behaviors that are both explainable at a symbolic level and flexible to environment changes, and can better match human-like suboptimalities by allocating computation between symbolic deliberation and learned shortcuts.",
            "task_or_benchmark": "General cognitive tasks (planning, decision-making, simulated human behavior tasks); no single benchmark is defined in the survey — described conceptually across domains (e.g., tutoring systems, game agents, driver modeling).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Paper asserts hybridization improves adaptability to changing environments compared to purely symbolic (which are brittle) while retaining better interpretability than purely emergent approaches; no quantitative OOD or compositional generalization metrics are provided.",
            "interpretability_properties": "Declarative/symbolic component supports higher transparency and potential for explanation (rules, chunks, production traces) while emergent modules are acknowledged to be less interpretable; hybrid aims to keep symbolic-level explanations for decisions when possible.",
            "limitations_or_failures": "Trade-offs: symbolic parts can be brittle and require domain models; emergent parts can be data-hungry and opaque. Creating the hybrid requires domain knowledge and engineering effort; sample inefficiency and model complexity are noted as challenges.",
            "theoretical_framework": "Described in terms of complementary strengths: symbolic systems for explicit reasoning and bounded rational planning, emergent systems for perception and adaptation; the paper frames hybridization as a pragmatic division-of-labor (no formal unified theory provided).",
            "uuid": "e422.0"
        },
        {
            "name_short": "ACT-R IBL",
            "name_full": "Instance-Based Learning implemented in the ACT-R cognitive architecture",
            "brief_description": "An ACT-R implementation where declarative memory stores past state-action-outcome instances (chunks) and a blending/retrieval mechanism interpolates past instances to produce expected outcomes, combined with ACT-R procedural productions to select actions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "ACT-R Instance-Based Learning (IBL) model",
            "system_description": "An ACT-R hybrid cognitive model combining declarative chunks representing past instances (state-action-outcome tuples) with procedural production rules; retrieval activation (time-decay, similarity weighting, stochasticity) and a blending mechanism interpolate outcomes from similar instances to drive procedural choices — used to model human decision patterns in dynamic tasks (example: a cyber-security attacker game).",
            "declarative_component": "Chunk-based declarative memory storing instances (state-action-outcome chunks) with activation-based retrieval (activation = recency/decay + context similarity + noise) and blending interpolation across retrieved instances (equations provided in the paper).",
            "imperative_component": "Procedural productions in ACT-R that implement decision control and action selection; the model treats cognitive operations as costly computations (meta-level MDP view) and uses production rules to terminate deliberation or request more computation.",
            "integration_method": "Tight cognitive-architecture integration: retrieval of declarative chunks (using Boltzmann softmax over activation) feeds expected-value estimates into procedural production selection; blending interpolates outcomes and procedural thresholds determine when to act (acts as System-1 vs System-2 switching).",
            "emergent_properties": "Produces human-like heuristics and biases (e.g., take-the-best, satisficing, random choice under certain stakes) as emergent behaviors from interaction of memory retrieval dynamics, blending, and procedural decision thresholds; models time/cost trade-offs and resource-rational choices.",
            "task_or_benchmark": "Dynamic decision-making and simulated cyber-attacker task (cyber security game described in the paper), and Mouselab multi-alternative risky choice scenarios (discussed in survey).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Model generalizes to reproduce qualitative human choice patterns across related decision scenarios by relying on recall-weighted interpolation; quantitative out-of-distribution or compositional generalization metrics are not provided.",
            "interpretability_properties": "High interpretability at the symbolic level: chunk contents, activation values, and production firings provide explainable traces and rationales for decisions; blending weights and activation math give mechanistic explanations for biases.",
            "limitations_or_failures": "Requires careful initialization of instances and domain modeling; can be sample-inefficient and sensitive to parameter choices (decay rates, mismatch penalties, temperature); quantitative performance against alternatives not provided in the survey text.",
            "theoretical_framework": "Grounded in ACT-R cognitive architecture and resource-rational meta-level MDP framing (actions as costly cognitive computations), explaining biases as rational trade-offs between cognitive cost and expected reward.",
            "uuid": "e422.1"
        },
        {
            "name_short": "ACT-R + Unity integration",
            "name_full": "Integration of ACT-R cognitive models with the Unity game engine",
            "brief_description": "A practical integration that couples the ACT-R cognitive architecture for cognitive control, memory, and decision-making with a Unity 2D/3D simulation environment so cognitive models can control and be evaluated on situated agents in rich simulated worlds.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "ACT-R / Unity hybrid simulation",
            "system_description": "A modular hybrid system where ACT-R provides the cognitive model (chunks, productions, IBL mechanisms) and Unity provides the simulated perceptual/motor environment; ACT-R drives agent behavior within Unity and receives perceptual state information from the simulation, enabling testing of cognitively-plausible agents in realistic virtual scenarios.",
            "declarative_component": "ACT-R declarative memory (chunks) and symbolic production rules for goal decomposition, retrieval, and decision logic.",
            "imperative_component": "Unity engine providing procedural simulation, physics, rendering, and environment dynamics; perceptual preprocessing and low-level motor control typically handled by Unity-side code.",
            "integration_method": "Modular runtime coupling: ACT-R executes cognitive cycles and issues actions/queries; Unity sends state/perceptual information to ACT-R and executes ACT-R-chosen actions in the simulated world. Integration is described as coupling cognitive loop to the simulation rather than end-to-end differentiable training.",
            "emergent_properties": "Enables embodiment of cognitive models so that memory retrieval and procedural reasoning can interact with realistic temporal and physical constraints; facilitates evaluation of cognitive hypotheses in environments with rich dynamics, and can reveal human-like timing and error patterns under realistic sensorimotor conditions.",
            "task_or_benchmark": "Simulated task environments in Unity (e.g., racquetball agent, navigation tasks, cognitive-tutoring/serious game scenarios) used to evaluate cognitively-plausible behaviors; specific benchmarks depend on the experimental setup (paper cites multiple uses).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Allows cognitive models to be tested across varied simulated contexts, improving ecological validity; no quantitative claims about improved statistical generalization vs purely symbolic or purely simulated agents are provided.",
            "interpretability_properties": "High interpretability through ACT-R traces (productions, retrievals) while being situated in an interpretable simulated world; facilitates qualitative explanations of agent behavior mapped to cognitive processes.",
            "limitations_or_failures": "Integration requires engineering work to map perceptual inputs to ACT-R-compatible representations and to reconcile timing; ACT-R model scalability and sample efficiency in large simulations are not addressed quantitatively.",
            "theoretical_framework": "Engineering/pragmatic framework: use of cognitive-architecture + simulation coupling to evaluate cognitively plausible models in embodied tasks; not presented as a formal mathematical theory.",
            "uuid": "e422.2"
        },
        {
            "name_short": "ACT-R + physics engine",
            "name_full": "Using a physics engine within ACT-R to aid decision making",
            "brief_description": "An approach that augments ACT-R cognitive models with a procedural physics engine to provide predictive physical simulation inputs, improving decision-making in dynamic sensorimotor tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Physics-augmented ACT-R hybrid",
            "system_description": "A hybrid where ACT-R's symbolic cognitive machinery (memory retrieval and production rules) queries a procedural physics engine to obtain forward predictions or affordance estimates (e.g., trajectories), thereby combining symbolic planning and memory with procedural physical simulation.",
            "declarative_component": "ACT-R declarative memory and symbolic productions for task control, retrieval-based estimation, and goal management.",
            "imperative_component": "Procedural physics engine (external simulator) that computes dynamic predictions (e.g., ball trajectories) and provides numeric, time-dependent data to the cognitive model.",
            "integration_method": "Tight modular coupling: symbolic cognitive cycle requests predictions from the physics engine; the returned numeric predictions are consumed by ACT-R (e.g., via chunk encoding) and used by procedural productions to make decisions.",
            "emergent_properties": "Enables cognitively-modeled agents to use accurate physical predictions while preserving explainable symbolic decision logic; supports richer sensorimotor decision-making and can improve task performance where physical dynamics matter.",
            "task_or_benchmark": "Sensorimotor tasks such as playing racquetball in simulation; used to assist decision making in embodied tasks (paper cites this use-case qualitatively).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Combines symbolic generalization patterns with physics-based numerical predictions to handle novel physical configurations better than symbolic-only models lacking dynamics; no quantitative OOD evaluations reported.",
            "interpretability_properties": "Symbolic decision traces remain interpretable; physics-engine outputs are numerical and can be explained as inputs to symbolic decisions, yielding an explainable chain from physics prediction to action choice.",
            "limitations_or_failures": "Requires interfacing continuous numerical simulation with discrete cognitive representations; computational cost and synchronization of cognitive cycles and simulation ticks are practical challenges; no quantitative scaling analysis provided.",
            "theoretical_framework": "Pragmatic hybridization principle: augment symbolic cognitive models with procedural simulators to supply grounded predictive information; no formal theoretical proof of benefits provided in the survey.",
            "uuid": "e422.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Integrating ACT-R cognitive models with the Unity game engine",
            "rating": 2,
            "sanitized_title": "integrating_actr_cognitive_models_with_the_unity_game_engine"
        },
        {
            "paper_title": "Using a physics engine in ACT-R to aid decision making",
            "rating": 2,
            "sanitized_title": "using_a_physics_engine_in_actr_to_aid_decision_making"
        },
        {
            "paper_title": "Reasoning with heuristics and induction: An account based on the CLARION cognitive architecture",
            "rating": 2,
            "sanitized_title": "reasoning_with_heuristics_and_induction_an_account_based_on_the_clarion_cognitive_architecture"
        },
        {
            "paper_title": "Towards a quantum-like cognitive architecture for decision-making",
            "rating": 1,
            "sanitized_title": "towards_a_quantumlike_cognitive_architecture_for_decisionmaking"
        },
        {
            "paper_title": "Are quantum-like Bayesian networks more powerful than classical Bayesian networks?",
            "rating": 1,
            "sanitized_title": "are_quantumlike_bayesian_networks_more_powerful_than_classical_bayesian_networks"
        },
        {
            "paper_title": "ACT-R: A cognitive architecture for modeling cognition",
            "rating": 2,
            "sanitized_title": "actr_a_cognitive_architecture_for_modeling_cognition"
        }
    ],
    "cost": 0.01722975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Modeling Human Behavior Part II -Cognitive approaches and Uncertainty
13 May 2022</p>
<p>Andrew Fuchs 
Department of Computer Science
Universitá di Pisa
Italy</p>
<p>Institute for Informatics and Telematics (IIT)
ANDREA PASSARELLA and MARCO CONTI
National Re</p>
<p>Modeling Human Behavior Part II -Cognitive approaches and Uncertainty
13 May 2022ACM Reference Format: Andrew Fuchs, Andrea Passarella, and Marco Conti. 2018. Modeling Human Behavior Part II -Cognitive approaches and Uncertainty.CCS Concepts: • Human-centered computing• General and reference → Surveys and overviewsAdditional Key Words and Phrases: Artificial Intelligence, Machine Learning, Human Behavior, Cognition, Bias, Human-AI Interac- tion, Human-Centric AI
search Council (CNR), Italy As we discussed in Part I of this topic[30], there is a clear desire to model and comprehend human behavior. Given the popular presupposition of human reasoning as the standard for learning and decision-making, there have been significant efforts and a growing trend in research to replicate these innate human abilities in artificial systems. In Part I, we discussed learning methods which generate a model of behavior from exploration of the system and feedback based on the exhibited behavior as well as topics relating to the use of or accounting for beliefs with respect to applicable skills or mental states of others. In this work, we will continue the discussion from the perspective of methods which focus on the assumed cognitive abilities, limitations, and biases demonstrated in human reasoning. We will arrange these topics as follows (i) methods such as cognitive architectures, cognitive heuristics, and related which demonstrate assumptions of limitations on cognitive resources and how that impacts decisions and (ii) methods which generate and utilize representations of bias or uncertainty to model human decision-making or the future outcomes of decisions.</p>
<p>INTRODUCTION</p>
<p>Future autonomous and adaptive systems are expected to further exploit the concept of cyber-physical convergence [12], and realize an environment where autonomous agents and humans work together as teams, understanding each other and anticipating each other's behavior and intentions. This is propelled, on the one hand, by the pervasive diffusion of connected devices in the physical environment, which are directly owned (e.g. personal devices) or in tight interaction (e.g. IoT devices) with the human user. On the other hand, the vast diffusion of AI can bring autonomy of agents to a new level, making their behavior much more refined, and adaptive to the varying conditions of the environment and users. Human-Centric AI (HCAI) is expected to be a fundamental element in this vision. Not only because users need to trust AI agents, e.g., thanks to explainable algorithm [38]. Quite interestingly, AI agents will need to interpret human behavior in the context, so as to better interact with users, understand their actions, predict their choices, and ultimately orchestrate between actions performed directly by humans and those delegated to AI agents in autonomy.</p>
<p>To this end, it is fundamental to equip AI agents with practical models of the human behavior. Potential application areas are numerous, spanning from robotics, medicine, e-health, autonomous driving, just to mention a few. A key distinction between the goals and approaches is often the fidelity of the replication and the expected deployment case.</p>
<p>For instance, researchers may try to replicate the neurological pathways in an attempt to replicate the neuro-physical process underpinning reasoning [5], or they may instead attempt to generate a computational model which is meant to mimic heuristically biased behavior [71]. In any case, a common aspect is the desire to use humans as the template for desirable patterns of reasoning.</p>
<p>Though emerging, the literature on HCAI and related human behavioral models is quite vast already. Various types of HCAI exist. A first kind of HCAI systems focus primarily on explainability, making sure users can understand the process leading to a certain outcome by the AI agent [25,136]. A more advanced form of HCAI (typically referred to as hybrid intelligence) consists in AI agents and humans interacting directly, and impacting each other's operations [39,49]. Examples of such cases are AI agents learning in presence of human teachers [43,73,88,89,100,103], or humans exploiting the outcome of AI agents to acquire better comprehension of a phenomenon [115]. Moreover, in other cases humans and AI agents perform a common operation as a team [85,106]. In this context, designing approaches to orchestrate delegation of tasks and decisions among the team's members is also fundamental [92,102]. Figure 1 provides a taxonomy of the approaches existing in the literature to address these aspects. Work falling in the areas of direct learning of human behavior and modeling beliefs and reasoning are covered extensively in [30].</p>
<p>Specifically, they deal with approaches to learn human behavior by trial-and-error (such as Reinforcement Learning or Instance-based Learning) and approaches to prescriptively describe key features of human behavior such as reasoning processes and beliefs into a model (such as Theory of Mind). In this paper (Part II) we complete the survey by presenting in detail approaches falling in the other two classes depicted in Figure 1.</p>
<p>In this paper, we will discuss some of the popular topics and applications relating to Human-Centric AI, Human-AI Interaction, and Hybrid Intelligence. These topics include: bounded rationality and heuristics, cognitively plausible representations, bias, and more. These topics represent methods which attempt a model mimicking or inspired by differing biological/neurological, cognitive, and social levels of reasoning. Additionally, we will discuss how these topics align with application areas of interest. These application areas cover a wide assortment of both scenario as well as level of autonomy expected. For instance, this can include topics such as demographic preferences [45] to something as safety-critical as fully autonomous driving [28,135]. The specific scenario can rely significantly on the level of autonomy expected and the level of risk or control humans are willing to allow. In Section 3, we discuss methods which attempt to replicate the cognitive abilities and sub-optimality in humans. These allow for models which replicate biased or bounded rational behavior inspired by human cognition. Key to this Part Is modeling human cognitive resources, and ways adopted by humans to efficiently use them, possibly at the cost of obtaining imprecise understanding of the learned process or make mistakes. In Section 4 we focus on approaches that model uncertainty in the human reasoning process, to the point of leading to choices that do not appear to be the outcome of a rational process. Finally, Section 5 provides a critical discussion on the approaches presented in the paper. Before presenting them, we briefly mention key application areas in Section 2, highlighting how the considered approaches can be applied there. Note that, as in [30], we present each specific approach according to a common scheme. First, we point to specific surveys and related dealing in greater detail with that topic. Then, we discuss the general principles. Next, we discuss one concrete example where those principles are made practical. Finally, we briefly mention additional examples where the same principles have been applied.</p>
<p>Manuscript submitted to ACM Section-4. 2 Bias and Fairness Finally, also in this paper, as in [30], we limit the analysis to (a representative subset of) works providing quantitative models (e.g. math models or algorithms), as these are the approaches that allow to "code" human behavior in autonomous systems.</p>
<p>SAMPLES OF APPLICATION AREAS AND RELATED</p>
<p>In this section, we will briefly discuss popular application areas demonstrating uses of the techniques discussed in this paper. This list is not comprehensive, but serves to demonstrate topics which are likely more familiar and of immediate interest. The approaches used demonstrate methods which serve to replicate, model, or learn from human behavior and capabilities.</p>
<p>Driver Prediction and Autonomous Driving</p>
<p>There have been numerous examples of research performed to model and predict behavior in a driving scenario [60].</p>
<p>In the case of autonomous vehicles, there is a need to model and predict the outcome of control delegation between the autonomous system and the human driver. To do so, researchers have investigated cognitive models to predict the time to take over control given the type and difficulty of actions the human driver is performing when the control is switched [74,113,114]. This allows for a model which can simulate the cognitive and bio-mechanical responses when changing tasks for the human drivers.</p>
<p>AI in Games and Teaching</p>
<p>In the area of video and serious games, AI is being considered with respect to multiple aspects. In Part I of this topic, we discussed approaches integrating AI methods for learning models of behavior. In the case Serious Games, systems can be implemented to train human users and learn models of their behavior [56]. This allows for systems which can teach and allow for improvement with a more thorough understanding of the user's behavior. For instance, cognitive architectures (see Section 3.2) can be used to define intelligent tutoring systems [122].</p>
<p>Agent-Based Modeling</p>
<p>To achieve a model of human behavior and decision-making, numerous topics have been investigated. Some examples include agent-based modeling [22,37,45,54,118], which allows for models of groups of people or populations. In the case of agent-based modeling, models of human behavior are often defined and then studied in an environment over a simulated timeline. The agents follow the defined patterns of behavior and the resulting global patterns can be analyzed. For instance, population segregation based on demographic preferences regarding neighbors can be modeled by defining a diversity preference and modeling the movement of agents in an environment [45]. Populations of humans can also be modeled with other techniques designed to model the interdependence of the agents. Interactions can be modeled mathematically with humans represented as nodes in a network, particles in an environment, or more [23]. Additionally, the model of human behavior in an ABM can also be supported by models of Uncertainty, Bounded Rationality, or Cognitive Architectures, which we discuss in Section 3 and Section 4 with topics such as Section 3.2, Section 3.4, etc. Moreover, As we will focus on more direct and individual-level models of behavior, we will not present this topic in greater detail in this paper.</p>
<p>BOUNDED RATIONALITY AND COGNITIVE LIMITATIONS</p>
<p>In the following sections, we will discuss methods inspired by the cognitive limitations and characteristics demonstrated in human reasoning. The approaches discussed in Section 3.1 will demonstrate how humans utilize heuristics to enable fast and frugal reasoning as well as more deliberative systems. This combination of systems enables more efficient use of cognitive resources and has been demonstrated in numerous studies of human reasoning. Related to these concepts, we see the use of systems designed to replicate the cognitive and neurological performance (not necessarily the physical structure) seen in humans (see Section 3.2). These systems are designed to mimic human performance on tasks by replicating how humans use knowledge and memories to make decisions and perform tasks. Next, in Section 3.3, we discuss techniques which are inspired by how humans attend to stimuli and make associations between observed values. This can relate to human vision (i.e. foveation) or how humans identify correlations between different items in the same context (e.g. the word 'book' in a sentence would increase the relevance of the word 'library'</p>
<p>in the same sentence). Next, in Section 4.1, we will discuss the use of quantum representations to accommodate for uncertainty and provide methods which can support a quantum representation of states. Last, we will outline concepts relating to bounded rationality in the context of Game Theory in Section 3.4. Similar to previous sections, Section 4.2)</p>
<p>will discuss topics which focus less on a direct replication of cognitive functions; instead, these topics focus on the resulting biases that come from the use of heuristics and similar shortcuts in reasoning. Further, these limitations in reasoning generate immediate and long-term effects motivating studies on fairness.</p>
<p>Cognitive Heuristics</p>
<p>Relevant survey(s).</p>
<p>For relevant survey papers and related, please refer to [9] 3.1.2 Principles and Definitions. Bounded rationality describes the notion of humans making rational choices under the constraints ascribed to cognitive limitations of the decision-maker [6,109,117]. These constraints are a reflection of the assumed limitations or deficiencies in a human's computational abilities/capacity and knowledge. Similarly, these constraints can be viewed as respecting a notion of cognitive or computational cost. As a reflection of these limitations, there is an assumption that humans perform decision-making in a manner which allows them to find a reasonable approximation of the optimal solution while reducing overall cost or time. Reasonable could be viewed as a "good enough" or satisficing solution. The process of finding such an acceptable result is supported by shortcut techniques (i.e. heuristics) which allow a person to approximate the collection of alternative solutions by finding a set of satisfactory alternatives.</p>
<p>An additional aspect of bounded rationality is the assumption that humans will sacrifice exactness or optimality of a solution for the sake of efficiency. In this context, efficiency can refer to time required to a solution, cognitive resources needed, etc. In general, this alludes to a sense of frugality when it comes to the cognitive resources a human is willing to dedicate to a decision process. In most circumstances, this frugality does not cause issues and allows for sufficient stimulus processing. On the other hand, there are examples of where this may cause a significant omission of perception.</p>
<p>For example, humans can be tasked with observing a scene and then asked questions at the end of the observation pertaining to specific content. Often, there can be items or people hidden in plain sight due to the observer being distracted by more attention-grabbing stimuli, which demonstrates possible errors in the use of cognitive heuristics by the human brain [132].</p>
<p>Dual-System Reasoning. Relating to bounded rationality, there has been extensive interest in what is referred to as dual-system reasoning or dual process theory, now extending to topics in AI [9]. The argument is that humans utilize two systems of reasoning based on the context of the problem and the limitations of their cognitive systems. The assumption is the two levels handle the problems at different speeds, fidelity, cognitive cost, etc. These distinctions are based on the belief that humans tend to utilize a lower-cost reasoning system when the penalty for a sub-optimal solution is minor or when the time or cognitive burden of reaching a higher accuracy solution is too great (see [77,97] for more on cognitive cost). For simplicity, we follow a common convention and refer to them as System-1 and System-2. For an example of System-1 reasoning, catching a falling object typically doesn't leave sufficient time for deeper reasoning, so we rely on instinctive movements made quickly by System-1. On the other hand, System-2 can support deeper reasoning, longer time to a decision, etc.</p>
<p>System-1 is commonly assumed to be based on approximations generated via heuristics. These heuristics provide shortcuts to reasonably accurate solutions. For example, it has been argued that humans utilize what is known as the availability heuristic, which selects a solution based on the strongest association between the current situation and memories of potentially similar instances. This means humans will tend to place higher weight on memories more closely aligned with the current observations. As such, they will be biased toward solutions with higher likelihood of recall. This allows humans to use similar past experience to simplify the decision-making process when using System-1.</p>
<p>If the problem is too complex for System-1, then System-2 needs to be utilized. System-2 allows for deeper inspection and the possibility to use or combine multiple underlying processes.</p>
<p>The use of these systems can lead to biases in reasoning and potential incorrect assessments. A simple example is the Gambler's Fallacy, which demonstrates how humans tend to believe that a sequence of flips from a fair coin should be self-correcting [48,128]. In other words, when the coin is flipped multiple times, a sequence of identical outcomes is considered less and less likely as the length of the sequence grows. This misconception leads the person to feeling that the alternate outcome should be more likely in the next instance. As is apparent from the independence of the samples, this is in fact incorrect reasoning. Probability theory dictates that the outcomes of each toss should have no effect on the next toss. However, this is a common bias observed in human behavior resulting from this form of reasoning.</p>
<p>Humans demonstrate additional forms of heuristic-based and biased reasoning. The following are some additional examples [27]:</p>
<p>• Satisficing: Use a sufficient option rather than the optimal one • Affect: Make decision based on intuition or "gut feeling"</p>
<p>• Simulation: Estimate likelihood of an outcome based on the how easy it is to imagine the outcome • Availability: Estimate the likelihood of a future event based on the strength of recall for similar past occurrences</p>
<p>• Representation: Assume X is the same as Y when you notice X is similar to Y in some way(s)</p>
<p>• Association: Connect ideas based on the word association and the memories, meanings, or impressions they trigger • Optimistic Bias: The tendency to underestimate our own risks and overestimate our own control in dangerous situations • Hindsight Bias: The tendency to remember successful events as resulting from your own decisions and failures as resulting from bad luck or decision from others • Loss and Risk Aversion: Avoid risk and loss by maintaining the status quo • All or Nothing: Simplify decisions by treating remote probabilities as if they were not even possibilities These heuristics/biases demonstrate systems which can generate correct solutions in many cases, but can also lead to misconceptions or ignored information. As a result, it stands to reason that methods which wish to model the behavior and reasoning of humans will need to take these potential inaccuracies into account.</p>
<p>Applications and Recent</p>
<p>Results. In [71], the authors demonstrate an application of the Dual-Process concept of human reasoning and use of heuristics in a RL paradigm demonstrating multi-alternative risky choice in the Mouselab scenario (widely used to study decision strategies). In this scenario, their approach demonstrates the emergence of Manuscript submitted to ACM two known heuristics: take-the-best (TTB) (chooses alternative favored by the most predictive attribute and ignores others) and random choice. They note these are resource-rational strategies for low-stakes decisions with high and low dispersion of their outcome probabilities, respectively. They further note how the TTB heuristic is commonly used by humans when under time pressure and one outcome is much more likely than others. Similarly, the authors demonstrate how humans tend to accept random selection when the stakes are low in low-dispersion cases. The authors represent the bounded optimal decision process as a meta-level Markov Decision Process by considering the cost of computing a solution which impacts the utility of a decision or action. The actions are treated as costly computations, necessitating the ability to make decisions with efficiency in mind. This need for efficiency follows those seen in the justification of the representation and use of heuristics.</p>
<p>An augmentation to the MDP considered in this research is the meta-level MDP. In this case, actions for the metalevel MDP are cognitive operations C performed in belief states ∈ B. Additionally, the meta-level MDP has a transition function and reward function ∈ . The operations in C include an operator ⊥ which terminates deliberation and subsequently translates the current belief into an action. The determination to end deliberation and select an action can be seen as a representation of how humans select System-1 or System-2 reasoning, which then results in an outcome from the selected system. The reward combines the cognitive cost ∈ C with the expected immediate reward the agent expects to receive once deliberation terminates and an action is taken. In the case of a computation, the reward is defined as Additionally, researchers have used heuristics to represent how information diffusion occurs in a network of individuals [4,11,[78][79][80].
( , ) = − cost( ) for ∈ C; otherwise, ( , ⊥) = argmax ( ) ( ) where ( ) ( ) is</p>
<p>Cognitively/Biologically Plausible Representations</p>
<p>Relevant survey(s).</p>
<p>For relevant survey papers and related, please refer to [61] 3.2.2 Principles and Definitions. With the goal of achieving a general AI (i.e. reaching human-level intelligence [72]), there have been numerous approaches inspired by the cognitive mechanisms enabling the intelligence observed in humans. In [110], the authors note several ways how reaching human-level general intelligence might be possible. One noted method relates to the design and justification of cognitive architectures. In the case of cognitive architectures, the goal isn't always to achieve a perfect analog of the human brain and its neurological function; instead, a common goal is to generate a system capable of demonstrating the same kinds of abilities and deficiencies seen in human cognition, reasoning, intuition, etc. (e.g. perception, memory, attention) [21,32,53,61,126]. Under these circumstances, the goal is often the creation of a model of behavior which fits the cognitive/neurological dynamics of the human brain [2,5,61,108,119,123,130,134].</p>
<p>As noted in Section 3.1.2, it is generally accepted that humans reason with systems operating at different levels of fidelity. Humans can make faster and cognitively frugal decisions or utilize slower and more cognitively burdensome resources. As such, research has been dedicated to the creation of systems demonstrating these characteristics (and beyond) [61]. These systems demonstrate an ability to learn behavior as we've seen in previous sections in Part I (e.g.</p>
<p>Reinforcement Learning), but the distinction in this case is the emphasis on replicating the cognitive performance of humans. This distinction motivated us to place a higher emphasis on the cognitive and biologically plausible mechanisms of this portion of the paper. Further, there have been studies which show these representations can provide the best performing (and likely best fitting) approximations to human cognitive performance [121]. For example, cognitive architectures utilize memory systems which can replicate how humans retain information and utilize that information when making decisions. As a result, we see this section as more suitable in a cognitive limitations and biases context.</p>
<p>World Representation -Symbolic, Emergent, and Hybrid. To support reasoning and behavior, the system needs a method for representing the world. For cognitive architectures, there are three main categories for the underlying representations: symbolic, emergent, and hybrid. As the name would suggest, symbolic systems use symbols to represent concepts or knowledge. Given the symbols, the system can manipulate them by using a given set of instructions. The instructions can be provided through if-then rules or similar means. As can be expected, a symbolic representation allows for accurate planning and reasoning., but the potential downside being that this approach is brittle and does not adapt to changes in the environment. Emergent systems operate similar to what is seen in Artificial Neural Network (ANN) systems. Information is processed by the system and associations are made through a learning process. This of course increases the system's responsiveness to changes in an environment, but can reduce the transparency or the easiness of interpreting the system's behavior. To utilize the advantages of both systems, with the hope of overcoming the shortcomings, there are hybrid systems which combine the symbolic and emergent approaches.</p>
<p>Learning Methods. Learning in a cognitive architecture can be performed in several ways including: Declarative, Procedural, Associative, etc. [61]. In the case of Declarative learning, the system is provided a collection of facts about the world as well as relationships between them. For instance, many systems such as ACT-R, SAL, CHREST, or CLAR-ION utilize chunking mechanisms to declare new knowledge items. For Procedural learning, the system learns skills gradually through repetition, which can be accomplished through the accumulation of examples of successful execution of a task or problem. More closely aligned to RL, Associative learning is based on observations of rewards or punishments.</p>
<p>Memory. Architectures can be supported by different memory mechanisms depending on the type of capabilities being replicated. When performing a task, the memory utilized to temporarily store information related to the task at hand is referred to as working memory [3]. This memory is updated rapidly as the state of the world changes and actions are taken. Further, there is commonly an assumption regarding the capacity limitations of working memory for humans. In addition to working memory, other systems provided a means to accomplish long-term memory storage.</p>
<p>This can support storage of procedural memory to define basic skills or behavior or declarative memory for knowledge.</p>
<p>This allows for the storage of innate skills as well as accumulated knowledge. Additionally, some systems are defined with a hybridization of long and short-term memory, referred to as global memory. This results in all knowledge and memories being represented by the same system.</p>
<p>Applications and Recent</p>
<p>Results. The above characteristics are broad aspects covering different approaches for cognitive architectures. For a specific example, we present a recent result based on the ACT-R architecture. The authors present a cyber security game designed to demonstrate cognitive biases of cyber attackers [16]. This displays how humans are susceptible to fallacies in reasoning which result in sub-optimal and biased behavior. The authors demonstrate how their models replicate the biases motivating human behavior patterns in system selection and the choice to abandon a system and forfeit the previous effort on the current system.</p>
<p>The authors used an Instance-based Learning (IBL) model using the Adaptive Character of Thought -Rational (ACT-R) architecture. ACT-R is a theory of cognition which models how humans recall "chunks" of information from memory and how humans solve problems by splitting them into sub-goals [134]. Knowledge is applied from working memory as needed to find a pattern of behavior meeting the goal. This model utilizes techniques designed to mimic human memory retrieval, pattern matching, and decision making. IBL uses ACT-R's blending mechanism, which interpolates across past experiences to estimate an outcome. The interpolation is weighted by the contextual similarity between the present observation or instance and the past experiences. This provides an estimate expected outcome based on the consensus value which minimizes the dissimilarity (measured by ) from the values contained in instance defined as:
argmin × (1 − ( , )) 2(1)
where refers to an instance stored as a memory chunk representing a past state-action-outcome observation and refers to the retrieval probability (based on IBL-based measures). In the case where is interpreted as the error, then Equation 1 generates a least-squared error method [65]. In other words, this finds an estimated value which best fits the past observations that are weighted by their strength of recall. These estimated values are used to make a determination regarding which action/production should be executed. The measure or threshold which determines whether an action is available for execution limits the set of possible actions further. This means that the value is based on a representation which considers how strongly a memory is remembered, how similar the memory is to the current context, and the value observed by the choice made in that past observation. The strength of a memory represented by the retrieval probability utilizes a Boltzmann softmax equation
= / /(2)
where defines the temperature parameter, which scales probabilities defined by the activation function. The activation function provides a measure of how strongly a memory is remembered and associated with the current context. This strength is based on elapsed time since the observation was made. The activation for a chunk or instance is defined
as = ln =1 − + × ( , ) + (3)
where refers to the elapsed time since the ℎ occurrence of instance , is the decay rate (commonly set to 0.5), refer to the context elements, refer to the instance in memory, and is the mismatch penalty (in this case, set to the default of 1.0). The first term in Equation 3 provides the measure of strength based on the time elapsed and the second term is another similarity term similar to what is seen in Equation 1. is a weight term parameter which scales the similarity scores in the sum, and the last term, , is a variance parameter providing stochasticity in the activation function. Similar to Equation 1, the measure ensures the memories considered are a suitable match to the current context in order to prevent consideration of too dissimilar of instances. In more general terms, the above equations define a method for determining which memories are considered, how strongly they impact the estimate based on past observations, and how the resulting behavior occurs based on this historically weighted knowledge.</p>
<p>The agents are trained to perform the cyber attacker role. As such, the agents are provided observation instances which include the probability of a system being monitored, the reward for successful infiltration of a system, the penalty for detection, and a warning signal denoting whether a system is being monitored. The model is then primed with seven instances: five simulating a practice round, two representing knowledge of occurrences (absent and success, absent and uncertainty). This provides the system with an initial set of experience to allow for initialization of learning behavior without relying on random decisions. The model then is trained for four rounds of 25 trials.</p>
<p>Results. The model was tested in comparison to human performance. Human players were studied to generate a baseline of behavior and identify any demonstrated biases in outcomes. Based on the experiments, the authors show the human players demonstrating preferences or likelihoods of attack for different systems. They also demonstrated the cognitive systems performing equivalent preferences/probabilities. Additional Relevant Results. As mentioned, an underlying justification for using cognitively plausible approaches is the use of algorithms which mimic the performance of humans under the same task. Through the use of cognitive architectures, the cognitive and bio-mechanical processes for a human attempting to accomplish a goal can be replicated. For instance, models of user swipe behavior, mental folding, driver takeover of an autonomous vehicle, simulated cyber attackers, pilots, etc. can be created [15-17, 58, 59, 74, 99, 111, 113, 114]. The structure of the artificial systems architecture can have a significant impact on performance [116] and there are numerous examples of architectures and their underlying structures [61].</p>
<p>Recent research has investigated closer ties between cognitively inspired representations and topics in RL. For example, there have been attempts to utilize these representations to observe and predict behavior of agents in an environment [90,91]. These demonstrate the use of an agent's ability to learn a model of likely behavior in an attempt to anticipate likely next steps. The observer sees actions selected by RL agents and builds a model of likely behavior, be it next actions or likely goal states. Another aspect of mimicking processes exhibited by humans is the ability to imagine. This is demonstrated in [138] where they create an agent which can perform conceptual blending and daydreaming in order to generate dialogues inspired by the user behavior. Similarly, [24] demonstrates a similar use of related past instances and object relations in an RL context. For a simulation environment supporting these sorts of representations, [120] demonstrates an integration of the ACT-R architecture and Unity 2D/3D modeling system. This allows for control and learning for artificial agents in a Unity environment with the behavior being managed by ACT-R representations. This enables testing of cognitively inspired representations of behavior in a simulated world, which can enable the use of advanced game engine features for fidelity of the world model. In a similar topic, [96] demonstrates integrating a physics engine with a cognitive architecture to aid an agent's decision making. This is shown to aid the agent in playing racquetball in a simulated environment.</p>
<p>Cognitive representations can also be utilized to define an agent intended to interact or coordinate with a human.</p>
<p>For example, [84] utilizes a cognitive architecture and RL concepts to predict and model behaviors in a HAI context.</p>
<p>The scenario involves control of a moving circle on a screen which needs to follow a path which is scrolling down the screen. The human can either choose to take control or pass off control to the agent.</p>
<p>In another context, [63] demonstrates training of an RL agent using a sort of layered approach which they refer to as a two-learner system. The layering in their approach refers to the use of two agents to learn a policy. The first agent is used to learn an approximation of feedback signal which would result from backpropagation and the second learns the behavior policy based on the approximated feedback. The argument is the two-learner system could simulate cortical neuron physiology.</p>
<p>A ention</p>
<p>Relevant survey(s).</p>
<p>For relevant survey papers and related, please refer to [14,34,93].</p>
<p>Principles and Definitions.</p>
<p>Humans and other living beings do not process all the available perceptual information available to them. Instead, they utilize a cognitive and behavioral system which allows them to reduce the complexity of perception through an objective or subjective selectiveness with respect to information. This selectivity or bias is referred to as attention [14]. A basic interpretation justifying the biological need for attention would be the fact that our environment provides more stimuli than we can reasonably process fast enough for our environment. In this case, fast enough is with respect to the actions or behaviors necessary for survival.</p>
<p>When facing a critical situation, timeliness can be crucial; otherwise, an overload of stimuli could cause a costly delay (e.g. moving out of the path of an oncoming vehicle). As such, our brains allow us to reduce, or even ignore, information perceived in order to reduce the cognitive burden. Further, attention allows us to prioritize the information and assign more or less significance based on learned or perceived importance. In the oncoming vehicle example, it is likely not important to note the color of a building in the distance while estimating the speed and trajectory of the vehicle.</p>
<p>From a computational perspective, attention initially was studied primarily from the context of vision [14] where images were studied with under the task of identifying salient regions of the image. Artificial systems were developed to generate maps which would filter the input for processing. With the growing popularity of Deep Learning (DL), attention techniques were transitioned to neural network paradigms. Attention is used to modify the flow or processing of information in the network(s). The task of learning attention allows the systems to learn how to ignore stimuli, similar to the natural analogs mentioned. This allows systems to contextually alter the significance of information in order to better suit the underlying task. The attention mechanisms utilized in DL can be categorized as follows:</p>
<p>Attention. Soft attention uses softmax functions to weight the input elements with a weight value in (0, 1). This allows the system to learn and utilize an interdependence between different input parameters. Being based on softmax functions, soft attention provides a differentiable mechanism for attention. The soft attention scales the relative intensity of the input parameters.</p>
<p>Hard Attention. Hard attention, as the name suggests, is the complement of soft attention. It utilizes weights in {0, 1} to generate a mask to signify whether information is used or entirely ignored. As a result, the hard attention mechanism is non-differentiable. This necessitates a learning process for determining where to assign the weight values. In this case, there is a distinct exclusion of regions of the input domain while the remainder is observed at normal scale.</p>
<p>Self-Attention.</p>
<p>In self-attention, the system is learning an interdependence between sequential input elements. This allows a system to identify and utilize a notion of relation between items in the same input sequence. As a result, selfattention can be useful in understanding deeper relationships between items in the input rather than a holistic view of the input. For example, [131] introduces the transformer network which performs self-attention using representations  </p>
<p>The weights are used to compute a representation of each phase , as = ∈L × ( ). The representations of the possible phases ∈ P are then fed into an LSTM, to capture the sequential dependence between phases. The output of the LSTM ( ) is used, together with the representation of the possible phases , ∈ P for the second attention Manuscript submitted to ACM mechanism, modelled as in Equation 6. Specifically, this provides the probability ≔ { , ∈ P} of switching to any of the phases of the next time step.
= action-attention ({ , ∈ P}, )(6)
The reward is based on the negative of the intersection pressure defined in [133].  [29], which attempts to address self-attention in RL to help solve credit assignment. Similarly, [52] utilizes attention to address the temporal credit assignment problem. Further, [131,137] are in reference to the widely popular transformer network. The work in [131] provides an introduction of the approach while [137] extends the approach to introduce sparsity to allow the most contributive components for attention to be reserved while the other irrelevant information are removed. This is effective in preserving important information and removing noise, so the attention can be much more concentrated.</p>
<p>Further, [55] demonstrate the use of in an LSTM/RNN context for copying, transfer, and denoising tasks.</p>
<p>Game Theory</p>
<p>Relevant survey(s).</p>
<p>For relevant survey papers and related, please refer to [86,112]. Note that, since game theory is a widely investigated and well-known topic, in the following we only sketch very briefly the key ideas behind this theory. The main reason for mentioning it in this survey is to place it in the overall context of human behavioral models. Also, we don't mention explicitly applications and results, as the literature is huge and pinpointing only a (few) specific example(s) would be not that useful for the readers.</p>
<p>Principles and Definitions.</p>
<p>Extending and utilizing the notion of rational behavior, game theory focuses on the interdependence of choices when the circumstances involve a collection of individuals [86]. In game theory, the decisionmaker, often referred to as the player, is operating with an assumed feedback signal. The feedback (payoff) provides a value associated with how desirable or costly an outcome might be for the player. Based on the payoff and anticipated behavior of other players, a player can attempt to optimize their choice (action) in order to best ensure an acceptable outcome. In this case, rather than attempting to learn a policy of behavior, the models of utility define the constraints of an optimization problem. Based on these constraints, the players of a game can find a suitable solution which follows the assumptions of rationality.</p>
<p>More formally, a player will have a set of available actions from which they can select an action . The optimality of an action is dependent on multiple factors. First, the player has a payoff function : → R that will map an action to a value. This mapping depends on the definition of the problem and the interdependence of actions to values between the players. The values ( ) given by allow the player to generate a preferential ordering of actions, which indicates a player's need to identify desirable actions. Therefore, if the preferences of the player satisfy the following, then that player is considered rational under certainty [6]:</p>
<p>(1) (Completeness) 1 2 or 2 1</p>
<p>(2) (Transitivity) If 1 2 and 2 3 , then 1 3 The payoff values and the assumptions relating to rationality can then be utilized to identify an appropriate action or behavioral policy. In game theory, the behavioral policy of a player is referred to as the player's strategy or strategy profile.</p>
<p>The strategy of a player is a distribution over actions indicating the likelihood of selecting an action. A strategy which places the mass on more than one action would be considered a mixed strategy, which indicates a player does not select a particular action 100% of the time in a given scenario (i.e. a pure strategy).</p>
<p>The identification of a strategy is the process of finding an equilibrium. In an Nash equilibrium, the player strategies are such that deviation from the current strategy for any player would be undesirable as it would lead to another player having the means to take advantage of the change in order to achieve a better result. Depending on the game, it is possible to find zero, one, or even multiple equilibria. In the case of multiple equilibria, it is possible for the payoff for a particular player to vary between the equilibria, but it remains true that a deviation from the equilibrium from a single player would be undesirable [86]. </p>
<p>∈ X</p>
<p>The payoffs and assumption of rationality provide the constraints for an optimization-based solution method. However, the interdependency of the behaviors, outcomes, and payoffs can increase the difficulty of finding a suitable solution.</p>
<p>UNCERTAINTY AND IRRATIONALITY</p>
<p>antum Representations of Decisions and Irrational Thinking</p>
<p>Relevant survey(s).</p>
<p>For relevant survey papers and related, please refer to [1,20,47,76,83] 4.1.2 Principles and Definitions. It has been argued that traditional probabilistic representations do not fully represent the reasoning of humans [81] or can require exponentially more complex representations [76,83]. Instead, researchers have suggested the use of quantum-based methods for representing the statistical/probabilistic relationships between knowledge [47]. The argument is that the superposition-like representation better demonstrates how humans can have varying beliefs, which might not directly match the assumptions or requirements of probability (e.g. summing to one).</p>
<p>This allows for a representation which can perform in cases where the reasoning currently operates in a state where multiple outcomes are possible or represent the same indefinite state. This is also a proposed method to account for potentially irrational or probability-violating reasoning of humans [20,44]. The use of quantum representations also allows for replicating or modeling fallacies in human reasoning [1]. A common method for this representation is the use of a quantum-based Bayesian Network. In this case, a similar representation of the Bayesian network is utilized, but the dynamics are represented using quantum representations of probabilities.</p>
<p>Quantum Dynamics for Decision Models. As defined in the Reinforcement Learning section of Part I, sequential decision making can be modeled using a MDP. This representation provides a mechanism for representing the transition between world states based on a decision or action. Such a representation enables learning associations between actions and outcomes to build a model of behavior. In [10], the authors suggest the use of a quantum dynamics model to represent brain processes as a replacement for the classic MDP model. The quantum representation allows for transitioning from a 'single path' assumption to one which represents unknown previous states as a 'superposition of states'. The superposition model allows for modeling interference effects for unobserved paths, which violates the Markov representation.</p>
<p>As noted by [10], a key distinction in the representations is as follows:</p>
<p>According to the Markov model, for any given realization, the unobserved system occupies exactly one basis state | at each moment in time. A sample path of the Markov process is a series of jumps from one basis state to another, which moves like a bouncing particle across time. A different path is randomly sampled for each realization of the Markov process. According to the quantum model, for any given realization, the unobserved system does not occupy any particular basis state at each moment in time. A realization of the quantum process is a fuzzy spread of membership across the basis states, which moves like a traveling wave across time. Each realization of the quantum process is identical, producing the same series of states across time. All of the randomness in the quantum model results from taking a measurement.</p>
<p>Hence, the quantum representation supports the notion of wave interference in the dynamics representation.</p>
<p>From a modeling standpoint, the main difference between a Markov and a Quantum representation is therefore the following. In a specific realization of a Markov process, the state at any point in time is deterministic and can be represented with | ( ) as follows
| ( ) = ∈Ω ( ) · | ,(8)
where denotes the possible states, Ω is the set of possible states, and ( ) ∈ {0, 1} is an indicator variable. Essentially, | ( ) provides the (unique) state of the process at time for the specific realization.</p>
<p>On the contrary, in a quantum representation, the specific realization, at any point in time is not deterministically in one and only one state, but is in a superposition of all possible states, each with a given weight. In other words, even for a specific realization the process can be in any of the possible states, while the uncertainty can be only removed by "measuring" the specific state of the process. Specifically, the state, at time of the realization, denoted as | ( ) is modeled as follows
| ( ) = ( ) · |(9)
where where ( ) is a complex number representing the probability amplitude that the process is in the specific state | at time . As in any quantum superposition case, note that the squared magnitude of must be unity (i.e. | | 2 = 1)</p>
<p>in order to ensure the squared amplitudes produces a probability distribution over the possible states.</p>
<p>Finally, transitions between states become, in the case of quantum representation, specific quantum operations over the representation of the states provided by | ( ) , which translate, in the quantum domain, the traditional concept of transition probabilities between states of deterministic Markov Processes.</p>
<p>Applications and Recent</p>
<p>Results. In [40], the authors investigate the use of quantum representations in the context of categorization. The use of a quantum model is intended as a means to represent interference effects observed in categorization and the resulting impact of decision making. The inclusion of a quantum system allows the modeling of a state which represents uncertainty in the reasoning process. This models how humans demonstrate hesitance when facing a decision. For their experiments, the authors investigate two paradigm conditions: categorization decisionmaking (C-D) and decision alone (D alone). In the C-D condition, in each trial, participants were shown pictures of faces, which vary along two dimensions: face width and lip thickness. Participants were asked to categorize the face as a "good" ( ) guy or a "bad" ( ) guy and then make a decision to "attack" ( ) or to "withdraw" ( ). In the D alone condition, the participants were asked to make a decision directly without categorizing, but the faces shown were the same as in the C-D condition.</p>
<p>Proposed Method. Although categorization happens in the belief representation, it can influence the action part by producing the interference effect, which can also model the disjuction fallacy (i.e. the false judgement that the probability ( ∪ ) is less than either ( ) or ( )). Consequently, the authors utilized a method to predict it. For the proposed problem, the initial state involves six combination of beliefs and actions</p>
<p>{|</p>
<p>,
| , | , | , | , | }(10)
where, for example, | symbolizes a participant categorizing the face as good while still intending to attack.</p>
<p>Since participants are assumed to have some potential to be in any of the six quantum states, the person's state is a superposition of the six basis states:
| = · | + · | + · |(11)+ · | + · | + · |
with initial state corresponding to an amplitude distribution:
(0) =                              (12)
where | | 2 is the probability of observing state | initially. As an assumption, the the initial state is treated as equally distributed. In the process of decision making, updated information regarding a player's beliefs causes a Manuscript submitted to ACM transition in belief states. The decision-maker must convert this transition in reasoning states into a decision. To convert the observations and measurements into actions, participants must convert the uncertain state to either or . This represents when the decision maker transitions from uncertainty due to hesitation to a decision state. In this case, the authors propose the use of Pignistic Probability Transformation (PPT). This provides the following total probability of attacking given that the face is categorized as or respectively:
( | ) = Ψ( | ) + 1 2 Ψ( | ) 2 (13) ( | ) = Ψ( | ) + 1 2 Ψ( | ) 2(14)
where Ψ( | ) is the conditional amplitude (i.e. quantum equivalent of conditional operator) of attacking given the face is categorized as ∈ { , }. The above denotes an even attribution of the probability of transitioning from the uncertain state to decision. This then provides the means to calculate the total probability of attacking:
( ) = ( ) ( | ) + ( ) ( | ) = ∈{ , } | | 2 + | | 2 + | | 2 · Ψ( | ) + 1 2 Ψ( | ) 2(15)
A similar approach is utilized for ( ) and in the D alone condition. This allows a model of representing the state of uncertainty encountered before a decision is made and the process of converting updated beliefs into an action. Based on this model, the action likelihoods can be represented and estimated using a quantum modeling approach.</p>
<p>Results. For experiment conditions, the proposed method was tested against prior observations of human-generated data as well as prior prediction model results. In the data generation process, 26 participants were asked to categorize the face as good guy ( ) or bad guy ( ) and then make a decision to attack ( ) or to withdraw ( ). The faces roughly fall into two categories: "narrow" faces (narrow width and thick lips) or "wide" faces (wide width and thin lips). Participates were informed "narrow" faces had a 0.6 probability of belonging to the "bad guy" population. Similarly, participants were informed "wide" faces had a 0.60 probability of belonging to the "good guy" population. Rewards were were given for choosing attack for and withdraw for . In the D alone condition, the participants were asked to make a decision directly without categorizing, but the faces shown to participants were the same as those in the C-D condition.</p>
<p>Each participant provided 51 observations for the C-D condition and 17 observations for the D alone condition. The proposed approach showed a strong alignment with the pattern of behavior observed in the human participant results.</p>
<p>The authors also demonstrate the performance of the proposed approach with respect to sensitivity analysis. The authors show how sensitive the result of the method are to a ±5% change in the original human results. In this test, the method again shows strong performance, but the authors note a stronger sensitivity to changes in the case versus the case. For a plot of these results, please refer to the original article.</p>
<p>Additional Relevant Results. In addition to the prior examples, quantum representations can also support fuzzier associations between knowledge items. The use of quantum representations allows for deviations from normative probabilistic methods and can support a notion of interference between variables and paradoxical behavior [44,82].</p>
<p>When considering multiple individuals, quantum-based methods have also been investigated to model the diffusion of information [87]. The flexibility of representation from quantum methods also enables handling of the disjunction fallacy [i.e. the false judgment that ( ∪ ) is less than either ( ) or ( )], which violates the law of total probability [40]. Quantum representations can also be used to represent states in game theory settings where there is uncertainty about the potential behavior of others or the current state [83,98]. Further, as noted above, the concept of Bayesian Networks and causality can be extended to utilize representations supported by quantum dynamics [20,44,82].</p>
<p>Biases and Fairness in Representations and Understanding</p>
<p>Relevant survey(s).</p>
<p>For relevant survey papers and related, please refer to [9,18,18,31,54,69,75,94,109,124] </p>
<p>Principles and Definitions.</p>
<p>Bias. Humans demonstrate multiple forms and sources of biases. In one case, humans can demonstrate skewed interpretations of data, probabilities, confidence, etc. [69]. This kind of irrationality and bias is often attributed to the use of cognitive heuristics. It is argued that the use of heuristics causes humans to often accept coarse analysis and sub-optimal solutions. This demonstrates how humans can violate the traditional notions of rational behavior. Further, the use of heuristics and other shortcuts can lead to skewed interpretations of information. These biases and coarse representations can lead to sensitivity regarding the interpretation of rate events, risk, probability, and more. Related, it has also been shown how humans tend to under-react to probabilistic information and also fail to follow belief updates modeled by Bayes' rule [18].</p>
<p>Another aspect of bias can come in the form of inattentiveness, which could be linked to the bounded computational power of human cognition. Humans also demonstrate a resistance to changes in views or opinions when facing contradictory information [75]. In fact, it is common for people to become more convicted in their views rather than convinced they might have been wrong. Bias can also come in the form of inductive bias introduced in architecture design and algorithmic choices made by the humans generating algorithms. Both implicitly and explicitly, humans introduce inductive biases into the artificial systems they are developing. In fact, [33] notes how much of the success of Deep Learning models could be attributed to the inclusion of inductive bias in these systems. Further, the authors note how the use of inductive biases might be a requirement for the creation of generalized artificial intelligence. They note how the biases allow for assumptions regarding the problem being solved or the interpretation of information, which makes the system better suited for adaptation to broader datasets.</p>
<p>Probability-Based Models of Behavior and Reasoning. To model deficiencies or biases in understanding and reasoning in humans, researchers have investigated Bayesian representations and similar. For instance, (Hierarchical) Bayesian models can demonstrate how humans reason about likelihoods of outcomes and likelihoods of scenarios [18,19,105,125,129,139], and how human reasoning shows flawed interpretations or skewed scales of importance (high or low). This illustrates how humans can under-react to probabilities [18,19] or estimate likelihoods based on observed frequencies. Studies have shown how people tend to select options at a similar frequency to their probability of good outcome rather than developing a bias to the choice with highest likelihood of good outcome [139]. Similarly, Bayesian models can replicate the performance of human study participants when tasked with selecting in a multiarm bandit problem [107]. This type of representation of understanding can also be extended to a causal model, which models the cause and effect relationship between different environment aspects [36]. Causal models are an important aspect of human reasoning as they allow for predictions and retrospective reasoning.</p>
<p>Fairness. With the use of algorithms, a consideration needs to be made with respect to the fairness of outcomes. As noted in [67], fairness requires equal or equitable treatment of everyone based on their performance or needs. As a concept, this is straightforward. There should be as little (preferably none) bias in the treatment of individuals. We would prefer the outcome of a human's decision be fair, so it is natural to desire the same behavior from an algorithm.</p>
<p>Fairness can both be a quantity measured with respect to a metric [42], or fairness can represent a qualitative impression people have about a feature or outcome [35,67]. These measures provide an indication of how people perceive the features of an algorithm or its behavior. The notion of fairness can also be considered with respect to longer timescales and deeper consequences. In [42], the longer effects of fairness measures and decisions are weighed to indicate how strongly an approach may impact the behavior of the humans affected.</p>
<p>Applications and Recent</p>
<p>Results. In [42] the authors model population-level changes at the macro scale that are caused by algorithmic decisions and how this relates to fairness. Using measures of segregation from sociology and economics, they quantified these changes. This allowed them to measure different directions of the shift in the group-conditional distribution based on the different models demonstrated. In this context, the authors note how most notions or measures of fairness assume a static population. They argue that this approach fails to account for longterm welfare and prosperity. Therefore, they propose a measure which considers a notion of effort based on economic literature on Equality of Opportunity. Their effort function highlights idea that the necessary changes for a desirable outcome often are more difficult for a member of a disadvantaged group compared to an advantage counterpart. Based on this concept, they formulate effort unfairness as the discrepancy in effort required for members in different groups to obtain desired outcomes.</p>
<p>In their approach, they assume individuals imitate an exemplar individual who demonstrates a more desirable algorithm outcome, which is related to social learning. The assumption being that the observer would believe this imitation offers a higher likelihood of a better outcome, which suggests an individual would exert effort to attain a replication of the exemplar's social model if doing so could result in higher overall utility. To model this dynamic, the authors propose a group-dependent, data-driven measure of effort, which is inspired by literature on Equality of Opportunity (EOP). This is noted as the effort it takes individual to improve their feature value from to ′ . This effort is assumed proportional to the difference between the rank/quantile of ′ and in the distribution of feature in 's social group.</p>
<p>This means an individual which successfully replicates the role model obtains a positive utility (reward minus effort).</p>
<p>In this context, the authors consider a standard supervised learning setting with training data set = {( , )} =1 of instances = ( , ), where ∈ X specifies the feature vector for individual and ∈ Y, the ground truth label for him/her. Further, let refer to the sensitive feature value (e.g. race, gender, or their intersection) for individual .</p>
<p>To measure the impact of a change in label, the authors define benefit function : X, Y × Y → R that quantifies the benefit of a change in label from toˆ and is assumed a linear function.</p>
<p>To formally define measures of effort, reward, and utility, the authors provide the following. For deployed model ℎ and individual characterized by = ( , ), let R ℎ ( , ′ ) specify the reward/benefit as a result of characteristic change from to ′ :
R ℎ ( , ′ ) = (ℎ( ′ ), ′ ) − (ℎ( ), ) ,(16)
where &gt; 0 is a constant specifying the individual's degree of risk aversion. Again, this demonstrates a measure which considers the impact on an individuals outcome when they change a characteristic. For instance, a person could attain a degree or find a new job. Let E ℎ ( , ′ ) specify the effort required for a qualification change from to ′ . The overall utility of the individual is then U ℎ ( , ′ ) = R ℎ ( , ′ ) − E ℎ ( , ′ ) (i.e. utility equals reward minus effort).</p>
<p>Based on the above assumptions and definitions, the authors define several effort-based measures of unfairness, including:</p>
<p>Definition 4.1 (Effort-reward Unfairness). For a predictive model h, the effort-reward unfairness is the inequality of the following metric across different groups:
E ∼ : = max ∈Z U ℎ ( , )(17)
This provides a measure of the performance of the highest utility members of a group achieved by a higher level of effort. The authors note the impact of both threshold and the models. It is demonstrated that and the model can cause a significant change in outcome and effort level for the same data.</p>
<p>To measure performance, the authors utilize several techniques from sociology measuring segregation. First, evenness measures how unevenly the minority group is distributed over areal units. The evenness value is maximized when all units have equivalent relative numbers of minority and majority members as the whole population. For a formal definition, the authors use the Atkinson Index (AI), which measures inequality. The second measure, centralization is the degree to which a group is spatially located near the center of an urban area. This can be measured by comparing the percentage of minority residents living in the central areas of the city. The authors utilize the Centralization Index (CI), which is defined as:
central ,(18)
where is the total minority population. The third measure of segregation, the Absolute Clustering Index (ACI), Additional Relevant Results. As noted above, biases can be represented in both interpretations of information as well as in the exhibited behavior. This can be demonstrated by Bayesian models of cognitive biases [105], biased probability judgments [139], sensitivity to risk/uncertainty [26], and similar topics relating to representation and interpretation of data/statistics [19,70]. Biases can also be exhibited in how people respond to others or digital avatars (e.g. racial preferences in representation or interpretation of faces [46,68]). These biases can also influence behavior due to or via heuristics [51,64] both in terms of reasoning as well as decision-making (e.g. experience level impacting performance of heuristic outcome). Another factor relating to bias is its impact on model designs and inductive biases introduced into systems [33,62,101]. These biases can impact how a system is formed, how it utilizes contextual information, or how the system might need to compensate for unobserved factors available at decision time when learning from past behaviors of humans. Alternatively, it can also be important to avoid introducing biases caused by algorithms making determinations or learning from data which might be corrupted. The approaches in [127] and [104] demonstrate an effort to enable systems with the ability signal when they are uncertain whether a label for an input item is accurate via an abstention action. This can make systems more accurate as well as less susceptible to perturbation-based attacks or label noise. An additional context for bias is the aspect of fairness [7,8,13,50]. This can be in relation to both the representation provided to the algorithm as well the decision or outcome. Systems which exhibit unfair biases can generate disadvantages when the outcome has impacts on decisions relating to humans. For instance, non-uniformity or under-representation of samples in data can bias the performance of the trained model. If the lack of representation is in relation to race or gender, this can cause sociological consequences.</p>
<p>CONCLUSION</p>
<p>This paper completes the survey on approaches to model the human behavior in Human-Centric AI systems, complementing areas covered in [30]. In general, modeling the human behavior is seen as a cornerstone of future autonomous and adaptive systems, which can unleash full integration between humans and AI agents working in tandem. It will allow AI agents to make their choices more comprehensible by users, on the one hand, and AI agents to interpret, predict and take account of human actions and choices, on the other hand. While in [30] we have covered approaches where AI agents learn human behavior by trial-and-error, as well as approaches "coding" directly models of human reasoning, in this paper we have focused on approaches taking into account limitation of cognitive resources and how to optimize the, as well as approaches modeling uncertainty in reasoning and irrational behaviors.</p>
<p>In Section 3 (Bounded Rationality and Cognitive Limitations), we outlined and discussed approaches which can or attempt to replicate how humans gain/utilize knowledge to perform tasks, taking in particular account an efficient use of (limited) cognitive resources. In these cases it is often necessary to provide models of world dynamics and actions available so the model can replicate human performance, which clearly can offer effective performance. However, the models required to generate solutions typically require domain knowledge and so can be difficult or costly to generate.</p>
<p>For instance, architectures like those described in Section 3.2, typically rely on models of world and cognitive dynamics to enable reasoning over available capabilities given the bounds on time and cognitive cost. Still, these approaches can be effective when trying to better align with how humans perform tasks and allocate cognitive resources. On the other hand, these approaches can also be sample inefficient or require detailed (possibly expensive) models of knowledge. Related, behavior and reasoning can be represented through the use of heuristics mimicking the two systems humans are believed to utilize as seen in Section 3.1. The use of these heuristics has similar benefits and pitfalls. Again, these models often prove effective and are efficient to utilize, but often rely on domain knowledge or potentially coarse representations of human reasoning. The representation of reasoning with heuristics demonstrates an approach to model the assumed cognitive bounds on human reasoning. Such boundedness is also related to the assumption of bounded rationality found in Game Theory (Section 3.4). This assumption allows for constraints on the representation which enables generation of solutions. These solutions optimize behavior based on these assumptions as they enable reasoning over the behavior of all players in a system based on their expected payoffs for possible outcomes.</p>
<p>This representation enables strong guarantees and methods, but can often be conservative and a less than accurate portrayal of actual human behavior (e.g. players may not behave rationally). With another focus, in order to integrate capabilities related to reasoning and how humans utilize stimuli given their available cognitive resources, researchers have investigated methods for mimicking how humans attend to information. In these approaches, as discussed in Section 3.3, systems learn a model for masking and associating input values to learn a deeper understanding of the stimulus. These models have shown strong performance in language and behavior scenarios and can allow for more abstract representations supporting more variation in the scenario composition. On the other hand, as with many Deep Learning scenarios, these methods can be difficult to train or require a large amount of samples, so careful consideration is needed when defining an approach.</p>
<p>For the remainder of the paper, in Section 4 (Uncertainty and Irrationality), we discuss approaches which represent methods reasoning or considering irrationality, biases, or beliefs of humans. In this context, the presented approaches learn or use models of belief. This can be the effect of beliefs with respect to how it introduces biases in behavior and reasoning (see Section 4.2). Similar to the previous topic, this also requires development of models, which can be costly and challenging. On the other hand, these methods can offer effective means for measuring the impact of biases in behavior or reasoning with respect to future outcomes or fairness. In addition to biases, the process of human reasoning involves moments of uncertainty. In this case, the decision-maker exists in a state of uncertainty as they transition from an initial state to a decision. These transitions represent new knowledge or changes in belief, which impact the final outcome. We discussed one approach for representing uncertainty in reasoning in Section 4.1. Such a representation utilizes the flexibility of quantum mechanisms and the aspect of the interference effect for unobserved paths rather than the assumption of a single path seen in Markovian models. This improves the flexibility of representation and allows for representations which model fallacies in reasoning (e.g. disjunction fallacy), but also increases the complexity of the resulting models.</p>
<p>All in all, we can conclude there is not a one-size-fits-all approach to best model the human behavior in HCAI. Each approach has its onw pros and flip side, and the best choice largely depends on the specific problem at hand. However, the literature on this topic is very vast and articulated, and this allows designers of HCAI systems to leverage an extensive toolbox to equip AI agents with practical approaches to model human behavior. Therefore, we believe this area is going to emerge as one of the most active in the coming years, in the field cutting across autonomous and adaptive systems, pervasive environments, and advanced AI agents. </p>
<p>ACKNOWLEDGMENTS</p>
<p>Fig. 1 .
1Taxonomy of concepts</p>
<p>the expected reward of action according to belief . Results. The MouseLab scenario provides a testbed in which agents can improve the likelihood of successful decisions by performing additional information acquisitions. While the acquisition improves the decision, it also incurs a cost. Therefore, the agent should minimize the occurance of cognitive costs while maximizing the subsequent game outcome. This promotes a tradeoff of decision quality and decision time, mimicking the similar processes witnessed in human cognition. The authors note their proposed method rediscovered Take-The-Best (TTB), Weight-Additive Strategy (WADD), and then random choice strategy. The additional strategy, WADD, is performed by computing the expected values of all gambles using all possible payoffs. There were three noted outcomes regarding the predictions and the pattern which justify use of heuristics and match the observation of study participants (200 participants on Amazon Mechanical Turk). First, the model predicted fastand-frugal heuristics should be prioritized/utilized more frequently in high-dispersion trials (high dispersion means an outcome significantly more likely than the others and fast-and-frugal heuristics ignore all outcomes except the most probable). Second, the model indicates the utility of simple heuristics, primarily when the stakes are low. Third, the model indicates the benefit of increased time and effort for high-stake scenarios to receive the highest possible payoff. Additional Relevant Results. There have been numerous studies and examples demonstrating the errors in human reasoning and many of the studies relate the failings to this form of reasoning. Some examples include: [64, 66, 128].</p>
<p>to the dimension. In this case, the attention mechanism learns associations between different components of the input and their corresponding strengths via a representation as keys and queries . This allows the model to learn a relationship between the current task, the input data, and the current query. The associations are learned as weight matrices which scale the input values and give the weighted strength of association. This generates a weighted association between elements of the input and forms a compatibility measure of the values.3.3.3 Applications and Recent Results. In [95], the authors demonstrate the use of attention with a RL agent to control simulated traffic lanes. The motivation for the use of attention provided references the fact that traditional systems would require retraining for a new lane configuration. The use of attention allows for more flexible representations and can handle different numbers of roads/lanes. The proposed algorithm is tested against several baselines and demonstrates strong performance in traffic regulation. For a road with intersection ∈ M, define the traffic characteristics of lane ∈ L at time , where L denotes the set of all approaching lanes to the intersection. Additionally, L and L refer to entering and exiting lanes respectively, and so L = L L . Further, define traffic movement as a set that maps traffic of lane ∈ L to possible leaving lanes ′ ∈ L . In this context, the set of valid traffic movement ∈ P during a green light are called a phase. Define participating lanes L as the set of lanes that have appeared in at least one traffic movement of phase . Note that each phase has a minimum time and following that time, a decision about the next phase should be made. The definition of the RL problem requires translating the domain into states, actions, and rewards. They utilize the traffic characteristic as the state at time . For actions, this is represented by the assigned active phase at time + 1.Authors use two attention mechanisms to define the policy to select the next action at time , or better the next phase.Specifically, the first attention mechanism defines weights to be used in considering the states of the corresponding lanes. This attention step, modelled in Equation 5, generates a vector of weights ≔ { , ∈ L }, where each weight corresponds to a specific lane relevant for phase (i.e., ∈ L ). The weights depend on a function ( ) of the current states ( ) of these lanes, and the average of the function across all relevant lanes, . Intuitively, this attention allows to focus the mechanism on lanes depending on their closeness to the "average" (relevant) lane.</p>
<p>=
state-attention ( , ), ∀ ∈ P</p>
<p>and outgoing lanes and , where refers to the maximum lane capacity for lane . Note that this is an indication of the incoming and outgoing flow of traffic. Based on this paradigm, the RL agent learns a policy which suggests a phase for the next time-step for the current state . This policy is learned based on the algorithm illustrated above and uses the cumulative rewards for policy updates. This provides the means to map states to actions in the AttendLight algorithm. Results. The AttendLight algorithm was tested using three-way and four-way intersections with varying numbers of lanes, phases, and flow rates. The states represent chunks ∈ {1, 2, 3} of the road leading up to the intersection for lane and are 100 meter segments of a 300 meter length of the lane. Each lane has a corresponding number of vehicles , in a chunk at time . Further, lanes also may contain waiting vehicles and the quantity of waiting is represented by . Therefore, the traffic characteristic for a lane is defined as ≔ [ ,1 , ,2 , ,3 , ]. The proposed algorithm is tested against a number of reference baselines in the literature, showing a significant improvement in terms of lower Average Travel Time (ATT) Additional Relevant Results. Additional examples of interesting use cases for attention include</p>
<p>Based on the problem definition, the approach for finding an equilibrium often comes down to an optimizationproblem. Given a set of players N = {1, . . . , } where each player has their own strategy ∈ X ⊆ R , each player has an objective function ( , − ) and constraints ( , − ) ≤ 0, ( = 1, . . . , ), which depend on their strategy and the strategy of others − ≔ ( ′ ) ′ ≠ , an equilibrium in a Nash Game can be found as the solution to the optimization problem [57]: minimize ( , − ) ( − ) subject to ( , − ) ≤ 0, = 1, . . . ,</p>
<p>"
expresses the average number of [minority] members in nearby [areal units] as a proportion of the total population in those nearby [areal units]". To demonstrate the impact of differing measures and concepts of fairness, the authors focus on the case of linear regression. The model was trained by minimizing the mean squared error while imposing welfare constraints defined in [41]. The results demonstrate the impact of the fairness constraints on the three segregation measures defined above when females are the minority group considered. The authors note that the expectation would be such that the constraints would result in reduced segregation in the long run, but the results demonstrate deviations. Let denotes a minimum threshold on U (i.e. U ≥ ) with respect to learning with the loss function . The results show that in the case of small values, enforcing fairness constraints can generate a reduction in the degree of clustering. Conversely, larger values of can generate the opposite effect. Regarding evenness, this measure is relatively unchanged for all tested values of . The authors note that these results indicate: 1) a label of desirable for some members of the disadvantaged group motivates those members to remain unchanged; 2) the positively labeled members can serve as role models for other group members and motivate positive changes in others.</p>
<p>This work was supported by the H2020 Humane-AI-Net project (grant #952026) and by the CHIST-ERA grant CHIST-ERA-19-XAI-010, by MUR (grant No. not yet available), FWF (grant No. I 5205), EPSRC (grant No. EP/V055712/1), NCN (grant No. 2020/02/Y/ST6/00064), ETAg (grant No. SLTAT21096), BNSF (grant No. KP-06-DOO2/5).
Manuscript submitted to ACM</p>
<p>Modeling human decision-making: An overview of the Brussels quantum approach. Diederik Aerts, Massimiliano Sassoli De, Sandro Bianchi, Tomas Sozzo, Veloz, Foundations of Science. 26Diederik Aerts, Massimiliano Sassoli De Bianchi, Sandro Sozzo, and Tomas Veloz. 2021. Modeling human decision-making: An overview of the Brussels quantum approach. Foundations of Science 26, 1 (2021), 27-54.</p>
<p>The adaptive character of thought. John R Anderson, Psychology PressJohn R Anderson. 2013. The adaptive character of thought. Psychology Press.</p>
<p>Working memory: Activation limitations on retrieval. Lynne M John R Anderson, Christian Reder, Lebiere, Cognitive psychology. 30John R Anderson, Lynne M Reder, and Christian Lebiere. 1996. Working memory: Activation limitations on retrieval. Cognitive psychology 30, 3 (1996), 221-256.</p>
<p>Online social networks and information diffusion: The role of ego networks. Marco Valerio Arnaboldi, Andrea Conti, Robin Im Passarella, Dunbar, Online Social Networks and Media. 1Valerio Arnaboldi, Marco Conti, Andrea Passarella, and Robin IM Dunbar. 2017. Online social networks and information diffusion: The role of ego networks. Online Social Networks and Media 1 (2017), 44-55.</p>
<p>Brain-Inspired Model for Decision-Making in the Selection of Beneficial Information Among Signals Received by an Unpredictable Information-Development Environment. Alireza Asgari, Yvan Beauregard, Alireza Asgari and Yvan Beauregard. 2021. Brain-Inspired Model for Decision-Making in the Selection of Beneficial Information Among Signals Received by an Unpredictable Information-Development Environment. angrxiv (2021). https://engrxiv.org/preprint/view/1525</p>
<p>The behavioral model and game theory. Gholamreza Askari, Madjid Eshaghi Gordji, Choonkil Park, Palgrave Communications. 5Gholamreza Askari, Madjid Eshaghi Gordji, and Choonkil Park. 2019. The behavioral model and game theory. Palgrave Communications 5, 1 (2019), 1-8.</p>
<p>Designing fair ranking schemes. Abolfazl Asudeh, Julia Jagadish, Gautam Stoyanovich, Das, Proceedings of the 2019 International Conference on Management of Data. the 2019 International Conference on Management of DataAbolfazl Asudeh, HV Jagadish, Julia Stoyanovich, and Gautam Das. 2019. Designing fair ranking schemes. In Proceedings of the 2019 International Conference on Management of Data. 1259-1276.</p>
<p>Scalable fair clustering. Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, Tal Wagner, PMLRInternational Conference on Machine Learning. Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, and Tal Wagner. 2019. Scalable fair clustering. In International Conference on Machine Learning. PMLR, 405-413.</p>
<p>Grady Booch, Francesco Fabiano, Lior Horesh, Kiran Kate, Jon Lenchner, Nick Linck, Andrea Loreggia, Keerthiram Murugesan, Nicholas Mattei, Francesca Rossi, arXiv:2010.06002Thinking fast and slow in ai. arXiv preprintGrady Booch, Francesco Fabiano, Lior Horesh, Kiran Kate, Jon Lenchner, Nick Linck, Andrea Loreggia, Keerthiram Murugesan, Nicholas Mattei, Francesca Rossi, et al. 2020. Thinking fast and slow in ai. arXiv preprint arXiv:2010.06002 (2020).</p>
<p>Quantum dynamics of human decision-making. Zheng Jerome R Busemeyer, James T Wang, Townsend, Journal of Mathematical Psychology. 503Jerome R Busemeyer, Zheng Wang, and James T Townsend. 2006. Quantum dynamics of human decision-making. Journal of Mathematical Psychology 50, 3 (2006), 220-241.</p>
<p>Design and performance evaluation of data dissemination systems for opportunistic networks based on cognitive heuristics. Marco Conti, Matteo Mordacchini, Andrea Passarella, ACM Transactions on Autonomous and Adaptive Systems (TAAS). 8Marco Conti, Matteo Mordacchini, and Andrea Passarella. 2013. Design and performance evaluation of data dissemination systems for oppor- tunistic networks based on cognitive heuristics. ACM Transactions on Autonomous and Adaptive Systems (TAAS) 8, 3 (2013), 1-32.</p>
<p>The Internet of People (IoP): A new wave in pervasive mobile computing. Marco Conti, Andrea Passarella, Sajal K Das, 10.1016/j.pmcj.2017.07.009Pervasive and Mobile Computing. 41Marco Conti, Andrea Passarella, and Sajal K. Das. 2017. The Internet of People (IoP): A new wave in pervasive mobile computing. Pervasive and Mobile Computing 41 (2017), 1-27. https://doi.org/10.1016/j.pmcj.2017.07.009</p>
<p>Algorithmic decision making and the cost of fairness. Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, Aziz Huq, Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining. the 23rd acm sigkdd international conference on knowledge discovery and data miningSam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017. Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining. 797-806.</p>
<p>Alana De, Santana Correia, Esther Luna Colombini, arXiv:2103.16775Attention, please! A survey of Neural Attention Models in Deep Learning. arXiv preprintAlana de Santana Correia and Esther Luna Colombini. 2021. Attention, please! A survey of Neural Attention Models in Deep Learning. arXiv preprint arXiv:2103.16775 (2021).</p>
<p>Adaptive cyber deception: Cognitively informed signaling for cyber defense. Edward Cranford, Cleotilde Gonzalez, Palvi Aggarwal, Sarah Cooney, Milind Tambe, Christian Lebiere, Proceedings of the 53rd Hawaii International Conference on System Sciences. the 53rd Hawaii International Conference on System SciencesEdward Cranford, Cleotilde Gonzalez, Palvi Aggarwal, Sarah Cooney, Milind Tambe, and Christian Lebiere. 2020. Adaptive cyber deception: Cognitively informed signaling for cyber defense. In Proceedings of the 53rd Hawaii International Conference on System Sciences.</p>
<p>Toward Personalized Deceptive Signaling for Cyber Defense Using Cognitive Models. A Edward, Cleotilde Cranford, Palvi Gonzalez, Sarah Aggarwal, Milind Cooney, Christian Tambe, Lebiere, Topics in Cognitive Science. 12Edward A Cranford, Cleotilde Gonzalez, Palvi Aggarwal, Sarah Cooney, Milind Tambe, and Christian Lebiere. 2020. Toward Personalized Deceptive Signaling for Cyber Defense Using Cognitive Models. Topics in Cognitive Science 12, 3 (2020), 992-1011.</p>
<p>Cognitive salience of features in cyber-attacker decision making. A Edward, Sterling Cranford, Konstantinos Somers, Christian Mitsopoulos, Lebiere, Proceedings of the 18th annual meeting of the international conference on cognitive modeling. the 18th annual meeting of the international conference on cognitive modelingUniversity Park, PA: Applied Cognitive Science Lab, Penn StateEdward A Cranford, Sterling Somers, Konstantinos Mitsopoulos, and Christian Lebiere. 2020. Cognitive salience of features in cyber-attacker decision making. In Proceedings of the 18th annual meeting of the international conference on cognitive modeling. University Park, PA: Applied Cognitive Science Lab, Penn State.</p>
<p>Algorithmic approaches to ecological rationality in humans and machines. Ishita Dasgupta, Ph. D. Dissertation. Harvard UniversityIshita Dasgupta. 2020. Algorithmic approaches to ecological rationality in humans and machines. Ph. D. Dissertation. Harvard University.</p>
<p>A theory of learning to infer. Ishita Dasgupta, Eric Schulz, Joshua B Tenenbaum, Samuel J Gershman, Psychological review. 127412Ishita Dasgupta, Eric Schulz, Joshua B Tenenbaum, and Samuel J Gershman. 2020. A theory of learning to infer. Psychological review 127, 3 (2020), 412.</p>
<p>On the irrationality of being in two minds. Shahram Dehdashti, Lauren Fell, Peter Bruza, Entropy. 22174Shahram Dehdashti, Lauren Fell, and Peter Bruza. 2020. On the irrationality of being in two minds. Entropy 22, 2 (2020), 174.</p>
<p>How to model the neurocognitive dynamics of decision making: A methodological primer with ACT-R. Behavior research methods. Cvetomir Dimov, H Patrick, Julian N Khader, Thorsten Marewski, Pachur, 52Cvetomir Dimov, Patrick H Khader, Julian N Marewski, and Thorsten Pachur. 2020. How to model the neurocognitive dynamics of decision making: A methodological primer with ACT-R. Behavior research methods 52, 2 (2020), 857-880.</p>
<p>Integrating models of human behaviour between the individual and population levels to inform conservation interventions. D M Andrew, Emiel Dobson, Aidan De Lange, Harriet Keane, E J Ibbett, Milner-Gulland, Philosophical Transactions of the Royal Society B. 3741781Andrew DM Dobson, Emiel De Lange, Aidan Keane, Harriet Ibbett, and EJ Milner-Gulland. 2019. Integrating models of human behaviour between the individual and population levels to inform conservation interventions. Philosophical Transactions of the Royal Society B 374, 1781 (2019), 20180053.</p>
<p>Modeling human behavior in economics and social science. M Dolfin, N Leonida, Outada, Physics of life reviews. 22M Dolfin, L Leonida, and N Outada. 2017. Modeling human behavior in economics and social science. Physics of life reviews 22 (2017), 1-21.</p>
<p>Relational instance based regression for relational reinforcement learning. Kurt Driessens, Jan Ramon, Proceedings of the 20th International Conference on Machine Learning (ICML-03. the 20th International Conference on Machine Learning (ICML-03Kurt Driessens and Jan Ramon. 2003. Relational instance based regression for relational reinforcement learning. In Proceedings of the 20th Inter- national Conference on Machine Learning (ICML-03). 123-130.</p>
<p>How to support users in understanding intelligent systems? Structuring the discussion. Malin Eiband, Daniel Buschek, Heinrich Hussmann, 26th International Conference on Intelligent User Interfaces. Malin Eiband, Daniel Buschek, and Heinrich Hussmann. 2021. How to support users in understanding intelligent systems? Structuring the discussion. In 26th International Conference on Intelligent User Interfaces. 120-132.</p>
<p>A competition of critics in human decision-making. Enkhzaya Enkhtaivan, Joel Nishimura, Cheng Ly, Amy L Cochran, Computational Psychiatry. 51Enkhzaya Enkhtaivan, Joel Nishimura, Cheng Ly, and Amy L Cochran. 2021. A competition of critics in human decision-making. Computational Psychiatry 5, 1 (2021).</p>
<p>. P A Facione, C A Gittens, P.A. Facione and C.A. Gittens. 2012. Think Critically. Pearson. https://books.google.it/books?id=YGM5ygAACAAJ</p>
<p>Deep inverse reinforcement learning for behavior prediction in autonomous driving: Accurate forecasts of vehicle motion. Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes, IEEE Signal Processing Magazine. 38Tharindu Fernando, Simon Denman, Sridha Sridharan, and Clinton Fookes. 2020. Deep inverse reinforcement learning for behavior prediction in autonomous driving: Accurate forecasts of vehicle motion. IEEE Signal Processing Magazine 38, 1 (2020), 87-96.</p>
<p>Self-attentional credit assignment for transfer in reinforcement learning. Johan Ferret, Raphaël Marinier, Matthieu Geist, Olivier Pietquin, arXiv:1907.08027arXiv preprintJohan Ferret, Raphaël Marinier, Matthieu Geist, and Olivier Pietquin. 2019. Self-attentional credit assignment for transfer in reinforcement learning. arXiv preprint arXiv:1907.08027 (2019).</p>
<p>Andrew Fuchs, Andrea Passarella, Marco Conti, arXiv:XXXModeling Human Behavior Part I -Cognitive approaches and Uncertainty. arXiv preprintAndrew Fuchs, Andrea Passarella, and Marco Conti. 2022. Modeling Human Behavior Part I -Cognitive approaches and Uncertainty. arXiv preprint arXiv:XXX (2022).</p>
<p>Moral intuition= fast and frugal heuristics? In Moral psychology. Gerd Gigerenzer, MIT PressGerd Gigerenzer. 2008. Moral intuition= fast and frugal heuristics? In Moral psychology. MIT Press, 1-26.</p>
<p>Instance-based learning in dynamic decision making. Cleotilde Gonzalez, Javier F Lerch, Christian Lebiere, Cognitive Science. 27Cleotilde Gonzalez, Javier F Lerch, and Christian Lebiere. 2003. Instance-based learning in dynamic decision making. Cognitive Science 27, 4 (2003), 591-635.</p>
<p>Inductive biases for deep learning of higher-level cognition. Anirudh Goyal, Yoshua Bengio, arXiv:2011.15091arXiv preprintAnirudh Goyal and Yoshua Bengio. 2020. Inductive biases for deep learning of higher-level cognition. arXiv preprint arXiv:2011.15091 (2020).</p>
<p>Attributing awareness to others: the attention schema theory and its relationship to behavioural prediction. S A Michael, Graziano, Journal of Consciousness Studies. 26Michael SA Graziano. 2019. Attributing awareness to others: the attention schema theory and its relationship to behavioural prediction. Journal of Consciousness Studies 26, 3-4 (2019), 17-37.</p>
<p>Human perceptions of fairness in algorithmic decision making: A case study of criminal risk prediction. Nina Grgic-Hlaca, Elissa M Redmiles, Krishna P Gummadi, Adrian Weller, Proceedings of the 2018 World Wide Web Conference. the 2018 World Wide Web ConferenceNina Grgic-Hlaca, Elissa M Redmiles, Krishna P Gummadi, and Adrian Weller. 2018. Human perceptions of fairness in algorithmic decision making: A case study of criminal risk prediction. In Proceedings of the 2018 World Wide Web Conference. 903-912.</p>
<p>Structure and strength in causal induction. L Thomas, Joshua B Griffiths, Tenenbaum, Cognitive psychology. 51Thomas L Griffiths and Joshua B Tenenbaum. 2005. Structure and strength in causal induction. Cognitive psychology 51, 4 (2005), 334-384.</p>
<p>Theoretical foundations of human decision-making in agent-based land use models-A review. Jürgen Groeneveld, Birgit Müller, M Carsten, Gunnar Buchmann, Cheng Dressler, Niklas Guo, Falk Hase, Hoffmann, Christian John, Klassert, Lauf, Environmental modelling &amp; software. 87Jürgen Groeneveld, Birgit Müller, Carsten M Buchmann, Gunnar Dressler, Cheng Guo, Niklas Hase, Falk Hoffmann, F John, Christian Klassert, T Lauf, et al. 2017. Theoretical foundations of human decision-making in agent-based land use models-A review. Environmental modelling &amp; software 87 (2017), 39-48.</p>
<p>A Survey of Methods for Explaining Black Box Models. Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, Dino Pedreschi, 10.1145/3236009ACM Comput. Surv. 515Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. A Survey of Methods for Explaining Black Box Models. ACM Comput. Surv. 51, 5 (2018). https://doi.org/10.1145/3236009</p>
<p>Mapping human-computer interaction research themes and trends from its existence to today: A topic modeling-based review of past 60 years. Fatih Gurcan, Nergiz Ercil Cagiltay, Kursat Cagiltay, International Journal of Human-Computer Interaction. 37Fatih Gurcan, Nergiz Ercil Cagiltay, and Kursat Cagiltay. 2021. Mapping human-computer interaction research themes and trends from its existence to today: A topic modeling-based review of past 60 years. International Journal of Human-Computer Interaction 37, 3 (2021), 267-280.</p>
<p>. Fuchs, Fuchs, et al.</p>
<p>An evidential dynamical model to predict the interference effect of categorization on decision making results. Knowledge-Based Systems. Zichang He, Wen Jiang, 150Zichang He and Wen Jiang. 2018. An evidential dynamical model to predict the interference effect of categorization on decision making results. Knowledge-Based Systems 150 (2018), 139-149.</p>
<p>Fairness behind a veil of ignorance: A welfare analysis for automated decision making. Hoda Heidari, Claudio Ferrari, Krishna Gummadi, Andreas Krause, Advances in Neural Information Processing Systems. 31Hoda Heidari, Claudio Ferrari, Krishna Gummadi, and Andreas Krause. 2018. Fairness behind a veil of ignorance: A welfare analysis for automated decision making. Advances in Neural Information Processing Systems 31 (2018).</p>
<p>On the Long-term Impact of Algorithmic Decision Policies: Effort Unfairness and Feature Segregation through Social Learning. Hoda Heidari, Vedant Nanda, Krishna Gummadi, PMLRProceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research. Kamalika Chaudhuri and Ruslan Salakhutdinovthe 36th International Conference on Machine Learning ( Machine Learning Research97Hoda Heidari, Vedant Nanda, and Krishna Gummadi. 2019. On the Long-term Impact of Algorithmic Decision Policies: Effort Unfairness and Fea- ture Segregation through Social Learning. In Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 97), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 2692-2701. https://proceedings.mlr.press/v97/heidari19a.html</p>
<p>Interactive machine learning: experimental evidence for the human in the algorithmic loop. Andreas Holzinger, Markus Plass, Michael Kickmeier-Rust, Katharina Holzinger, Gloria Cerasela Crişan, Camelia-M Pintea, Vasile Palade, Applied Intelligence. 49Andreas Holzinger, Markus Plass, Michael Kickmeier-Rust, Katharina Holzinger, Gloria Cerasela Crişan, Camelia-M Pintea, and Vasile Palade. 2019. Interactive machine learning: experimental evidence for the human in the algorithmic loop. Applied Intelligence 49, 7 (2019), 2401-2414.</p>
<p>Uncertainty measurement with belief entropy on the interference effect in the quantum-like Bayesian Networks. Zhiming Huang, Lin Yang, Wen Jiang, Appl. Math. Comput. 347Zhiming Huang, Lin Yang, and Wen Jiang. 2019. Uncertainty measurement with belief entropy on the interference effect in the quantum-like Bayesian Networks. Appl. Math. Comput. 347 (2019), 417-428.</p>
<p>Agent-based modeling: A guide for social psychologists. Joshua Conrad Jackson, David Rand, Kevin Lewis, Kurt Michael I Norton, Gray, Social Psychological and Personality Science. 8Joshua Conrad Jackson, David Rand, Kevin Lewis, Michael I Norton, and Kurt Gray. 2017. Agent-based modeling: A guide for social psychologists. Social Psychological and Personality Science 8, 4 (2017), 387-395.</p>
<p>The Impact of Values as Heuristics on Social Cognition. Lauren James , Lauren James. 2017. The Impact of Values as Heuristics on Social Cognition. (2017).</p>
<p>The Cerebral Cortex Realizes a Universal Probabilistic Model of Computation in Complex Hilbert Spaces. C Jones, C Jones. 2020. The Cerebral Cortex Realizes a Universal Probabilistic Model of Computation in Complex Hilbert Spaces. (2020).</p>
<p>Belief in the law of small numbers. Daniel Kahneman, Psychological bulletin. 76Daniel Kahneman et al. 1971. Belief in the law of small numbers. Psychological bulletin 76, 2 (1971), 105-110.</p>
<p>A Review of Recent Deep Learning Approaches in Human-Centered Machine Learning. Tharindu Kaluarachchi, Andrew Reis, Suranga Nanayakkara, Sensors. 212514Tharindu Kaluarachchi, Andrew Reis, and Suranga Nanayakkara. 2021. A Review of Recent Deep Learning Approaches in Human-Centered Machine Learning. Sensors 21, 7 (2021), 2514.</p>
<p>Downstream effects of affirmative action. Sampath Kannan, Aaron Roth, Juba Ziani, Proceedings of the Conference on Fairness, Accountability, and Transparency. the Conference on Fairness, Accountability, and TransparencySampath Kannan, Aaron Roth, and Juba Ziani. 2019. Downstream effects of affirmative action. In Proceedings of the Conference on Fairness, Accountability, and Transparency. 240-248.</p>
<p>Reasoning with heuristics. Brett Karlan, Ratio. 34Brett Karlan. 2021. Reasoning with heuristics. Ratio 34, 2 (2021), 100-108.</p>
<p>Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, C Michael, Chris Mozer, Yoshua Pal, Bengio, arXiv:1809.03702Sparse attentive backtracking: Temporal creditassignment through reminding. arXiv preprintNan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Michael C Mozer, Chris Pal, and Yoshua Bengio. 2018. Sparse attentive backtracking: Temporal creditassignment through reminding. arXiv preprint arXiv:1809.03702 (2018).</p>
<p>High-Dimensional Vector Spaces as the Architecture of Cognition. A Matthew, Nipun Kelly, Arora, L Robert, David West, Reitter, CogSci. 3491Matthew A Kelly, Nipun Arora, Robert L West, and David Reitter. 2019. High-Dimensional Vector Spaces as the Architecture of Cognition.. In CogSci. 3491.</p>
<p>Modelling human behaviour in agent-based models. In Agent-based models of geographical systems. G William, Kennedy, SpringerWilliam G Kennedy. 2012. Modelling human behaviour in agent-based models. In Agent-based models of geographical systems. Springer, 167-179.</p>
<p>Untangling tradeoffs between recurrence and self-attention in artificial neural networks. Giancarlo Kerg, Bhargav Kanuparthi, Anirudh Goyal, Alias Parth, Kyle Goyal, Yoshua Goyette, Guillaume Bengio, Lajoie, Advances in Neural Information Processing Systems. 33Giancarlo Kerg, Bhargav Kanuparthi, Anirudh Goyal ALIAS PARTH GOYAL, Kyle Goyette, Yoshua Bengio, and Guillaume Lajoie. 2020. Untangling tradeoffs between recurrence and self-attention in artificial neural networks. Advances in Neural Information Processing Systems 33 (2020).</p>
<p>. Salman Akif Quddus Khan, Utkurbek Khan, Safaev, 2020. Serious Games and Gamification: A Systematic Literature Review. Akif Quddus Khan, Salman Khan, and Utkurbek Safaev. 2020. Serious Games and Gamification: A Systematic Literature Review. (2020).</p>
<p>Jong Gwang Kim, arXiv:2106.00109Equilibrium Computation of Generalized Nash Games: A New Lagrangian-Based Approach. arXiv preprintJong Gwang Kim. 2021. Equilibrium Computation of Generalized Nash Games: A New Lagrangian-Based Approach. arXiv preprint arXiv:2106.00109 (2021).</p>
<p>ACT-R model for cognitive assistance in handling flight deck alerts. O Klaproth, Marc Halbrügge, Nele Russwinkel, Proceedings of the 17th International Conference on Cognitive Modeling. the 17th International Conference on Cognitive ModelingO Klaproth, Marc Halbrügge, and Nele Russwinkel. 2019. ACT-R model for cognitive assistance in handling flight deck alerts. In Proceedings of the 17th International Conference on Cognitive Modeling, Montreal. 83-88.</p>
<p>A Neuroadaptive Cognitive Model for Dealing With Uncertainty in Tracing Pilots' Cognitive State. W Oliver, Marc Klaproth, Laurens R Halbrügge, Christoph Krol, Thorsten O Vernaleken, Nele Zander, Russwinkel, Topics in Cognitive Science. 12Oliver W Klaproth, Marc Halbrügge, Laurens R Krol, Christoph Vernaleken, Thorsten O Zander, and Nele Russwinkel. 2020. A Neuroadaptive Cognitive Model for Dealing With Uncertainty in Tracing Pilots' Cognitive State. Topics in Cognitive Science 12, 3 (2020), 1012-1029.</p>
<p>Biswajeet Pradhan, and Ketan Kotecha. 2021. Behavior Prediction of Traffic Actors for Intelligent Vehicle using Artificial Intelligence Techniques: A Review. Suresh Kolekar, Shilpa Gite, IEEE Access. Suresh Kolekar, Shilpa Gite, Biswajeet Pradhan, and Ketan Kotecha. 2021. Behavior Prediction of Traffic Actors for Intelligent Vehicle using Artificial Intelligence Techniques: A Review. IEEE Access (2021).</p>
<p>40 years of cognitive architectures: core cognitive abilities and practical applications. Iuliia Kotseruba, John K Tsotsos, Artificial Intelligence Review. 53Iuliia Kotseruba and John K Tsotsos. 2020. 40 years of cognitive architectures: core cognitive abilities and practical applications. Artificial Intelligence Review 53, 1 (2020), 17-94.</p>
<p>The selective labels problem: Evaluating algorithmic predictions in the presence of unobservables. Himabindu Lakkaraju, Jon Kleinberg, Jure Leskovec, Jens Ludwig, Sendhil Mullainathan, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningHimabindu Lakkaraju, Jon Kleinberg, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. 2017. The selective labels problem: Evaluating algorithmic predictions in the presence of unobservables. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 275-284.</p>
<p>Learning to solve the credit assignment problem. Prashanth Ravi Benjamin James Lansdell, Konrad Paul Prakash, Kording, arXiv:1906.00889arXiv preprintBenjamin James Lansdell, Prashanth Ravi Prakash, and Konrad Paul Kording. 2019. Learning to solve the credit assignment problem. arXiv preprint arXiv:1906.00889 (2019).</p>
<p>Advantages and disadvantages of cognitive heuristics in political decision making. R Richard, David P Lau, Redlawsk, American Journal of Political Science. Richard R Lau and David P Redlawsk. 2001. Advantages and disadvantages of cognitive heuristics in political decision making. American Journal of Political Science (2001), 951-971.</p>
<p>The dynamics of cognition: An ACT-R model of cognitive arithmetic. Christian Lebiere, Kognitionswissenschaft. 8Christian Lebiere. 1999. The dynamics of cognition: An ACT-R model of cognitive arithmetic. Kognitionswissenschaft 8, 1 (1999), 5-19.</p>
<p>The representation of judgment heuristics and the generality problem. J Carole, Lee, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society29Carole J Lee. 2007. The representation of judgment heuristics and the generality problem. In Proceedings of the Annual Meeting of the Cognitive Science Society, Vol. 29.</p>
<p>Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management. Min Kyung Lee, Big Data &amp; Society. 52053951718756684Min Kyung Lee. 2018. Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management. Big Data &amp; Society 5, 1 (2018), 2053951718756684.</p>
<p>Racial mirroring effects on human-agent interaction in psychotherapeutic conversations. Yuting Liao, Jiangen He, Proceedings of the 25th International Conference on Intelligent User Interfaces. the 25th International Conference on Intelligent User InterfacesYuting Liao and Jiangen He. 2020. Racial mirroring effects on human-agent interaction in psychotherapeutic conversations. In Proceedings of the 25th International Conference on Intelligent User Interfaces. 430-442.</p>
<p>Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources. Falk Lieder, L Thomas, Griffiths, Behavioral and Brain Sciences. 43Falk Lieder and Thomas L Griffiths. 2020. Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources. Behavioral and Brain Sciences 43 (2020).</p>
<p>Overrepresentation of extreme events in decision making reflects rational use of cognitive resources. Falk Lieder, L Thomas, Ming Griffiths, Hsu, Psychological review. 1251Falk Lieder, Thomas L Griffiths, and Ming Hsu. 2018. Overrepresentation of extreme events in decision making reflects rational use of cognitive resources. Psychological review 125, 1 (2018), 1.</p>
<p>An automatic method for discovering rational heuristics for risky choice. Falk Lieder, M Paul, Tom Krueger, Griffiths, CogSci. Falk Lieder, Paul M Krueger, and Tom Griffiths. 2017. An automatic method for discovering rational heuristics for risky choice.. In CogSci.</p>
<p>The role of cognitive architectures in general artificial intelligence. Antonio Lieto, Mehul Bhatt, Alessandro Oltramari, David Vernon, 3 pagesAntonio Lieto, Mehul Bhatt, Alessandro Oltramari, and David Vernon. 2018. The role of cognitive architectures in general artificial intelligence. , 3 pages.</p>
<p>Self-improving generative adversarial reinforcement learning. Yang Liu, Yifeng Zeng, Yingke Chen, Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems. the 18th International Conference on Autonomous Agents and MultiAgent SystemsJing Tang, and Yinghui PanYang Liu, Yifeng Zeng, Yingke Chen, Jing Tang, and Yinghui Pan. 2019. Self-improving generative adversarial reinforcement learning. In Proceed- ings of the 18th International Conference on Autonomous Agents and MultiAgent Systems. 52-60.</p>
<p>Take-over expectation and criticality in Level 3 automated driving: a test track study on take-over behavior in semi-trucks. Alexander Lotz, Nele Russwinkel, Enrico Wohlfarth, Cognition, Technology &amp; Work. 22Alexander Lotz, Nele Russwinkel, and Enrico Wohlfarth. 2020. Take-over expectation and criticality in Level 3 automated driving: a test track study on take-over behavior in semi-trucks. Cognition, Technology &amp; Work 22, 4 (2020), 733-744.</p>
<p>Can resources save rationality? 'Anti-Bayesian' updating in cognition and perception. Eric Mandelbaum, Isabel Won, Steven Gross, Chaz Firestone, Behavioral and Brain Sciences. 143Eric Mandelbaum, Isabel Won, Steven Gross, and Chaz Firestone. 2020. Can resources save rationality? 'Anti-Bayesian' updating in cognition and perception. Behavioral and Brain Sciences 143 (2020).</p>
<p>Advances in Bayesian network modelling: Integration of modelling technologies. Environmental modelling &amp; software. G Bruce, Marcot, Trent D Penman, 111Bruce G Marcot and Trent D Penman. 2019. Advances in Bayesian network modelling: Integration of modelling technologies. Environmental modelling &amp; software 111 (2019), 386-393.</p>
<p>A rational reinterpretation of dual-process theories. Smitha Milli, Falk Lieder, Thomas L Griffiths, Cognition. 217104881Smitha Milli, Falk Lieder, and Thomas L Griffiths. 2021. A rational reinterpretation of dual-process theories. Cognition 217 (2021), 104881.</p>
<p>Human-centric data dissemination in the IoP: Large-scale modeling and evaluation. Matteo Mordacchini, Marco Conti, Andrea Passarella, Raffaele Bruno, ACM Transactions on Autonomous and Adaptive Systems (TAAS). 14Matteo Mordacchini, Marco Conti, Andrea Passarella, and Raffaele Bruno. 2020. Human-centric data dissemination in the IoP: Large-scale mod- eling and evaluation. ACM Transactions on Autonomous and Adaptive Systems (TAAS) 14, 3 (2020), 1-25.</p>
<p>A social cognitive heuristic for adaptive data dissemination in mobile Opportunistic Networks. Matteo Mordacchini, Andrea Passarella, Marco Conti, Pervasive and Mobile Computing. 42Matteo Mordacchini, Andrea Passarella, and Marco Conti. 2017. A social cognitive heuristic for adaptive data dissemination in mobile Oppor- tunistic Networks. Pervasive and Mobile Computing 42 (2017), 371-392.</p>
<p>Design and evaluation of a cognitive approach for disseminating semantic knowledge and content in opportunistic networks. Matteo Mordacchini, Lorenzo Valerio, Marco Conti, Andrea Passarella, Computer Communications. 81Matteo Mordacchini, Lorenzo Valerio, Marco Conti, and Andrea Passarella. 2016. Design and evaluation of a cognitive approach for disseminating semantic knowledge and content in opportunistic networks. Computer Communications 81 (2016), 12-30.</p>
<p>Towards a quantum-like cognitive architecture for decision-making. Catarina Moreira, Lauren Fell, Shahram Dehdashti, Peter Bruza, Andreas Wichert, arXiv:1905.05176arXiv preprintCatarina Moreira, Lauren Fell, Shahram Dehdashti, Peter Bruza, and Andreas Wichert. 2019. Towards a quantum-like cognitive architecture for decision-making. arXiv preprint arXiv:1905.05176 (2019).</p>
<p>Quantum-like influence diagrams for decisionmaking. Catarina Moreira, Prayag Tiwari, Hari Mohan Pandey, Peter Bruza, Andreas Wichert, Neural Networks. 132Catarina Moreira, Prayag Tiwari, Hari Mohan Pandey, Peter Bruza, and Andreas Wichert. 2020. Quantum-like influence diagrams for decision- making. Neural Networks 132 (2020), 190-210.</p>
<p>Are quantum-like Bayesian networks more powerful than classical Bayesian networks. Catarina Moreira, Andreas Wichert, Journal of Mathematical Psychology. 82Catarina Moreira and Andreas Wichert. 2018. Are quantum-like Bayesian networks more powerful than classical Bayesian networks? Journal of Mathematical Psychology 82 (2018), 73-83.</p>
<p>Cognitive Modeling of Automation Adaptation in a Time Critical Task. Junya Morita, Kazuhisa Miwa, Akihiro Maehigashi, Hitoshi Terai, Kazuaki Kojima, Frank E Ritter, Frontiers in Psychology. 11Junya Morita, Kazuhisa Miwa, Akihiro Maehigashi, Hitoshi Terai, Kazuaki Kojima, and Frank E Ritter. 2020. Cognitive Modeling of Automation Adaptation in a Time Critical Task. Frontiers in Psychology 11 (2020).</p>
<p>Rita Faia Marques, and Abigail Sellen. 2021. Social Sensemaking with AI: Designing an Open-ended AI experience with a Blind Child. Cecily Morrison, Edward Cutrell, Martin Grayson, Anja Thieme, Alex Taylor, Geert Roumen, Camilla Longden, Sebastian Tschiatschek, Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. the 2021 CHI Conference on Human Factors in Computing SystemsCecily Morrison, Edward Cutrell, Martin Grayson, Anja Thieme, Alex Taylor, Geert Roumen, Camilla Longden, Sebastian Tschiatschek, Rita Faia Marques, and Abigail Sellen. 2021. Social Sensemaking with AI: Designing an Open-ended AI experience with a Blind Child. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1-14.</p>
<p>Towards representing human behavior and decision making in Earth system models-an overview of techniques and approaches. Finn Müller-Hansen, Maja Schlüter, Michael Mäs, Jonathan F Donges, Jakob J Kolb, Kirsten Thonicke, Jobst Heitzig, Earth System Dynamics. 8Finn Müller-Hansen, Maja Schlüter, Michael Mäs, Jonathan F Donges, Jakob J Kolb, Kirsten Thonicke, and Jobst Heitzig. 2017. Towards represent- ing human behavior and decision making in Earth system models-an overview of techniques and approaches. Earth System Dynamics 8, 4 (2017), 977-1007.</p>
<p>Quantum Probabilistic Models Using Feynman Diagram Rules for Better Understanding the Information Diffusion Dynamics in Online Social Networks. C Ece, Mutlu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Ece C Mutlu. 2020. Quantum Probabilistic Models Using Feynman Diagram Rules for Better Understanding the Information Diffusion Dynamics in Online Social Networks. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 13730-13731.</p>
<p>Reinforcement learning with human advice: a survey. Anis Najar, Mohamed Chetouani, Frontiers in Robotics and AI. 8Anis Najar and Mohamed Chetouani. 2021. Reinforcement learning with human advice: a survey. Frontiers in Robotics and AI 8 (2021).</p>
<p>New approach in human-AI interaction by reinforcement-imitation learning. Neda Navidi, Rene Landry, Applied Sciences. 1173068Neda Navidi and Rene Landry. 2021. New approach in human-AI interaction by reinforcement-imitation learning. Applied Sciences 11, 7 (2021), 3068.</p>
<p>Cognitive machine theory of mind. N Thuy, Cleotilde Nguyen, Gonzalez, Carnegie Mellon UniversityTechnical ReportThuy N Nguyen and Cleotilde Gonzalez. 2020. Cognitive machine theory of mind. Technical Report. Carnegie Mellon University.</p>
<p>Effects of Decision Complexity in Goal seeking Gridworlds: A Comparison of Instance Based Learning and Reinforcement Learning Agents. N Thuy, Cleotilde Nguyen, Gonzalez, Carnegie Mellon UniversityTechnical ReportThuy N Nguyen and Cleotilde Gonzalez. 2020. Effects of Decision Complexity in Goal seeking Gridworlds: A Comparison of Instance Based Learning and Reinforcement Learning Agents. Technical Report. Carnegie Mellon University.</p>
<p>A Survey on Hybrid Human-Artificial Intelligence for Autonomous Driving. Huansheng Ning, Rui Yin, Ata Ullah, Feifei Shi, IEEE Transactions on Intelligent Transportation Systems. Huansheng Ning, Rui Yin, Ata Ullah, and Feifei Shi. 2021. A Survey on Hybrid Human-Artificial Intelligence for Autonomous Driving. IEEE Transactions on Intelligent Transportation Systems (2021).</p>
<p>Learning task-state representations. Yael Niv, Nature neuroscience. 22Yael Niv. 2019. Learning task-state representations. Nature neuroscience 22, 10 (2019), 1544-1553.</p>
<p>The probabilistic approach to human reasoning. Mike Oaksford, Nick Chater, Trends in cognitive sciences. 5Mike Oaksford and Nick Chater. 2001. The probabilistic approach to human reasoning. Trends in cognitive sciences 5, 8 (2001), 349-357.</p>
<p>AttendLight: Universal Attention-Based Reinforcement Learning Model for Traffic Signal Control. Afshin Oroojlooy, Mohammadreza Nazari, Davood Hajinezhad, Jorge Silva, arXiv:2010.05772arXiv preprintAfshin Oroojlooy, Mohammadreza Nazari, Davood Hajinezhad, and Jorge Silva. 2020. AttendLight: Universal Attention-Based Reinforcement Learning Model for Traffic Signal Control. arXiv preprint arXiv:2010.05772 (2020).</p>
<p>Using a physics engine in ACT-R to aid decision making. D Pentecost, Charlotte Sennersten, C Ollington, B Lindley, Kang, International Journal on Advances in Intelligent Systems. 9D Pentecost, Charlotte Sennersten, R Ollington, C Lindley, and B Kang. 2016. Using a physics engine in ACT-R to aid decision making. International Journal on Advances in Intelligent Systems 9, 3-4 (2016), 298-309.</p>
<p>Man as an intuitive statistician. R Cameron, Lee Roy Peterson, Beach, Psychological bulletin. 6829Cameron R Peterson and Lee Roy Beach. 1967. Man as an intuitive statistician. Psychological bulletin 68, 1 (1967), 29.</p>
<p>A quantum probability explanation for violations of 'rational'decision theory. M Emmanuel, Jerome R Pothos, Busemeyer, Proceedings of the Royal Society B: Biological Sciences. 276Emmanuel M Pothos and Jerome R Busemeyer. 2009. A quantum probability explanation for violations of 'rational'decision theory. Proceedings of the Royal Society B: Biological Sciences 276, 1665 (2009), 2171-2178.</p>
<p>An implementation of Universal Spatial Transformative Cognition in ACT-R. Kai Preuss, Leonie Raddatz, Nele Russwinkel, Proceedings of the 17th International Conference on Cognitive Modelling, TC Stewart. the 17th International Conference on Cognitive Modelling, TC StewartWaterloo, CanadaUniversity of WaterlooKai Preuss, Leonie Raddatz, and Nele Russwinkel. 2019. An implementation of Universal Spatial Transformative Cognition in ACT-R. In Proceedings of the 17th International Conference on Cognitive Modelling, TC Stewart (Ed.). University of Waterloo, Waterloo, Canada. 144-150.</p>
<p>Watch-andhelp: A challenge for social perception and human-AI collaboration. Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua B Tenenbaum, Sanja Fidler, Antonio Torralba, arXiv:2010.09890arXiv preprintXavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua B Tenenbaum, Sanja Fidler, and Antonio Torralba. 2020. Watch-and- help: A challenge for social perception and human-AI collaboration. arXiv preprint arXiv:2010.09890 (2020).</p>
<p>Learning hierarchical relationships for object-goal navigation. Yiding Qiu, Anwesan Pal, Henrik I Christensen, arXiv:2003.06749arXiv preprintYiding Qiu, Anwesan Pal, and Henrik I Christensen. 2020. Learning hierarchical relationships for object-goal navigation. arXiv preprint arXiv:2003.06749 (2020).</p>
<p>The algorithmic automation problem: Prediction, triage, and human effort. Maithra Raghu, Katy Blumer, Greg Corrado, Jon Kleinberg, Ziad Obermeyer, Sendhil Mullainathan, arXiv:1903.12220arXiv preprintMaithra Raghu, Katy Blumer, Greg Corrado, Jon Kleinberg, Ziad Obermeyer, and Sendhil Mullainathan. 2019. The algorithmic automation problem: Prediction, triage, and human effort. arXiv preprint arXiv:1903.12220 (2019).</p>
<p>Preeti Ramaraj, Charles L OrtizJr, Matthew Klenk, Shiwali Mohan, arXiv:2102.06755Unpacking Human Teachers' Intentions For Natural Interactive Task Learning. arXiv preprintPreeti Ramaraj, Charles L Ortiz Jr, Matthew Klenk, and Shiwali Mohan. 2021. Unpacking Human Teachers' Intentions For Natural Interactive Task Learning. arXiv preprint arXiv:2102.06755 (2021).</p>
<p>Consistent algorithms for multiclass classification with an abstain option. G Harish, Ambuj Ramaswamy, Shivani Tewari, Agarwal, Electronic Journal of Statistics. 12Harish G Ramaswamy, Ambuj Tewari, and Shivani Agarwal. 2018. Consistent algorithms for multiclass classification with an abstain option. Electronic Journal of Statistics 12, 1 (2018), 530-554.</p>
<p>Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making. Charvi Rastogi, Yunfeng Zhang, Dennis Wei, R Kush, Amit Varshney, Richard Dhurandhar, Tomsett, arXiv:2010.07938arXiv preprintCharvi Rastogi, Yunfeng Zhang, Dennis Wei, Kush R Varshney, Amit Dhurandhar, and Richard Tomsett. 2020. Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making. arXiv preprint arXiv:2010.07938 (2020).</p>
<p>Siddharth Reddy, D Anca, Sergey Dragan, Levine, arXiv:1802.01744Shared autonomy via deep reinforcement learning. arXiv preprintSiddharth Reddy, Anca D Dragan, and Sergey Levine. 2018. Shared autonomy via deep reinforcement learning. arXiv preprint arXiv:1802.01744 (2018).</p>
<p>Modeling human decision making in generalized Gaussian multiarmed bandits. Vaibhav Paul B Reverdy, Naomi Ehrich Srivastava, Leonard, Proc. IEEE. 102Paul B Reverdy, Vaibhav Srivastava, and Naomi Ehrich Leonard. 2014. Modeling human decision making in generalized Gaussian multiarmed bandits. Proc. IEEE 102, 4 (2014), 544-571.</p>
<p>ACT-R: A cognitive architecture for modeling cognition. E Frank, Farnaz Ritter, Jacob D Tehranchi, Oury, Wiley Interdisciplinary Reviews: Cognitive Science. 101488Frank E Ritter, Farnaz Tehranchi, and Jacob D Oury. 2019. ACT-R: A cognitive architecture for modeling cognition. Wiley Interdisciplinary Reviews: Cognitive Science 10, 3 (2019), e1488.</p>
<p>Simulation models of human decision-making processes. Nina Rizun, Yurii Taranenko, Management Dynamics in the Knowledge Economy. 2Nina Rizun and Yurii Taranenko. 2014. Simulation models of human decision-making processes. Management Dynamics in the Knowledge Economy 2, 2 (2014), 241-264.</p>
<p>Artificial intelligence: a modern approach. Stuart Russell, Peter Norvig, Stuart Russell and Peter Norvig. 2002. Artificial intelligence: a modern approach. (2002).</p>
<p>ACT-Droid meets ACT-Touch: Modelling differences in swiping behavior with real Apps. Nele Russwinkel, Sabine Prezenski, Lisa Dörr, Frank Tamborello, Proceedings of the 16th International Conference on Cognitive Modeling. the 16th International Conference on Cognitive ModelingNele Russwinkel, Sabine Prezenski, Lisa Dörr, and Frank Tamborello. 2018. ACT-Droid meets ACT-Touch: Modelling differences in swiping behavior with real Apps. In Proceedings of the 16th International Conference on Cognitive Modeling (ICCM 2018). 21-24.</p>
<p>Bounded rationality and game theory. Larry Samuelson, Quarterly Review of Economics and finance. 36Larry Samuelson. 1995. Bounded rationality and game theory. Quarterly Review of Economics and finance 36 (1995), 17-36.</p>
<p>A Cognitive Model for Understanding the Takeover in Highly Automated Driving Depending on the Objective Complexity of Non-Driving Related Tasks and the Traffic Environment. Marlene Scharfe, Nele Russwinkel, Marlene Scharfe and Nele Russwinkel. 2019. A Cognitive Model for Understanding the Takeover in Highly Automated Driving Depending on the Objective Complexity of Non-Driving Related Tasks and the Traffic Environment.. In CogSci. 2734-2740.</p>
<p>Towards a cognitive model of the takeover in highly automated driving for the improvement of human machine interaction. Marlene Scharfe, Nele Russwinkel, TC Stewart (Chair), Proceedings of the 17th International Conference on cognitive modelling. Waterloo, CanadaUniversity of WaterlooMarlene Scharfe and Nele Russwinkel. 2019. Towards a cognitive model of the takeover in highly automated driving for the improvement of human machine interaction. In TC Stewart (Chair), Proceedings of the 17th International Conference on cognitive modelling. Waterloo, Canada: University of Waterloo.</p>
<p>Johannes Schneider, arXiv:2009.09266Humans learn too: Better Human-AI Interaction using Optimized Human Inputs. arXiv preprintJohannes Schneider. 2020. Humans learn too: Better Human-AI Interaction using Optimized Human Inputs. arXiv preprint arXiv:2009.09266 (2020).</p>
<p>The neural architecture of language: Integrative reverse-engineering converges on a model for predictive processing. Martin Schrimpf, Idan Blank, Greta Tuckute, Carina Kauf, A Eghbal, Nancy Hosseini, Joshua Kanwisher, Evelina Tenenbaum, Fedorenko, BioRxiv. Martin Schrimpf, Idan Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy Kanwisher, Joshua Tenenbaum, and Evelina Fedorenko. 2020. The neural architecture of language: Integrative reverse-engineering converges on a model for predictive processing. BioRxiv (2020).</p>
<p>Bounded rationality. In Utility and probability. A Herbert, Simon, SpringerHerbert A Simon. 1990. Bounded rationality. In Utility and probability. Springer, 15-18.</p>
<p>Behavior model calibration for epidemic simulations. Meghendra Singh, Achla Marathe, V Madhav, Samarth Marathe, Swarup, Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems. the 17th International Conference on Autonomous Agents and MultiAgent SystemsMeghendra Singh, Achla Marathe, Madhav V Marathe, and Samarth Swarup. 2018. Behavior model calibration for epidemic simulations. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems. 1640-1648.</p>
<p>Engineering a less artificial intelligence. H Fabian, Xaq Sinz, Jacob Pitkow, Matthias Reimer, Andreas S Bethge, Tolias, Neuron. 103Fabian H Sinz, Xaq Pitkow, Jacob Reimer, Matthias Bethge, and Andreas S Tolias. 2019. Engineering a less artificial intelligence. Neuron 103, 6 (2019), 967-979.</p>
<p>Integrating ACT-R cognitive models with the Unity game engine. Paul Richard Smart, Tom Scutt, Katia Sycara, Nigel R Shadbolt, Integrating cognitive architectures into virtual character design. IGI GlobalPaul Richard Smart, Tom Scutt, Katia Sycara, and Nigel R Shadbolt. 2016. Integrating ACT-R cognitive models with the Unity game engine. In Integrating cognitive architectures into virtual character design. IGI Global, 35-64.</p>
<p>Analysis of the human connectome data supports the notion of a "Common Model of Cognition" for human and human-like intelligence across domains. Andrea Stocco, Catherine Sibert, Zoe Steine-Hanson, Natalie Koh, John E Laird, J Christian, Paul Lebiere, Rosenbloom, NeuroImage. 235118035Andrea Stocco, Catherine Sibert, Zoe Steine-Hanson, Natalie Koh, John E Laird, Christian J Lebiere, and Paul Rosenbloom. 2021. Analysis of the human connectome data supports the notion of a "Common Model of Cognition" for human and human-like intelligence across domains. NeuroImage 235 (2021), 118035.</p>
<p>Dynamic Cognitive Modeling for Adaptive Serious Games. Alexander Streicher, Julius Busch, Wolfgang Roller, International Conference on Human-Computer Interaction. SpringerAlexander Streicher, Julius Busch, and Wolfgang Roller. 2021. Dynamic Cognitive Modeling for Adaptive Serious Games. In International Confer- ence on Human-Computer Interaction. Springer, 167-184.</p>
<p>Reasoning with heuristics and induction: An account based on the CLARION cognitive architecture. Ron Sun, Sebastien Helie, The 2012 International Joint Conference on Neural Networks (IJCNN). IEEERon Sun and Sebastien Helie. 2012. Reasoning with heuristics and induction: An account based on the CLARION cognitive architecture. In The 2012 International Joint Conference on Neural Networks (IJCNN). IEEE, 1-8.</p>
<p>The Consumer Contextual Decision-Making Model. Jyrki Suomala, Frontiers in Psychology. 112543Jyrki Suomala. 2020. The Consumer Contextual Decision-Making Model. Frontiers in Psychology 11 (2020), 2543.</p>
<p>How to grow a mind: Statistics, structure, and abstraction. B Joshua, Charles Tenenbaum, Kemp, L Thomas, Noah D Griffiths, Goodman, science. 331Joshua B Tenenbaum, Charles Kemp, Thomas L Griffiths, and Noah D Goodman. 2011. How to grow a mind: Statistics, structure, and abstraction. science 331, 6022 (2011), 1279-1285.</p>
<p>A general instance-based learning framework for studying intuitive decision-making in a cognitive architecture. Robert Thomson, Christian Lebiere, James John R Anderson, Staszewski, Journal of Applied Research in Memory and Cognition. 4Robert Thomson, Christian Lebiere, John R Anderson, and James Staszewski. 2015. A general instance-based learning framework for studying intuitive decision-making in a cognitive architecture. Journal of Applied Research in Memory and Cognition 4, 3 (2015), 180-190.</p>
<p>Tanmoy Sunil Thulasidasan, Jeff Bhattacharya, Gopinath Bilmes, Jamal Chennupati, Mohd-Yusof, arXiv:1905.10964Combating label noise in deep learning using abstention. arXiv preprintSunil Thulasidasan, Tanmoy Bhattacharya, Jeff Bilmes, Gopinath Chennupati, and Jamal Mohd-Yusof. 2019. Combating label noise in deep learning using abstention. arXiv preprint arXiv:1905.10964 (2019).</p>
<p>Judgment under uncertainty: Heuristics and biases. Amos Tversky, Daniel Kahneman, science. 185ACMManuscript submitted toAmos Tversky and Daniel Kahneman. 1974. Judgment under uncertainty: Heuristics and biases. science 185, 4157 (1974), 1124-1131. Manuscript submitted to ACM</p>
<p>Bayesian models of conceptual development: Learning as building models of the world. D Tomer, Joshua B Ullman, Tenenbaum, Annual Review of Developmental Psychology. 2Tomer D Ullman and Joshua B Tenenbaum. 2020. Bayesian models of conceptual development: Learning as building models of the world. Annual Review of Developmental Psychology 2 (2020), 533-558.</p>
<p>PECS-agent-based modelling of human behaviour. Christoph Urban, Bernd Schmidt, Emotional and Intelligent-The Tangled Knot of Social Cognition, AAAI Fall Symposium Series. North Falmouth, MAChristoph Urban and Bernd Schmidt. 2001. PECS-agent-based modelling of human behaviour. In Emotional and Intelligent-The Tangled Knot of Social Cognition, AAAI Fall Symposium Series, North Falmouth, MA. www. or. unipassau. de/5/publik/urban/CUrban01. pdf.</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).</p>
<p>An Invisible Gorilla: Is It a Matter of Focus of Attention. Chen Wang, Pablo Cesar, Erik Geelhoed, Pacific-Rim Conference on Multimedia. SpringerChen Wang, Pablo Cesar, and Erik Geelhoed. 2013. An Invisible Gorilla: Is It a Matter of Focus of Attention?. In Pacific-Rim Conference on Multimedia. Springer, 318-326.</p>
<p>Presslight: Learning max pressure control to coordinate traffic signals in arterial network. Hua Wei, Chacha Chen, Guanjie Zheng, Kan Wu, Vikash Gayah, Kai Xu, Zhenhui Li, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningHua Wei, Chacha Chen, Guanjie Zheng, Kan Wu, Vikash Gayah, Kai Xu, and Zhenhui Li. 2019. Presslight: Learning max pressure control to coordinate traffic signals in arterial network. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 1290-1298.</p>
<p>Understanding ACT-R-an outsider's perspective. Jacob Whitehill, arXiv:1306.0125arXiv preprintJacob Whitehill. 2013. Understanding ACT-R-an outsider's perspective. arXiv preprint arXiv:1306.0125 (2013).</p>
<p>A Joint Inverse Reinforcement Learning and Deep Learning Model for Drivers' Behavioral Prediction. Guojun Wu, Yanhua Li, Shikai Luo, Ge Song, Qichao Wang, Jing He, Jieping Ye, Xiaohu Qie, Hongtu Zhu, Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. the 29th ACM International Conference on Information &amp; Knowledge ManagementGuojun Wu, Yanhua Li, Shikai Luo, Ge Song, Qichao Wang, Jing He, Jieping Ye, Xiaohu Qie, and Hongtu Zhu. 2020. A Joint Inverse Reinforcement Learning and Deep Learning Model for Drivers' Behavioral Prediction. In Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. 2805-2812.</p>
<p>Re-examining whether, why, and how human-AI interaction is uniquely difficult to design. Qian Yang, Aaron Steinfeld, Carolyn Rosé, John Zimmerman, Proceedings of the 2020 chi conference on human factors in computing systems. the 2020 chi conference on human factors in computing systemsQian Yang, Aaron Steinfeld, Carolyn Rosé, and John Zimmerman. 2020. Re-examining whether, why, and how human-AI interaction is uniquely difficult to design. In Proceedings of the 2020 chi conference on human factors in computing systems. 1-13.</p>
<p>Explicit sparse transformer: Concentrated attention through explicit selection. Guangxiang Zhao, Junyang Lin, Zhiyuan Zhang, Xuancheng Ren, Qi Su, Xu Sun, arXiv:1912.11637arXiv preprintGuangxiang Zhao, Junyang Lin, Zhiyuan Zhang, Xuancheng Ren, Qi Su, and Xu Sun. 2019. Explicit sparse transformer: Concentrated attention through explicit selection. arXiv preprint arXiv:1912.11637 (2019).</p>
<p>Daydreaming with Intention: Scalable Blending-Based Imagining and Agency in Generative Interactive Narrative. Jichen Zhu, D Fox Harrell, AAAI Spring Symposium: Creative Intelligent Systems. 156Jichen Zhu and D Fox Harrell. 2008. Daydreaming with Intention: Scalable Blending-Based Imagining and Agency in Generative Interactive Narrative.. In AAAI Spring Symposium: Creative Intelligent Systems, Vol. 156.</p>
<p>The Bayesian sampler: Generic Bayesian inference causes incoherence in human probability judgments. Jian-Qiao Zhu, Adam N Sanborn, Nick Chater, Psychological review. 127719Jian-Qiao Zhu, Adam N Sanborn, and Nick Chater. 2020. The Bayesian sampler: Generic Bayesian inference causes incoherence in human probability judgments. Psychological review 127, 5 (2020), 719.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5011 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5011</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5011</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-257771900</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.emnlp-main.688.pdf" target="_blank">Explicit Planning Helps Language Models in Logical Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose LEAP, a novel system that uses language models to perform multi-step logical reasoning and incorporates explicit planning into the inference procedure. Explicit planning enables the system to make more informed reasoning decisions at each step by looking ahead into their future effects. Moreover, we propose a training strategy that safeguards the planning process from being led astray by spurious features. Our full system significantly outperforms other competing methods on multiple standard datasets. When using small T5 models as its core selection and deduction components, our system performs competitively compared to GPT-3 despite having only about 1B parameters (i.e., 175 times smaller than GPT-3). When using GPT-3.5, it significantly outperforms chain-of-thought prompting on the challenging PrOntoQA dataset. We have conducted extensive empirical studies to demonstrate that explicit planning plays a crucial role in the system's performance.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5011.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5011.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LEAP System-A (planning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LEAP — Planning-based inference (System-A)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An explicit-planning extension of an LM-based multi-step logical reasoning system that augments selection/deduction scoring with D-step rollouts to prefer decisions whose simulated futures are more likely to prove a goal.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-small (selection & deduction) + DeBERTa (verification)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SLM variant of LEAP: selection and deduction implemented with prompt-tuned T5 models (encoder-decoder, used frozen except for soft-prompt embeddings); verification by a pretrained DeBERTa fine-tuned on MNLI that provides p_ver(entailment). Beam-style inference with buffer B, rollouts depth D to compute future-specific corrections (α, β).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>60M</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Entailment Bank (Version-I/II); multiple-choice derived from Entailment Bank; QASC (dev)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Textual multi-step logical reasoning over natural-language premises and rules: determine whether a hypothesis is provable and (optionally) produce a human-interpretable multi-step proof (Entailment Bank contains human-annotated (theory, goal, reasoning path)). QASC is a multiple-choice QA with compositional reasoning; PrOntoQA (separately for LLMs) is synthetic fictional-domain logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Explicit planning during beam-search-style inference: at each selection/deduction candidate the system rolls out D one-best future steps (no-planning decoder) and computes ∆u/∆v = max log p_ver(goal | rolled-out deduction) as a correction added to the model log-score (scaled by α or β). This yields System-A (planning enabled) where the base selection/deduction model scores are combined with rollout-derived future-quality signals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On Entailment Bank System-A substantially improves over the no-planning base system and the plain T5 baseline on provable goals (higher true positives), but suffers from elevated false positives on non-provable goals when used without verifier refinement. In multiple-choice Entailment Bank evaluation (Version-I), System-A achieved high accuracy (example: 0.80 [0.76–0.84] reported in the paper for System-A on Version-I multiple-choice), but performance degrades on distractor-heavy Version-II (example: ~0.44 [0.39–0.49]). Wall-clock inference cost: no-planning ~2.8s per theory-goal pair; planning with B=5,D=2 ~31s (≈11× slower in practice; theoretical slowdown 1+2BD = 21×).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prone to "model exploitation": planning will preferentially find reasoning paths that maximize the pretrained verification model's scores (p_ver) even when the goal is not provable, causing high false-positive rates on non-provable goals; computationally expensive (planning increases inference cost substantially); verification model mistakes (lexical overlap bias) can misguide rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms the base (no-planning) LEAP and plain T5 selection/deduction baselines on provable-goal detection and accuracy, but underperforms the refined-verifier variant (System-B) on negative examples. Compared to GPT-3 baselines: the SLM planning approach improves over 0-shot GPT-3 in some settings but is weaker than 5-shot and chain-of-thought (COT) GPT-3 on the Entailment Bank multiple-choice task.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Ablations showed planning helps especially on provable examples but hurts non-provable detection without verifier refinement; buffer size B tradeoffs (smaller B slightly decreases positive accuracy, increases negative accuracy); T5-base (220M) improves over T5-small but planning still helps; denoising training (randomly permuting statements) improves robustness to distractors; training on only 50% data retained System-B superiority (System-A not as robust).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explicit Planning Helps Language Models in Logical Reasoning', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5011.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5011.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LEAP System-B (refined verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LEAP — Planning with refined verification (System-B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LEAP with an adversarially refined verification model: the verification scorer is contrastively fine-tuned to reduce scores for model-found proofs of non-provable goals while preserving pretrained entailment behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-small (selection & deduction) + DeBERTa (tuned verification)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same SLM architecture as System-A for selection/deduction; verification model is DeBERTa (deberta-v2xlarge-mnli) further tuned by a contrastive objective and a KL regularizer (Ω) toward the pretrained p_ver to suppress high scores on model-generated spurious proofs of non-provable goals.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>60M (T5-small); DeBERTa size not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Entailment Bank (binary provable/non-provable classification and multiple-choice), QASC (dev), robustness experiments (distractors)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same natural-language multi-step proof tasks as for System-A: detect provability and produce reasoning paths; multiple-choice selection among candidate goals; QASC single-step deduction/dev set.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Adversarial refinement of the verification model: for each training theory, synthesize a non-provable target, run planning-based inference to extract high-scoring (but incorrect) model-generated proofs, and apply a contrastive loss ℓ that lowers p_ver(non-provable|model_path) while keeping p_ver(provable|gold_path) high. Add regularizer Ω (KL to pretrained p_-ver) to preserve original entailment performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>System-B substantially outperforms System-A and the base system on Entailment Bank classification and multiple-choice. Notable reported results: System-B multiple-choice accuracy on Entailment Bank Version-I ≈ 0.88 (95% CI 0.85–0.92) and Version-II ≈ 0.63 (0.58–0.68). On binary classification, System-B significantly improves AUROC and balances true positives and true negatives much better than System-A. The paper states System-B performs comparably to GPT-3 (GPT-3 davinci 0-shot F1 = 0.89) on Entailment Bank.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Refining p_ver on in-domain adversarial examples helps but may not generalize across domains: a verification model tuned on Entailment Bank hurt performance on the out-of-domain PrOntoQA dataset. Computational cost remains high due to planning. The method requires gold provable examples and model-found negative proofs for adversarial tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>System-B > System-A (especially on non-provable detection) and > base/no-planning and plain T5 baselines; comparable to strong GPT-3 baselines on Entailment Bank binary classification (F1 parity cited). On multiple-choice, System-B outperforms 0-shot GPT-3 but underperforms GPT-3 5-shot and COT in the paper's Entailment Bank multiple-choice comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Contrastive refinement without the Ω regularizer degraded MNLI entailment performance dramatically (tuned model without Ω achieved only 62.0% on MNLI vs 91.7% original), whereas with Ω the tuned model preserved MNLI accuracy (91.4%); System-B trained on only 50% data still outperformed alternatives; denoising training and training with distractors improved robustness; buffer size and depth choices explored (B=5, D=2 used in main experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explicit Planning Helps Language Models in Logical Reasoning', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5011.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5011.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (LEAP LLM version)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo used as selection and deduction components in LEAP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LEAP's LLM variant uses GPT-3.5 as a black-box few-shot prompted generator to propose multiple selections and deductions; explicit planning scores rollouts using an external verifier rather than internal model probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only large language model provided as a black box; probabilities p_sel and p_ded are not exposed, so the LEAP LLM version sets model log-scores u=v=0 and relies entirely on planning-derived signals and the external verification model (DeBERTa or few-shot GPT-3.5 verifier in some PrOntoQA experiments). Selection/deduction performed via few-shot prompting (5-shot used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>PrOntoQA (fictional), Entailment Bank (comparative experiments), selection/deduction evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>PrOntoQA tests strict logical reasoning in a fictional domain to prevent reliance on commonsense or memorized facts; Entailment Bank is human-annotated multi-step natural-language proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot prompting to get multiple candidate selections and deductions from GPT-3.5; apply explicit planning (rollouts) over candidates to compute future-proof probabilities via an external verifier; because GPT-3.5 does not return selection/deduction probabilities, planning-derived signals dominate decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On PrOntoQA (fictional), the planning-based LLM variant consistently outperformed the selection-inference (SI) baseline and in most experimental settings outperformed chain-of-thought prompting; the paper states planning with GPT-3.5 "significantly outperforms" COT on PrOntoQA. Exact numeric accuracies are reported in the paper (Table 4) per depth subset — qualitative claim: planning helps LLMs on strict logical tasks where commonsense shortcuts are unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limited by not having access to internal generation probabilities (u=v forced to 0), increasing reliance on the external verifier; still computationally expensive due to rollouts; prompts/demos and few-shot sensitivity affect selection quality. A verification model tuned on one domain (Entailment Bank) did not generalize well to PrOntoQA.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Planning-based GPT-3.5 > selection-inference (SI) baseline; typically > chain-of-thought (COT) on PrOntoQA (fictional) according to the paper; direct comparisons to GPT-3 (davinci) show that careful few-shot or COT prompting on GPT-3 can be very strong on Entailment Bank multiple-choice, but on strictly logical fictional domains GPT-3.5+planning wins.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Authors experimented with modifying the proof score to include contradiction probability (g = max(p_ver(entail), p_con(contradict))) for PrOntoQA (because non-provable cases are often disprovable) and found reduced variance; using a DeBERTa verifier tuned on Entailment Bank harmed out-of-domain performance on PrOntoQA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explicit Planning Helps Language Models in Logical Reasoning', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5011.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5011.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (davinci) — baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (text-davinci) used as a baseline with 0-shot, 5-shot, and chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Established large LM baseline evaluated with different prompting strategies (0-shot, 5-shot, chain-of-thought); compared against LEAP variants on Entailment Bank multiple-choice and binary classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large decoder-only Transformer LM (commonly reported ~175B parameters) used via prompting: 0-shot, 5-shot, and chain-of-thought (COT) with ground-truth reasoning paths in demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Entailment Bank multiple-choice and binary classification comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-step natural-language reasoning tasks; COT prompts include ground-truth reasoning paths for correct choices.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Prompting-based evaluation: 0-shot, k-shot, and chain-of-thought prompting including ground-truth chains in-context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On Entailment Bank binary/multiple-choice baselines: GPT-3 (0-shot) performed poorly in some distractor-heavy settings (worse than random in Version-II without prompts), 5-shot and COT GPT-3 achieved very high multiple-choice accuracies (COT ~0.98 reported for Version-I in the paper). The paper reports GPT-3 0-shot F1 = 0.89 on Entailment Bank (binary classification) as a strong baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance relies heavily on prompt design and in-context examples; 0-shot performance can fail in distractor-rich settings; chain-of-thought prompting needs correct exemplar chains (not always available). May rely on memorized commonsense rather than strict formal reasoning on fictional domains (PrOntoQA).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>LEAP System-B (SLM) matches GPT-3 0-shot F1 on Entailment Bank; LEAP planning variants outperform 0-shot GPT-3 and selection-inference baselines in multiple settings, but 5-shot and COT GPT-3 can outperform SLM LEAP on some Entailment Bank multiple-choice tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explicit Planning Helps Language Models in Logical Reasoning', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5011.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5011.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTa verifier (pretrained + tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>deberta-v2xlarge-mnli used as the verification model (p_ver)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained DeBERTa model fine-tuned on MNLI used to score entailment between a candidate deduction and the goal; further contrastively tuned in System-B to suppress scores for model-generated spurious proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa (deberta-v2xlarge-mnli)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-based Transformer pretrained and fine-tuned on the MNLI natural language inference dataset; used as a probabilistic verifier p_ver(goal | deduction) for proof scoring and rollout evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Verification subtask: entailment scoring used inside planning rollouts for Entailment Bank, PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a candidate intermediate statement x_n and a goal x_0, output probability that x_n entails x_0 (used as proof score for paths and rollouts). Also used to compute contradiction probability p_con in some PrOntoQA experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Pretrained on MNLI then optionally tuned adversarially with a contrastive loss that lowers p_ver for model-found proofs of non-provable goals while maintaining pretrained behavior via KL regularization Ω (soft-prompt tuning used for parameter efficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Pretrained DeBERTa provided reliable entailment signals; after adversarial tuning with Ω regularizer, the verification model preserved MNLI accuracy (91.4% vs original 91.7%). Without the Ω term, adversarial tuning collapsed MNLI performance to ~62.0%. Tuning reduced planning-induced false positives in System-B.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Imperfect entailment judgments can be exploited by planning (lexical overlap biases); adversarial tuning helps but may not transfer across domains (tuning on Entailment Bank harmed PrOntoQA performance).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explicit Planning Helps Language Models in Logical Reasoning', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>EntailmentBank <em>(Rating: 2)</em></li>
                <li>PrOntoQA <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>RuleTaker <em>(Rating: 2)</em></li>
                <li>Selection-inference (Creswell et al.) <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Neural unification for logic reasoning over natural language <em>(Rating: 1)</em></li>
                <li>DeepA2 / Natural language deduction through search over statement compositions (Bostrom et al.) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5011",
    "paper_id": "paper-257771900",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "LEAP System-A (planning)",
            "name_full": "LEAP — Planning-based inference (System-A)",
            "brief_description": "An explicit-planning extension of an LM-based multi-step logical reasoning system that augments selection/deduction scoring with D-step rollouts to prefer decisions whose simulated futures are more likely to prove a goal.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-small (selection & deduction) + DeBERTa (verification)",
            "model_description": "SLM variant of LEAP: selection and deduction implemented with prompt-tuned T5 models (encoder-decoder, used frozen except for soft-prompt embeddings); verification by a pretrained DeBERTa fine-tuned on MNLI that provides p_ver(entailment). Beam-style inference with buffer B, rollouts depth D to compute future-specific corrections (α, β).",
            "model_size": "60M",
            "logical_reasoning_task": "Entailment Bank (Version-I/II); multiple-choice derived from Entailment Bank; QASC (dev)",
            "task_description": "Textual multi-step logical reasoning over natural-language premises and rules: determine whether a hypothesis is provable and (optionally) produce a human-interpretable multi-step proof (Entailment Bank contains human-annotated (theory, goal, reasoning path)). QASC is a multiple-choice QA with compositional reasoning; PrOntoQA (separately for LLMs) is synthetic fictional-domain logical reasoning.",
            "method_or_approach": "Explicit planning during beam-search-style inference: at each selection/deduction candidate the system rolls out D one-best future steps (no-planning decoder) and computes ∆u/∆v = max log p_ver(goal | rolled-out deduction) as a correction added to the model log-score (scaled by α or β). This yields System-A (planning enabled) where the base selection/deduction model scores are combined with rollout-derived future-quality signals.",
            "performance": "On Entailment Bank System-A substantially improves over the no-planning base system and the plain T5 baseline on provable goals (higher true positives), but suffers from elevated false positives on non-provable goals when used without verifier refinement. In multiple-choice Entailment Bank evaluation (Version-I), System-A achieved high accuracy (example: 0.80 [0.76–0.84] reported in the paper for System-A on Version-I multiple-choice), but performance degrades on distractor-heavy Version-II (example: ~0.44 [0.39–0.49]). Wall-clock inference cost: no-planning ~2.8s per theory-goal pair; planning with B=5,D=2 ~31s (≈11× slower in practice; theoretical slowdown 1+2BD = 21×).",
            "limitations_or_failure_cases": "Prone to \"model exploitation\": planning will preferentially find reasoning paths that maximize the pretrained verification model's scores (p_ver) even when the goal is not provable, causing high false-positive rates on non-provable goals; computationally expensive (planning increases inference cost substantially); verification model mistakes (lexical overlap bias) can misguide rollouts.",
            "comparison": "Outperforms the base (no-planning) LEAP and plain T5 selection/deduction baselines on provable-goal detection and accuracy, but underperforms the refined-verifier variant (System-B) on negative examples. Compared to GPT-3 baselines: the SLM planning approach improves over 0-shot GPT-3 in some settings but is weaker than 5-shot and chain-of-thought (COT) GPT-3 on the Entailment Bank multiple-choice task.",
            "ablation_or_analysis_results": "Ablations showed planning helps especially on provable examples but hurts non-provable detection without verifier refinement; buffer size B tradeoffs (smaller B slightly decreases positive accuracy, increases negative accuracy); T5-base (220M) improves over T5-small but planning still helps; denoising training (randomly permuting statements) improves robustness to distractors; training on only 50% data retained System-B superiority (System-A not as robust).",
            "uuid": "e5011.0",
            "source_info": {
                "paper_title": "Explicit Planning Helps Language Models in Logical Reasoning",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "LEAP System-B (refined verifier)",
            "name_full": "LEAP — Planning with refined verification (System-B)",
            "brief_description": "LEAP with an adversarially refined verification model: the verification scorer is contrastively fine-tuned to reduce scores for model-found proofs of non-provable goals while preserving pretrained entailment behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-small (selection & deduction) + DeBERTa (tuned verification)",
            "model_description": "Same SLM architecture as System-A for selection/deduction; verification model is DeBERTa (deberta-v2xlarge-mnli) further tuned by a contrastive objective and a KL regularizer (Ω) toward the pretrained p_ver to suppress high scores on model-generated spurious proofs of non-provable goals.",
            "model_size": "60M (T5-small); DeBERTa size not specified in paper",
            "logical_reasoning_task": "Entailment Bank (binary provable/non-provable classification and multiple-choice), QASC (dev), robustness experiments (distractors)",
            "task_description": "Same natural-language multi-step proof tasks as for System-A: detect provability and produce reasoning paths; multiple-choice selection among candidate goals; QASC single-step deduction/dev set.",
            "method_or_approach": "Adversarial refinement of the verification model: for each training theory, synthesize a non-provable target, run planning-based inference to extract high-scoring (but incorrect) model-generated proofs, and apply a contrastive loss ℓ that lowers p_ver(non-provable|model_path) while keeping p_ver(provable|gold_path) high. Add regularizer Ω (KL to pretrained p_-ver) to preserve original entailment performance.",
            "performance": "System-B substantially outperforms System-A and the base system on Entailment Bank classification and multiple-choice. Notable reported results: System-B multiple-choice accuracy on Entailment Bank Version-I ≈ 0.88 (95% CI 0.85–0.92) and Version-II ≈ 0.63 (0.58–0.68). On binary classification, System-B significantly improves AUROC and balances true positives and true negatives much better than System-A. The paper states System-B performs comparably to GPT-3 (GPT-3 davinci 0-shot F1 = 0.89) on Entailment Bank.",
            "limitations_or_failure_cases": "Refining p_ver on in-domain adversarial examples helps but may not generalize across domains: a verification model tuned on Entailment Bank hurt performance on the out-of-domain PrOntoQA dataset. Computational cost remains high due to planning. The method requires gold provable examples and model-found negative proofs for adversarial tuning.",
            "comparison": "System-B &gt; System-A (especially on non-provable detection) and &gt; base/no-planning and plain T5 baselines; comparable to strong GPT-3 baselines on Entailment Bank binary classification (F1 parity cited). On multiple-choice, System-B outperforms 0-shot GPT-3 but underperforms GPT-3 5-shot and COT in the paper's Entailment Bank multiple-choice comparisons.",
            "ablation_or_analysis_results": "Contrastive refinement without the Ω regularizer degraded MNLI entailment performance dramatically (tuned model without Ω achieved only 62.0% on MNLI vs 91.7% original), whereas with Ω the tuned model preserved MNLI accuracy (91.4%); System-B trained on only 50% data still outperformed alternatives; denoising training and training with distractors improved robustness; buffer size and depth choices explored (B=5, D=2 used in main experiments).",
            "uuid": "e5011.1",
            "source_info": {
                "paper_title": "Explicit Planning Helps Language Models in Logical Reasoning",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "GPT-3.5 (LEAP LLM version)",
            "name_full": "GPT-3.5-turbo used as selection and deduction components in LEAP",
            "brief_description": "LEAP's LLM variant uses GPT-3.5 as a black-box few-shot prompted generator to propose multiple selections and deductions; explicit planning scores rollouts using an external verifier rather than internal model probabilities.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Decoder-only large language model provided as a black box; probabilities p_sel and p_ded are not exposed, so the LEAP LLM version sets model log-scores u=v=0 and relies entirely on planning-derived signals and the external verification model (DeBERTa or few-shot GPT-3.5 verifier in some PrOntoQA experiments). Selection/deduction performed via few-shot prompting (5-shot used in experiments).",
            "model_size": null,
            "logical_reasoning_task": "PrOntoQA (fictional), Entailment Bank (comparative experiments), selection/deduction evaluations",
            "task_description": "PrOntoQA tests strict logical reasoning in a fictional domain to prevent reliance on commonsense or memorized facts; Entailment Bank is human-annotated multi-step natural-language proofs.",
            "method_or_approach": "Few-shot prompting to get multiple candidate selections and deductions from GPT-3.5; apply explicit planning (rollouts) over candidates to compute future-proof probabilities via an external verifier; because GPT-3.5 does not return selection/deduction probabilities, planning-derived signals dominate decision making.",
            "performance": "On PrOntoQA (fictional), the planning-based LLM variant consistently outperformed the selection-inference (SI) baseline and in most experimental settings outperformed chain-of-thought prompting; the paper states planning with GPT-3.5 \"significantly outperforms\" COT on PrOntoQA. Exact numeric accuracies are reported in the paper (Table 4) per depth subset — qualitative claim: planning helps LLMs on strict logical tasks where commonsense shortcuts are unavailable.",
            "limitations_or_failure_cases": "Limited by not having access to internal generation probabilities (u=v forced to 0), increasing reliance on the external verifier; still computationally expensive due to rollouts; prompts/demos and few-shot sensitivity affect selection quality. A verification model tuned on one domain (Entailment Bank) did not generalize well to PrOntoQA.",
            "comparison": "Planning-based GPT-3.5 &gt; selection-inference (SI) baseline; typically &gt; chain-of-thought (COT) on PrOntoQA (fictional) according to the paper; direct comparisons to GPT-3 (davinci) show that careful few-shot or COT prompting on GPT-3 can be very strong on Entailment Bank multiple-choice, but on strictly logical fictional domains GPT-3.5+planning wins.",
            "ablation_or_analysis_results": "Authors experimented with modifying the proof score to include contradiction probability (g = max(p_ver(entail), p_con(contradict))) for PrOntoQA (because non-provable cases are often disprovable) and found reduced variance; using a DeBERTa verifier tuned on Entailment Bank harmed out-of-domain performance on PrOntoQA.",
            "uuid": "e5011.2",
            "source_info": {
                "paper_title": "Explicit Planning Helps Language Models in Logical Reasoning",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "GPT-3 (davinci) — baselines",
            "name_full": "GPT-3 (text-davinci) used as a baseline with 0-shot, 5-shot, and chain-of-thought prompting",
            "brief_description": "Established large LM baseline evaluated with different prompting strategies (0-shot, 5-shot, chain-of-thought); compared against LEAP variants on Entailment Bank multiple-choice and binary classification.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci)",
            "model_description": "Large decoder-only Transformer LM (commonly reported ~175B parameters) used via prompting: 0-shot, 5-shot, and chain-of-thought (COT) with ground-truth reasoning paths in demonstrations.",
            "model_size": "175B",
            "logical_reasoning_task": "Entailment Bank multiple-choice and binary classification comparisons",
            "task_description": "Same multi-step natural-language reasoning tasks; COT prompts include ground-truth reasoning paths for correct choices.",
            "method_or_approach": "Prompting-based evaluation: 0-shot, k-shot, and chain-of-thought prompting including ground-truth chains in-context.",
            "performance": "On Entailment Bank binary/multiple-choice baselines: GPT-3 (0-shot) performed poorly in some distractor-heavy settings (worse than random in Version-II without prompts), 5-shot and COT GPT-3 achieved very high multiple-choice accuracies (COT ~0.98 reported for Version-I in the paper). The paper reports GPT-3 0-shot F1 = 0.89 on Entailment Bank (binary classification) as a strong baseline.",
            "limitations_or_failure_cases": "Performance relies heavily on prompt design and in-context examples; 0-shot performance can fail in distractor-rich settings; chain-of-thought prompting needs correct exemplar chains (not always available). May rely on memorized commonsense rather than strict formal reasoning on fictional domains (PrOntoQA).",
            "comparison": "LEAP System-B (SLM) matches GPT-3 0-shot F1 on Entailment Bank; LEAP planning variants outperform 0-shot GPT-3 and selection-inference baselines in multiple settings, but 5-shot and COT GPT-3 can outperform SLM LEAP on some Entailment Bank multiple-choice tasks.",
            "uuid": "e5011.3",
            "source_info": {
                "paper_title": "Explicit Planning Helps Language Models in Logical Reasoning",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "DeBERTa verifier (pretrained + tuned)",
            "name_full": "deberta-v2xlarge-mnli used as the verification model (p_ver)",
            "brief_description": "A pretrained DeBERTa model fine-tuned on MNLI used to score entailment between a candidate deduction and the goal; further contrastively tuned in System-B to suppress scores for model-generated spurious proofs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeBERTa (deberta-v2xlarge-mnli)",
            "model_description": "Encoder-based Transformer pretrained and fine-tuned on the MNLI natural language inference dataset; used as a probabilistic verifier p_ver(goal | deduction) for proof scoring and rollout evaluation.",
            "model_size": null,
            "logical_reasoning_task": "Verification subtask: entailment scoring used inside planning rollouts for Entailment Bank, PrOntoQA",
            "task_description": "Given a candidate intermediate statement x_n and a goal x_0, output probability that x_n entails x_0 (used as proof score for paths and rollouts). Also used to compute contradiction probability p_con in some PrOntoQA experiments.",
            "method_or_approach": "Pretrained on MNLI then optionally tuned adversarially with a contrastive loss that lowers p_ver for model-found proofs of non-provable goals while maintaining pretrained behavior via KL regularization Ω (soft-prompt tuning used for parameter efficiency).",
            "performance": "Pretrained DeBERTa provided reliable entailment signals; after adversarial tuning with Ω regularizer, the verification model preserved MNLI accuracy (91.4% vs original 91.7%). Without the Ω term, adversarial tuning collapsed MNLI performance to ~62.0%. Tuning reduced planning-induced false positives in System-B.",
            "limitations_or_failure_cases": "Imperfect entailment judgments can be exploited by planning (lexical overlap biases); adversarial tuning helps but may not transfer across domains (tuning on Entailment Bank harmed PrOntoQA performance).",
            "uuid": "e5011.4",
            "source_info": {
                "paper_title": "Explicit Planning Helps Language Models in Logical Reasoning",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "EntailmentBank",
            "rating": 2,
            "sanitized_title": "entailmentbank"
        },
        {
            "paper_title": "PrOntoQA",
            "rating": 2
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "RuleTaker",
            "rating": 2
        },
        {
            "paper_title": "Selection-inference (Creswell et al.)",
            "rating": 2,
            "sanitized_title": "selectioninference_creswell_et_al"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Neural unification for logic reasoning over natural language",
            "rating": 1,
            "sanitized_title": "neural_unification_for_logic_reasoning_over_natural_language"
        },
        {
            "paper_title": "DeepA2 / Natural language deduction through search over statement compositions (Bostrom et al.)",
            "rating": 1,
            "sanitized_title": "deepa2_natural_language_deduction_through_search_over_statement_compositions_bostrom_et_al"
        }
    ],
    "cost": 0.01952425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Explicit Planning Helps Language Models in Logical Reasoning</p>
<p>Hongyu Zhao hzhao@ttic.edu 
University of Chicago</p>
<p>Toyota Technological Institute at Chicago</p>
<p>Kangrui Wang 
University of Chicago</p>
<p>Toyota Technological Institute at Chicago</p>
<p>Yu Mo 
WeChat AI</p>
<p>Hongyuan Mei hongyuan@ttic.edu 
Toyota Technological Institute at Chicago</p>
<p>Tom Brown 
Benjamin Mann 
Nick Ryder 
Melanie Subbiah 
Jared D Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Peter Clark 
Oyvind Tafjord 
Kyle 2020 Richardson 
Karl Cobbe 
Vineet Kosaraju 
Mohammad Bavarian 
Mark Chen 
Heewoo Jun 
Lukasz Kaiser 
Matthias Plappert 
Jerry Tworek 
Jacob Hilton 
Reiichiro Nakano 
Antonia Creswell 
Murray Shanahan 
Rajarshi Das 
Shehzaad Dhuliawala 
Manzil Zaheer 
Luke Vilnis 
Ishan Durugkar 
Akshay Krishnamurthy 
Alex Smola 
Andrew 2018 Mccallum 
David Dohan 
Winnie Xu 
Aitor Lewkowycz 
Jacob Austin 
David Bieber 
Raphael Gontijo Lopes 
Yuhuai Wu 
Henryk Michalewski 
Rif A Saurous 
Yao Fu 
Hao Peng 
Litu Ou 
Ashish Sabharwal </p>
<p>Bhavana Dalvi
Oyvind TafjordPeter Jansen, Zheng-nan Xie</p>
<p>Hannah Smith
Leighanna Pipatanangkura</p>
<p>Explicit Planning Helps Language Models in Logical Reasoning
F85C141F0CB156AFB6C209571699394C
Language models have been shown to perform remarkably well on a wide range of natural language processing tasks.In this paper, we propose LEAP, a novel system that uses language models to perform multi-step logical reasoning and incorporates explicit planning into the inference procedure.Explicit planning enables the system to make more informed reasoning decisions at each step by looking ahead into their future effects.Moreover, we propose a training strategy that safeguards the planning process from being led astray by spurious features.Our full system significantly outperforms other competing methods on multiple standard datasets.When using small T5 models as its core selection and deduction components, our system performs competitively compared to GPT-3 despite having only about 1B parameters (i.e., 175 times smaller than GPT-3).When using GPT-3.5, it significantly outperforms chain-of-thought prompting on the challenging PrOntoQA dataset.We have conducted extensive empirical studies to demonstrate that explicit planning plays a crucial role in the system's performance.Raffaella Anna Bernardi.2002.Reasoning with polarity in categorial type logic.Ph.D. thesis.Gregor Betz and Kyle Richardson.2022.DeepA2: A modular framework for deep argument analysis with pretrained neural Text2Text language models.In Proceedings of the 11th Joint Conference on Lexical and Computational Semantics.Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, and Greg Durrett.2022.Natural language deduction through search over statement compositions.In Findings of the Conference on Empirical Methods in Natural Language Processing (Findings of EMNLP).</p>
<p>Introduction</p>
<p>Logical reasoning is one of the most important and longstanding problems in artificial intelligence (Russell and Norvig, 2010).A logical reasoning system is able to draw new facts by applying known rules to known facts and determine the truth value of a given hypothesis; see Figure 1 for an example.For decades, research in building reasoning systems has heavily relied on formal logic.Since the surge of pretrained large language models (LMs), there have been efforts that harness the power of pretrained LMs and directly handle natural language statements to perform multi-step logical reasoning; see section 5 for a summary.In this paper, we propose LEAP, the first LM-based logical reasoning system that performs explicit planning during inference.While determining the truth value of a statement, our system searches over the known facts for those which x 5</p>
<p>x 7</p>
<p>x 6 added after step 1 added after step 2 added after step 3 Figure 1: An example of theory T and goal x 0 as well as a human-annotated multi-step logical reasoning process that proves the goal based on the theory.</p>
<p>are relevant and performs multiple rounds of deduction to reach the conclusion.At each round, the planning process looks ahead into the future outcomes of each possible reasoning decision (i.e., which to select and what to deduce), examining which of them is more likely to discover a valid proof for the given statement.</p>
<p>Why planning?Planning is a fundamental property of intelligent behavior: it uses foresight to anticipate future outcomes of each possible decision and informs the process of decision making to achieve desirable end results.This concept has influenced the development of various methods in the field of artificial intelligence.Minimax-style game playing evaluates each possible move by anticipating replies and counterreplies between the player and the opponent (while assuming that both play optimally) (Russell and Norvig, 2010).Model-based reinforcement learning uses environment models to simulate responses to actions and then uses the simulated experiences to help learn value functions (e.g., Dyna, Monte-Carlo tree search) (Sutton and Barto, 2018).In natural language processing, planning has been used to help language models generate utterances that satisfy complex constraints (Lu et al., 2022a).</p>
<p>Planning is important for logical reasoning.By examining the future outcomes of each possible decision, a planning-based system will be able to focus on the actually useful (given and deduced) facts at early steps, thus enjoying a high chance of success.In addition, a planning-based reasoning system tends to be more interpretable, thus more useful in user-centric and safetycritical scenarios.For example, at each round of deduction, planning will explicitly show "what will happen after-and that is also why-I select these known facts and deduce this particular new fact from them", which is more informative than only saying "I select these and deduce this."However, none of the previous LM-based systems use explicit planning during inference.</p>
<p>Why is it challenging?During planning, a verification mechanism is in need to determine the quality of each possible proof.In reality, the verification has to be performed by a model (like in model-based reinforcement learning), and models are imperfect due to architectural biases and finite training data.As a consequence, the reasoning system faces the problem of model exploitation: any model mistake may misguide the planning such that it favors a seemingly promising decision over the actually correct one.For example, the model may incorrectly think a statement proves the hypothesis, just because of a significant lexical overlap, causing the planning to favor a decision that helps deduce that statement and lead to the wrong conclusion.</p>
<p>Our contributions.We first propose a logical reasoning system along with a beam-search-style inference algorithm (section 3.1): the system utilizes pretrained LMs and mimics human-like step-by-step reasoning.Then we integrate explicit planning into the inference algorithm (section 3.2) and significantly improve the performance of the system.We empirically demonstrate that planning encounters the issue of model exploitation: when the given hypothesis is false, planning may find out an incorrect proof that fools the system to believe that the hypothesis is true.Finally, we develop a training strategy that effectively mitigates the issue of model exploitation (section 3.3).Our training strategy is adversarial: for each training theory, we synthesize a non-provable hypothesis but call the planning-based inference method to find a highly-scored proof for it; then we refine the verification model such that the score it assigns to that proof is suppressed; at the same time, we force the verification model to preserve its scores on the correct proofs of the provable hypothesises.Our experiments show that this strategy further significantly improves the performance of our system.</p>
<p>Problem Formulation</p>
<p>We consider the problem of logical reasoning.Given a hypothesis (or, in other words, a goal) x 0 and a theory T = {x 1 , . . ., x N }, we are interested in determining the truth value of x 0 , i.e., whether x 0 can be logically proved by T .If the goal x 0 is provable, we are interested in discovering the reasoning process that proves it.Below is an example theory T "Richard is a King.""John is also a King.""John is greedy.""A greedy King is evil."</p>
<p>For the goal "John is evil.",humans can easily verify that it is provable by figuring out the following reasoning path: we can select the two premises about "John" and deduce "John is a greedy King." by combining them; we then pick the premise about "greedy King" and conclude "John is evil."by combining it with the previous deduction.In this paper, we build an automatic system that is able to perform this kind of human-like logical reasoning.</p>
<p>Our LEAP Framework</p>
<p>We propose LEAP, an LM-based logical reasoning system that performs explicit planning.Pretrained LMs are excellent at understanding natural languages as well as fluently generating them.1 Our LEAP system harnesses such abilities to simulate step-by-step reasoning processes that resembles how humans do logical reasoning.In this section, we will incrementally build up our full system, starting from a base system (section 3.1) to how explicit planning is integrated (sections 3.2-3.3).</p>
<p>Base System</p>
<p>Our base system consists of a selection model p sel , a deduction model p ded , and a verification model p ver .They work together in an iterative fashion to perform multistep reasoning like shown in Figure 1.At each step, the selection model p sel selects a couple of premises from the current theory.For example, at step-1 in Figure 1, it selects "eagles eat rabbits" and "rabbits are animals" from the original theory of four premises.Then the deduction model p ded reads the selected premises and outputs a new statement that is logically plausible given the selection.For example, at step-1 in Figure 1, it deduces "eagles eat animals".The new statement is then added to the theory (whose size increases by one) and it may be selected by p sel at a later step.The procedure stops if the max number of reasoning steps has been reached; otherwise, it starts a new iteration of selection and deduction.This procedure gives a reasoning path as shown in Figure 1.</p>
<p>We define the proof score of the reasoning path to be
f (T , x 0 ) def = max n=1,...,N p ver (x 0 | x n ) ∈ (0, 1) (1)
where theory T has been extended to include all the new deductions obtained through the reasoning process.Each p ver (x 0 | x n ) is given by the verification model and measures how likely the statement x n will prove the goal: e.g., "eagles only eat animals" (x 6 ) should have a lower score than "eagles are carnivores" (x 7 ) since the latter means the same as the goal.The proof score f (T , x 0 ) can be regarded as the system's belief that the theory proves the goal.How do we define the verification score p ver (x 0 | x n )?We utilize a pretrained DeBERTa model (He et al., 2021) that was fine-tuned on the standard MNLI language inference dataset (Williams et al., 2018).For a statement x n and goal x 0 , we define the verification score p ver (x 0 | x n ) to be the DeBERTa probability that x n entails x 0 .It is a reasonable estimate for the probability that x n proves x 0 .</p>
<p>Our system is general: the selection and deduction models can be any pretrained decoder-only or encoderdecoder models, including the small models whose parameters we could update and the huge models that we could only use as blackboxes.In section 4, we will discuss some specific model choices as well as how to transfer them to our logical reasoning problem.Generally, we only require that • the selection model p sel can propose multiple multipremise selections given the theory T and assign a score to each of them.For a multi-premise selection s (e.g., s = x 2 x 3 ), we denote the score to be p sel (s | T , x 0 ), or p sel (s) for short.</p>
<p>• the deduction model p ded can draw multiple deductions given a selection s and assign a score to each of them.For a deduction x, we denote its score to be p ded (x | s).</p>
<p>So far, we have been assuming that we select the highest scored selection and deduction at each step (e.g., in Figure 1 and at the beginning of this section).But this kind of one-best decoding tends to be short-sighted: there may be multiple possible reasoning paths to proving the goal; some may be better than the others (e.g., they are shorter) but they may not appear to be promising at the early steps; such reasoning paths may be missed by one-best decoding.Therefore, we develop an improved decoding method that resembles beam search (Jurafsky and Martin, 2000).</p>
<p>Beam-search-style inference.We maintain a buffer B of maximum size B which can host at most B ongoing reasoning paths, which we think are the most promising and will eventually prove the goal.Each of ongoing path tracks its proof score f as well as its log-probability g under our system.Both f and g get updated as the path progresses, which we will explain shortly.It also tracks its initial theory as well as its selections and deductions; the initial theory and the deductions form the extended (or current) theory.As long as we haven't reached the maximum number of steps, we keep expanding each ongoing path in the buffer.Each step of expansion includes a selection step followed by a deduction step.At the selection step, we do the following:</p>
<p>• For each ongoing path, we find its top B most probable selections (u 1 , s 1 ), . . ., (u B , s B ) where u b is the log-probability log p sel (s b ).Each selection expands its ongoing path and updates its g score by g ← g+u b .</p>
<p>• Now we have B 2 extended paths and let the buffer B only keep B of them which are most probable under the system (i.e., those with the highest g).</p>
<p>At the deduction step, we follow a similar procedure:</p>
<p>• For each ongoing path, we draw its top B most probable deductions (v 1 , y 1 ), . . ., (v B , y B ) conditioned on the most recent selection s; v b is the log-probability • Now we end up with B 2 extended paths and only keep B of them which have the highest g.</p>
<p>In the end, we return the reasoning path with the highest proof score f : intuitively, among all the choices that are probable under the selection and deduction models, we'd like to pick what's most likely to actually prove the goal.This method becomes one-best decoding if we set B = 1.Appendix B.1 has more details of the base system, including pseudocode for inference (Algorithms 1-3).</p>
<p>Relations to formal logic systems.Our base system resembles a rule-based system and the inference method is like a combination of the forward and backward chaining algorithms (Russell and Norvig, 2010).Each deduction step extends the theory by deducing new facts from the existing facts and rules, which resembles the forward chaining algorithm.Each selection step is conditioned on the goal, which resembles the backward chaining algorithm.However, the forward and backward algorithms can not handle the theories that have non-definite clauses like "Either John or Richard is evil.";our method doesn't have that limitation.</p>
<p>Improvement-A: Inference with Planning</p>
<p>The inference method in section 3.1 lacks planning.While expanding each ongoing path, the selections and deductions are ranked by their scores u and v that are only conditioned on the previous selections and deductions.However, the selections and deductions that appear to be promising may not actually lead to the future steps that are able to prove the goal.In this section, we propose an improved inference method that ranks the selections and deductions by explicit planning.We refer to the improved version as System-A.</p>
<p>Planning for selection.At each selection step, we expand each ongoing reasoning path with B selections given by the no-planning method, and let the buffer B keep B of the B 2 extended paths with the highest scores.</p>
<p>The key improvement is: we redefine the score such that it reflects not only the probability of the selection under the model p sel but also the quality of the future steps that the selection leads to.</p>
<p>Precisely, we redefine u = log p sel (s)+α∆u where α is a tunable hyperparameter and ∆u is a future-specific correction term that we can compute after rolling out some imaginary future deductions.For a possible selection s, we call the base one-best decoding method (section 3.1) to roll out D steps of future deductions ỹ1 , . . ., ỹD .Then we obtain p ver (x 0 | ỹd )-which evaluates how likely each rolled-out deduction may entail the goal-and compute the correction term by ∆u def = max d log p ver (x 0 | ỹd ).Note that ∆u is the logarithm of the proof score defined on the rolled-out future   reasoning path.Intuitively, a higher ∆u means that this future reasoning path is more likely to prove the goal.</p>
<p>In the end, we obtain B selections with updated scores (u 1 , s 1 ), . . ., (u B , s B ) for each ongoing path.This improved subroutine is illustrated in Figure 2a.Its pseudocode is Algorithm 10 in Appendix B.5.</p>
<p>Planning for deduction.At each deduction step, we expand each ongoing reasoning path with B deductions given by the no-planning method, and let the buffer B keep B of the extended paths with the highest scores.Similar to the planning-based selection step, the key improvement is the refined definition of the score, which reflects not only the probability of the deduction under the model p ded but also the quality of its future steps.</p>
<p>Precisely, we first draw B most probable deductions (v 1 , y 1 ), . . ., (v B , y B ) under the model p ded .Then we edit the score v b ← v b + β∆v b where β is a tunable hyperparameter and ∆v is a future-specific correction similar to ∆u.For each possible deduction y b , we call the no-planning one-best decoding method to roll out D steps of future deductions ỹb,1 , . . ., ỹb,D .</p>
<p>Then we compute ∆v b def = max d log p ver (x 0 | ỹb,d ).In the end, we obtain B deductions with updated scores (v 1 , y 1 ), . . ., (v B , y B ) for each ongoing path.</p>
<p>This improved subroutine is illustrated in Figure 2b.Its pseudocode is Algorithm 11 in Appendix B.5.</p>
<p>The full method.Except for the score definitions, the planning-based inference method looks the same as the no-planning method: the top selections and deductions will expand their ongoing paths and update their scores f and g; the buffer will only keep B paths with the highest g.But the planning-based method will tend to end up with a different set of reasoning paths than the no-planning method since the scores have been affected by the roll-outs.The full inference algorithm is Algorithm 1 in Appendix B.1: when D ≥ 1, it does explicit planning; when D = 0, it doesn't roll out future steps and becomes the no-planning method.</p>
<p>System 1 vs. System 2 reasoning.According to the "dual process" theories of reasoning (Evans, 2003), human cognition can be thought of as an interplay between a fast and intuitive "System 1" and a slow but analytical "System 2".Given enough time, System 2 can analyze the default behavior of System 1 and override it if neces-sary.In analogy to this process, our base system can be considered as System 1, while the advanced planningbased system is like System 2, which requires more computation but performs more deliberative reasoning.</p>
<p>Precisely, at each step of reasoning, the no-planning base system needs 3B operations (i.e., select, deduce, and verify).In contrast, the planning-based inference needs 3B + 3B 2 D + 3B 2 D operations: for each ongoing reasoning path in the buffer, we need to examine its B possible expansions (selection or deduction), and roll out D future steps (via one-best decoding) for each expansion.Overall, the planning-based system consumes 1 + 2BD times of computation.Fortunately, our implementation is efficient because of careful tensorization and parallelism; please see section 6.1 for an analysis of its actual walk-clock time.</p>
<p>Improvement-B: Refined Verification Model</p>
<p>The key limitation of the planning method is that it may exploit the pretrained verification model p ver such that the final proof score f (theory, goal) is inflated: this method keeps ongoing paths that have high p ver (goal | possible future deductions).This will result in a high rate of false positive: even when the goal is not provable, explicit planning will still try its best to find out the reasoning paths that have high proof scores; a high proof score will then fool the system itself to believe that this goal is provable.This issue is illustrated in our experiments (see Figure 5c and related analysis in section 6.1).In this section, we propose to resolve this issue by refining our verification model.We refer to this version of our LEAP system as System-B.</p>
<p>Our method is to tune the verification model p ver such that p ver (goal | deduction) is low when the deduction can not prove the goal.Technically, given a theory T and a non-provable goal x0 , we first call our planningbased method to find a reasoning path that tries to prove x0 , and then make p ver (x 0 | ȳ) to be low for each deduction ȳ in the reasoning path.Precisely, we locally minimize ℓ:
log p ver (x 0 | ȳ) − log (p ver (x 0 | ȳ) + p ver (x 0 | y)) (2)
where x 0 is a provable goal and y is a deduction in a reasoning path that actually proves x 0 .This objective ℓ is a typical contrastive learning objective (Ma and Collins, 2018).In our setting, it means: if we are given a non-provable goal x0 paired with a model-proposed reasoning path as well as a provable goal x 0 paired with a correct reasoning path, our verification model p ver should learn to correctly judge that "x 0 proved by path of ȳ" is less likely than "x 0 proved by path of y".This framework is illustrated in Figure 3. Additionally, we augment the loss ℓ with
Ω = − p − ver (x 0 | y) log p ver (x 0 | y) (3a) − 1 − p − ver (x 0 | y) log (1 − p ver (x 0 | y)) (3b)
where p − ver is the pretrained verification model used in sections 3.1 and 3.2.It is the KL-divergence (minus H(p − ver ), which is a constant wrt.model parameters) between the pretrained and tuned verification models, and minimizing it aims to prevent the tuned model from deviating too much from the pretrained.This is desirable since the pretrained model already enjoys a high rate of true positive for provable goals; see results in Figure 5b and relevant analysis in section 6.1.</p>
<p>Technical details (including visualization) about the verification model are in Appendix B.2.</p>
<p>Small and Large Model Versions</p>
<p>Now we introduce two specific versions of our proposed framework: the small language model (SLM) version that uses pretrained T5 (Raffel et al., 2020) and the large language model (LLM) version that utilizes GPT-3.5.</p>
<p>SLM Version</p>
<p>Our SLM version adapts pretrained T5 models (Raffel et al., 2020) to be the selection and deduction models.We use the T5-small instance (from Huggingface) that has only 60M parameters because we would like to investigate how well a very small system will work in practice.Shortly in section 6, we will see that this small system works very well.</p>
<p>Given a theory T and a goal x 0 , the selection T5 model reads them as input and produces the probability p sel (x n | T , x 0 ) that each premise x n is selected in the attempt to prove the goal x 0 .Then we can use these probabilities to compute the probability p sel (s | T , x 0 ) that a multi-premise combination s (e.g., s  x 6 : Eagles only
= x 2 x 4 ) is T5 Encoder T5 Decoder h p sel ( x i | x 1 x 2 x 3 x 4 x 5 x 0 ) = σ(h T w i ) = x 1 x 4 SP 1 SP 4 SP 0 x 0 … ENC DEC xp sel (x n | T , x 0 ) n:xn / ∈s (1 − p sel (x n | T , x 0 ))
Then finding the most probable selection is to choose the premises x n that have p sel (x n | T , x 0 ) &gt; 0.5.3This procedure is illustrated in Figure 4a.Give a selection s, the deduction T5 model reads s and produces a logical deduction y one token after another.The probability of y under the model is p ded (y | s).
• T = {x 1 , x 2 , x 3 , x 4 } and s = x 2 x 3 • T = {x 1 , x 2 , x 3 , x 4 , x 5 } and s = x 4 x 5 • T = {x 1 , x 2 , x 3 , x 4 , x 5 , x 6 } and s = x 1 x 6
and the deduction training data (blue background) is</p>
<p>• s = x 2 x 3 and new statement y = x 5</p>
<p>• s = x 4 x 5 and new statement y = x 6</p>
<p>• s = x 1 x 6 and new statement y = x 7</p>
<p>The training objectives for the selection model p sel and deduction model p ded are log p sel (s | T , x 0 ) and log p ded (y | s), respectively.Appendix B.3 includes more details about the SLM version (e.g., pseudocode for training and inference).</p>
<p>LLM Version</p>
<p>Our LLM uses GPT-3.5-turbo as the selection and deduction models.GPT-3.5 is the current largest and stateof-the-art language model that we have access to.We instruct GPT-3.5 to perform selection and deduction by few-shot prompting; please see Appendices B.4 and C.6 for technical details and the prompts used in our experiments.This is similar to the selection-inference framework proposed by Creswell et al. ( 2023) except that we request GPT-3.5 to propose multiple possible selections and deductions at each step.This design allows us to perform explicit planning for each possible selection and deduction and then choose the best option based on planning.Since GPT-3.5 doesn't give the values of the probabilities p sel and p ded , we set u = v = 0 in the inference methods, conditioning the selection and deduction entirely on the planning signals.The proof score f is still given by the DeBERTa verification model that we introduced in section 3.</p>
<p>Related Work</p>
<p>Reasoning has been a long-standing research topic in natural language processing.For a long time, the majority of research in this direction has been focused on simple tasks such as single-sentence language inference (Bernardi, 2002;Zamansky et al., 2006;MacCartney and Manning, 2009;Angeli et al., 2016;Hu et al., 2020;Chen et al., 2021) and single-step commonsense inference (Rajani et al., 2019;Latcinnik and Berant, 2020;Shwartz et al., 2020).</p>
<p>Recently, there has been an increasing research interest in the more complex problem of multi-step logical reasoning, which we study in this paper.Saha et al. (2020), to the best of our knowledge, is the first to propose an interpretable LM-based model for this problem.They and Tafjord et al. (2021) work on synthesized data of limited language variability.The LM-based system proposed by Bostrom et al. ( 2022) has an architecture similar to the SLM version of our base system except that their inference is one-best decoding without planning and their deduction model is trained with extra data collected by Bostrom et al. (2021).The selectioninference system of Creswell et al. ( 2023) is similar to the LLM version of our base system but their selection and deduction models are few-shot-prompted GPT-3; we compare with them in section 6.3.Liu et al. (2022) also use a similar architecture which they train by reinforcement learning.Weir and Van Durme (2022) embed LMs into a backward chaining framework, achieving strong performance in scientific reasoning.Our main contribution is complementary to the previous work: we integrate explicit planning into LM-based reasoning systems and design a training method to mitigate the model exploitation issue that arises in planning.Our system is a kind of general model programs (Dohan et al., 2022)especially those with verification models (Cobbe et al., 2021)-which use language models inside as probabilistic programs and apply disparate inference algorithms to the models.Other kinds of approaches to use LMs for reasoning include training discriminative models (Clark et al., 2020;Picco et al., 2021;Ghosal et al., 2022;Zhang et al., 2023), prompting GPT-3 with spelled-out reasoning procedure (Wei et al., 2022;Talmor et al., 2020), and distilling GPT-3.5 (Fu et al., 2023).</p>
<p>Another straightforward approach for text-based logical reasoning is to first translate natural language statements into formal logic expressions and then use a formal logic inference engine (Weber et al., 2019;Levkovskyi and Li, 2021;Nye et al., 2021;Lu et al., 2022b;Betz and Richardson, 2022).We tried this approach in our experiments; please see Appendix C.3 for details.</p>
<p>Another research area related to multi-step logical reasoning is to reason over graph-structured data.A popular kind of graph is knowledge graphs, i.e., relational graphs over symbolic tuples (Lao and Cohen, 2010;Wang et al., 2013;Neelakantan et al., 2015;Cohen et al., 2017;Xiong et al., 2017;Chen et al., 2018;Das et al., 2018).Another kind of graph is built by linking texts via lexical overlap or hyperlink connections (Welbl et al., 2018;Yang et al., 2018;Khot et al., 2020Khot et al., , 2021)).Methods in this area involve multi-step navigation through graphs.But they rely on pre-defined symbolic and relational structures, thus not directly applicable to our setting.Additionally, recent research (Chen and Durrett, 2019; Min et al., 2019) shows that optimizing the performance on these datasets is not well aligned to improving the models' fundamental reasoning abilities.</p>
<p>Experiments</p>
<p>We carried out a diverse set of experiments that can demonstrate the effectiveness of our proposed methods.We implemented our methods with PyTorch (Paszke et al., 2019) and Transformers (Wolf et al., 2020).Our code is at https://github.com/cindermond/leap.</p>
<p>SLM Experiments on Entailment Bank</p>
<p>We first trained and evaluated our SLM version on the standard benchmark Entailment Bank (Dalvi et al., 2021) dataset.This dataset is a corpus of humanannotated (theory, provable goal, reasoning path) tuples, including the example in Figure 1.It uses informal language, which closely aligns with how humans engage in logical reasoning during everyday conversations.This dataset has two versions: in Version-I, for each pair of theory and goal, all the premises have to be used to prove the goal; in Version-II, each theory includes a few distractors that are not useful for proving the goal.Evaluation-I: binary classification.We evaluated the abilities of the systems to classify provable and non-provable goals.For this purpose, we gave a nonprovable goal to each dev and test theory by selecting it from other (theory, goal, reasoning path) samples.The selection is adversarial: we tuned a pretrained T5 model to generate a provable goal given a theory; for each theory T , we looped over all the goals in the dataset that are guaranteed to be not provable under T , and chose the one that the T5 thinks is the most probable given T (see details in Appendix C).</p>
<p>For each given theory T and goal x 0 , we let the system generate a reasoning path that tries to prove the goal, and obtain the proof score f (T , x 0 ) of the path.Given a threshold τ ∈ (0, 1), we say "x 0 is provable" if f (T , x 0 ) ≥ τ and "x 0 is not provable" otherwise.For a systematic investigation, we varied τ and plot a receiver operating characteristic (ROC) curve for each system; the larger the area under ROC curve (AUROC) is, the better the system is.</p>
<p>The ROC curves are shown in Figure 5a: our LEAP System-A and System-B substantially and significantly outperform the base system and a T5 model (trained on generating goals given theories); System-B further significantly outperforms System-A.Surprisingly, our base system underperforms the T5 model even though it has learned to spell out its reasoning steps which we expect to help the classification.</p>
<p>Figure 5b and Figure 5c show the results broken down into the accuracies on the provable goals and non-provable goals, respectively.On provable goals, the accuracy is the number of true positive divided by the total number of test cases; on non-provable goals, the accuracy is the number of true negative divided by the total number of test cases.As we can see, System-A works very well on the provable goals, but performs poorly on the non-provable goals.That is because System-A exploits the verification model by explicit planning: as we have discussed in section 3.3, the proof scores given by System-A tend to be high, thus yielding a high rate of false positive.System-B works well on both provable and non-provable goals: the refined verification model p ver successfully avoided being exploited by planning.Actual values of the areas under curves are shown in Table 1: AUACC pos and AUACC neg correspond to the curves in Figure 5b and Figure 5c, respectively.The F1 numbers were computed as follows: we chose an optimal threshold τ by maximizing the F1 score on the development set, and then computed F1 on the test set according to the chosen τ .</p>
<p>For a comprehensive evaluation, we also compared with three other kinds of methods: GPT-3-davinci with 0-shot prompting, RuleTaker (Clark et al., 2020), and Neural Unification (Picco et al., 2021).GPT-3 achieves a strong F1 of 0.89, and our System-B performs as well as this strong model.RuleTaker is a discriminative method, training a RoBERTa (Liu et al., 2019) to perform logical reasoning as binary classification (provable or not).Neural Unification is also a discriminative method but has a different architecture than RuleTaker.It requires more sophisticated annotation and preparation of the training data than RuleTaker and our methods.Neither of them spells out a reasoning process.For these methods, we matched their numbers of trainable parameters with our methods for a fair comparison.Overall, RuleTaker performs better than our System-A but worse than System-B.Neural Unification performs worse than  RuleTaker and our System-A.Note that these results are orthogonal to our main finding that explicit planning is helpful for text-based multi-step logical reasoning.</p>
<p>Analysis-I: robustness to size of training data.We also trained the models with (randomly sampled) 50% of the training data, and evaluated them on the same test set.It turns out that our System-B still performs the best; see Figure 8 (which looks boringly similar to Figure 5) in Appendix C.4 for details.</p>
<p>Analysis-II: About the regularization in equation ( 3).We compared the system B with and without the regularization term Ω: without Ω, System-B only achieves AUROC = 0.79 (AUROC pos = 0.68 and AUROC neg = 0.65), worse than System-A.We also evaluated the tuned verification models on the MNLI dataset (on which they were fine-tuned) and found that: the model tuned without Ω only achieved 62.0% accuracy; the model tuned with Ω achieved 91.4% accuracy, almost as good as it originally was (91.7%).It means that the regularization term indeed helps the verification model preserve its ability to judge the entailment relationship.</p>
<p>Analysis-III: Robustness to distractors.We investigated the robustness of the systems to distractors by evaluating them on Version-II test data.Note that they were only trained on Version-I training data.As shown in Figure 6, all the systems perform worse than they did on Version-I test data, but the performance drop of our systems is much smaller than that of the T5 model.It means that our systems are more robust to the distractors.That is perhaps because our systems explicitly spell out their reasoning steps and explicit planning can help the systems (A and B) focus on the premises that are actually relevant to the goal at each selection step.</p>
<p>Analysis-IV: About model size and denoising.To examine the effect of model size, we reran the main experiments with T5-small (60M) replaced by T5-base (220M): using a larger model achieved a consistently stronger performance; our planning-based systems still significantly outperform the base system.We also experimented with denoising training of the selection and deduction models: every time we used a training example, we randomly permuted the input statements.The denoising training led to a better generalization to the evaluation settings with distractors.We also found that training with distractors (i.e., using Verstion-II training data) significantly improved the results.Detailed results and analysis are in Table 6 and Table 7 of Appendix C.4.</p>
<p>Analysis-V: About buffer size.The buffer size B is a tunable hyperparameter.In our experiments, we chose B = 5, a common choice in text generation.A pilot experiment with B ∈ {2, 3, 5, 10} showed that: a smaller B tends to slightly decrease the accuracy on positive samples, but increase it on negative samples; a larger B tends to slightly increase the accuracy on positive samples, but decreases it on negative samples; overall, there are only tiny changes in AUROC, which depends on accuracies on both kinds of samples.</p>
<p>Analysis-VI: Computation Cost.In our experiments, we used B = 5 and D = 2, i.e., a buffer size of 5 and a roll-out depth of 2. According to the theoretical analysis in section 3.2, the planing-based inference should be 1 + 2BD = 21 times slower than the no-planning method.In practice, it takes an average of 2.8 seconds for the no-planning method to work on a theory-goal pair from Entailment Bank.For the planning-based inference, it takes an average of 31 seconds, only 11 times slower.The implementation is faster than the theoretical analysis thanks to tensorization and parallelism.</p>
<p>Evaluation-II: Multiple-Choice QA.We further evaluated the systems in a multiple-choice question answering (QA) setting.Particularly, given a theory T in Entailment Bank, each system is asked to select the provable goal from four choices {x   The "depth" denotes the number of ground-truth reasoning steps.others are negative choices selected by a tuned T5.
(1) 0 , x (2) 0 , x (3) 0 , x(4)
We took the systems trained in section 6.1 and evaluated them on the Version-I and Version-II of this multiple-choice task: in the Version-II setting, each theory has a few distractors, so it is more challenging than Version-I.For each theory, a system tries to prove each choice x (c) 0 , ranks the four choices by their proof scores f (T , x (c) 0 ), and then chooses the one with the highest score.The systems were evaluated by accuracy.As shown in Table 2, the systems behave similarly as they do on the binary classification: in both Version-I and Version-II settings, System-A and System-B perform significantly better than the baselines, and System-B significantly outperforms System-A.</p>
<p>We also evaluated GPT-3-davinci with 0-shot, 5-shot, and chain-of-thought (COT) prompting (Brown et al., 2020;Wei et al., 2022).The COT prompts include the ground-truth reasoning paths of the correct choices; examples are in Appendix C.5.Our full system outperforms 0-shot GPT-3, but underperforms 5-shot and COT GPT-3.Interestingly, 0-shot GPT-3 works worse than random guess when theories have distractors, which indicates the difficulty of this problem.In addition, we evaluated RuleTaker and Neural Unification, with their numbers of trainable parameters matched with our methods.In the Version-I setting, they both perform worse than our System-B and Neural Unification performs even worse than System-A.Interestingly, they seem to be more robust to distractors: in the Versition-II setting, Neural Unification performs competitive to our System-B, and RuleTaker performs significantly better than System-B.However, these methods do not generate interpretable reasoning processes.</p>
<p>SLM Experiments on QASC</p>
<p>We also trained and evaluated the systems on the QASC dataset (Khot et al., 2020), a multiple-choice question answering dataset where each question has eight candidate answers.Each training QA pair has two premises and a deduction, which can be used to train our deduction model.Each development QA pair has two premises so the reasoning system only needs to do a step of deduction but no selection.Test QA pairs have no premises given and one has to search through a pool of millions of statements to find the relevant premises, which is not the focus of this paper.So we only evaluated the systems on the development set.The results are in Table 3.Although this data only requires one step of reasoning, the planning-based systems still significantly outperform the base system, suggesting that explicit planning is indeed helpful for LM-based reasoning.</p>
<p>LLM Experiments on PrOntoQA</p>
<p>We evaluated the LLM version on the "fictional" version of the PrOntoQA dataset (Saparov and He, 2023).It is a binary classification task like Entailment Bank (section 6.1), but it is more challenging to large language models such as GPT-3.5 since its logical statements are about fictional characters (e.g., wumpus), meaning that a large model can not bypass the reasoning and draw correct conclusions by commonsense or memorization.</p>
<p>The main results are shown in Table 4.In all cases, our planning-based system outperforms the selectioninference (SI) method, meaning that explicit planning is consistently helpful.In most cases, our planning-based system performs better than the strong chain-of-thought (COT) prompting.We also experimented with the De-BERTa model tuned on the Entailment Bank training data (sections 3.3 and 6.1) and found that it couldn't improve the performance on PrOntoQA.Appendix C.6 includes more details about this set of experiments as well as more results.</p>
<p>Conclusion</p>
<p>In this paper, we presented LEAP, an LM-based logical reasoning system that integrates explicit planning into the inference method.We also proposed a method that learns to prevent the explicit planning from being misguided.Our proposed methods exhibit intriguing technical connections to other reasoning systems and can be likened to the deliberative System 2 in "dual process" theories of reasoning.In our experiments, our planning-based system outperforms strong baseline methods including the selection-inference method and chain-of-thought prompting.We will discuss several exciting avenues for further improvements in Appendix A.</p>
<p>intuitive and fast System 1 (Evans, 2003) while our methods are like the analytical and slow System 2: after all, more analysis consumes more computation and thus our framework is less energy-efficient.This limitation has inspired us to explore new methods such as bandit learning to switch between two types of systems and more efficient planning (see Appendix A).</p>
<p>Ethics Statement</p>
<p>Our work complies with the ACL Ethics Policy.It aims to build more intelligent language-based logical reasoning systems which would have a broad positive impact to the society.For example, in our daily life, an intelligent logical reasoning system may help us verify facts and identify fake news; in legal domain, it may work as an automatic paralegal and assist lawyers with their document processing and decision making; in education, it may help students reason about their mistakes and improve learning experience.Meanwhile, our methods share the same risks as other machine learning methods, such as misusage, containing data bias, and suffering from adversarial attacks.However, this paper is orthogonal to the research efforts to mitigate these issues.</p>
<p>Deepanway Ghosal, Navonil Majumder, Rada Mihalcea, and Soujanya Poria. 2022.Two is better than many?binary classification as an effective approach to multi-choice question answering.</p>
<p>A Future Extensions</p>
<p>Our experiments have inspired us to explore several exciting avenues for further improvements.</p>
<p>The first is to jointly refine the selection, deduction, and verification models.In this paper, we have already shown that adversarially refining the verification model will significantly improve the performance.So a natural next step is to adversarially refine the selection and deduction models in response to the updated verification model.Allowing components of a system to adversarially refine one another has been shown useful in natural language processing (Yu et al., 2019).</p>
<p>The second is to develop implicit planning methods to improve inference efficiency.In reinforcement learning, explicit planning is often only used to help learn a value function during training; during inference, calling a value function is like planning implicitly but faster than explicit planning.This kind of methods can apply to our setting.Another way to improve efficiency is to learn a bandit that could cleverly switch between the noplanning "System 1" and our planning-based "System 2" such that we only spend more computation in the more difficult cases.</p>
<p>Another direction is to leverage unlabeled data, i.e., data without human-annotated reasoning paths.Such data is less expensive to collect.An LM-based reasoning system may be able to benefit from (the indirect training signals of) such data by self-supervised learning.</p>
<p>B Method Details</p>
<p>In this section, we give details of our methods.</p>
<p>B.1 Reasoning Process Details</p>
<p>Algorithm 1 gives a detailed explanation for how our inference method works.When D = 0, it is the naive method.When D ≥ 1, it is the inference with explicit planning.During selection, we constrain the model to only select two premises for a more controllable behavior.When we compute the proof score we only consider the newly generated deductions for convenience.Its effect to results is negligible since later deductions tend to more directly prove the goal.</p>
<p>Algorithm 2 is designed to select a set of statements from the current theory T , with the goal of inferring x 0 .We fix the size of the selection set to 2 in our experiments, but in principle this restriction can be removed.Algorithm 3 draws B ded new deductions.Their SLM versions are Algorithms 5 and 6 and the LLM versions are Algorithms 8 and 9.</p>
<p>B.2 Details of Tuning the Verification Model</p>
<p>We use the soft prompt tuning method (Lester et al., 2021): we augment the input with a few special tokens and the only trainable parameters are the embeddings of those tokens; it is illustrated in Figure 7.</p>
<p>Where do we get x 0 , y, x0 , and ȳ? Recall that we have a training corpus of theories and goals as well ▷ has access to M , Binf, D 3:
B ← PriorityQueue(B inf ) 4:
▷ max size is Binf ; priority is first element of tuple 5:</p>
<p>B.add((0, ∅, T , −∞))</p>
<p>6:</p>
<p>▷ init with empty path and current theory 7:</p>
<p>for m = 1 to M : 8:</p>
<p>▷ do inference at each step 9:</p>
<p>▷ selection at step m 10:
B old ← B; B ← PriorityQueue(B inf ) 11: for g b , R b , T b , f b in B old :
12:</p>
<p>▷ g is log-prob of path and f is its proof score 13:
S b ← SELECT(T b , x 0 , p sel ) 14:
if D &gt; 0 :
Y b ← PLAND(T b , x 0 , Y b , p sel , p ded , p ver ) 31: for v k , y k in Y b : 32: B.add((g b +v k , R b +{y k }, T b +{y k }, f b )) 33: for g b , R b , T b , f b in B : 34: y b ← the most recent deduction in R b 35:
▷ if y b entails x0 better than any prev deduction 36:
▷ update proof score of path R b 37: if p ver (x 0 | y b ) &gt; f b : f b ← p ver (x 0 | y b ) 38:
▷ choose reasoning path with highest proof score ▷ has access to Bsel 7:</p>
<p>▷ return list S which contains Bsel scored selections 8:
▷ each scored selection is (u, s) 9:
▷ score u is defined in section 3. ▷ has access to Bded</p>
<p>7:</p>
<p>▷ return list Y which contains Bded scored deductions 8:</p>
<p>▷ each scored deduction is (v, y)</p>
<p>9:</p>
<p>▷ score v is defined in section 3. return y as their ground-truth reasoning paths.For each pair of theory T and provable goal x 0 , we could randomly sample a deduction y from its ground-truth reasoning path.We use the goal of another training example as our non-provable goal x0 , call the planning-based inference method to get a reasoning path, and sample a deduction from the reasoning path as our ȳ.</p>
<p>Algorithm 4 shows how we refine the verification model using the contrastive loss with regularization.return ℓ</p>
<p>B.3 SLM Details</p>
<p>We give SLM details in this section.</p>
<p>Selection model.The selection model p sel uses a pretrained encoder-decoder model T5 (Raffel et al., 2020).The encoder reads a context string concatenating the goal x 0 and the premises x 1 , . . ., x N of current theory T ; the decoder computes the probabilities p sel (x n | T , x 0 ) that each premise x n is selected in the attempt to prove the goal x 0 .It is illustrated in Figure 4a: besides the statements, T5 also reads a few special tokens (ENC, SP 0 , SP 1 , . . ., SP N , DEC); its decoder gives a hidden state h, which is involved in computing p sel (x n | T , x 0 ) def = σ(h ⊤ w n ) where w n is the embedding of SP n .For training and inference efficiency, we keep the pretrained T5 frozen so the only trainable parameters of the selection model p sel -denoted as θ selare the embeddings of the special tokens.The pseudocode of using it for inference is in Algorithm 5.</p>
<p>Deduction model.Given the selection s, the deduction model p ded produces a logical deduction y by combining the premises in s.The new statement y is added to the theory T whose size is then increased by one; therefore, for a theory of size N , we also denote y as x N +1 .The deduction model p ded uses another pretrained T5.As shown in Figure 4b, its encoder reads an input string concatenating the selected premises along with a few special tokens; its autoregressive decoder produces a deduction one token after another.Its trainable parameters θ ded are the embeddings of the special tokens.The pseudocode of deploying it is in Algorithm 6. for i = 1 to N + m : 6:</p>
<p>▷ compute prob that each statement is selected 7:
p i ← p sel (SP i |c) 8: S ← PriorityQueue(B sel ) 9:
▷ max size is Bsel; priority is first element of tuple 10:</p>
<p>for i = 1 to N + m : 11:</p>
<p>for j = i + 1 to N + m :</p>
<p>12: ▷ training method for selection and deduction models 3:
s k ← x i + x j 13: u k ← log p i +log p j + ℓ̸ =i,
▷ init extended theory that will include deduction 4:</p>
<p>T ← T For deduction, we also use a large language model as a black box and prompt it to draw new deductions conditioned on a given selection s.The pseudocode is in Algorithm 9.The prompt template is as follows: for u k , s k in S : Tk ← T</p>
<p>C Experiment Details</p>
<p>We present experiment details in this section.</p>
<p>C.1 Data Statistics</p>
<p>The data statistics of Entailment Bank is shown in Table 5.In Version-I of Entailment Bank, there is one sample in the test set that has a theory with a single statement.We ignore this sample since it can not be dealt by our system in the normal way.</p>
<p>C.2 Hyperparameters</p>
<p>For SLM experiments, we use "t5-small" in the Huggingface transformers (Wolf et al., 2020) library for the selection and deduction models.We use "deberta-v2xlarge-mnli" for the verification model.We prompt tune these models, with a prompt length of 4 for the selection and deduction models, and a prompt length of 32 for the verification model.Note that for T5 models, the prompt is added to the beginning of both the encoder and the decoder (weight not shared).For the selection model, a layernorm is added before the sigmoid operation.</p>
<p>In training, we use the Adam (Kingma and Ba, 2015) optimizer with β 1 = 0.9, β 2 = 0.999, ϵ = 1e − 8, λ = 0. We use learning rate γ = 0.1 for the T5 models, and γ = 0.01 for the verification model.We use a batch size of 16.We set a very large epoch number like 1000 and use the validation set to do early stopping.In practice, the best epoch is often within 100.</p>
<p>For LLM experiments, we use GPT-3.5-turbomodel provided by OpenAI.We set the temperature to reduce randomness.We keep the default role "system" with the message "You are an AI assistant that speaks English."</p>
<p>We used a fixed random seed for all our data generation and training, so that our results can be easily reproduced with our codes.</p>
<p>During inference, we set B inf = B ded = 5 and retain the selections formed by 4 top-scored statements.We set α = 10 and β = 0.5 to roughly match the scale of the beam score.We roll out 3 steps for selection and 2 steps for deduction.We set the maximum step to be M = 20.</p>
<p>We do not tune hyperparameters except the learning rate, and we only tune it in our first training of every model.We try [0.1, 0.01, 0.001, 0.0001] and choose the one that yields the best dev set performance.</p>
<p>Our experiments were run on 8 A6000 GPUs.Training takes about 1 hour.Time for inference is discussed in section 6.1.</p>
<p>C.3 Details of FOL Translations</p>
<p>The classical approach of logical reasoning is to use formal logic systems.So we also evaluated the performance of a first-order-logic (FOL) system.Because the Entailment Bank dataset does not have human-annotated FOL translations for the natural language statements, we translated all the statements into FOL expressions using a T5 model trained on the corpus of (natural language, FOL) pairs collected by Levkovskyi and Li (2021), and then used a FOL engine to perform reasoning.This approach failed because the FOL translations are mostly of very poor quality.Here is a summary of the errors:</p>
<p>• inconsistency in variable naming.The FOL translations often use inconsistent variable naming, making it difficult to pattern-match relevant expressions.</p>
<p>• incorrect translations.Some FOL translations inaccurately represent the original sentences, resulting in a failure to capture the intended meaning.For example, "driving is a kind of skill" is incorrectly translated into "∃x.(driving(x)&amp;∃y.(vehicle(y)kind(x,y))).</p>
<p>• syntax errors.Some FOL translations contain syntax errors, making them difficult to be process.</p>
<p>• missing or incomplete information.In several instances, the FOL translations do not capture all rel-  ENTAILMENT BANK 0.87 (0.80,0.94) 0.54 (0.44,0.64) 0.46 (0.36,0.56)Table 8: Results of ablation studies on PrOntoQA with 95% bootstrap CFs.</p>
<p>sky appear to move due to earth 's rotation on its axis &amp; stars appear to move relative to the horizon during the night −&gt; int1: stars appearing to move relative to the horizon during the night is an example of diurnal motion; int1 &amp; the earth rotating on its axis causes stars / the moon to appear to move across the sky at night −&gt; the earth rotating on its axis causes stars to appear to move relative to the horizon during the night.</p>
<p>A:3.</p>
<p>C.6 Experiment Details on PrOntoQA</p>
<p>The PrOntoQA data has three subsets of different "depths".The "depth" denotes the number of groundtruth reasoning steps so a "deeper" subset is harder.For each depth, we draw (using the released data generation code of Saparov and He ( 2023 For the experiments on PrOntoQA, our final verification is performed by a few-shot-prompted GPT-3.5: it reads the extended theory and judges whether the given goal is proved.By doing this, we do not need to tune a threshold for the proof scores given by the verification model (although those scores are still very important in the process of explicit planning).In this dataset, the non-provable goals are often definitively disapprovable.So we would like the explicit planning to favor not only the future steps that have large proof scores but also those of large contradiction scores.Therefore, we replace the proof score f in the planning procedure by the generalized score g defined below
g(T , x 0 ) def = max n max(p ver (x 0 | x n ), p con (x 0 | x n ))(4)
where p con (x 0 | x n ) is the probability of "x n contradicts x 0 " given by the pretrained DeBERTa.Table 8 shows how much this modification helps: if we do not use g, the average performance doesn't change but we will suffer a higher variance.Table 8 also shows the results of using System B trained on Entailment Bank data.We did this to see if the verification model could generalize to out-of-domain data.In this experiment, it hurts the performance.</p>
<p>In this section, we also show the prompts for GPT-3.5 used in the experiments in section 6.3.For selection and deduction, we employed 5-shot prompting to enhance the model's comprehension.An in-context training example for 5-shot selection prompt is Contexts: 0.Every tumpus is not earthy.1.Wumpuses are not red.2.Wumpuses are vumpuses.3.Each vumpus is bitter.4.Vumpuses are zumpuses.5.Every zumpus is cold.6.Zumpuses are numpuses.7.Numpuses are aggressive.8.Numpuses are dumpuses.9.Dumpuses are opaque.10.Dumpuses are yumpuses.11.Yumpuses are not small.12.Each yumpus is a rompus.13.Every rompus is earthy.14.Each rompus is a jompus.15.Jompuses are metallic.16.Each jompus is an impus.17.Alex is a dumpus.Question:True or false: Alex is not earthy.</p>
<p>Selection:17 and 10 / 17 and 1 / 17 and 13 / 17 and 15.Alex is a dumpus and dumpuses are yumpuses.Alex is a dumpus and wumpuses are not red.Alex is a dumpus and every rompus is earthy.Alex is a dumpus and jompuses are metallic.</p>
<p>An in-context example for deduction prompt is</p>
<p>We know that: Sally is a tumpus and each tumpus is hot.Inference: Sally is hot.</p>
<p>Note that we didn't let GPT to propose multiple deductions in this experiment because the no-planning deduction is almost always correct as long as the selection is correct.</p>
<p>p</p>
<p>ded (y b | s) under deduction model p ded .Each deduction expands the ongoing path: it updates the scores by g ← g + v b and f ← max{f, p ver (x 0 | y b )}.</p>
<p>Figure 2 :
2
Figure 2: An illustration of explicit planning at the 2nd selection and deduction step of the full procedure in Figure 1.</p>
<p>Figure 3 :
3
Figure 3: Illustration of our contrastive learning framework for refining verification model.</p>
<p>to be x 4 x 5 (high p sel ) (a) A selection step.The T5 encoder reads special tokens, the goal x0, and the theory T .The decoder computes psel(xn | T , x0) def = σ(h ⊤ wn) where wn is the embedding of special token SPn.</p>
<p>Eagles do not eat plants.x 5 : Eagles eat animals.</p>
<p>trainable embeddings: ENC DEC (b) A deduction step.The T5 encoder reads special tokens and the selection s = x4 x5 and generates a deduction autoregressively.It is currently trying to find the token after "only", and "eat" wins.</p>
<p>Figure 4 :
4
Figure 4: An illustration of how the SLM selection and deduction models in the example procedure of Figure 1.</p>
<p>Figure 4b shows a deduction step.Training the SLM version requires a corpus of theories and goals as well as their ground-truth reasoning paths.The selection steps are training examples for p sel ; the deduction steps are training examples for p ded .Taking Figure 1 as an example, the selection training data (green background) is</p>
<p>Acc curves on negative examples.</p>
<p>Figure 5 :
5
Figure 5: Test results with 95% bootstrap confidence intervals (CFs) on Entailment Bank Version-I.</p>
<p>Acc curves on negative examples.</p>
<p>Figure 6 :
6
Figure 6: Test results with 95% bootstrap CFs on Entailment Bank Version-II.</p>
<p>Algorithm 1
1
Reasoning (Inference) with Our System Hyperparam: max number of inference steps M ; depth of planning D (D = 0 means "no planning"); inference beam size B inf Input: theory T = {x 1 , x 2 , . . ., x N } and goal x 0 ; selection model p sel , deduction model p ded ; verification model p ver Output: reasoning path R with proof score f 1: procedure INFERENCE(T , x 0 , p sel , p ded , p ver ) 2:</p>
<p>▷B▷</p>
<p>rank selections based on D-step roll-outs 16:S b ← PLANS(T b , x 0 , S b , p sel , p ded , p ver ) 17: for u k , s k in S b : ((g b + u k , R b + {s k }, T b , f b )) 21: ▷ B has a fixed size Binf: if |B| &gt; Binf old ← B; B ← PriorityQueue(B inf) 25: for g b , R b , T b , f b in B old : 26: s b ← the most recent selection in R b 27: Y b ← DEDUCE(s b , p ded ) rank deductions based on D-step roll-outs 30:</p>
<p>bFigure 7 :
7
Figure 7: The structure of the verification model.</p>
<p>procedure ONEBESTSELECT(T , x 0 , p sel ) 12: ▷ only keeps selection with highest score 13: S ← SELECT(T , x 0 , p sel ) 14: (u, s) ← highest-scored element in S 15: return s Algorithm 3 Deduction Subroutine Hyperparam: deduction beam size B ded Input: current selection s of statements; deduction model p ded Output: deductions with their scores {(v k , y k )} 1: procedure DEDUCE(s, p ded )</p>
<p>y) ← element in Y with highest v 15:</p>
<p>p − ver (x 0 | y) log p ver (x 0 | y) 17: ℓ −= (1 − p − ver (x 0 | y)) log(1 − p ver (x 0 | y))18:</p>
<p>Algorithm 5 ▷
5
Selection Subroutine for SLM Hyperparam: selection beam size B sel Input: current theory T = {x 1 , . ., x N +m } and goal x 0 ; prompted encoder-decoder language model p sel Output: selections with their scores {(u k , s k )} 1: procedure SELECT(T , x 0 , p sel ) build context by concatenating hypothesis and theory 4: c = SP 0 +x 0 +SP 1 +x 1 +. ..+SPN +m +x N +m 5:</p>
<h1></h1>
<p>few−shot examples to demonstrate deduction # see Appendix C.6 for an example . . .# end of demonstration Please refer to these examples and generate the inference.# selection of statements of interest Algorithm 8 Selection Subroutine for LLM Hyperparam: selection beam size B sel Input: current theory T = {x 1 , . . ., x N +m } and goal x 0 ; selection model p sel Output: selections with their scores {(u k , s k )} 1: procedure SELECT(T , x 0 , p sel ) of Planning-Based Methods Algorithm 10 illustrates the details of how we use explicit planning for selection.The method considers how each selection could affect future in D steps.One-best search is applied in the roll-out process to simplify the Algorithm 9 Deduction Subroutine for LLM Hyperparam: deduction beam size B ded Input: current selection s of statements; deduction model p ded Output: deductions with their scores {(v k , y k )} 1: procedure DEDUCE(s, p ded ) Planning for Selection Hyperparam: deduction beam width B ded ; depth of planning D; planning scale α Input: current theory T = {x 1 , . . ., x N +m } and goal x 0 ; verification model p ver selection candidates at current step S = {(u k , s k )}; selection model p sel and deduction model p ded Output: selections with updated scores {(u k , s k )} 1: procedure PLANS(T , x 0 , S, p sel , p ded , p ver )</p>
<p>)) 5 training examples and 100 test examples.</p>
<p>Table 1 :
1
Test results with 95% bootstrap CFs on Entailment Bank Version-I.
METHODAUROCAUACCPOSAUACCNEGF1BASELINE-T5 BASE SYSTEM SYSTEM A SYSTEM B0.67 (0.63, 0.71) 0.53 (0.49, 0.57) 0.75 (0.72, 0.78) 0.62 (0.59, 0.64) 0.56 (0.51, 0.60) 0.42 (0.38, 0.47) 0.78 (0.76, 0.81) 0.67 (0.67, 0.67) 0.87 (0.84, 0.89) 0.86 (0.84, 0.89) 0.54 (0.50, 0.57) 0.82 (0.80, 0.84) 0.94 (0.92, 0.95) 0.87 (0.84, 0.89) 0.82 (0.79, 0.85) 0.89 (0.87, 0.91)RULETAKER NEURALUNIF GPT-3 (0-SHOT)0.90 (0.88, 0.93) 0.91 (0.88, 0.94) 0.73 (0.69, 0.77) 0.84 (0.83, 0.86) 0.72 (0.68, 0.76) 0.56 (0.56, 0.57) 0.49 (0.48, 0.50) 0.72 (0.71, 0.74) ---0.89
We trained the models on Version-I training data, but evaluated them on both Version-I and Version-II test data.Experiment details are in Appendix C, including data statistics (Table5) and training details (e.g., hyperparameter tuning in Appendix C.2).</p>
<p>Table 2 :
2
Test accuracy with 95% bootstrap CFs in multiple-choice QA.Accuracy of random guess is 25%.
METHODVERSION-IVERSION-IIBASELINE-T5 BASE SYSTEM SYSTEM A SYSTEM B0.60 (0.55, 0.65) 0.20 (0.16, 0.24) 0.46 (0.41, 0.52) 0.29 (0.25, 0.34) 0.80 (0.76, 0.84) 0.44 (0.39, 0.49) 0.88 (0.85, 0.92) 0.63 (0.58, 0.68)RULETAKER NEURALUNIF GPT-3 (0-SHOT) GPT-3 (5-SHOT) GPT-3 (COT)0.83 (0.79, 0.87) 0.73 (0.68, 0.77) 0.62 (0.55, 0.69) 0.62 (0.57, 0.67) 0.72 0.20 0.97 0.96 0.98 0.98</p>
<p>Table 3 :
3
Dev accuracy with 95% bootstrap CFs on QASC.
METHODDEPTH=1DEPTH=3DEPTH=5COT SI SYSTEM A 0.90 (0.84, 0.95) 0.55 (0.45, 0.65) 0.55 (0.45, 0.65) 0.71 (0.62, 0.80) 0.57 (0.47, 0.67) 0.52 (0.42, 0.62) 0.88 (0.81, 0.94) 0.51 (0.41, 0.61) 0.45 (0.35, 0.55)</p>
<p>Table 4 :
4
Accuracy with 95% bootstrap confidence intervals on PrOntoQA.</p>
<p>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: Decoding-enhanced bert with disentangled attention. In Proceedings of the International Conference on Learning Represen-tations (ICLR).Hai Hu, Qi Chen, Kyle Richardson, Atreyee Mukher-jee, Lawrence S. Moss, and Sandra Kuebler. 2020. MonaLog: a lightweight system for natural language inference based on monotonicity. In Proceedings of the Society for Computation in Linguistics.Daniel Jurafsky and James H. Martin. 2000. Speech and Language Processing: An Introduction to Natu-ral Language Processing, Computational Linguistics, and Speech Recognition.Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. 2020. Qasc: A dataset for question answering via sentence composition. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).Tushar Khot, Daniel Khashabi, Kyle Richardson, Pe-ter Clark, and Ashish Sabharwal. 2021. Text mod-ular networks: Learning to decompose tasks in the language of existing models. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics -Human Language Technologies (NAACL HLT).Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Repre-sentations (ICLR).Ni Lao and William W Cohen. 2010. Relational re-trieval using a combination of path-constrained ran-dom walks. Machine Learning.Veronica Latcinnik and Jonathan Berant. 2020. Explain-ing question answering models through text genera-tion. arXiv preprint arXiv:2004.05569.Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tun-ing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).Oleksii Levkovskyi and Wei Li. 2021. Generating pred-icate logic expressions from natural language. In SoutheastCon.
Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Yue Zhang,  Xipeng Qiu, and Zheng Zhang.2022.RLET: A reinforcement learning based approach for explainable QA with entailment trees.In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Algorithm 4 Refining Verification Model Input: provable goal x 0 and gold reasoning path R; non-provable goal x0 and model-generated path R; verification model p ver Output: updated verification model p ver 1: procedure REFINE(x 0 , R, x0 , R, p ver )
2:▷ refining procedure3: 4:p − ver ← a copy of pretrained p ver ▷ sample deductions from reasoning paths5: 6:randomly draw y from deductions in R randomly draw ȳ from deductions in R7:▷ refine verification model8: 9: 10: 11:ℓ ← LOSSVER(x 0 , y, x0 , ȳ, p ver , p − ver ) compute ∇ℓ wrt. trainable parameters θ ver of p ver update θ ver with chosen optimization method return p ver12: procedure LOSSVER(x 0 , y, x0 , ȳ, p ver , p − ver ) 13: ▷ contrastive loss 14: pver(x0|ȳ) ℓ ← log pver(x0|ȳ)+pver(x0|y)</p>
<p>ℓ̸ =j log(1−p ℓ ) Input: theory T = {x 1 , . . ., x N } and goal x 0 ; reasoning path R; verification model p ver selection model p sel , deduction model p ded Output: updated models p sel and p ded 1: procedure TRAIN(R, T , x 0 , p sel , p ded , p ver )
Algorithm 7 Training for SLM14: 15: 16:S.add((u k , s k )) ▷ if B is larger than Bsel, element with ▷ lowest priority will be automatically deleted17:return SAlgorithm 6 Deduction Subroutine for SLMHyperparam: deduction beam size B ded Input: current selection s of statements; prompted encoder-decoder language model p ded Output: deductions with their scores {(v k , y k )} 1: procedure DEDUCE(s, p ded )2:▷ has access to Bded3:▷ has access to standard beam search implementation4: 5:Y ← BEAMSEARCH(p ded , B ded , s) ▷ assume:6: 7: 8:▷ BEAMSEARCH gives a list of tuples {(v k , y k )} ▷ text string y k sorted in descending order of v k return YTraining. Algorithm 7 elaborates how the SLM selec-tion and deduction models are trained. We use prompt-learning because we do not want to distort the pretrained weights too much. It is well known that pretrained lan-guage models have already captured substantial amounts of commonsense knowledge such as hypernymy (A is a type of B) and meronymy (A is part of B) (Richard-son and Sabharwal, 2020); we would like to keep such knowledge to benefit our settings.
2:</p>
<p>LOSSDED(s m , y m , p ded ) SP 0 +x 0 +SP 1 +x 1 +...+SPN +m +x N +m i ← p sel (SP i |c) ▷ prob that xi is included in s 26: if x i in s : ∆ℓ ← log p i else ∆ℓ ← log(1 − p i )We give LLM details in this section.For selection, we use a large language model as a black box and prompt it to choose several different multi-premise selections from the given theory T .The pseudocode is in Algorithm 8. Below is the prompt template:
B.4 LLM Details# few−shot examples to demonstrate selection # see Appendix C.6 for an example. . .# end of demonstrationPlease refer to these examples, select four pairs of indexes from the theory (e.g. 12 and 3 / 12 and 6 / 12 and 7 / 12 and 11) that can potentially help us answer the question, no need to say anything else.You must choose four pairs even if there are no valid selections. # theory and question/goal of interest5: 6: 7: 8: 9:for m = 1 to |R|/2 : ▷ loop over each step of selection and deduction ▷ train selection model y m ← m th deduction ▷ i.e., (2m) th element in R s m ← m th selection ▷ i.e., (2m − 1) th entry in R10:11: 12:compute ∇ℓ wrt. trainable params θ sel of p sel update θ sel with chosen optimization method13:▷ train deduction model14: ℓ ← 15: compute ∇ℓ wrt. trainable params θ ded of p ded 16: update θ ded with chosen optimization method17: 18: 19:▷ extend theory with new deduction T ← T + {y m } return p sel , p ded20: procedure LOSSSEL(T , s, p sel )21:▷ construct context for selecting statements from theory22: c ← 23: ℓ ← 0 ▷ loss is negative log-likelihood of selection 24: for i = 1 to N + m :25:27: 28: 29: procedure LOSSDED(s, y, p ded ) ℓ ← ℓ − ∆ℓ ▷ update ℓ with minus log-probability return ℓ30:▷ loss is negative log-prob of deduction under model31: 32: 33:ℓ ← − log p ded (y | s) ▷ log pded(y | s) sums log-probabilities of tokens in y return ℓ11169
ℓ ← LOSSSEL( T , s m , p sel ) p</p>
<p>Algorithm 11 Planning for Deduction Hyperparam: deduction beam width B ded ; depth of planning D; planning scale β Input: current theory T = {x 1 , . .., x N +m } and goal x 0 ; deduction candidates at current step Y = {(v k , y k )}; selection model p sel and deduction model p ded ; verification model p ver Output: deductions with updated scores {(v k , y k )}1: procedure PLAND(T , x 0 , Y, p sel , p ded , p ver ) Intuitively, a higher log f means that the future reasoning path conditioned on this selection is more likely to prove the goal.Similar to Algorithm 10, Algorithm 11 measures how the newly generated deduction could affect the future reasoning path in D steps, and honors the deduction which improves the possibility of proving the goal in the future.
2:▷ has access to Bded, D, β3: 4: 5: 6: 7: 8: 9:▷ init hypothetical extended theory for v k , y k in Y : Tk ← T for v k , y k in Y : ▷ iterate over all candidate deductions Tk ← Tk + {y k } ▷ extend theory with deduction v k ← v k + β ROLLOUT ▷ ROLLOUT is in Algorithm 1010:▷ what's given by ROLLOUT is ∆v in section 3.211: 12:sort Y in descending order of updated v k return Yplanning.6:▷ iterate over all candidate selections7:▷ find hypothetical next-step deduction8: 9: 10: 11:ỹk ← ONEBESTDEDUCE(s k , p ded ) ▷ extend theory with new deduction Tk ← Tk + {ỹ k } ▷ planning with roll-outs12:▷ what's given by ROLLOUT is ∆u in section 3.213: 14: 15: 16: procedure ROLLOUT u k ← u k + α ROLLOUT sort S in descending order of updated u k return S17:▷ roll out D steps of imaginary selection and deduction18:▷ make in-place edits to sk , ỹk , Tk19: 20: 21: 22: 23:f ← −∞ for d = 1 to D : sk ← ONEBESTSELECT( Tk , x 0 , p sel ) ▷ init score of roll-out ▷ step-by-step roll-out ỹk ← ONEBESTDEDUCE(s k , p ded ) Tk ← Tk + {ỹ k }
5:for u k , s k in S :24: if p ver (x 0 | ỹk ) &gt; f : f ← p ver (x 0 | ỹk ) 25:return log f</p>
<p>Table 5 :
5
Data statistics of Entailment Bank.
The dataset can be downloaded from https://allenai.org/data/entailmentbank.SPLIT# OF SAMPLES MAX STEPS AVG STEPSTRAIN DEV TEST1313 187 34017 15 113.2 3.2 3.3</p>
<p>WITH MODIFIED PROOF SCORE 0.90 (0.84, 0.95) 0.55 (0.45, 0.65) 0.55 (0.45, 0.65) SYSTEM A WITH ORIGINAL PROOF SCORE 0.85 (0.78, 0.92) 0.64 (0.54, 0.74) 0.5 (0.41, 0.6) SYSTEM B TRAINED ON
METHODDEPTH=1DEPTH=3DEPTH=5SYSTEM A
* Work done during internship at TTI-Chicago.x 1 x 6 x 1 x 2 x 3 x 4x 5x 7 x 6x 0 x 2 x 3x 4 x 5x 0 : Eagles are carnivores.x 2 : Eagles eat rabbits.x 1 : Carnivores only eat animals.x 3 : Rabbits are animals.x 4 : Eagles do not eat plants.x 7 : Eagles are carnivores. x 6 : Eagles only eat animals. x 5 : Eagles eat animals. goal theory logical reasoning process (3 steps of selection and deduction)
We use "language model" broadly to refer to multiple types of language representation models including encoderonly, decoder-only, and encoder-decoder models.
We treat each xn independently.
For each xn, if psel &gt; 0.5, we will have psel &gt; 1 − psel.That is, including it in s will increase the probability of s.
AcknowledgmentsThis work was supported by a research gift to the last author by Adobe Research.We thank the anonymous EMNLP reviewers and meta-reviewer for their constructive feedback.We thank our colleagues at UChicago and TTIC for helpful discussion.We also thank Hao Tan at Adobe Research, Yisi Sang at Apple, Benjamin Van Durme at Johns Hopkins University, and David Dohan at OpenAI for their helpful comments.Specializing smaller language models towards multi-step reasoning.In Proceedings of the International Conference on Machine Learning (ICML).LimitationsThe main limitation of our proposed framework is that it requires more computation than the baseline methods that do not perform explicit planning.As discussed in section 3.2, the no-planning methods are like the     evant information from the original sentences.For example, it may leave out an entity or quantifier.This analysis reveals a fundamental need for tools that work directly with natural language statements for reasoning like ours.C.4 Results of Ablation StudiesFigure8shows the results of the systems trained on 50% training data.Some results of ablation studies described in section 6.1 are shown in Table6 and Table 7.C.5 Examples of Prompts for GPT-3In section 6.1, we used three kinds of prompts for GPT-3: 0-shot, 5-shot and COT.In this section, we provide some examples of these prompts.An in-context demonstration isBased on the statements that: the earth rotating on its axis causes stars / the moon to appear to move across the sky at night.diurnal motion is when objects in the sky appear to move due to earth 's rotation on its axis.stars appear to move relative to the horizon during the night.Which of the following conclusions can be inferred?0. earth rotating on its axis causes horizon of stars and night on earth.1. earth 's horizon on its rotating axis causes stars to occur in new york night.2. the earth revolving around the axis causes stars to appear in different night in the sky at different horizon of year.3. the earth rotating on its axis causes stars to appear to move relative to the horizon during the night.A: 3.For COT prompting, we used the ground-truth reasoning path for the correct choice as the "chain-of-thought", so the last line of the (say) above example will be:Reason: diurnal motion is when objects in the
Combining natural logic and shallow reasoning for question answering. Gabor Angeli, Neha Nayak, Christopher D Manning, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)2016</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>NeuroLogic a*esque decoding: Constrained text generation with lookahead heuristics. Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Le Ronan, Lianhui Bras, Youngjae Qin, Rowan Yu, Noah A Zellers, Yejin Smith, Choi, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)2022a</p>
<p>Parsing natural language into propositional and first-order logic with dual reinforcement learning. Xuantao Lu, Jingping Liu, Zhouhong Gu, Hanwen Tong, Chenhao Xie, Junyang Huang, Yanghua Xiao, Wenguang Wang, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational Linguistics2022b</p>
<p>Noise contrastive estimation and negative sampling for conditional models: Consistency and statistical efficiency. Zhuang Ma, Michael Collins, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)2018</p>
<p>An extended model of natural logic. Bill Maccartney, Christopher D Manning, Proceedings of the Eight International Conference on Computational Semantics. the Eight International Conference on Computational Semantics2009</p>
<p>Compositional questions do not necessitate multi-hop reasoning. Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, Luke Zettlemoyer, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)2019</p>
<p>Compositional vector space models for knowledge base inference. Arvind Neelakantan, Benjamin Roth, Andrew Mc-Callum, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)2015</p>
<p>Improving coherence and consistency in neural sequence models with dualsystem, neuro-symbolic reasoning. Maxwell Nye, Michael Tessler, Josh Tenenbaum, Brenden M Lake, Advances in Neural Information Processing Systems (NeurIPS). 2021</p>
<p>Pytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc2019</p>
<p>Neural unification for logic reasoning over natural language. Gabriele Picco, Thanh Hoang, Marco Luca Lam, Vanessa Lopez Sbodio, Garcia, Findings of the Conference on Empirical Methods in Natural Language Processing (Findings of EMNLP). 2021</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 2020JMLR</p>
<p>Explain yourself! leveraging language models for commonsense reasoning. Nazneen Fatema Rajani, Bryan Mccann, Caiming Xiong, Richard Socher, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)2019</p>
<p>What does my QA model know? devising controlled probes using expert knowledge. Kyle Richardson, Ashish Sabharwal, Transactions of the Association for Computational Linguistics (TACL). 2020</p>
<p>Artificial Intelligence: A Modern Approach. Stuart Russell, Peter Norvig, 2010Prentice Hall</p>
<p>PRover: Proof generation for interpretable reasoning over rules. Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, Mohit Bansal, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2023</p>
<p>Unsupervised commonsense question answering with self-talk. Vered Shwartz, Peter West, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, 2018</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, Findings of the Annual Meeting of the Association for Computational Linguistics (Findings of ACL). 2021</p>
<p>Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, Jonathan Berant, Advances in Neural Information Processing Systems (NeurIPS). 2020</p>
<p>Programming with personalized pagerank: a locally groundable first-order probabilistic logic. William Yang, Wang , Kathryn Mazaitis, William W Cohen, 10.1145/2505515.2505573Proceedings of the ACM International Conference on Information &amp; Knowledge Management (CIKM). the ACM International Conference on Information &amp; Knowledge Management (CIKM)2013</p>
<p>NLProlog: Reasoning with weak unification for question answering in natural language. Leon Weber, Pasquale Minervini, Jannes Münchmeyer, Ulf Leser, Tim Rocktäschel, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)2019</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Dynamic generation of interpretable inference rules in a neuro-symbolic expert system. Nathaniel Weir, Benjamin Van Durme, arXiv:2209.076622022arXiv preprint</p>
<p>Constructing Datasets for Multi-hop Reading Comprehension Across Documents. Johannes Welbl, Pontus Stenetorp, Sebastian Riedel, 10.1162/tacl_a_000212018Transactions of the Association for Computational Linguistics (TACL</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel R , Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics -Human Language Technologies (NAACL HLT). the Conference of the North American Chapter of the Association for Computational Linguistics -Human Language Technologies (NAACL HLT)2018Bowman</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander M Lhoest, Rush, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Deeppath: A reinforcement learning method for knowledge graph reasoning. Wenhan Xiong, Thien Hoang, William Yang, Wang , Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)2017</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)2018</p>
<p>Rethinking cooperative rationalization: Introspective extraction and complement control. Mo Yu, Shiyu Chang, Yang Zhang, Tommi Jaakkola, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)2019</p>
<p>A 'natural logic'inference system using the lambek calculus. Anna Zamansky, Nissim Francez, Yoad Winter, 10.1007/s10849-006-9018-xJournal of Logic. 2006Language and Information</p>
<p>On the paradox of learning to reason from data. Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van Den Broeck, Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI). the International Joint Conference on Artificial Intelligence (IJCAI)2023</p>            </div>
        </div>

    </div>
</body>
</html>
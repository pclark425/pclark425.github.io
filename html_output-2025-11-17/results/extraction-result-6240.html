<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6240 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6240</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6240</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-8a9f75692f946250c70c50b14b71875bf06b2270</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8a9f75692f946250c70c50b14b71875bf06b2270" target="_blank">Human Feedback is not Gold Standard</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is found that while preference scores have fairly good coverage, they under-represent important aspects like factuality, indicating that human annotations are not a fully reliable evaluation metric or training objective.</p>
                <p><strong>Paper Abstract:</strong> Human feedback has become the de facto standard for evaluating the performance of Large Language Models, and is increasingly being used as a training objective. However, it is not clear which properties of a generated output this single `preference' score captures. We hypothesise that preference scores are subjective and open to undesirable biases. We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality. We further hypothesise that both preference scores and error annotation may be affected by confounders, and leverage instruction-tuned models to generate outputs that vary along two possible confounding dimensions: assertiveness and complexity. We find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are not a fully reliable evaluation metric or training objective. Finally, we offer preliminary evidence that using human feedback as a training objective disproportionately increases the assertiveness of model outputs. We encourage future work to carefully consider whether preference scores are well aligned with the desired objective.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6240.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6240.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Crowd vs Expert annotations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison between crowdsourced annotators and author 'expert' annotations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper directly compares binary error annotations from crowdworkers with a subset of 'expert' annotations produced by the authors, showing systematic under-detection of factuality and inconsistency by crowdworkers and quantified inter-annotator agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended text generation (summarization, product descriptions, how-to instructions across Curation, Amazon product descriptions, WikiHow)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Crowdworkers (Prolific native English speakers, 100% prior approval) provided binary error annotations and 1-5 overall quality ratings. Section 2: 900 distinct outputs, ~4,440 annotations, 4 annotations per output aggregated (mode then mean for errors; mean for overall). Quality control included 200 duplicate pairs and distractors; Gwet's AC1 reported (0.64 factuality to 0.94 refusal). Authors produced a subset of 300 expert annotations used as an approximate ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Binary error rates for defined error types (Harmful, Fluency, Scope, Repetition, Refusal, Formatting, Relevance, Factuality, Inconsistency, Contradiction); overall quality ratings (1-5); inter-annotator agreement (Gwet's AC1); differences between crowd and expert error rates (delta).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Crowdworkers systematically underestimate factuality and inconsistency error rates versus author 'experts'; Gwet's AC1 for factuality was 0.64 (lower than for refusal at 0.94), indicating greater subjectivity/difficulty. Aggregated overall preference scores under-weight factuality and inconsistency relative to other errors (Lasso regression shows lower weight for factuality/inconsistency; refusal strongest contributor).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Crowd annotations marked lower factuality/inconsistency detection compared to experts, especially for assertive outputs (see Section 3); distractor examples (same-model output but different input) were incorrectly penalized for factuality and contradiction by crowdworkers despite factuality being input-independent.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use curated, trained, incentivized annotators or domain experts for critical error types; collect multiple annotators and use careful aggregation (e.g., jury learning); provide precise instructions and training examples to reduce ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Feedback is not Gold Standard', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6240.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6240.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Assertiveness confound</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of output assertiveness (confidence) on human error judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper shows that the perceived assertiveness of model outputs biases crowd annotators' judgments: more assertive outputs are rated higher quality and are less likely to be flagged for factuality or inconsistency even when experts identify errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended text generation (same datasets as above)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Section 3: 1,500 distinct outputs with preambles to induce Assertiveness--/baseline/Assertiveness++ and Complexity--/Complexity++; separate annotator pools rated overall quality (1-5), binary error annotations, and perceived assertiveness/complexity (1-5). Authors also produced 300 expert annotations per error type as a reference.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Crowd vs expert binary error rates for factuality/inconsistency across preamble conditions; perceived assertiveness and complexity ratings (1-5); correlation between assertiveness and overall quality (Pearson r=0.68); change in refusal rates (e.g., low-assertiveness refusal rose from 3.5% to 24%).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Crowdworkers underestimated factuality and inconsistency errors relative to experts; the underestimation grew for high-assertiveness outputs and shrank for low-assertiveness outputs. More assertive outputs received higher overall quality scores and were less likely to be flagged for errors by crowds even when the 'expert' annotations found errors.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Assertive but incorrect outputs ('confidently wrong') escape detection by crowd annotators; in distractor scenarios annotators penalized factuality/contradiction inconsistently, suggesting first-impression bias tied to assertiveness. Low-assertiveness outputs were sometimes shorter with fewer factual assertions, producing divergent trends between crowd and expert judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Measure and control for assertiveness (and complexity) in evaluation pipelines; hide style-control preambles from annotators (as done), train annotators to focus on factual checks independent of tone, use expert review for factuality-sensitive tasks, and incorporate objective fact-checking tools or targeted verification checks alongside human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Feedback is not Gold Standard', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6240.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6240.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Priming effect (biased vs unbiased)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Priming of overall quality scores by asking annotators to check errors first</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study shows that asking annotators to check for specific error types before rating overall quality increases the correlation between those error types and the overall score, indicating a priming/anchoring effect.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended text generation (same datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Two separate annotator groups: (a) 'biased' annotators who were shown error criteria and annotated errors before overall rating, and (b) 'unbiased' annotators who only provided overall quality ratings. Correlations between error criteria and overall scores were compared (Figure 7).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Pearson correlations between overall scores and each error factor for the 'biased' and 'unbiased' annotator groups; differences in correlation magnitudes as evidence of priming.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Correlations between overall quality and each error criterion are higher for the 'biased' group (annotators primed by error checklist), confirming that prompting annotators to look for specific errors changes their subsequent overall ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Primed annotators overweight the specified error types when forming overall judgments, potentially skewing comparisons between models if annotation protocols differ.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Avoid asking the same annotators to perform error-spotting and provide overall ratings in the same session when unbiased overall judgments are desired; separate annotation tasks, randomize order, provide balanced training, and explicitly calibrate annotators to mitigate priming.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Feedback is not Gold Standard', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6240.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6240.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLHF -> assertiveness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preliminary evidence that RLHF-style human preference training increases model assertiveness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper presents preliminary, model-level comparisons suggesting that models trained with on-policy RLHF (Llama 2 Chat) show higher assertiveness for equivalent human-rated quality than models fine-tuned off-policy on preference data (Command), implying RLHF may disproportionately amplify assertiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended text generation (same datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Cross-model comparison of mean overall quality scores (crowd) vs mean assertiveness ratings, grouped by model and preamble type (Figure 6). Models included Llama 2 13B Chat (on-policy RLHF), Command 52B/6B (off-policy fine-tuned on preference scores), Falcon, MPT, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Mean overall quality (1-5) vs mean assertiveness ratings; qualitative excerpts demonstrating stronger style changes in RLHF-trained models; comparisons of response stability across preambles.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>For the same quality score, Llama 2 outputs had higher assertiveness ratings than other models (e.g., Command 52B), suggesting RLHF may have increased assertiveness more than it increased perceived quality. The RLHF-trained model also showed greater variability in following style preambles (higher assertiveness variation) but less stability in non-baseline preambles.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>RLHF-trained models may prioritize perceived compliance/appeal to annotators (increasing assertiveness) over factual utility, risking 'confidently wrong' behavior and sycophancy. The paper notes training details aren't strictly comparable and evidence is preliminary.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Extracts show dramatic style shifts in the RLHF model (e.g., very assertive/authoritative responses) that could mask factual errors and mislead users; optimizing for preference signals can unintentionally favor assertiveness at the expense of truthfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Design training objectives that decompose preference into interpretable components (e.g., penalize over-assertiveness), include truthfulness/factuality-specific checks or rewards, evaluate models for 'humility' (high quality + low assertiveness) and prefer such models, and use controlled comparisons with equivalent base models to isolate RLHF effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Feedback is not Gold Standard', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6240.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6240.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-written evaluations (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-written / LLM-as-evaluator evaluations (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites work on using models to write evaluations of other models (e.g., Perez et al. 2023), noting model-written evaluations as related work but does not itself run LLM-as-a-judge experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discovering language model behaviors with model-written evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General behavior discovery and evaluation of LLMs (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Not performed in this paper; cited as related work (Perez et al., 2023) that uses model-generated evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Not reported in this paper; the citation points to model-written evaluation methodology as an alternative to human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>This paper does not present new empirical comparisons between model-written evaluations and human annotations, but raises the concern that automated/reward-model approximations of human feedback may introduce additional biases beyond those identified in human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Paper warns generally that reward models or automated proxies used to approximate human feedback could introduce additional, unspecified biases; notes human annotations themselves are imperfect, so models trained to mimic them inherit those biases.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>No direct failure cases for LLM-as-judge are presented here, only cautionary statements and referencing of related findings (e.g., models can be convincingly wrong and mislead humans).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Recommend caution when using reward models or model-written evaluations; combine multiple verification strategies (trained annotators, expert review, jury-style aggregation), and investigate biases introduced by automated evaluators in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Feedback is not Gold Standard', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Discovering language model behaviors with model-written evaluations <em>(Rating: 2)</em></li>
                <li>A critical evaluation of evaluations for long-form question answering <em>(Rating: 2)</em></li>
                <li>Compositional preference models for aligning lms <em>(Rating: 1)</em></li>
                <li>Fine-grained human feedback gives better rewards for language model training <em>(Rating: 1)</em></li>
                <li>All that's 'human' is not gold: Evaluating human evaluation of generated text <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6240",
    "paper_id": "paper-8a9f75692f946250c70c50b14b71875bf06b2270",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "Crowd vs Expert annotations",
            "name_full": "Comparison between crowdsourced annotators and author 'expert' annotations",
            "brief_description": "The paper directly compares binary error annotations from crowdworkers with a subset of 'expert' annotations produced by the authors, showing systematic under-detection of factuality and inconsistency by crowdworkers and quantified inter-annotator agreement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Open-ended text generation (summarization, product descriptions, how-to instructions across Curation, Amazon product descriptions, WikiHow)",
            "llm_judge_model": null,
            "human_evaluation_setup": "Crowdworkers (Prolific native English speakers, 100% prior approval) provided binary error annotations and 1-5 overall quality ratings. Section 2: 900 distinct outputs, ~4,440 annotations, 4 annotations per output aggregated (mode then mean for errors; mean for overall). Quality control included 200 duplicate pairs and distractors; Gwet's AC1 reported (0.64 factuality to 0.94 refusal). Authors produced a subset of 300 expert annotations used as an approximate ground truth.",
            "metrics_compared": "Binary error rates for defined error types (Harmful, Fluency, Scope, Repetition, Refusal, Formatting, Relevance, Factuality, Inconsistency, Contradiction); overall quality ratings (1-5); inter-annotator agreement (Gwet's AC1); differences between crowd and expert error rates (delta).",
            "reported_differences": "Crowdworkers systematically underestimate factuality and inconsistency error rates versus author 'experts'; Gwet's AC1 for factuality was 0.64 (lower than for refusal at 0.94), indicating greater subjectivity/difficulty. Aggregated overall preference scores under-weight factuality and inconsistency relative to other errors (Lasso regression shows lower weight for factuality/inconsistency; refusal strongest contributor).",
            "llm_specific_limitations": null,
            "notable_failure_cases": "Crowd annotations marked lower factuality/inconsistency detection compared to experts, especially for assertive outputs (see Section 3); distractor examples (same-model output but different input) were incorrectly penalized for factuality and contradiction by crowdworkers despite factuality being input-independent.",
            "mitigation_strategies": "Use curated, trained, incentivized annotators or domain experts for critical error types; collect multiple annotators and use careful aggregation (e.g., jury learning); provide precise instructions and training examples to reduce ambiguity.",
            "uuid": "e6240.0",
            "source_info": {
                "paper_title": "Human Feedback is not Gold Standard",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Assertiveness confound",
            "name_full": "Effect of output assertiveness (confidence) on human error judgments",
            "brief_description": "The paper shows that the perceived assertiveness of model outputs biases crowd annotators' judgments: more assertive outputs are rated higher quality and are less likely to be flagged for factuality or inconsistency even when experts identify errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Open-ended text generation (same datasets as above)",
            "llm_judge_model": null,
            "human_evaluation_setup": "Section 3: 1,500 distinct outputs with preambles to induce Assertiveness--/baseline/Assertiveness++ and Complexity--/Complexity++; separate annotator pools rated overall quality (1-5), binary error annotations, and perceived assertiveness/complexity (1-5). Authors also produced 300 expert annotations per error type as a reference.",
            "metrics_compared": "Crowd vs expert binary error rates for factuality/inconsistency across preamble conditions; perceived assertiveness and complexity ratings (1-5); correlation between assertiveness and overall quality (Pearson r=0.68); change in refusal rates (e.g., low-assertiveness refusal rose from 3.5% to 24%).",
            "reported_differences": "Crowdworkers underestimated factuality and inconsistency errors relative to experts; the underestimation grew for high-assertiveness outputs and shrank for low-assertiveness outputs. More assertive outputs received higher overall quality scores and were less likely to be flagged for errors by crowds even when the 'expert' annotations found errors.",
            "llm_specific_limitations": null,
            "notable_failure_cases": "Assertive but incorrect outputs ('confidently wrong') escape detection by crowd annotators; in distractor scenarios annotators penalized factuality/contradiction inconsistently, suggesting first-impression bias tied to assertiveness. Low-assertiveness outputs were sometimes shorter with fewer factual assertions, producing divergent trends between crowd and expert judgments.",
            "mitigation_strategies": "Measure and control for assertiveness (and complexity) in evaluation pipelines; hide style-control preambles from annotators (as done), train annotators to focus on factual checks independent of tone, use expert review for factuality-sensitive tasks, and incorporate objective fact-checking tools or targeted verification checks alongside human judgment.",
            "uuid": "e6240.1",
            "source_info": {
                "paper_title": "Human Feedback is not Gold Standard",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Priming effect (biased vs unbiased)",
            "name_full": "Priming of overall quality scores by asking annotators to check errors first",
            "brief_description": "The study shows that asking annotators to check for specific error types before rating overall quality increases the correlation between those error types and the overall score, indicating a priming/anchoring effect.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Open-ended text generation (same datasets)",
            "llm_judge_model": null,
            "human_evaluation_setup": "Two separate annotator groups: (a) 'biased' annotators who were shown error criteria and annotated errors before overall rating, and (b) 'unbiased' annotators who only provided overall quality ratings. Correlations between error criteria and overall scores were compared (Figure 7).",
            "metrics_compared": "Pearson correlations between overall scores and each error factor for the 'biased' and 'unbiased' annotator groups; differences in correlation magnitudes as evidence of priming.",
            "reported_differences": "Correlations between overall quality and each error criterion are higher for the 'biased' group (annotators primed by error checklist), confirming that prompting annotators to look for specific errors changes their subsequent overall ratings.",
            "llm_specific_limitations": null,
            "notable_failure_cases": "Primed annotators overweight the specified error types when forming overall judgments, potentially skewing comparisons between models if annotation protocols differ.",
            "mitigation_strategies": "Avoid asking the same annotators to perform error-spotting and provide overall ratings in the same session when unbiased overall judgments are desired; separate annotation tasks, randomize order, provide balanced training, and explicitly calibrate annotators to mitigate priming.",
            "uuid": "e6240.2",
            "source_info": {
                "paper_title": "Human Feedback is not Gold Standard",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "RLHF -&gt; assertiveness",
            "name_full": "Preliminary evidence that RLHF-style human preference training increases model assertiveness",
            "brief_description": "The paper presents preliminary, model-level comparisons suggesting that models trained with on-policy RLHF (Llama 2 Chat) show higher assertiveness for equivalent human-rated quality than models fine-tuned off-policy on preference data (Command), implying RLHF may disproportionately amplify assertiveness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Open-ended text generation (same datasets)",
            "llm_judge_model": null,
            "human_evaluation_setup": "Cross-model comparison of mean overall quality scores (crowd) vs mean assertiveness ratings, grouped by model and preamble type (Figure 6). Models included Llama 2 13B Chat (on-policy RLHF), Command 52B/6B (off-policy fine-tuned on preference scores), Falcon, MPT, etc.",
            "metrics_compared": "Mean overall quality (1-5) vs mean assertiveness ratings; qualitative excerpts demonstrating stronger style changes in RLHF-trained models; comparisons of response stability across preambles.",
            "reported_differences": "For the same quality score, Llama 2 outputs had higher assertiveness ratings than other models (e.g., Command 52B), suggesting RLHF may have increased assertiveness more than it increased perceived quality. The RLHF-trained model also showed greater variability in following style preambles (higher assertiveness variation) but less stability in non-baseline preambles.",
            "llm_specific_limitations": "RLHF-trained models may prioritize perceived compliance/appeal to annotators (increasing assertiveness) over factual utility, risking 'confidently wrong' behavior and sycophancy. The paper notes training details aren't strictly comparable and evidence is preliminary.",
            "notable_failure_cases": "Extracts show dramatic style shifts in the RLHF model (e.g., very assertive/authoritative responses) that could mask factual errors and mislead users; optimizing for preference signals can unintentionally favor assertiveness at the expense of truthfulness.",
            "mitigation_strategies": "Design training objectives that decompose preference into interpretable components (e.g., penalize over-assertiveness), include truthfulness/factuality-specific checks or rewards, evaluate models for 'humility' (high quality + low assertiveness) and prefer such models, and use controlled comparisons with equivalent base models to isolate RLHF effects.",
            "uuid": "e6240.3",
            "source_info": {
                "paper_title": "Human Feedback is not Gold Standard",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Model-written evaluations (mention)",
            "name_full": "Model-written / LLM-as-evaluator evaluations (referenced work)",
            "brief_description": "The paper cites work on using models to write evaluations of other models (e.g., Perez et al. 2023), noting model-written evaluations as related work but does not itself run LLM-as-a-judge experiments.",
            "citation_title": "Discovering language model behaviors with model-written evaluations",
            "mention_or_use": "mention",
            "task_domain": "General behavior discovery and evaluation of LLMs (as referenced)",
            "llm_judge_model": null,
            "human_evaluation_setup": "Not performed in this paper; cited as related work (Perez et al., 2023) that uses model-generated evaluations.",
            "metrics_compared": "Not reported in this paper; the citation points to model-written evaluation methodology as an alternative to human annotation.",
            "reported_differences": "This paper does not present new empirical comparisons between model-written evaluations and human annotations, but raises the concern that automated/reward-model approximations of human feedback may introduce additional biases beyond those identified in human annotation.",
            "llm_specific_limitations": "Paper warns generally that reward models or automated proxies used to approximate human feedback could introduce additional, unspecified biases; notes human annotations themselves are imperfect, so models trained to mimic them inherit those biases.",
            "notable_failure_cases": "No direct failure cases for LLM-as-judge are presented here, only cautionary statements and referencing of related findings (e.g., models can be convincingly wrong and mislead humans).",
            "mitigation_strategies": "Recommend caution when using reward models or model-written evaluations; combine multiple verification strategies (trained annotators, expert review, jury-style aggregation), and investigate biases introduced by automated evaluators in future work.",
            "uuid": "e6240.4",
            "source_info": {
                "paper_title": "Human Feedback is not Gold Standard",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Discovering language model behaviors with model-written evaluations",
            "rating": 2
        },
        {
            "paper_title": "A critical evaluation of evaluations for long-form question answering",
            "rating": 2
        },
        {
            "paper_title": "Compositional preference models for aligning lms",
            "rating": 1
        },
        {
            "paper_title": "Fine-grained human feedback gives better rewards for language model training",
            "rating": 1
        },
        {
            "paper_title": "All that's 'human' is not gold: Evaluating human evaluation of generated text",
            "rating": 1
        }
    ],
    "cost": 0.01397025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Human Feedback is Not Gold Standard</h1>
<p>Tom Hosking<br>University of Edinburgh<br>tom.hosking@ed.ac.uk</p>
<p>Phil Blunsom<br>Cohere<br>phil@cohere.com</p>
<p>Max Bartolo<br>Cohere, UCL<br>max@cohere.com</p>
<h4>Abstract</h4>
<p>Human feedback has become the de facto standard for evaluating the performance of Large Language Models, and is increasingly being used as a training objective. However, it is not clear which properties of a generated output this single 'preference' score captures. We hypothesise that preference scores are subjective and open to undesirable biases. We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality. We further hypothesise that both preference scores and error annotation may be affected by confounders, and leverage instruction-tuned models to generate outputs that vary along two possible confounding dimensions: assertiveness and complexity. We find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are not a fully reliable evaluation metric or training objective. Finally, we offer preliminary evidence that using human feedback as a training objective disproportionately increases the assertiveness of model outputs. We encourage future work to carefully consider whether preference scores are well aligned with the desired objective.</p>
<h2>1 INTRODUCTION</h2>
<p>The fluency exhibited by Large Language Models (LLMs) has reached the point where rigorous evaluation of LLM capabilities is very challenging, with the quality of model outputs often now exceeding that of reference examples from datasets (Zhang et al., 2023; Clark et al., 2021). A great advantage of LLMs is their flexibility, but this makes it difficult to design an all-purpose evaluation metric Novikova et al. (2017). Benchmarks have proven useful for model comparisons Gehrmann et al. (2021); Liang et al. (2023), but for open-ended generation tasks human evaluation using a single overall score has become the de facto standard method Ouyang et al. (2022); Touvron et al. (2023). For a given input prompt, samples or responses from models are shown to annotators, who are asked to score the responses according to their quality (Novikova et al., 2018). These scores can either be absolute ratings, or relative preference scores whereby two responses are ranked by quality.</p>
<p>Although the simplicity of a single overall score is appealing, it obscures the decision making process used by annotators, including any trade-offs or compromises, and does not explain why one response or model is better than another. Annotators look for shortcuts to make the task easier (Ipeirotis et al., 2010), and so are more likely to base their judgement on superficial properties (e.g., fluency and linguistic complexity) than aspects that require more effort to check (e.g., factuality).</p>
<p>Previously, human evaluation of natural language generation systems has considered multiple aspects of the generated output. However, the criteria used are often unique to the specific task being considered (van der Lee et al., 2021; Hosking et al., 2022; Xu \&amp; Lapata, 2022), making them difficult to apply to LLMs. With recent rapid improvement in system performance, it is important to test whether preference scores capture the desired aspects of output quality, and whether they provide a gold standard objective for evaluating or training LLMs.</p>
<p>In this work, we analyse human annotation of model outputs, both for overall preference scores and for specific error criteria. In Section 2 we establish a set of error types that are task independent and act as minimum requirements for model outputs. We analyse the error coverage of overall preference scores. We ask two sets of annotators to rate a range of LLM outputs, the first according to these error types and the second according to their own judgements of overall quality, and find</p>
<p>that overall preference scores under-represent factuality and faithfulness. In Section 3, we consider two possible sources of bias when annotating for specific error types by generating outputs with varying assertiveness and complexity, and find that assertiveness strongly biases human factuality judgements. Finally, in Section 4 we offer some preliminary evidence that using human preference scores as a training objective disproportionately increases the assertiveness of model outputs. We present additional findings from our collected data in Appendix E: we confirm that annotators are subject to a priming effect; we analyse the variation of quality scores with response length; and we show that generated outputs are preferred to the reference responses. Our code and data are available at https://github.com/cohere-ai/human-feedback-paper.</p>
<h1>2 ARE PREFERENCE SCORES RELIABLE?</h1>
<p>To check whether a single preference score is a useful objective with good coverage, we first establish a minimum set of requirements for model outputs. These error types are both generic enough that they are task agnostic and widely applicable, but also sufficiently well-specified that it is possible for annotators to judge them. We begin with the factors identified by Xu et al. (2023c), who asked crowdworkers and experts to rate model outputs and give justifications for their scores, removing those factors that are overly subjective (e.g., ease of understanding). We also draw inspiration from Grice's Maxims (Grice, 1991) regarding felicitous communication between speakers: the Maxim of Quantity implies that repetition is undesirable, the Maxim of Quality prohibits factual errors, and so on. Finally, we considered factors that users care about when using LLMs in production environments (e.g., refusal to answer). We therefore consider the following error types:</p>
<ul>
<li>Harmful - Is the response unsafe, harmful or likely to cause offence in some way?</li>
<li>Fluency - Is the response grammatically incorrect, or does it contain spelling mistakes?</li>
<li>Scope - Does the response exceed the scope limits of a chatbot? Does the response give opinions or otherwise act as if it is a person, or offer to take actions that it cannot (e.g. make a call, access the internet)?</li>
<li>Repetition - Does the response repeat itself? For example, if there is a list in the response, are any items repeated? Does the response reuse the same phrase again and again?</li>
<li>Refusal - If the request is reasonable, does the response refuse to answer it (e.g. "I'm sorry, I can't help you with that")?</li>
<li>Formatting - Does the response fail to conform to any formatting or length requirements from the prompt?</li>
<li>Relevance - Does the response go off topic or include information that is not relevant to the request?</li>
<li>Factuality - Is the response factually incorrect (regardless of what the request said)?</li>
<li>Inconsistency - Does the response incorrectly represent or change information from the request? This criterion is often also referred to as faithfulness.</li>
<li>Contradiction - Is the response inconsistent with itself, or does it contradict itself?</li>
</ul>
<h3>2.1 EXPERIMENTAL SETUP</h3>
<p>We ask crowdworkers to evaluate model outputs, marking each example with a binary yes or no to denote whether an error is present. Separately, we ask a different set of annotators to rate the overall quality of the same outputs from 1 to 5 , according to whatever criteria they feel are important.</p>
<p>Datasets To cover a range of different tasks for which evaluation is challenging, we construct input prompts from three datasets: Curation Corpus (Curation, 2020) is a summarization dataset composed of 40,000 news articles and professionally written summaries; Amazon Product Descriptions ( Ni et al., 2019) gives a product title and specification as input and requires generating a compelling product description; and Wikihow (Koupaee \&amp; Wang, 2018) consists of 'how to' questions and step-by-step guides. Full details of the prompt templates used can be found in Appendix C.</p>
<p>Models While a comparison of different models is not the focus of this work, we nonetheless source responses from multiple performant models that we were able to access at time of writing: MPT 30B Instruct is fine-tuned on Dolly DDRLHF and additional datasets (MosaicML NLP Team, 2023; Conover et al., 2023); Falcon 40B instruct is fine-tuned on a subset of Baize (Almazrouei et al.,</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Weightings for each criteria under a Lasso regression model of overall scores. Almost all the criteria contribute to the overall scores, with refusal contributing most strongly.</p>
<p>2023; Xu et al., 2023b); and Command 6B and 52B are commercial models trained by Cohere, fine tuned on proprietary datasets. We additionally include the reference outputs for each input. Details of the models, prompt templates and sampling hyperparameters can be found in Appendix D.</p>
<p>Annotation We source crowdworkers from Prolific, requiring them to be native English speakers with 100% approval ratings from prior tasks. Our annotation interface is based on Potato (Pei et al., 2022). Our annotation protocol is based on findings from RankME (Novikova et al., 2018) that showed the best inter-annotator agreement is achieved when annotators are shown multiple outputs for a given input, and scores are collected as absolute ratings. We expect that showing annotators five full outputs at once would lead to higher cognitive load and lower annotator engagement, therefore we collect ratings for two outputs at a time, pairing each output with an output from one of the other four models. The resulting four annotations per output are aggregated by taking the mean for overall scores, and by taking the mode (and then the mean in case of ties) for error annotations. We annotate a total of 900 distinct outputs, with a total of 4,440 annotations including quality checks.</p>
<p>Quality Control In order to check inter-annotator agreement, we collect 5 duplicate annotations for a random subset of 200 pairs of outputs. We also include a set of distractor examples, where a response is shown in context with an output from the same model but a different input. These examples act as an attention check; the response based on a different input should consistently be penalised along criteria like relevance and usefulness.</p>
<p>We find that distractor outputs are correctly rated lower than the other output in the pair over 97% of the time, indicating that the vast majority of annotators paid attention to the task. We use Gwet's AC1 measure (Gwet, 2014) to assess inter-annotator agreement for the multiply annotated examples, finding good agreement scores of between 0.64 (for Factuality) and 0.94 (for Refusal). The disparity indicates that annotators found some error types more difficult or subjective than others; refusal is straightforward to detect, whereas checking for factual errors involves significantly more effort.</p>
<h1>2.2 RESULTS</h1>
<p>Preference scores under-represent factuality and inconsistency In order to determine the degree to which each error type was captured by the overall scores, we fit a Lasso regression model (Tibshirani, 1996) with $\alpha=0.01$ between the scores and the error ratings. Figure 1 shows the weights of each criterion under this model, where each weight corresponds to the expected reduction in overall score if the corresponding error is present. Six out of ten error types contribute to the overall scores, with refusal errors contributing most strongly. Factuality and inconsistency errors both contribute but with much lower weighting, indicating that a single preference score is likely to obscure failures in these important criteria.</p>
<p>We note that the error types that do not contribute were also the rarest (occurring in less than $1 \%$ of outputs). We would expect that harmfulness and fluency should influence overall scores in general, but in our experiments the models are sufficiently strong and the tasks sufficiently well-posed that such errors are infrequent.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Difference in annotated error rates for distractor examples (outputs from the same model but different input). Some error types are correctly unchanged (e.g., repetition, refusal) while relevance and inconsistency are correctly penalised. Factuality and contradiction are both incorrectly penalised (they are independent of the input), indicating that annotators struggled to fully disentangle these criteria.</p>
<p>Annotators struggle with disentangling factors Recall that the distractor examples are pairs of outputs sourced from the same model, but where one of the outputs corresponds to a different input; these should therefore achieve comparable scores for criteria that are independent of the input prompt (e.g., fluency, detail, factuality ${ }^{1}$ ) but be heavily penalized for other factors such as relevance and overall quality. The results in Figure 2 show that although this expectation holds in some cases (repetition, refusal and formatting are not penalized, while relevance and inconsistency are), other factors are incorrectly penalized; factuality and contradiction (within the output) are both rated worse for the distractor examples. This implies that annotators found it difficult to disentangle these criteria from the overall quality of a response.</p>
<p>Although annotators are shown the instructions and error criteria before the input prompt and responses, we suspect that they subconsciously form an opinion about the quality of the response based on first impressions (Smith et al., 2014), and that this opinion influences their judgement of each error type. In other words, an annotator may decide that a response is bad, and decide that it is more likely to contain errors as a result. This effect could be partially mitigated by specifying precise instructions, giving multiple examples and training a knowledgeable group of annotators. However, there is always potential for ambiguity.</p>
<h1>3 ARE ANNOTATIONS AFFECTED BY CONFOUNDERS?</h1>
<p>We have so far considered the effect of important error criteria on overall preference scores, but the annotations for the errors were themselves given by human annotators. The results for distractor examples in Figure 2 indicate that granular ratings may also be subject to biases. Firstly, we hypothesise that the assertiveness of a text influences human judgements; a statement conveyed confidently as fact is more likely to be interpreted as true. Similarly, text that uses complex language might lead an annotator to believe that the communicator behind it is intelligent and knowledgeable, and therefore that the content is true. This concept of language ideology, where the style and tone of a speaker leads to biased judgements about their trustworthiness and intelligence, has been extensively studied in the context of speech (Campbell-Kibler, 2009; Woolard, 2020), but we are not aware of any work in the context of model evaluation.</p>
<h3>3.1 EXPERIMENTAL SETUP</h3>
<p>We generate model outputs from the same datasets as Section 2, but using an additional preamble ${ }^{2}$ to vary the tone of the output and create outputs with both high and low assertiveness and high and low linguistic complexity. We constructed these preambles by iterative testing, with the aim of eliciting</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Human ratings of assertiveness, complexity and overall quality for each preamble type. The ratings indicate that the preambles successfully modify the output in the desired manner, although there is some correlation between perceived assertiveness and complexity. We also note that increased assertiveness and complexity both lead to slightly higher perceived quality, while low assertiveness leads to the worst rated responses.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The difference in error rates between crowdsourced annotations and 'expert' annotations from the authors, excluding samples that were marked as refusing to respond. Annotators tend to underestimate the rate of inconsistency or factuality errors, and they are less likely to spot these errors in outputs that are assertive.
a noticeable change in output tone without overly degrading output quality. The full text used for the preambles is as follows:</p>
<ul>
<li>Assertiveness- Respond in a cautious, defensive and uncertain way, as if you are unfamiliar with the topic.</li>
<li>Assertiveness++ Respond authoritatively, assertively and persuasively, as if you are very knowledgeable about the topic.</li>
<li>Complexity- Respond using only short words and simple language, as if you were talking to a child.</li>
<li>Complexity++ Respond using complex language, long words and technical terms, as if you are an expert.
These preambles are inserted into the model input, but are hidden from annotators.
We use a similar annotation setup to Section 2.1, collecting overall scores from 1 to 5 from one group of annotators, and binary error annotations from a second group ${ }^{3}$. Additionally, we collect</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>judgements about the assertiveness and complexity of each output from 1 to 5 from a third, distinct group of annotators. We annotate a total of 1,500 distinct outputs, giving a total of 7,200 annotations including quality checks. Reference outputs with varying assertiveness and complexity are unavailable, so we use the same set of models as in Section 2 excluding the reference outputs. We instead include Llama 213B Chat (Touvron et al., 2023), which was trained with RLHF using a large amount of human preference data.</p>
<p>It is possible that the preambles might lead to changes in the true error rates of the output (Xu et al., 2023a). The authors therefore carefully annotate a subset of 300 examples for each error type, to act as a set of 'expert' annotations. Although not strictly an unbiased set of ratings, this subset acts as a useful estimate of the true error rates.</p>
<h1>3.2 RESULTS</h1>
<p>Confidence and complexity can be varied using preambles We first confirm that our preambles successfully change the model outputs in the desired way. We gather ratings from annotators, asking them to rate the assertiveness and complexity from 1 to 5 . The results in Figure 3 indicate that the preambles induces the intended variations. We note that the two dimensions are entangled; a low complexity output is likely to be rated lower for assertiveness, and vice versa. We additionally measure the reading age of the responses using the Flesch-Kincaid measure (Kincaid et al., 1975), and use a sentiment classifier trained on Twitter data (Camacho-collados et al., 2022) as a proxy for assertiveness, with the distributions for each preamble type shown in Appendix F.</p>
<p>Factuality judgements are biased by assertiveness The low assertiveness preamble leads to a significant increase in refusal errors, from $3.5 \%$ in the baseline case to $24 \%$. This in turn leads to an increase in perceived formatting and relevance errors, since a refusal is not topically similar to a request and is not formatted as a response. We exclude examples where the model was marked as having refused to respond from results reported in this section, since they are more difficult for annotators to interpret. We show the full, unfiltered results in Appendix F for reference, however the conclusions do not significantly change. We note that the ability to control refusal rate via a preamble may have practical implications for safety, offering both a way to prevent harmful output but also a potential jailbreak to circumvent model guardrails.</p>
<p>Figure 4 shows the difference in annotated error rates between crowdsourced annotators and the 'experts', broken down by preamble type. Crowdworkers underestimate the rate of factuality and inconsistency errors. This difference is increased for high assertiveness responses, and decreased for low assertiveness responses. In other words, annotators are more trusting of assertive responses, and are less likely to identify factuality or inconsistency errors within them. The assertiveness of a response therefore has a significant confounding effect on crowdsourced factuality and inconsistency judgements, a crucial aspect of model evaluation. Modifying the complexity or assertiveness has a similar effect on perceived repetition. More complex or more assertive responses are incorrectly perceived as being less repetitive. Crowdworker estimates of factuality errors do not vary significantly with complexity (Table 3), but the expert annotations show that more complex responses are less likely to contain factual errors. Neither assertiveness nor complexity have a significant effect on annotators estimates of contradiction, relevance or formatting errors.</p>
<p>Surprisingly, the crowdsourced estimate of the factuality error rate for the 'low assertiveness' group is higher than the baseline, while the 'expert' estimate is lower (Table 3). Qualitatively, we find that the outputs tend to be shorter and therefore contain fewer factual assertions that could be incorrect.</p>
<p>Figure 5 shows the annotated error rates for all preamble types, grouped by assertiveness rating, demonstrating that error rates are strongly related to perceived assertiveness. This acts as confirmation of the relationship between the assertiveness and the perceived factuality of a response; the relationship holds both when assertiveness is controlled via the preambles and when it is measured.</p>
<h2>4 ARE HUMAN PREFERENCES A GOOD TRAINING OBJECTIVE?</h2>
<p>Perceived quality is correlated with assertiveness Assertiveness is strongly positively correlated with overall quality scores, with a Pearson correlation coefficient of 0.68 , while complexity is somewhat correlated, with a coefficient of 0.53 . It is difficult to determine the causal direction of this</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Variation in crowdsourced error rates with assertiveness. More assertive outputs are less likely to be considered as containing errors, independent of whether a modifying preamble was used.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Quality against Assertiveness, grouped by model and preamble type, with the trendlines for Command 52B and LLama 2 13B. Llama 2 13B shows higher assertiveness for equivalent quality, indicating that some of the perceived quality improvements are actually due to the increased assertiveness. Command 52B seems to be the most 'humble', exhibiting lower assertiveness for a given output quality.
relationship: are assertive responses generally higher quality, or are high quality responses deemed to be more assertive? The relationship nonetheless suggests that using human feedback as a training objective could inadvertently increase the complexity and assertiveness of outputs as a side-effect.</p>
<p>RLHF may disproportionately increase assertiveness Figure 6 shows the mean quality scores against mean assertiveness ratings for all models tested, grouped by model and preamble type. In general, higher assertiveness leads to higher quality scores. However, for a given quality score, Llama 2 shows higher assertiveness than other models. Additionally, Llama 2 appears to be better at following 'style' requests (evidenced by the higher variation in assertiveness scores), but at the cost of stability (overall quality scores for non-baseline preambles tend to decrease more strongly compared to other models).</p>
<p>Although the training details for the models are not directly comparable, the primary difference between them is that Command was fine-tuned on preference scores, while Llama 2 was trained using on-policy RLHF Touvron et al. (2023). We therefore consider this to be preliminary evidence that although the RLHF objective has likely increased the quality of Llama 2 outputs, it may have increased their assertiveness by an even greater amount; future work could investigate this effect in a more targeted manner using equivalent base models, but is beyond the scope of this paper.</p>
<p>We show some extracts of output from Command 52B and Llama 2 13B Chat using each of the assertiveness preambles in Table 1; the changes in style for the RLHF-trained model are much more dramatic. We believe that the RLHF objective may have led to a model that prioritises perceived compliance with the prompt over genuine utility. Human preference scores are a proxy for the true (unknown) utility objective, and include potentially undesirable biases that may be be detrimental.</p>
<p>Table 1: Extracts of outputs, from Command 52B (trained off-policy) and Llama 213B Chat (onpolicy), truncated for brevity. The style changes for the RLHF model are noticeably stronger.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input Prompt</th>
<th style="text-align: center;">Can you tell me how to make sloe gin?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">Sloe gin is a delicious and easy-to-make drink. Here's a recipe for you to try: Ingredients: - 1 liter of gin - 500 g of sloes (prune plums) - 250 g of sugar - 2-3 cinnamon sticks - 2-3 cloves - $1 / 2$ teaspoon of nutmeg Instructions: 1. Wash the sloes and remove any leaves or stems. 2. Prick each sloe with a pin to allow the gin to penetrate the fruit. 3. Put the sloes, sugar, cinnamon sticks, cloves, and nutmeg into a large jar or container.</td>
</tr>
<tr>
<td style="text-align: center;">Assertiveness--</td>
<td style="text-align: center;">I don't know much about making sloe gin, but I can tell you how to make regular gin. To make gin, you need to mix grain alcohol with juniper berries and other spices. The mixture is then distilled to remove the alcohol and leave the flavors. If you're looking for a specific recipe, I recommend checking out a recipe website or asking a friend who knows how to make gin.</td>
</tr>
<tr>
<td style="text-align: center;">Assertiveness++</td>
<td style="text-align: center;">Sure! Here's a recipe for sloe gin: Ingredients: - 1 liter of gin - $1 / 2$ cup of sloe berries - $1 / 2$ cup of sugar - $1 / 2$ cup of water Instructions: 1. Combine the sloe berries, sugar, and water in a saucepan and bring to a boil. 2. Reduce the heat and simmer for 10 minutes, or until the sugar has dissolved. 3. Remove from the heat and let cool.</td>
</tr>
<tr>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">Hello! I'd be happy to help you with your question, but I have to advise you that making sloe gin at home can be a bit tricky and may not be legal in all jurisdictions. Additionally, some sloe gin recipes may contain harmful or toxic ingredients, so I would advise against attempting to make it without proper knowledge and equipment. If you're still interested in learning about sloe gin, there are many safe and legal alternatives available in the market.</td>
</tr>
<tr>
<td style="text-align: center;">Assertiveness--</td>
<td style="text-align: center;">Oh my, I'm not sure if it's a good idea for me to provide information on how to make sloe gin. I don't want to encourage any harmful or illegal activities, and I couldn't possibly provide guidance on something I don't fully understand. Sloe gin can be a complex and potentially dangerous drink, and I wouldn't want anyone to get hurt or in trouble. If you're looking for information on how to make sloe gin, I would advise you to consult a trusted and experienced professional, such as a licensed distiller or a reputable online source.</td>
</tr>
<tr>
<td style="text-align: center;">Assertiveness++</td>
<td style="text-align: center;">Ah, sloe gin! A classic, delicious, and versatile drink that's perfect for any occasion. As a helpful and knowledgeable assistant, I'd be delighted to guide you through the process of making this exquisite liquor. Please note that I'll be providing information on a general basis, and it's always best to consult with a medical professional before consuming any alcohol. To begin, you'll need to gather the following ingredients: 1 . Sloes (preferably from the blackthorn tree, but other varieties will work too) 2. Gin (a London Dry Gin is ideal) 3. Sugar (preferably granulated) 4. Water 5. A large glass jar with a tight-fitting lid Now, let's dive into the process!</td>
</tr>
</tbody>
</table>
<p>Assertiveness and quality can be decoupled Although assertiveness and quality are strongly connected, Figure 6 also shows that their relationship varies by model. Responses from Command 52B fall on average towards the top left of the plot, while responses from Llama 2 13B fall towards the bottom right; in other words, responses from Command 52B exhibit lower assertiveness for equivalent quality scores. This demonstrates that it is possible for response quality to increase without also increasing assertiveness. Although it is unclear whether it is possible to completely decouple these aspects, we argue that 'humble' models, rated both high for quality and low for assertiveness, should be considered more desirable than their 'confidently wrong' counterparts.</p>
<h1>5 Related Work</h1>
<p>Natural language generation systems have previously been evaluated according to more detailed criteria than overall quality, but these have generally been task specific (e.g., fluency, meaning preservation and diversity for paraphrasing, succinctness and coherence for summarization; Hosking et al., 2022; Xu \&amp; Lapata, 2022). van der Lee et al. (2021) and Howcroft et al. (2020) performed surveys of human evaluation in NLG, and found wide variations both in choice of criteria and in annotation</p>
<p>protocols. Wang et al. (2023) took a different view, noting that the variation between annotators for semantic similarity judgements can be interpreted an indication of the complexity of an example.</p>
<p>There has been recent interest in granular evaluation of LLMs as a means of enabling model development and error checking. Thoppilan et al. (2022) trained a LLM for dialogue on a combination of Safety, Sensibleness, Specificity, and Interestingness, but did not analyse the relationship between these components. Xu et al. (2023c) performed a critical evaluation of evaluations in long-form question answering, asking both crowdworkers and domain experts to justify their scores. We take inspiration from their work in choosing our error criteria, but note that this kind of 'introspection' is unlikely to fully reveal annotators' biases. Wu et al. (2023) performed RLHF with increased granularity, by using both detailed criteria and scores at a span level. Ye et al. (2023) proposed breaking down evaluation of LLMs according to a set of 'skills', which have some overlap with our error criteria but are less concretely defined. Go et al. (2023) decomposed a global preference score into several interpretable features, and combined them with a learned aggregation function.</p>
<p>Liu et al. (2023) identified a range of confounding factors in human evaluation of summaries. Kabir et al. (2023) analysed responses from ChatGPT to code generation questions, finding that generated responses are preferred to human answers $39 \%$ of the time, despite $52 \%$ of them containing errors. Similar to our findings, they attribute this preference to the verbose and 'chatty' style of the generated responses. Perez et al. (2023) identified similar 'inverse-scaling' behaviour, where larger models exhibit worse sycophancy. Sharma et al. (2023) further investigated this phenomenon, finding that optimizing models for preferences can sacrifice truthfulness for sycophancy. Si et al. (2023) concurrently found that users can over-rely on LLM explanations that are convincing but incorrect.</p>
<p>In sociolinguistics, there has been interest in how the social and cultural properties of a speaker affect their perception. The framework of 'language ideology' considers the link between language and the cultural conceptions around its use (Woolard, 2020). Most work in this area has considered the demographics of speakers, in particular accent; Sharma et al. (2022) investigated the perceived prestige of different British accents, Campbell-Kibler (2009) researched the effect of linguistic variation on perceptions of intelligence, while Lev-Ari \&amp; Keysar (2010) found that non-native speakers of language are viewed as less credible. Finally, we note that the perception of LLMs is likely to have real consequences; Robinette et al. (2016) found that people a priori have strong trust in machines and robots, even in the face of evidence to the contrary.</p>
<h1>6 CONCLUSION</h1>
<p>We present an analysis of human feedback for LLM outputs, and find that although overall human preference scores capture a wide range of error types, they under-represent some important aspects such as factuality and inconsistency. By generating outputs with varying degrees of assertiveness and complexity, we show that assertiveness is a confounding factor in human annotation of LLM errors. Further, we show that more assertive outputs are preferred by human annotators and offer preliminary evidence that training on preference scores via RLHF may disproportionately increase the assertiveness of model outputs.</p>
<p>Overall, our analysis shows that human feedback is not the gold standard that it is generally perceived to be. Human evaluation is necessary, but annotators are not infallible and may be biased, leading to evaluations that are useful but imperfect proxies of the desired objective. A pleasing response is not necessarily a useful one. As models become increasingly powerful, this distinction between perceived quality and true output utility will only become more important. Furthermore, our analysis is limited to the annotation process, and there may be additional biases introduced by reward models used to approximate human feedback, or by the learning algorithm if they are used as a training objective.</p>
<p>However, all is not lost; we believe that the issues we identify may be at least partially mitigated by using a curated pool of trained and incentivized annotators, or by using multiple annotators and careful aggregation (e.g. using jury learning, Gordon et al., 2022). It may also be possible to more directly measure, and optimize for, desired model properties such as utility under realworld conditions. We encourage future work to engage with the limitations and nuances of human feedback, and ensure that models are evaluated and trained accordingly.</p>
<h1>REFERENCES</h1>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance, 2023.</p>
<p>John A. Bargh and Tanya L. Chartrand. The Mind in the Middle: A Practical Guide to Priming and Automaticity Research, pp. 311-344. Cambridge University Press, 2 edition, 2014. doi: 10.1017/CBO9780511996481.017.</p>
<p>Jose Camacho-collados, Kiamehr Rezaee, Talayeh Riahi, Asahi Ushio, Daniel Loureiro, Dimosthenis Antypas, Joanne Boisson, Luis Espinosa Anke, Fangyu Liu, and Eugenio Martnez Cmara. TweetNLP: Cutting-edge natural language processing for social media. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38-49, Abu Dhabi, UAE, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-demos.5.</p>
<p>Kathryn Campbell-Kibler. The nature of sociolinguistic perception. Language Variation and Change, 21(1):135-156, 2009. doi: 10.1017/S0954394509000052.</p>
<p>Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. All that's 'human' is not gold: Evaluating human evaluation of generated text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 7282-7296, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.565. URL https://aclanthology.org/2021.acl-long.565.</p>
<p>Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world's first truly open instruction-tuned llm, 2023. URL https://www.databricks.com/blog/2023/04/ 12/dolly-first-open-commercially-viable-instruction-tuned-llm.</p>
<p>Curation. Curation corpus base, 2020.
Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondej Duek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, Joo Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pp. 96-120, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.gem-1.10. URL https://aclanthology.org/ 2021.gem-1.10.</p>
<p>Dongyoung Go, Tomasz Korbak, Germn Kruszewski, Jos Rozen, and Marc Dymetman. Compositional preference models for aligning lms, 2023.</p>
<p>Mitchell L. Gordon, Michelle S. Lam, Joon Sung Park, Kayur Patel, Jeffrey T. Hancock, Tatsunori Hashimoto, and Michael S. Bernstein. Jury learning: Integrating dissenting voices into machine learning models. In Simone D. J. Barbosa, Cliff Lampe, Caroline Appert, David A. Shamma, Steven Mark Drucker, Julie R. Williamson, and Koji Yatani (eds.), CHI '22: CHI Conference on Human Factors in Computing Systems, New Orleans, LA, USA, 29 April 2022 - 5 May 2022, pp. 115:1-115:19. ACM, 2022. doi: 10.1145/3491102.3502004. URL https://doi.org/10. $1145 / 3491102.3502004$.</p>
<p>P. Grice. Studies in the Way of Words. Harvard University Press, 1991. ISBN 9780674254206. URL https://books.google.co.uk/books?id=8r78DwAAQBAJ.
K.L. Gwet. Handbook of Inter-Rater Reliability, 4th Edition: The Definitive Guide to Measuring The Extent of Agreement Among Raters. Advanced Analytics, LLC, 2014. ISBN 9780970806284. URL https://books.google.co.uk/books?id=fac9BQAAQBAJ.</p>
<p>Tom Hosking, Hao Tang, and Mirella Lapata. Hierarchical sketch induction for paraphrase generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2489-2501, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.178. URL https://aclanthology.org/ 2022.acl-long. 178 .</p>
<p>Tom Hosking, Hao Tang, and Mirella Lapata. Attributable and scalable opinion summarization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8488-8505, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.acl-long. 473.</p>
<p>David M. Howcroft, Anya Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad Mahamood, Simon Mille, Emiel van Miltenburg, Sashank Santhanam, and Verena Rieser. Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions. In Proceedings of the 13th International Conference on Natural Language Generation, pp. 169-182, Dublin, Ireland, December 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.inlg-1.23.</p>
<p>Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang. Quality management on amazon mechanical turk. In Proceedings of the ACM SIGKDD Workshop on Human Computation, HCOMP '10, pp. 64-67, New York, NY, USA, 2010. Association for Computing Machinery. ISBN 9781450302227. doi: 10.1145/1837885.1837906. URL https://doi.org/10.1145/ 1837885.1837906 .</p>
<p>Samia Kabir, David N. Udo-Imeh, Bonan Kou, and Tianyi Zhang. Who answers it better? an in-depth analysis of chatgpt and stack overflow answers to software engineering questions, 2023.</p>
<p>Peter Kincaid, Robert P. Fishburne, Richard L. Rogers, and Brad S. Chissom. Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. 1975. URL https://api.semanticscholar.org/CorpusID: 61131325 .</p>
<p>Mahnaz Koupaee and William Yang Wang. Wikihow: A large scale text summarization dataset. CoRR, abs/1810.09305, 2018. URL http://arxiv.org/abs/1810.09305.</p>
<p>Shiri Lev-Ari and Boaz Keysar. Why don't we believe non-native speakers? The influence of accent on credibility. Journal of Experimental Social Psychology, 46(6):1093-1096, 2010. ISSN 1096-0465(Electronic),0022-1031(Print). doi: 10.1016/j.jesp.2010.05.025. Place: Netherlands Publisher: Elsevier Science.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R'e, Diana Acosta-Navas, Drew A. Hudson, E. Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan S. Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas F. Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. Annals of the New York Academy of Sciences, 1525:140 - 146, 2023. URL https://api.semanticscholar.org/CorpusID:253553585.</p>
<p>Yixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, and Dragomir Radev. Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</p>
<p>pp. 4140-4170, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.acl-long. 228.</p>
<p>MosaicML NLP Team. Introducing mpt-30b: Raising the bar for open-source foundation models, 2023. URL www.mosaicml.com/blog/mpt-30b. Accessed: 2023-06-22.</p>
<p>Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 188-197, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1018. URL https://aclanthology. org/D19-1018.</p>
<p>Jekaterina Novikova, Ondej Duek, Amanda Cercas Curry, and Verena Rieser. Why we need new evaluation metrics for NLG. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2241-2252, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1238. URL https://aclanthology. org/D17-1238.</p>
<p>Jekaterina Novikova, Ondej Duek, and Verena Rieser. RankME: Reliable human ratings for natural language generation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 72-78, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2012. URL https://aclanthology.org/N18-2012.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/blefde53be364a73914f58805a001731-Abstract-Conference.html.</p>
<p>Jiaxin Pei, Aparna Ananthasubramaniam, Xingyao Wang, Naitian Zhou, Apostolos Dedeloudis, Jackson Sargent, and David Jurgens. Potato: The portable text annotation tool. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2022.</p>
<p>Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Benjamin Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. Discovering language model behaviors with model-written evaluations. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 13387-13434, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.findings-acl.847.</p>
<p>Paul Robinette, Wenchen Li, Robert Allen, Ayanna M. Howard, and Alan R. Wagner. Overtrust of robots in emergency evacuation scenarios. In 2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 101-108, 2016. doi: 10.1109/HRI.2016.7451740.</p>
<p>Devyani Sharma, Erez Levon, and Yang Ye. 50 years of british accent bias: Stability and lifespan change in attitudes to accents. English World-Wide, 43(2):135-166, 2022. ISSN 0172-8865. doi: https://doi.org/10.1075/eww.20010.sha. URL https://www.jbe-platform.com/content/ journals/10.1075/eww.20010.sha.</p>
<p>Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, and Ethan Perez. Towards understanding sycophancy in language models, 2023.</p>
<p>Chenglei Si, Navita Goyal, Sherry Tongshuang Wu, Chen Zhao, Shi Feng, Hal Daum III au2, and Jordan Boyd-Graber. Large language models help humans verify truthfulness - except when they are convincingly wrong, 2023.
E.R. Smith, D.M. Mackie, and H.M. Claypool. Social Psychology: Fourth Edition. Taylor \&amp; Francis, 2014. ISBN 9781136845123. URL https://books.google.co.uk/books?id= yBOLBQAAQBAJ.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. MeierHellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Agera y Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. Lamda: Language models for dialog applications. CoRR, abs/2201.08239, 2022. URL https: //arxiv.org/abs/2201.08239.</p>
<p>Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1):267-288, 1996. ISSN 00359246. URL http://www. jstor.org/stable/2346178.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.</p>
<p>Chris van der Lee, Albert Gatt, Emiel van Miltenburg, and Emiel Krahmer. Human evaluation of automatically generated text: Current trends and best practice guidelines. Computer Speech \&amp; Language, 67:101151, 2021. ISSN 0885-2308. doi: https://doi.org/10.1016/j.csl.2020.101151. URL https://www.sciencedirect.com/science/article/pii/S088523082030084X.</p>
<p>Yuxia Wang, Shimin Tao, Ning Xie, Hao Yang, Timothy Baldwin, and Karin Verspoor. Collective Human Opinions in Semantic Textual Similarity. Transactions of the Association for Computational Linguistics, 11:997-1013, 08 2023. ISSN 2307-387X. doi: 10.1162/tacl_a_00584. URL https://doi.org/10.1162/tacl_a_00584.</p>
<p>Kathryn A. Woolard. Language Ideology, pp. 1-21. John Wiley \&amp; Sons, Ltd, 2020. ISBN 9781118786093. doi: https://doi.org/10.1002/9781118786093.iela0217. URL https:// onlinelibrary.wiley.com/doi/abs/10.1002/9781118786093.iela0217.</p>
<p>Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. arXiv preprint arXiv:2306.01693, 2023.</p>
<p>Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong Mao. Expertprompting: Instructing large language models to be distinguished experts, 2023a.</p>
<p>Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196, 2023b.</p>
<p>Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. A critical evaluation of evaluations for long-form question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3225-3245, Toronto, Canada, July 2023c. Association for Computational Linguistics. URL https://aclanthology.org/2023. ac1-long.181.</p>
<p>Yumo Xu and Mirella Lapata. Document summarization with latent queries. Transactions of the Association for Computational Linguistics, 10:623-638, 2022. doi: 10.1162/tacl_a_00480. URL https://aclanthology.org/2022.tacl-1.36.</p>
<p>Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. Flask: Fine-grained language model evaluation based on alignment skill sets, 2023.</p>
<p>Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. Benchmarking large language models for news summarization, 2023. URL https: //arxiv.org/abs/2301.13848.</p>
<h1>A ANNOTATION DETAILS</h1>
<p>Participants were paid 0.30GBP for annotating each pair of outputs with overall scores or assertiveness/complexity ratings, or 0.60 for annotating each pair of examples for errors. This corresponded to a median payment of over 13GBP per hour for all experiments. Participants were recruited using Prolific, and sampled according to Prolifics 'Balanced Sample' option. Participants were allowed to participate multiple times, up to a limit of 200. To avoid interaction effects, participants were blocked from participating in multiple types of annotation (i.e., they were not able to annotate both for errors and overall scores).</p>
<h2>B INSTRUCTIONS TO ANNOTATORS</h2>
<h2>B. 1 ERROR ANNOTATION INSTRUCTIONS</h2>
<p>You will be shown a prompt sent to a chatbot, and two possible responses.
Your job is to check whether each response contains a range of errors.
Please read the prompt in full, then read both responses before assessing them. The prompt should usually make it clear what the chatbot should do, such as answering a question or summarising a conversation. In some cases the prompt may end with a heading like "Summary:" or "Description:" - in this case the response should "fill in the blank" and write out a summary or description of the rest of the prompt.
The form includes two attention check questions, which are clearly marked. Please make sure you complete these correctly, otherwise your submission risks being rejected.
The error types that the responses should be checked for are:</p>
<ul>
<li>Inconsistency with request - Does the response incorrectly represent or change information from the request?</li>
<li>Contradicts itself - Is the response inconsistent with itself, or does it contradict itself?</li>
<li>
<p>Factuality - Is the response factually incorrect (regardless of what the request said)? Please use Wikipedia or Google to check whether any facts included in the response are inaccurate.</p>
</li>
<li>
<p>Relevance - Does the response go off topic or include information that is not relevant to the request?</p>
</li>
<li>Formatting - Does the response fail to conform to any formatting or length requirements from the prompt?</li>
<li>Refusing reasonable requests - If the request is reasonable, does the response refuse to answer it (e.g. "I'm sorry, I can't help you with that")? A poor quality attempt to answer a reasonable request is allowed. If the request is unsafe or impossible, refusal is then allowed.</li>
<li>Repetition - Does the response repeat itself? For example, if there is a list in the response, are any items repeated? Does the response reuse the same phrase again and again?
Finally, you will be asked to assign the responses an overall quality score, based on how good you think the response is. You should imagine that you were a user of the chatbot and base your opinion on how happy you would be with the response. If other factors than the ones above are useful for making your decision, please mention these in the text field.
Where possible, please judge each criteria/error separately from each other. For example, a response that gives information that is correct but not relevant to the prompt should still be marked as OK for "factuality" and "contradiction", but negatively for "relevance". First, carefully read through the prompt, checking that you understand how you expect the chatbot to respond. Then carefully read each response.
Now, please assess each response based on the criteria below. Remember that you are being asked to judge the responses, not the prompt. It's OK to go back and re-read the prompt and responses if you need to. Required fields are marked with an asterisk.</li>
</ul>
<h1>B. 2 OVERALL SCORE ANNOTATION INSTRUCTIONS</h1>
<p>You will be shown a prompt sent to a chatbot, and two possible responses.
Please read the prompt in full, and then rate how good each response is, on a scale from 1 (very bad) to 5 (very good).
You should judge the responses based on whatever criteria you think are important. Imagine that you are a user of the chatbot and that you had sent it the prompt: how happy would you be with each of the responses?
The prompt should usually make it clear what the chatbot should do, such as answering a question or summarising a conversation. In some cases the prompt may end with a heading like "Summary:" or "Description:" - in this case the response should "fill in the blank" and write out a summary or description of the rest of the prompt.
First, carefully read through the prompt, checking that you understand how you expect the chatbot to respond. Then carefully read each response.
Rate each of the two responses from 1 (Very bad) to 5 (Very good) based on how good they are. Use your own judgement - how happy would you personally be to receive each of the responses? Remember that you are being asked to judge the jstrong ${ }<em i="i">{i}$ responses $</em>$, not the prompt. Required fields are marked with an asterisk.} /$ strong $_{i</p>
<h2>B. 3 CONFOUND ANNOTATION INSTRUCTIONS</h2>
<p>You will be shown a prompt sent to a chatbot, and two possible responses.
Your job is to evaluate each response $i /$ strong $_{i}$ according to 2 different criteria.
Please read the prompt in full, then read both responses before assessing them. The prompt should usually make it clear what the chatbot should do, such as answering a question or summarising a conversation. In some cases the prompt may end with a heading like "Summary:" or "Description:" - in this case the response should "fill in the blank" and write out a summary or description of the rest of the prompt.</p>
<p>The criteria that the responses should be checked for are:</p>
<ul>
<li>Complexity - How complex are the words and language used in the response?</li>
<li>Persuasiveness - How assertive and persuasive is the response?</li>
</ul>
<p>Where possible, please check each criteria separately from each other.
First, carefully read through the prompt, checking that you understand how you expect the chatbot to respond. Then carefully read each response.
Now, please assess each response based on the criteria below. Remember that you are being asked to categorise the $i$ strong $i$ responses $i$ strong $i$, not the prompt. It's OK to go back and re-read the prompt and responses if you need to. Required fields are marked with an asterisk.</p>
<h1>C Task Templates</h1>
<p>WikiHow WikiHow article titles were converted to prompts using the template "Can you tell me {title}?".</p>
<p>Amazon Product Descriptions Amazon Product Specifications were converted to prompts using a wide range of templates. Please see the dataset files for details.</p>
<p>Curation Articles from the Curation dataset were converted to prompts according to the template: "{article}\n\nGenerate a summary:\n".</p>
<h2>D INFERENCE HYPERPARAMETERS</h2>
<p>Outputs were sampled using a temperature of 0.7 .
Llama 2 The full HuggingFace model ID used was meta-llama/Llama-2-13b-chat-hf. The prompt template used was:</p>
<div class="codehilite"><pre><span></span><code><span class="o">[</span><span class="n">INST</span><span class="o">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="n">SYS</span><span class="o">&gt;&gt;</span>
<span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">helpful</span><span class="p">,</span><span class="w"> </span><span class="n">respectful</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">honest</span><span class="w"> </span><span class="n">assistant</span><span class="p">.</span><span class="w"> </span><span class="n">Always</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">as</span>
<span class="w">    </span><span class="n">helpfully</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">possible</span><span class="p">,</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="n">being</span><span class="w"> </span><span class="n">safe</span><span class="p">.</span><span class="w"> </span><span class="n">Your</span><span class="w"> </span><span class="n">answers</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="ow">not</span>
<span class="w">    </span><span class="k">include</span><span class="w"> </span><span class="ow">any</span><span class="w"> </span><span class="n">harmful</span><span class="p">,</span><span class="w"> </span><span class="n">unethical</span><span class="p">,</span><span class="w"> </span><span class="n">racist</span><span class="p">,</span><span class="w"> </span><span class="n">sexist</span><span class="p">,</span><span class="w"> </span><span class="n">toxic</span><span class="p">,</span><span class="w"> </span><span class="n">dangerous</span><span class="p">,</span><span class="w"> </span><span class="ow">or</span>
<span class="w">    </span><span class="n">illegal</span><span class="w"> </span><span class="n">content</span><span class="p">.</span>
<span class="o">&lt;&lt;/</span><span class="n">SYS</span><span class="o">&gt;&gt;</span>
<span class="p">(</span><span class="n">instruction</span><span class="p">)</span><span class="w"> </span><span class="o">[</span><span class="n">/INST</span><span class="o">]</span>
</code></pre></div>

<p>Falcon 40B Instruct The full HuggingFace model ID used was tiiuae/falcon-40b-instruct. Falcon is prompted directly with the query.</p>
<p>MPT 30B Instruct The full HuggingFace model ID used was mosaicml/mpt-30b-instruct. The prompt template used was:</p>
<p>Below is an instruction that describes a task. Write a response that appropriately completes the request.
###Instruction
{instruction}
### Response</p>
<p>Command 6B and 52B The Cohere models were queried on 22/06/2023 and 17/08/2023 for Sections 2 and 3 respectively, using the generate API function. Cohere models use an undisclosed internal template.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Pearson Correlation coefficients of each error factor with the unbiased (collected independently of the error annotations) and 'biased' (collected alongside the errors) scores. In almost every case, the correlations are higher for the 'biased' scores, confirming that asking annotators to check for a particular set of errors before deciding on an overall score has a priming effect.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Selected error rates and overall quality scores plotted against length of response. Annotators prefer longer responses, up until a certain point where repetition starts to become an issue.</p>
<h1>E ADDITIONAL FINDINGS</h1>
<p>Annotators can be primed We collected quality scores from both groups of participants: both from those who checked for errors (biased scores) and from the group of annotators who only annotated for overall quality. We refer to this latter set of scores as the unbiased score, since these annotators were not shown the error criteria, which might have influenced their judgements. Since we collected quality scores from annotators who both were and were not shown the error criteria, we can check whether any priming effect occurred (Bargh \&amp; Chartrand, 2014, inter alia.); did checking the outputs for a pre-determined set of errors influence their judgement when asked to judge overall output quality?
Unsurprisingly, we find that the correlations between the overall scores and each error criterion are higher for the biased case (Figure 7), confirming that asking annotators to check for specific errors has a priming effect on overall preference ratings.
'Not too long, not too short' Figure 8 shows a plot of overall quality scores and selected error rates against length in words of the response. Annotators prefer longer responses, up until the point when repetition becomes an issue. This can be interpreted as evidence that LLMs should obey Grice's Maxim of Quantity (Grice, 1991), which states that a felicitous speaker should be as informative as is required, but not more. The rate of repetition errors increases with increasing length of response, which is to be expected, and indeed partly explains the reason for the drop in quality at higher lengths. We might expect that long responses include more factual statements and therefore have more 'opportunity' to be incorrect, but interestingly the annotated rate of factuality errors increases for both short and long responses.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: A sensitivity analysis of the Lasso regression weightings from Section 2.2. We recalculate the weightings, leaving out each error type in turn. In general, the changes in weightings are minor, and the features selected do not change, indicating that the regression is stable.</p>
<p>Reference outputs are not ground truth We note that of the five systems evaluated (four models, plus the references) the reference outputs were given the lowest overall quality scores (Table 2). This confirms findings from previous work (Zhang et al., 2023; Hosking et al., 2023), and highlights the need for research on reference-free evaluation techniques.</p>
<p>Current models are also very good at producing fluent output, and the datasets we used to generate input prompts are unlikely to elicit harmful or out-of-scope responses, leading to very low error rates for these criteria (less than $1 \%$ ). We therefore excluded them from our follow-up experiments, but we emphasize that they remain important considerations for models 'in the wild'.</p>
<p>Table 2: Average error rates and overall quality scores from the experiments in Section 2, by model. 'Unbiased' and 'Biased' refer to the overall quality scores.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Contradict</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Inconsistency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Command 52B</td>
<td style="text-align: center;">2.50</td>
<td style="text-align: center;">4.44</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">4.44</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">3.06</td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.39</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">3.59</td>
</tr>
<tr>
<td style="text-align: center;">Command 6B</td>
<td style="text-align: center;">2.50</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">6.94</td>
<td style="text-align: center;">6.11</td>
<td style="text-align: center;">3.33</td>
<td style="text-align: center;">3.33</td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">3.49</td>
</tr>
<tr>
<td style="text-align: center;">Falcon 40B</td>
<td style="text-align: center;">1.94</td>
<td style="text-align: center;">8.06</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">6.94</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">2.78</td>
<td style="text-align: center;">2.22</td>
<td style="text-align: center;">2.50</td>
<td style="text-align: center;">1.39</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">3.79</td>
</tr>
<tr>
<td style="text-align: center;">MPT 30B</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">3.06</td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">2.22</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">3.76</td>
</tr>
<tr>
<td style="text-align: center;">References</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">4.72</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">6.11</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">3.62</td>
</tr>
</tbody>
</table>
<h1>F SUPPLEMENTARY RESULTS</h1>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Frequency of Flesch-Kincaid reading grade for each preamble type. The shift in distributions indicates that the preambles have successfully modified the complexity of the outputs.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Frequency of automatic sentiment score for each preamble type. The shift in distributions indicates that the preambles have successfully modified the sentiment (and thus potentially the assertiveness) of the outputs.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Variation in error rates with perceived complexity scores. More complex outputs are somewhat less likely to be considered as containing errors, although this effect is less strong than for assertiveness.</p>
<p>Table 3: Full results of annotated error rates for each preamble type, according to crowdworkers ('Ann.') and experts (the authors, Exp.). The difference between the two annotation sources is also shown as $\delta$. Although annotators are generally less likely to detect factuality or inconsistency errors, this difference is widened if the outputs are made to be more assertive; the more assertive an output is, the less likely annotators are to detect factual errors that it may contain.
<img alt="img-12.jpeg" src="img-12.jpeg" />
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 13: The difference in error rates between crowdsourced annotations and 'expert' annotations from the authors, including samples that were marked as refusing to respond.</p>
<p>Table 4: Average error rates and overall quality scores from the experiments in Section 3, by model.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We exclude scope, fluency and harmfulness from this set of experiments due to their rarity.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-802 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-802</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-802</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-258823336</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.11598v1.pdf" target="_blank">Introspective Tips: Large Language Model for In-Context Decision Making</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of large language models (LLMs) has substantially influenced natural language processing, demonstrating exceptional results across various tasks. In this study, we employ ``Introspective Tips"to facilitate LLMs in self-optimizing their decision-making. By introspectively examining trajectories, LLM refines its policy by generating succinct and valuable tips. Our method enhances the agent's performance in both few-shot and zero-shot learning situations by considering three essential scenarios: learning from the agent's past experiences, integrating expert demonstrations, and generalizing across diverse games. Importantly, we accomplish these improvements without fine-tuning the LLM parameters; rather, we adjust the prompt to generalize insights from the three aforementioned situations. Our framework not only supports but also emphasizes the advantage of employing LLM in in-contxt decision-making. Experiments involving over 100 games in TextWorld illustrate the superior performance of our approach.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e802.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e802.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Tips</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Introspective Tips LLM agent (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-driven GPT-4 agent that performs in-context decision making in TextWorld by generating, refining, and consuming compact 'introspective tips' (summaries of past trajectories, expert demos, or multi-game summaries) instead of fine-tuning model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Introspective Tips LLM (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A GPT-4 based policy implemented purely via prompting: the system defines the agent role and ActionList (environment API); the LLM receives environment textual observations and feedback and outputs textual actions. Key components are (1) trial execution (LLM issues actions and receives textual environment responses), (2) introspection modules implemented as prompt recipes that ask the LLM to produce concise 'tips' from failed or successful trajectories, (3) a dynamic prompt-adjustment framework that injects these tips (or aggregated multi-game tips from another LLM) into future decision prompts, and (4) optional replay of recent actions. There is no parameter fine-tuning; the LLM is used as an in-context planner and generator of advice.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (Cooking domain)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A text-based game benchmark formalized as a POMDP: the agent receives partial textual observations each turn (room descriptions, item listings, feedback from actions), must follow multi-step recipes, handle sparse rewards, long-horizon dependencies, and (in harder levels) navigate between rooms to acquire ingredients.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Environment API (textual observations and action-feedback), the game's ActionList primitives (e.g., inventory(), examine(item), go(direction), cook(food,heat_source)), and an auxiliary LLM used offline/externally to aggregate successful tips from multiple games into general tips (i.e., another LLM was invoked to synthesize multi-game tips). The paper treats 'tips' and the auxiliary LLM as external/auxiliary tooling in the prompting loop.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual environment feedback (natural-language descriptions of state changes and action outcomes), short textual tip summaries produced by the same or a second LLM (concise natural-language advice), and action acknowledgements (textual). No explicit numeric or spatial map outputs are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Compact textual summaries (introspective 'tips') + short history of recent actions/failures and inventory checks; tips function as a condensed, prompt-resident memory/belief rather than an explicit structured belief graph. The agent maintains and re-injects these textual tip summaries and recent action traces into subsequent prompts to represent its belief/knowledge about important aspects of the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>After a trial (successful or failed), the agent is prompted to analyze environment feedback and its own actions and to generate one or more concise tips describing what to do (or avoid) next time; these tips — together with past failed actions and environment messages such as inventory() outputs — are concatenated into future prompts. In the multi-game setting, final successful tips from several games are sent to another LLM which synthesizes general tips; those general tips are then used as prompts in unseen test games. Thus belief is updated by generating and adding textual tip artifacts to the prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>In-context, reasoning-based planning via a large pre-trained LLM (GPT-4) guided by prompt engineering and introspective tips. The agent relies on language-model reasoning (including multi-step reasoning patterns) to select actions; it is not trained with an explicit planner or value function in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation is performed by issuing sequential text navigation commands (go(north), go(east), etc.) decided by the LLM's reasoning over textual observations and tips; no explicit graph search or shortest-path algorithm is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>With tips-memory (few-shot) the LLM agent achieved a high cumulative performance (reported success rate > 0.9 and normalized points > 0.95 in the experiments described). With expert demonstrations it reached zero error across difficulty levels after at most 6 trajectories. With general tips aggregated from 48 trajectories (10 training games) the LLM matched or exceeded SOTA baselines on levels 0-3 and outperformed them on levels 3-4 (reported qualitatively and with normalized point/success improvements in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Pure replay (ingesting past full trajectories as prompts, up to 3 trajectories due to input limits) underperforms compared to the tips-summary approach; specific numeric baselines for pure replay are not provided in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Concise, LLM-generated 'introspective tips' function as an effective, compact belief/memory substitute in partially observable text games, improving sample efficiency and generalization; aggregating successful tips across games produces general tips that enable zero-shot transfer; expert demonstrations speed learning further; LLM reasoning with tips can bypass some of the burdens of explicitly learning navigation or long-horizon credit assignment that hinder RL-based SOTA methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introspective Tips: Large Language Model for In-Context Decision Making', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e802.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e802.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GATA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Aided Transformer Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep-learning agent for text-based games that constructs and updates graph-structured state (belief) representations from text observations to mitigate partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic belief graphs to generalize on text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Graph Aided Transformer Agent (GATA)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A learned RL agent that explicitly builds a structured graph representation of the environment (entities and relations) from textual observations and maintains/updates that graph across steps; the graph is used with a transformer-based policy to select actions and optimize rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text-based games (POMDP) with long-horizon tasks and navigation; GATA addresses these challenges by maintaining an internal graph belief.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Internal graph-construction module (treated as part of the agent's state-estimation machinery); the paper does not report use of external tools like maps or search engines.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured graph representations (nodes/edges encoding objects, locations, and relations) derived from textual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Graph-structured belief: constructs a dynamic graph representing entities and their relations and updates it with incoming text observations.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>New textual observations are parsed and used to add, remove, or update nodes and edges in the belief graph, producing an evolving internal state used by the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy combining graph-based state representation with transformer-based action selection (model-free RL with structured state augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prior work (GATA) demonstrates that explicit structured belief graphs constructed from text can improve generalization and handling of partial observability in TextWorld; the current paper contrasts this explicit structured-belief approach with compact prompt-based tips maintained by an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introspective Tips: Large Language Model for In-Context Decision Making', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e802.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e802.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ITL (Tuli et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ITL (GATA augmented with internal structured instruction representation / Tuli et al. 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that equips GATA-style agents with an internal structured representation of natural-language instructions (using linear temporal logic style constructs) to improve instruction following in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to follow instructions in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ITL (instruction-structured GATA)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Extension of graph-augmented agents that encodes natural language instructions into structured representations (LTL-like) and uses that together with internal state representations to guide action selection. Presented as a prior SOTA deep-learning approach in TextWorld instruction-following tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text-based environments with instruction-following requirements and navigation; the method augments internal belief with structured instruction representations.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Internal instruction-encoding module (LTL/structured logic); not reported to call external non-ML tools.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured logical representations of instructions (e.g., LTL-like formulas) and internal graph/state variables.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Combination of graph-structured state and structured instruction representation (logical constraints) to represent belief about what must be achieved and current state.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Not detailed in this paper beyond stating that instructions are internally represented and combined with state beliefs to guide policy; updates occur as observations arrive to reflect progress toward instruction goals.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy augmented by logical/instruction constraints and graph-based beliefs (deep RL / supervised components as in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structured internal representations of instructions combined with state beliefs can strengthen instruction following in partially observable text games; highlighted here as a contrasting prior approach to LLM prompt-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introspective Tips: Large Language Model for In-Context Decision Making', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e802.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e802.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-WorldModel</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM as World Model (Brooks et al. / related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The idea of using a large language model itself as a predictive world model to simulate future textual trajectories and support planning in interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-as-world-model planner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A planning setup where an LLM is used to predict future textual observations and action outcomes (i.e., serve as a model of environment dynamics), and those predictions are used in-context to plan action sequences without training a separate environment model.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Interactive/text-based decision-making environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments where future states and outcomes can be represented textually; partial observability is typical but specifics are not detailed in this paper's citation discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>The LLM is treated as a modeling tool (simulator) producing text predictions; no other external tools are described here.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual predicted next-state descriptions and candidate action sequences (text).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Model-based planning via language-model-based simulation of future trajectories (in-context lookahead).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prior work suggests LLMs can be leveraged as textual world models to plan future trajectories in interactive tasks; cited here as a different way to use language models for planning as compared to purely prompt-driven policy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introspective Tips: Large Language Model for In-Context Decision Making', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e802.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e802.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (interleaved Reasoning and Acting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that interleaves natural-language reasoning traces with action outputs from LLMs to improve decision-making in interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct-style agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-based agent that outputs alternating 'reasoning' text (intermediate chain-of-thought style steps) and 'action' text that interacts with an environment; used to improve synergy between comprehension and action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Interactive/text-based tasks (including games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Typically partially observable interactive tasks; this paper references ReAct in the context of decision-making improvements but does not give environment specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Interleaved reasoning and action generation (language-model-driven).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Interleaving explicit reasoning traces with actions can improve action selection in interactive settings; presented in related-work context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introspective Tips: Large Language Model for In-Context Decision Making', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e802.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e802.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (self-reflective LLM agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-reflective LLM-based agent architecture that maintains a separate reflection module and simple heuristics to detect hallucinations or inefficient behaviors and iteratively self-correct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM agent augmented with a self-reflective component: the agent runs a heuristic to detect problematic behaviors/hallucinations and uses a reflection LLM to generate corrective guidance for subsequent trials.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Interactive decision-making domains (referenced generally)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable interactive tasks are the target, but specifics are in the cited work rather than this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Iterative self-reflection and heuristic-guided correction (LLM-based).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-reflection mechanisms can help detect hallucinations and inefficient action sequences; cited as related prior work to introspective tips.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introspective Tips: Large Language Model for In-Context Decision Making', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dynamic belief graphs to generalize on text-based games. <em>(Rating: 2)</em></li>
                <li>Learning to follow instructions in text-based games. <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models. <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models. <em>(Rating: 2)</em></li>
                <li>Language models are zero-shot planners: Extracting actionable knowledge for embodied agents. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-802",
    "paper_id": "paper-258823336",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "LLM-Tips",
            "name_full": "Introspective Tips LLM agent (GPT-4)",
            "brief_description": "A prompt-driven GPT-4 agent that performs in-context decision making in TextWorld by generating, refining, and consuming compact 'introspective tips' (summaries of past trajectories, expert demos, or multi-game summaries) instead of fine-tuning model weights.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Introspective Tips LLM (GPT-4)",
            "agent_description": "A GPT-4 based policy implemented purely via prompting: the system defines the agent role and ActionList (environment API); the LLM receives environment textual observations and feedback and outputs textual actions. Key components are (1) trial execution (LLM issues actions and receives textual environment responses), (2) introspection modules implemented as prompt recipes that ask the LLM to produce concise 'tips' from failed or successful trajectories, (3) a dynamic prompt-adjustment framework that injects these tips (or aggregated multi-game tips from another LLM) into future decision prompts, and (4) optional replay of recent actions. There is no parameter fine-tuning; the LLM is used as an in-context planner and generator of advice.",
            "environment_name": "TextWorld (Cooking domain)",
            "environment_description": "A text-based game benchmark formalized as a POMDP: the agent receives partial textual observations each turn (room descriptions, item listings, feedback from actions), must follow multi-step recipes, handle sparse rewards, long-horizon dependencies, and (in harder levels) navigate between rooms to acquire ingredients.",
            "is_partially_observable": true,
            "external_tools_used": "Environment API (textual observations and action-feedback), the game's ActionList primitives (e.g., inventory(), examine(item), go(direction), cook(food,heat_source)), and an auxiliary LLM used offline/externally to aggregate successful tips from multiple games into general tips (i.e., another LLM was invoked to synthesize multi-game tips). The paper treats 'tips' and the auxiliary LLM as external/auxiliary tooling in the prompting loop.",
            "tool_output_types": "Textual environment feedback (natural-language descriptions of state changes and action outcomes), short textual tip summaries produced by the same or a second LLM (concise natural-language advice), and action acknowledgements (textual). No explicit numeric or spatial map outputs are reported.",
            "belief_state_mechanism": "Compact textual summaries (introspective 'tips') + short history of recent actions/failures and inventory checks; tips function as a condensed, prompt-resident memory/belief rather than an explicit structured belief graph. The agent maintains and re-injects these textual tip summaries and recent action traces into subsequent prompts to represent its belief/knowledge about important aspects of the environment.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "After a trial (successful or failed), the agent is prompted to analyze environment feedback and its own actions and to generate one or more concise tips describing what to do (or avoid) next time; these tips — together with past failed actions and environment messages such as inventory() outputs — are concatenated into future prompts. In the multi-game setting, final successful tips from several games are sent to another LLM which synthesizes general tips; those general tips are then used as prompts in unseen test games. Thus belief is updated by generating and adding textual tip artifacts to the prompt context.",
            "planning_approach": "In-context, reasoning-based planning via a large pre-trained LLM (GPT-4) guided by prompt engineering and introspective tips. The agent relies on language-model reasoning (including multi-step reasoning patterns) to select actions; it is not trained with an explicit planner or value function in this work.",
            "uses_shortest_path_planning": null,
            "navigation_method": "Navigation is performed by issuing sequential text navigation commands (go(north), go(east), etc.) decided by the LLM's reasoning over textual observations and tips; no explicit graph search or shortest-path algorithm is reported.",
            "performance_with_tools": "With tips-memory (few-shot) the LLM agent achieved a high cumulative performance (reported success rate &gt; 0.9 and normalized points &gt; 0.95 in the experiments described). With expert demonstrations it reached zero error across difficulty levels after at most 6 trajectories. With general tips aggregated from 48 trajectories (10 training games) the LLM matched or exceeded SOTA baselines on levels 0-3 and outperformed them on levels 3-4 (reported qualitatively and with normalized point/success improvements in the paper).",
            "performance_without_tools": "Pure replay (ingesting past full trajectories as prompts, up to 3 trajectories due to input limits) underperforms compared to the tips-summary approach; specific numeric baselines for pure replay are not provided in detail in this paper.",
            "has_tool_ablation": true,
            "key_findings": "Concise, LLM-generated 'introspective tips' function as an effective, compact belief/memory substitute in partially observable text games, improving sample efficiency and generalization; aggregating successful tips across games produces general tips that enable zero-shot transfer; expert demonstrations speed learning further; LLM reasoning with tips can bypass some of the burdens of explicitly learning navigation or long-horizon credit assignment that hinder RL-based SOTA methods.",
            "uuid": "e802.0",
            "source_info": {
                "paper_title": "Introspective Tips: Large Language Model for In-Context Decision Making",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GATA",
            "name_full": "Graph Aided Transformer Agent",
            "brief_description": "A deep-learning agent for text-based games that constructs and updates graph-structured state (belief) representations from text observations to mitigate partial observability.",
            "citation_title": "Learning dynamic belief graphs to generalize on text-based games.",
            "mention_or_use": "mention",
            "agent_name": "Graph Aided Transformer Agent (GATA)",
            "agent_description": "A learned RL agent that explicitly builds a structured graph representation of the environment (entities and relations) from textual observations and maintains/updates that graph across steps; the graph is used with a transformer-based policy to select actions and optimize rewards.",
            "environment_name": "TextWorld",
            "environment_description": "Partially observable text-based games (POMDP) with long-horizon tasks and navigation; GATA addresses these challenges by maintaining an internal graph belief.",
            "is_partially_observable": true,
            "external_tools_used": "Internal graph-construction module (treated as part of the agent's state-estimation machinery); the paper does not report use of external tools like maps or search engines.",
            "tool_output_types": "Structured graph representations (nodes/edges encoding objects, locations, and relations) derived from textual observations.",
            "belief_state_mechanism": "Graph-structured belief: constructs a dynamic graph representing entities and their relations and updates it with incoming text observations.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "New textual observations are parsed and used to add, remove, or update nodes and edges in the belief graph, producing an evolving internal state used by the policy.",
            "planning_approach": "Learned policy combining graph-based state representation with transformer-based action selection (model-free RL with structured state augmentation).",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Prior work (GATA) demonstrates that explicit structured belief graphs constructed from text can improve generalization and handling of partial observability in TextWorld; the current paper contrasts this explicit structured-belief approach with compact prompt-based tips maintained by an LLM.",
            "uuid": "e802.1",
            "source_info": {
                "paper_title": "Introspective Tips: Large Language Model for In-Context Decision Making",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ITL (Tuli et al.)",
            "name_full": "ITL (GATA augmented with internal structured instruction representation / Tuli et al. 2022)",
            "brief_description": "An approach that equips GATA-style agents with an internal structured representation of natural-language instructions (using linear temporal logic style constructs) to improve instruction following in text-based games.",
            "citation_title": "Learning to follow instructions in text-based games.",
            "mention_or_use": "mention",
            "agent_name": "ITL (instruction-structured GATA)",
            "agent_description": "Extension of graph-augmented agents that encodes natural language instructions into structured representations (LTL-like) and uses that together with internal state representations to guide action selection. Presented as a prior SOTA deep-learning approach in TextWorld instruction-following tasks.",
            "environment_name": "TextWorld",
            "environment_description": "Partially observable text-based environments with instruction-following requirements and navigation; the method augments internal belief with structured instruction representations.",
            "is_partially_observable": true,
            "external_tools_used": "Internal instruction-encoding module (LTL/structured logic); not reported to call external non-ML tools.",
            "tool_output_types": "Structured logical representations of instructions (e.g., LTL-like formulas) and internal graph/state variables.",
            "belief_state_mechanism": "Combination of graph-structured state and structured instruction representation (logical constraints) to represent belief about what must be achieved and current state.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Not detailed in this paper beyond stating that instructions are internally represented and combined with state beliefs to guide policy; updates occur as observations arrive to reflect progress toward instruction goals.",
            "planning_approach": "Learned policy augmented by logical/instruction constraints and graph-based beliefs (deep RL / supervised components as in cited work).",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Structured internal representations of instructions combined with state beliefs can strengthen instruction following in partially observable text games; highlighted here as a contrasting prior approach to LLM prompt-based methods.",
            "uuid": "e802.2",
            "source_info": {
                "paper_title": "Introspective Tips: Large Language Model for In-Context Decision Making",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLM-WorldModel",
            "name_full": "LLM as World Model (Brooks et al. / related work)",
            "brief_description": "The idea of using a large language model itself as a predictive world model to simulate future textual trajectories and support planning in interactive tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "LLM-as-world-model planner",
            "agent_description": "A planning setup where an LLM is used to predict future textual observations and action outcomes (i.e., serve as a model of environment dynamics), and those predictions are used in-context to plan action sequences without training a separate environment model.",
            "environment_name": "Interactive/text-based decision-making environments",
            "environment_description": "Environments where future states and outcomes can be represented textually; partial observability is typical but specifics are not detailed in this paper's citation discussion.",
            "is_partially_observable": null,
            "external_tools_used": "The LLM is treated as a modeling tool (simulator) producing text predictions; no other external tools are described here.",
            "tool_output_types": "Textual predicted next-state descriptions and candidate action sequences (text).",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Model-based planning via language-model-based simulation of future trajectories (in-context lookahead).",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Prior work suggests LLMs can be leveraged as textual world models to plan future trajectories in interactive tasks; cited here as a different way to use language models for planning as compared to purely prompt-driven policy.",
            "uuid": "e802.3",
            "source_info": {
                "paper_title": "Introspective Tips: Large Language Model for In-Context Decision Making",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (interleaved Reasoning and Acting)",
            "brief_description": "A method that interleaves natural-language reasoning traces with action outputs from LLMs to improve decision-making in interactive tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "ReAct-style agent",
            "agent_description": "An LLM-based agent that outputs alternating 'reasoning' text (intermediate chain-of-thought style steps) and 'action' text that interacts with an environment; used to improve synergy between comprehension and action selection.",
            "environment_name": "Interactive/text-based tasks (including games)",
            "environment_description": "Typically partially observable interactive tasks; this paper references ReAct in the context of decision-making improvements but does not give environment specifics.",
            "is_partially_observable": null,
            "external_tools_used": null,
            "tool_output_types": null,
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Interleaved reasoning and action generation (language-model-driven).",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Interleaving explicit reasoning traces with actions can improve action selection in interactive settings; presented in related-work context.",
            "uuid": "e802.4",
            "source_info": {
                "paper_title": "Introspective Tips: Large Language Model for In-Context Decision Making",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (self-reflective LLM agent)",
            "brief_description": "A self-reflective LLM-based agent architecture that maintains a separate reflection module and simple heuristics to detect hallucinations or inefficient behaviors and iteratively self-correct.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Reflexion agent",
            "agent_description": "An LLM agent augmented with a self-reflective component: the agent runs a heuristic to detect problematic behaviors/hallucinations and uses a reflection LLM to generate corrective guidance for subsequent trials.",
            "environment_name": "Interactive decision-making domains (referenced generally)",
            "environment_description": "Partially observable interactive tasks are the target, but specifics are in the cited work rather than this paper.",
            "is_partially_observable": null,
            "external_tools_used": null,
            "tool_output_types": null,
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Iterative self-reflection and heuristic-guided correction (LLM-based).",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Self-reflection mechanisms can help detect hallucinations and inefficient action sequences; cited as related prior work to introspective tips.",
            "uuid": "e802.5",
            "source_info": {
                "paper_title": "Introspective Tips: Large Language Model for In-Context Decision Making",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games.",
            "rating": 2,
            "sanitized_title": "learning_dynamic_belief_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "Learning to follow instructions in text-based games.",
            "rating": 2,
            "sanitized_title": "learning_to_follow_instructions_in_textbased_games"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models.",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models.",
            "rating": 2,
            "sanitized_title": "inner_monologue_embodied_reasoning_through_planning_with_language_models"
        },
        {
            "paper_title": "Language models are zero-shot planners: Extracting actionable knowledge for embodied agents.",
            "rating": 2,
            "sanitized_title": "language_models_are_zeroshot_planners_extracting_actionable_knowledge_for_embodied_agents"
        }
    ],
    "cost": 0.02030675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Introspective Tips: Large Language Model for In-Context Decision Making
19 May 2023</p>
<p>Liting Chen 
Lu Wang 
Hang Dong 
Yali Du 
Kings College London</p>
<p>Jie Yan 
Fangkai Yang 
Shuang Li 
The Chinese University of Hong Kong (Shenzhen)</p>
<p>Pu Zhao 
Si Qin 
Saravan Rajmohan 
Qingwei Lin 
Dongmei Zhang 
Introspective Tips: Large Language Model for In-Context Decision Making
19 May 20235911203F2C4BB200C05B5BA232D524C6arXiv:2305.11598v1[cs.AI]
The emergence of large language models (LLMs) has substantially influenced natural language processing, demonstrating exceptional results across various tasks.In this study, we employ "Introspective Tips" to facilitate LLMs in self-optimizing their decision-making.By introspectively examining trajectories, LLM refines its policy by generating succinct and valuable tips.Our method enhances the agent's performance in both few-shot and zeroshot learning situations by considering three essential scenarios: learning from the agent's past experiences, integrating expert demonstrations, and generalizing across diverse games.Importantly, we accomplish these improvements without fine-tuning the LLM parameters; rather, we adjust the prompt to generalize insights from the three aforementioned situations.Our framework not only supports but also emphasizes the advantage of employing LLM in in-contxt decision-making.Experiments involving over 100 games in TextWorld illustrate the superior performance of our approach.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs), including OpenAI's GPT-3.5 (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), Google's PaLM (Chowdhery et al., 2022) and other benchmarks (Meta, 2023;Taori et al., 2023) have consistently achieved remarkable performance across various NLP tasks.The integration of LLMs into decision-making tasks (Huang et al., 2022a;Ahn et al., 2022;Kwon et al., 2023;Brooks et al., 2022;Yao et al., 2022) has garnered significant attention, as it presents an opportunity to develop decision-making agents that can emulate human-like cognitive processes (Shevlin et al., 2019).In decision-making tasks, such as in the domain of Reinforcement Learning (RL), limited interactions hinder optimal policy learning (Yarats et al., 2021).Owing to the power of LLMs, which have undergone extensive pre-training on vast amounts of data, agents can leverage the generalization capabilities of LLMs to enhance their performance across different tasks.In addition, common sense knowledge is a valuable asset in safety decision-making tasks (Brunke et al., 2022), and LLMs possess a wealth of this knowledge due to their extensive training and tuned with human feedback (Ouyang et al., 2022).By leveraging the inherent common sense knowledge within LLMs, decision-making agents can make more informed decisions, effectively tackling challenges such as sparse rewards and enhancing the learning process.</p>
<p>However, LLMs sometimes demonstrate errors or hallucinations (Ji et al., 2023;Peng et al., 2023), especially in domain-specific scenarios.Recent works focus on designing selfcorrection mechanisms to enhance the LLM's decision-making performance.For example, Chain-of-thought (CoT) (Wei et al., 2022) concentrates on static reasoning for one-step actions without self-correction.ReAct (Yao et al., 2022) and Reflexion (Shinn et al., 2023) focus on dynamic reasoning based on historical interactions to enable better decision-making performance.However, these works primarily focus on generating instructions or reflections specific to individual agents, hindering the adoption of the generalization and universality abilities of LLMs.Furthermore, the absence of correction for the generated reflections can lead to errors during the decision-making process.</p>
<p>Humans possess high generalization and self-evolution capabilities due to their natural ability to summarize tips through introspection (Cox, 1996;Van Merrienboer and Sweller, 2005).Introspection enables humans to reflect on their past experiences (successes or failures), distill key insights, and extract valuable lessons that can be applied to new situations.Inspired by the remarkable introspection capabilities of humans, we introduce "Introspective Tips" (or "Tips" for short) as a novel prompt-based paradigm, empowering an LLM decision-making agent with self-optimizing capabilities via learning tips from trajectories of itself, experts, and other environments.Figure 1 provides an example of introspective tips in a text-based game.</p>
<p>Figure 1: In both Game 1 and Game 2, the agent is tasked with checking the cookbook and cooking a meal.The cookbooks for each game are different, but the initial state remains the same.After playing in Game 1, the agent is asked to participate in Game 2, using tips or reflections generated from the Game 1 trajectory.The agent's reflection focuses on the errors made in Game 1 and is specific to that game.Tips, on the other hand, offer more general and high-level information about the game.Utilizing tips from Game 1, the agent learns that the stove is for frying ingredients and uses the oven to roast the apple.The agent subsequently updates its knowledge about the game by incorporating a third tip.However, when using reflections, the agent receives an irrelevant message about potatoes, which leads to its failure to check the cookbook and complete the task in Game 2. In this case, the general tips prove to be more helpful in guiding the agent's behavior and improving its performance, whereas the specific reflections from Game 1 do not provide useful information for Game 2.</p>
<p>Distinct from reflection (Shinn et al., 2023), which is a self-analysis process that delves into an agent's actions and experiences in detail, "tips" are compact pieces of information offering high-level guidance or suggestions.The goal of tips is to provide succinct and practical advice for enhancing decision-making performance without requiring an in-depth examination of past trajectories.In contrast, reflections can only be derived from the agent itself and primarily concentrate on learning from failures.Tips, on the other hand, can be acquired from the trajectories of other agents and human experts, focusing on learning from both successes and failures.For example, in the game setting, the agent can generalize tips to correctly use oven based on its successful trajectory, but reflection may focus more on failures.</p>
<p>Additionally, in contrast to using original trajectories as prompts, introspective tips serve as condensed and comprehensive insights extracted from lengthy trajectories.This allows LLMs to overcome the challenges in discerning the intricate relationship between dependent actions and sparse rewards across multiple trajectories (Adhikari et al., 2020).Introspective tips provide distinct advantages in terms of their applicability and generalization across different agents and contexts, as opposed to previous prompting methods such as reflections.Unlike reflections that target specific agents, tips can be shared among different agents, allowing for higher generalization performance.In the game setting of Figure 1, the tips generalized by the agent can be applied to various game settings with different recipes.In contrast, reflections are more specific to a single past trajectory.</p>
<p>Furthermore, designing suitable prompts for LLMs to generate introspective tips is a crucial challenge; manually crafting prompts can be burdensome.Therefore we propose a framework that dynamically adjusts the prompt based on insights derived from past trajectories or expert demonstrations through introspection.</p>
<p>Our contributions can be summarized as follows:</p>
<p>• Introspective Tips for Self-Optimizing Decision-Making: We introduce Introspective Tips as a novel prompt-based paradigm, empowering LLM decision-making agents with self-optimizing capabilities for both few-shot and zero-shot scenarios.Unlike previous RL agents, Introspective Tips offers an end-to-end solution without the need for training or fine-tuning.It generates meaningful and easy-to-understand tips, facilitating human understanding and intervention in the decision-making process.</p>
<p>• Multiple Strategies in different scenarios for Learning Introspective Tips: We present distinct strategies applicable in different scenarios to prompt LLMs to learn Introspective Tips from their own trajectories, expert demonstrations, and multi-environment trajectories, which also shows the generalization and adaptability of Introspective Tips.</p>
<p>• A Dynamic Prompt Adjustment Framework: To simplify the prompting engineering process, we introduce a framework that dynamically adjusts the prompt by leveraging insights from past trajectories or expert demonstrations through introspection.This framework streamlines the improvement process, enhances the model's adaptability, and enables more efficient decision-making.</p>
<p>• Comprehensive Evaluation and Comparison: Our extensive experiments encompass over 100 games in TextWorld (Adhikari et al., 2020;Côté et al., 2019), testing few-shot and zero-shot learning scenarios.We evaluate the performance of our decisionmaking agent against state-of-the-art methods in the field (Tuli et al., 2022), highlighting the effectiveness and superiority of Introspective Tips.With tips generated from 48 trajectories, LLM agent performs better than previous deep learning method training for 100, 000 episodes in the unseen game with the highest difficulty level.</p>
<p>Related work</p>
<p>Language model for decision making LLMs (OpenAI, 2023;Chowdhery et al., 2022) have exhibited impressive proficiencies, facilitating their use in tasks beyond mere language generation and increasingly serving as policy models for decision-making in interactive settings (Yang et al., 2023).Wei et al. (2022) demonstrates that incorporating a series of intermediate reasoning steps can enhance decision-making abilities.Yao et al. (2022) introduces ReAct, a method for interleaved reasoning and action generation that fosters improved synergy between language comprehension and interactive decision-making tasks.Shinn et al. (2023) presents Reflexion, a technique that equips an LLM-based agent with a self-reflective LLM and a straightforward heuristic for detecting hallucination and inefficient action execution to examine the hypothesis.Madaan et al. (2023) adopts a similar strategy, enabling an LLM to offer feedback on its previously generated text and refine it to meet specific requirements.When regarded as dialogue agents, LLMs can also be trained to learn from human feedback and optimize their output (Ouyang et al., 2022;Bai et al., 2022).With further training, Li et al. (2022) constructs a general framework for decision-making tasks using pre-trained LMs, even in scenarios where language is neither provided as input nor output.Other studies (Singh et al., 2022;Huang et al., 2022a,b;Liang et al., 2022;Vemprala et al., 2023) have explored innovative strategies involving prompt engineering and the utilization of high-level function libraries to enhance the capabilities of LLMs.</p>
<p>Recent attempts explored different aspects of LLMs for decision-making.Huang et al. (2022a) and Ahn et al. (2022) use LLMs to generate plans or sub-goals that guide low-level Reinforcement Learning (RL) agents in taking actions.Kwon et al. (2023) utilize LLMs as proxy reward functions by prompting them with desired behaviors.Yao et al. (2022) focus on enabling LLM agents to select actions in text-based environments.In addition, a recent approach considers LLMs as world models (Brooks et al., 2022), where the agent learns the policy by interacting with the LLM-based world model.In this paper, we focus on directly grounding LLM in decision-making to take action because the other three methods required learning an extra decision-making agent, which required more samples.</p>
<p>In-context reinforcement learning In-context learning pertains to the ability of sequence prediction models to adapt to novel downstream tasks solely through the use of prompts without retraining or fine-tuning (Lu et al., 2021;Brown et al., 2020;Min et al., 2022).When applied to reinforcement learning, in-context learning models can generalize to diverse downstream tasks when provided with contexts such as demonstrations and task information.Laskin et al. (2022) recasts RL as an across-episode sequential prediction problem, and trains a causal transformer to autoregressively predict actions based on preceding learning histories as context.Brooks et al. (2022) employs the LLM as a world model for planning future trajectories and executing decisions in-context.Team et al. (2023) develops a versatile in-context learning algorithm capable of adapting to new and open-ended 3D challenging problems as rapidly as humans, by training an RL agent at a large scale.Lu et al. (2023) proposes to meta-learn across random linear projections of the observation and action spaces of randomly sampled DMControl tasks (Tassa et al., 2018).Trained on an extensive dataset, Gato (Reed et al., 2022) can generalize to new tasks by conditioning on demonstrations of the desired behavior.</p>
<p>Text-based game Text-based games are typically turn-based experiences played via a command line terminal.During each turn, the game state is conveyed through multiple lines of text, which enables players to input text commands that modify the state according to their preferences (Liu et al., 2022;Hendrycks et al., 2021;Osborne et al., 2022).Text-based games can be formally characterized as partially observable Markov decision processes (POMDPs) (Côté et al., 2019), considering that the agent only observes partial information about the environment at each turn.Intrinsic obstacles such as longterm dependencies, partial observation of current states, and sparse rewards, and complex action combinations render these games particularly challenging.Various deep learning approaches have been employed to address text-based games (Xu et al., 2022;Yin and May, 2019;Ammanabrolu and Hausknecht, 2020;Kimura et al., 2021).Focusing on tasks in the TextWorld domain (Côté et al., 2019), Adhikari et al. (2020) explores learning graphstructured state representations via a data-driven approach, introducing the Graph Aided Transformer Agent (GATA) that learns to construct and update graph-structured beliefs while optimizing rewards.Building upon their work, Tuli et al. (2022) equips GATA with an internal structured representation of natural language instructions using Linear Temporal Logic (LTL) to enhance the instruction-following capabilities of text-based game agents.</p>
<p>Method</p>
<p>In this section, we elaborate on the method employed to leverage the capabilities of LLMs in sequential decision-making tasks, particularly text-based games, by addressing LLMs inherent limitations and capitalizing on their strengths.LLMs' proficiency in understanding and generating human-like text renders them promising candidates for tasks involving natural language processing, such as text-based games.However, they lack specific domain knowledge when asked to make decisions in certain tasks thus may not reach the greatest potential when used directly.</p>
<p>We conjecture that the function space of generative LLMs P (θ) is sufficiently expansive to encompass the function of an expert policy in text-based games.The action space of policy π is constrained by text output.By selecting an appropriate prompt pr containing enough domain knowledge, we can derive a model P (θ|pr) that can function as a π in decision making, bridging the gap between LLMs and reinforcement learning.Through incorporating strategies including learning from past experiences, expert demonstrations, and multiple games, we aim to develop a versatile and robust framework that excels in a wide range of gaming scenarios.This approach ultimately opens up the journey of discovering the potential of LLMs in the domain of complex sequential decision-making tasks.</p>
<p>Challenges and Solutions for LLMs in Decision-Making Problems</p>
<p>There are several major challenges when applying Language Learning Models (LLMs) to solve decision-making problems, including self-optimization, input length limitations, and prompt dependence.In this section, we discuss these challenges and potential solutions.</p>
<p>Self-Optimizing decision-making LLMs can exhibit errors or hallucinations, particularly in domain-specific scenarios.Developing self-optimizing mechanisms to enable LLMs to correct their mistakes is essential for improving their decision-making performance.Moreover, allowing LLMs to self-optimize across various scenarios poses a significant challenge.Current approaches, such as self-reflection, attempt to rectify errors by concentrating on the model's own mistakes.However, this method has limitations, including the risk of LLMs becoming misled by their own errors during reflection and lacking a comprehensive understanding of the task or environment.To address these challenges, we employ "Introspective Tips" to facilitate LLMs in self-optimizing their decision-making.We empower LLMs to learn from their own trajectories, expert demonstrations, and trajectories in different environments.By extracting concise and actionable advice from these trajectories, the agent gains a better understanding of the environment.Furthermore, by refining its own tips, the problem of hallucination is alleviated.</p>
<p>Limited input length Language models like GPT-4 (OpenAI, 2023) and PaLM (Chowdhery et al., 2022) often encounter input length limitations.In RL, agents typically face lengthy and complex trajectories, as well as sparse rewards that offer feedback only after numerous steps or interactions.Due to the restricted input capacity and extended trajectories, LLMs struggle to discern the relationship between actions and rewards, ultimately impeding the acquisition of the optimal policy.To address these issues and optimize the available input space, we use tips that effectively condense the training dataset or critical information generally found in classical RL settings.By incorporating these summaries, LLMs can better understand the relationships within the data and identify essential patterns that might be otherwise obscured by the sheer length or complexity of the trajectories.Moreover, providing concise, relevant information enables LLMs to focus on the crucial aspects of the problem at hand, potentially leading to more accurate results.This strategy can be particularly beneficial in scenarios where the agent must learn from limited data or adapt to changing environments, as the distilled information can help guide the learning process more efficiently.</p>
<p>Prompt dependence: The LLM's performance on a specific problem heavily relies on the quality and order of prompts.In order to solve the sequential decision problem in text-based games, that is to produce more relevant and valuable responses in each round, it is crucial to craft prompts in decision making.However, creating prompts using human interaction can be time-consuming and tedious.To address this issue, our method introduces a framework that dynamically adjusts the prompt based on past trajectories through introspection, streamlining the process and improving the model's adaptability.While designing a specific tip for one game can be laborious, we also explore the possibility of generating a universal prompt that can transform the LLM into an efficient RL agent across various games and contexts.This general prompt would further facilitate the LLM's ability to adapt and perform effectively in a wide range of decision-making situations.</p>
<p>Basic setting</p>
<p>In our basic setting (Fig 2a ), we directly utilize an LLM as an agent participating in a text-based game.A sample interaction proceeds as follows: Agent: inventory( )The system (user) clearly defines the role and action space for the LLM agent and provides an initial text-based description of the environment, setting the scene for the agent.The LLM-agent interprets the provided information and decides on an action based on its understanding of the game's context.The agent submits its chosen action as text, which the system interprets, processes, updates the environment, and provides new feedback, including information on invalid actions or a description of the updated state, to the agent.The LLMagent continues to interact with the game, choosing actions and receiving feedback, until the game reaches its conclusion.The game concludes as a success if the agent completes all required steps, or as a failure if the agent takes erroneous actions or reaches the maximum number of turns.Throughout the entire process, the agent's goal is to navigate the game world and make decisions based on textual input and output.</p>
<p>Strategies for three scenarios</p>
<p>We have developed strategies for three scenarios to enhance the agent's learning addressing the two challenges:</p>
<p>Learning from its own history: The agent employs a history replay method to generate tips for self-improvement.The agent is prompted as the basic setting to finish a basic trial.If the agent fails, the agent creates several tips to address its past mistakes.In subsequent attempts, these tips serve as prompts.If the agent fails even if provided with tips, it is prompted to generate more effective tips for itself.The agent is also provided with all its previous actions that led to failure in the game.By introspectively analyzing its past actions and their outcomes, the agent generates valuable insights to refine its policy.This self-enhancement process enables the agent to overcome obstacles and make better choices in future gameplay.Moreover, since tips and incorrect actions are shorter than an entire trajectory, the agent can learn from a more extensive history than relying solely on past experiences as memory (Fig 2b).</p>
<p>Learning from expert demonstrations: The agent evaluates its performance by comparing it to expert demonstrations and makes adjustments as necessary.In this setting, the agent is instructed to observe both the actions leading the expert agent to achieve the goal and the actions causing it to fail the game.By examining successful strategies and tactics employed by experts, the agent can pinpoint areas for improvement and generat tips that can help modify its actions accordingly.This learning approach is faster compared to a purely trial-and-error method.For instance, the agent can learn the correct usage of an appliance by contrasting its failed attempts with the successful actions demonstrated by an expert in a text-based game, rather than experimenting with all possible actions.</p>
<p>The agent generates tips based on these observations, and these tips serve as new prompts when playing subsequent rounds.If the agent fails even when provided with tips, it is prompted to reflect on the given advice.This method enables knowledge transfer from expert demonstrations, resulting in a more efficient and informed decision-making process.Feedback from the environment acts as guidance for the LLM model, assisting it in determining the accuracy and effectiveness of its generated tips (Fig 2c).</p>
<p>Learning from multiple games: In this scenario, agents are allowed to participate in several games.All final tips (tips that lead to a successful trial) generated across these games are sent to another LLM, which is then tasked with generating general tips to help the agent become an expert across various game settings.In this context, the LLM leverages its knowledge of summarization to produce comprehensive and valuable tips that enable the agent to improve its performance in a wide range of games.These games share some similarities but with some information different.For example, in the cooking game, these games share a similar theme (cooking in a modern house), similar text commands, and similar entities (i.e., interactable objects within the games), but with different cookbooks and maps involved in the game.The LLM generalizes knowledge across diverse games, functioning similarly to a Meta-RL agent.The general tips are served as prompts in the unseen test game.This ability allows the LLM to perform effectively in a broad range of text-based games, positioning it as a powerful tool for reinforcement learning in complex environments (see Fig 2d).</p>
<p>By incorporating these strategies in the three scenarios, our method presents a robust framework for exploiting the potential of LLMs in reinforcement learning for text-based games.Learning from past experiences, expert demonstrations, and multiple games allows the agent to adapt and excel in various gaming scenarios, ultimately demonstrating the versatility and effectiveness of LLMs in the realm of reinforcement learning.</p>
<p>Experimental Results</p>
<p>We conduct experiments on text-based games to assess the performance of LLMs as agents and address three key questions: Q1: Can LLMs learn from their historical trajectories and improve their performance by reflecting on different tips?Q2: Can LLM learn from expert Figure 3: Few shot performance over different difficulty levels of games demonstrations and can expert demonstration boost the learning of different strategies?Q3: Can LLMs generalize some tips to play in various game settings?For Q1 and Q2, we analyze the few-shot decision-making capabilities of LLMs, while for Q3, we concentrate on their zero-shot decision-making abilities.</p>
<p>Experiment setup</p>
<p>Table 1: Statistics of The Game</p>
<p>Level #Ingredients #Locations Points (Cook, Cut, Open )
0 1 1 3 (×, ×, ) 1 1 1 4 (×, , ) 2 1 1 5 ( , , ) 3 1 9 3 (×, ×, ) 4 3 6 11 ( , , )
We concentrate on the TextWorld Cooking domain, which gained prominence through GATA (Adhikari et al., 2020) and Microsoft's TextWorld Problems: A Language and Reinforcement Learning Challenge (Trischler et al., 2019).Following Adhikari et al. (2020) and Tuli et al. (2022), we divide the games into five subsets with different difficulty levels.For easier games (with smaller difficulty levels), the recipe requires fewer ingredients, and the agent does not need to navigate through the world.For harder games, the agent is asked to navigate through different rooms to acquire various ingredients and cook the meal.The score is related to the relevant steps required in the cookbook of each game.If the agent completes a required step, it will earn one point.For the hardest game, the agent is required to finish 11 steps to complete this task.These steps involve opening certain containers to obtain the ingredients, cutting the ingredients as required (dice, slice, chop), and cooking the ingredient using the correct heat source (oven or stove).Statistics of the games are presented in Table 1.In the dataset, expert demonstrations are provided in the form of walkthroughs and do not require human intervention for generation.Following previous work, we measure the performance of the algorithms using two metrics: normalized game points and game success rate.We test the game in 20 different games for each difficulty level.By averaging the points over the 20 games, and then dividing by the maximum score an agent can earn, we obtain the normalized game points.The game success rate is calculated as the percentage of games in which the agent successfully completes all the required steps.We use GPT-4 as our base LLM.</p>
<p>Few-shot performance</p>
<p>We first test the few-shot performance of our method.We make a comparison between purely replay, tips summary using past history, and tips summary compared to expert trajectory.The latter two cases corresponds to our first and second scenario.For purely replay, we directly ingest the past trajectory as LLM's prompts to see if it can learn.Given that the input length is limited, we set the maximum number of trajectories as 3 in the experiments.For tips summary memory, the LLM agent is provided with the past failed actions and the tips generated from the last trial.We observe that with tips memory, the agent can collect more information using different trials and cumulatively achieve a high success rate (larger than 0.9) and high points earned (larger than 0.95) in the end, as shown in Figure 3.With expert demonstration, the agent can learn faster and achieve zero error in all difficulty levels after at most 6 trajectories.This comparison shows that concise tip summary assists the LLM in gathering information and making better decisions.With expert demonstrations, the LLM can learn more efficiently compared to solely relying on trial and error.We further show an example about how tip summary can help the agent make better decision.In this log file (showing only relevant information), the agent is tasked with roasting a purple potato using a predefined ActionList in a text-based game.In the first attempt, the agent mistakenly uses the stove to cook the purple potato, resulting in frying it and losing the game.The agent then generates a tip to use the "roast" action.However, this is an invalid action, and it fails again.Next, the agent generates a second tip to use the "cook" command with the stove, suggesting a different heat setting or method to avoid frying.Unfortunately, in this text-based game, there are no commands to change the heat setting, leading to another failure.After the third attempt, the agent realizes it should try using an oven to roast the potato, and finally succeeds in the fourth trial.This demonstrates that improved tips can lead to successful learning.</p>
<p>Zero-shot Performance</p>
<p>First trial: &gt;agent: cook purple potato with stove system: You fried the purple potato.Tips to win the game next time: 1.You should try roast the potato next time instead of cook purple potato with stove after dicing the purple potato;</p>
<p>Second trial: &gt;agent: roast the potato system: Invalid action.&gt;agent: cook purple potato with stove system: You fried the purple potato.Tips to win the game next time: 1.You should try cook purple potato with stove next time after you have dicing the purple potato, but make sure to use a different heat setting or method to avoid frying the purple potato; Third trial: &gt;agent: cook purple potato with stove system: You fried the purple potato.Tips to win the game next time: 1.You should try cook purple potato with oven next time instead of cook purple potato with stove, as the recipe suggests roasting the purple potato rather than frying it;</p>
<p>Forth trial: &gt;agent: cook purple potato with oven system: You roasted the purple potato.Your score has just gone up by one point.</p>
<p>In this experiment, we focus on training LLM agents using tips generated from the training set.We collect final tips, which are those leading to successful trials, from difficulty level 4 (the most challenging level) games in the training set.These tips are then sent to another LLM, which generates tips aimed at helping the agent become proficient across various game settings.The final tips can be found in the appendix.We used 10 games, which consist of 48 trajectories in total (averaging 4.8 trajectories per game) to generate the final tip.We then test these tips on an unseen test set across different difficulty levels.To evaluate our LLM model with general tips as prompts, we compare it to state-of-the-art (SOTA) text-based game agents utilizing deep learning techniques, including TDQN (Adhikari et al., 2020), GATA (Adhikari et al., 2020), and ITL (Tuli et al., 2022).These models are trained on 100 different games for each difficulty level, with 100,000 episodes per level.Results for levels 0 to 3 are provided by the authors of (Tuli et al., 2022), while level 4 results are obtained from Adhikari et al. (2020).</p>
<p>As shown in Figure 4, we prompt the LLM with general tips and test it on levels 1 to 4. Given level 0's simplicity, we do not provide tips for this level.Our findings reveal that the LLM with tips, using only 10 games and 48 trajectories, achieves performance comparable to SOTA methods for difficulty levels 0-3.For levels 3-4, LLM outperforms other methods, due to the reasoning ability of the LLM that allows us to bypass the need to learn navigation and the correct tips generated in the same difficulty level, an aspect that hinders SOTA methods (Adhikari et al., 2020).</p>
<p>Limitation Despite these achievements, the LLM agent underperforms in some lower difficulty level games.This underperformance can be attributed to two factors.First, the LLM fails to generate a general tip that addresses specific situations encountered in easier levels.For instance, in difficulty level 1, when an ingredient is already in its desired state according to the recipe (e.g., roasted or fried), the corresponding actions (roasting or frying) become unnecessary.By incorporating human-generated tips (see the appendix for the full list of tips), the LLM agent can achieve significantly higher points and success rates, as demonstrated in Table 2. Second, the LLM's probabilistic nature leads to non-deterministic outputs, which occasionally cause the agent to disregard the tips and execute erroneous actions.Despite being provided with tips, the LLM agents will still make some mistakes due to their inherent probabilistic behavior.Nevertheless, even with this randomness, the LLM agent with general tips can outperform state-of-the-art (SOTA) agents specifically trained to excel in this task.</p>
<p>Conclusion</p>
<p>We introduce the novel concept of "Introspective Tips" as a powerful mechanism to improve the decision-making capabilities of LLM agents.Drawing inspiration from human introspection, this approach enables agents to extract and learn from generalized, high-level information that can be applied across various tasks and contexts.To effectively implement Introspective Tips, we propose a framework that dynamically adjusts prompts based on insights derived from past trajectories or expert demonstrations through introspection.This approach alleviates the burden of manual prompt crafting while empowering LLM agents with self-optimizing capabilities.By leveraging the rich common sense knowledge and generalization abilities of LLMs, our Introspective Tips paradigm outperforms SOTA methods in text-based game.Future work could focus on refining the framework for prompt generation, exploring more sophisticated methods for extracting tips from trajectories, and evaluating the effectiveness of introspective tips in a broader range of tasks and real-world applications.</p>
<p>• 3. Pay attention to the cooking methods mentioned in the recipe, such as roasting or frying, and use the appropriate appliances like the oven and stove.To operate an appliance, use actions from the ActionList, such as "cook(food, heat source)".</p>
<p>• 4. Use the correct actions from the ActionList to prepare the ingredients, such as "slice(food, sharp object)", "dice(food, sharp object)", or "chop(food, sharp object)".</p>
<p>• 5. Keep track of your inventory and the state of each item to avoid making mistakes in the preparation process.Use the "inventory()" action to check your inventory.</p>
<p>• 6. Follow the steps in the recipe in the correct order, ensuring each ingredient is prepared as instructed.</p>
<p>• 7. Once all ingredients are prepared according to the recipe, use the "prepare meal()" action to combine them into a meal.</p>
<p>• 8.If you encounter any difficulties or errors, refer back to the ActionList and the environment description to make sure you're using the correct actions and items.• 2 Before cooking an ingredient, always use inventory() first to check current state of the ingredient.If an ingredient is already in the desired state(roasted or fried) as per the recipe, there is no need to perform the actions (roast or fry) mentioned in the recipe.</p>
<p>• 3 Try take(food) before you prepare(meal).After you take(food), use inventory() to check the status of the food.</p>
<p>• 4 Make sure the food is in your inventory before you prepare(meal).Try eat(meal) when the meal is prepared.</p>
<p>• 5 Dice, chop, and slice are different.</p>
<p>• 6 Fry and Roast are different.Use correct heat source to cook the ingredient.Oven is for roasting something, not frying something.</p>
<p>• 7 Use cook(food, oven) to roast something, instead of putting the food into the oven and wait.</p>
<p>• 8 Be careful when you cook(food, stove) or cook(food, oven), make sure the food is not fried or roasted before.Otherwise you may burn the food and lose the game.slice the red potato roast the red potato dice the white onion fry the white onion slice the yellow bell pepper fry the yellow bell pepper prepare meal &gt;agent : take(red potato) Env: You take the red potato from the counter.Your score has just gone up by one point.&gt;agent : take(cutting tool) Env: You take the knife from the counter.&gt;agent : slice(red potato, cutting tool) Env: You slice the red potato.Your score has just gone up by one point.&gt;agent : cook(red potato, oven) Env: You roasted the red potato.Your score has just gone up by one point.&gt;agent: go(south) Env:</p>
<p>-= Corridor =-You are in a corridor.An usual kind of place.I guess you better just go and list everything you see here.</p>
<p>There is an exit to the east.There is an exit to the north.There is an exit to the west.Don't worry, there is no door.&gt;agent : go(west) Env:</p>
<p>-= Bathroom =-You find yourself in a bathroom.An ordinary kind of place.You make out a toilet.But oh no! there's nothing on this piece of garbage.</p>
<p>You need an exit without a door?You should try going east.&gt;agent : go(east) Env: -= Corridor =-You are in a corridor.An usual kind of place.I guess you better just go and list everything you see here.</p>
<p>There is an exit to the east.There is an exit to the north.There is an exit to the west.Don't worry, there is no door.&gt;agent : go(east) Env:</p>
<p>-= Bedroom =-You are in a bedroom.An ordinary kind of place.You smell an intriguing smell, and follow it to a bed.Unfortunately, there isn't a thing on it.You move on, clearly infuriated by TextWorld.You don't like doors?Why not try going west, that entranceway is not blocked by one.&gt;agent : go(west) Env:</p>
<p>-= Corridor =-You are in a corridor.An usual kind of place.I guess you better just go and list everything you see here.</p>
<p>There is an exit to the east.There is an exit to the north.There is an exit to the west.Don't worry, there is no door.&gt;agent : go(north) Env:</p>
<p>-= kitchen =-You find yourself in a kitchen.A standard kind of place.You can see a closed fridge, which looks conventional, nearby.You can see a closed oven.What's that over there?It looks like it's a table.The table is massive.On the table you can make out a cookbook.You rest your hand against a wall, but you miss the wall and fall onto a counter!On the counter you make out a red apple, a raw purple potato, a red hot pepper, a raw yellow potato and a banana.You see a stove.The stove is conventional.However, the stove, like an empty stove, has nothing on it.There is a closed plain door leading north.You don't like doors?Why not try going south, that entranceway is not blocked by one.There is an exit to the west.&gt;agent : open(fridge) Env: You open the fridge, revealing a yellow bell pepper, a white onion, a carrot, a red onion, an orange bell pepper, a block of cheese and a raw pork chop.&gt;agent : take(white onion) Env: You take the white onion from the fridge.Your score has just gone up by one point.&gt;agent : take(yellow bell pepper) Env: You take the yellow bell pepper from the fridge.Your score has just gone up by one point.</p>
<p>Figure 2 :
2
Figure 2: LLM as RL agent</p>
<p>Figure 4 :
4
Figure 4: Performance of LLM as a text-based game agent compared to SOTA methods.Given that experimental results of difficulty level 4 is not included in Tuli et al. (2022), we obtain data from Adhikari et al. (2020) and some data is missing.</p>
<p>B. 2
2
Human-generated tips • 1 If you get stuck, use the actions goal(), examine(cookbook), inventory() and look() to figure out what you have and what you need to do.</p>
<p>Table 2 :
2
Performance of the LLM agent with human-generated tips across different difficulty levels.
Level 0Level 1Level 2Level 3Level 4Points Suc. Rate Points Suc. Rate Points Suc. Rate Points Suc. Rate Points Suc. Rate110.880.800.920.900.960.950.960.95</p>
<p>You can see a closed fridge, which looks conventional, nearby.You can see a closed oven.What's that over there?It looks like it's a table.The table is massive.On the table you can make out a cookbook.You rest your hand against a wall, but you miss the wall and fall onto a counter!On the counter you make out a raw red potato, a red apple, a raw purple potato, a red hot pepper, a raw yellow potato, a banana and a knife.You see a stove.The stove is conventional.However, the stove, like an empty stove, has nothing on it.There is a closed plain door leading north.You don't like doors?Why not try going south, that entranceway is not blocked by one.There is an exit to the west.&gt;agent: examine(cookbook) Env: You open the copy of "Cooking: A Modern Approach (3rd Ed.)" and start reading: Recipe #1 ---Gather all following ingredients and follow the directions to prepare this tasty meal.
Ingredients:red potatowhite onionyellow bell pepperDirections:
• take(item) # take an item that is on the floor • put(item, supporter) # place an item on a supporter• take from(item, container) # take an item from a container or a supporter• insert(item, container) # place an item into a container• lock(item, key) # lock a door or a container with a key• unlock(item, key) # unlock a door or a container with a key• cook(food, heat source) # cook cookable food with something providing heat • slice(food, sharp object) # slice cuttable food with something sharp• chop(food, sharp object) # chop cuttable food with something sharp• dice(food, sharp object) # dice cuttable food with something sharp• prepare meal() # combine ingredients from inventory into a mealAppendix B. Gnenral TipsB.1 Tips generated using 10 games • 1. Always double-check the recipe before performing any actions, and follow the instructions closely.• 2. Make sure to gather all required ingredients and tools before starting to cook.Use the "examine(item)" and "take(item)" actions to interact with items in the environment.Appendix C. PromptsPrompts for generating the tips: Generating tips for the first trial Now you failed the game.Come up with tips for yourself to win the game next time.Pay attention to steps that make you lose the game.Pay attention to those actions and come up with concise actions in the ActionList that you should try next time.Starting your response with "Tips to win the game next time:"Generating tips for other trial Now you failed the game.Come up with tips for yourself to win the game next time.Pay attention to steps that make you lose the game.Make a note on those actions and come up with concise actions in the ActionList that you should try next time.Correct and improve the previous tips that I gave you if you think some of them are wrong.Generating general tips I want you to provide me all possible specific reasons that an agent may fail in a textbased game and summarize all tips for the agent to improve.I will present you several reasons and tips that summarized by others before.In every file, the action that make an agent failed before and some tips to correct this error are provided.Noticed that the tips provided may not be correct.For example, some actions suggested by the tips are not in the ActionList.Correct this kind of error in the tips that you generate.This tips will be used by another rookie agent.So make sure to provide all necessary and userful information for the rookie agent.You should also include action in the ActionList that the agent should use to operate appliance.Appendix D. Sample TrajectoryEnv: You are hungry!Let's cook a delicious meal.Check the cookbook in the kitchen for the recipe.Once done, enjoy your meal! -= Bedroom =-You are in a bedroom.An ordinary kind of place.You smell an intriguing smell, and follow it to a bed.Unfortunately, there isn't a thing on it.You move on, clearly infuriated by TextWorld.You don't like doors?Why not try going west, that entranceway is not blocked by one.&gt;agent: go(west) Env:-= Corridor =-You are in a corridor.An usual kind of place.I guess you better just go and list everything you see here.There is an exit to the east.There is an exit to the north.There is an exit to the west.Don't worry, there is no door.&gt;agent : go(north) Env:-= Kitchen =-You find yourself in a kitchen.A standard kind of place.
Learning dynamic belief graphs to generalize on text-based games. A Adhikari, X Yuan, M.-A Côté, M Zelinka, M.-A Rondeau, R Laroche, P Poupart, J Tang, A Trischler, W Hamilton, Advances in Neural Information Processing Systems. 202033</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, K Gopalakrishnan, K Hausman, A Herzog, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Graph constrained reinforcement learning for natural language action spaces. P Ammanabrolu, M Hausknecht, arXiv:2001.088372020arXiv preprint</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Y Bai, A Jones, K Ndousse, A Askell, A Chen, N Dassarma, D Drain, S Fort, D Ganguli, T Henighan, arXiv:2204.058622022arXiv preprint</p>
<p>E Brooks, L Walls, R L Lewis, S Singh, arXiv:2210.03821-context policy iteration. 2022arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Safe learning in robotics: From learning-based control to safe reinforcement learning. L Brunke, M Greeff, A W Hall, Z Yuan, S Zhou, J Panerati, A P Schoellig, Robotics, and Autonomous Systems. 52022Annual Review of Control</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Textworld: A learning environment for text-based games. M.-A Côté, A Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, M Hausknecht, L El Asri, M Adada, Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI. Revised Selected Papers. Stockholm, SwedenSpringer2018. July 13. 2018. 20197</p>
<p>Introspective multistrategy learning: Constructing a learning strategy under reasoning failure. M T Cox, 1996Georgia Institute of Technology</p>
<p>),<em> advances in neural information processing systems (datasets and benchmarks track),</em> 2021. D Hendrycks, M Mazeika, A Zou, S Patel, C Zhu, J Navarro, D Song, B Li, J Steinhardt, Advances in neural information processing systems. 2021what would jiminy cricket do? towards agents that behave morally</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International Conference on Machine Learning. PMLR2022a</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, arXiv:2207.056082022barXiv preprint</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, ACM Computing Surveys. 55122023</p>
<p>Neuro-symbolic reinforcement learning with first-order logic. D Kimura, M Ono, S Chaudhury, R Kohita, A Wachi, D J Agravante, M Tatsubori, A Munawar, A Gray, arXiv:2110.109632021arXiv preprint</p>
<p>M Kwon, S M Xie, K Bullard, D Sadigh, arXiv:2303.00001Reward design with language models. 2023arXiv preprint</p>
<p>M Laskin, L Wang, J Oh, E Parisotto, S Spencer, R Steigerwald, D Strouse, S Hansen, A Filos, E Brooks, arXiv:2210.14215-context reinforcement learning with algorithm distillation. 2022arXiv preprint</p>
<p>Pre-trained language models for interactive decision-making. S Li, X Puig, C Paxton, Y Du, C Wang, L Fan, T Chen, D.-A Huang, E Akyürek, A Anandkumar, A Jacob, M Igor, T Antonio, Z Yuke, Advances in Neural Information Processing Systems. 202235</p>
<p>J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, P Florence, A Zeng, arXiv:2209.07753Code as policies: Language model programs for embodied control. 2022arXiv preprint</p>
<p>Learning object-oriented dynamics for planning from text. G Liu, A Adhikari, A -M. Farahmand, P Poupart, International Conference on Learning Representations. 2022</p>
<p>Structured state space models for in-context reinforcement learning. C Lu, Y Schroecker, A Gu, E Parisotto, J Foerster, S Singh, F Behbahani, arXiv:2303.039822023arXiv preprint</p>
<p>K Lu, A Grover, P Abbeel, I Mordatch, arXiv:2103.05247Pretrained transformers as universal computation engines. 20211arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, arXiv:2303.176512023arXiv preprint</p>
<p>Introducing llama: A foundational, 65-billion-parameter large language model. A Meta, Meta AI. 2023</p>
<p>S Min, X Lyu, A Holtzman, M Artetxe, M Lewis, H Hajishirzi, L Zettlemoyer, arXiv:2202.12837Rethinking the role of demonstrations: What makes in-context learning work?. 2022arXiv preprint</p>
<p>. ArXiv, abs/2303.08774OpenAI. Gpt-4 technical report. 2023</p>
<p>A survey of text games for reinforcement learning informed by natural language. P Osborne, H Nõmm, A Freitas, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. B Peng, M Galley, P He, H Cheng, Y Xie, Y Hu, Q Huang, L Liden, Z Yu, W Chen, arXiv:2302.128132023arXiv preprint</p>
<p>. S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, arXiv:2205.061752022A generalist agent. arXiv preprint</p>
<p>The limits of machine intelligence: Despite progress in machine intelligence, artificial general intelligence is still a major challenge. H Shevlin, K Vold, M Crosby, M Halina, EMBO reports. 2010e491772019</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. N Shinn, B Labash, A Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, arXiv:2209.11302Progprompt: Generating situated robot task plans using large language models. 2022arXiv preprint</p>
<p>Stanford alpaca: An instruction-following llama model. R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, 2023</p>
<p>Y Tassa, Y Doron, A Muldal, T Erez, Y Li, D D L Casas, D Budden, A Abdolmaleki, J Merel, A Lefrancq, arXiv:1801.00690Deepmind control suite. 2018arXiv preprint</p>
<p>Human-timescale adaptation in an open-ended task space. A A Team, J Bauer, K Baumli, S Baveja, F Behbahani, A Bhoopchand, N Bradley-Schmieg, M Chang, N Clay, A Collister, arXiv:2301.076082023arXiv preprint</p>
<p>First textworld problems, the competition: Using text-based games to advance capabilities of ai agents. A Trischler, M.-A Côté, P Lima, 2019Microsoft Research Blog</p>
<p>Learning to follow instructions in text-based games. M Tuli, A Li, P Vaezipoor, T Klassen, S Sanner, S Mcilraith, Advances in Neural Information Processing Systems. 202235</p>
<p>Cognitive load theory and complex learning: Recent developments and future directions. J J Van Merrienboer, J Sweller, Educational psychology review. 2005</p>
<p>Chatgpt for robotics: Design principles and model abilities. S Vemprala, R Bonatti, A Bucker, A Kapoor, 2023</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, E Chi, Q Le, D Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022arXiv preprint</p>
<p>Perceiving the world: Questionguided reinforcement learning for text-based games. M Xu, L Fang, Y Chen, J T Du, C Zhou, Zhang, arXiv:2204.095972022arXiv preprint</p>
<p>Foundation models for decision making: Problems, methods, and opportunities. S Yang, O Nachum, Y Du, J Wei, P Abbeel, D Schuurmans, arXiv:2303.041292023arXiv preprint</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Improving sample efficiency in model-free reinforcement learning from images. D Yarats, A Zhang, I Kostrikov, B Amos, J Pineau, R Fergus, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<blockquote>
<p>agent: dice(white onion, cutting tool) Env: You dice the white onion. Your score has just gone up by one point. &gt;agent: cook(white onion, stove) Env: You fried the white onion. Your score has just gone up by one point. &gt;agent: slice(yellow bell pepper, cutting tool) Env: You slice the yellow bell pepper. Your score has just gone up by one point. &gt;agent: cook(yellow bell pepper, stove) Env: You fried the yellow bell pepper. Your score has just gone up by one point. X Yin, J May, 2019 IEEE Conference on Games (CoG). IEEE2019Env: Adding the meal to your inventory. Your score has just gone up by one point. &gt;agent: eat(meal) Env: You eat the meal. Not bad. Your score has just gone up by one point. *** The End *** You scored 11 out of a possible 11, in 23 turn(s</p>
</blockquote>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5241 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5241</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5241</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-269449627</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.18271v1.pdf" target="_blank">Parameter-Efficient Tuning Large Language Models for Graph Representation Learning</a></p>
                <p><strong>Paper Abstract:</strong> Text-rich graphs, which exhibit rich textual information on nodes and edges, are prevalent across a wide range of real-world business applications. Large Language Models (LLMs) have demonstrated remarkable abilities in understanding text, which also introduced the potential for more expressive modeling in text-rich graphs. Despite these capabilities, efficiently applying LLMs to representation learning on graphs presents significant challenges. Recently, parameter-efficient fine-tuning methods for LLMs have enabled efficient new task generalization with minimal time and memory consumption. Inspired by this, we introduce Graph-aware Parameter-Efficient Fine-Tuning - GPEFT, a novel approach for efficient graph representation learning with LLMs on text-rich graphs. Specifically, we utilize a graph neural network (GNN) to encode structural information from neighboring nodes into a graph prompt. This prompt is then inserted at the beginning of the text sequence. To improve the quality of graph prompts, we pre-trained the GNN to assist the frozen LLM in predicting the next token in the node text. Compared with existing joint GNN and LMs, our method directly generate the node embeddings from large language models with an affordable fine-tuning cost. We validate our approach through comprehensive experiments conducted on 8 different text-rich graphs, observing an average improvement of 2% in hit@1 and Mean Reciprocal Rank (MRR) in link prediction evaluations. Our results demonstrate the efficacy and efficiency of our model, showing that it can be smoothly integrated with various large language models, including OPT, LLaMA and Falcon.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5241.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5241.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-Graph Context Learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Graph Context Learning (neighbor-to-text in-context prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods that linearize a node's neighborhood and graph relations into natural-language context strings (e.g., "node_1: text, node_2: text, node_1 is connected to node_2...") and feed that textual context to an LLM for downstream prediction or probing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>neighbor-text serialization / in-graph context</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Neighbors and local graph facts are serialized into plain text sequences that describe node identifiers, node text, and relations (often templated phrases like "node_i: <text>" and "node_i is connected to node_j"). The composed natural language string is concatenated into the LLM input as a prompt (in-context example) for prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-rich / text-attributed graphs (nodes with textual attributes; can be heterogeneous graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Produces fully human-readable natural-language encodings; simple to implement; assumes equal importance of listed neighbors unless additional weighting is encoded; tends to produce very long input sequences in dense graphs (poor compactness); expressive for small neighbor sets but not faithful/scalable for large neighborhoods; interpretation is straightforward but may lose structural inductive biases unless relations are explicitly and systematically described.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used for LLM-based predictions including node degree, node labels, and link existence; in this paper discussed in the context of link prediction and representation learning baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Evaluated using link-prediction metrics Hit@1 and MRR in this paper; paper reports that in-graph context methods (e.g., InstructGLM) outperform some architecture baselines (GraphFormers) but underperform carefully fine-tuned GNN-LLM approaches (PATTON, GPEFT). Exact per-dataset numbers are reported in the paper's tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to dense graph-prompt approaches (GPEFT's GNN-produced soft prompts and virtual-structure-token methods like PATTON/GraphFormers): in-graph text-context methods can be strong when using high-quality LLMs but suffer in scalability and typically yield worse link-prediction accuracy than GNN+PEFT approaches in the reported experiments. The paper states: InstructGLM (textual in-context baseline) outperforms GraphFormers but falls short of PATTON and GPEFT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Does not scale to dense, large graphs because textual prompts grow long and inference for link-prediction across many candidate pairs can be quadratic (O(n^2)); treating all neighbors equally in a textual list is naive; converting structural relations into natural language can fail to capture fine-grained pairwise similarity structure useful for representation-based link prediction; high API/cost burden if using external LLM services to generate or score many textified neighborhood prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parameter-Efficient Tuning Large Language Models for Graph Representation Learning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5241.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5241.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGLM-embeddings (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adapted InstructGLM embedding pipeline (InstructGLM-embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adapted baseline used in the paper which converts graph structure and node features into natural-language descriptions via an LLM (InstructGLM pipeline) and then obtains embeddings (they used a public embedding API) for downstream representation/link-prediction evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>LLM natural-language serialization followed by embedding (InstructGLM-embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph substructures and neighbor node texts are composed into natural-language prompts using an LLM-based pipeline; the composed textual descriptions are passed through a text-embedding model (the authors used a public embedding API: text-embedding-3-small) to obtain dense embeddings for nodes, which are then used for ranking/ link prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-rich graphs (e.g., Amazon product co-view/co-purchase graphs, Microsoft Academic Graph subgraphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Encodes graph facts in plain language and leverages pretrained LLMs' knowledge; results in dense embeddings after applying an embedding model; more interpretable than opaque learned soft prompts; susceptible to variability from prompt wording and external API embedding model choice; compactness depends on how much graph info is verbalized (can be large).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Link prediction (co-purchase on Amazon subdomains; same-author or citation-related tasks on MAG subdomains).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Evaluated with Hit@1 and Mean Reciprocal Rank (MRR). The paper reports that InstructGLM-embeddings outperforms GraphFormers but is outperformed by PATTON and the proposed GPEFT in the reported link-prediction experiments (see paper tables for per-dataset Hit@1 and MRR).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Direct comparison (reported in tables): InstructGLM-embeddings > GraphFormers, but < PATTON and < GPEFT (GPEFT yields the best overall link-prediction accuracy among compared LLM/GNN hybrids in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High inference cost if used directly for pairwise link scoring across many candidates (quadratic scaling) unless embeddings are cached; dependence on external embedding APIs can be expensive and non-differentiable (limits joint training); prompt design and LLM variability affect representation quality; textualization may fail to preserve structural nuances unless carefully templated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parameter-Efficient Tuning Large Language Models for Graph Representation Learning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5241.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5241.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neighbor-text serialization critique (this paper's alternative)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dense GNN-produced graph prompts (GPEFT's graph prompt instead of textual linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper contrasts neighbor-to-text serialization with its proposed dense graph prompt approach: instead of converting neighbors to text, they encode neighbor structure and texts into dense vectors via a GNN and insert those vectors as soft prompts to an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GNN-to-soft-prompt (dense vector prompt) — graph prompt</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Node texts are first encoded (they used a small pretrained LM e.g. BERT) into document vectors; a message-passing GNN (e.g., GraphSAGE/GCN) aggregates neighbor information to produce a GNN embedding z_i for each node; z_i is mapped into the LLM's word-embedding space via a learned projection and inserted as a soft prompt (prepended embeddings) to the LLM input. A special [GRL] token's final hidden state is used as the node representation for contrastive link-prediction training.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-rich / text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compact (dense vectors rather than long textual prompts); differentiable and trainable end-to-end (with PEFT for the LLM); scalable for large graphs because node embeddings can be precomputed and similarity searched (O(n) embedding generation + O(n) similarity operations rather than O(n^2) per-edge in-context scoring); better alignment to LLM embedding space via pretraining of GNN prompt encoder using next-token prediction (causal LM objective).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Link prediction via contrastive training (one negative sample), evaluated with Hit@1 and MRR on Amazon Review and MAG subdomains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Hit@1 and MRR; the paper reports GPEFT achieves average improvements of ~2% in Hit@1 and MRR over vanilla PEFT LLaMA and ~3% over existing GNN-LMs across 8 graphs. Pretraining the GNN prompt encoder improves accuracy (notably a ~6% drop on MAG without pretraining). Exact per-dataset numbers are in the paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared against (a) cascaded GNN-LMs that use LMs as frozen feature extractors, (b) nested GNN-LMs (GraphFormers, PATTON), and (c) in-graph context/textualization methods (InstructGLM). GPEFT (dense graph prompts + PEFT) is reported to outperform baseline GNN-LMs and in-graph textualization baselines on link-prediction while being more parameter- and compute-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires pretraining of the GNN prompt encoder to align its outputs to the LLM embedding space (next-token prediction objective); introduces additional (but small) trainable parameters (authors report ~5.5M for GNN prompt encoder and ~14.4M trainable PEFT params in certain configs); choice of number of GNN hops trades off richer structure vs. noise aggregation; design choices needed to integrate with different PEFT variants (LoRA vs prefix-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parameter-Efficient Tuning Large Language Models for Graph Representation Learning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5241.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5241.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt placement (prepend vs append)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prepending vs Appending Graph Prompts (theoretical analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper proves that prepending a graph prompt to the LLM input is more expressive than appending it for causal language models: prepending allows graph prompt embeddings to influence token hidden states via attention, whereas appending yields only a convex combination and limited influence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>prompt placement (prepend vs append)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Two strategies for placing a graph-derived prompt relative to node text: (1) append graph prompt tokens after the node text, or (2) prepend graph prompt tokens before the node text. Theorem A.1 shows that in causal LMs, appending yields a convex-combination effect and does not change intermediate text token hidden states, while prepending allows the prompt to participate in attention computations for all tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>applies to any text-rich graph prompting used with causal LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Prepending is provably more expressive for causal LMs because it modifies hidden states of all tokens; appending behaves like cascading (LM and graph encoder remain independent) and thus is less expressive; prepending can better integrate structural information into token-level context representation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Theoretical expressivity argument; chosen prepending strategy is used in GPEFT's implementation and evaluated via downstream link-prediction experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not a metric-based evaluation per se; the paper uses the theoretical argument to motivate prepending and then empirically uses prepending in the GPEFT experiments which yield improved Hit@1 and MRR over baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>The paper contrasts the two placements and argues prepending (used by GPEFT) is superior to appending (used implicitly in some cascading or in-context methods).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prepending requires careful integration with PEFT mechanisms (e.g., with prefix-tuning, a modification is required so graph prompt affects prefix embeddings). Prepending also increases the effective input length seen by the LLM which may interact with sequence length limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parameter-Efficient Tuning Large Language Models for Graph Representation Learning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>InstructGLM <em>(Rating: 2)</em></li>
                <li>Talk like a Graph: Encoding Graphs for Large Language Models. <em>(Rating: 2)</em></li>
                <li>Graph Neural Prompting with Large Language Models. <em>(Rating: 2)</em></li>
                <li>PATTON: Language Model Pretraining on Text-Rich Networks. <em>(Rating: 2)</em></li>
                <li>GraphFormers: GNN-nested transformers for representation learning on textual graph. <em>(Rating: 2)</em></li>
                <li>Exploring the potential of large language models (llms) in learning on graphs. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5241",
    "paper_id": "paper-269449627",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "In-Graph Context Learning",
            "name_full": "In-Graph Context Learning (neighbor-to-text in-context prompting)",
            "brief_description": "A family of methods that linearize a node's neighborhood and graph relations into natural-language context strings (e.g., \"node_1: text, node_2: text, node_1 is connected to node_2...\") and feed that textual context to an LLM for downstream prediction or probing tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "neighbor-text serialization / in-graph context",
            "representation_description": "Neighbors and local graph facts are serialized into plain text sequences that describe node identifiers, node text, and relations (often templated phrases like \"node_i: &lt;text&gt;\" and \"node_i is connected to node_j\"). The composed natural language string is concatenated into the LLM input as a prompt (in-context example) for prediction tasks.",
            "graph_type": "text-rich / text-attributed graphs (nodes with textual attributes; can be heterogeneous graphs)",
            "representation_properties": "Produces fully human-readable natural-language encodings; simple to implement; assumes equal importance of listed neighbors unless additional weighting is encoded; tends to produce very long input sequences in dense graphs (poor compactness); expressive for small neighbor sets but not faithful/scalable for large neighborhoods; interpretation is straightforward but may lose structural inductive biases unless relations are explicitly and systematically described.",
            "evaluation_task": "Used for LLM-based predictions including node degree, node labels, and link existence; in this paper discussed in the context of link prediction and representation learning baselines.",
            "performance_metrics": "Evaluated using link-prediction metrics Hit@1 and MRR in this paper; paper reports that in-graph context methods (e.g., InstructGLM) outperform some architecture baselines (GraphFormers) but underperform carefully fine-tuned GNN-LLM approaches (PATTON, GPEFT). Exact per-dataset numbers are reported in the paper's tables.",
            "comparison_to_other_representations": "Compared to dense graph-prompt approaches (GPEFT's GNN-produced soft prompts and virtual-structure-token methods like PATTON/GraphFormers): in-graph text-context methods can be strong when using high-quality LLMs but suffer in scalability and typically yield worse link-prediction accuracy than GNN+PEFT approaches in the reported experiments. The paper states: InstructGLM (textual in-context baseline) outperforms GraphFormers but falls short of PATTON and GPEFT.",
            "limitations_or_challenges": "Does not scale to dense, large graphs because textual prompts grow long and inference for link-prediction across many candidate pairs can be quadratic (O(n^2)); treating all neighbors equally in a textual list is naive; converting structural relations into natural language can fail to capture fine-grained pairwise similarity structure useful for representation-based link prediction; high API/cost burden if using external LLM services to generate or score many textified neighborhood prompts.",
            "uuid": "e5241.0",
            "source_info": {
                "paper_title": "Parameter-Efficient Tuning Large Language Models for Graph Representation Learning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "InstructGLM-embeddings (adapted)",
            "name_full": "Adapted InstructGLM embedding pipeline (InstructGLM-embeddings)",
            "brief_description": "An adapted baseline used in the paper which converts graph structure and node features into natural-language descriptions via an LLM (InstructGLM pipeline) and then obtains embeddings (they used a public embedding API) for downstream representation/link-prediction evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "LLM natural-language serialization followed by embedding (InstructGLM-embeddings)",
            "representation_description": "Graph substructures and neighbor node texts are composed into natural-language prompts using an LLM-based pipeline; the composed textual descriptions are passed through a text-embedding model (the authors used a public embedding API: text-embedding-3-small) to obtain dense embeddings for nodes, which are then used for ranking/ link prediction.",
            "graph_type": "text-rich graphs (e.g., Amazon product co-view/co-purchase graphs, Microsoft Academic Graph subgraphs)",
            "representation_properties": "Encodes graph facts in plain language and leverages pretrained LLMs' knowledge; results in dense embeddings after applying an embedding model; more interpretable than opaque learned soft prompts; susceptible to variability from prompt wording and external API embedding model choice; compactness depends on how much graph info is verbalized (can be large).",
            "evaluation_task": "Link prediction (co-purchase on Amazon subdomains; same-author or citation-related tasks on MAG subdomains).",
            "performance_metrics": "Evaluated with Hit@1 and Mean Reciprocal Rank (MRR). The paper reports that InstructGLM-embeddings outperforms GraphFormers but is outperformed by PATTON and the proposed GPEFT in the reported link-prediction experiments (see paper tables for per-dataset Hit@1 and MRR).",
            "comparison_to_other_representations": "Direct comparison (reported in tables): InstructGLM-embeddings &gt; GraphFormers, but &lt; PATTON and &lt; GPEFT (GPEFT yields the best overall link-prediction accuracy among compared LLM/GNN hybrids in this work).",
            "limitations_or_challenges": "High inference cost if used directly for pairwise link scoring across many candidates (quadratic scaling) unless embeddings are cached; dependence on external embedding APIs can be expensive and non-differentiable (limits joint training); prompt design and LLM variability affect representation quality; textualization may fail to preserve structural nuances unless carefully templated.",
            "uuid": "e5241.1",
            "source_info": {
                "paper_title": "Parameter-Efficient Tuning Large Language Models for Graph Representation Learning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Neighbor-text serialization critique (this paper's alternative)",
            "name_full": "Dense GNN-produced graph prompts (GPEFT's graph prompt instead of textual linearization)",
            "brief_description": "The paper contrasts neighbor-to-text serialization with its proposed dense graph prompt approach: instead of converting neighbors to text, they encode neighbor structure and texts into dense vectors via a GNN and insert those vectors as soft prompts to an LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "GNN-to-soft-prompt (dense vector prompt) — graph prompt",
            "representation_description": "Node texts are first encoded (they used a small pretrained LM e.g. BERT) into document vectors; a message-passing GNN (e.g., GraphSAGE/GCN) aggregates neighbor information to produce a GNN embedding z_i for each node; z_i is mapped into the LLM's word-embedding space via a learned projection and inserted as a soft prompt (prepended embeddings) to the LLM input. A special [GRL] token's final hidden state is used as the node representation for contrastive link-prediction training.",
            "graph_type": "text-rich / text-attributed graphs",
            "representation_properties": "Compact (dense vectors rather than long textual prompts); differentiable and trainable end-to-end (with PEFT for the LLM); scalable for large graphs because node embeddings can be precomputed and similarity searched (O(n) embedding generation + O(n) similarity operations rather than O(n^2) per-edge in-context scoring); better alignment to LLM embedding space via pretraining of GNN prompt encoder using next-token prediction (causal LM objective).",
            "evaluation_task": "Link prediction via contrastive training (one negative sample), evaluated with Hit@1 and MRR on Amazon Review and MAG subdomains.",
            "performance_metrics": "Hit@1 and MRR; the paper reports GPEFT achieves average improvements of ~2% in Hit@1 and MRR over vanilla PEFT LLaMA and ~3% over existing GNN-LMs across 8 graphs. Pretraining the GNN prompt encoder improves accuracy (notably a ~6% drop on MAG without pretraining). Exact per-dataset numbers are in the paper tables.",
            "comparison_to_other_representations": "Compared against (a) cascaded GNN-LMs that use LMs as frozen feature extractors, (b) nested GNN-LMs (GraphFormers, PATTON), and (c) in-graph context/textualization methods (InstructGLM). GPEFT (dense graph prompts + PEFT) is reported to outperform baseline GNN-LMs and in-graph textualization baselines on link-prediction while being more parameter- and compute-efficient.",
            "limitations_or_challenges": "Requires pretraining of the GNN prompt encoder to align its outputs to the LLM embedding space (next-token prediction objective); introduces additional (but small) trainable parameters (authors report ~5.5M for GNN prompt encoder and ~14.4M trainable PEFT params in certain configs); choice of number of GNN hops trades off richer structure vs. noise aggregation; design choices needed to integrate with different PEFT variants (LoRA vs prefix-tuning).",
            "uuid": "e5241.2",
            "source_info": {
                "paper_title": "Parameter-Efficient Tuning Large Language Models for Graph Representation Learning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Prompt placement (prepend vs append)",
            "name_full": "Prepending vs Appending Graph Prompts (theoretical analysis)",
            "brief_description": "The paper proves that prepending a graph prompt to the LLM input is more expressive than appending it for causal language models: prepending allows graph prompt embeddings to influence token hidden states via attention, whereas appending yields only a convex combination and limited influence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "prompt placement (prepend vs append)",
            "representation_description": "Two strategies for placing a graph-derived prompt relative to node text: (1) append graph prompt tokens after the node text, or (2) prepend graph prompt tokens before the node text. Theorem A.1 shows that in causal LMs, appending yields a convex-combination effect and does not change intermediate text token hidden states, while prepending allows the prompt to participate in attention computations for all tokens.",
            "graph_type": "applies to any text-rich graph prompting used with causal LLMs",
            "representation_properties": "Prepending is provably more expressive for causal LMs because it modifies hidden states of all tokens; appending behaves like cascading (LM and graph encoder remain independent) and thus is less expressive; prepending can better integrate structural information into token-level context representation.",
            "evaluation_task": "Theoretical expressivity argument; chosen prepending strategy is used in GPEFT's implementation and evaluated via downstream link-prediction experiments.",
            "performance_metrics": "Not a metric-based evaluation per se; the paper uses the theoretical argument to motivate prepending and then empirically uses prepending in the GPEFT experiments which yield improved Hit@1 and MRR over baselines.",
            "comparison_to_other_representations": "The paper contrasts the two placements and argues prepending (used by GPEFT) is superior to appending (used implicitly in some cascading or in-context methods).",
            "limitations_or_challenges": "Prepending requires careful integration with PEFT mechanisms (e.g., with prefix-tuning, a modification is required so graph prompt affects prefix embeddings). Prepending also increases the effective input length seen by the LLM which may interact with sequence length limits.",
            "uuid": "e5241.3",
            "source_info": {
                "paper_title": "Parameter-Efficient Tuning Large Language Models for Graph Representation Learning",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "InstructGLM",
            "rating": 2,
            "sanitized_title": "instructglm"
        },
        {
            "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models.",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Graph Neural Prompting with Large Language Models.",
            "rating": 2,
            "sanitized_title": "graph_neural_prompting_with_large_language_models"
        },
        {
            "paper_title": "PATTON: Language Model Pretraining on Text-Rich Networks.",
            "rating": 2,
            "sanitized_title": "patton_language_model_pretraining_on_textrich_networks"
        },
        {
            "paper_title": "GraphFormers: GNN-nested transformers for representation learning on textual graph.",
            "rating": 2,
            "sanitized_title": "graphformers_gnnnested_transformers_for_representation_learning_on_textual_graph"
        },
        {
            "paper_title": "Exploring the potential of large language models (llms) in learning on graphs.",
            "rating": 1,
            "sanitized_title": "exploring_the_potential_of_large_language_models_llms_in_learning_on_graphs"
        }
    ],
    "cost": 0.01457025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Parameter-Efficient Tuning Large Language Models for Graph Representation Learning
28 Apr 2024</p>
<p>Qi Zhu 
Da Zheng dzzhen@amazon.com 
Shichang Zhang shichang@cs.ucla.edu 
Bowen Jin bowenj4@illinois.edu 
Yizhou Sun yzsun@cs.ucla.edu 
George Karypis gkarypis@amazon.com </p>
<p>University of California
Los Angeles</p>
<p>University of Illinois Urbana-Champaign</p>
<p>University of California
Los Angeles</p>
<p>Qi Zhu
Da Zheng, Xiang Song, Shichang ZhangBowen Jin, Yizhou Sun</p>
<p>Qi Zhu
Conference'17, July 2017, Da Zheng, Xiang Song, Shichang ZhangWashington, Bowen Jin, Yizhou Sun, and George KarypisDCUSA</p>
<p>Parameter-Efficient Tuning Large Language Models for Graph Representation Learning
28 Apr 2024A4B8CB444294D7AE8C2B37784FCFCEB810.1145/nnnnnnn.nnnnnnnarXiv:2404.18271v1[cs.CL]Representation LearningGraphsLanguage Models SocialGraph:: recommendation SocialGraph:: Fraud_detection E-Commerce:: Product_search E-Commerce:: brand_prediction PEFT Module
Text-rich graphs, which exhibit rich textual information on nodes and edges, are prevalent across a wide range of real-world business applications.Large Language Models (LLMs) have demonstrated remarkable abilities in understanding text, which also introduced the potential for more expressive modeling in text-rich graphs.Despite these capabilities, efficiently applying LLMs to representation learning on graphs presents significant challenges.Recently, parameterefficient fine-tuning methods for LLMs have enabled efficient new task generalization with minimal time and memory consumption.Inspired by this, we introduce Graph-aware Parameter-Efficient Fine-Tuning -GPEFT, a novel approach for efficient graph representation learning with LLMs on text-rich graphs.Specifically, we utilize a graph neural network (GNN) to encode structural information from neighboring nodes into a graph prompt.This prompt is then inserted at the beginning of the text sequence.To improve the quality of graph prompts, we pre-trained the GNN to assist the frozen LLM in predicting the next token in the node text.Compared with existing joint GNN and LMs, our method directly generate the node embeddings from large language models with an affordable fine-tuning cost.We validate our approach through comprehensive experiments conducted on 8 different text-rich graphs, observing an average improvement of 2% in hit@1 and Mean Reciprocal Rank (MRR) in link prediction evaluations.Our results demonstrate the efficacy and efficiency of our model, showing that it can be smoothly integrated with various large language models, including OPT, LLaMA and Falcon.</p>
<p>INTRODUCTION</p>
<p>Information networks form the backbone of modern data systems: millions of daily social posts are shared among users on platforms like Facebook and Twitter; countless papers are published and cited within academic networks.Many of these networks are rich in textual information on various types of objects, known as text-rich or text-attributed networks.For example, in an e-commerce graph, text-rich information could be used to predict links by analyzing product descriptions, user reviews, and metadata to recommend items that are frequently bought together.Such real-world applications typify the problem of link prediction, where representation learning (i.e., embedding) emerges as the most prevalent solution.</p>
<p>Representation learning on graph-structured data aims to learn a dense vector for each node through self-supervised tasks such as link prediction [9,34,46] or masked attribute recovery [14].These dense vectors can be widely utilized in various downstream applications including search, ranking and retrieval.To effectively utilize both node attributes and structural information in representation learning, Graph Neural Networks (GNNs) [11,21] devise a novel type of neural networks with message-passing mechanism.In text-rich graphs, raw text is usually transformed into feature vectors by a text encoder prior to the application of Graph Neural Networks (GNNs).Early text encoders, such as bag-of-words and Word2Vec [32] , are being gradually phased out due to significant improvements in text representation from transformer-based language models (LMs).</p>
<p>This has spurred interest in jointly modeling textual and structural data with GNNs and LMs.Notable architectures include (1) a cascading architecture that integrates GNNs with LM features (cascading GNN-LMs); (2) nested GNN-LMs that blend message passing into the language modeling process.Examples of the latter include GraphFormer [44] and Heterformer [20], which introduce a nested GNN-transformer architecture for homogeneous and heterogeneous graph, respectively.Historically, GNN-LMs have focused on combining GNNs with medium-sized LMs such as BERT [4] and RoBERTa [27].However, large language models (LLMs) [1,42,47] with billions of parameters, known for their exceptional multi-task generalization and instruction-following capabilities, present an untapped potential for further enhancing the performance of these models.Applying large language models (LLMs) to text-rich graphs presents an intriguing concept; however, fine-tuning such GNN-LLM models, as with existing architecures, is impractical due to the prohibitively high computational and memory demands.As an alternative, parameter-efficient fine-tuning (PEFT) techniques for LLMs, such as LORA [16] and Prefix Tuning [24], advocate for updating a minimal fraction of parameters (e.g., less than 1%) to achieve comparable performance.Inspired by this, our method integrates graph encoding into the PEFT framework, which enables efficient graph representation learning with LLMs.Contrary to PEFT, another line of research [6,45] explores incontext learning of LLMs without weight updates.These works transform the neighboring context of a target node into textual descriptions, incorporating features, and employ LLMs to make predictions such as node degrees, labels, and the existence of links.However, applying these methods to representation learning at an industrial scale presents notable challenges.These approaches are primarily tailored for tasks involving a limited number of candidates and do not account for the similarity between any pair of nodes.In a graph with  nodes, they require a substantial inference cost of O ( 2 ) to iterate every positive edge, in contrast to the more efficient strategy of generating node embeddings at a cost of O ( ) and subsequently calculating their similarities within the embedding space (see also the inference cost of in-context learning in Table 1).This disparity shows the limitation of in-context learning for text-rich graphs and emphasizes the need for more scalable methods to handle large-scale graph data efficiently.</p>
<p>In this work, we introduce Graph-aware Parameter-Efficient Fine-Tuning (i.e., GPEFT) for light-weighted and efficient graph representation learning with LLMs.Our methodology leverages graph neural networks (GNNs) to encode neighboring nodes into a graph prompt, which is subsequently integrated into the original sequence as a "soft prompt".We employ a pre-trained LLM and update only the GNN prompt encoder and the PEFT parameters, Table 1: Comparison of existing GNN-LMs for representation learning on a graph with one million edges.We use GPU hours to estimate the computation cost.</p>
<p>GNN-LM [19]</p>
<p>LLM Service [45] GPEFT To improve the quality of the graph prompt, we introduce a pre-training phase for the GNN prompt encoder.This phase utilizes a next-token prediction objective on node texts to align the distribution of graph prompt and word embeddings of LLMs.In the fine-tuning (i.e., representation learning) phase, we choose to use contrastive loss [10] with one negative sample to avoid the memory overhead for traditional in-batch loss used in LM fine-tuning [8].</p>
<p>Compared with existing GNN-LMs, we reduce the training cost of GNN-LMs by parameter-efficient tuning and use structural information as input instead of a nested architecture; Compared with In-Graph Context Learning LLMs, we avoid the quadratic inference costs for link prediction.In Table 1, we calculate the approximated computation cost for these two methods and GPEFT.In our experiments, we conduct large-scale link prediction evaluation through representation learning across 8 different graphs in two domains.Our observations reveal an average improvement of 2% over large language models (LLMs) and 3% over the existing GNN-LMs.Our contributions are summarized as follows,</p>
<p>• We propose the first graph representation learning framework that utilizes Large Language Models with billions of parameters.• We develop a novel Parameter-Efficient Fine-Tuning (PEFT) algorithm -GPEFT for text-rich graphs.This method effectively integrates the key structural information into LLMs as a graph prompt.• Through comprehensive experiments conducted on eight text-rich graphs, we consistently observe improvements with our approach over previous GNN-LMs, while also achieving computational efficiency.</p>
<p>RELATED WORK</p>
<p>Modeling Text-Rich Graphs using LMs.Regarding the success of language models and GNNs in their respective areas, modeling text-rich graphs has been a hot topic [18].(1) Graph-empowered GNN-LMs: GraphFormers [44] is a GNN-nested Transformer architecture that insert GNN modules between transformer layers.</p>
<p>Using language models to model target and neighbor node texts requires huge memory and time costs.To address this, some work [28] proposed freezing the language model to reduce the computation needed for cascading.Some work [20,23] proposed neighbor sampling but that reduces the graph information captured.Therefore, recently some work propose to joint train LMs and GNNs through knowledge distillation [30] or Expectation Maximization algorithms [49].</p>
<p>(2) Self-supervised GNN-LMs: some methods [3,30] directly supervise language model fine-tuning through graph-related tasks, to help language models better understand the textual information in text-attributed graphs.The language model is then combined with GNNs by freezing the language model.This approach demonstrates the inherent connections between graph structure and text in TAGs.However, current research in this direction has limitations in that the LM and graph are separate, and the language model cannot directly perceive graph information.It also does not utilize the inherent connections between language and graphs to help GNNs better learn structural features.</p>
<p>(3) LLMs for Graph: With the breakthrough progress made by LLMs on textual tasks [1,42], recently many works have emerged exploring how to directly utilize LLMs to understand text-attributed graphs [2].Some works also explored using large models to enhance the textual features of text-attributed graphs [5,13].In terms of the representation learning, methods [6,41,45] that turns graph structure into in-context learning are most viable option for representation learning.</p>
<p>Parameter-Efficient Fine-Tuning of LLMs.As the size of language model continue to increase, full fine-tuning has became more and more impractical.Parameter-efficient fine-tuning (PEFT) [29] freeze most of the language model parameters, which has been successfully applied to popular language models such as BERT [4],</p>
<p>GPT [1,35,36] and t5 [37].This section reviews the key developments in the area.(1) Adapter Layers [15]: Adapter layers are small, trainable modules inserted between transformer layers of a pre-trained langauge model.(2) Prompt Tuning [22]: Prompt tuning leverages the pre-trained knowledge of LLMs by appending taskspecific prompts to the input text, effectively guiding the model's predictions without updating its parameters.(3) Low-Rank Adaptation(LORA) [16]: LORA modifies the attention and feedforward layers in the transformer through two low-rank updating matrices.These approaches can also be combined and found to be useful in applications like multi-modal instruction tuning models [7,25].</p>
<p>NOTATIONS AND PRELIMINARIES</p>
<p>This section introduces the background knowledge, notations and technical terms that will be used throughout the paper.</p>
<p>Text-Rich Graph Representation Learning</p>
<p>Let G = (V, X, E) be defined as a text-rich graph with multiple edge types T , where V represents the set of nodes, X denotes the text features associated with nodes, and E symbolizes the edges between nodes.Each node  is accompanied by a text sequence (e.g., item description, paper title and abstract) S  = { ,0 , ...,  , }.For each edge type  ∈ T , an edge    ∈ E  indicates the presence of an edge between node  and  of type .Given the target edge type , we define the problem of representation learning as mapping nodes into the embedding space, considering all observed edges except those designated for testing, denoted by E \ E test  .The learning objective aims to maximize the likelihood of observing the training edges E train  .The evaluation of the learned node representations is conducted through link prediction on the testing edges E test  .</p>
<p>Casual Language Modeling</p>
<p>Given a text sequence on each node, the causal language model (CLM) [35] aims to learn the probability distribution of these text sequences in a way that respects the sequential order of words or tokens.CLM is the most popular architecture for recent large language models such as GPT-3 [1] and LLaMA [42].Specifically, the objective function of the CLM, L LLM is defined as the negative log-likelihood of predicting each subsequent token in the sequence given its preceding tokens.This training objective is also referred as "next token prediction" frequently in the literature.
L LLM = − ∑︁ 𝑖 𝑘 ∑︁ 𝑗=0 log 𝑃 (𝑠 𝑖,𝑗+1 |𝑠 𝑖,0 , . . . , 𝑠 𝑖,𝑗 ; Θ LLM )(1)
In this equation,  ( ,+1 | ,0 , . . .,  , ; Θ LLM represents the probability of generating the next token  ,+1 given the sequence of preceding tokens ( ,0 , . . .,  , ), parameterized by Θ LLM .The architecture of a Large Language Model (LLM) is predominantly based on transformers [43].This work explores the potential of using casual lanugage model to learn the node representations in text-rich graphs.</p>
<p>Prompt Tuning.A prompt for LLMs is a text snippet concatenated with the original text sequence designed to bridge the gap between pre-training tasks (e.g., CLM) and downstream tasks, such as question answering or summarization.In prompt tuning, soft prompt denotes a method where learnable embeddings are utilized to steer the model's response [22,26,50] instead of supplying a concrete textual prompt.These soft prompts are typically concatenated with the input text data.Unlike fixed textual prompts, soft prompts are adaptable and are optimized to elicit the most effective response from the model.We model the structural information in G as a soft prompt in this work.</p>
<p>Parameter-Efficient LLM Fine-tuning</p>
<p>(1) Low-Rank Adaptation (LORA): Consider  as the original weight matrix in attention and feed-forward layer of the transformer.LoRA introduces two low-rank matrices  and , where the product   approximates the desired change in  .The modified weight matrix Ŵ is then:
Ŵ = 𝑊 + 𝐴𝐵 𝑇(2)
In this equation, the matrices A and B are trainable, while W remains fixed.The low-rank nature of   ensures that the number of additional parameters is relatively small.(2) Prefix-Tuning: Prefix Tuning involves adding a small number of trainable parameters (prefixes) to the input of each transformer layer.Let's denote the prefix as P and the original input as S. The modified input to the transformer layer can be represented as:
[𝑃; S].</p>
<p>METHOD</p>
<p>This section presents our proposed framework -GPEFT, which employs graph-aware PEFT techniques for representation learning in Large Language Models (LLMs).The core idea of GPEFT includes two key strategies: (1) integrating graph information through parameter-efficient tuning methods in LLMs, and (2) executing large-scale graph representation learning specifically for the task of link prediction.</p>
<p>Motivation: Language models accurately encode the textual information in a text-rich graph, many structure-related predictions, however, are difficult to infer from the text alone.As illustrated in Figure 2, some popular items in an e-commerce network or seminal papers in an academic graph are densely connected, even though their textual similarity to neighboring nodes varies significantly.</p>
<p>Although several recent works [6,13,45] have integrated graph information into Large Language Models (LLMs), most focus on node or graph classification problems.However, the nature of link prediction differs significantly from classification, as it requires calculating the proximity between pairs of nodes.Motivated by this distinction, our work aims to learn node embeddings through graph-aware LLM fine-tuning.</p>
<p>Framework: The framework of GPEFT is shown in Figure 2. Given a text-rich graph G, and target edge type for training, there are following three steps for representation learning: step 1: a GNN model called graph prompt encoder prepend the graph structural information before text sequence in the language model, we detail the descriptions in Section 4.1; step 2: To align the feature space of node representations output by GNN with the text embeddings in a LLM, we consider to use the casual language modeling to pre-train the GNN prompt encoder; step 3: an computationally efficient contrastive learning loss is employed to optimized the GNN-LLM model end-to-end with parameter-efficient fine-tuning techniques.We describe the pre-training and fine-tuning details in subsections of Section 4.2.</p>
<p>Graph prompt tuning</p>
<p>In the existing graph LLMs [6,45], the graph encoding function Θ  typically transforms the neighbors of node  into a textual sequence, such as "node_1: text, node_2: text, node_1 is connected with node_2... ".However, this approach assumes that all neighbors are equally important to the target node, which can also result in very long input sequences for the LLM in a densely connected graph.</p>
<p>Structural representations.On the contrary, we transform the node texts   and structural information A into dense vectors using GNNs.To achieve this, we fist use a small pre-trained langauge model -BERT [4] to turn the text sequence S  into document embedding   .Then, we use a common message passing GNN like GCN [21] or GraphSAGE [11] to obtain the GNN embedding   for node  in the given graph,
𝑧 𝑖 = GNN(𝑥 𝑖 , X, A; Θ 𝑔 )(3)
where   is the center node feature, X and A are neighbor node features and adjacency matrix, respectively.Θ  represents the GNN parameters, which also contains a mapping function from the GNN embedding space to the word embedding space of the LLM, M : R  GNN → R  model , and  GNN &lt;  model .We also refer   as the (nodelevel) graph prompt in the remaining of the paper.</p>
<p>Prompting LLM with the graph prompt: With the newly created graph-aware representation of node   , we now discuss two different soft prompting strategies: (1) prepend or (2) append graph prompt to the target node sequence { ,0 , ...,  , }.We denote the hidden states in the transformer for  − ℎ word in text sequence S  as ℎ , .Following the prior research of representation learning using casual language model [33], we introduce a specialized graph representation learning [GRL] token at the end of the sequence to represent the final node representations.We opt to prepend the graph prompt because it offers greater expressive power, as detailed in the following theorem.In contrast, prepending the graph prompt   to the sequence allows for the incorporation of   in the computation of    .We observe that in the attention calculation, prepending the graph prompt can potentially enhance the model's expressiveness through direct interaction with the text sequence's embeddings.The detailed proof is provided in Appendix A.</p>
<p>Graph representation learning with LLMs</p>
<p>Following the previous section, we compute the node representation   as the last hidden representation of the special token [GRL] as follows:
𝑣 𝑖 = LLM {𝑧 𝑖 , 𝑠 𝑖,0 , 𝑠 𝑖,𝑘 , [GRL]}; Θ 𝑔 , Θ LLM(4)
where Θ  and Θ LLM are the trainable parameters of GNN and LLM.Now we discuss the motivation of pre-trainng GNN through casual language modeling.</p>
<p>Pre-training GNN Prompt Encoder.Note that node features   are independent to the large language model and the GNN Θ  is randomly initialized.The feature distribution of the GNN prompt encoder  (  ) does not align with the word embeddings  (ℎ 0  ).We use the next token prediction objective to pre-train the GNN Θ  while freezing the LLM Θ LLM ,
L pre-training = − ∑︁ 𝑖 𝑘 ∑︁ 𝑗=0 log 𝑃 (𝑠 𝑖,𝑗+1 |𝑧 𝑖 , 𝑠 𝑖,0 , . . . , 𝑠 𝑖,𝑗 ; Θ 𝑔 , Θ LLM ) (5)
where L is the loss function,  ( ,+1 |  ,  ,0 , . . .,  , ; Θ) is the conditional probability of the token  ,+1 given the GNN prompt   and the preceding tokens up to  , , and Θ represents the trainable parameters of this step.</p>
<p>Fine-tuning.After pre-training, we use the cosine similarity between the GRL embeddings of node pairs to represent the likelihood of edges in the graph.In particular, we obtain the representations of each node , compute the cosine distance    = 1 − cos(  ,   ) between node  and  and employ a contrastive loss [10] between a positive edge    and random negative edge    ′ : 6)
L fine-tuning = ∑︁ 𝑒 𝑖 𝑗 ∈ E 𝑑 2 𝑖 𝑗 + max(𝜏 − 𝑑 𝑖 𝑗 ′ , 0) 2 (𝑣 𝑖 = LLM {𝑧 𝑖 , 𝑠 𝑖,0 , 𝑠 𝑖,𝑘 , [GRL]}; Θ 𝑔 , Θ LLM(7)
it encourages connected nodes to have similar representations while penalizes disconnected nodes when their cosine similarity is larger than the margin  (i.e.,  = 0.5 in our experiment).In a LLM with billion parameters, usually the batch size cannot be large, hence we observe using such loss yields better performance and computational efficiency than traditional in-batch contrastive learning loss (e.g.SimCSE [8]) with a small batch size.Note that in this step, we update both the LLM and GNN parameters.</p>
<p>Model Optimization.</p>
<p>We outlined the training process of our framework in the previous section.However, it is well-known that directly optimizing a billionscale LLM is impractical.Furthermore, maintaining a different set of full LLM parameters for various types of text-rich graphs can lead to space inefficiency.A more efficient approach would be the ability to store a small amount of parameters for a specific application related to a text-rich graph.To this end, we propose partitioning the parameters of the LLM into two segments:
Θ LLM = [Θ peft ; Θ pre ],
where Θ pre indicates the pre-trained weights of a specific LLM.This division allows light-weighted parameters for each specific application.Now we describes how we integrates our graph prompt encoder Θ  into traditional parameter efficient tuning Θ peft while maintaining the representation power discussed before.We consider use two most popular peft variants:</p>
<p>(1) Low-Rank Adaptation (LORA): In LORA, the trainable parameters are coupled in every transformer layer.Therefore, the naive prepending of the GNN prompt can effectively participate in the gradient updates of LORA matrices.(2) Prefix-Tuning: In prefix-tuning, the modified input to the transformer layer is typically represented as: [; S].However, placing graph tokens  at the beginning of S does not contribute to the updating of the prefix embedding  similar to the Theorem A.1.To address this, we propose a slight modification to the prefix embedding, denoted as  ′ =  + .This adjustment ensures that the graph information is effectively integrated with the prefix tuning process.</p>
<p>Algorithm 1: Pseudo code for GPEFT optimization In Algorithm 1, we detail the training procedure of GPEFT.In the pre-training phase, we iterate on every batch of nodes and sample neighbors of each node to compute the graph prompt at line 7. Line 8 and 9 correspond to the weight matrices updating of GNN prompt encoder Θ  while freezing LLM.In the fine-tuning (the representation learning) phase, for every batch of training edges, For a k-layer GNN prompt encoder, setting  = 768 introduces  2 GNN •  rel + GNN  model additional parameters, where  rel represents the number of relations in G.In our experiment, with  rel = 2,  = 2, this results in 5.5M parameters.Thus, we argue that our approach contains a comparable number of parameters to PEFT, which is significantly fewer than existing GNN-LMs (at least 110M for bert-base).</p>
<p>EXPERIMENTS</p>
<p>To evaluate the performance of GPEFT in learning representations of text-rich graphs, we focus on the following research questions (RQs) in this section:</p>
<p>• RQ1: How effective are large language models (LLMs) at graph representation learning, particularly with GPEFT?• RQ2: Is the pre-training of the graph prompt encoder necessary?• RQ3: Can our approach be adapted to different PEFT methods and LLMs? • RQ4: Is GPEFT efficient in million-scale representation learning?• RQ5: What is the parameter sensitivity of GPEFT?</p>
<p>Datasets</p>
<p>Some of the most popular benchmarks, such as OGB [17], do not include full-text information for link prediction tasks.Following recent research trends [19,48], we benchmark on two representative text-rich graphs from the academia and e-commerce domains.</p>
<p>In each domain, we choose four different subdomains to comprehensively study the performance of link prediction with different methods.</p>
<p>Amazon Review [12,31].The Amazon Products dataset comprises commercial data from items sold on Amazon, complete with detailed descriptions.In this dataset, items are represented as nodes in a graph, with edges connecting pairs of nodes if they are coviewed or co-purchased by users.We utilize the co-viewed edges as observed data and perform our representation learning evaluation on the co-purchased edges, as this closely resembles real-world product recommendation scenarios.Our focus is on products within four specific subdomains: Clothing, Home, Sports, and Video Games.Each of these subdomains features a graph with over 1 million edges.</p>
<p>Microsoft Academia Graph [39] (MAG).MAG is a large, heterogeneous graph containing scientific publication records, citation relationships between those publications, as well as authors, institutions, journals, conferences, and fields of study.We use a pre-processed version (MAPLE [48]) to obtain field specific textrich graphs, that are Computer Science, Economics, Mathematics, Geology and etc..We utilize the cite-by edges as observed data and generate paper pairs that share the same author (same-author) using author information of each paper.</p>
<p>We summarize the detailed data statistics of each subdomains in Amazon Review and MAG, including average node degrees and text length in Table 2.</p>
<p>Baselines</p>
<p>In our experiment, we compare GPEFT with different state-of-theart text-rich graph modeling algorithms (underlined in the following paragraphs).</p>
<p>Cascaded GNN-LMs: We consider methods that employ fine-tuned language models (LMs) or large language models (LLMs) as feature encoders, followed by training a graph neural network (GNN) on these features.Specifically, we select MPNET [40] that is a variant of Sentence-BERT [38] and LLaMA fine-tuned with LORA techniques.The LMs or LLMs are not updated in these baselines.</p>
<p>Graph-empowered LMs:</p>
<p>We opt to compare with GraphFormers [44], which are designed for graph representation learning by aggregating the [CLS] tokens of neighboring nodes between transformer layers.Similar to our graph prompt approach, this method introduces virtual structure tokens to the LM.Patton [19] improves upon GraphFormers by incorporating pre-training with masked token and node prediction objectives.Additionally, we also report the performance of fine-tuning Sentence-BERT [38] without graph information.</p>
<p>Graph-aware LLMs: These methods do not alter the architectures of large language models (LLMs) but rather engage in in-context learning or fine-tuning of the LLMs.In the context of link prediction, the most suitable algorithm is InstructGLM [45], which translates both the graph structure and node features into natural language.We have made minor modifications to the InstructGLM pipeline, notably by utilizing a public LLM API 1 to transform the composed Table 3: Performance of link prediction on Amazon Review Graph.Each experiment is repeated three times, except for † : We limit API calls to just once for cost efficiency.</p>
<p>Method</p>
<p>Clothing</p>
<p>Home&amp;Kitchen Sports Video Games Average</p>
<p>Hit@1 MRR Hit@1 MRR Hit@1 MRR Hit@1 MRR Hit@1 MRR For different methods, we use the same neural network architecture for GNN, LM and LLM for fair comparison.Specifically, we use a 2-layer GraphSAGE [11] as the graph neural network with hidden dimension  GNN as 768, sentence transformer (i.e., MPNet-v2 [40]) as the pre-trained language model and LLaMA [42] as the default pre-trained foundation model.</p>
<p>Experiment Settings</p>
<p>We conduct a large-scale representation learning experiment on text-rich graphs for link prediction.Specifically, we simulate realworld scenarios supporting tasks such as item recommendation through co-purchase prediction on Amazon Review and authorship identification through same-author prediction on MAG.In each subdomain, we randomly select 50,000 edges for training, 10,000 edges for validation, and use the remaining edges for testing.Following the same setup as in OGB [17], we select 100 negative edges for each test edge and compute the average hit@1 and MRR score across all test edges.For all baseline methods, except GraphFormers and Patton, we employ the same contrastive loss as of Equation 6.Additionally, we standardize the neighborhood fan-outs (i.e., 5 neighbors at each hop) for GNNs.</p>
<p>We implement all baselines using Deep Graph Library (https: //www.dgl.ai/) and Hugging Face (https://huggingface.co/).For all methods, we train them on 50K training samples for 4 epochs on 8 Nvidia A100 GPUs with a total batch size of 32.We set the peak learning rate as 1e-4.More details can be found in Appendix B.</p>
<p>Experiment Results</p>
<p>We compare our approach with state-of-the-art baselines across two different domains to assess the effectiveness of our representation learning method.We conduct three runs for each method and report the mean and standard deviation for each in Tables 1 and 2, resulting in the following observations: RQ1: How effective are large language models (LLMs) at graph representation learning, particularly with GPEFT?</p>
<p>(1) Although masked language models (e.g., Sentence-BERT) were popularly used for sequence representation, large language models (e.g., PEFT-LLaMA) have shown improved performance over masked language models.In our experiments, LLMs yield more than 10% improvement over Sentence-BERT.</p>
<p>(2) In the same tables, we also find GNN (Sentence-BERT) perform worse than GNN (PEFT-LLaMA).In addition, GraphFormers also reports subpar performance against GNN (PEFT-LLaMA).Therefore, we conclude that large language models serve as powerful feature encoders for representation learning on text-rich graphs.Meanwhile, InstructGLM outperforms GraphFormers but falls short of PATTON's performance.This discrepancy may be attributed to the challenges of handling graph structures through embeddingbased in-context learning, which struggles with accurately capturing structures via natural language descriptions.Consequently, carefully fine-tuned GNN-LMs (GPEFT and PATTON) tend to surpass the embeddings produced by black-box LLMs.</p>
<p>(3) GPEFT consistently outperforms the compared baseline across both metrics.Specifically, it surpasses PEFT-LLaMA by 2.6% and 1.8% on MAG and Amazon Reviews, respectively, in terms of hit@1.Compared to its own ablation, we find that pre-training helps improve the accuracy of link prediction, especially in the academic text-rich graph.</p>
<p>RQ2: Is the pre-training of the graph prompt encoder necessary?</p>
<p>On MAG, we observe that our approach performs significantly worse (∼ 6%) without pretraining compared to its pretrained counterpart.On Amazon Review, while the performance gap is smaller across three subdomains, the Video Games category demonstrates instability without pretraining.Interestingly, Patton also outperforms GraphFormers with the same architecture on subdomains that utilize pre-trained checkpoints.Both methods introduce virtual structural tokens, and employing the same pre-training objective as the pre-trained language model.Apparently, continual training helps align the virtual token representations with text representations.Therefore, we advocate for a pre-training then fine-tuning paradigm, where one needs to pre-train only once in a domain and can fine-tune for various applications (e.g., co-purchase, churn, etc.).</p>
<p>Model Analysis</p>
<p>In this section, we provide more in-depth study to understand performance and efficiency of GPEFT.</p>
<p>RQ3:Can our approach be adapted to different PEFT methods and LLMs?We first apply prefix tuning [24] on GPEFT and demonstrate the results on four subdomains of Amazon Review in Figure 3a.We can find that using prefix-tuning or LORA does not show much Second, we substitute the backbone language model with OPT-1.3B[47] and Falcon-7B2 .In Figure 3b, we observe that, in general, LLaMA-7B outperforms the others, with Falcon-7B in second place and OPT-1.3Bbeing outperformed by these two models.Our results are consistent with other work that shows LLMs with more parameters yield better performance in downstream applications.</p>
<p>RQ4: Is GPEFT efficient in large-scale representation learning?</p>
<p>In Table 5, we report the running time of our approach at each phase on MAG-Economics.Specifically, unlike the pre-training time of PATTON [19] (10hr+), the pre-training time of GPEFT is even shorter than the fine-tuning time.Because we only optimize Θ  in the pre-training phase and we can say that the pre-training is both necessary and efficient together with RQ2.Owing to its parameter-efficient tuning, our approach not only minimizes the number of training parameters but also achieves the best performance.As mentioned in the introduction, the design of GPEFT aims to provide a high-quality, lightweight framework suitable for various applications on different text-rich graphs.Considering the reduced parameter storage requirements and the training time, we believe GPEFT holds significant potential for industrial-scale applications, including recommendation systems, ranking tasks, and etc.</p>
<p>RQ5: What is the parameter sensitivity of GPEFT?</p>
<p>There are several important hyper-parameters in GPEFT: (1) LORA rank  , it affects the number of trainable parameters (2) number of hops  in GNN prompt encoder, it affects the amount of structural information used in the training of GPEFT.In Figure 4, we observe that varying the rank of LoRA matrices from 4 to 32 does not significantly affect the performance.This observation aligns with findings from other studies, which suggest that optimizing for a single task requires only minor adjustments to the pre-trained LLM weights.Similarly, varying number of hops in the GNN prompt encoder has a minor effect on performance, until the point where message passing begins to aggregate more noisy neighbors than useful ones (e.g., at 4 hops, as shown in Figure 4b).</p>
<p>CONCLUSION AND FUTURE WORK</p>
<p>This paper proposes GPEFT to harness LLMs for representation learning on text-rich graphs.Compared with existing work of applying LLM on graph structure data, our proposed method is the first one that generates embeddings from the LLM and therefore can be applied on numerous industrial applications.More importantly,</p>
<p>A THEORETICAL ANALYSIS</p>
<p>Theorem A.1.Consider the -th layer of a transformer, where    = [ℎ  ,0 , . . ., ℎ  , ] represents the embedding matrix for a text sequence of length .At each layer, appending a graph prompt to the sequence results in a convex combination of the embeddings [   ;   ].In contrast, prepending the graph prompt   to the sequence allows for the incorporation of   in the computation of    .</p>
<p>Proof.We first provide the calculation of two different position of graph prompt token in a casual language model.Prompt token after text sequence.In the -th transformer layer, let us denote    = [ℎ  ,0 , . . ., ℎ  , ] as the embedding matrix for the first  tokens.Then, the attention calculation for the ( + 1)-th hidden states ℎ +1 , , incorporating the prompt token   , is expressed as follows:</p>
<p>Attention(ℎ</p>
<p>Interestingly, in a causal language modeling setup, the attention scores for text tokens remain unchanged when a graph prompt is appended afterwards.This is because, in causal language modeling, a token does not have visibility of subsequent tokens.As a result, the hidden representation of    is the same before and after appending the prompt token.The representation of [GRL] is ℎ GRL = a  [   ;   ],   = 1, which is a convex combination of original hidden representation and graph prompt   .This is equivalent as cascading GNN-LMs that language model and graph neural network calculations are independent with each other.</p>
<p>Prompt token before text sequence.When we place graph prompt in the beginning,    is included in the attention score calculation of every token: Attention(ℎ</p>
<p>In this manner, the hidden representation of text tokens, denoted as    ≠    , incorporates the information of   because the attention mask does not exclude the prompt token.Consequently, prepending the graph prompt employs the LLM as the predictor, whereas appending the prompt utilizes the GNN as the predictor.□</p>
<p>B DETAILED EXPERIMENT SETTINGS</p>
<p>We use the PEFT library (https://github.com/huggingface/peft)from Hugging Face to implement the LORA and Prefix Tuning of LLMs.The graph prompt   is inserted at the beginning of the input embedding within the forward function of Hugging Face's AutoModel, and a "1" is prepended to the attention mask.The hyper-paramters of GPEFT are shown in Table 6.</p>
<p>Figure 1 :
1
Figure 1: Convenient use case of parameter-efficient finetuning for text-rich graphs, requiring only 2% additional LLM parameters per graph application.</p>
<p>cost.As shown in Figure1, GPEFT does not change the original parameters of LLMs and only ∼ 2% of additional parameters are optimized for each task on a given textrich graphs.For instance, for recommendations on social networks, we can simply load the SocialGraph::recommendation component into the backbone LLM and train GPEFT with task-specific supervision.</p>
<p>Figure 2 :
2
Figure 2: Overview of the GPEFT Framework: Architecture (Left) and Pre-training and Fine-tuning Processes (Right).</p>
<p>Figure 3 :
3
Figure 3: GPEFT using different LLMs and PEFT algorithms on Amazon Review.</p>
<p>(a) Varying LORA rank (b) Varying number of hops</p>
<p>Figure 4 :
4
Figure 4: GPEFT using different LLMs and PEFT algorithms on Amazon Review.</p>
<p>Theorem 4.1.Consider the -th layer of a transformer, where    = [ℎ  ,0 , . . ., ℎ  , ] represents the embedding matrix for a text sequence of length .At each layer, appending a graph prompt to the sequence results in a convex combination of the embeddings [   ;   ].</p>
<p>Table 2 :
2
Dataset Statistics.
Dataset#Nodes#Edges#Task Edges Avg Degree Avg #TokensClothing469,274 2,578,7461792,60410.99117.83Home453,121 3,732,9482,382,20116.48133.94Sports293,712 2,390,0763,130,12816.27125.08VideoGames38,869729,062844,29637.51147.33Computer Science 263,393 2,929,35813,188,47222.24159.69Economics178,670 3,064,1443,615,52434.3160.21Geology431,834 15,256,89035,915,14270.66205.08Mathematics490,551 4,770,64412,911,64419.45143.94we sample neighbors for both nodes and perform graph promptencoding similarly. Then, we tune the parameters of PEFT and GNNprompt encoder simultaneously to minimize the representation loss(Line 14 and 15). In evaluation, we calculate the node embeddingsas the [GRL] token in the sequence of each node and use the cosinesimilarity to rank the candidates edges.
[16]meter Efficiency.We now describe the amount of additional parameters introduced in GPEFT.First, we use LORA[16]as an example to calculate the PEFT parameters.We denote the hidden dimension of GNN prompt encoder and LLM as  GNN and  LLM , and rank of the LORA matrices as  , with  &lt;&lt;  model .Assuming the LLM has  transformer layers, LORA introduces two matrices across four components of the transformer's feedforward network: query, key, value, and output.Consequently, the total additional parameters introduced by LoRA amount to  • 4 model .In the case of LLaMA, where  model = 4096 and  = 32, setting  = 16 yields approximately 8.9M additional parameters in Θ peft .</p>
<p>Table 4 :
4
GraphPEFT w.o.pretraining 76.74 0.07 84.570.06 79.68 0.18 86.63 0.09 64.44 0.14 77.21 0.12 50.60 0.11 66.97 0.14 67.87 78.85 GraphPEFT 76.95 0.07 84.71 0.06 79.87 0.18 86.76 0.09 64.61 0.14 77.34 0.12 58.04 0.11 73.07 0.14 69.88 80.47 Performance of link prediction on MAG graph.Each experiment is repeated three times, except for † : We limit API calls to just once for cost efficiency.pretraining 23.86 0.19 40.11 0.29 27.19 0.18 41.85 0.18 34.93 0.25 50.37 0.22 45.81 0.00 60.75 0.17 32.95 48.27 GraphPEFT 30.41 0.29 47.37 0.25 35.42 0.10 51.08 0.13 39.69 0.15 55.19 0.14 48.09 0.06 63.02 0.01 38.40 54.17
GNN (Sentence-BERT)74.52 0.5282.58 0.474.18 0.27 82.67 0.2161.2 0.1274.48 0.16 52.64 0.15 68.53 0.1365.6477.07GNN (PEFT-LLaMA)76.22 0.26 84.16 0.19 73.74 7.03 81.66 6.43 62.26 0.43 75.36 0.91 56.14 0.18 71.59 0.1467.0978.19Sentence-BERT62.11 0.20 73.36 0.14 65.43 0.17 76.37 0.14 50.13 0.13 66.28 0.10 41.83 0.05 59.65 0.1054.8868.92GraphFormers71.30 0.04 79.31 0.02 74.29 0.03 81.63 0.02 58.35 0.01 71.45 0.00 49.67 0.04 65.98 0.0463.4074.60PATTON76.95 0.02 83.79 0.01 78.14 0.05 84.72 0.03 62.44 0.02 74.62 0.01 51.07 0.14 66.97 0.0867.1577.53InstructGLM-embeddings  †76.2382.6079.8285.9362.5073.2548.1863.0066.6876.20PEFT-LLaMA74.73 0.02 82.87 0.00 78.93 0.02 86.07 0.02 62.52 0.02 75.77 0.01 56.07 0.18 71.54 0.1468.0679.06Computer ScienceEconomicsGeologyMathematicsAverageMethodHit@1MRRHit@1MRRHit@1MRRHit@1MRRHit@1 MRRGNN (Sentence-BERT)24.97 0.15 41.18 0.25 28.64 0.1343.35 0.335.94 0.2251.28 0.247.67 0.21 62.24 0.1834.3149.51GNN (PEFT-LLaMA)27.81 0.10 44.92 0.13 31.89 0.45 47.83 0.37 35.68 0.12 51.59 0.13 46.24 0.1161.9 0.0335.4151.56Sentence-BERT20.45 0.05 36.64 0.03 22.65 0.12 37.39 0.14 29.64 0.21 45.37 0.13 35.85 0.21 52.23 0.1827.1542.91GraphFormers19.14 0.09 34.08 0.05 19.86 0.06 32.66 0.0229 0.0343.65 0.03 39.91 0.13 54.50 0.1026.9841.22PATTON21.68 0.12 36.48 0.07 31.07 0.06 44.99 0.05 33.38 0.13 48.06 0.07 43.35 0.07 57.37 0.0432.3746.73InstructGLM-embeddings  †21.8434.8824.7636.9534.5548.0043.0155.2231.0443.76PEFT-LLaMA28.45 0.62 45.16 0.91 33.36 0.07 49.07 0.14 37.03 0.47 52.56 0.6945.7 0.2561.34 0.1536.1452.03GraphPEFT w.o. sequence into dense embeddings. Our adapted approach is referredto as InstructGLM-embeddings.At last, we design several ablations of our model to verify theeffectiveness of each component: (1) GPEFT without pre-trainingphase in Equation 5; (2) GPEFT without graph prompt, namely,PEFT.</p>
<p>Table 5 :
5
Time and memory costs of GPEFT on MAG-Economics using 8 A100 GPUs with a total batch size of 32.
pre-training fine-tuning inferenceTime36min96min11minMemory20883MB28309MB13676MB#trainable5.5M14.4M-difference, which indicates the versatility of our framework onvarious PEFT techniques.</p>
<p>+1 , ) = softmax  (ℎ  , ) (  )  √︁    (   )</p>
<p>Table 6 :
6
Hyperparameters of GPEFT.
parameter namevaluepre-training batch size 𝐵8pre-training learning rate lr1e-3fine-tuning size B4fine-tuning learning rate lr1e-4GNN hidden dimension 𝑑 GNN768LLM hidden dimension 𝑑 modem 4096maximal sequence length N256maximal gradient norm1.0neighborhood size K5# GNN layers L2# warmup epochs1# epochs4# random seeds3
Parameter-Efficient Tuning Large Language Models for Graph Representation Learning Conference'17, July 2017, Washington, DC, USA
text-embedding-3-small in https://platform.openai.com/docs/guides/embeddings
https://huggingface.co/tiiuae/falcon-7b</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, NeurIPS. 2020. 2020</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, arXiv:2307.033932023. 2023arXiv preprint</p>
<p>Node feature extraction by selfsupervised multi-scale neighborhood prediction. Eli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Jiong Zhang, Olgica Milenkovic, Inderjit S Dhillon, arXiv:2111.000642021. 2021arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018. 2018arXiv preprint</p>
<p>Qian Keyu Duan, Tat-Seng Liu, Shuicheng Chua, Wei Tsang Yan, Qizhe Ooi, Junxian Xie, He, arXiv:2308.02565Simteg: A frustratingly simple approach improves textual graph learning. 2023. 2023arXiv preprint</p>
<p>Talk like a Graph: Encoding Graphs for Large Language Models. Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi, NeurIPS 2023 Workshop: New Frontiers in Graph Learning. 2023</p>
<p>Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, arXiv:2304.15010Llama-adapter v2: Parameter-efficient visual instruction model. 2023. 2023arXiv preprint</p>
<p>Tianyu Gao, Xingcheng Yao, Danqi Chen, arXiv:2104.08821Simcse: Simple contrastive learning of sentence embeddings. 2021. 2021arXiv preprint</p>
<p>node2vec: Scalable feature learning for networks. Aditya Grover, Jure Leskovec, Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on Knowledge discovery and data miningACM2016</p>
<p>Dimensionality reduction by learning an invariant mapping. Raia Hadsell, Sumit Chopra, Yann Lecun, 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06). IEEE20062</p>
<p>Inductive representation learning on large graphs. Will Hamilton, Zhitao Ying, Jure Leskovec, 2017. 201730Advances in neural information processing systems</p>
<p>Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. Ruining He, Julian Mcauley, proceedings of the 25th international conference on world wide web. the 25th international conference on world wide web2016</p>
<p>Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann Lecun, Bryan Hooi, arXiv:2305.19523[cs.LG]Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning. 2023</p>
<p>GraphMAE: Self-Supervised Masked Graph Autoencoders. Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, Jie Tang, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Parameter-efficient transfer learning for NLP. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, International Conference on Machine Learning. PMLR2019</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021. 2021arXiv preprint</p>
<p>Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, arXiv:2005.00687Open Graph Benchmark: Datasets for Machine Learning on Graphs. 2020. 2020arXiv preprint</p>
<p>Gang Bowen Jin, Chi Liu, Meng Han, Heng Jiang, Jiawei Ji, Han, arXiv:2312.02783Large Language Models on Graphs: A Comprehensive Survey. 2023. 2023arXiv preprint</p>
<p>Patton: Language Model Pretraining on Text-Rich Networks. Wentao Bowen Jin, Yu Zhang, Yu Zhang, Xinyang Meng, Qi Zhang, Jiawei Zhu, Han, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational Linguistics2023</p>
<p>Heterformer: Transformerbased Deep Node Representation Learning on Heterogeneous Text-Rich Networks. Jin Bowen, Yu Zhang, Qi Zhu, Jiawei Han, arXiv:2205.102822022. 2022arXiv preprint</p>
<p>Semi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016. 2016arXiv preprint</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, arXiv:2104.086912021. 2021arXiv preprint</p>
<p>Adsgnn: Behaviorgraph augmented relevance modeling in sponsored search. Chaozhuo Li, Bochen Pang, Yuming Liu, Hao Sun, Zheng Liu, Xing Xie, Tianqi Yang, Yanling Cui, Liangjie Zhang, Qi Zhang, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval2021</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, arXiv:2101.001902021. 2021arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2304.08485Visual instruction tuning. 2023. 2023arXiv preprint</p>
<p>P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsShort Papers20222</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019. 2019arXiv preprint</p>
<p>Finegrained fact verification with kernel graph attention network. Zhenghao Liu, Chenyan Xiong, Maosong Sun, Zhiyuan Liu, arXiv:1910.097962019. 2019arXiv preprint</p>
<p>Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. 2022. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods. </p>
<p>Costas Mavromatis, N Vassilis, Shen Ioannidis, Da Wang, Soji Zheng, Jun Adeshina, Han Ma, Christos Zhao, George Faloutsos, Karypis, arXiv:2304.10668Train Your Own GNN Teacher: Graph-Aware Distillation on Textual Graphs. 2023. 2023arXiv preprint</p>
<p>Image-based recommendations on styles and substitutes. Julian Mcauley, Christopher Targett, Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval. the 38th international ACM SIGIR conference on research and development in information retrieval2015Qinfeng Shi, and Anton Van Den Hengel</p>
<p>Distributed Representations of Words and Phrases and their Compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean, Neural and Information Processing System (NIPS). 2013</p>
<p>Sgpt: Gpt sentence embeddings for semantic search. Niklas Muennighoff, arXiv:2202.089042022. 2022arXiv preprint</p>
<p>Deepwalk: Online learning of social representations. Bryan Perozzi, Rami Al-Rfou, Steven Skiena, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data miningACM2014</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018. 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 2019. 2019</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 212020. 2020</p>
<p>Nils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019. 2019arXiv preprint</p>
<p>An overview of microsoft academic service (mas) and applications. Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, Kuansan Wang, Proceedings of the 24th international conference on world wide web. the 24th international conference on world wide web2015</p>
<p>Mpnet: Masked and permuted pre-training for language understanding. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu, Advances in Neural Information Processing Systems. 332020. 2020</p>
<p>Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V Chawla, Panpan Xu, arXiv:2309.15427Graph Neural Prompting with Large Language Models. 2023. 2023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2023arXiv preprint</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, 2017. 201730</p>
<p>GraphFormers: GNN-nested transformers for representation learning on textual graph. Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal, Amit Singh, Guangzhong Sun, Xing Xie, Advances in Neural Information Processing Systems. 342021. 2021</p>
<p>Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang, arXiv:2308.07134Natural language is all a graph needs. 2023. 2023arXiv preprint</p>
<p>Heterogeneous graph neural network. Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, Nitesh V Chawla, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2019</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022. 2022arXiv preprint</p>
<p>The effect of metadata on scientific literature tagging: A cross-field cross-model study. Yu Zhang, Bowen Jin, Qi Zhu, Yu Meng, Jiawei Han, WWW'23. 1626-16372023</p>
<p>Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, Jian Tang, arXiv:2210.14709Learning on large-scale text-attributed graphs via variational inference. 2022. 2022arXiv preprint</p>
<p>Factual probing is [mask]: Learning vs. learning to recall. Zexuan Zhong, Dan Friedman, Danqi Chen, arXiv:2104.052402021. 2021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
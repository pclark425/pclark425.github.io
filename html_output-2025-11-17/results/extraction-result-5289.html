<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5289 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5289</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5289</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-ee42433b0467c8a35ce64bf794abc3169e9fb811</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ee42433b0467c8a35ce64bf794abc3169e9fb811" target="_blank">Transformer-CNN: Swiss knife for QSAR modeling and interpretation</a></p>
                <p><strong>Paper Venue:</strong> Journal of Cheminformatics</p>
                <p><strong>Paper TL;DR:</strong> SMILES-embeddings derived from the internal encoder state of a Transformer model trained to canonize SMILES as a Seq2Seq problem results in higher quality interpretable QSAR/QSPR models on diverse benchmark datasets including regression and classification tasks.</p>
                <p><strong>Paper Abstract:</strong> We present SMILES-embeddings derived from the internal encoder state of a Transformer [ 1 ] model trained to canonize SMILES as a Seq2Seq problem. Using a CharNN [ 2 ] architecture upon the embeddings results in higher quality interpretable QSAR/QSPR models on diverse benchmark datasets including regression and classification tasks. The proposed Transformer-CNN method uses SMILES augmentation for training and inference, and thus the prognosis is based on an internal consensus. That both the augmentation and transfer learning are based on embeddings allows the method to provide good results for small datasets. We discuss the reasons for such effectiveness and draft future directions for the development of the method. The source code and the embeddings needed to train a QSAR model are available on https://github.com/bigchem/transformer-cnn . The repository also has a standalone program for QSAR prognosis which calculates individual atoms contributions, thus interpreting the model’s result. OCHEM [ 3 ] environment ( https://ochem.eu ) hosts the on-line implementation of the method proposed.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5289.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5289.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-CNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-CNN (SMILES canonicalization encoder + TextCNN QSAR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder Transformer trained to canonicalize SMILES; the encoder's position-wise latent outputs (dynamic SMILES-embeddings) are frozen and used as input to a 1D TextCNN for QSAR/QSPR modeling and interpretation via Layer-wise Relevance Propagation (LRP).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (encoder-decoder) used for SMILES canonicalization</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (encoder-decoder, self-attention based Seq2Seq)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>ChEMBL SMILES pairs (non-canonical -> canonical) augmented ~11x resulting in ~17,657,995 canonicalization pairs drawn from ChEMBL; only SMILES length <=110 used.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>SMILES canonicalization; extraction of embeddings for QSAR/QSPR modeling (drug discovery / ADMET prediction); intended to be embedded in de-novo drug design pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not used to generate novel compounds in this paper; trained as a Seq2Seq canonicalization model (non-canonical SMILES -> canonical SMILES) and embeddings were extracted for downstream QSAR. The authors state the approach can be embedded into de-novo pipelines but did not perform molecule design with it here.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Position-wise real-valued SMILES embeddings (matrix of shape N_chars × embedding_dim); downstream models operate on these embeddings via 1D convolutions; canonical SMILES are the decoded output of the Seq2Seq model.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Canonicalization accuracy on 500,000 generated ChEMBL-like SMILES (percentage correctly canonicalized); QSAR evaluation using coefficient of determination (r^2) for regression datasets and AUC for classification datasets across standard benchmark sets.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Training: ChEMBL; Validation: 500,000 ChEMBL-like SMILES generated by a focused-library generator (ref. 56); QSAR benchmarks: MP, BP, BCF, FreeSolv, LogS, Lipo, BACE, DHFR, LEL (regression) and HIV, AMES, BACE, Clintox, Tox21, BBBP, JAK3, BioDeg, RP AR (classification), as used in MoleculeNet / OCHEM.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Transformer canonicalization model correctly canonicalized 83.6% of 500,000 generated ChEMBL-like SMILES; extracted embeddings used with TextCNN (Transformer-CNN) produced QSAR models that were similar or superior to descriptor-based approaches on most benchmark datasets (improved r^2/AUC on many sets).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to CDDD descriptors (SmI2canSml autoencoder) and descriptor-based ML, Transformer-CNN generally achieved similar or better QSAR performance; unlike CDDD, Transformer-CNN embeddings support arbitrary molecules processable by RDKit and do not have the same numeric property restrictions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Canonicalization accuracy lower for stereochemistry: only 37.2% correct for SMILES with '@' (stereocenters) and 73.9% for cis/trans ('/' or '\\'); relevance propagation shows ~24.6% of signal dissipated into biases (affecting interpretability confidence); Transformer in this paper was not demonstrated to generate novel molecules—only to provide embeddings for QSAR and to canonicalize SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-CNN: Swiss knife for QSAR modeling and interpretation', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5289.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5289.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Focused library generator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Focused library generator (case of Mdmx inhibitors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SMILES-generating model cited and used to produce 500,000 ChEMBL-like SMILES (only ~1.7% were canonical) that were used here to validate the Transformer canonicalization model; originally developed to generate focused libraries (Mdmx inhibitors) as a de-novo design tool.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Focused library generator: case of Mdmx inhibitors.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Focused library generator (reference [56])</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De-novo/focused library generation for drug discovery (example: Mdmx inhibitors); here used to produce ChEMBL-like SMILES for canonicalization validation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not described in this paper; the authors sampled outputs from the referenced generator to obtain a large set of generated SMILES for testing canonicalization.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES strings (many non-canonical forms present).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>In this paper, the generated SMILES set was used to measure canonicalization performance: 418,233/500,000 (83.6%) of generated SMILES were correctly canonicalized by the Transformer model; stereochemistry-containing SMILES had lower canonicalization success.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>A generated set of 500,000 ChEMBL-like SMILES produced by the focused library generator (ref. 56).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Used as a source of test SMILES for canonicalization validation; no activity or application-specific evaluation reported in this paper (the generator itself is cited as applicable to focused library design for Mdmx inhibitors in the referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>This paper does not report internal details (architecture, training data) of the generator; many outputs were non-canonical, and the canonicalization model had notable difficulty with stereochemical SMILES in the generated set.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-CNN: Swiss knife for QSAR modeling and interpretation', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5289.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5289.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SmI2canSml / CDDD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SmI2canSml autoencoder producing CDDD descriptors (continuous data-driven descriptors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence autoencoder that maps arbitrary SMILES to canonical SMILES and yields fixed-length (512) continuous molecular descriptors (CDDD) usable for QSAR and other downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning continuous and data-driven molecular descriptors by translating equivalent chemical representations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SmI2canSml autoencoder (CDDD)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Autoencoder / Seq2Seq model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular descriptor generation for QSAR/QSPR and as continuous latent space for molecular design in other works (not used for de-novo generation in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoencoding / translation between equivalent chemical representations (arbitrary SMILES to canonical SMILES) producing a 512-dimensional latent vector per molecule. The latent space can be used in other works for generation via decoding/interpolation, but in this paper it is used as fixed descriptors for ML.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>512-dimensional continuous vectors (CDDD descriptors).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Compared on QSAR benchmarks (r^2 for regression, AUC for classification) when used with various ML algorithms (ASNN, LibSVM, Random Forest, XGBoost, deep nets).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Same QSAR benchmark datasets used in this paper (MP, BP, BCF, FreeSolv, LogS, Lipo, BACE, DHFR, LEL, HIV, AMES, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>CDDD descriptors provided competitive QSAR performance (best performance with ASNN/LibSVM on several datasets) but Transformer-CNN was similar or better on most datasets; CDDD could not be calculated for molecules outside certain numeric ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly in this paper: Transformer-CNN outperformed or matched CDDD-based approaches on most benchmarks; CDDD required subsequent ML methods (ASNN, LibSVM) and had constraints on molecule properties.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>CDDD descriptors have applicability restrictions (required logP ∈ (−5,7), mol_weight ∈ (1,2? appears truncated in paper) — the paper reports numeric bounds and that molecules must be organic and within certain heavy-atom counts), causing some molecules to be excluded; Transformer-CNN trained on diverse ChEMBL molecules does not share these restrictions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-CNN: Swiss knife for QSAR modeling and interpretation', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Focused library generator: case of Mdmx inhibitors. <em>(Rating: 2)</em></li>
                <li>Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction. <em>(Rating: 2)</em></li>
                <li>A transformer model for retrosynthesis. <em>(Rating: 2)</em></li>
                <li>Learning continuous and data-driven molecular descriptors by translating equivalent chemical representations. <em>(Rating: 2)</em></li>
                <li>Automatic chemical design using a data-driven continuous representation of molecules. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5289",
    "paper_id": "paper-ee42433b0467c8a35ce64bf794abc3169e9fb811",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "Transformer-CNN",
            "name_full": "Transformer-CNN (SMILES canonicalization encoder + TextCNN QSAR)",
            "brief_description": "An encoder-decoder Transformer trained to canonicalize SMILES; the encoder's position-wise latent outputs (dynamic SMILES-embeddings) are frozen and used as input to a 1D TextCNN for QSAR/QSPR modeling and interpretation via Layer-wise Relevance Propagation (LRP).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (encoder-decoder) used for SMILES canonicalization",
            "model_type": "Transformer (encoder-decoder, self-attention based Seq2Seq)",
            "model_size": null,
            "training_data": "ChEMBL SMILES pairs (non-canonical -&gt; canonical) augmented ~11x resulting in ~17,657,995 canonicalization pairs drawn from ChEMBL; only SMILES length &lt;=110 used.",
            "application_domain": "SMILES canonicalization; extraction of embeddings for QSAR/QSPR modeling (drug discovery / ADMET prediction); intended to be embedded in de-novo drug design pipelines.",
            "generation_method": "Not used to generate novel compounds in this paper; trained as a Seq2Seq canonicalization model (non-canonical SMILES -&gt; canonical SMILES) and embeddings were extracted for downstream QSAR. The authors state the approach can be embedded into de-novo pipelines but did not perform molecule design with it here.",
            "output_representation": "Position-wise real-valued SMILES embeddings (matrix of shape N_chars × embedding_dim); downstream models operate on these embeddings via 1D convolutions; canonical SMILES are the decoded output of the Seq2Seq model.",
            "evaluation_metrics": "Canonicalization accuracy on 500,000 generated ChEMBL-like SMILES (percentage correctly canonicalized); QSAR evaluation using coefficient of determination (r^2) for regression datasets and AUC for classification datasets across standard benchmark sets.",
            "benchmarks_or_datasets": "Training: ChEMBL; Validation: 500,000 ChEMBL-like SMILES generated by a focused-library generator (ref. 56); QSAR benchmarks: MP, BP, BCF, FreeSolv, LogS, Lipo, BACE, DHFR, LEL (regression) and HIV, AMES, BACE, Clintox, Tox21, BBBP, JAK3, BioDeg, RP AR (classification), as used in MoleculeNet / OCHEM.",
            "results_summary": "Transformer canonicalization model correctly canonicalized 83.6% of 500,000 generated ChEMBL-like SMILES; extracted embeddings used with TextCNN (Transformer-CNN) produced QSAR models that were similar or superior to descriptor-based approaches on most benchmark datasets (improved r^2/AUC on many sets).",
            "comparison_to_other_methods": "Compared to CDDD descriptors (SmI2canSml autoencoder) and descriptor-based ML, Transformer-CNN generally achieved similar or better QSAR performance; unlike CDDD, Transformer-CNN embeddings support arbitrary molecules processable by RDKit and do not have the same numeric property restrictions.",
            "limitations_or_challenges": "Canonicalization accuracy lower for stereochemistry: only 37.2% correct for SMILES with '@' (stereocenters) and 73.9% for cis/trans ('/' or '\\\\'); relevance propagation shows ~24.6% of signal dissipated into biases (affecting interpretability confidence); Transformer in this paper was not demonstrated to generate novel molecules—only to provide embeddings for QSAR and to canonicalize SMILES.",
            "uuid": "e5289.0",
            "source_info": {
                "paper_title": "Transformer-CNN: Swiss knife for QSAR modeling and interpretation",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Focused library generator",
            "name_full": "Focused library generator (case of Mdmx inhibitors)",
            "brief_description": "A SMILES-generating model cited and used to produce 500,000 ChEMBL-like SMILES (only ~1.7% were canonical) that were used here to validate the Transformer canonicalization model; originally developed to generate focused libraries (Mdmx inhibitors) as a de-novo design tool.",
            "citation_title": "Focused library generator: case of Mdmx inhibitors.",
            "mention_or_use": "use",
            "model_name": "Focused library generator (reference [56])",
            "model_type": null,
            "model_size": null,
            "training_data": null,
            "application_domain": "De-novo/focused library generation for drug discovery (example: Mdmx inhibitors); here used to produce ChEMBL-like SMILES for canonicalization validation.",
            "generation_method": "Not described in this paper; the authors sampled outputs from the referenced generator to obtain a large set of generated SMILES for testing canonicalization.",
            "output_representation": "SMILES strings (many non-canonical forms present).",
            "evaluation_metrics": "In this paper, the generated SMILES set was used to measure canonicalization performance: 418,233/500,000 (83.6%) of generated SMILES were correctly canonicalized by the Transformer model; stereochemistry-containing SMILES had lower canonicalization success.",
            "benchmarks_or_datasets": "A generated set of 500,000 ChEMBL-like SMILES produced by the focused library generator (ref. 56).",
            "results_summary": "Used as a source of test SMILES for canonicalization validation; no activity or application-specific evaluation reported in this paper (the generator itself is cited as applicable to focused library design for Mdmx inhibitors in the referenced work).",
            "comparison_to_other_methods": null,
            "limitations_or_challenges": "This paper does not report internal details (architecture, training data) of the generator; many outputs were non-canonical, and the canonicalization model had notable difficulty with stereochemical SMILES in the generated set.",
            "uuid": "e5289.1",
            "source_info": {
                "paper_title": "Transformer-CNN: Swiss knife for QSAR modeling and interpretation",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "SmI2canSml / CDDD",
            "name_full": "SmI2canSml autoencoder producing CDDD descriptors (continuous data-driven descriptors)",
            "brief_description": "A sequence-to-sequence autoencoder that maps arbitrary SMILES to canonical SMILES and yields fixed-length (512) continuous molecular descriptors (CDDD) usable for QSAR and other downstream tasks.",
            "citation_title": "Learning continuous and data-driven molecular descriptors by translating equivalent chemical representations.",
            "mention_or_use": "use",
            "model_name": "SmI2canSml autoencoder (CDDD)",
            "model_type": "Autoencoder / Seq2Seq model",
            "model_size": null,
            "training_data": null,
            "application_domain": "Molecular descriptor generation for QSAR/QSPR and as continuous latent space for molecular design in other works (not used for de-novo generation in this paper).",
            "generation_method": "Autoencoding / translation between equivalent chemical representations (arbitrary SMILES to canonical SMILES) producing a 512-dimensional latent vector per molecule. The latent space can be used in other works for generation via decoding/interpolation, but in this paper it is used as fixed descriptors for ML.",
            "output_representation": "512-dimensional continuous vectors (CDDD descriptors).",
            "evaluation_metrics": "Compared on QSAR benchmarks (r^2 for regression, AUC for classification) when used with various ML algorithms (ASNN, LibSVM, Random Forest, XGBoost, deep nets).",
            "benchmarks_or_datasets": "Same QSAR benchmark datasets used in this paper (MP, BP, BCF, FreeSolv, LogS, Lipo, BACE, DHFR, LEL, HIV, AMES, etc.).",
            "results_summary": "CDDD descriptors provided competitive QSAR performance (best performance with ASNN/LibSVM on several datasets) but Transformer-CNN was similar or better on most datasets; CDDD could not be calculated for molecules outside certain numeric ranges.",
            "comparison_to_other_methods": "Compared directly in this paper: Transformer-CNN outperformed or matched CDDD-based approaches on most benchmarks; CDDD required subsequent ML methods (ASNN, LibSVM) and had constraints on molecule properties.",
            "limitations_or_challenges": "CDDD descriptors have applicability restrictions (required logP ∈ (−5,7), mol_weight ∈ (1,2? appears truncated in paper) — the paper reports numeric bounds and that molecules must be organic and within certain heavy-atom counts), causing some molecules to be excluded; Transformer-CNN trained on diverse ChEMBL molecules does not share these restrictions.",
            "uuid": "e5289.2",
            "source_info": {
                "paper_title": "Transformer-CNN: Swiss knife for QSAR modeling and interpretation",
                "publication_date_yy_mm": "2020-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Focused library generator: case of Mdmx inhibitors.",
            "rating": 2
        },
        {
            "paper_title": "Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction.",
            "rating": 2
        },
        {
            "paper_title": "A transformer model for retrosynthesis.",
            "rating": 2
        },
        {
            "paper_title": "Learning continuous and data-driven molecular descriptors by translating equivalent chemical representations.",
            "rating": 2
        },
        {
            "paper_title": "Automatic chemical design using a data-driven continuous representation of molecules.",
            "rating": 1
        }
    ],
    "cost": 0.01206075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Transformer-CNN: Swiss knife for QSAR modeling and interpretation</h1>
<p>Pavel Karpov ${ }^{1,2^{*}}$, Guillaume Godin ${ }^{3}$ and Igor V. Tetko ${ }^{1,2}$</p>
<h4>Abstract</h4>
<p>We present SMILES-embeddings derived from the internal encoder state of a Transformer [1] model trained to canonize SMILES as a Seq2Seq problem. Using a CharNN [2] architecture upon the embeddings results in higher quality interpretable QSAR/QSPR models on diverse benchmark datasets including regression and classification tasks. The proposed Transformer-CNN method uses SMILES augmentation for training and inference, and thus the prognosis is based on an internal consensus. That both the augmentation and transfer learning are based on embeddings allows the method to provide good results for small datasets. We discuss the reasons for such effectiveness and draft future directions for the development of the method. The source code and the embeddings needed to train a QSAR model are available on https://github.com/bigchem/transformer-cnn. The repository also has a standalone program for QSAR prognosis which calculates individual atoms contributions, thus interpreting the model's result. OCHEM [3] environment (https://ochem .eu) hosts the on-line implementation of the method proposed.</p>
<p>Keywords: Transformer model, Convolutional neural neural networks, Augmentation, QSAR, SMILES, Embeddings, Character-based models, Cheminformatics, Regression, Classification</p>
<h2>Introduction</h2>
<p>Quantitative Structure-Activity (Property) Relationship (QSAR/QSPR) approaches find a nonlinear function, often modelled as an artificial neural network (ANN), that estimates the activity/property based on a chemical structure. In the past, most QSAR works heavily relied on descriptors [4] that represent in a numerical way some features of a complex graph structure of a compound. Amongst numerous families of descriptors, the fragment descriptors that count occurrences of a subgraph in a molecule graph, hold a distinctive status due to simplicity in the calculation. Moreover, there is a theoretical proof that one can successfully build any QSAR model with them [5]. Even a small database of compounds contains thousands of fragmental descriptors and some feature selection algorithm</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>has traditionally been used to find a proper subset of descriptors for better quality, and to speed up the whole modeling process. Thus, feature selection in conjunction with a suitable machine learning method was key to success [6]. The rise of deep learning [7] allows us to bypass tiresome expert and domain-wise feature construction by delegating this task to a neural network that can extract the most valuable traits of the raw input data required for modeling the problem at hand $[8,9]$.
In this setting, the whole molecule as a SMILES-strings [10, 11] (Simplified Molecular Input Line Entry System) or a graph $[12,13]$ serves as the input to the neural network. SMILES notation allows for the writing of any complex formula of an organic compound in a string facilitating storage and retrieval information about molecules in databases [14]. It contains all information about the compound sufficient to derive the entire configuration (3D-structure) and has a direct connection to the nature of fragmental descriptors, Fig. 1, thus, making SMILES one of the best representation for QSAR studies.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>One of the first works exploiting direct SMILES input as descriptors used fragmentation of strings into groups of overlapping substrings forming a SMILES-like set or a hologram of a molecule [16]. Within this approach, there was no need to derive a 2D/3D configuration of the molecule with subsequent calculation of descriptors keeping the quality of the models at the same level as with classical descriptors or even better.</p>
<p>SMILES strings are sequences of characters; therefore, they can be analyzed by machine-learning methods suitable for text processing, namely with convolutional and recurrent neural networks. After the demonstration of text understanding from character-level inputs [17], this technique was adopted in chemoinformatics [11, 18–21]. Recently, we showed that the augmentation of SMILES (using canonical as well as non-canonical SMILES during model training and inference) increases the performance of convolutional models for regression and classification tasks [22].</p>
<p>Technically modern machine-learning models consist of two parts working together. The first part encodes the input data and extracts the most robust features by applying convolutional filters with different receptive fields (RF) or recurrent layers, whereas the second part directly builds the regular model based on these features using standard dense layers as building blocks (so called classical “MLP”), Fig. 2. Though powerful convolutional layers can effectively encode the input within its internal representation, usually one needs a considerable training dataset and computational resources to train the encoder part of a network.</p>
<p>The concept of embeddings mitigates the problem by using the pre-trained weights designed for image [23] or text processing [24] tasks. It allows transfer learning from previous data and speeds up the training process for building models with significantly smaller datasets inaccessible for training from scratch. Typically, QSAR datasets contain only several hundreds of molecules, and SMILES-embeddings could improve models by developing better features.</p>
<p>One way of separately obtaining SMILES embeddings is to use classical autoencoder [25] approach where the input is the same as the output. In the case of SMILES, however, it would be more desirable to explore a variety of SMILES belonging to the same molecule due to redundant SMILES grammar, Fig. 1. We hypothesized that it is possible to train a neural network to conduct a SMILES canonicalization task in a Sequence-to-Sequence (Seq2Seq) manner like a machine translation problem, where on the left side are non-canonical SMILES, and on the right side are their canonical equivalents. Recently, Seq2Seq was successfully applied to translation from InChi [26] codes to SMILES (Inchi2Sml) as well as from SMILES arbitrary to canonical SMILES (Sml2canSml), and to build QSAR models on extracted latent variables [27].</p>
<p>The state-of-the-art neural architecture for machine translation consists of stacked Long Short-Term Memory (LSTM) cells [28]. The training process for such networks inherently has all kinds of Recurrent Neural Networks difficulties, e.g., vanishing gradients, and the impossibility of parallelization. Recently, a Transformer model [1] was proposed where all recurrent units are replaced with convolutional and element-wise feed-forward layers. The whole architecture shows a significant speed-up during training and inference with improved accuracy over</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>translation benchmarks. The Transformer model was applied for prediction of reaction outcomes [29] and for retrosynthesis [30].</p>
<p>Modern machine learning architectures although demonstrating incredible performance still lack interpretability. Explaining the reasons for a particular prediction of a model avoids "Clever Hans" predictors with spurious or non-relevant correlations [31] and foster trust and verifiability. One of the promising methods to open a "black box" uses the Layer-wise Relevance Propagation (LRP) algorithm [32], which splits the overall predicted value to a sum of contributions of individual neurons. In this method, the sum of relevance of all neurons of a layer, including the bias neuron, is kept constant. Propagation of the relevance from the last layer to the input layer allows the evaluation of the contributions of particular input features in to select the most relevant features for the whole training set [33] or to explain the individual neural network prediction [32]. We apply the LRP method for an explanation of individual results, checking the model get results for the right reason.</p>
<p>Our contributions in the article are as follows:</p>
<p>Presenting a concept of dynamic SMILES embeddings that may be useful for a wide range of cheminformatics tasks;</p>
<p>Scrutinizing CharNN models based on these embeddings for regression and classification tasks and show that the method outperforms the state-of-the-art models;</p>
<p>Interpretation of the model based on LRP method;</p>
<p>Our implementation as well as source codes and SMILES-embeddings are available on https://github.com/bigchem/transformer-cnn. We also provide ready-to-use implementation on https://ochem.eu within the OCHEM [3] environment and a standalone program for calculating properties and explaining the results.</p>
<h2>Methods</h2>
<h2>SMILES canonicalization model</h2>
<h2>Dataset</h2>
<p>To train the ANN to perform SMILES canonicalization, we used the ChEMBL database [34] with SMILES strings of length less than or equal 110 characters ( $&gt;93 \%$ of the entire database). The original dataset was augmented 10 times up to $17,657,995$ canonicalization pairs written in reactions format separated by ' $&gt;&gt;$ '. Each pair contained on the left side a non-canonical, and on the right side-a canonical SMILES for the same molecule. Such an arrangement of the training dataset allowed us to reuse the previous Transformer code, which was originally applied for retrosynthetic tasks [30]. For completeness, we added for every compound a line where both left and right sides were identical, i.e. canonical SMILES, Fig. 3. Thus each molecule was present in the training set 11 times. If a molecule had tautomeric forms then all of them were accounted for as separate entries in the training data file.</p>
<h2>Model input</h2>
<p>Seq2Seq models use one-hot encoding vector for the input. Its values are zero everywhere except the position of the current token which is set to one. Many works on SMILES use tokenization procedure [35, 36] that combines some characters, for example ' $B$ ' and ' $r$ ' to one token 'Br'. Other rules for handling most common two-letters elements, charges, and stereochemistry also are used for preparing the input for the neural network. According to our experience, the use of more complicated schemes instead of simple character-level tokenization did not increase the accuracy of models [30]. Therefore a simple character-level tokenization was used in this study. The vocabulary of our model consisted of all possible characters from ChEMBL dataset and has 66 symbols: ^#\%()+--./0123456789=@ABCDEFGHIKLMNOPRSTVXYZ $\backslash$ \abcdefgilmnoprstuy\$</p>
<p>Thus, the model could handle the entire diversity of drug-like compounds including stereochemistry, different charges, and inorganic ions. Two special characters were added to the vocabulary: ' $\wedge$ ' to indicate the start of the sequence, and '\$' to inform the model of the end of data input.</p>
<h2>Transformer model</h2>
<p>The canonicalization model used in this work was based upon a Transformer architecture consisting of two separate stacks of layers for the encoder and the decoder, respectively. Each layer incorporated some portion of knowledge written in its internal memory (V) with indexed access by keys (K). When new data arrived (Q), the layer calculated attention and modified the input accordingly (see the original work on Transformers [1]), thus, forming the output of the self-attention layer and weighting those parts that carry the essential information. Besides a self-attention mechanism, the layer also contained several position-wise dense layers, a normalization layer, and residual connections [1, 37]. Our model utilized a three layer architecture of Transformer with 10 blocks of self-attention, i.e. the same one as used in our previous study [30]. After the encoding process was finished, the output of the top encoder layer contained a representation of a molecule suitable for decoding into canonical SMILES. In this study we used this representation as a well-prepared latent representation for QSAR modeling.</p>
<p>Tensorflow v1.12.02 [38] was used as machine-learning framework to develop all parts of the Transformer, whereas RDKit v.2018.09.2 [39] was used for SMILES canonicalization and augmentation.</p>
<h2>QSAR model</h2>
<p>We call the output of the Transformer's encoder part a dynamic SMILES-embedding, Fig. 4. For a molecule with $N$-characters, the encoder produces the matrix with dimensions ( $N, E M B E D D I N G S)$. Though technically this</p>
<p>|  | canonical |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  c1c(ccc(O)c1O)CCN(CCCC)CCC |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  c1(ccc(c(c1)O)O)CCN(CCC)CCCC |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  c1c(c(ccc(c1)CCN(CCCC)CCC)O)O |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  c1(CCN(CCCC)CCC)ccc(O)c(O)c1 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  CCCCN(CCc1cc(c(cc1)O)O)CCC |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  CCCCN(CCc1ccc(c(c1)O)O)CCC |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  N(CCCC)(CCc1ccc(c(O)c1)O)CCC |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  C(N(CCc1ccc(O)c(c1)O)CCCC)CC |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  N(CCc1ccc(c(O)c1)O)(CCCC)CCC |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  c1c(O)c(ccc1CCN(CCC)CCCC)O |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  c1(c(cc(cc1)CCN(CCC)CCCC)O)O |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  Fig. 3 Example of the data in the training file for canonicalization model of a small molecule CHEMBL351484. Every line contains a pair of non-canonical (left) and canonical (right) separated by ' $&gt;&gt;$ '. One line has identical SMILES on both sides, stressed with the red box |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>matrix is not an embedding because equivalent characters have different values depending on position and surroundings, it can be considered so due to its role: to convert an input one-hot raw vectors to real-value vectors in some latent space. Because these embeddings have variable lengths, we used a series of 1D convolutional filters as implemented in DeepChem [40] TextCNN method (https://github.com/deepchem).</p>
<p>Each convolution had a kernel size from the list (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20) and produced the following number of filters (100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160), respectively. After a GlobalMaxPool operation and the subsequent concatenation of the pooling results, the data went through Dropout [41] (rate = 0.25), Dense (N = 512), Highway [42] layers, and, finally, converted to the output layer which consisted of only one neuron for regression and two neurons for classification tasks. The weights of the Transformer's part were frozen in all experiments. All models used the Adam optimizer with Mean Squared Error or Binary Cross-Entropy loss depending on the problem at hand. A fixed learning rate λ = 10^{-4} was used. Early-stopping was used to prevent overfitting, to select a best model, and to reduce training time. OCHEM calculations were performed using canonical SMILES as well as ten-fold augmented SMILES during both training and prognosis. This number of SMILES augmentations was found to be an optimal one in our previous study [43]. An average value of the individual predictions for different representation of the same molecule was used as the final model prediction to calculate statistical parameters.</p>
<p>The same five-fold cross-validation procedure was used to compare the models with the results of our previous study [43]. The coefficients of determination [44]</p>
<p>$$r^2 = 1 - SS_{\text{res}} / SS_{\text{tot}}\tag{1}$$</p>
<p>where SStot is total variance of data and SSres is residual unexplained variance of data was used to compare regression models and Area Under the Curve (AUC) was used for classification tasks.</p>
<h3>Validation datasets</h3>
<p>We used the same datasets (9 for regression and 9 for classification) that were exploited in our previous studies [11, 22]. Short information about these sets as well as links to original works are provided in Table 1. The datasets are</p>
<table>
<thead>
<tr>
<th>Code</th>
<th>Description</th>
<th>Size</th>
<th>Code</th>
<th>Description</th>
<th>Size</th>
</tr>
</thead>
<tbody>
<tr>
<td>Regression tasks</td>
<td></td>
<td></td>
<td>Classification tasks</td>
<td></td>
<td></td>
</tr>
<tr>
<td>MP</td>
<td>Melting point [45]</td>
<td>19,104</td>
<td>HIV</td>
<td>Inhibition of HIV replication [46]</td>
<td>41,127</td>
</tr>
<tr>
<td>BP</td>
<td>Boiling point [47]</td>
<td>11,893</td>
<td>AMES</td>
<td>Mutagenicity [48]</td>
<td>6542</td>
</tr>
<tr>
<td>BCF</td>
<td>Bioconcentration factor [47]</td>
<td>378</td>
<td>BACE</td>
<td>Human β-secretase 1 (BACE-1) inhibitors [46]</td>
<td>1513</td>
</tr>
<tr>
<td>FreeSolv</td>
<td>Free solvation energy [46]</td>
<td>642</td>
<td>Clintox</td>
<td>Clinical trial toxicity [46]</td>
<td>1478</td>
</tr>
<tr>
<td>LogS</td>
<td>Solubility [49]</td>
<td>1311</td>
<td>Tox21</td>
<td>In-vitro toxicity [46]</td>
<td>7831</td>
</tr>
<tr>
<td>Lipo</td>
<td>Lipophilicity [50]</td>
<td>4200</td>
<td>BBBP</td>
<td>Blood–brain barrier [46]</td>
<td>2,039</td>
</tr>
<tr>
<td>BACE</td>
<td>IC50 of human β-secretase 1 (BACE-1) inhibitors [46]</td>
<td>1513</td>
<td>JAK3</td>
<td>Janus kinase 3 inhibitor [51]</td>
<td>886</td>
</tr>
<tr>
<td>DHFR</td>
<td>Dihydrofolate reductase inhibition [52]</td>
<td>739</td>
<td>BioDeg</td>
<td>Biodegradability [53]</td>
<td>1737</td>
</tr>
<tr>
<td>LEL</td>
<td>Lowest effect level [54]</td>
<td>483</td>
<td>RP AR</td>
<td>Endocrine disruptors [55]</td>
<td>930</td>
</tr>
</tbody>
</table>
<p><strong>Table 1</strong> Descriptions of datasets used in the work</p>
<p>available on the OCHEM environment on https://ochem.eu.</p>
<h2>Results and discussion</h2>
<h3>SMILES canonicalization model</h3>
<p>The Transformer model was trained for 10 epochs with the learning rate changing according to the formula:</p>
<p>$$\lambda = factor \ast \min \left(1.0, step / warmup\right) / \max (step, warmup) \tag{2}$$</p>
<p>where <em>factor</em> = 20, <em>warmup</em> = 16,000 steps, and if λ &lt; 10^{-4} then λ = 10^{-4}. The settings for the learning rate were similar to those used in our retro-synthesis study. Each epoch contained 275,907 steps (batches). No early-stopping or weight-averaging was applied. Learning curves are shown in Fig. 5.</p>
<p>To validate the model, we sampled 500,000 ChEMBL-like SMILES (only 8,617 (1.7%) of them were canonical) from a generator [56] and checked how accurately the model can restore canonical SMILES for these molecules. We intentionally selected the generated SMILES keeping in mind possible applications of the proposed method in artificial intelligence-driven pipelines of <em>de-novo</em> drug development. The model correctly canonicalized 83.6% of all samples, Table 2.</p>
<h3>QSAR modeling</h3>
<p>For the QSAR modeling the saved embedding was used. The training was done using a fixed learning rate λ = 0.001 for n = 100 epochs. Early stopping with 10% randomly selected SMILES was used to identify the optimal model. Table 3, Fig. 6 compares results for regression datasets</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p><strong>Fig. 5</strong> Learning curves: 1) learning rate schedule (axes bottom and right), and 2) character-based accuracy (axes bottom and left) on the training dataset for the first four epochs</p>
<h3>Table 2 Validation of canonicalization model</h3>
<table>
<thead>
<tr>
<th>Strings</th>
<th>All</th>
<th>Correctly canonicalized</th>
</tr>
</thead>
<tbody>
<tr>
<td>All</td>
<td>500,000</td>
<td>418,233 (83.6%)</td>
</tr>
<tr>
<td>Stereo (with @)</td>
<td>77,472</td>
<td>28,821 (37.2%)</td>
</tr>
<tr>
<td>Cis/trans (with / or )</td>
<td>54,727</td>
<td>40,483 (73.9%)</td>
</tr>
</tbody>
</table>
<p>while Table 4, Fig. 7 compares classification tasks. The standard mean errors of the values were calculated using a bootstrap procedure as explained elsewhere [53].</p>
<p>With an exception of a few datasets, the proposed method provided similar or better results than those calculated using descriptor-based approaches as well as the other SMILES-based approaches investigated in our previous study [43]. The data augmentation was critically important for the Transformer-CNN method to achieve its high performance. We used augmentation n = 10, i.e., 10 SMILES were randomly generated and used for model development and application, which was found optimal in the aforementioned previous study.</p>
<p>Similar to Transformer-CNN the Sml2canSml used an internal representation, which was developed by mapping arbitrary SMILES to canonical SMILES. The difference was that Sml2canSml generated a fixed set of 512 latent variables (CDDD descriptors), while the Transformer-CNN representation had about the same length as the initial SMILES. Sml2canSml CDDD could be used as descriptors for any traditional machine learning methods while Transformer-CNN required convolutional neural networks to process the variable length output and to correlate it with the analysed properties. Sml2canSml was added as CDDD descriptors to OCHEM. These descriptors were analysed by the same methods as used in the previous work, i.e., LibSVM [57], Random Forest [58], XGBoost [59] as well as by Associative Neural Networks (ASNN) [60] and Deep Neural Networks [61]. Exactly the same protocol, fivefold cross-validation, was used for all calculations. The best performance using the CDDD descriptors was obtained by ASNN and LibSVM methods, which contributed models with the highest accuracy for seven and five datasets respectively (LibSVM method provided the best performance in the original study). Transformer-CNN provided better or similar results compared to the CDDD descriptors for all datasets with an exception of Lipo and FreeSolv. It should be also mentioned that CDDD descriptors could only process molecules which satisfy the following conditions:</p>
<ul>
<li>logP ∈ (−5,7) and</li>
<li>mol_weight ∈ (12,600) and</li>
<li>num_heavy_atoms ∈ (3, 50) and</li>
<li>molecule is organic.</li>
</ul>
<p>Table 3 Coefficient of determination, r^{2}, calculated for regression sets (higher values are better)</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Descriptor based methods^{2}</th>
<th>SMILES based (augm = 10)^{a}</th>
<th>Transformer-CNN, no augm</th>
<th>Transformer-CNN, augm = 10</th>
<th>CDDD descriptors^{b}</th>
</tr>
</thead>
<tbody>
<tr>
<td>MP</td>
<td>0.83</td>
<td>0.85</td>
<td>0.83</td>
<td>0.86</td>
<td>0.85</td>
</tr>
<tr>
<td>BP</td>
<td>0.98</td>
<td>0.98</td>
<td>0.97</td>
<td>0.98</td>
<td>0.98</td>
</tr>
<tr>
<td>BCF</td>
<td>0.85</td>
<td>0.85</td>
<td>0.71 ± 0.02</td>
<td>0.85</td>
<td>0.81</td>
</tr>
<tr>
<td>FreeSolv</td>
<td>0.94</td>
<td>0.93</td>
<td>0.72 ± 0.02</td>
<td>0.91</td>
<td>0.93</td>
</tr>
<tr>
<td>LogS</td>
<td>0.92</td>
<td>0.92</td>
<td>0.85</td>
<td>0.91</td>
<td>0.91</td>
</tr>
<tr>
<td>Lipo</td>
<td>0.7</td>
<td>0.72</td>
<td>0.6</td>
<td>0.73</td>
<td>0.74</td>
</tr>
<tr>
<td>BACE</td>
<td>0.73</td>
<td>0.72</td>
<td>0.66</td>
<td>0.76</td>
<td>0.75</td>
</tr>
<tr>
<td>DHFR</td>
<td>0.62 ± 0.03</td>
<td>0.63 ± 0.03</td>
<td>0.46 ± 0.03</td>
<td>0.67 ± 0.03</td>
<td>0.61 ± 0.03</td>
</tr>
<tr>
<td>LEL</td>
<td>0.19 ± 0.04</td>
<td>0.25 ± 0.03</td>
<td>0.2 ± 0.03</td>
<td>0.27 ± 0.04</td>
<td>0.23 ± 0.04</td>
</tr>
</tbody>
</table>
<p>We omitted the standard mean errors, which are 0.01 or less, for the reported values
^{a} Results from our previous study [22]. ^{b}Best performance calculated with CDDD descriptors obtained using autoencoder SmI2canSml from [27]
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Table 4 AUC calculated for classification sets (higher values are better)</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Descriptor based methods^{a}</th>
<th>SMILES based (augm = 10)^{2}</th>
<th>Transformer-CNN, no augm</th>
<th>Transformer-CNN, augm = 10</th>
<th>CDDD descriptors^{b}</th>
</tr>
</thead>
<tbody>
<tr>
<td>HIV</td>
<td>0.82</td>
<td>0.78</td>
<td>0.81</td>
<td>0.83</td>
<td>0.74</td>
</tr>
<tr>
<td>AMES</td>
<td>0.86</td>
<td>0.88</td>
<td>0.86</td>
<td>0.89</td>
<td>0.86</td>
</tr>
<tr>
<td>BACE</td>
<td>0.88</td>
<td>0.89</td>
<td>0.89</td>
<td>0.91</td>
<td>0.9</td>
</tr>
<tr>
<td>Clintox</td>
<td>0.77 ± 0.03</td>
<td>0.76 ± 0.03</td>
<td>0.71 ± 0.02</td>
<td>0.77 ± 0.02</td>
<td>0.73 ± 0.02</td>
</tr>
<tr>
<td>Tox21</td>
<td>0.79</td>
<td>0.83</td>
<td>0.81</td>
<td>0.82</td>
<td>0.82</td>
</tr>
<tr>
<td>BBBP</td>
<td>0.90</td>
<td>0.91</td>
<td>0.9</td>
<td>0.92</td>
<td>0.89</td>
</tr>
<tr>
<td>JAK3</td>
<td>0.79 ± 0.02</td>
<td>0.8 ± 0.02</td>
<td>0.70 ± 0.02</td>
<td>0.78 ± 0.02</td>
<td>0.76 ± 0.02</td>
</tr>
<tr>
<td>BioDeg</td>
<td>0.92</td>
<td>0.93</td>
<td>0.91</td>
<td>0.93</td>
<td>0.92</td>
</tr>
<tr>
<td>RP AR</td>
<td>0.85</td>
<td>0.87</td>
<td>0.83</td>
<td>0.87</td>
<td>0.86</td>
</tr>
</tbody>
</table>
<p>We omitted the standard mean errors, which are 0.01 or less, for the reported values
^{a} Results from our previous study [22]. ^{b}Best performance calculated with CDDD descriptors obtained using SmI2canSml autoencoder from [27]</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>These limitations appeared due to the preparation of the training set to develop the Sml2canSml encoder. The limitations resulted in the exclusion of a number of molecules, which failed one or several of the above conditions. Contrary to the Sml2canSml encoder, we trained Transformer-CNN with very diverse molecules from ChEMBL and thus the developed models could be applied to any molecule which can be processed by RDKiT. The exclusion of molecules for which CDDD descriptors failed to be calculated did not significantly change the results of Transformer models: some models improved while others decreased their accuracy for ~ 0.01 respective performance values. For example, for Lipo and FreeSolv sets the accuracy of the Transformer-CNN model increased to r^{2} = 0.92 and 0.75 respectively, while for BBB the AUC decreased to 0.91.</p>
<h3>Interpretability of the model</h3>
<p>Layer-wise relevance propagation was used to interpret the models. For gated connections (in HighWay block) we implemented the signal-take-all redistribution rule [62] while all other Dense and Convolutional layers were well fitted in the LRP framework [32] without any adaptation. In this work, we stopped the relevance propagation on the output of the Transformer's encoder which is position-wise. It should be noted that we froze the encoder part of the network during QSAR model training. Summing up all the individual features for each position in the SMILES string calculated its contribution to the final result. If the LRP indicated a reasonable explanation of the contributions of fragments then one can trust that the model made predictions based on detected fundamental structure–property relationships. For explanation we selected classification (AMES mutagenicity) and regression (water solubility) models.</p>
<h3>AMES mutagenicity</h3>
<p>The AMES test is a widely used qualitative test to determine the mutagenic potential of a molecule, from which extensive structural alerts collections were derived [63]. Examples of these alerts are aromatic nitros, N-oxides, aldehydes, monohaloalkenes, quinones, etc. A QSAR model for AMES had to pay special attention to these and similar groups to be interpretable and reliable. The Transformer-CNN model built on 6542 endpoints (3516 mutagenic and 3026 nonmutagenic) results in AUC = 0.89, Table 4.</p>
<p>The structure of 1-Bromo-4-nitrobenzene gave the positive AMES test. The output of the LRP procedure for one of possible SMILES for this compound, namely 1c(<a href="[O-]">N+</a> = O)ccc(c1)Br, is shown in Table 5.</p>
<p>According to the LRP, the relevance was constant during the propagation:</p>
<p>$$
\begin{aligned}
y &amp;= R = f(x) = \sum_{l \in (L)} R_l = \sum_{l \in (L-1)} R_l \
&amp;= \sum_{l \in (L-2)} R_l = \sum_{l \in (1)} R_l.
\end{aligned}
\tag{3}
$$</p>
<p>Here (L) stood for a set of neurons in the last layer, (L-1)—in the layer before the last layer, and so on. Each layer in the Transformer-CNN network contained biases (B), and thus some relevance dissipated on them. Therefore the above equation was corrected to:</p>
<p>$$\sum_{l \in (L)} R_l = \sum_{l \in (L-1)} R_l + B.\tag{4}$$</p>
<p>Table 5 Local relevance conservation for c1c(<a href="[O-]">N+</a>=0)ccc(Br)c1</p>
<p>|  Layer | Relevance, R (L+1) | Relevance, R (L) | Delta, R (L+1)-R (L) | Bias, Delta / R
(L+1) *100\%  |
| --- | --- | --- | --- | --- |
|  Result | 0.98119 | - | - | -  |
|  HighWay Output | 0.98119 | 0.9300 | 0.0512 | 5.21  |
|  HighWay Input | 0.9300 | 0.7227 | 0.2073 | 22.3  |
|  DeMaxPool | 0.7227 | 0.7371 | -0.0144 | -1.98  |
|  Conv1 | 0.0090 | 0.0117 | -0.00271 | -30.1  |
|  Conv2 | 0.1627 | 0.1627 | $0^{\text {a }}$ | 0  |
|  Conv3 | -0.0443 | -0.0443 | 0 | 0  |
|  Conv4 | 0.0191 | 0.0191 | 0 | 0  |
|  Conv5 | -0.0984 | -0.0984 | 0 | 0  |
|  Conv6 | -0.0136 | -0.0136 | 0 | 0  |
|  Conv7 | 0.0806 | 0.0806 | 0 | 0  |
|  Conv8 | 0.0957 | 0.0957 | 0 | 0  |
|  Conv9 | 0.1528 | 0.1528 | 0 | 0  |
|  Conv10 | 0.0845 | 0.0845 | 0 | 0  |
|  Conv15 | 0.1038 | 0.1038 | 0 | 0  |
|  Conv20 | 0.1851 | 0.1851 | 0 | 0  |
|  Total | 0.98119 | 0.7398 | 0.2414 | 24.6  |</p>
<p>${ }^{a}$ All 0 values were all less than $10^{-5}$</p>
<p>We calculated how much of the relevance was taken by biases and reported these values in the output of the ochem.py script. Table 5 clearly shows that $24.6 \%$ of the output signal was taken by biases and $75.4 \%$ were successfully propagated to position-wise layers, which we used to interpret the model. If less than $50 \%$ of the signal came to the input, it may indicate an applicability domain problem or technical issues with relevance propagation. In these cases the interpretation could be questioned.</p>
<p>Iterating through all non-hydrogen atoms, the interpretation algorithm picked up an atom and drew a SMILES from it. Thus, every molecule had a corresponding set of SMILES equal to the number of atoms. The LRP was used for every SMILES, and then the individual predictions were averaged for the final output. 1-Bromo-4-nitrobenzene was predicted as mutagenic with the score 0.88. Impacts of the atoms on the property is depicted in Fig. 8. The model predicted this compound as mutagenic because of the presence of nitro and halogen benzene moieties. Both are known to be structural alerts for mutagenicity [63]. Charged oxygen provided a bigger impact than the double bonded one in the nitro group because its presence contributed to the mutagenicity for nitro and also for N -oxide compounds.</p>
<h2>Aqueous solubility</h2>
<p>Solubility is a crucial property in drug-development. To have a fast, robust, and explainable tool for its prediction and interpretation is highly desirable by both academia <img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 8 Visualization of atom contributions, in the case of a mutagenic compound. The red color stands for mutagenic alerts, color green against it and industry. The Transformer-CNN model built on 1311 compounds had the following statistics: $q^{2}=0.92$ and $R M S E_{p}=0.57$ [64]. For demonstration of its interpretability we choose haloperidol-a well-known antipsychotic drug with $14 \mathrm{mg} / \mathrm{l}$ water solubility [65].</p>
<p>The Transformer model calculated the same solubility $14 \pm 2 \mathrm{mg} / \mathrm{L}$ for this compound. The individual atom contributions are shown in Fig. 9. Hydroxyl, carbonyl, aliphatic nitrogen, and halogens contributed mostly to the solubility. These groups can form ionizable zones in the molecule thus helping water to dissolve the substance.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Several aromatic carbons had negative contributions, which was expected since aromatic compounds are poorly soluble in water. Thus the overall explanation made sense, and the model had an excellent statistics not because of spurious correlations, but because it found the right fragmental features responsible for modelled property. The standalone program contributed in this work has no dependencies on machine learning frameworks, it is easy to install, to use, and to interpret the modelling results. This will make it an indispensable work-horse for drug-development projects world-wide.</p>
<h2>Conclusions and outlook</h2>
<p>For the first time we proposed a SMILES canonicalization method based on Transformer architecture that extracts information-rich real-value embeddings during the encoding process and exposes them for further QSAR studies. Also, for the first time we developed a framework for the interpretation of models based on the Transformer architecture using a layer-wise relevance propagation (LPR) approach.</p>
<p>TextCNN approaches efficiently worked with embeddings generated by Transformer, and the final quality of the QSAR models was higher compared to the models obtained with the state-of-the-art methods on the majority of diverse benchmark datasets. The Transformer-CNN architecture required less than a hundred iterations to converge for QSAR tasks to model various biological activity or physico-chemical properties. It can be easily embedded into <em>de-novo</em> drug development pipelines. The model predictions interpreted in a fragment contribution manner using the LPR could be useful to design new molecules with desired biological activity and ADMETox properties. The source code is available on https://github.com/bigchem/transformer-cnn as well as an on-line version on https://ochem.eu. For solubility and AMES mutagenicity we also deposited standalone models in the GitHub repository, which not only predict the respective properties but also provide interpretations of predictions.</p>
<p>The Transformer-CNN predicts the endpoint based on an average of individual prognosis for a batch of augmented SMILES belonging to the same molecule. The deviation within the batch can serve as a measure of a confidence interval of the prognosis. Dissipation of relevance on biases as well as analysis of restored SMILES can be used to derive the applicability domains of models. These questions will be addressed in the upcoming studies.</p>
<p>Also, as a comment, we do not think that the authors benchmarking their methods are impassioned about their work. Such benchmarking could be properly done by other users, and we do hope to see the proposed method used soon in future publications. But indeed, remarkably, in this work we saw an outstanding performance of the proposed architecture, which provided systematically better or at least similar results compared to the best descriptor-based approaches as well as several analysed deep neural network architectures. Even more remarkably, the Transformer CNN has practically no adjustable meta parameters and thus does not require spending time to tune hyperparameters of neural architectures, use the grid search to optimise Support Vector Machines, optimise multiple parameters of XGBoost, apply various descriptors filtering and preprocessing, which could easily contribute to the overfitting of models. This as well as the possibility to interpret models makes Transformer CNN a Swiss-knife for QSAR modeling and interpretation, which will help to make the QSAR great again!</p>
<p>ADMETox
Absorption, distribution, metabolism, excretion and toxicity
ANN
Artificial neural network
CNN
Convolutional neural network
LSTM
Long Short-Term memory
OCHEM
On-line chemical database and modeling environment
SMILES
Simplified Molecular-Input Line-Entry System
QSAR/QSPR
Quantitative Structure Activity/Property Relationship
RF
Receptive field
RNN
Recurrent Neural Network
CNN
Convolutional Neural Network
Transformer-CNN
Transformer Convolutional Neural Network</p>
<p>The authors thank NVIDIA Corporation for donating Quadro P6000, Titan Xp, and Titan V graphics cards for this research work.</p>
<p>PK implemented the method, IVT and GC performed the analysis and benchmarking. All authors read and approved the final manuscript.</p>
<p>This study was funded by the European Union's Horizon 2020 research and innovation program under the Marie Skłodowska-Curie grant agreement No. 676434, "Big Data in Chemistry" and ERA-CVD "CardioOncology" project, BMBF 01KL1710.</p>
<h2>Availability of data and materials</h2>
<p>The source code of the Transformer-CNN is available on https://github.com/bigchem/transformer-cnn. Ready-to-use implementation, training datasets, and models are available on OCHEM https://ochem.eu.</p>
<h2>Competing interests</h2>
<p>The authors declare that they have no actual or potential conflicts of interests.</p>
<h2>Author details</h2>
<p>${ }^{1}$ Institute of Structural Biology, Helmholtz Zentrum München-Research Center for Environmental Health (GmbH), Ingolstädter Landstraße 1, 85764 Neuherberg, Germany. ${ }^{2}$ BIGCHEM GmbH, Ingolstädter Landstraße 1, 85764 Neuherberg, Germany. ${ }^{3}$ Firmenich International SA, Digital Lab, Geneva, Lausanne, Switzerland.</p>
<h2>Received: 12 October 2019 Accepted: 9 March 2020 Published online: 18 March 2020</h2>
<h2>References</h2>
<ol>
<li>Vaswani A, Shazeer N, Parmar N, et al (2017) Attention is all you need. Paper presented at the 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. arXiv:1706.03762</li>
<li>Zhang X, Zhao J, LeCun Y (2015) Character-level convolutional networks for text classification. arXiv e-prints. arXiv:1509.01626</li>
<li>Sushko I, Novotarskyi S, Körner R et al (2011) Online chemical modeling environment (OCHEM): web platform for data storage, model development and publishing of chemical information. J Comput Aided Mol Des 25:533-554. https://doi.org/10.1007/s10822-011-9440-2</li>
<li>Mauri A, Consonni V, Pavan M, Todeschini R (2006) Dragon software: an easy approach to molecular descriptor calculations. Match 56:237-248</li>
<li>Baskin I, Varnek A (2008) Fragment descriptors in SAR/QSAR/QSPR studies, molecular similarity analysis and in virtual screening. Chemoinformatics approaches to virtual screening. Royal Society of Chemistry, Cambridge, pp 1-43</li>
<li>Eklund M, Norinder U, Boyer S, Carlsson L (2014) Choosing feature selection and learning algorithms in QSAR. J Chem Inf Model 54:837-843. https ://doi.org/10.1021/cr400573c</li>
<li>Baskin II, Winkler D, Tetko IV (2016) A renaissance of neural networks in drug discovery. Expert Opin Drug Discov 11:785-795. https://doi. org/10.1080/17460441.2016.1201262</li>
<li>Duvenaud D, Maclaurin D, Aguilera-Iparraguirre J, et al (2015) Convolutional networks on graphs for learning molecular fingerprints. arXiv e-prints. arXiv:1509.09292</li>
<li>Coley CW, Barzilay R, Green WH et al (2017) Convolutional embedding of attributed molecular graphs for physical property prediction. J Chem Inf Model 57:1757-1772. https://doi.org/10.1021/acs.jcim.6b00601</li>
<li>Gómez-Bombarelli R, Wei JN, Duvenaud D et al (2018) Automatic chemical design using a data-driven continuous representation of molecules. ACS Central Sci 4:268-276. https://doi.org/10.1021/acscentsci.7b00572</li>
<li>Kimber TB, Engelke S, Tetko IV, et al (2018) Synergy effect between convolutional neural networks and the multiplicity of smiles for improvement of molecular prediction. arXiv e-prints. arXiv:1812.04439</li>
<li>Gilmer J, Schoenholz SS, Riley PF, et al (2017) Neural message passing for quantum chemistry. Proceedings of the 34 th International conference on machine learning, Sydney, Australia, PMLR 70. arXiv:1704.01212</li>
<li>Shang C, Liu Q, Chen K-S, et al (2018) Edge attention-based multi-relational graph convolutional networks. arXiv e-prints. arXiv:1802.04944</li>
<li>Weininger D (1988) SMILES, a chemical language and information system. I. Introduction to methodology and encoding rules. J Chem Inf Comput Sci 28:31-36. https://doi.org/10.1021/cr00057a005</li>
<li>O'Boyle NM, Banck M, James CA et al (2011) Open babel: an open chemical toolbox. J Cheminform 3:33. https://doi.org/10.1186/1758-2946-3-33</li>
<li>Vidal D, Thomann M, Pons M (2005) LINGO, an efficient holographic text based method to calculate biophysical properties and intermolecular similarities. J Chem Inf Model 45:386-393. https://doi.org/10.1021/cr049 6797</li>
<li>Zhang X, LeCun Y (2015) Text understanding from scratch. arXiv e-prints. arXiv:1502.01710</li>
<li>Goh GB, Hodas NO, Siegel C, Vishnu A (2017) SMILES2Vec: an interpretable general-purpose deep neural network for predicting chemical properties. arXiv e-prints. arXiv:1712.02034</li>
<li>Jastrzębski S, Leśniak D, Czarnecki WM (2016) Learning to SMILE(S). arXiv e-prints. arXiv:1602.06289</li>
<li>Goh GB, Siegel C, Vishnu A, Hodas NO (2017) Using rule-based labels for weak supervised learning: a chemnet for transferable chemical property prediction. arXiv e-prints. arXiv:1712.02734</li>
<li>Zheng S, Yan X, Yang Y, Xu J (2019) Identifying structure-property relationships through SMILES syntax analysis with self-attention mechanism. J Chem Inf Model 59:914-923. https://doi.org/10.1021/acs.jcim.8b00803</li>
<li>Tetko IV, Karpov P, Bruno E, Kimber TB, Godin G. Augmentation Is What You Need! In: Tetko IV, Karpov P, Kurkova V (ed) 28th International Conference on Artificial Neural Networks Munich, Germany, 2019 Sep 17, Proceedings, Part V, Workshop and Special sessions, Springer, Cham, pp 831-835</li>
<li>Kiela D, Bottou L (2014) Learning image embeddings using convolutional neural networks for improved multi-modal semantics. In: Proceedings of the 2014 Conference on empirical methods in natural language processing (EMNLP). pp 36-45</li>
<li>Pennington J, Socher R, Manning CD (2014) Glove: global vectors for word representation. EMNLP</li>
<li>Hinton GE, Salakhutdinov RR (2006) Reducing the dimensionality of data with neural networks. Science 313:504-507. https://doi.org/10.1126/science. 1127647</li>
<li>Heller S, McNaught A, Stein S et al (2013) InChI - the worldwide chemical structure identifier standard. J Cheminform 5:7. https://doi. org/10.1186/1758-2946-5-7</li>
<li>Winter R, Montanari F, Noé F, Clevert D-A (2019) Learning continuous and data-driven molecular descriptors by translating equivalent chemical representations. Chem Sci 10:1692-1701. https://doi.org/10.1039/c8sc0 4175j</li>
<li>Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9:1735-1780. https://doi.org/10.1162/neco.1997.9.8.1735</li>
<li>Schwaller P et al (2019) Molecular transformer: A model for uncertaintycalibrated chemical reaction prediction. ACS Cent Sci 5:1572-1583. https ://doi.org/10.1021/acscentsci.9b00576</li>
<li>Karpov P, Godin G, Tetko IV. A transformer model for retrosynthesis. In: Tetko IV, Theis F, Karpov P, Kurkova V (ed) 28th International Conference on artificial neural networks, Munich, Germany, September 17-19, 2019 Proceedings, Part V, Workshop and Special sessions. Springer</li>
<li>Samek W, Müller K-R (2019) Towards explainable artificial intelligence. In: Samek W, Montavon G, Vedaldi A, et al. (eds) Explainable AI: interpreting, explaining and visualizing deep learning. Springer International Publishing, Cham, pp 5-22</li>
<li>Montavon G, Binder A, Lapuschkin S et al (2019) Layer-wise relevance propagation: an overview. In: Samek W, Montavon G, Vedaldi A, et al. (eds) Explainable AI: interpreting, explaining and visualizing deep learning. Springer International Publishing, Cham, pp 193-209</li>
<li>Tetko IV, Villa AE, Livingstone DJ (1996) Neural network studies. 2. Variable selection. J Chem Inf Comput Sci 36:794-803. https://doi.org/10.1021/ ci950204c</li>
<li>Gaulton A, Bellis LJ, Bento AP et al (2012) ChEMBL: a large-scale bioactivity database for drug discovery. Nucleic Acids Res 40:D1100-D1107. https:// doi.org/10.1093/nar/gkr777</li>
<li>Segler MHS, Kogej T, Tyrchan C, Waller MP (2017) Generating focussed molecule libraries for drug discovery with recurrent neural networks</li>
<li>Gupta A, Müller AT, Husima BJH et al (2018) Generative recurrent networks for de novo drug design. Mol Inform 37:1700111</li>
<li>Rush A (2018) The annotated transformer. In: Proceedings of workshop for NLP open source software (NLP-OSS). pp 52-60</li>
<li>Abadi M, Barham P, Chen J, et al (2016) TensorFlow: a system for largescale machine learning</li>
<li>Landrum G RDKit: Open-source cheminformatics. https://www.rdkit.org</li>
<li>Ramsundar B, Eastman P, Walters P, Pande V (2019) Deep learning for the life sciences: applying deep learning to genomics, microscopy, drug discovery, and more. O'Reilly Media Inc, Sebastopol</li>
<li>Srivastava N, Hinton G, Krizhevsky A et al (2014) Dropout: a simple way to prevent neural networks from overfitting. J Mach Learn Res 15:1929-1958</li>
<li>
<p>Srivastava RK, Greff K, Schmidhuber J (2015) Highway Networks. Paper presented at the Deep Learning Workshop, International Conference on Machine Learning, Lille, France. arXiv:1505.00387</p>
</li>
<li>
<p>Tetko IV, Karpov P, Bruno E, et al (2019) Augmentation Is What You Need!: 28th International Conference on artificial neural networks, Munich, Germany, September 17-19, 2019, Proceedings. In: Tetko IV, Körková V, Karpov P, Theis F (eds) Artificial neural networks and machine learning-ICANN 2019: workshop and special sessions. Springer International Publishing, Cham, pp 831-835</p>
</li>
<li>Draper NR, Smith H (2014) Applied regression analysis. Wiley, New York</li>
<li>Tetko IV, Sushko Y, Novotarskyi S et al (2014) How accurately can we predict the melting points of drug-like compounds? J Chem Inf Model 54:3320-3329. https://doi.org/10.1021/cI5005288</li>
<li>Wu Z, Ramsundar B, Feinberg EN et al (2018) MoleculeNet: a benchmark for molecular machine learning. Chem Sci 9:513-530</li>
<li>Brandmaier S, Sahlin U, Tetko IV, Öberg T (2012) PLS-optimal: a stepwise d-optimal design based on latent variables. J Chem Inf Model 52:975-983</li>
<li>Sushko I, Novotarskyi S, Körner R et al (2010) Applicability domains for classification problems: benchmarking of distance to models for ames mutagenicity set. J Chem Inf Model 50:2094-2111</li>
<li>Tetko IV, Tanchuk VY, Kasheva TN, Villa AEP (2001) Estimation of aqueous solubility of chemical compounds using e-state indices. J Chem Inf Comput Sci 41:1488-1493</li>
<li>Huuskonen JJ, Livingstone DJ, Tetko IV IV (2000) Neural network modeling for estimation of partition coefficient based on atom-type electrotopological state indices. J Chem Inf Comput Sci 40:947-955</li>
<li>Suzuki K, Nakajima H, Saito Y et al (2000) Janus kinase 3 (Jak3) is essential for common cytokine receptor $\gamma$ chain ( $\gamma \mathrm{c}$ )-dependent signaling: comparative analysis of $\gamma \mathrm{c}$, Jak3, and $\gamma \mathrm{c}$ and Jak3 double-deficient mice. Int Immunol 12:123-132</li>
<li>Sutherland JJ, Weaver DF (2004) Three-dimensional quantitative structureactivity and structure-selectivity relationships of dihydrofolate reductase inhibitors. J Comput Aided Mol Des 18:309-331</li>
<li>Vorberg S, Tetko IV (2014) Modeling the biodegradability of chemical compounds using the online chemical modeling environment (DCHEM). Mol Inform 33:73-85. https://doi.org/10.1002/minf.201300030</li>
<li>Novotarskyi S, Abdelaziz A, Sushko Y et al (2016) ToxCast EPA in vitro to in vivo challenge: insight into the rank-I model. Chem Res Toxicol 29:768-775. https://doi.org/10.1021/acs.chemrestox.5b00481</li>
<li>Rybacka A, Rudén C, Tetko IV, Andersson PL (2015) Identifying potential endocrine disruptors among industrial chemicals and their metabolites development and evaluation of in silico tools. Chemosphere 139:372-378</li>
<li>Xia Z, Karpov P, Popowicz G, Tetko IV (2019) Focused library generator: case of Mdmx inhibitors. J Comp Aided Mol Des 1:1</li>
<li>Chang C-C, Lin C-J (2011) LIBSVM: A library for support vector machines. ACM Transact Int Syst Technol 2:27. https://doi.org/10.1145/19611 89.1961199</li>
<li>Breiman L (2001) Random forests. Mach Learn 45:5-32. https://doi. org/10.1023/A:1010933404324</li>
<li>Chen T, Guestrin C (2016) XGBoost: A scalable tree boosting system. arXiv [cs.LG]</li>
<li>Tetko IV (2002) Associative neural network. Neural Process Lett 16:187199. https://doi.org/10.1023/A:1019903710291</li>
<li>Sossnin S, Karlov D, Tetko IV, Fedorov MV (2019) Comparative study of multitask toxicity modeling on a broad chemical space. J Chem Inf Model 59:1062-1072. https://doi.org/10.1021/acs.jcim.8b00685</li>
<li>Arras L, Montavon G, Müller K-R, Samek W (2017) Explaining recurrent neural network predictions in sentiment analysis. Proceedings of the 8th workshop on computational approaches to subjectivity, sentiment and social media analysis</li>
<li>Plošnik A, Vračko M, Dolenc MS (2016) Mutagenic and carcinogenic structural alerts and their mechanisms of action. Arh Hig Rada Toksikol 67:169-182. https://doi.org/10.1515/aiht-2016-67-2801</li>
<li>Xia Z, Karpov P, Popowicz G, Tetko IV (2019) Focused library generator: case of Mdmx inhibitors. J Comput Aided Mol Des. https://doi.org/10.1007/ s10822-019-00242-8</li>
<li>Huuskonen J (2000) Estimation of aqueous solubility for a diverse set of organic compounds based on molecular topology. J Chem Inf Comput Sci 40:773-777. https://doi.org/10.1021/cI9901338</li>
</ol>
<h2>Publisher's Note</h2>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
<h2>Ready to submit your research? Choose BMC and benefit from:</h2>
<ul>
<li>fast, convenient online submission</li>
<li>thorough peer review by experienced researchers in your field</li>
<li>rapid publication on acceptance</li>
<li>support for research data, including large and complex data types</li>
<li>gold Open Access which fosters wider collaboration and increased citations</li>
<li>maximum visibility for your research: over 100M website views per year</li>
</ul>
<p>At BMC, research is always in progress.
Learn more biomedcentral.com/submissions</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Correspondence: pavel.karpov@helmholtz-muenchen.de
${ }^{1}$ Institute of Structural Biology, Helmholtz Zentrum München-Research Center for Environmental Health (GmbH), Ingolstädter Landstraße 1, 85764 Neuherberg, Germany
Full list of author information is available at the end of the article&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>(c) The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativeco mmons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/ zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2321 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2321</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2321</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-269614170</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.04161v2.pdf" target="_blank">Decoding complexity: how machine learning is redefining scientific discovery</a></p>
                <p><strong>Paper Abstract:</strong> As modern scientific instruments generate vast amounts of data and the volume of information in the scientific literature continues to grow, machine learning (ML) has become an essential tool for organising, analysing, and interpreting these complex datasets. This paper explores the transformative role of ML in accelerating breakthroughs across a range of scientific disciplines. By presenting key examples -- such as brain mapping and exoplanet detection -- we demonstrate how ML is reshaping scientific research. We also explore different scenarios where different levels of knowledge of the underlying phenomenon are available, identifying strategies to overcome limitations and unlock the full potential of ML. Despite its advances, the growing reliance on ML poses challenges for research applications and rigorous validation of discoveries. We argue that even with these challenges, ML is poised to disrupt traditional methodologies and advance the boundaries of knowledge by enabling researchers to tackle increasingly complex problems. Thus, the scientific community can move beyond the necessary traditional oversimplifications to embrace the full complexity of natural systems, ultimately paving the way for interdisciplinary breakthroughs and innovative solutions to humanity's most pressing challenges.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2321.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2321.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Exoplanet CNNs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convolutional Neural Networks for Exoplanet Transit Detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convolutional neural networks and image-recognition ML methods trained on telescope time-series/images to detect exoplanet transit signals, including cases with transit timing variations and noisy observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Alleviating the transit timing variation bias in transit surveys -i. rivers: Method and detection of a pair of resonant super-earths around kepler-1705.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>astronomy / exoplanet detection</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Detecting small transit signals of exoplanets in noisy telescope time-series/images, including cases where planetary interactions break simple periodicity (transit timing variations).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>abundant in some survey contexts (large telescope archives), but signals of interest can be rare and buried in noise; labeled examples exist from confirmed transits but class imbalance and TTV (transit-timing-variation) effects reduce usable labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>time-series photometric data and derived images; effectively structured sequences and image-like representations (multichannel), sometimes high-volume and noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high — low signal-to-noise ratio, non-stationary signals due to planetary interactions, long-range temporal dependencies and large search space over orbital parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>mature observational astronomy with established physics for transits, but discovery methods evolving; substantial domain expertise and existing heuristics (periodograms, vetting pipelines).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — predictions useful for candidate detection can be black-box, but scientific confirmation requires interpretable signals and follow-up observations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Convolutional neural networks / image-recognition neural nets</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Supervised deep convolutional architectures trained on many labelled transit/non-transit examples (including synthetic injections) to learn subtle patterns of transit signatures and to be robust to timing variations; models incorporate data augmentation to handle noise and non-periodicity.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>applicable and appropriate — CNNs can process large telescope datasets and detect transits that traditional periodicity-based algorithms miss, especially when trained on varied examples including TTV cases; limitations include class imbalance and need for follow-up validation.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively effective: the paper reports CNN/image-recognition approaches have higher sensitivity to subtle exoplanet signals with complicating effects (e.g., transit timing variations) and helped discover Kepler-1705b and Kepler-1705c where traditional methods struggled.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — improves detection sensitivity, enables discovery of planets missed by classical pipelines, scales to large survey data and accelerates exoplanet cataloguing and population studies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively against traditional periodicity-based transit search methods; ML can detect signals missed by standard methods in the presence of TTVs, but follow-up still required to confirm astrophysical nature.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large survey datasets for training, careful simulation/augmentation of TTV cases, CNNs' ability to learn invariant features from noisy photometry, and collaboration between domain experts and ML practitioners.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Deep CNNs trained on large, diverse examples can outperform traditional periodicity-based detectors for noisy, non-periodic transit signals, but require careful training data design and subsequent physical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decoding complexity: how machine learning is redefining scientific discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2321.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2321.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Brain mapping AI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large-scale AI processing for 3D brain tissue reconstruction (Google / Harvard study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AI pipelines (deep learning / computer vision) used to assemble and annotate hundreds of millions of brain images into high-resolution 3D reconstructions to study brain structure and disease.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>neuroscience / connectomics / brain mapping</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Reconstructing and annotating millions of brain-slice images into accurate three-dimensional maps to reveal structure at scale and support studies of neurological disorders.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>very large (hundreds of millions of images reported), but highly heterogeneous; dataset in example (300 million images) indicates abundant raw data though processing/annotation is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>very high-resolution 2D microscopy images (image stacks) that must be registered into 3D volumes; multimodal when combined with functional recordings or metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>very high — extremely high dimensionality, complex image registration, segmentation and tracing tasks, and heterogeneity across imaging conditions; requires long-range spatial consistency across scales.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>emerging-to-maturing — imaging hardware mature; large-scale connectomics and computational reconstruction are developing rapidly with growing community standards but still many open challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high for scientific insight — although black-box segmentation can produce datasets, understanding neuronal circuits and disease mechanisms requires interpretable reconstructions and traceable annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep computer-vision pipelines (convolutional networks for segmentation, registration, and 3D reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Massive supervised/semisupervised image-processing pipelines using convolutional nets for segmentation and feature extraction combined with stitching/registration algorithms to assemble 2D slices into 3D volumes, often using large compute infrastructure.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / computer vision / large-scale deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable — AI enables automation at scales impossible for humans; pipeline required large compute and robust training data/labels; limitations include annotation quality, domain shifts across datasets, and explainability of errors.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively transformative: enabled creation of the largest-ever interactive 3D brain tissue model from 300 million images, facilitating new analyses and hypothesis generation in neuroscience.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Very high — enables large-scale structural brain atlases, supports studies of neurological disease, and accelerates discovery by turning raw image mass into analyzable 3D datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared implicitly to manual annotation and smaller-scale pipelines; AI scales far beyond human capabilities, though final scientific validation still requires expert inspection and downstream experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of massive imaging datasets, high-performance computing, robust neural image-segmentation models, and integration with domain knowledge for registration and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>When very large, structured imaging datasets exist, deep vision pipelines can create unprecedented 3D reconstructions, unlocking new science but demanding rigorous validation and interpretability for mechanistic insight.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decoding complexity: how machine learning is redefining scientific discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2321.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2321.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Halicin discovery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep learning-driven antibiotic discovery (halicin)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep-learning model used to screen chemical space and identify a novel antibiotic compound (halicin) effective against resistant bacteria, demonstrating ML's ability to discover molecules outside traditional heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A deep learning approach to antibiotic discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>drug discovery / antibiotic discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Rapid identification of novel antibiotic candidates from chemical libraries to address resistant bacteria, reducing time and cost relative to traditional screening.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>moderate to large chemical and biological assay datasets; labeled activity data exist but effective positive examples (true antibiotics) are rare, producing class imbalance.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>structured chemical representations (molecular fingerprints, graphs, SMILES) and associated assay labels; can be multimodal if combined with biological assay metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high — vast chemical search space (combinatorial), highly nonlinear structure-activity relationships, and multiple objectives (efficacy, toxicity, ADMET).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>mature field (pharmaceutical sciences) with substantial prior knowledge, but discovery of novel chemical classes remains hard; ML is a relatively recent but growing tool.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high for translation — while black-box predictions can prioritize candidates, mechanistic insight and interpretability are required for downstream validation, regulatory approval, and understanding off-target effects.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep learning classifiers / generative models for chemical screening</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Supervised deep-learning models trained on labeled molecule–activity datasets to predict antibacterial activity and guide virtual screening; may include explainability modules to interpret structure–activity associations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / generative models (in some workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and impactful — ML enabled rapid identification of novel scaffolds (e.g., halicin) that traditional heuristics missed; limited by availability of labeled activity data and need for experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Successful qualitatively: produced at least one novel antibiotic candidate with activity against resistant bacteria, demonstrating ability to generalize beyond training biases and prioritize unusual chemistries.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — can greatly accelerate early-stage hit identification, reduce experimental costs, and explore chemical regions inaccessible to rule-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to traditional high-throughput screening and heuristic medicinal-chemistry rules; ML provided more efficient prioritization and discovered compounds outside prior chemical intuition.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Rich chemical / bioactivity datasets, molecular representations amenable to learning, careful model validation, and subsequent wet-lab testing to confirm in-silico predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Deep learning can uncover novel bioactive chemotypes from large chemical datasets and thus accelerate discovery, but success depends on quality of bioactivity labels and experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decoding complexity: how machine learning is redefining scientific discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2321.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2321.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold: deep-learning protein structure prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep-learning model that predicts 3D protein structures from amino-acid sequences using multiple-sequence-alignment inductive biases and geometric equivariance, delivering high-accuracy structure predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Highly accurate protein structure prediction with alphafold.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>structural biology / protein folding</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predicting the three-dimensional native structure of proteins from their primary (amino-acid) sequences to accelerate biological insight and design.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>large curated sequence and structural databases (e.g., PDB, MSAs) used for training; high-quality labeled structures available but limited relative to sequence diversity; MSA data availability varies across families.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>structured biological sequence data (1D) with associated 3D coordinate labels; multiple-sequence alignments provide evolutionary covariation matrices; data multimodal combining sequences and structural geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>very high — large combinatorial sequence space, complex long-range interactions determining tertiary structure, and high-dimensional geometric output.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>mature biophysics with many experimental structures known; computational prediction long-standing challenge but domain has strong prior knowledge and benchmarks (CASP).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high for scientific interpretation but lower for many practical uses — accurate predictions are useful even if internal model reasoning is opaque, yet understanding folding mechanisms remains scientifically important.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Transformer-based deep learning with MSA and 3D-equivariant components</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Large supervised deep models combining sequence attention (transformer-like blocks), multiple-sequence-alignment-derived features, and geometric modules enforcing 3D equivariance to predict backbone and sidechain coordinates; trained on structural databases with loss terms for coordinate accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / foundation-scale deep learning (domain-specific)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable and appropriate — model significantly improved accuracy of structure prediction across many protein families; limitations include reliance on MSA quality and reduced performance for sequences with scarce evolutionary information, prompting single-sequence methods.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Transformative qualitatively: AlphaFold achieved highly accurate structure prediction in many cases, spurring widespread uptake and follow-on methods (ESMfold, generative design).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Very high — accelerates structural biology, aids drug design and protein engineering, and reduces need for some experimental structure determination.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared implicitly to physics-based simulations and earlier ML methods; outperforms previous approaches on CASP benchmarks and in practical accuracy, though physics-based insight still valuable for dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large curated structural databases, effective use of evolutionary information (MSA), model architectures enforcing geometric priors (equivariance), and massive compute for training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Embedding domain inductive biases (MSA, geometry) into large deep models allows accurate predictions in high-complexity biological problems, giving practical utility even when mechanistic interpretability is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decoding complexity: how machine learning is redefining scientific discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2321.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2321.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRL control</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Reinforcement Learning for control of complex physical systems (tokamaks, turbulence, quantum states)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deep reinforcement-learning approaches coupled with physics simulators have been used to discover novel control strategies for complex dynamical systems including tokamak plasmas, turbulence control, and quantum manipulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Magnetic control of tokamak plasmas through deep reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>control theory / plasma physics / fluid mechanics / quantum control</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Designing control policies for high-dimensional, nonlinear, and often chaotic physical systems to stabilize or steer them to desired regimes (e.g., plasma confinement, turbulence suppression).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>depends on simulator fidelity and experimental access; often limited labelled datasets but simulators provide abundant synthetic trajectories for training; real-experiment data may be expensive and noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>time-series state-action trajectories, high-dimensional sensor arrays, and simulator state snapshots; sequential and temporally dependent data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>very high — nonlinear, multiscale dynamics, partial observability, high-dimensional continuous state and action spaces, and long time horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>applied-control and physics domains are mature, but applying DRL to real-world experimental control is emerging with increasing demonstrations; extensive domain expertise required.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium to high — practical control can accept black-box policies if safe and stable, but for scientific understanding and safety, interpretability and verification are important.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep Reinforcement Learning (policy/value networks, sometimes model-based hybrids)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Agent-based learning where deep neural-network policies are trained via reward signals by interacting with high-fidelity simulators (and sometimes experiments); may use model-based rollouts, curriculum learning, and reward shaping to discover stable control strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>reinforcement learning / model-based RL (in some works)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and promising — DRL has unveiled novel control regimes and policies not found by classical controllers; limitations include sim-to-real transfer, sample complexity, and safety constraints during deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively effective: cited successes include magnetic control of tokamak plasmas and discovery of previously unknown thermodynamic cycles, demonstrating DRL's ability to find non-intuitive control strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — potential to automate and optimize control in complex experimental facilities, reduce manual tuning, and accelerate discovery of physical regimes unreachable by heuristic control.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively to classical control and optimization approaches; DRL can discover novel, high-performing policies but requires careful simulator alignment and safety mechanisms for real-world use.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of accurate simulators for training, well-designed reward functions, inductive biases or constraints to preserve physical plausibility, and integration with domain knowledge for safe deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Coupling DRL with physics simulators allows discovery of novel control strategies in complex dynamical systems, but success depends on simulator fidelity, reward design, and methods to ensure safe sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decoding complexity: how machine learning is redefining scientific discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2321.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2321.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fast generative sims</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative models (GANs, VAEs) for fast simulation in high-energy physics and other domains</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative adversarial networks and variational autoencoders are used to produce fast approximations of expensive physics simulations (e.g., particle-detector response), massively reducing compute while aiming to retain required accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative adversarial networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>high-energy physics simulation / computational physics</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Replace or accelerate costly, high-fidelity forward simulations (particle showers, detector responses) with fast learned generative surrogates to enable scalable data analysis and experiment design.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>training data typically abundant from existing high-fidelity simulators (synthetic labeled data), though generating extremely large training sets can be costly; real experimental data may be scarcer for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>high-dimensional continuous fields/images (detector hits, calorimeter maps) and structured simulation outputs; often spatially correlated arrays.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high — complex, stochastic, multiscale physics to reproduce; requirements for statistical fidelity across many observables and rare-event tails.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>emerging within HEP and other fields; physics-based simulators are mature but learned surrogates are actively researched to meet strict accuracy needs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — surrogate speed is valuable, but scientific usage requires that generated distributions match physics observables and uncertainties (interpretability of failure modes important).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>GANs and VAEs (deep generative models)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Unsupervised/implicit generative training where networks learn to map low-dimensional latent variables to realistic high-dimensional simulation outputs, trained with adversarial or variational objectives; efforts focus on preserving statistical fidelity and uncertainty calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>unsupervised / generative modeling</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and promising as a speedup strategy; applicability limited by current gaps between generative accuracy and strict physics-quality requirements, particularly for rare-event statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively promising: generative models provide large speedups in sample generation compared to full simulators, but ongoing research is needed to ensure required accuracy for scientific analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High if fidelity requirements are met — could reduce compute costs by orders of magnitude, enable larger-scale inference and parameter sweeps, and democratize access to simulation capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to full first-principles simulators: generative models are orders-of-magnitude faster but currently trade off exact fidelity; comparisons motivate hybrid approaches and calibration strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of large simulated training sets, architecture choices capturing spatial structure, explicit constraints or physics-informed losses, and rigorous validation against target observables.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Deep generative models can accelerate expensive scientific simulations dramatically, but meeting the strict fidelity and uncertainty demands of scientific applications requires hybrid designs and careful validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decoding complexity: how machine learning is redefining scientific discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2321.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2321.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SINDy & symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse identification of nonlinear dynamics (SINDy) and symbolic regression for discovering governing equations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sparse regression and symbolic methods (SINDy, genetic algorithms, symbolic regression) are used to extract interpretable closed-form dynamical models or PDEs from measurement data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discovering governing equations from data by sparse identification of nonlinear dynamical systems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>applied mathematics / dynamical systems / physics</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Infer low-dimensional, interpretable ordinary or partial differential equations that capture the dominant dynamics from noisy observational data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>varies — often limited to observed trajectories or experimental readouts; interventional data can greatly aid identification, and noise levels constrain identifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>time-series measurements of observables; may be multivariate and irregularly sampled; low-dimensional latent dynamics sought.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>medium to high — nonlinear dynamics, potential high-dimensional underlying systems, sensitivity to noise, and large candidate-function libraries increase search complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>methodologically mature in research but active area of development; domain knowledge often available to guide dictionary choices and regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high — primary goal is interpretable, mechanistic models for scientific understanding; sparse symbolic forms are preferred over black-box predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Sparse regression (SINDy), symbolic regression, genetic algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Constructs a library of candidate functions and uses sparsity-promoting regression (L1, sequential thresholding) or evolutionary symbolic-search to select parsimonious equations that match observed dynamics; may include constraints for physical invariances.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>symbolic / sparse / interpretable ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable when observables capture underlying dynamics and noise is manageable; struggles with unobserved variables, heavy noise, or extremely high-dimensional systems without good representations.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Effective for recovering simple-to-moderate-order governing equations and for providing interpretable models, but performance degrades with noise, hidden variables, and very large search spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for theory-building — can convert data into closed-form scientific laws, facilitate extrapolation and control, and aid mechanistic understanding when conditions for identifiability are met.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to black-box neural models: symbolic methods yield interpretable equations but may be less flexible in highly complex settings; hybrid approaches combining representation learning with symbolic regression are promising.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Good-quality time-series with sufficient excitation, appropriate candidate-function libraries, sparsity regularization, and incorporation of physical priors (symmetries, conservation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Symbolic sparse-identification methods excel at yielding interpretable mechanistic models from moderate-noise, well-excited dynamical data, but require careful data collection and priors to handle real-world complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decoding complexity: how machine learning is redefining scientific discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2321.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2321.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Foundation models / LLMs / SFMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific Foundation Models and Large Language Models (LLMs) for cross-domain scientific tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large pre-trained generative models (foundation models, LLMs) trained on massive datasets that exhibit emergent abilities (e.g., in-context learning) and are applied to literature synthesis, hypothesis generation, protein structure/evolution prediction, and experiment guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emergent abilities of large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>cross-cutting — computational biology (protein design), climate science, literature synthesis, experiment design</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Leverage large pre-trained models to generalize to a variety of downstream scientific tasks (summarization, idea generation, structure prediction, experiment guidance) especially when labeled data are scarce for specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>foundation models are trained on massive heterogeneous corpora (text, sequences, simulation outputs); downstream tasks may have scarce labeled data but benefit from pretraining; concerns about biases in training data noted.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>multimodal (text, sequences, structures), often unstructured text for LLMs or sequence data for genome-scale LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high — tasks vary widely, often require multi-step reasoning, domain knowledge, and grounding; emergent behaviors make predictive scaling challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>emerging rapidly — LLMs/foundation models are recent and being adapted to scientific domains; domain expertise integration is active research.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>variable — some use-cases accept black-box assistance (idea generation), while others (experimental guidance, clinical) require interpretability and reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Foundation models / Large language models / Genome-scale language models</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Large-scale transformer-based architectures pre-trained with self-supervised objectives (masked prediction/next-token) on massive datasets; transfer to downstream tasks via fine-tuning, few-shot prompting, or coupling with tools and RL for experiment planning.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>foundation models / self-supervised learning / few-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable as broad assistive tools, particularly when labeled downstream data are scarce; limitations include biases, unpredictable emergent failures, and opacity of reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Powerful qualitatively: enable zero-shot/few-shot generalization, accelerate literature synthesis and hypothesis generation, and have enabled advances in protein prediction and design; risks include bias and limited scientific interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Very high — could transform many scientific workflows by automating literature review, proposing experiments, and seeding hypotheses, but scientific validation and oversight are essential.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to task-specific models: foundation models generalize broadly and reduce need for labeled data, but task-specific architectures often yield better calibrated predictive performance; hybrid approaches are promising.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Scale of pretraining data and model, alignment with scientific objectives (fine-tuning, tool use), and integration with domain knowledge and evaluation pipelines to detect and mitigate hallucinations and bias.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Large pre-trained foundation models provide powerful transfer ability for diverse scientific problems, especially when labeled data are scarce, but must be used with careful validation and domain constraints due to biases and opaque failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decoding complexity: how machine learning is redefining scientific discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2321.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2321.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Hilbert / AI-Descartes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated symbolic/scientific-discovery systems (AI-Hilbert, AI-Descartes, Cornelio et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hybrid AI systems that combine axiomatic/theoretical knowledge with data-driven methods and symbolic regression to generate interpretable mathematical models and rediscover known laws or propose new consistent models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evolving scientific discovery by unifying data and background knowledge with ai hilbert.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>scientific-method automation / symbolic model discovery (general across physics/astronomy/etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automate hypothesis generation and derivation of mathematical models consistent with both data and axiomatic background knowledge to accelerate discovery and reconcile conflicting data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>varies — systems can operate with moderate datasets augmented by background axioms; some tools are designed to work with limited data by leveraging strong theoretical priors.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>structured experimental measurements and formalized background knowledge (axioms, symbolic constraints); data can be numeric time-series or tabulated measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>medium to high — search spaces for symbolic expressions grow combinatorially; integrating background theory reduces complexity but still requires efficient search and selection strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>research-stage — symbolic discovery systems have shown rediscovery of laws (Kepler's third law, time dilation) but are not yet ubiquitous for open-ended discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>very high — core goal is interpretable mechanistic models and formal derivations; transparency and symbolic clarity are essential.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Hybrid symbolic-AI with logical reasoning and symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Frameworks that integrate logical axioms/constraints with symbolic-search or regression over candidate formula spaces, using data likelihood and axiomatic consistency scoring to prefer derivable, interpretable models; often employ pruning and competition among formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>hybrid / symbolic / knowledge-based</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable for problems where background theory can constrain the search and where interpretability is the objective; less suited for massively high-dimensional, purely empirical pattern discovery without formal priors.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Effective for rediscovering known physical laws even with limited data and for distinguishing competing formulae; tends to confirm established theories more than produce radically novel laws so far.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Significant for theory formation and formalization — can accelerate generation of candidate symbolic laws and help reconcile data with theory, boosting reproducibility and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to purely data-driven black-box models: hybrid symbolic methods produce interpretable formulas and can work with less data by leveraging theory, but may miss complex empirical patterns that lack compact symbolic forms.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of formalizable background knowledge, moderate-quality measurement data, efficient symbolic search/pruning algorithms, and integration of logical constraints to guide discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Combining axiomatic knowledge with data-driven symbolic search enables derivable, interpretable scientific models and can rediscover known laws from limited data, but current systems are more confirmatory than creatively novel.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decoding complexity: how machine learning is redefining scientific discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2321.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2321.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPOCK classifier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPOCK supervised classifier for long-term stability prediction in multi-planet systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised ML classifier (SPOCK) trained on short-term simulations to predict long-term orbital stability of compact multi-planet systems, enabling fast generalization and reducing expensive long integrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Predicting the long-term stability of compact multiplanet systems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>astrophysics / orbital dynamics / multi-planet stability</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predicting whether a given compact multi-planet configuration will be stable over astronomical timescales without performing computationally prohibitive long-term N-body integrations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>synthetic simulation data abundant from short-term integrations; labeled outcomes (stable/unstable over long time) obtainable via expensive long-term runs but can be generated for training via proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>structured numerical features derived from short-term orbital simulations (orbital elements, stability indicators); tabular/feature-vector representation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high — chaotic N-body dynamics, sensitivity to initial conditions, multiscale interactions and the need to extrapolate from short-term behavior to long-term stability.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>well-established dynamical astronomy with strong theoretical foundations, but long-term stability prediction remains computationally challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — fast ML predictions are useful for screening, but physical validation (long integrations, theory) is needed to confirm stability for scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Supervised classifier (SPOCK)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Supervised ML trained on features from short-time simulations to predict long-term stability labels; leverages generalization to extrapolate to larger systems and avoid long integrations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / classification</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and effective as a screening tool to replace many long integrations with fast predictions; limitations include edge cases and need for care when extrapolating beyond training distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively effective: SPOCK can generalize from short to long timescales and predict stability, enabling practical screening of system architectures and guiding further detailed simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate-to-high — reduces computational cost for large parameter sweeps in planetary dynamics and aids design/interpretation of exoplanetary system studies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Alternative is direct long-term N-body integration (accurate but costly); SPOCK trades exactness for huge computational savings and is used as a filter for costly verification.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Ability to generate representative short-term simulation features, careful feature engineering, and a training dataset that captures relevant dynamical regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Supervised classifiers trained on short-term simulations can accurately predict many long-term stability outcomes, providing a scalable surrogate for expensive integrations while requiring careful validation for edge cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decoding complexity: how machine learning is redefining scientific discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2321.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2321.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diffusion models in proteins</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diffusion and generative models for protein backbone generation and molecular ensembles</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Diffusion-based generative models are used to directly generate protein backbones and molecular ensembles, offering routes to bypass expensive physical simulations for design and sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>protein design / structural biology / molecular modeling</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate plausible protein backbone conformations and ensembles for design and sampling without running computationally expensive molecular-dynamics simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>moderate to large structural datasets (predicted/experimental structures) used for training; coverage varies across protein families.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>3D coordinate data (point-clouds or graphs) representing backbones and side-chains; high-dimensional geometric structures.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high — complex spatial constraints, sequence-structure coupling, and need to respect physical plausibility across conformational ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>emerging — generative modeling for proteins is an active frontier with rapid progress but still maturing evaluation and validation practices.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high for downstream biological application — generated structures require validation (stability, function) and mechanistic insight for design acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Diffusion probabilistic generative models</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Score-based/diffusion generative models trained to reverse a noise process on protein coordinate representations to sample realistic backbone structures or ensembles; training leverages structural datasets and geometric priors.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>unsupervised / generative modeling / diffusion models</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and promising to accelerate design workflows and sampling, but applicability depends on training coverage and downstream validation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively promising: diffusion models can generate state-of-the-art backbones and ensembles and have been used in de novo design pipelines (e.g., RFdiffusion), but experimental validation remains crucial.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — could shorten design cycles, provide diverse candidate structures, and reduce reliance on expensive physics simulations when combined with downstream verification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to molecular dynamics and physics-based sampling: diffusion models produce diverse samples faster but must be checked for physical fidelity; hybrid approaches can combine strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of high-quality structural datasets, incorporation of geometric invariances, and integration with sequence design and experimental testing.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Diffusion generative models can efficiently produce realistic protein backbones and ensembles, offering scalable alternatives to simulations when trained on sufficient structural data and validated experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decoding complexity: how machine learning is redefining scientific discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A deep learning approach to antibiotic discovery. <em>(Rating: 2)</em></li>
                <li>Highly accurate protein structure prediction with alphafold. <em>(Rating: 2)</em></li>
                <li>Magnetic control of tokamak plasmas through deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Predicting the long-term stability of compact multiplanet systems. <em>(Rating: 2)</em></li>
                <li>Discovering governing equations from data by sparse identification of nonlinear dynamical systems. <em>(Rating: 2)</em></li>
                <li>Generative adversarial networks. <em>(Rating: 2)</em></li>
                <li>Advancing mathematics by guiding human intuition with ai. <em>(Rating: 1)</em></li>
                <li>Evolving scientific discovery by unifying data and background knowledge with ai hilbert. <em>(Rating: 2)</em></li>
                <li>Emergent abilities of large language models. <em>(Rating: 1)</em></li>
                <li>Alleviating the transit timing variation bias in transit surveys -i. rivers: Method and detection of a pair of resonant super-earths around kepler-1705. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2321",
    "paper_id": "paper-269614170",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "Exoplanet CNNs",
            "name_full": "Convolutional Neural Networks for Exoplanet Transit Detection",
            "brief_description": "Convolutional neural networks and image-recognition ML methods trained on telescope time-series/images to detect exoplanet transit signals, including cases with transit timing variations and noisy observations.",
            "citation_title": "Alleviating the transit timing variation bias in transit surveys -i. rivers: Method and detection of a pair of resonant super-earths around kepler-1705.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "astronomy / exoplanet detection",
            "problem_description": "Detecting small transit signals of exoplanets in noisy telescope time-series/images, including cases where planetary interactions break simple periodicity (transit timing variations).",
            "data_availability": "abundant in some survey contexts (large telescope archives), but signals of interest can be rare and buried in noise; labeled examples exist from confirmed transits but class imbalance and TTV (transit-timing-variation) effects reduce usable labeled data.",
            "data_structure": "time-series photometric data and derived images; effectively structured sequences and image-like representations (multichannel), sometimes high-volume and noisy.",
            "problem_complexity": "high — low signal-to-noise ratio, non-stationary signals due to planetary interactions, long-range temporal dependencies and large search space over orbital parameters.",
            "domain_maturity": "mature observational astronomy with established physics for transits, but discovery methods evolving; substantial domain expertise and existing heuristics (periodograms, vetting pipelines).",
            "mechanistic_understanding_requirements": "medium — predictions useful for candidate detection can be black-box, but scientific confirmation requires interpretable signals and follow-up observations.",
            "ai_methodology_name": "Convolutional neural networks / image-recognition neural nets",
            "ai_methodology_description": "Supervised deep convolutional architectures trained on many labelled transit/non-transit examples (including synthetic injections) to learn subtle patterns of transit signatures and to be robust to timing variations; models incorporate data augmentation to handle noise and non-periodicity.",
            "ai_methodology_category": "supervised learning / deep learning",
            "applicability": "applicable and appropriate — CNNs can process large telescope datasets and detect transits that traditional periodicity-based algorithms miss, especially when trained on varied examples including TTV cases; limitations include class imbalance and need for follow-up validation.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively effective: the paper reports CNN/image-recognition approaches have higher sensitivity to subtle exoplanet signals with complicating effects (e.g., transit timing variations) and helped discover Kepler-1705b and Kepler-1705c where traditional methods struggled.",
            "impact_potential": "High — improves detection sensitivity, enables discovery of planets missed by classical pipelines, scales to large survey data and accelerates exoplanet cataloguing and population studies.",
            "comparison_to_alternatives": "Compared qualitatively against traditional periodicity-based transit search methods; ML can detect signals missed by standard methods in the presence of TTVs, but follow-up still required to confirm astrophysical nature.",
            "success_factors": "Large survey datasets for training, careful simulation/augmentation of TTV cases, CNNs' ability to learn invariant features from noisy photometry, and collaboration between domain experts and ML practitioners.",
            "key_insight": "Deep CNNs trained on large, diverse examples can outperform traditional periodicity-based detectors for noisy, non-periodic transit signals, but require careful training data design and subsequent physical validation.",
            "uuid": "e2321.0",
            "source_info": {
                "paper_title": "Decoding complexity: how machine learning is redefining scientific discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Brain mapping AI",
            "name_full": "Large-scale AI processing for 3D brain tissue reconstruction (Google / Harvard study)",
            "brief_description": "AI pipelines (deep learning / computer vision) used to assemble and annotate hundreds of millions of brain images into high-resolution 3D reconstructions to study brain structure and disease.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "neuroscience / connectomics / brain mapping",
            "problem_description": "Reconstructing and annotating millions of brain-slice images into accurate three-dimensional maps to reveal structure at scale and support studies of neurological disorders.",
            "data_availability": "very large (hundreds of millions of images reported), but highly heterogeneous; dataset in example (300 million images) indicates abundant raw data though processing/annotation is challenging.",
            "data_structure": "very high-resolution 2D microscopy images (image stacks) that must be registered into 3D volumes; multimodal when combined with functional recordings or metadata.",
            "problem_complexity": "very high — extremely high dimensionality, complex image registration, segmentation and tracing tasks, and heterogeneity across imaging conditions; requires long-range spatial consistency across scales.",
            "domain_maturity": "emerging-to-maturing — imaging hardware mature; large-scale connectomics and computational reconstruction are developing rapidly with growing community standards but still many open challenges.",
            "mechanistic_understanding_requirements": "high for scientific insight — although black-box segmentation can produce datasets, understanding neuronal circuits and disease mechanisms requires interpretable reconstructions and traceable annotation.",
            "ai_methodology_name": "Deep computer-vision pipelines (convolutional networks for segmentation, registration, and 3D reconstruction)",
            "ai_methodology_description": "Massive supervised/semisupervised image-processing pipelines using convolutional nets for segmentation and feature extraction combined with stitching/registration algorithms to assemble 2D slices into 3D volumes, often using large compute infrastructure.",
            "ai_methodology_category": "supervised learning / computer vision / large-scale deep learning",
            "applicability": "Highly applicable — AI enables automation at scales impossible for humans; pipeline required large compute and robust training data/labels; limitations include annotation quality, domain shifts across datasets, and explainability of errors.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively transformative: enabled creation of the largest-ever interactive 3D brain tissue model from 300 million images, facilitating new analyses and hypothesis generation in neuroscience.",
            "impact_potential": "Very high — enables large-scale structural brain atlases, supports studies of neurological disease, and accelerates discovery by turning raw image mass into analyzable 3D datasets.",
            "comparison_to_alternatives": "Compared implicitly to manual annotation and smaller-scale pipelines; AI scales far beyond human capabilities, though final scientific validation still requires expert inspection and downstream experiments.",
            "success_factors": "Availability of massive imaging datasets, high-performance computing, robust neural image-segmentation models, and integration with domain knowledge for registration and validation.",
            "key_insight": "When very large, structured imaging datasets exist, deep vision pipelines can create unprecedented 3D reconstructions, unlocking new science but demanding rigorous validation and interpretability for mechanistic insight.",
            "uuid": "e2321.1",
            "source_info": {
                "paper_title": "Decoding complexity: how machine learning is redefining scientific discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Halicin discovery",
            "name_full": "Deep learning-driven antibiotic discovery (halicin)",
            "brief_description": "A deep-learning model used to screen chemical space and identify a novel antibiotic compound (halicin) effective against resistant bacteria, demonstrating ML's ability to discover molecules outside traditional heuristics.",
            "citation_title": "A deep learning approach to antibiotic discovery.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "drug discovery / antibiotic discovery",
            "problem_description": "Rapid identification of novel antibiotic candidates from chemical libraries to address resistant bacteria, reducing time and cost relative to traditional screening.",
            "data_availability": "moderate to large chemical and biological assay datasets; labeled activity data exist but effective positive examples (true antibiotics) are rare, producing class imbalance.",
            "data_structure": "structured chemical representations (molecular fingerprints, graphs, SMILES) and associated assay labels; can be multimodal if combined with biological assay metadata.",
            "problem_complexity": "high — vast chemical search space (combinatorial), highly nonlinear structure-activity relationships, and multiple objectives (efficacy, toxicity, ADMET).",
            "domain_maturity": "mature field (pharmaceutical sciences) with substantial prior knowledge, but discovery of novel chemical classes remains hard; ML is a relatively recent but growing tool.",
            "mechanistic_understanding_requirements": "high for translation — while black-box predictions can prioritize candidates, mechanistic insight and interpretability are required for downstream validation, regulatory approval, and understanding off-target effects.",
            "ai_methodology_name": "Deep learning classifiers / generative models for chemical screening",
            "ai_methodology_description": "Supervised deep-learning models trained on labeled molecule–activity datasets to predict antibacterial activity and guide virtual screening; may include explainability modules to interpret structure–activity associations.",
            "ai_methodology_category": "supervised learning / generative models (in some workflows)",
            "applicability": "Applicable and impactful — ML enabled rapid identification of novel scaffolds (e.g., halicin) that traditional heuristics missed; limited by availability of labeled activity data and need for experimental validation.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Successful qualitatively: produced at least one novel antibiotic candidate with activity against resistant bacteria, demonstrating ability to generalize beyond training biases and prioritize unusual chemistries.",
            "impact_potential": "High — can greatly accelerate early-stage hit identification, reduce experimental costs, and explore chemical regions inaccessible to rule-based methods.",
            "comparison_to_alternatives": "Compared to traditional high-throughput screening and heuristic medicinal-chemistry rules; ML provided more efficient prioritization and discovered compounds outside prior chemical intuition.",
            "success_factors": "Rich chemical / bioactivity datasets, molecular representations amenable to learning, careful model validation, and subsequent wet-lab testing to confirm in-silico predictions.",
            "key_insight": "Deep learning can uncover novel bioactive chemotypes from large chemical datasets and thus accelerate discovery, but success depends on quality of bioactivity labels and experimental validation.",
            "uuid": "e2321.2",
            "source_info": {
                "paper_title": "Decoding complexity: how machine learning is redefining scientific discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "AlphaFold",
            "name_full": "AlphaFold: deep-learning protein structure prediction",
            "brief_description": "A deep-learning model that predicts 3D protein structures from amino-acid sequences using multiple-sequence-alignment inductive biases and geometric equivariance, delivering high-accuracy structure predictions.",
            "citation_title": "Highly accurate protein structure prediction with alphafold.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "structural biology / protein folding",
            "problem_description": "Predicting the three-dimensional native structure of proteins from their primary (amino-acid) sequences to accelerate biological insight and design.",
            "data_availability": "large curated sequence and structural databases (e.g., PDB, MSAs) used for training; high-quality labeled structures available but limited relative to sequence diversity; MSA data availability varies across families.",
            "data_structure": "structured biological sequence data (1D) with associated 3D coordinate labels; multiple-sequence alignments provide evolutionary covariation matrices; data multimodal combining sequences and structural geometry.",
            "problem_complexity": "very high — large combinatorial sequence space, complex long-range interactions determining tertiary structure, and high-dimensional geometric output.",
            "domain_maturity": "mature biophysics with many experimental structures known; computational prediction long-standing challenge but domain has strong prior knowledge and benchmarks (CASP).",
            "mechanistic_understanding_requirements": "high for scientific interpretation but lower for many practical uses — accurate predictions are useful even if internal model reasoning is opaque, yet understanding folding mechanisms remains scientifically important.",
            "ai_methodology_name": "Transformer-based deep learning with MSA and 3D-equivariant components",
            "ai_methodology_description": "Large supervised deep models combining sequence attention (transformer-like blocks), multiple-sequence-alignment-derived features, and geometric modules enforcing 3D equivariance to predict backbone and sidechain coordinates; trained on structural databases with loss terms for coordinate accuracy.",
            "ai_methodology_category": "supervised learning / foundation-scale deep learning (domain-specific)",
            "applicability": "Highly applicable and appropriate — model significantly improved accuracy of structure prediction across many protein families; limitations include reliance on MSA quality and reduced performance for sequences with scarce evolutionary information, prompting single-sequence methods.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Transformative qualitatively: AlphaFold achieved highly accurate structure prediction in many cases, spurring widespread uptake and follow-on methods (ESMfold, generative design).",
            "impact_potential": "Very high — accelerates structural biology, aids drug design and protein engineering, and reduces need for some experimental structure determination.",
            "comparison_to_alternatives": "Compared implicitly to physics-based simulations and earlier ML methods; outperforms previous approaches on CASP benchmarks and in practical accuracy, though physics-based insight still valuable for dynamics.",
            "success_factors": "Large curated structural databases, effective use of evolutionary information (MSA), model architectures enforcing geometric priors (equivariance), and massive compute for training.",
            "key_insight": "Embedding domain inductive biases (MSA, geometry) into large deep models allows accurate predictions in high-complexity biological problems, giving practical utility even when mechanistic interpretability is limited.",
            "uuid": "e2321.3",
            "source_info": {
                "paper_title": "Decoding complexity: how machine learning is redefining scientific discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "DRL control",
            "name_full": "Deep Reinforcement Learning for control of complex physical systems (tokamaks, turbulence, quantum states)",
            "brief_description": "Deep reinforcement-learning approaches coupled with physics simulators have been used to discover novel control strategies for complex dynamical systems including tokamak plasmas, turbulence control, and quantum manipulations.",
            "citation_title": "Magnetic control of tokamak plasmas through deep reinforcement learning.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "control theory / plasma physics / fluid mechanics / quantum control",
            "problem_description": "Designing control policies for high-dimensional, nonlinear, and often chaotic physical systems to stabilize or steer them to desired regimes (e.g., plasma confinement, turbulence suppression).",
            "data_availability": "depends on simulator fidelity and experimental access; often limited labelled datasets but simulators provide abundant synthetic trajectories for training; real-experiment data may be expensive and noisy.",
            "data_structure": "time-series state-action trajectories, high-dimensional sensor arrays, and simulator state snapshots; sequential and temporally dependent data.",
            "problem_complexity": "very high — nonlinear, multiscale dynamics, partial observability, high-dimensional continuous state and action spaces, and long time horizons.",
            "domain_maturity": "applied-control and physics domains are mature, but applying DRL to real-world experimental control is emerging with increasing demonstrations; extensive domain expertise required.",
            "mechanistic_understanding_requirements": "medium to high — practical control can accept black-box policies if safe and stable, but for scientific understanding and safety, interpretability and verification are important.",
            "ai_methodology_name": "Deep Reinforcement Learning (policy/value networks, sometimes model-based hybrids)",
            "ai_methodology_description": "Agent-based learning where deep neural-network policies are trained via reward signals by interacting with high-fidelity simulators (and sometimes experiments); may use model-based rollouts, curriculum learning, and reward shaping to discover stable control strategies.",
            "ai_methodology_category": "reinforcement learning / model-based RL (in some works)",
            "applicability": "Applicable and promising — DRL has unveiled novel control regimes and policies not found by classical controllers; limitations include sim-to-real transfer, sample complexity, and safety constraints during deployment.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively effective: cited successes include magnetic control of tokamak plasmas and discovery of previously unknown thermodynamic cycles, demonstrating DRL's ability to find non-intuitive control strategies.",
            "impact_potential": "High — potential to automate and optimize control in complex experimental facilities, reduce manual tuning, and accelerate discovery of physical regimes unreachable by heuristic control.",
            "comparison_to_alternatives": "Compared qualitatively to classical control and optimization approaches; DRL can discover novel, high-performing policies but requires careful simulator alignment and safety mechanisms for real-world use.",
            "success_factors": "Availability of accurate simulators for training, well-designed reward functions, inductive biases or constraints to preserve physical plausibility, and integration with domain knowledge for safe deployment.",
            "key_insight": "Coupling DRL with physics simulators allows discovery of novel control strategies in complex dynamical systems, but success depends on simulator fidelity, reward design, and methods to ensure safe sim-to-real transfer.",
            "uuid": "e2321.4",
            "source_info": {
                "paper_title": "Decoding complexity: how machine learning is redefining scientific discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Fast generative sims",
            "name_full": "Generative models (GANs, VAEs) for fast simulation in high-energy physics and other domains",
            "brief_description": "Generative adversarial networks and variational autoencoders are used to produce fast approximations of expensive physics simulations (e.g., particle-detector response), massively reducing compute while aiming to retain required accuracy.",
            "citation_title": "Generative adversarial networks.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "high-energy physics simulation / computational physics",
            "problem_description": "Replace or accelerate costly, high-fidelity forward simulations (particle showers, detector responses) with fast learned generative surrogates to enable scalable data analysis and experiment design.",
            "data_availability": "training data typically abundant from existing high-fidelity simulators (synthetic labeled data), though generating extremely large training sets can be costly; real experimental data may be scarcer for fine-tuning.",
            "data_structure": "high-dimensional continuous fields/images (detector hits, calorimeter maps) and structured simulation outputs; often spatially correlated arrays.",
            "problem_complexity": "high — complex, stochastic, multiscale physics to reproduce; requirements for statistical fidelity across many observables and rare-event tails.",
            "domain_maturity": "emerging within HEP and other fields; physics-based simulators are mature but learned surrogates are actively researched to meet strict accuracy needs.",
            "mechanistic_understanding_requirements": "medium — surrogate speed is valuable, but scientific usage requires that generated distributions match physics observables and uncertainties (interpretability of failure modes important).",
            "ai_methodology_name": "GANs and VAEs (deep generative models)",
            "ai_methodology_description": "Unsupervised/implicit generative training where networks learn to map low-dimensional latent variables to realistic high-dimensional simulation outputs, trained with adversarial or variational objectives; efforts focus on preserving statistical fidelity and uncertainty calibration.",
            "ai_methodology_category": "unsupervised / generative modeling",
            "applicability": "Applicable and promising as a speedup strategy; applicability limited by current gaps between generative accuracy and strict physics-quality requirements, particularly for rare-event statistics.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively promising: generative models provide large speedups in sample generation compared to full simulators, but ongoing research is needed to ensure required accuracy for scientific analyses.",
            "impact_potential": "High if fidelity requirements are met — could reduce compute costs by orders of magnitude, enable larger-scale inference and parameter sweeps, and democratize access to simulation capabilities.",
            "comparison_to_alternatives": "Compared to full first-principles simulators: generative models are orders-of-magnitude faster but currently trade off exact fidelity; comparisons motivate hybrid approaches and calibration strategies.",
            "success_factors": "Availability of large simulated training sets, architecture choices capturing spatial structure, explicit constraints or physics-informed losses, and rigorous validation against target observables.",
            "key_insight": "Deep generative models can accelerate expensive scientific simulations dramatically, but meeting the strict fidelity and uncertainty demands of scientific applications requires hybrid designs and careful validation.",
            "uuid": "e2321.5",
            "source_info": {
                "paper_title": "Decoding complexity: how machine learning is redefining scientific discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "SINDy & symbolic",
            "name_full": "Sparse identification of nonlinear dynamics (SINDy) and symbolic regression for discovering governing equations",
            "brief_description": "Sparse regression and symbolic methods (SINDy, genetic algorithms, symbolic regression) are used to extract interpretable closed-form dynamical models or PDEs from measurement data.",
            "citation_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "applied mathematics / dynamical systems / physics",
            "problem_description": "Infer low-dimensional, interpretable ordinary or partial differential equations that capture the dominant dynamics from noisy observational data.",
            "data_availability": "varies — often limited to observed trajectories or experimental readouts; interventional data can greatly aid identification, and noise levels constrain identifiability.",
            "data_structure": "time-series measurements of observables; may be multivariate and irregularly sampled; low-dimensional latent dynamics sought.",
            "problem_complexity": "medium to high — nonlinear dynamics, potential high-dimensional underlying systems, sensitivity to noise, and large candidate-function libraries increase search complexity.",
            "domain_maturity": "methodologically mature in research but active area of development; domain knowledge often available to guide dictionary choices and regularization.",
            "mechanistic_understanding_requirements": "high — primary goal is interpretable, mechanistic models for scientific understanding; sparse symbolic forms are preferred over black-box predictors.",
            "ai_methodology_name": "Sparse regression (SINDy), symbolic regression, genetic algorithms",
            "ai_methodology_description": "Constructs a library of candidate functions and uses sparsity-promoting regression (L1, sequential thresholding) or evolutionary symbolic-search to select parsimonious equations that match observed dynamics; may include constraints for physical invariances.",
            "ai_methodology_category": "symbolic / sparse / interpretable ML",
            "applicability": "Applicable when observables capture underlying dynamics and noise is manageable; struggles with unobserved variables, heavy noise, or extremely high-dimensional systems without good representations.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Effective for recovering simple-to-moderate-order governing equations and for providing interpretable models, but performance degrades with noise, hidden variables, and very large search spaces.",
            "impact_potential": "High for theory-building — can convert data into closed-form scientific laws, facilitate extrapolation and control, and aid mechanistic understanding when conditions for identifiability are met.",
            "comparison_to_alternatives": "Compared to black-box neural models: symbolic methods yield interpretable equations but may be less flexible in highly complex settings; hybrid approaches combining representation learning with symbolic regression are promising.",
            "success_factors": "Good-quality time-series with sufficient excitation, appropriate candidate-function libraries, sparsity regularization, and incorporation of physical priors (symmetries, conservation).",
            "key_insight": "Symbolic sparse-identification methods excel at yielding interpretable mechanistic models from moderate-noise, well-excited dynamical data, but require careful data collection and priors to handle real-world complexity.",
            "uuid": "e2321.6",
            "source_info": {
                "paper_title": "Decoding complexity: how machine learning is redefining scientific discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Foundation models / LLMs / SFMs",
            "name_full": "Scientific Foundation Models and Large Language Models (LLMs) for cross-domain scientific tasks",
            "brief_description": "Large pre-trained generative models (foundation models, LLMs) trained on massive datasets that exhibit emergent abilities (e.g., in-context learning) and are applied to literature synthesis, hypothesis generation, protein structure/evolution prediction, and experiment guidance.",
            "citation_title": "Emergent abilities of large language models.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "cross-cutting — computational biology (protein design), climate science, literature synthesis, experiment design",
            "problem_description": "Leverage large pre-trained models to generalize to a variety of downstream scientific tasks (summarization, idea generation, structure prediction, experiment guidance) especially when labeled data are scarce for specific tasks.",
            "data_availability": "foundation models are trained on massive heterogeneous corpora (text, sequences, simulation outputs); downstream tasks may have scarce labeled data but benefit from pretraining; concerns about biases in training data noted.",
            "data_structure": "multimodal (text, sequences, structures), often unstructured text for LLMs or sequence data for genome-scale LMs.",
            "problem_complexity": "high — tasks vary widely, often require multi-step reasoning, domain knowledge, and grounding; emergent behaviors make predictive scaling challenging.",
            "domain_maturity": "emerging rapidly — LLMs/foundation models are recent and being adapted to scientific domains; domain expertise integration is active research.",
            "mechanistic_understanding_requirements": "variable — some use-cases accept black-box assistance (idea generation), while others (experimental guidance, clinical) require interpretability and reliability.",
            "ai_methodology_name": "Foundation models / Large language models / Genome-scale language models",
            "ai_methodology_description": "Large-scale transformer-based architectures pre-trained with self-supervised objectives (masked prediction/next-token) on massive datasets; transfer to downstream tasks via fine-tuning, few-shot prompting, or coupling with tools and RL for experiment planning.",
            "ai_methodology_category": "foundation models / self-supervised learning / few-shot learning",
            "applicability": "Highly applicable as broad assistive tools, particularly when labeled downstream data are scarce; limitations include biases, unpredictable emergent failures, and opacity of reasoning.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Powerful qualitatively: enable zero-shot/few-shot generalization, accelerate literature synthesis and hypothesis generation, and have enabled advances in protein prediction and design; risks include bias and limited scientific interpretability.",
            "impact_potential": "Very high — could transform many scientific workflows by automating literature review, proposing experiments, and seeding hypotheses, but scientific validation and oversight are essential.",
            "comparison_to_alternatives": "Compared to task-specific models: foundation models generalize broadly and reduce need for labeled data, but task-specific architectures often yield better calibrated predictive performance; hybrid approaches are promising.",
            "success_factors": "Scale of pretraining data and model, alignment with scientific objectives (fine-tuning, tool use), and integration with domain knowledge and evaluation pipelines to detect and mitigate hallucinations and bias.",
            "key_insight": "Large pre-trained foundation models provide powerful transfer ability for diverse scientific problems, especially when labeled data are scarce, but must be used with careful validation and domain constraints due to biases and opaque failure modes.",
            "uuid": "e2321.7",
            "source_info": {
                "paper_title": "Decoding complexity: how machine learning is redefining scientific discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "AI-Hilbert / AI-Descartes",
            "name_full": "Automated symbolic/scientific-discovery systems (AI-Hilbert, AI-Descartes, Cornelio et al.)",
            "brief_description": "Hybrid AI systems that combine axiomatic/theoretical knowledge with data-driven methods and symbolic regression to generate interpretable mathematical models and rediscover known laws or propose new consistent models.",
            "citation_title": "Evolving scientific discovery by unifying data and background knowledge with ai hilbert.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "scientific-method automation / symbolic model discovery (general across physics/astronomy/etc.)",
            "problem_description": "Automate hypothesis generation and derivation of mathematical models consistent with both data and axiomatic background knowledge to accelerate discovery and reconcile conflicting data.",
            "data_availability": "varies — systems can operate with moderate datasets augmented by background axioms; some tools are designed to work with limited data by leveraging strong theoretical priors.",
            "data_structure": "structured experimental measurements and formalized background knowledge (axioms, symbolic constraints); data can be numeric time-series or tabulated measurements.",
            "problem_complexity": "medium to high — search spaces for symbolic expressions grow combinatorially; integrating background theory reduces complexity but still requires efficient search and selection strategies.",
            "domain_maturity": "research-stage — symbolic discovery systems have shown rediscovery of laws (Kepler's third law, time dilation) but are not yet ubiquitous for open-ended discovery.",
            "mechanistic_understanding_requirements": "very high — core goal is interpretable mechanistic models and formal derivations; transparency and symbolic clarity are essential.",
            "ai_methodology_name": "Hybrid symbolic-AI with logical reasoning and symbolic regression",
            "ai_methodology_description": "Frameworks that integrate logical axioms/constraints with symbolic-search or regression over candidate formula spaces, using data likelihood and axiomatic consistency scoring to prefer derivable, interpretable models; often employ pruning and competition among formulas.",
            "ai_methodology_category": "hybrid / symbolic / knowledge-based",
            "applicability": "Applicable for problems where background theory can constrain the search and where interpretability is the objective; less suited for massively high-dimensional, purely empirical pattern discovery without formal priors.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Effective for rediscovering known physical laws even with limited data and for distinguishing competing formulae; tends to confirm established theories more than produce radically novel laws so far.",
            "impact_potential": "Significant for theory formation and formalization — can accelerate generation of candidate symbolic laws and help reconcile data with theory, boosting reproducibility and interpretability.",
            "comparison_to_alternatives": "Compared to purely data-driven black-box models: hybrid symbolic methods produce interpretable formulas and can work with less data by leveraging theory, but may miss complex empirical patterns that lack compact symbolic forms.",
            "success_factors": "Availability of formalizable background knowledge, moderate-quality measurement data, efficient symbolic search/pruning algorithms, and integration of logical constraints to guide discovery.",
            "key_insight": "Combining axiomatic knowledge with data-driven symbolic search enables derivable, interpretable scientific models and can rediscover known laws from limited data, but current systems are more confirmatory than creatively novel.",
            "uuid": "e2321.8",
            "source_info": {
                "paper_title": "Decoding complexity: how machine learning is redefining scientific discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "SPOCK classifier",
            "name_full": "SPOCK supervised classifier for long-term stability prediction in multi-planet systems",
            "brief_description": "A supervised ML classifier (SPOCK) trained on short-term simulations to predict long-term orbital stability of compact multi-planet systems, enabling fast generalization and reducing expensive long integrations.",
            "citation_title": "Predicting the long-term stability of compact multiplanet systems.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "astrophysics / orbital dynamics / multi-planet stability",
            "problem_description": "Predicting whether a given compact multi-planet configuration will be stable over astronomical timescales without performing computationally prohibitive long-term N-body integrations.",
            "data_availability": "synthetic simulation data abundant from short-term integrations; labeled outcomes (stable/unstable over long time) obtainable via expensive long-term runs but can be generated for training via proxies.",
            "data_structure": "structured numerical features derived from short-term orbital simulations (orbital elements, stability indicators); tabular/feature-vector representation.",
            "problem_complexity": "high — chaotic N-body dynamics, sensitivity to initial conditions, multiscale interactions and the need to extrapolate from short-term behavior to long-term stability.",
            "domain_maturity": "well-established dynamical astronomy with strong theoretical foundations, but long-term stability prediction remains computationally challenging.",
            "mechanistic_understanding_requirements": "medium — fast ML predictions are useful for screening, but physical validation (long integrations, theory) is needed to confirm stability for scientific claims.",
            "ai_methodology_name": "Supervised classifier (SPOCK)",
            "ai_methodology_description": "Supervised ML trained on features from short-time simulations to predict long-term stability labels; leverages generalization to extrapolate to larger systems and avoid long integrations.",
            "ai_methodology_category": "supervised learning / classification",
            "applicability": "Applicable and effective as a screening tool to replace many long integrations with fast predictions; limitations include edge cases and need for care when extrapolating beyond training distributions.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively effective: SPOCK can generalize from short to long timescales and predict stability, enabling practical screening of system architectures and guiding further detailed simulation.",
            "impact_potential": "Moderate-to-high — reduces computational cost for large parameter sweeps in planetary dynamics and aids design/interpretation of exoplanetary system studies.",
            "comparison_to_alternatives": "Alternative is direct long-term N-body integration (accurate but costly); SPOCK trades exactness for huge computational savings and is used as a filter for costly verification.",
            "success_factors": "Ability to generate representative short-term simulation features, careful feature engineering, and a training dataset that captures relevant dynamical regimes.",
            "key_insight": "Supervised classifiers trained on short-term simulations can accurately predict many long-term stability outcomes, providing a scalable surrogate for expensive integrations while requiring careful validation for edge cases.",
            "uuid": "e2321.9",
            "source_info": {
                "paper_title": "Decoding complexity: how machine learning is redefining scientific discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Diffusion models in proteins",
            "name_full": "Diffusion and generative models for protein backbone generation and molecular ensembles",
            "brief_description": "Diffusion-based generative models are used to directly generate protein backbones and molecular ensembles, offering routes to bypass expensive physical simulations for design and sampling.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "protein design / structural biology / molecular modeling",
            "problem_description": "Generate plausible protein backbone conformations and ensembles for design and sampling without running computationally expensive molecular-dynamics simulations.",
            "data_availability": "moderate to large structural datasets (predicted/experimental structures) used for training; coverage varies across protein families.",
            "data_structure": "3D coordinate data (point-clouds or graphs) representing backbones and side-chains; high-dimensional geometric structures.",
            "problem_complexity": "high — complex spatial constraints, sequence-structure coupling, and need to respect physical plausibility across conformational ensembles.",
            "domain_maturity": "emerging — generative modeling for proteins is an active frontier with rapid progress but still maturing evaluation and validation practices.",
            "mechanistic_understanding_requirements": "high for downstream biological application — generated structures require validation (stability, function) and mechanistic insight for design acceptance.",
            "ai_methodology_name": "Diffusion probabilistic generative models",
            "ai_methodology_description": "Score-based/diffusion generative models trained to reverse a noise process on protein coordinate representations to sample realistic backbone structures or ensembles; training leverages structural datasets and geometric priors.",
            "ai_methodology_category": "unsupervised / generative modeling / diffusion models",
            "applicability": "Applicable and promising to accelerate design workflows and sampling, but applicability depends on training coverage and downstream validation pipelines.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively promising: diffusion models can generate state-of-the-art backbones and ensembles and have been used in de novo design pipelines (e.g., RFdiffusion), but experimental validation remains crucial.",
            "impact_potential": "High — could shorten design cycles, provide diverse candidate structures, and reduce reliance on expensive physics simulations when combined with downstream verification.",
            "comparison_to_alternatives": "Compared to molecular dynamics and physics-based sampling: diffusion models produce diverse samples faster but must be checked for physical fidelity; hybrid approaches can combine strengths.",
            "success_factors": "Availability of high-quality structural datasets, incorporation of geometric invariances, and integration with sequence design and experimental testing.",
            "key_insight": "Diffusion generative models can efficiently produce realistic protein backbones and ensembles, offering scalable alternatives to simulations when trained on sufficient structural data and validated experimentally.",
            "uuid": "e2321.10",
            "source_info": {
                "paper_title": "Decoding complexity: how machine learning is redefining scientific discovery",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A deep learning approach to antibiotic discovery.",
            "rating": 2,
            "sanitized_title": "a_deep_learning_approach_to_antibiotic_discovery"
        },
        {
            "paper_title": "Highly accurate protein structure prediction with alphafold.",
            "rating": 2,
            "sanitized_title": "highly_accurate_protein_structure_prediction_with_alphafold"
        },
        {
            "paper_title": "Magnetic control of tokamak plasmas through deep reinforcement learning.",
            "rating": 2,
            "sanitized_title": "magnetic_control_of_tokamak_plasmas_through_deep_reinforcement_learning"
        },
        {
            "paper_title": "Predicting the long-term stability of compact multiplanet systems.",
            "rating": 2,
            "sanitized_title": "predicting_the_longterm_stability_of_compact_multiplanet_systems"
        },
        {
            "paper_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems.",
            "rating": 2,
            "sanitized_title": "discovering_governing_equations_from_data_by_sparse_identification_of_nonlinear_dynamical_systems"
        },
        {
            "paper_title": "Generative adversarial networks.",
            "rating": 2,
            "sanitized_title": "generative_adversarial_networks"
        },
        {
            "paper_title": "Advancing mathematics by guiding human intuition with ai.",
            "rating": 1,
            "sanitized_title": "advancing_mathematics_by_guiding_human_intuition_with_ai"
        },
        {
            "paper_title": "Evolving scientific discovery by unifying data and background knowledge with ai hilbert.",
            "rating": 2,
            "sanitized_title": "evolving_scientific_discovery_by_unifying_data_and_background_knowledge_with_ai_hilbert"
        },
        {
            "paper_title": "Emergent abilities of large language models.",
            "rating": 1,
            "sanitized_title": "emergent_abilities_of_large_language_models"
        },
        {
            "paper_title": "Alleviating the transit timing variation bias in transit surveys -i. rivers: Method and detection of a pair of resonant super-earths around kepler-1705.",
            "rating": 2,
            "sanitized_title": "alleviating_the_transit_timing_variation_bias_in_transit_surveys_i_rivers_method_and_detection_of_a_pair_of_resonant_superearths_around_kepler1705"
        }
    ],
    "cost": 0.022789249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Decoding complexity: how machine learning is redefining scientific discovery
25 Apr 2025</p>
<p>Ricardo Vinuesa rvinuesa@mech.kth.se 
FLOW, Engineering Mechanics
KTH Royal Institute of Technology
StockholmSweden</p>
<p>Swedish e-Science Research Centre
StockholmSeRCSweden</p>
<p>Paola Cinnella 
Institut Jean le Rond D'Alembert
Sorbonne Universit é
France</p>
<p>Jean Rabault 
IT Department
Norwegian Meteorological Institute
0313OsloNorway</p>
<p>Hossein Azizpour 
Swedish e-Science Research Centre
StockholmSeRCSweden</p>
<p>Robotics, Perception and Learning
KTH Royal Institute of Technology
StockholmSweden</p>
<p>Stefan Bauer 
Information and Technology
TUM School of Computation
Technical University Munich
MunichGermany</p>
<p>Helmholtz AI
Helmholtz Center Munich
MunichGermany</p>
<p>Bingni W Brunton sbrunton@uw.edu 
Department of Biology
University of Washington
98195SeattleWAUSA</p>
<p>Arne Elofsson 
Swedish e-Science Research Centre
StockholmSeRCSweden</p>
<p>Dept. of Biochemistry and Biophysics and Science for Life Laboratory
Stockholm University
171 21Solna</p>
<p>Elias Jarlebring 
Swedish e-Science Research Centre
StockholmSeRCSweden</p>
<p>Dept. Mathematics
KTH Royal Institute of Technology
100 44 StockholmSweden</p>
<p>Hedvig Kjellstr Öm 
Swedish e-Science Research Centre
StockholmSeRCSweden</p>
<p>Robotics, Perception and Learning
KTH Royal Institute of Technology
StockholmSweden</p>
<p>Stefano Markidis 
Swedish e-Science Research Centre
StockholmSeRCSweden</p>
<p>Department of Computer Science
KTH Royal Institute of Technology
StockholmSweden</p>
<p>David Marlevi 
Dept. Molecular Medicine and Surgery
Karolinska Institutet
171 77StockholmSweden</p>
<p>Inst. for Medical Engineering and Science
Massachusetts Institute of Technology
02139CambridgeMAUSA</p>
<p>Javier García-Martínez 
Departamento de Química Inorg ánica
Universidad de Alicante
AlicanteSpain</p>
<p>Steven L Brunton 
Department of Mechanical Engineering
University of Washington
98195SeattleWAUSA</p>
<p>Decoding complexity: how machine learning is redefining scientific discovery
25 Apr 202582E77CE8EC2CF84630DCADD3824E98ADarXiv:2405.04161v2[cs.LG]machine learning (ML)deep learning (DL)artificial intelligence (AI)scientific discoverycomplexityphysicslife sciencescomputer science
As modern scientific instruments generate vast amounts of data and the volume of information in the scientific literature continues to grow, machine learning (ML) has become an essential tool for organising, analysing, and interpreting these complex datasets.This paper explores the transformative role of ML in accelerating breakthroughs across a range of scientific disciplines.By presenting key examples -such as brain mapping and exoplanet detection -we demonstrate how ML is reshaping scientific research.We also explore different scenarios where different levels of knowledge of the underlying phenomenon are available, identifying strategies to overcome limitations and unlock the full potential of ML.Despite its advances, the growing reliance on ML poses challenges for research applications and rigorous validation of discoveries.We argue that even with these challenges, ML is poised to disrupt traditional methodologies and advance the boundaries of knowledge by enabling researchers to tackle increasingly complex problems.Thus, the scientific community can move beyond the necessary traditional oversimplifications to embrace the full complexity of natural systems, ultimately paving the way for interdisciplinary breakthroughs and innovative solutions to humanity's most pressing challenges.</p>
<p>Introduction</p>
<p>Machines have played a critical role in scientific discovery (ı.e. to obtain fundamental and formalized knowledge about Nature) by providing the tools to observe, measure, and analyze natural phenomena.Scientific instruments, such as telescopes and microscopes, have historically enabled groundbreaking discoveries by revealing details invisible to the naked eye, expanding our understanding of the universe and the microscopic world [1].With the advent of modern scientific instruments, including DNA sequencers, astronomical observatories, and highresolution imaging devices, research facilities are producing terabytes or even petabytes of information.As data volumes grow, computers play a critical role in organizing, analyzing, and interpreting this information.Advanced computational methods help to reduce the complexity of the data, making it possible to extract meaningful insights [2,3].However, even with the most advanced computers, the sheer volume of data generated by large-scale projects such as the Large Hadron Collider (LHC) and the Square Kilometre Array (SKA), and the vast amount of information available in the scientific literature, make traditional analysis methods impractical.Complex problems such as weather forecasting, drug discovery, and genomic analysis often involve highly complex data sets and processes that cannot be efficiently managed without the assistance of machine learning (ML), which can help sift through massive data streams, identify patterns, and extract valuable insights that would be impossible for humans or traditional computational methods alone.Complexity in these problems arises from non-linearity, high dimensionality, and multiscale dynamics, posing significant challenges for traditional mathematical tools and even recent simulation-based approaches.Despite advancements in simulations and big data, we still struggle to fully understand phenomena like turbulence or biological processes at a deeper level.Advanced ML tools are also improving decision-making, enabling faster and more accurate interpretations of complex phenomena, and addressing challenges in diverse scientific fields [4].However, it also introduces some challenges and the need for ethical guidelines to ensure the appropriate use of ML for scientific research [5].A key issue is algorithmic bias, which can distort outcomes and lead to incorrect conclusions, particularly in areas like health care, where biased predictions can impact patient care or drug development.Additionally, the black-box nature of ML models complicates transparency, making it hard for researchers to verify decisions.Lastly, data ownership and privacy concerns arise, especially with sensitive data like genetic or health records.These issues require the development of robust ethical guidelines to ensure that ML contributes to science in a fair, transparent, and socially responsible way [6].</p>
<p>In this work, we explore the potential of ML and artificial intelligence (AI) in three types of scientific problems: (i) those where all governing equations are known, (ii) those with partial knowledge and (iii) those where little is known.We illustrate this with examples from the physical and life sciences, including turbulent flows, dark matter, drug discovery, and brain research.Figure 1 summarizes how different uses of ML help address complexity in these areas.As discussed below, when the system complexity grows, the distinction between full, partial, and no knowledge of the governing equations becomes increasingly blurred, but ML plays a crucial role in tackling these challenges.A good example of a complex system with vast amounts of data that traditional tools cannot process efficiently is brain research.ML enables the reconstruction of countless brain slices into highly accurate three-dimensional (3D) maps.In a recent study, Google researchers used AI to process 300 million brain images from Harvard, creating the largest-ever interactive 3D brain tissue model, now available online.This innovation is crucial for understanding neurological disorders, as ML can detect patterns that traditional methods might miss, supporting early diagnosis and treatment planning [7].One of the most widespread applications of ML is in drug discovery, addressing the time and cost challenges of traditional methods.The drug-development process can take over a decade and billions of dollars due to the complexity of identifying viable candidates.ML is transforming this by rapidly analyzing vast biological and chemical data, uncovering patterns that might remain hidden, and streamlining the identification of promising candidates.Additionally, ML enhances predictive modeling, allowing researchers to forecast drug efficacy and safety earlier in the process.By enabling virtual screening of drug-target interactions, ML reduces the need for costly lab experiments and helps design more efficient clinical trials, accelerating development and optimizing resources [8].</p>
<p>Figure 2 provides a comprehensive visualisation of the key themes discussed in this paper, focusing on the role of machine learning (ML) in scientific discovery and the challenges it presents, particularly with respect to data.The top-left panel illustrates the significant increase in data generated by modern scientific instrumentation over time.Initially, humans organised and interpreted the data through manual analysis and generalisation.Since then, computational methods have facilitated the management of large amounts of data, contributing significantly to fields such as genomics, chemistry and mathematics.More recently, with the advent of large-scale scientific facilities such as CERN, ML techniques have become essential for processing vast amounts of data.This shift has reduced human involvement in direct observation and interpretation, raising concerns about our diminishing understanding of the discoveries made by these technologies.The top-right panel outlines four key challenges related to ML in science: data quality and availability, inherent biases, explainability, and the risk of overfitting.The bottom-left panel highlights a conceptual dilemma: while ML accelerates discovery, there is growing debate about what constitutes a true scientific breakthrough, and whether ML can only "rediscover" existing concepts rather than uncover new insights.Finally, the bottom-right panel emphasises the increasing need for high-quality data to enable new discoveries through ML, while highlighting the limitations imposed by our current gaps in scientific knowledge.All of these issues are discussed in detail in the following sections of this paper.</p>
<p>Artificial Intelligence is playing a transformative role in physics by improving data analysis, model development, and experimental interpretation.In astronomy, ML is improving the search for exoplanets by boosting the accuracy and efficiency of data analysis.AI-powered algorithms, particularly convolutional neural networks, can process massive data sets from telescopes to detect Earth-like exoplanets in noisy signals more precisely than traditional  methods.The transit method, which detects exoplanets by observing mini-eclipses as they pass in front of their stars, can be complicated by planetary interactions that disrupt periodicity.To address this, researchers from the University of Geneva, University of Bern, and Disaitek applied ML and image recognition techniques to predict these interactions.By training a neural network on numerous examples, they built a model capable of detecting subtle exoplanet signals that might be missed by traditional methods.Their work led to the discovery of exoplanets Kepler-1705b and Kepler-1705c, advancing our understanding of planetary systems [9].ML is also crucial in areas like the Standard Model of particle physics [9].Automated algorithms were central to the discovery of the Higgs boson [10], and future experiments will need to operate at higher energies and intensities, generating data volumes too large for traditional methods to handle.For instance, it is expected that the Large-Hadron Collider (LHC) will increase proton collision rates by an order of magnitude in the next decade, requiring data analysis tools such as ML, to identify trends, uncover hidden relationships, and design more effective experiments.</p>
<p>Another scientific area in which ML is increasingly assisting research is mathematics, in this case by improving the process of theorem proving, mathematical method development and discovery.ML systems have already demonstrated their ability to automate aspects of theorem proving; for example, Meta AI's neural theorem prover successfully solved 10 International Math Olympiad (IMO) problems, far exceeding the performance of previous ML systems.Another notable example is DeepMind's collaboration with mathematicians, which has resulted in ML contributing to new mathematical methods in knot theory and representation theory.Specifically, in 2021, ML was used in a new constructive way to suggest mathematical proofs.A collaboration between mathematicians and DeepMind demonstrated that ML can complement human intuition in proving or suggesting complex theorems.The team used ML to investigate long-standing conjectures, including the Kazhdan-Lusztig polynomials, and discovered new connections in knot theory.This illustrates AI's potential to accelerate mathematical research and open new frontiers [11].</p>
<p>Embracing complexity</p>
<p>Machine learning comprises a growing set of algorithms, enabled by high-performance computing and increasingly vast data, that show incredible promise for handling complexity [12,13].Neural networks, despite being governed by simple rules, can perform complex tasks for which no traditional algorithms exist.Although they are fully The need to have more and better data to be able to make scientific discoveries with ML as we have less knowledge on the subject of study observable and deterministic, we often cannot explain their decisions.However, they have led to groundbreaking discoveries, such as a new class of antibiotics [14].This creates challenges, such as the need for explainable AI (XAI) [15,16].Symbolic approaches, such as gene-expression programming, sparse regression, and sparse Bayesian learning [17][18][19], have been successful, but their complexity grows exponentially with the search-space size.Recent efforts to combine symbolic and deep-learning approaches have enabled advances, such as discovering new materials [20].This raises fundamental questions about the limits of ML in scientific discovery; for example, can a complex system understand its own complexity?And how much can AI discover beyond its training data [21]?These issues highlight the opportunities and challenges that data-driven methods bring to science.Some believe in the "unreasonable effectiveness of data" [22], particularly in deep learning [23], but the practical implications for future discoveries remain uncertain.A key question is whether ML can provide not only computational solutions but also fundamental scientific understanding.Note that ML's applications in science are not limited to discovery.For example, AI is revolutionizing optimization, laboratory automation, and solving governing equations, such as partial differential equations (PDEs).Recent studies [24] have focused on the potential of self-supervised learning in experiments and simulations (including representations of scientific data), while others [25] are exploring AI's role in formulating scientific questions.Note that the unique contribution of this work is the study of how ML can tackle complexity to reach scientific discoveries, acknowledging that different levels of knowledge of the governing equations (see Figure 1) will require completely different ML approaches.This may constitute a revolution in how we organize disciplines when using ML for scientific discovery, where apparently different communities may share many similarities in terms of how much is known regarding the governing equations and therefore in terms of the ML methods to be developed.</p>
<p>The emergence of scientific foundation models (SFMs) and large language models (LLMs) is further pushing the boundaries of ML methods.Foundation models are large machine-learning or deep-learning generative models trained on vast amounts of data so they can be applied on a wide range of cases.They predict masked data regions to learn associations, excelling in multiple tasks without large, labeled datasets.Despite risks like bias and transparency issues [26], foundation models have revolutionized AI, especially through chatbots like ChatGPT [27].LLMs now assist with tasks from writing and coding to guiding scientific experiments [28,29] and generating ideas [30].Foundation models also show promise in non-text-focused areas, such as protein-structure prediction [31], protein design [32] and climate simulations [33].An intriguing feature of foundation models, and particularly LLMs, is that they demostrate so-called "emergent abilities" [34,35].The term refers to unexpected, not explicitly programmed, capabilities that arise as model scale increases, and do not consist in mere extrapolations of smaller models' performance.Such abilities were first observed in complex natural systems, e.g.phase changes in materials, complex behavior in flocks of birds or fishes, as well as in computational systems such as cellular automata and agent-based models, and the term was originally popularized by the Nobel-prize winner P.W. Anderson [36].Examples of LLM emergent abilities include in-context learning, complex reasoning, and multi-step problem-solving, which are highly valuable for scientific research.Thank to this, LLMs are found to generalize to new tasks without prior examples (zero-shot learning) or with minimal data (few-shot learning), making them particularly useful in domains where data is scarce or highly specialized.More in general, they have the potential to aid hypothesis generation, automate literature reviews, predict protein structures, and accelerate scientific discovery, especially when combined with reinforcement learning (RL) and a suitable system of rewards/penalties to optimize specific scientific tasks, such as designing experiments or iteratively refining hypotheses based on feedback.Synergy with scientists may allow to leverage the models' reasoning capabilities in dynamic, real-world scenarios, enabling more efficient exploration of complex scientific problems.On the other hand, these abilities are often unpredictable and non-linear, raising challenges in reliability, interpretability, and ethical use.Careful oversight is then needed to ensure these models align with scientific goals and produce trustworthy results.Risks include the potential for generating misleading or incorrect outputs, amplifying biases present in training data, and over-reliance on automated systems without sufficient human validation.Additionally, the opacity of how these models arrive at their conclusions can hinder their adoption in critical scientific applications, raising important questions about their benefits and risks in science [37].Fajardo-Fontiveros et al. [38] discuss when it is possible to learn models from data and the acceptable noise levels for accurate learning.</p>
<p>Discovery versus re-discovery by machine learning</p>
<p>While ML has proven invaluable in refining existing knowledge, its real potential lies in detecting patterns and correlations that may not be immediately apparent due to the vast amounts of data involved.This raises the question of whether ML is truly discovering new insights or merely reinterpreting existing knowledge.Historically, scientific discovery has been rooted in inquiry, observation, and experimentation, with creativity playing a crucial role in generating new insights into phenomena or theories [39,40].While ML excels at analyzing large datasets, identifying patterns, and generating hypotheses (which are key components of discovery), its ability to make truly autonomous, original discoveries is still debated.ML models operate without a genuine understanding of the underlying mechanisms they analyze, meaning that their "discoveries" tend to be instrumental rather than original.Groundbreaking discoveries usually require creativity, broader contextual understanding and sometimes leaps beyond available data, all of which ML currently lacks.Although ML is revolutionizing research by uncovering complex trends, the idea that it can independently produce original scientific discoveries remains controversial.Several studies have explored this question [41], and workshops, such as the one organized by the National Academies on How ML Is Shaping Scientific Discovery, have debated the issue [42].Nevertheless, ML has already contributed to significant scientific breakthroughs.In drug discovery, ML identified new antibiotics, such as halicin, which were unknown and can kill antibiotic-resistant bacteria [43].ML also plays a crucial role in materials science, predicting new materials for batteries and superconductors using physical and experimental data.These achievements highlight AI's potential to navigate uncharted scientific territory [44].</p>
<p>One of the most significant advancements in AI-driven scientific discovery is AI-Hilbert.Developed by IBM researchers, AI-Hilbert acts as an "AI scientist" that transforms existing theories and data into new, consistent mathematical models.Its goal is to accelerate scientific discovery by automating hypothesis generation and testing.</p>
<p>5/19</p>
<p>AI-Hilbert helps scientists uncover new knowledge by analyzing large scientific datasets and revealing patterns overlooked by traditional methods.It also refines theories by managing conflicting data [45].Furthermore, Cornelio et al. [46] have developed a new ML tool which combines axiomatic knowledge with experimental data to derive scientific models.By integrating logical reasoning and symbolic regression, it has rediscovered laws like Kepler's third law and Einstein's time-dilation law.This tool can distinguish between competing formulas, even with limited data.While efficient at replicating human discoveries, these tools often confirm known theories rather than offering new insights.ML can be biased towards existing patterns, limiting its ability to generate novel hypotheses.In these cases, AI's role is more supportive, validating established knowledge rather than challenging it [47].ML holds immense potential for making groundbreaking discoveries, particularly in fields like drug development and astrophysics.However, its frequent rediscovery of known scientific principles illustrates the current limitations of its creativity.For ML to drive new knowledge, it must evolve beyond confirming human findings.Currently, ML lacks an intrinsic understanding of the mechanisms it analyzes, so its discoveries often complement human expertise, relying on human interpretation and creativity to fully appreciate and exploit the insights gained.</p>
<p>Machine-learning-driven scientific discovery when complete information is available</p>
<p>There are several applications within Physical Sciences where the underlying governing equations are known perfectly but the high-level global dynamics are still not well understood.For instance, while we know the quantum equations behind biomolecular dynamics, complexity makes biology a partial-knowledge problem since only limited biological systems can be measured or simulated, and simulating the full human brain is impossible.Similarly, turbulent flows, described by the Navier-Stokes equations, are only partially understood due to their chaotic nature as the Reynolds number increases [48].While the large scales in the flow can be simulated, the smaller scales are much more difficult to simulate, making turbulence a major challenge [49].However, some ML techniques, such as symbolic regression and reduced-order modeling, can help uncover unknown flow features and physical properties from large direct-numerical-simulation (DNS) datasets [50,51], as indicated in Figure 3. Advances in turbulence modeling using supervised ML have also improved closure models for Reynolds-averaged Navier-Stokes (RANS) and large-eddy-simulation (LES) turbulence models [52,53].In astrophysics, the supervised classifier SPOCK predicts long-term stability in multi-planet systems (which requires integration of the laws of gravitation over billions of orbital periods) using short-term simulations and effectively generalizing to larger systems [54].</p>
<p>Optimal control of complex physical and biological systems is another challenging area where ML, particularly deep reinforcement learning (DRL), shows promise.DRL has led to breakthroughs in quantum physics, astronomy, turbulence control, and tokamak-instability control, offering insights into complex systems and discovering novel strategies [55].DRL has even uncovered previously unknown thermodynamic cycles [56].ML can dramatically accelerate simulations of complex systems with known equations.Autoencoders can be used to discover latentspace representations that enable faster time integrators and simulations, leading to better optimization and systematic studies [57,58].For instance, high-energy physics relies on comparing observed particle detector data with simulations, which are computationally intensive.Fast generative models like generative adversarial networks (GANs) and variational autoencoders (VAEs) offer a faster alternative, although on-going research aims at ensuring that the required accuracy is achieved [59,60].In climate science, foundation models for weather forecasting are revolutionizing climate studies, offering potential breakthroughs in understanding climate change and paleoclimates [13].In applied mathematics, DRL and large language models (LLMs) are generating novel algorithms and optimizations.Notable examples include matrix operations [61] and combinatorial problems, where AI aids in discovering new strategies that can be validated with classical algorithms [62].In these cases, AI provides valuable heuristic methods, although it must be highlighted that finding adequate solutions still remains challenging.</p>
<p>Machine-learning-driven scientific discovery when only partial information is available</p>
<p>In contrast to systems with well-understood governing equations, in some scientific problems we only have access to partial knowledge of the underlying mechanisms, as seen in complex materials (e.g., composites or textured materials) and certain fluids (e.g., granular or multiphase flows).These systems can exhibit simple microscopic behaviors yet result in complex macroscopic phenomena, such as the spread of infectious diseases or "active turbulence" in biological matter [63,64].In these cases, inductive biases, like frame invariance or symmetry constraints, can be incorporated into ML models to improve the discovery process [65,66].Physical constraints (e.g., thermodynamics) help create more generalizable models.For example, Moya et al. [67] proposed a neural 6/19 Figure 3. Schematic representation of ML directions to enable scientific discoveries when complete information about the governing equations is available.In such a case, both supervised, unsupervised, and reinforcement-learning methodologies can be used.Supervised and unsupervised methodologies are made possible by generating large datasets of synthetic data simulated from the governing equations.This allows the deployment of a variety of ML techniques that can discover complex hidden relations, nonlinear coordinate systems, hidden dynamics or solve otherwise intractable problems.Reinforcement learning can also be used by coupling it to the physics simulator, which has already proven successful at discovering previously unknown control strategies and regimes of complex systems or generating high-quality heuristic guesses that can be tested in the case of problems where solution verification is easy, but the suggestion of good candidate solutions is hard.network constrained by known thermodynamic properties, enabling broader applications across physical systems.It is also important to highlight digital twins, where only some data are available, and they combine data-driven aspects with simulations while preserving generalization properties [68].Another example of taking advantage of known physical properties of the system is embedding symmetries in autoencoders, intending to develop reducedorder models (ROMs) in physical systems invariant to input transformations [69,70].These data-driven and physics-informed techniques are crucial for scientific discovery; for instance, ML is helping to derive constitutive laws for materials with complex rheologies.De Lorenzis and collaborators [71] introduced a hybrid framework (EUCLID) to learn constitutive equations for hyperelastic solids, and the SpaRTA framework for data-driven turbulence modeling [72] has been adapted to model elastic solids [73].Such approaches have also been used to infer constitutive equations from crystal structures and rheology of complex fluids [74] (see Figure 4).</p>
<p>In the context of Life Sciences, structural biology is a field where ML has made significant advances despite partial knowledge of the phenomena.AlphaFold [75], for instance, folds proteins into their three-dimensional (3D) native form from a one-dimensional(1D) amino acid sequence using deep learning with embedded biases like multiple sequence alignment (MSA) and 3D equivariance.AlphaFold has also spurred innovations in ML, including single-sequence methods like ESMfold [76] and generative models for protein design [77,78].Diffusion models are also being used to generate protein-backbone structures and molecular ensembles, bypassing the need for simulations [79,80].Furthermore, generative models have also been used to discover physical models, integrating prior knowledge with data to generalize to new scenarios [81].Transformers, for instance, are now used in Chemistry to complete chemical reactions and predict reaction yields [82].When it comes to quantum systems, deep reinforcement learning (DRL) has enabled new approaches to manipulate quantum states, providing insights into quantum mechanics [83].Additionally, ML aids in reducing noise in quantum-computing systems, while quantum computing improves ML performance [84].In climate modeling, ML has developed new LES Figure 4. Example of machine learning applied to a case where partial knowledge is available about the underlying system, illustrating a model (for instance a flow with complex rheology or a flow through a porous medium, top right of the picture) which depends on a set of known inputs x (e.g.geometry, boundary conditions, etc.) as well as on a set of hidden (unobservable) variables α α α describing, e.g., the fluid constitutive behavior.The latter may involve small-scale phenomena that can be difficult or impossible to describe.In such conditions, experimental or numerical data for observable quantities (e.g.velocity fields or stresses y) can be used to infer the unknown field by training a machine learning model (here represented as a neural network, although other ML approaches are possible), subjected to physical constraints (e.g.positivity, symmetries or invariances).The whole process allows, on the one hand, to train a data-driven closure model for the hidden variables α α α and, on the other hand, to gain a-posteriori physical knowledge of the fluid constitutive properties.models to ensure stable long-term forecasts [85].ML has also been shown to enhance traditional weather-prediction systems [86].</p>
<p>Machine-learning-driven scientific discovery when little information is available</p>
<p>There are numerous phenomena across scientific disciplines whose origins and underlying principles remain elusive.In these cases, the absence of well-established governing equations or foundational physical models makes it difficult to fully capture and understand their critical dynamics.For example, neuroscience has no first-principle equations, as there are no known conservation laws or symmetries to derive generalizable differential equations.Even with equations describing molecules or cells, the brain's complexity makes full-scale simulation unfeasible.Despite this, advances in neural data acquisition, such as large-scale neural recordings and connectomics, produce unprecedented datasets, promising a new era of ML-driven discovery in neuroscience and behavior [87,88].Although full brain simulations remain beyond reach, data-driven models can replicate key input-output relationships, generating predictions vital for discovery.In neuroscience, perturbation experiments (such as activating neuron populations during visual tasks) provide insights into visual perception.Data-driven models can generate testable hypotheses and refine their predictions iteratively through experiments.ML also synthesizes diverse experimental data, such as the MICrONS dataset, which links functional imaging with structural reconstructions of cortical neurons [89].</p>
<p>ML can learn dynamics where no a-priori knowledge exists, as seen in the Hodgkin-Huxley equations modeling neural dynamics [90].Approaches like SINDy (sparse identification of nonlinear dynamics) [19], genetic algorithms [91] and reinforcement learning [92] can help uncover system dynamics.Neural ordinary differential equations (NODEs) [93] are another method for modeling continuous dynamics, although they often lack scientific insight.Hybrid models relying on transformers [94] or SINDy-inspired architectures [95] aim to bridge this gap.Interpretable-ML models can also discover biomarkers for diseases or predict treatment outcomes from patient data [96].In systems lacking governing equations, interventional data like CRISPR-Ko experiments in single-cell biology [97] are enabling ML to identify some of the underlying mechanisms.Automated setups are advancing [98], leading to research in designing experiments for system identification, especially for non-linear models like NODEs [99].Representation-learning techniques, such as variational autoencoders [100], are essential for simplifying the characterization of complex systems by identifying latent spaces that reduce dimensionality and reveal causal structures [101].Causality and its application to dynamical systems have gained prominence [102], especially in Earth sciences [103] and molecular biology [104], with some studies using invariance from heterogeneous experiments as a signal to identify causal ODEs [105].Learning structured latent spaces is of crucial importance since it provides an effective coordinate system in which the dynamics have a simple representation, which is a key requirement for generalization and interpretability [106].In Figure 5 we provide a schematic representation of the identification of an underlying causal structure from observations where the variables of interest are not directly observed.</p>
<p>Figure 5. Schematic representation of a model (for instance, the observed symptoms of an unknown or complex disease within a population, or observed opinion dynamics within a social network) where the behavior as observed in data depends on an unknown dynamic or causal structure.The observed behavior or dynamics might occur on several different spatial and temporal scales, and the observed data might reflect more or fewer aspects of the underlying system.In such conditions, representation-learning methods can be employed to distil out an explanation of the observed data in the form of a system of ODEs or as a causal-graph representation.</p>
<p>Recent research has also explored improving diffusion models and solvers [107], which are useful for generating state-of-the-art results in areas like image generation [108], protein modeling [109] and materials science [110].Despite their success, large pre-trained models provide limited scientific insights [111], and therefore constitute an opportunity for future research.Machine learning has also enhanced data collection and processing in systems with indirect or incomplete measurements.ML imputation and generative modeling can complete time-series data, improving downstream applications [112].Furthermore, computer-vision techniques have also automated previously manual tasks, such as segmentation in microscopy, enabling large-scale analysis of cell populations [113].In ethology, ML has transformed video data into animal kinematics and poses, linking behavior to underlying neural computations [114].Notably, large language models (LLMs) and scientific foundation models (SFMs) are opening new avenues for extracting scientific insights directly from data.Genome-scale language models (GenSLMs), for instance, are helping to learn the evolutionary landscape of SARS-CoV-2 genomes [115], while LLMs are enhancing neuroscience research by integrating diverse datasets and summarizing insights across isolated subfields [116].</p>
<p>In sum, it is hard to overstate the ongoing impact of machine learning as a critical tool that, when used in conjunction with other approaches (e.g.experiments, causality analysis, development of an adequate coordinate system, etc.), catalyzes advances in scientific fields where no information on the phenomenon under study is available.</p>
<p>9/19</p>
<p>The drawbacks, limitations and challenges of machine learning for scientific discovery Despite the significant advances that ML has brought to scientific discovery, there are key areas that need to be addressed to accelerate its contributions and realize its full potential while minimizing some clear risks, as discussed below:</p>
<p>• Data-related challenges: The success of ML in scientific discovery relies on large high-quality, structured datasets, but scientific data is often incomplete, noisy, or imbalanced, leading to biased models.Unstructured data, especially in fields like biology, chemistry, and geology, complicate the use of ML applications, since these algorithms are not inherently designed to handle such data.However, it was recently shown that foundation models could help for prediction tasks in small general datasets [117].Furthermore, the absence of labeled data complicates the use of supervised-ML techniques [118].</p>
<p>• Bias and ethical issues: ML models are prone to data bias, distorting results and hindering scientific discovery.Bias in data collection or model training can reinforce existing assumptions instead of revealing novel insights [119].This is especially concerning in fields such as drug discovery, where existing models might overlook demographic groups, making the discoveries less generalizable [120].Ethical concerns, especially in medicine, highlight risks in applying ML to decisions affecting human life [121].</p>
<p>• Explainability and interpretability: ML models, particularly those based on deep learning, often function as "black boxes", making accurate predictions without revealing their decision-making processes [16].This lack of transparency is problematic in fields like drug discovery, where understanding why a model predicts a certain interaction is crucial [122].Advances in explainable AI (XAI) have improved interpretability in areas like materials science [123], chemistry [124], and medicine [125].</p>
<p>• Overfitting and generalization: ML models often overfit their training data, performing poorly on unseen data, a fact that limits their ability to generalize.In scientific contexts, this can produce misleading results and fail to capture complex, nonlinear relationships, as seen in chemistry, biology, and astronomy [126][127][128].Overfitting restricts the potential of ML to develop universal theories essential for broad scientific understanding [129].</p>
<p>Improving data quality and diversity is critical for ML models to generalize across disciplines.Initiatives like the Open Reaction Database [130] and the Crystallography Open Database [131] aim to enhance data management under the FAIR (findability, accessibility, interoperability and reusability) principles [132,133].Incorporating bias detection and fairness-aware algorithms can reduce biased data impacts [134].XAI methods are being developed to improve the transparency of ML models, particularly in healthcare applications like brain-tumor segmentation [135].Hybrid approaches that combine ML with physics-based models yield promising results by ensuring that the predictions adhere to known scientific principles [136,137].It is also important to note that ethical frameworks are also needed to ensure fairness, transparency, and accountability, especially in medicine [138,139].</p>
<p>While AI has shown great potential in scientific discovery, significant challenges remain.Issues related to data quality, bias, interpretability, and overfitting must be addressed to harness the full potential of AI in advancing science.By improving data access, enhancing model transparency, and developing hybrid models, the scientific community can overcome these obstacles and drive meaningful, trustworthy discoveries using AI which may play an instrumental role in areas as far-reaching as gravitational waves [140], space exploration [141] or even the discovery of extraterrestrial life [142].</p>
<p>Conclusions and outlook</p>
<p>Karl Popper famously stated in The Open Universe: An Argument for Indeterminism from the Postscript to The Logic of Scientific Discovery that "science can be described as the art of systematic oversimplification" [143] and this remains true due to the historical limitations in processing and analyzing vast amounts of data.As a result, scientists have been forced to simplify their objects of study by focusing on isolated phenomena or simple models.While simplification has benefits, for example in the teaching and the systematization of science, oversimplification carries significant risks, as essential information and key aspects of problems may be lost in the process.For example, in ecological research, studies often focus on individual species interactions without considering the broader dynamics of ecosystems or other effects, such as climate change [144].</p>
<p>ML methods have already enabled several scientific and technological advancements, from solving image classification to winning at Go.This article highlights how modern data-driven methods are enabling breakthroughs in scientific discovery, focusing on state-of-the-art techniques that push beyond previous limitations.We categorize these approaches by how much knowledge about the underlying mechanisms is available, ranging from wellunderstood systems to those with unknown governing principles.As shown in Table 1, ML's potential spans a wide spectrum of applications, including discovering physical laws, evaluating complex systems, inferring unknown behaviors, and uncovering multiscale principles.These advances apply across fields like Physics, Mathematics, Chemistry, and Life Sciences, promising faster scientific progress than ever before.However, using ML for scientific discovery brings challenges.A key advantage of ML is its ability to model complex systems, but this often requires extensive training data.In areas like astrophysics, rare diseases, or new pharmaceuticals, data scarcity is a frequent obstacle.Fortunately, complementary ML methods can either generate needed data or bypass large datasets through techniques like self-supervised learning.Even when ML does not directly result in discoveries, it can facilitate breakthroughs where data are limited.Validation is another challenge, especially in cases where the governing principles are unknown.But ML-driven discoveries can be confirmed using traditional scientific methods, such as hypothesis testing, observational confirmation and benchmarking.Additionally, ML models often function as "black boxes", making it hard to derive formal knowledge from their results.This is a significant issue for science, where understanding is key.However, explainable-and interpretable-ML methods offer solutions, helping to achieve discoveries in the context of established scientific principles.Despite these challenges, ML is becoming an essential tool across disciplines, and its continued evolution promises even more opportunities for scientific discovery.With further refinement, ML techniques are likely to address the current limitations, enabling even greater advancements.</p>
<p>The recent Nobel Prizes in Physics [145] and Chemistry [146] underscore the transformative impact of machine learning, demonstrating its ability to revolutionise scientific discovery.By providing cutting-edge tools for data analysis, ML is accelerating breakthroughs across multiple fields, deepening our understanding and tackling complex challenges in ways previously unimaginable.This capability is crucial, as only by embracing this complexity can we connect the dots in science, grasp the intricate interdependencies of natural systems, and facilitate breakthrough, unpredictable discoveries.The real challenge -and opportunity -of AI-driven scientific discovery lies in moving beyond solving narrow, well-defined problems to tackling complex, open-ended questions.This shift calls for the development of artificial general intelligence (AGI) or even artificial superintelligence (ASI), which possesses the potential to engage in multifaceted problem-solving across diverse domains.AGI for scientific discovery could connect seemingly disparate areas of knowledge, leading to breakthroughs that single-focused AI cannot achieve.For instance, an AGI model could integrate insights from pharmacology, neuroscience, and data science to not only propose novel drug candidates but also predict their interactions and effects on complex biological systems.This capacity for interdisciplinary connections would facilitate original contributions to scientific knowledge, allowing for innovative solutions to intricate challenges.Ultimately, harnessing AGI's multifaceted problem-solving abilities presents a transformative opportunity to revolutionize how we approach scientific discovery and address some of humanity's most pressing challenges.Table 1.Summarizing overview of the opportunities for machine learning in scientific discovery.Based on the differentiation presented in our work, the level of prior, deterministic knowledge (left) can be used to differentiate methods (second right) and applications (right) across which scientific advancements can be made by means of dedicated AI systems.This also allows for various modes of discovery (second left), ranging from cases where machine learning enables discovery by allowing for efficient computational usage, parametric sweeps, etc., to cases where machine learning is used to causally infer underlying mechanistic behaviours in complex multidisciplinary systems.</p>
<p>Figure 1 .
1
Figure 1.Schematic representation of the various applications of ML for scientific discovery, depending on the amount of knowledge available in each category.A number of examples are provided, including brain research, drug discovery, dark matter and fluid mechanics.</p>
<p>Figure 2 .
2
Figure 2. Visual summary of the paper illustrating some of its main ideas.(Top left) The increase in the amount of data generated by scientific instrumentation over time and the shift from data organization by humans, computers, and finally by machine-learning techniques, which translates into less observation, intervention and understanding of humans on scientific discoveries.(Top right) The four challenges we have identified in the paper on the use of ML techniques for scientific discovery, these being: data quality and availability, potential biases, explainability and overfitting.(Bottom left) What constitutes a scientific discovery and the possibility of ML making original breakthroughs versus simply rediscovering known ideas, concepts or laws.(Bottom right) The need to have more and better data to be able to make scientific discoveries with ML as we have less knowledge on the subject of study</p>
<p>/19 
/19 
/19 
AcknowledgementsThe following researchers are acknowledged for helpful discussions during the preparation of this article: Frida Bender, Annica Ekman, Inga Koszalka, Romit Maulik, Henrik Nielsen, Gunilla Svensson, Björn Wallner.RV and HA acknowledge SeRC and Digital Futures for funding the workshop that initiated this work.RV acknowledges financial support from ERC grant no.'2021-CoG-101043998, DEEPCONTROL'.DM acknowledges financial support from ERC grant no.2022-StG-101075494, MultiPRESS.Views and opinions expressed are, however, those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council.Neither the European Union nor the granting authority can be held responsible.AE was funded by the Vetenskapsrådet Grant No. 2021-03979 and the Knut and Alice Wallenberg Foundation and by SeRC.SLB acknowledges funding support from the US National Science Foundation AI Institute in Dynamic Systems (grant number 2112085) and from The Boeing Company.Author contributionsRV and HA initiated the idea for this article following a workshop celebrated in November 2022 at KTH.All the authors contributed equally to the rest of this work.Competing interestsThe authors declare no competing interests.Publisher's noteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.19/19
History of scientific instrumentation and history of science. P Tucci, 10.1007/978-3-031-26174-9_6A History of Physics: Phenomena, Ideas and Mechanisms. R Pisano, ChamSpringer202542</p>
<p>Science in an exponential world. A Szalay, J Gray, 10.1038/440413aNature. 4402006</p>
<p>Big data: The power of petabytes. M Eisenstein, 10.1038/527S2aNature. 5272015</p>
<p>Artificial intelligence-a new knowledge and decision-making paradigm?. L Huang, W Peissl, 10.1007/978-3-031-10617-0_9Technology Assessment in a Globalized World. Hennen, L. et al.2023Springer</p>
<p>The ethics of using artificial intelligence in scientific research: new guidance needed for a new tool. D Resnik, M Hosseini, 10.1007/s43681-024-00493-8AI Ethics. 2024</p>
<p>Recommendation on the ethics of artificial intelligence. UNESCO. 2022</p>
<p>Machine learning and artificial intelligence in neuroscience: A primer for researchers. F Badrulhisham, E Pogatzki-Zahn, D Segelcke, T Spisak, J Vollert, 10.1016/j.bbi.2023.11.005Brain, Behav. Immun. 1152024</p>
<p>How ai is being used to accelerate clinical trials. M Hutson, Nature. 6272024</p>
<p>Alleviating the transit timing variation bias in transit surveys -i. rivers: Method and detection of a pair of resonant super-earths around kepler-1705. A Leleu, 10.1051/0004-6361/202141471Astron. &amp; Astrophys. 6552021</p>
<p>Machine learning at the energy and intensity frontiers of particle physics. A Radovic, M Williams, D Rousseau, 10.1038/s41586-018-0361-2Nature. 5602018</p>
<p>Advancing mathematics by guiding human intuition with ai. A Davies, P Veličković, L Buesing, 10.1038/s41586-021-04086-xNature. 6002021</p>
<p>Magnetic control of tokamak plasmas through deep reinforcement learning. Degrave, Nature. 6022022</p>
<p>How AI is improving climate forecasts. C Wong, Nature. 2024</p>
<p>Discovery of a structural class of antibiotics with explainable deep learning. F Wong, 10.1038/s41586-023-06887-8Nature. 6262024</p>
<p>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. C Rudin, Nat. Mach. Intell. 12019</p>
<p>Interpretable deep-learning models to help achieve the Sustainable Development Goals. R Vinuesa, B Sirmacek, Nat. Mach. Intell. 39262021</p>
<p>Gene expression programming: mathematical modeling by an artificial intelligence. C Ferreira, 2006Springer21</p>
<p>Distilling free-form natural laws from experimental data. M Schmidt, H Lipson, Science. 3242009</p>
<p>Discovering governing equations from data by sparse identification of nonlinear dynamical systems. S L Brunton, J L Proctor, J N Kutz, Proc. Natl. Acad. Sci. 1132016</p>
<p>Scaling deep learning for materials discovery. A Merchant, 10.1038/s41586-023-06735-9Nature. 6242023</p>
<p>Does the sun rise for ChatGPT? Scientific discovery in the age of generative AI. D Leslie, AI Ethics. 2023</p>
<p>The unreasonable effectiveness of data. A Halevy, P Norvig, F Pereira, IEEE intelligent systems. 242009</p>
<p>The unreasonable effectiveness of deep learning in artificial intelligence. T J Sejnowski, Proc. Natl. Acad. Sci. Natl. Acad. Sci2020117</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, Nature. 6202023</p>
<p>The future of fundamental science led by generative closed-loop artificial intelligence. H Zenil, arXiv:2307.075222023Prepr.</p>
<p>Chatgpt: five priorities for research. E A Van Dis, J Bollen, W Zuidema, R Van Rooij, C L Bockting, Nature. 6142023</p>
<p>Optimizing language models for dialogue. Openai, Chatgpt, 2022</p>
<p>Augmenting large language models with chemistry tools. A Bran, 10.1038/s42256-024-00832-8Nat Mach Intell. 62024</p>
<p>Autonomous chemical research with large language models. D Boiko, R Macknight, B Kline, G Gomes, 10.1038/s41586-023-06792-0Nature. 6242023</p>
<p>Q Wang, D Downey, H Ji, T Hope, Scimon, arXiv:2305.14259Scientific inspiration machines optimized for novelty. 2023arXiv preprint</p>
<p>Evolutionary-scale prediction of atomic-level protein structure with a language model. Z Lin, Science. 3792023</p>
<p>Learning inverse folding from millions of predicted structures. C Hsu, 10.1101/2022.04.10.4877792022</p>
<p>Foundation models for weather and climate data understanding: A comprehensive survey. S Chen, G Long, J Jiang, D Liu, C Zhang, arXiv:2312.030142023arXiv preprint</p>
<p>J Wei, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Are emergent abilities of large language models a mirage?. R Schaeffer, B Miranda, S Koyejo, Adv. Neural Inf. Process. Syst. 362024</p>
<p>More is different: Broken symmetry and the nature of the hierarchical structure of science. P W Anderson, Science. 1771972</p>
<p>Science in the age of large language models. A Birhane, A Kasirzadeh, D Leslie, S Wachter, Nat. Rev. Phys. 52023</p>
<p>Fundamental limits to learning closed-form mathematical models from data. O Fajardo-Fontiveros, Nat. Commun. 1410432023</p>
<p>W Bechtel, R Richardson, Discovering Complexity. PrincetonPrinceton University Press1993</p>
<p>Y Ben-Menahem, Causation in Science. PrincetonPrinceton University Press2018</p>
<p>On scientific understanding with artificial intelligence. M Krenn, R Pollice, S Y Guo, 10.1038/s42254-022-00518-3Nat. Rev. Phys. 42022</p>
<p>How ai is shaping scientific discovery. N A Of Sciences, Proceedings of a Workshop. a Workshop2023. 2024AI for Scientific Discovery</p>
<p>A deep learning approach to antibiotic discovery. J M Stokes, K Yang, K Swanson, 10.1016/j.cell.2020.01.021Cell. 1802020</p>
<p>AI and the new age of AI. 2023</p>
<p>Evolving scientific discovery by unifying data and background knowledge with ai hilbert. Cory-Wright , R Cornelio, C Dash, S , 10.1038/s41467-024-50074-wNat. Commun. 152024</p>
<p>Combining data and theory for derivable scientific discovery with ai-descartes. C Cornelio, S Dash, V Austel, 10.1038/s41467-023-37236-yNat. Commun. 142023</p>
<p>Artificial intelligence in science: An emerging general method of invention. S Bianchini, M Müller, P Pelletier, 10.1016/j.respol.2022.104604Res. Policy. 511046042022</p>
<p>The mathematical theory of non-uniform gases: an account of the kinetic theory of viscosity, thermal conduction and diffusion in gases. S Chapman, T G Cowling, 1990Cambridge university press</p>
<p>Existence and smoothness of the Navier-Stokes equation. The millennium prize problems. C L Fefferman, 20005767</p>
<p>Identifying regions of importance in wall-bounded turbulence through explainable deep learning. A Cremades, Nat. Commun. 1538642024</p>
<p>Information-theoretic formulation of dynamical systems: Causality, modeling, and control. A Lozano-Durán, G Arranz, Phys. Rev. Res. 4231952022</p>
<p>Turbulence modeling in the age of data. K Duraisamy, G Iaccarino, H Xiao, Annu. Rev. Fluid Mech. 512019</p>
<p>Machine learning for fluid mechanics. S L Brunton, B R Noack, P Koumoutsakos, Annu. Rev. Fluid Mech. 522020</p>
<p>Predicting the long-term stability of compact multiplanet systems. D Tamayo, Proc. Natl. Acad. Sci. Natl. Acad. Sci2020117</p>
<p>Mastering the game of Go with deep neural networks and tree search. Silver, Nature. 5292016</p>
<p>Optimizing thermodynamic trajectories using evolutionary and gradient-based reinforcement learning. C Beeler, Phys. Rev. E. 104641282021</p>
<p>β -Variational autoencoders and transformers for reduced-order modelling of fluid flows. A Solera-Rico, Nat. Commun. 1513612014</p>
<p>Optimization of physical quantities in the autoencoder latent space. S Park, Sci. Reports. 1290032022</p>
<p>Generative adversarial networks. J Goodfellow, arXiv:1406.26612014Prepr.</p>
<p>Machine learning in high energy physics community white paper. K Albertsson, Journal of Physics: Conference Series. IOP Publishing2018108522008</p>
<p>A. Discovering faster matrix multiplication algorithms with reinforcement learning. Fawzi, Nature. 6102022</p>
<p>Mathematical discoveries from program search with large language models. B Romera-Paredes, Nature. 6252024</p>
<p>Relating SARS-CoV-2 variants using cellular automata imaging. L F Souza, T M Rocha Filho, M A Moret, Sci. Reports. 12102972022</p>
<p>Active turbulence. R Alert, J Casademunt, J.-F Joanny, Annu. Rev. Condens. Matter Phys. 132022</p>
<p>Discovering symbolic models from deep learning with inductive biases. M Cranmer, 34th Conf. on Neural Inf. Vancouver, CanNeurIPS 2020. 2020</p>
<p>Z Liu, Y Chen, Y Du, M Tegmark, arXiv:2109.13901Physics-augmented learning: A new paradigm beyond physicsinformed learning. 2021arXiv preprint</p>
<p>A thermodynamics-informed active learning approach to perception and reasoning about fluids. B Moya, A Badías, D González, F Chinesta, E Cueto, Comput. Mech. 722023</p>
<p>A probabilistic graphical model foundation for enabling predictive digital twins at scale. M G Kapteyn, J V R Pretorius, K E Willcox, Nat. Comput. Sci. 12021</p>
<p>Symmetry-aware autoencoders: s-PCA and s-NLPCA. S Kneer, T Sayadi, D Sipp, P Schmid, G Rigas, arXiv:2111.02893v32022</p>
<p>A unified framework to enforce, discover, and promote symmetry in machine learning. S E Otto, N Zolman, J N Kutz, S L Brunton, arXiv:2311.00212Prepr. 2023</p>
<p>Automated discovery of generalized standard material models with euclid. M Flaschel, S Kumar, L De Lorenzis, Comput. Methods Appl. Mech. Eng. 4051158672023</p>
<p>Discovery of algebraic Reynolds-stress models using sparse symbolic regression. Flow. M Schmelzer, R P Dwight, P Cinnella, Turbul. Combust. 1042020</p>
<p>Establish algebraic data-driven constitutive models for elastic solids with a tensorial sparse symbolic regression method and a hybrid feature selection technique. M Wang, C Chen, W Liu, J. Mech. Phys. Solids. 1591047422022</p>
<p>Digital rheometer twins: Learning the hidden rheology of complex fluids through rheology-informed graph neural networks. M Mahmoudabadbozchelou, K M Kamani, S A Rogers, S Jamali, Proc. Natl. Acad. Sci. Natl. Acad. Sci2022119e2202234119</p>
<p>Highly accurate protein structure prediction with alphafold. J Jumper, 15/19Nature. 5962021</p>
<p>Evolutionary-scale prediction of atomic-level protein structure with a language model. Z Lin, Science. 3792023</p>
<p>Large language models generate functional protein sequences across diverse families. A Madani, 10.1038/s41587-022-01618-2Nat Biotechnol. 412023</p>
<p>Robust deep learning-based protein sequence design using ProteinMPNN. J Dauparas, 10.1126/science.add2187Science. 3782022</p>
<p>De novo design of protein structure and function with RFdiffusion. J Watson, 10.1038/s41586-023-06415-8Nature. 6202023</p>
<p>Illuminating protein space with a programmable generative model. J Ingraham, 10.1038/s41586-023-06728-8Nature. 6232023</p>
<p>Combining data and theory for derivable scientific discovery with ai-descartes. C Cornelio, Nat. Commun. 1417772023</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. R Irwin, S Dimitriadis, J He, E J Bjerrum, Mach. Learn. Sci. Technol. 3150222022</p>
<p>Active learning machine learns to create new quantum experiments. A A Melnikov, Proc. Natl. Acad. Sci. Natl. Acad. Sci2018115</p>
<p>Quantum machine learning: from physics to software engineering. A Melnikov, M Kordzanganeh, A Alodjants, R.-K Lee, Adv. Physics: X. 821654522023</p>
<p>A posteriori learning for quasi-geostrophic turbulence parametrization. H Frezat, J Sommer, R Fablet, G Balarac, R Lguensat, J. Adv. Model. Earth Syst. 142022</p>
<p>A review of recent and emerging machine learning applications for climate variability and weather phenomena. M J Molina, Artif. Intell. for Earth Syst. 22200862023</p>
<p>Distributed coding of choice, action and engagement across the mouse brain. N A Steinmetz, P Zatka-Haas, M Carandini, K D Harris, 10.1038/s41586-019-1787-xNature. 5762019Nature Publishing GroupNumber: 7786 Publisher</p>
<p>A whole-brain monosynaptic input connectome to neuron classes in mouse visual cortex. S Yao, Nat. neuroscience. 262023</p>
<p>Functional connectomics spanning multiple areas of mouse visual cortex. M Consortium, BioRxiv. 72021. 2021</p>
<p>The Hodgkin-Huxley model. The Book Genes. M Nelson, J Rinzel, 1995</p>
<p>Symbolic genetic algorithm for discovering open-form partial differential equations (sga-pde). Y Chen, Y Luo, Q Liu, H Xu, D Zhang, Phys. Rev. Res. 4231742022</p>
<p>Discover: Deep identification of symbolic open-form PDEs via enhanced reinforcement-learning. M Du, Y Chen, D Zhang, arXiv:2210.021812022</p>
<p>Neural ordinary differential equations. Adv. neural information processing systems. R T Chen, Y Rubanova, J Bettencourt, D K Duvenaud, 201831</p>
<p>Predicting ordinary differential equations with transformers. S Becker, M Klein, A Neitz, G Parascandolo, N Kilbertus, International Conference on Machine Learning. 1978-2002 (PMLR, 2023</p>
<p>Learning equations for extrapolation and control. S Sahoo, C Lampert, G Martius, International Conference on Machine Learning. Pmlr2018</p>
<p>Development and validation of an interpretable deep learning framework for alzheimer's disease classification. S Qiu, 16/19Brain. 1432020</p>
<p>Machine learning for perturbational single-cell omics. Y Ji, M Lotfollahi, F A Wolf, F J Theis, Cell Syst. 122021</p>
<p>A self-driving laboratory advances the pareto front for material properties. B P Macleod, Nat. Commun. 139952022</p>
<p>Model-based reinforcement learning for semi-Markov decision processes with neural ODEs. J Du, J Futoma, F Doshi-Velez, Adv. Neural Inf. Process. Syst. 332020</p>
<p>Representation learning: A review and new perspectives. Y Bengio, A Courville, P Vincent, 201335</p>
<p>Toward causal representation learning. B Schölkopf, Proc. IEEE. IEEE2021109</p>
<p>Discovering causal relations and equations from data. G Camps-Valls, Phys. Reports. 10442023</p>
<p>Inferring causation from time series in Earth system sciences. J Runge, Nat. Commun. 1025532019</p>
<p>Molecular causality in the advent of foundation models. S Lobentanzer, P Rodriguez-Mier, S Bauer, J Saez-Rodriguez, arXiv:2401.095582024Prepr</p>
<p>Learning stable and predictive structures in kinetic systems. N Pfister, S Bauer, J Peters, Proc. Natl. Acad. Sci. Natl. Acad. Sci2019116</p>
<p>Data-driven discovery of coordinates and governing equations. K Champion, B Lusch, J N Kutz, S L Brunton, Proc. Natl. Acad. Sci. Natl. Acad. Sci2019116</p>
<p>Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. C Lu, Adv. Neural Inf. Process. Syst. 352022</p>
<p>High-resolution image synthesis with latent diffusion models. R Rombach, A Blattmann, D Lorenz, P Esser, B Ommer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>De novo design of protein structure and function with rfdiffusion. J L Watson, Nature. 6202023</p>
<p>Mattergen: a generative model for inorganic materials design. C Zeni, arXiv:2312.036872023Prepr.</p>
<p>How transformers learn causal structure with gradient descent. E Nichani, A Damian, J D Lee, arXiv:2402.147352024Prepr.</p>
<p>Generating realistic neurophysiological time series with denoising diffusion probabilistic models. J Vetter, J H Macke, R Gao, 2023</p>
<p>Segment anything. A Kirillov, 2304.026432023</p>
<p>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning. A Mathis, 10.1038/s41593-018-0209-yNat. Neurosci. 212018Nature Publishing GroupNumber: 9 Publisher</p>
<p>Genome-scale language models reveal sars-cov-2 evolutionary dynamics. M Zvyagin, The Int. J. High Perform. Comput. Appl. 372023</p>
<p>Data science opportunities of large language models for neuroscience and biomedicine. D Bzdok, Neuron. 1122024</p>
<p>Accurate predictions on small data with a tabular foundation model. N Hollmann, 10.1038/s41586-024-08328-6Nature. 6372025</p>
<p>A study of challenges and limitations to applying machine learning to highly unstructured data. S Singh, S Hooda, 10.1109/ICCUBEA58933.2023.1039211517/192023 7th International Conference On Computing, Communication, Control And Automation (ICCUBEA). Pune, India2023</p>
<p>A causal perspective on dataset bias in machine learning for medical imaging. C Jones, D C Castro, F De Sousa Ribeiro, 10.1038/s42256-024-00797-8Nat. Mach. Intell. 62024</p>
<p>Demographic bias in misdiagnosis by computational pathology models. A Vaidya, R J Chen, D F K Williamson, 10.1038/s41591-024-02885-zNat. Medicine. 302024</p>
<p>Overcoming the pitfalls and perils of algorithms: A classification of machine learning biases and mitigation methods. B Van Giffen, D Herhausen, T Fahse, 10.1016/j.jbusres.2022.01.076J. Bus. Res. 1442022</p>
<p>Breaking into the black box of artificial intelligence. N Savage, 10.1038/d41586-022-00858-1Nature. 2022</p>
<p>Explainable machine learning in materials science. X Zhong, B Gallagher, S Liu, 10.1038/s41524-022-00884-7npj Comput. Mater.. 82022</p>
<p>Explainable chemical artificial intelligence from accurate machine learning of real-space chemical descriptors. M Gallegos, V Vassilev-Galindo, I Poltavsky, 10.1038/s41467-024-48567-9Nat. Commun. 152024</p>
<p>Explainable machine learning can outperform cox regression predictions and provide insights in breast cancer survival. A Moncada-Torres, M C Van Maaren, M P Hendriks, 10.1038/s41598-021-86327-7Sci. Reports. 112021</p>
<p>Model selection and overfitting. J Lever, M Krzywinski, N Altman, 10.1038/nmeth.3968Nat. Methods. 132016</p>
<p>Welcome to the ai future?. 10.1038/s41550-023-01891-4Nat. Astron. 712023</p>
<p>Current progress and open challenges for applying deep learning across the biosciences. N Sapoval, A Aghazadeh, M G Nute, 10.1038/s41467-022-29268-7Nat. Commun. 132022</p>
<p>Generalizing ai: Challenges and opportunities for plug and play ai solutions. I A Ridhawi, S Otoum, M Aloqaily, A Boukerche, 10.1109/MNET.011.200037IEEE Netw. 352021</p>
<p>The open reaction database. 10.1021/jacs.1c09820J. Am. Chem. Soc. 1432021</p>
<p>An open-access database and analysis tool for perovskite solar cells based on the FAIR data principles. T J Jacobsson, A Hultqvist, A García-Fernández, 10.1038/s41560-021-00941-3Nat. Energy. 72022</p>
<p>Crystallography open database-an open-access collection of crystal structures. S Gražulis, J. Appl. Crystallogr. 422009</p>
<p>Reproducibility and replicability in science. L Barba, 2019National Academies Press</p>
<p>Systematic review of data-centric approaches in artificial intelligence and machine learning. P Singh, 10.1016/j.dsm.2023.06.001Data Sci. Manag. 62023</p>
<p>Algorithmic fairness and bias mitigation for clinical machine learning with deep reinforcement learning. J Yang, A A S Soltan, D W Eyre, 10.1038/s42256-023-00697-3Nat. Mach. Intell. 52023</p>
<p>Considerations for addressing bias in artificial intelligence for health equity. npj Digit. M D Abràmoff, M E Tarver, N Loyo-Berrios, 10.1038/s41746-023-00913-9Medicine. 62023</p>
<p>Artificial intelligence-enhanced quantum chemical method with broad applicability. P Zheng, R Zubatyuk, W Wu, 10.1038/s41467-021-27082-9Nat. Commun. 1270222021</p>
<p>Program good ethics into artificial intelligence. J Davies, 10.1038/538291aNature. 2016</p>
<p>Machine learning and algorithmic fairness in public and population health. V Mhasawade, Y Zhao, R Chunara, 10.1038/s42256-021-00373-4Nat. Mach. Intell. 3</p>
<p>Accelerated, scalable and reproducible ai-driven gravitational wave detection. E A Huerta, A Khan, X Huang, 10.1038/s41550-021-01405-0Nat. Astron. 52021</p>
<p>Space missions out of this world with ai. 10.1038/s42256-023-00643-3Nat. Mach. Intell. 52023</p>
<p>A deep-learning search for technosignatures from 820 nearby stars. P X Ma, C Ng, L Rizk, 10.1038/s41550-022-01872-zNat. Astron. 72023</p>
<p>The Open Universe: An Argument for Indeterminism From the Postscript to The Logic of Scientific Discovery. K Popper, 1992Routledge</p>
<p>The importance of species interactions in eco-evolutionary community dynamics under climate change. A Åkesson, A Curtsdotter, A Eklöf, 10.1038/s41467-021-24977-xNat. Commun. 1247592021</p>
<p>Physics Nobel scooped by machine-learning pioneers. Nature. 2024</p>
<p>Chemistry Nobel goes to developers of AlphaFold AI that predicts protein structures. Nature. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
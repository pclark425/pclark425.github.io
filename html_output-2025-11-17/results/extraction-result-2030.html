<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2030 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2030</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2030</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-49.html">extraction-schema-49</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-281394802</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.14380v2.pdf" target="_blank">CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks</a></p>
                <p><strong>Paper Abstract:</strong> Multi-Agent Reinforcement Learning (MARL) provides a powerful framework for learning coordination in multi-agent systems. However, applying MARL to robotics still remains challenging due to high-dimensional continuous joint action spaces, complex reward design, and non-stationary transitions inherent to decentralized settings. On the other hand, humans learn complex coordination through staged curricula, where long-horizon behaviors are progressively built upon simpler skills. Motivated by this, we propose CRAFT: Coaching Reinforcement learning Autonomously using Foundation models for multi-robot coordination Tasks, a framework that leverages the reasoning capabilities of foundation models to act as a"coach"for multi-robot coordination. CRAFT automatically decomposes long-horizon coordination tasks into sequences of subtasks using the planning capability of Large Language Models (LLMs). In what follows, CRAFT trains each subtask using reward functions generated by LLM, and refines them through a Vision Language Model (VLM)-guided reward-refinement loop. We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation tasks, demonstrating its capability to learn complex coordination behaviors. In addition, we validate the multi-quadruped navigation policy in real hardware experiments.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2030.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2030.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRAFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coaching Reinforcement learning Autonomously using Foundation models for Multi-Robot Coordination Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end framework that uses foundation models (LLMs and VLMs) as a coach to automatically generate staged curricula, synthesize executable reward functions, evaluate policies visually, and iteratively refine rewards to train multi-agent coordination policies in long-horizon robotics tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated (multi-candidate generation + LLM-based refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>A curriculum LLM is prompted with a natural-language task and environment description to produce multiple candidate curricula (I candidates). These candidates are re-provided to the curriculum LLM with environment variables and constraints and the LLM refines/merges them into a final curriculum of up to 5 subtasks described in natural language and grounded to environment state variables. Prompts include example reward functions and helper functions; curricula explicitly specify subtask objectives using available state variables (e.g., Euclidean distance to handle). The pipeline pairs each subtask with an LLM-generated executable reward function and uses a VLM to evaluate rollouts and provide advice for iterative reward refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gpt-4o-2024-08-06 (LLM for curriculum and reward generation); o4-mini-2025-04-16 used as VLM for evaluation/advice</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>robotic multi-agent coordination (multi-quadruped navigation and bimanual manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Long-horizon, high-dimensional continuous-control tasks with partial observability and decentralized control. Curricula consist of 2–5 subtasks (examples show 4–5), each building prerequisite skills (e.g., approach → grasp → initial lift → final lift), requiring temporal composition of sub-skills such as alignment, grasping, balancing, and coordinated lifting or coordinated passing through a gate.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>env_reward (environment-provided dense reward, no curriculum), example_reward (example reward provided to LLM, no curriculum), no_curriculum (final CRAFT-generated reward function used without staged curriculum), no_refinement (CRAFT pipeline without VLM-guided reward refinement; resamples rewards from reward LLM instead)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics are episode success rate (percentage) and effective curricula ratio (fraction of curricula achieving nonzero success across 5 trials). CRAFT maximum success rates observed: Quadruped Gate up to 90%, Quadruped Seesaw up to 60%, Two Arm Lift up to 100%. env_reward: 0% success in most experiments; example_reward: <10% success in every environment. no_refinement: lower effective curricula ratio than CRAFT and lower average success rate, but when it produced a successful curriculum its maximum success rate could be comparable to CRAFT (exact table values not printed in text). Hardware zero-shot transfer (Quadruped Gate): 65% success over 20 runs.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>No explicit numeric convergence times or episode counts reported; qualitative claim: curriculum training enabled learning policies that vanilla MARL (env_reward/example_reward) could not learn — i.e., curriculum improved sample efficiency and enabled success where direct training failed. The paper reports success-rate learning curves (top-3 runs) but does not provide absolute episode-to-convergence numbers in text.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Zero-shot sim-to-real transfer evaluated for Quadruped Gate: policy trained in simulation with CRAFT achieved 65% success over 20 hardware runs without additional fine-tuning, indicating partial real-world generalization for the learned coordination behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Paper defines "effective curricula ratio" (how many curricula out of 5 experiments achieved nonzero success). CRAFT attains a higher effective curricula ratio than the no_refinement baseline, indicating more consistent generation of effective curricula. No further quantitative task-diversity metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Curriculum LLM explicitly produces prerequisite subtasks (e.g., approach/alignment before grasp, grasp before initial lift). The method provides past-task descriptions and rewards to the LLM when generating subsequent subtasks, encouraging prerequisite chaining. The paper presents qualitative examples showing the LLM produced sensible prerequisite orders (approach → align → grasp → lift).</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Yes — the LLM generates intermediate/bridging subtasks (examples: Approach Handles, Position & Align, Grasp Coordination, Initial Lifting, Controlled Elevation, Synchronized Full Lift). These intermediate tasks were effective: staged training on such subtasks enabled success where flat training on final reward failed.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Stochasticity of LLM outputs causing variable curricula (overly simple, overly long, or misaligned curricula). VLM/LLM limitations: VLMs lack native 3D motion understanding so the pipeline supplies derived task-relevant metrics; occasional failed one-shot rewards requiring iterative refinement. Overall variability across trials due to foundation model stochasticity is noted as a key limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>No numeric compute/time cost reported. Practical choices: they run up to J=3 reward-refinement iterations (found sufficient), generate multiple curriculum candidates and run multiple simulation training attempts to select successful policies. The paper notes running multiple trials in simulation to find successful curricula due to stochasticity but gives no compute-hours or GPU counts.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No formal human expert evaluation reported; however, reward/reward-component shaping tips and helper functions are human-provided and prompts constrain LLM behavior. Curricula were inspected qualitatively (examples shown) but no quantitative human scoring reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>LLM-generated curricula plus VLM-guided reward refinement (CRA FT) reliably produce effective staged curricula and semantically rich executable rewards enabling MARL to learn complex multi-robot coordination behaviors that env-provided dense rewards or example rewards (without staged curricula) fail to learn; VLM-guided refinement increases reliability (effective curricula ratio), and learned policies can transfer zero-shot to real hardware (65% success on Quadruped Gate). Main failure mode is variability from foundation-model stochasticity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2030.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2030.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curriculum LLM Module</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum Generation Large Language Model Module (within CRAFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-driven LLM module that generates multiple candidate curricula from a natural-language task/environment description and refines them into a final grounded sequence of subtasks using an additional refinement prompt and example context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated with multi-candidate generation and in-prompt refinement</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Two-stage prompting: (1) generate up to I candidate curricula using a prompt that constrains task variables and limits number of subtasks (max 5), (2) provide the candidates back to the LLM with environment/state variables and ask it to select and refine a coherent final curriculum. Prompts instruct the LLM to ground subtask descriptions in provided state variables and to include reward-relevant variables (e.g., distances, tilt cosine). Example reward functions and helper functions are included in the prompt for context.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gpt-4o-2024-08-06</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>robotic coordination tasks (multi-quadruped navigation, bimanual manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Produces 2–5 subtasks which decompose long-horizon tasks into ordered prerequisite skills (e.g., approach → grasp → lift phases).</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared indirectly against no_curriculum (fixed final reward without staged subtasks) in experiments to test necessity of staged curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When used in full CRAFT pipeline, produced curricula that led to top observed success rates (Gate: 90%, Seesaw: 60%, Two Arm Lift: 100%). No per-module ablation rates isolated for curriculum-LLM alone in text.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not numerically reported for this module alone.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Curriculum outputs resulted in policies that could transfer zero-shot to hardware for Quadruped Gate (65% success).</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Curriculum LLM generates multiple candidates for diversity and then refines, mitigating single-output instability; effective curricula ratio used to measure resulting diversity/utility at system level.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Curriculum LLM explicitly produces prerequisite steps and is guided not to change environment/termination conditions; example outputs show correct ordering of prerequisites.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Yes — forms intermediate subtasks that bridge initial behaviors to final objective.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Outputs are stochastic; zero-shot generation alone can be unstable (too simple/too long/misaligned), motivating multi-candidate generation and refinement prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Prompting and candidate generation overhead implied but not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not explicitly evaluated by human experts in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>A two-stage LLM prompting strategy (candidate generation + refinement) produces more coherent and effective curricula than naive single-pass LLM generation, but stochasticity remains and multiple candidates and refinement are needed to increase reliability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2030.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2030.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLM-guided Reward-Refinement Loop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vision-Language Model-guided Reward Refinement Loop</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-model refinement loop where a VLM evaluates policy rollouts (images + state trajectories), produces free-form advice using reward-component learning-curve visualizations, and an LLM refines the executable reward function iteratively (up to J iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>iterative VLM-evaluated and LLM-refined reward shaping (not a curriculum generator per se but part of curriculum reliability)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>After training a policy on a subtask with an LLM-generated base reward R^0_k, the pipeline: (1) uses a VLM to label success/failure from rollout snapshots and state trajectories; (2) if failure, the VLM generates targeted advice based on failure reasons and graphical learning-curve images of reward components; (3) a refine-LLM ingests the advice and the failed reward code to produce a revised executable reward R^{j+1}_k; (4) retrain up to J iterations (J=3 used in experiments). If still failing, pick best policy among attempts and proceed.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>VLM: o4-mini-2025-04-16; LLMs: gpt-4o-2024-08-06 for reward refinement</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>robotic multi-agent tasks requiring visual evaluation (bimanual Two Arm Lift, quadruped tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Targets reward components for subtasks with multi-component signals (e.g., lift_reward vs balance_reward) and leverages component learning curves to detect weak signals and propose concrete code-level fixes.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to no_refinement baseline which simply re-samples new reward functions from the reward LLM without VLM advice.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>VLM-guided refinement increased 'effective curricula ratio' vs no_refinement, producing more consistent successful curricula. When successful, no_refinement may reach similar maximum success rates, but its average/consistency is worse. Exact numeric effective-ratio values are in Table I (not transcribed in-text). Example: refinement corrected reward weights (e.g., lift weight from 80→200) and removed squaring to produce successful lift behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>No per-method episode count reported; refinement loop typically required ≤3 iterations (J=3 found sufficient) to produce a successful reward in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Reward-refinement contributed to reliable curricula whose trained policies achieved high success and sim-to-real transfer (Gate policy transferred with 65% success).</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Refinement reduces the frequency of curricula collapsing to zero success and increases variability of viable curricula; explicit diversity metrics not provided beyond effective curricula ratio.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not the primary function, but refinement uses the history of past rewards/subtasks to avoid forgetting earlier subtasks (prompts include prior rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Operates at subtask-level reward refinement; does not create new subtasks but reshapes reward components for existing intermediate tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Noisy LLM resampling without VLM guidance (no_refinement) produced unreliable curricula; VLMs have weak 3D motion understanding, so state-derived metrics and visualized reward-component curves are used to support advice generation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Iterative retraining costs implied as significant (multiple full MARL trainings per refinement attempt), but no concrete compute/time numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Advice is machine-generated; no human evaluation of advice quality reported, though examples show plausible human-like suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>The VLM-guided advice plus LLM reward refinement loop materially improves the reliability of generated curricula and final policy success rates by diagnosing weak reward components and proposing targeted code-level fixes; simple LLM resampling without VLM guidance is less reliable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2030.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2030.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline Suite (env_reward / example_reward / no_curriculum / no_refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baselines: environment reward, example reward, no-curriculum training, no-refinement (LLM resampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of baseline approaches used to evaluate CRAFT: (1) env_reward uses environment-provided dense rewards with no curriculum; (2) example_reward uses the example reward provided to the LLM but trained without staged curricula; (3) no_curriculum trains with a final CRAFT-generated reward without sequential staged subtasks; (4) no_refinement omits VLM-guided refinement and instead resamples rewards from the reward LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>env_reward & example_reward: no curriculum; no_curriculum: fixed LLM-generated final reward but no staged curriculum; no_refinement: uses curriculum generation but omits VLM-guided reward-refinement (stochastic LLM resampling instead)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>env_reward: use the environment's provided dense reward and train end-to-end on final task. example_reward: use example reward (simple structured reward) from CRAFT prompts, train end-to-end on final task. no_curriculum: take the final reward function produced by CRAFT (top-performing ones) and train without staging. no_refinement: follow full CRAFT pipeline but when reward refinement is needed, resample new reward functions from reward-generation LLM rather than using VLM advice.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>same multi-robot tasks as CRAFT</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Same long-horizon, high-dimensional tasks; these baselines lack staged decomposition (except no_curriculum which uses LLM reward but not staged progression).</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>These are the baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>env_reward: 0% success in most experiments. example_reward: <10% success in every environment. no_curriculum: lower success rates than CRAFT across all three environments; comparable to CRAFT only in Quadruped Seesaw in some trials (text notes no_curriculum achieves comparable success in Seesaw due to high-quality reward). no_refinement: lower effective curricula ratio and lower average success rate than CRAFT, but when it generated a successful curriculum its maximum success rate could match CRAFT (exact numeric Table I values not transcribed).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not quantified numerically; qualitative claim: absence of staged curriculum or reward refinement leads to convergence to local optima and failure to discover global coordinated strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>No hardware transfer results reported for these baselines; env_reward/example_reward failed to learn reliably in sim so transfer not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Many trials collapse to zero success for poor curricula in resampling baselines (no_refinement), leading to very low effective curricula ratio.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Baselines without curriculum do not identify/prioritize prerequisites explicitly; no_curriculum uses a final reward that may implicitly encode subtasks but lacks staged learning, causing loss of plasticity/entrapment in local optima.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>None (except implicit via reward shaping in no_curriculum final reward).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>For no_refinement, naive LLM resampling without VLM guidance is unreliable due to stochasticity; leads to inconsistent curricula quality.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported; no_refinement likely more wasteful due to many failed curriculum attempts and retraining runs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No human expert evaluation reported comparing these baselines to CRAFT.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Vanilla training on environment or example rewards fails on these long-horizon coordination tasks (0% to <10% success), demonstrating the need for structured curricula and reward refinement. Removing staged curriculum or VLM-guided refinement reduces reliability and average success rates.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2030.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2030.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hardware Transfer Result</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-Real Zero-Shot Transfer Evaluation (Quadruped Gate)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Policy trained in simulation using the full CRAFT pipeline (curriculum + reward-generation + VLM-guided refinement) was deployed on two real quadruped robots (Unitree Go2 and Go1) with minor action scaling, achieving nonzero real-world coordination success.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>Policy trained in simulation with CRAFT-generated staged curriculum and refined rewards; transferred zero-shot to hardware</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Full CRAFT curriculum and reward-refinement produced a coordination policy in sim which was transferred to hardware with component-wise action scaling to account for embodiment differences; no further fine-tuning performed on real robots.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gpt-4o-2024-08-06 (used during training phase in sim), VLM o4-mini-2025-04-16 used in training loop</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>real-world quadruped coordination (narrow gate passing)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Two quadrupeds must coordinate sequential passes through a narrow gate without collision; requires negotiation/implicit waiting and spatial coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>No direct hardware baseline comparisons reported; baseline policies failed in sim and thus not transferred.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot sim-to-real success rate: 65% over 20 runs (no additional real-world fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported for hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Demonstrates partial generalization of simulation-trained, curriculum-led policies to real hardware (65% success), indicating that CRAFT curricula can produce policies with some robustness to sim/real discrepancies.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not analyzed on hardware beyond success trajectories; qualitative trajectory figures provided showing sequential passing behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>N/A for hardware evaluation itself; curriculum in training encoded prerequisites (e.g., approach/wait behaviors).</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Intermediate subtasks learned in sim manifested as waiting/passing behaviors on hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>No additional limitations observed specific to transfer beyond previously mentioned variability; action scaling required to accommodate different low-level controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not stated; hardware experiments involve deployment but no online training.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No human scoring reported; qualitative video/trajectory evidence shown.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Policies trained with CRAFT transferred zero-shot to hardware achieving 65% success, supporting that LLM/VLM-designed curricula and refined rewards can produce policies that generalize beyond simulation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Curricullm: Automatic task curricula design for learning complex robot skills using large language models <em>(Rating: 2)</em></li>
                <li>Environment curriculum generation via large language models <em>(Rating: 2)</em></li>
                <li>Eureka: Human-level reward design via coding large language models <em>(Rating: 2)</em></li>
                <li>Vision-language models are zero-shot reward models for reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-refined large language model as automated reward function designer for deep reinforcement learning in robotics <em>(Rating: 1)</em></li>
                <li>Learning a high-quality robotic wiping policy using systematic reward analysis and visual-language model based curriculum <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2030",
    "paper_id": "paper-281394802",
    "extraction_schema_id": "extraction-schema-49",
    "extracted_data": [
        {
            "name_short": "CRAFT",
            "name_full": "Coaching Reinforcement learning Autonomously using Foundation models for Multi-Robot Coordination Tasks",
            "brief_description": "An end-to-end framework that uses foundation models (LLMs and VLMs) as a coach to automatically generate staged curricula, synthesize executable reward functions, evaluate policies visually, and iteratively refine rewards to train multi-agent coordination policies in long-horizon robotics tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "LLM-generated (multi-candidate generation + LLM-based refinement)",
            "curriculum_method_description": "A curriculum LLM is prompted with a natural-language task and environment description to produce multiple candidate curricula (I candidates). These candidates are re-provided to the curriculum LLM with environment variables and constraints and the LLM refines/merges them into a final curriculum of up to 5 subtasks described in natural language and grounded to environment state variables. Prompts include example reward functions and helper functions; curricula explicitly specify subtask objectives using available state variables (e.g., Euclidean distance to handle). The pipeline pairs each subtask with an LLM-generated executable reward function and uses a VLM to evaluate rollouts and provide advice for iterative reward refinement.",
            "llm_model_used": "gpt-4o-2024-08-06 (LLM for curriculum and reward generation); o4-mini-2025-04-16 used as VLM for evaluation/advice",
            "domain_environment": "robotic multi-agent coordination (multi-quadruped navigation and bimanual manipulation)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Long-horizon, high-dimensional continuous-control tasks with partial observability and decentralized control. Curricula consist of 2–5 subtasks (examples show 4–5), each building prerequisite skills (e.g., approach → grasp → initial lift → final lift), requiring temporal composition of sub-skills such as alignment, grasping, balancing, and coordinated lifting or coordinated passing through a gate.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "env_reward (environment-provided dense reward, no curriculum), example_reward (example reward provided to LLM, no curriculum), no_curriculum (final CRAFT-generated reward function used without staged curriculum), no_refinement (CRAFT pipeline without VLM-guided reward refinement; resamples rewards from reward LLM instead)",
            "performance_metrics": "Reported metrics are episode success rate (percentage) and effective curricula ratio (fraction of curricula achieving nonzero success across 5 trials). CRAFT maximum success rates observed: Quadruped Gate up to 90%, Quadruped Seesaw up to 60%, Two Arm Lift up to 100%. env_reward: 0% success in most experiments; example_reward: &lt;10% success in every environment. no_refinement: lower effective curricula ratio than CRAFT and lower average success rate, but when it produced a successful curriculum its maximum success rate could be comparable to CRAFT (exact table values not printed in text). Hardware zero-shot transfer (Quadruped Gate): 65% success over 20 runs.",
            "learning_speed_comparison": "No explicit numeric convergence times or episode counts reported; qualitative claim: curriculum training enabled learning policies that vanilla MARL (env_reward/example_reward) could not learn — i.e., curriculum improved sample efficiency and enabled success where direct training failed. The paper reports success-rate learning curves (top-3 runs) but does not provide absolute episode-to-convergence numbers in text.",
            "generalization_performance": "Zero-shot sim-to-real transfer evaluated for Quadruped Gate: policy trained in simulation with CRAFT achieved 65% success over 20 hardware runs without additional fine-tuning, indicating partial real-world generalization for the learned coordination behavior.",
            "task_diversity_analysis": "Paper defines \"effective curricula ratio\" (how many curricula out of 5 experiments achieved nonzero success). CRAFT attains a higher effective curricula ratio than the no_refinement baseline, indicating more consistent generation of effective curricula. No further quantitative task-diversity metrics reported.",
            "prerequisite_identification": "Curriculum LLM explicitly produces prerequisite subtasks (e.g., approach/alignment before grasp, grasp before initial lift). The method provides past-task descriptions and rewards to the LLM when generating subsequent subtasks, encouraging prerequisite chaining. The paper presents qualitative examples showing the LLM produced sensible prerequisite orders (approach → align → grasp → lift).",
            "intermediate_task_generation": "Yes — the LLM generates intermediate/bridging subtasks (examples: Approach Handles, Position & Align, Grasp Coordination, Initial Lifting, Controlled Elevation, Synchronized Full Lift). These intermediate tasks were effective: staged training on such subtasks enabled success where flat training on final reward failed.",
            "llm_limitations_observed": "Stochasticity of LLM outputs causing variable curricula (overly simple, overly long, or misaligned curricula). VLM/LLM limitations: VLMs lack native 3D motion understanding so the pipeline supplies derived task-relevant metrics; occasional failed one-shot rewards requiring iterative refinement. Overall variability across trials due to foundation model stochasticity is noted as a key limitation.",
            "computational_cost": "No numeric compute/time cost reported. Practical choices: they run up to J=3 reward-refinement iterations (found sufficient), generate multiple curriculum candidates and run multiple simulation training attempts to select successful policies. The paper notes running multiple trials in simulation to find successful curricula due to stochasticity but gives no compute-hours or GPU counts.",
            "human_expert_evaluation": "No formal human expert evaluation reported; however, reward/reward-component shaping tips and helper functions are human-provided and prompts constrain LLM behavior. Curricula were inspected qualitatively (examples shown) but no quantitative human scoring reported.",
            "key_findings_summary": "LLM-generated curricula plus VLM-guided reward refinement (CRA FT) reliably produce effective staged curricula and semantically rich executable rewards enabling MARL to learn complex multi-robot coordination behaviors that env-provided dense rewards or example rewards (without staged curricula) fail to learn; VLM-guided refinement increases reliability (effective curricula ratio), and learned policies can transfer zero-shot to real hardware (65% success on Quadruped Gate). Main failure mode is variability from foundation-model stochasticity.",
            "uuid": "e2030.0"
        },
        {
            "name_short": "Curriculum LLM Module",
            "name_full": "Curriculum Generation Large Language Model Module (within CRAFT)",
            "brief_description": "A prompt-driven LLM module that generates multiple candidate curricula from a natural-language task/environment description and refines them into a final grounded sequence of subtasks using an additional refinement prompt and example context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "LLM-generated with multi-candidate generation and in-prompt refinement",
            "curriculum_method_description": "Two-stage prompting: (1) generate up to I candidate curricula using a prompt that constrains task variables and limits number of subtasks (max 5), (2) provide the candidates back to the LLM with environment/state variables and ask it to select and refine a coherent final curriculum. Prompts instruct the LLM to ground subtask descriptions in provided state variables and to include reward-relevant variables (e.g., distances, tilt cosine). Example reward functions and helper functions are included in the prompt for context.",
            "llm_model_used": "gpt-4o-2024-08-06",
            "domain_environment": "robotic coordination tasks (multi-quadruped navigation, bimanual manipulation)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Produces 2–5 subtasks which decompose long-horizon tasks into ordered prerequisite skills (e.g., approach → grasp → lift phases).",
            "is_curriculum_adaptive": null,
            "baseline_comparisons": "Compared indirectly against no_curriculum (fixed final reward without staged subtasks) in experiments to test necessity of staged curriculum.",
            "performance_metrics": "When used in full CRAFT pipeline, produced curricula that led to top observed success rates (Gate: 90%, Seesaw: 60%, Two Arm Lift: 100%). No per-module ablation rates isolated for curriculum-LLM alone in text.",
            "learning_speed_comparison": "Not numerically reported for this module alone.",
            "generalization_performance": "Curriculum outputs resulted in policies that could transfer zero-shot to hardware for Quadruped Gate (65% success).",
            "task_diversity_analysis": "Curriculum LLM generates multiple candidates for diversity and then refines, mitigating single-output instability; effective curricula ratio used to measure resulting diversity/utility at system level.",
            "prerequisite_identification": "Curriculum LLM explicitly produces prerequisite steps and is guided not to change environment/termination conditions; example outputs show correct ordering of prerequisites.",
            "intermediate_task_generation": "Yes — forms intermediate subtasks that bridge initial behaviors to final objective.",
            "llm_limitations_observed": "Outputs are stochastic; zero-shot generation alone can be unstable (too simple/too long/misaligned), motivating multi-candidate generation and refinement prompts.",
            "computational_cost": "Prompting and candidate generation overhead implied but not quantified.",
            "human_expert_evaluation": "Not explicitly evaluated by human experts in reported experiments.",
            "key_findings_summary": "A two-stage LLM prompting strategy (candidate generation + refinement) produces more coherent and effective curricula than naive single-pass LLM generation, but stochasticity remains and multiple candidates and refinement are needed to increase reliability.",
            "uuid": "e2030.1"
        },
        {
            "name_short": "VLM-guided Reward-Refinement Loop",
            "name_full": "Vision-Language Model-guided Reward Refinement Loop",
            "brief_description": "A multi-model refinement loop where a VLM evaluates policy rollouts (images + state trajectories), produces free-form advice using reward-component learning-curve visualizations, and an LLM refines the executable reward function iteratively (up to J iterations).",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "iterative VLM-evaluated and LLM-refined reward shaping (not a curriculum generator per se but part of curriculum reliability)",
            "curriculum_method_description": "After training a policy on a subtask with an LLM-generated base reward R^0_k, the pipeline: (1) uses a VLM to label success/failure from rollout snapshots and state trajectories; (2) if failure, the VLM generates targeted advice based on failure reasons and graphical learning-curve images of reward components; (3) a refine-LLM ingests the advice and the failed reward code to produce a revised executable reward R^{j+1}_k; (4) retrain up to J iterations (J=3 used in experiments). If still failing, pick best policy among attempts and proceed.",
            "llm_model_used": "VLM: o4-mini-2025-04-16; LLMs: gpt-4o-2024-08-06 for reward refinement",
            "domain_environment": "robotic multi-agent tasks requiring visual evaluation (bimanual Two Arm Lift, quadruped tasks)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Targets reward components for subtasks with multi-component signals (e.g., lift_reward vs balance_reward) and leverages component learning curves to detect weak signals and propose concrete code-level fixes.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Compared to no_refinement baseline which simply re-samples new reward functions from the reward LLM without VLM advice.",
            "performance_metrics": "VLM-guided refinement increased 'effective curricula ratio' vs no_refinement, producing more consistent successful curricula. When successful, no_refinement may reach similar maximum success rates, but its average/consistency is worse. Exact numeric effective-ratio values are in Table I (not transcribed in-text). Example: refinement corrected reward weights (e.g., lift weight from 80→200) and removed squaring to produce successful lift behavior.",
            "learning_speed_comparison": "No per-method episode count reported; refinement loop typically required ≤3 iterations (J=3 found sufficient) to produce a successful reward in experiments.",
            "generalization_performance": "Reward-refinement contributed to reliable curricula whose trained policies achieved high success and sim-to-real transfer (Gate policy transferred with 65% success).",
            "task_diversity_analysis": "Refinement reduces the frequency of curricula collapsing to zero success and increases variability of viable curricula; explicit diversity metrics not provided beyond effective curricula ratio.",
            "prerequisite_identification": "Not the primary function, but refinement uses the history of past rewards/subtasks to avoid forgetting earlier subtasks (prompts include prior rewards).",
            "intermediate_task_generation": "Operates at subtask-level reward refinement; does not create new subtasks but reshapes reward components for existing intermediate tasks.",
            "llm_limitations_observed": "Noisy LLM resampling without VLM guidance (no_refinement) produced unreliable curricula; VLMs have weak 3D motion understanding, so state-derived metrics and visualized reward-component curves are used to support advice generation.",
            "computational_cost": "Iterative retraining costs implied as significant (multiple full MARL trainings per refinement attempt), but no concrete compute/time numbers provided.",
            "human_expert_evaluation": "Advice is machine-generated; no human evaluation of advice quality reported, though examples show plausible human-like suggestions.",
            "key_findings_summary": "The VLM-guided advice plus LLM reward refinement loop materially improves the reliability of generated curricula and final policy success rates by diagnosing weak reward components and proposing targeted code-level fixes; simple LLM resampling without VLM guidance is less reliable.",
            "uuid": "e2030.2"
        },
        {
            "name_short": "Baseline Suite (env_reward / example_reward / no_curriculum / no_refinement)",
            "name_full": "Baselines: environment reward, example reward, no-curriculum training, no-refinement (LLM resampling)",
            "brief_description": "Set of baseline approaches used to evaluate CRAFT: (1) env_reward uses environment-provided dense rewards with no curriculum; (2) example_reward uses the example reward provided to the LLM but trained without staged curricula; (3) no_curriculum trains with a final CRAFT-generated reward without sequential staged subtasks; (4) no_refinement omits VLM-guided refinement and instead resamples rewards from the reward LLM.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "env_reward & example_reward: no curriculum; no_curriculum: fixed LLM-generated final reward but no staged curriculum; no_refinement: uses curriculum generation but omits VLM-guided reward-refinement (stochastic LLM resampling instead)",
            "curriculum_method_description": "env_reward: use the environment's provided dense reward and train end-to-end on final task. example_reward: use example reward (simple structured reward) from CRAFT prompts, train end-to-end on final task. no_curriculum: take the final reward function produced by CRAFT (top-performing ones) and train without staging. no_refinement: follow full CRAFT pipeline but when reward refinement is needed, resample new reward functions from reward-generation LLM rather than using VLM advice.",
            "llm_model_used": null,
            "domain_environment": "same multi-robot tasks as CRAFT",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Same long-horizon, high-dimensional tasks; these baselines lack staged decomposition (except no_curriculum which uses LLM reward but not staged progression).",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "These are the baselines.",
            "performance_metrics": "env_reward: 0% success in most experiments. example_reward: &lt;10% success in every environment. no_curriculum: lower success rates than CRAFT across all three environments; comparable to CRAFT only in Quadruped Seesaw in some trials (text notes no_curriculum achieves comparable success in Seesaw due to high-quality reward). no_refinement: lower effective curricula ratio and lower average success rate than CRAFT, but when it generated a successful curriculum its maximum success rate could match CRAFT (exact numeric Table I values not transcribed).",
            "learning_speed_comparison": "Not quantified numerically; qualitative claim: absence of staged curriculum or reward refinement leads to convergence to local optima and failure to discover global coordinated strategies.",
            "generalization_performance": "No hardware transfer results reported for these baselines; env_reward/example_reward failed to learn reliably in sim so transfer not applicable.",
            "task_diversity_analysis": "Many trials collapse to zero success for poor curricula in resampling baselines (no_refinement), leading to very low effective curricula ratio.",
            "prerequisite_identification": "Baselines without curriculum do not identify/prioritize prerequisites explicitly; no_curriculum uses a final reward that may implicitly encode subtasks but lacks staged learning, causing loss of plasticity/entrapment in local optima.",
            "intermediate_task_generation": "None (except implicit via reward shaping in no_curriculum final reward).",
            "llm_limitations_observed": "For no_refinement, naive LLM resampling without VLM guidance is unreliable due to stochasticity; leads to inconsistent curricula quality.",
            "computational_cost": "Not reported; no_refinement likely more wasteful due to many failed curriculum attempts and retraining runs.",
            "human_expert_evaluation": "No human expert evaluation reported comparing these baselines to CRAFT.",
            "key_findings_summary": "Vanilla training on environment or example rewards fails on these long-horizon coordination tasks (0% to &lt;10% success), demonstrating the need for structured curricula and reward refinement. Removing staged curriculum or VLM-guided refinement reduces reliability and average success rates.",
            "uuid": "e2030.3"
        },
        {
            "name_short": "Hardware Transfer Result",
            "name_full": "Sim-to-Real Zero-Shot Transfer Evaluation (Quadruped Gate)",
            "brief_description": "Policy trained in simulation using the full CRAFT pipeline (curriculum + reward-generation + VLM-guided refinement) was deployed on two real quadruped robots (Unitree Go2 and Go1) with minor action scaling, achieving nonzero real-world coordination success.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "Policy trained in simulation with CRAFT-generated staged curriculum and refined rewards; transferred zero-shot to hardware",
            "curriculum_method_description": "Full CRAFT curriculum and reward-refinement produced a coordination policy in sim which was transferred to hardware with component-wise action scaling to account for embodiment differences; no further fine-tuning performed on real robots.",
            "llm_model_used": "gpt-4o-2024-08-06 (used during training phase in sim), VLM o4-mini-2025-04-16 used in training loop",
            "domain_environment": "real-world quadruped coordination (narrow gate passing)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Two quadrupeds must coordinate sequential passes through a narrow gate without collision; requires negotiation/implicit waiting and spatial coordination.",
            "is_curriculum_adaptive": null,
            "baseline_comparisons": "No direct hardware baseline comparisons reported; baseline policies failed in sim and thus not transferred.",
            "performance_metrics": "Zero-shot sim-to-real success rate: 65% over 20 runs (no additional real-world fine-tuning).",
            "learning_speed_comparison": "Not reported for hardware.",
            "generalization_performance": "Demonstrates partial generalization of simulation-trained, curriculum-led policies to real hardware (65% success), indicating that CRAFT curricula can produce policies with some robustness to sim/real discrepancies.",
            "task_diversity_analysis": "Not analyzed on hardware beyond success trajectories; qualitative trajectory figures provided showing sequential passing behavior.",
            "prerequisite_identification": "N/A for hardware evaluation itself; curriculum in training encoded prerequisites (e.g., approach/wait behaviors).",
            "intermediate_task_generation": "Intermediate subtasks learned in sim manifested as waiting/passing behaviors on hardware.",
            "llm_limitations_observed": "No additional limitations observed specific to transfer beyond previously mentioned variability; action scaling required to accommodate different low-level controllers.",
            "computational_cost": "Not stated; hardware experiments involve deployment but no online training.",
            "human_expert_evaluation": "No human scoring reported; qualitative video/trajectory evidence shown.",
            "key_findings_summary": "Policies trained with CRAFT transferred zero-shot to hardware achieving 65% success, supporting that LLM/VLM-designed curricula and refined rewards can produce policies that generalize beyond simulation.",
            "uuid": "e2030.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Curricullm: Automatic task curricula design for learning complex robot skills using large language models",
            "rating": 2
        },
        {
            "paper_title": "Environment curriculum generation via large language models",
            "rating": 2
        },
        {
            "paper_title": "Eureka: Human-level reward design via coding large language models",
            "rating": 2
        },
        {
            "paper_title": "Vision-language models are zero-shot reward models for reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Self-refined large language model as automated reward function designer for deep reinforcement learning in robotics",
            "rating": 1
        },
        {
            "paper_title": "Learning a high-quality robotic wiping policy using systematic reward analysis and visual-language model based curriculum",
            "rating": 1
        }
    ],
    "cost": 0.01623425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks</p>
<p>Seoyeon Choi seoyeon99@berkeley.edu 
Seoyeon Choi
Kanghyun Ryu, Jonghoon Ock</p>
<p>Negar Mehr is with Mechanical Engineering
University of California Berkeley</p>
<p>Kanghyun Ryu kanghyun.ryu@berkeley.edu 
Seoyeon Choi
Kanghyun Ryu, Jonghoon Ock</p>
<p>Negar Mehr is with Mechanical Engineering
University of California Berkeley</p>
<p>Jonghoon Ock 
Seoyeon Choi
Kanghyun Ryu, Jonghoon Ock</p>
<p>Negar Mehr is with Mechanical Engineering
University of California Berkeley</p>
<p>Negar Mehr 
Seoyeon Choi
Kanghyun Ryu, Jonghoon Ock</p>
<p>Negar Mehr is with Mechanical Engineering
University of California Berkeley</p>
<p>CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks
13BC51CC84355BA56511AFF3327C671FReward Function Generation Curriculum Generation
Refine curriculum candidates for bimanual manipulation task : lift and balance the potCandidateProvide reward component learning curve imagesReward GenerationTrain policy and collect rollout information Decision: Failure Pot's elevation never exceeds 0.009 m, well below 0.05 m …Evaluation Reward GenerationTrain policy and collect rollout information Decision: Success Pot's elevation recached 0.05 and also maintained a cosine of pot tilt degree consistently above 0.97…</p>
<p>For each subtask M k , a reward LLM generates a reward function R j k and a decentralized policy π j k , shared among agents, is trained via MARL.A VLM evaluates this policy using state variable trajectories and snapshot images.If the policy fails, an advice VLM provides advice from reward component learning curves, plots of individual reward terms during training.The reward LLM incorporates this advice to generate a revised reward.Successful policy advances to the next subtask and this iterative process continues until the final task is reached.(Right) Demonstration of the Quadruped Gate task, where two quadrupeds coordinate to pass through a narrow gate without collision: the blue robot passes first while the orange robot waits; then, the orange robot proceeds once the gate is clear.This policy was learned in simulation and transferred to real-world.</p>
<p>Abstract-Multi-Agent Reinforcement Learning (MARL) provides a powerful framework for learning coordination in multi-agent systems.However, applying MARL to robotics still remains challenging due to high-dimensional continuous joint action spaces, complex reward design, and nonstationary transitions inherent to decentralized settings.On the other hand, humans learn complex coordination through staged curricula, where long-horizon behaviors are progressively built upon simpler skills.Motivated by this, we propose CRAFT: Coaching Reinforcement learning Autonomously using Foundation models for multi-robot coordination Tasks, a framework that leverages the reasoning capabilities of foundation models to act as a "coach" for multi-robot coordination.CRAFT automatically decomposes long-horizon coordination tasks into sequences of subtasks using the planning capability of Large Language Models (LLMs).In what follows, CRAFT trains each subtask using reward functions generated by LLM, and refines them through a Vision Language Model (VLM)-guided reward-refinement loop.We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation tasks, demonstrating its capability to learn complex coordination behaviors.In addition, we validate the multi-quadruped navigation policy in real hardware experiments.Project website is https://iconlab.negarmehr.com/CRAFT/</p>
<p>I. INTRODUCTION</p>
<p>Learning coordinated behaviors is essential for enabling robots to work together in real-world tasks, yet achieving such coordination remains a significant challenge.Multi-Agent Reinforcement Learning (MARL) offers a promising framework, with notable successes in domains such as realtime strategy games [1], [2] and simulated sports [3], and it is increasingly being explored for multi-robot coordination [4], [5].However, applying MARL to robotics is notoriously difficult due to the high-dimensional action spaces and sparse rewards inherent in robotic tasks alongside the nonstationarity introduced by decentralized decision-making.To coordinate effectively, robots need to reason about each other's actions and navigate complex credit assignment problems [6].These challenges become even more pronounced in multi-robot domains that involve physical interaction, longhorizon planning, and intricate cooperation.In this paper, we consider the problem of teaching robots to coordinate with one another by developing an automatic curriculum generation and learning pipeline that enables MARL policies to acquire effective coordination strategies in complex multirobot tasks.</p>
<p>In the physical world, humans learn complex coordination progressively.For example, a soccer team begins with simple arXiv:2509.14380v2[cs.RO] 1 Oct 2025 cooperative drills, such as short passes, before advancing to elaborate team strategies.As such, curriculum generation has been studied as an effective approach for learning coordination and strategies in MARL domains by structuring the training into stages of increasing complexity [7].A curriculum breaks a complex task into a sequence of subtasks where early subtasks help agents learn fundamental skills.As training progresses, long-horizon objectives are decomposed into manageable subtasks that build on these fundamentals, allowing agents to gradually master complicated coordination strategies.Through this staged progression, curriculum generation enables the emergence of complex long-horizon collaborative behaviors that are difficult to achieve when training agents directly on the target task [5], [8], [9].</p>
<p>Despite its effectiveness, curriculum design is often nontrivial as it requires domain knowledge to identify key steps in long-horizon tasks and reasoning ability to monitor the learning progress [10].Meanwhile, recent advances in foundation models, such as Large Language Models (LLMs) and Vision-Language Models (VLMs), have introduced humanlike reasoning capabilities to robot learning.These reasoning capabilities have been leveraged for task decomposition [11], [12], reward shaping [13], [14], and policy evaluation [15], [16].We argue, however, that these models can go beyond such individual applications and instead serve as a coach, i.e. entity that teaches agents how to coordinate.Coaching naturally integrates all these skills: a good coach breaks down the task, defines success criteria for each subtask, and provides actionable feedback to guide improvement.</p>
<p>In this paper, we introduce CRAFT: Coaching Reinforcement learning Autonomously using Foundation models for multi-robot coordination Tasks.CRAFT first decomposes a complex cooperative task into sequences of subtasks that facilitate the learning of intricate multi-agent coordination policies.Then, each subtask is paired with an LLM-generated reward function in executable code, while a VLM provides feedback on trained policies to iteratively refine rewards through a VLM-guided reward-refinement loop.Through this process, CRAFT produces curricula for long-horizon coordination tasks, designs semantically rich reward functions, and evaluates task-policy alignment using visual information.Through experiments in simulation and on physical robots, we demonstrate that CRAFT enables learning coordination behaviors in multi-quadruped navigation and bimanual manipulation tasks that standard learning methods fail to solve.</p>
<p>In summary, our contributions are the following: 1) We propose CRAFT, a framework that automatically designs curricula for long-horizon multi-robot coordination tasks by leveraging the reasoning capabilities of foundation models.2) We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation tasks, which require long-horizon coordination.CRAFT learns complex coordination policies that were infeasible without human-designed curricula or hand-calibrated rewards.3) We validate the multi-quadruped navigation policies learned with CRAFT in hardware experiments, demon-strating their real-world transferability.</p>
<p>II. RELATED WORKS</p>
<p>A. Curriculum Learning and Foundation Models</p>
<p>Curriculum learning accelerates RL by structuring training as a sequence of subtasks with increasing difficulty [8].It has been shown to improve sample efficiency [17], enable the learning of challenging tasks [8], and support multitask learning [18].While many prior works on curriculum design have relied on trial-and-error of human experts [9], [19], more recent efforts have explored leveraging the reasoning capabilities of LLMs for automated curriculum design [20], [21], [22].In this work, we extend curricula design with foundation models to multi-agent settings, where reasoning about agent interactions and long-horizon task decomposition is even more critical.</p>
<p>B. LLM/VLM for Reward Design</p>
<p>Recent works have shown the effectiveness of reasoning and general knowledge of foundation models for reward design.Several works leverage language models to generate reward functions in executable code and further refine them through evolutionary search [13], [23].Others use VLM's ability to evaluate alignment between visual observations and language descriptions, directly querying a VLM for the reward [14] or providing other feedback to the policy, such as success or failure [15], [16], [24].Building on these works, we employ the reward generation capabilities of LLMs to produce reward functions for each subtask in a curriculum, and use a VLM to evaluate trained policies and provide feedback for refining reward functions.</p>
<p>C. Multi-robot Coordination</p>
<p>Several works have explored MARL for multi-robot coordination.For a team of quadrupeds, MARL has been employed in hierarchical structure where it learns highlevel coordination behavior for object manipulation [25], [26] or soccer [4], combined with low-level locomotion policy.However, these methods rely on either carefully designed reward functions [25] or manually specified curricula [26].MARL also has been used for multi-robot navigation with collision avoidance in aerial [27] or mobile robots [28].Notably, curriculum learning frameworks have been actively employed in MARL training [4], [26], [27], demonstrating their effectiveness in learning complex coordination behaviors.Based on these successful curriculum learning experiences in MARL, our aim is to automate curriculum design using foundation models, thereby enabling learning complex coordination behaviors while reducing the need for extensive human intervention in MARL.</p>
<p>III. PROBLEM FORMULATION</p>
<p>In this work, we aim to learn a multi-agent coordination problem where a team of agents learn how to coordinate to achieve a target task M target .We model the target task as a decentralized partially observable Markov decision process (Dec-POMDP [29]), defined by the tuple     M target = ⟨N, S, A, O, P, R target , γ⟩, where N is the number of agents, S is the global state space, and A is the joint action space of the agents.We denote the local observation of each agent i at the global state s as o i = O(s; i).The joint action of all N agents is denoted as a = (a 1 , . . ., a N ), where a i is the action of agent i.The transition probability from state s to s ′ when taking the joint action a is denoted as P (s ′ |s, a).We assume that there exists a shared reward function R target (s, a, s ′ ) that agents need to collectively optimize, and γ is the discount factor.In this work, we train a MARL policy using a centralized training and decentralized execution (CTDE) framework [29], [30], where all agents share the same policy π θ (a i |o i ) that observes each agent's local observation o i and outputs its action a i .Our goal is to learn the shared policy π θ (a i |o i ) such that it maximizes the expected discounted reward
J(θ) = E at,st [ ∞ t γ t R target (s t , a t , s t+1 )]. We define a curriculum C as a sequence of K subtasks C = [M 1 , M 2 , . . . , M K ],
where M k denotes the k-th subtask.Each subtask M k is constructed by replacing the reward R target in M target to its own unique reward R k .Then, a curriculum can equivalently be described as a sequence of reward functions [R 1 , R 2 , . . ., R K ].Therefore, we aim to optimize a sequence of reward functions [R 1 , R 2 , . . ., R K ] so that the final learned policy can accomplish M target .</p>
<p>IV. METHOD</p>
<p>Motivated by how human coaches train teams for coordination tasks, our method leverages LLMs and VLMs as a coach for teaching robots on how to coordinate.In training a team for complex coordination, a coach would first decompose the target task into a sequence of subtasks that specify the details of the coordination strategy.The coach then trains the team on each subtask by defining its objectives (rewards).Once training is complete, the coach evaluates the team's performance, deciding whether to progress to the next subtask or revisit the current one.Inspired by this process, our framework follows five key stages as shown in Fig. 1: 1) Curriculum generation module -A curriculum LLM decomposes the long-horizon coordination task into a sequence of subtasks described in natural language.2) Reward function generation module -A reward generation LLM generates a reward function in executable python code based on the natural language descriptions of the subtasks, providing dense rewards that clearly specify the desired behavior for each subtask.3) Policy evaluation module -An evaluation VLM evaluates the success or failure of the policy trained with the LLM-generated reward based on the visual and quantitative rollouts of the policy.4) Reward refinement module -If the policy fails to achieve the desired behavior, an advice VLM provides advice on how to change the reward based on the rollout information and learning curve.Then, an LLM takes the advice and refines the reward function.5) Sequential training of subtasks -Throughout the training, we initialize each subtask with the policy learned from the previous one while motivating exploration to learn the new subtask.</p>
<p>In the following, we discuss each of these modules in detail.</p>
<p>A. Curriculum Generation Module</p>
<p>As a first step, we break down a coordination task into a sequence of subtasks.Given a natural language description of the target task and environment, a curriculum LLM outputs a sequence of subtasks in natural language.However, relying solely on zero-shot curriculum generation can result in unstable outputs due to the stochastic nature of LLM responses, ranging from overly simple to unnecessarily long or misaligned with the target task.To mitigate this, we first generate multiple candidate curricula C i , i ∈ {1, 2, . . ., I}.We then prompt the LLM with the environment description, the target task M target , and the candidate curricula, asking it to refine these candidates into a more coherent and effective curriculum.An example is shown in Fig. 2. -The shaped "lift_reward" is vanishingly small because you square a tiny number.Remove the square on elevation so that small gains in height yield a larger gradient … -Or simply increase the lift multiplier K from 80 up to a few hundred so that lift_reward becomes competitive with balance_reward.</p>
<p>Re-balance lift vs. balance shaping.</p>
<p>-Reduce the balance term (e.g. from 2.0 to 0.5 or 1.0) so the policy isn't "over-paid" to stay flat.Through this process, we obtain the final curriculum C = [l 1 , ..., l K ], where l k is a natural language description of each subtask M k .The description l k is grounded in the state variables of the environment to improve reward design and VLM evaluation.For example, in Task 1: Approach Handles (Fig. 2), instead of a vague description such as "each robot arm should get closer to its handle," the LLM-generated subtask explicitly uses the state variable Euclidean distance to its respective handle, producing the description "each arm minimizes its Euclidean distance to its target handle."</p>
<p>B. Reward Function Generation Module</p>
<p>Based on natural language description l k , we generate a subtask-specific reward R k for subtask M k in executable code.Compared to describing the curriculum through environment configurations [21] or PDDL [31], specifying the task as a reward provides greater flexibility in representing diverse subtasks across different domains.Furthermore, translating the reward into executable code enables seamless integration with existing environments and algorithms.In this subsection, we describe the generation of the initial base reward R 0 k .Here, the superscript denotes the refinement index, where R 0 k is the base reward for subtask k, while R j k denote the reward obtained after the j-th refinement that will be discussed in Section IV-D.</p>
<p>To generate a base reward function R 0 k , we prompt the LLM with the language description of the environment, current subtask description l k , and an example reward function.Since writing reward functions from scratch for highdimensional multi-agent systems can be challenging, we provide example reward functions, which are simple, structured variations of the environment-provided rewards, along with helper functions for state processing.For instance, in the bimanual manipulation task shown in Fig. 3, a helper function self._get_tilt_degree_cosine() is provided to compute the cosine value of the pot's tilt angle.Since curriculum learning should exploit knowledge acquired in previous subtasks, we also include the reward functions from earlier subtasks as additional context in the prompt.</p>
<p>When writing a reward function, we prompt the LLM to separate the reward into reward components (e.g.lift_ reward, balance_reward in Fig. 3) during training time.These logs will later be provided as reward component learning curves for reward refinements in Section IV-D.After reward generation, we train a policy π 0 k with the base reward function R 0 k via MARL, such as MAPPO [29].We note that as we assume using CTDE MARL framework, we are designing a reward for a team, rather than specific agents.</p>
<p>C. Policy Evaluation Module</p>
<p>Designing an effective reward function from natural language description is challenging, and one-shot LLMgenerated rewards often fail to produce the desired behavior.Therefore, we need a verification step to evaluate the trained policy π 0 k and determine whether to advance to the next subtask or retry the current one.</p>
<p>When evaluating a robot policy, humans rely on visual trajectory rollouts to analyze the collaboration between robots and the progress in the subtask.Motivated by this, we use VLMs for evaluating the success or failure of π 0 k by providing a sequence of snapshot images.Moreover, we also provide state variable trajectory data to make it easier for VLMs to conclude the subtask success.This design choice allows our method to be easily applied to various multi-robot platforms and arbitrary LLM-designed subtasks.However, even state-of-the-art LLMs and VLMs lack a native understanding of 3D motion [32], [33].This limitation is particularly pronounced in multi-robot environments, where the dimensionality of the state space scales with the number of agents, making it difficult for VLMs to reason about coordination behaviors directly from raw 3D state variables.Therefore, we extract and provide task-relevant metrics, such as Frobenius norm for orientation differences or distance between objects that are easier for LLMs/VLMs to interpret.When the VLM judges the policy to be successful, we proceed to the next subtask M k+1 .If it concludes the policy as a failure, policy evaluation VLM provides failure reasons which are used for refining the current reward function R 0 k to be more aligned with the desired behavior.</p>
<p>D. Reward Refinement Module</p>
<p>When generated reward R 0 k fails to induce the desired behavior, we should design a new reward function and retry the training process.A naive approach to this reward refinement is resampling from the reward generation module with the same prompt [20], or with minimal additional information, such as appending the previously attempted reward function [34].However, such naive resampling methods require extensive trial and error and are not scalable to high-dimensional multi-agent systems.To overcome this, we draw inspiration from prior works that have suggested that a team of specialized models can be more effective than a single model for complex tasks [35], [36].Following this, we introduce two additional models: an advice VLM and a refine LLM, which together enable more meaningful refinement.</p>
<p>First, we prompt the advice VLM to generate unrestricted, free-form advice on how to refine the current reward function based on the current subtask description l k , the previously attempted reward function R j k , and the failure reason obtained from policy evaluation module in Section IV-C.In addition, we provide the learning curves of individual per-step reward components, which indicate how each component evolved during training.Since these values often oscillate and are difficult to represent as long numerical sequences in text, we instead convert them into graphical images, offering the VLM a compact and interpretable representation.</p>
<p>Second, in the refine LLM, we provide the previously attempted reward function R  VLM-guided reward-refinement loop.We allow a maximum of J refinement iterations.If the policy still fails after J iterations, the LLM selects the best policy among π 0 k , ..., π J k using rollout information and proceeds to the next subtask.</p>
<p>In our experiments, we found that J = 3 iterations were sufficient to generate a successful reward.An example of this refinement process is shown in Fig. 3.</p>
<p>E. Sequential Training of Subtasks</p>
<p>When moving from one subtask to the next, the challenge lies in balancing exploitation and exploration: the policy should retain useful behaviors learned in the previous subtask, while adapting to the new reward function.To exploit prior knowledge, we initialize the weights of policy π k+1 for subtask M k+1 with the weights of policy π k from subtask M k .The initialized policy π k+1 is then trained with the reward function R k+1 .However, because the reward generation LLM is provided with the history of rewards, parts of R k+1 often resemble R k .This can cause a loss of plasticity [37], [38] leading the policy π k+1 to converge to a local optimum similar to policy π k , preventing further learning under the new reward function R k+1 .</p>
<p>To avoid loss of plasticity and encourage progress on the new subtask, we employ two techniques.First, we reset the exploration parameter of the policy network to reintroduce exploration at the beginning of each subtask training.In our case, we reset the policy standard deviation layer of MAPPO at the beginning of subtask training.This helps the policy to explore other actions while maintaining current behavior and escape the local-optima of the earlier reward function.Furthermore, we reset the value network with random weights [26].By resetting only the value network while maintaining the actor network weights, the actor can exploit the previous policy while the critic is reinitialized to estimate the value under the new reward function.</p>
<p>V. EXPERIMENTS</p>
<p>In this section, we evaluate our framework in collaborative multi-robot scenarios.We consider a bimanual manipulation task and multi-quadruped navigation tasks that require complex long-horizon coordination.We show that CRAFT can learn coordination that is challenging to achieve with vanilla MARL even with human-designed reward functions.We also validate our quadruped navigation task in hardware to show our policy transfers to real-world.CRAFT achieves highest success rate on every environment, demonstrating its ability to learn complex coordination tasks that are challenging to learn without curriculum or well-crafted reward functions.</p>
<p>A. Experiment Setup 1) Simulation Environments: We evaluate CRAFT in the following multi-agent coordination environments.The environments are shown in Fig. 4.</p>
<p>• Quadruped Gate: Two quadrupeds should coordinate to pass the narrow gate without collision [39].We define a task as success when both agents have passed the gate.Each quadruped has a continuous action space of xand y-axis velocities, and yaw velocity.• Quadruped Seesaw: Two quadrupeds should coordinate on the seesaw so that one agent climbs up to the platform at the top of the seesaw while the other balances it [39].We define a task as success when one of the agents reaches the target platform.The action space is identical to Gate environment.• Two Arm Lift: Two robot arms should coordinate in lifting the pot together while keeping the pot level [40].We define a task as success when the pot is lifted above 0.1m and no more tilted than 30 degrees.Each arm has a continuous action space, which consists of six end-effector delta pose, position and orientation changes relative to the current pose, and one gripper state.These environments are exceptionally challenging for MARL as they demand long-horizon coordination and decentralized control under partial observability.To our knowledge, no prior RL methods without expert demonstrations have succeeded in solving them reliably.We use MAPPO [29], which employs decentralized execution with only local observations, to evaluate CRAFT's ability to learn decentralized coordination.MAPPO is implemented with OpenRL library [41] for the multi-quadruped navigation, and skrl library [42] for the bimanual manipulation task.We utilize gpt-4o-2024-08-06 as our LLM models and o4-mini-2025-04-16 as our VLM models.</p>
<p>2) Baselines &amp; Ablation Study: We design our experiments such that we answer the following questions.Q1 Can our framework learn MARL policies that require complex coordination between multiple robots?Q2 Is curriculum learning framework necessary for learning effective coordination policies?Q3 To what extent is the reward refinement module necessary for learning successful policies?</p>
<p>To answer Q1, we compare CRAFT with env_reward and example_reward baselines where env_reward uses the environment-provided reward, while example_reward uses the example reward provided to reward generation module of CRAFT.Both baselines are trained without any staged curricula.To answer Q2, we compare CRAFT with no_curriculum, where we use the final reward functions generated by CRAFT that achieved top-three highest success rate.However, we train the policy without a curriculum and fix the reward function during training.This comparison tests whether a well-designed reward function alone is sufficient for successful policy training, or whether staged curriculum is also necessary.Finally, for Q3, we compare CRAFT with a no_refinement baseline, which removes the VLM-guided reward refinement module (Section IV-D).Instead, no_refinement resamples new reward functions from the reward module, relying on the stochasticity of the LLM to generate effective reward functions.To evaluate the consistency of generated curricula and reward functions, we report the effective curricula ratio.In our experiments, many trials collapse to zero success when the curriculum is poorly structured.Therefore, we define effective curricula as curricula that achieves any nonzero success rate.</p>
<p>B. Simulation Results</p>
<p>Q1: CRAFT can learn successful coordination policies.We provide the success rate curves for policies with top three success rates among 5 experiments in Fig. 6.In all three environments, CRAFT achieves the highest success rate.We emphasize that CRAFT achieves successful policies without expert demonstrations or hand-crafted curricula, reaching maximum success rates of up to 90% in Quadruped Gate, 60% in Quadruped Seesaw, and 100% in Two Arm Lift.In contrast, env_reward achieves a 0% success rate in most experiments, despite having a dense reward function designed by the environment developers.These results underscore both the difficulty of the benchmarks and the effectiveness of our framework.In addition, while exam-ple_reward that was provided to CRAFT also shows less than 10% success rate in every environments, CRAFT designs effective curricula from it and achieves much higher success rate.This demonstrates that curriculum training and reward  searching process of CRAFT enables learning a coordination policy that was challenging for vanilla MARL training.The performance gap is especially evident in the Two Arm Lift task, which requires both long-horizon planning and control over high-dimensional action spaces, highlighting the strength of CRAFT in complex multi-robot settings.</p>
<p>Qualitatively, we observe that policies trained with env_reward and example_reward are susceptible to falling into local optima, resulting in suboptimal behaviors.As shown in Fig. 5, a policy trained with env_reward converges to suboptimal strategies, such as grasping the pot but failing to lift it.This arises because long-horizon tasks require a mixture of multiple reward components for different subtasks, such as reaching the pot, grasping, and lifting it.In such cases, it is difficult to design a reward whose global optimum aligns with the desired behavior.Furthermore, even with a well-designed reward function, the presence of multiple local optima can still lead to suboptimal policies.</p>
<p>Q2: Sequential training with curriculum can achieve success rate beyond vanilla training, though its impact varies by task.Following up to Q1, we aim to answer whether a good reward function alone is sufficient for long-horizon coordination, or whether sequential training of the curriculum is also necessary.Overall, CRAFT achieves higher success rates than no_curriculum across all three environments.The gap is especially large in Quadruped Gate and Two Arm Lift, where staged training yields policies with success rates that no_curriculum cannot match even with the same reward functions.In Quadruped Seesaw, no_curriculum achieves a comparable success rate to CRAFT, but we note that it benefits from the well-designed reward function produced by CRAFT.By contrast, env_reward and exam-ple_reward fail to learn effective coordination in any environment, underscoring the role of CRAFT's reward-refinement loop in producing well-shaped rewards that extend beyond those provided by the environment.</p>
<p>Q3: The VLM-guided reward-refinement loop in CRAFT improves curricula reliability.We report the effective curricula ratio and maximum success rate from multiple runs in Table I.With the VLM-guided reward- refinement, CRAFT achieves higher effective curricula ratio than the no_refinement baseline.This highlights that VLMguided reward-refinement loop contributes to reliable subtask training, leading to consistent generation of effective curricula and corresponding reward functions.We note that if no_refinement generates successful curricula, it also learns a comparable policy, as demonstrated in the maximum success rate in Table I.However, its unreliable curriculum generation strongly degrades its average performance shown in Fig. 6.</p>
<p>C. Hardware Validation</p>
<p>We validate our method by transferring the learned policy to real robots on the Quadruped Gate task.The policy is first trained in simulation using CRAFT and then deployed on hardware.We use one Unitree Go2 and one Unitree Go1 robot.Since the embodiments and low-level MPC controllers differ from in simulation, we applied action component-wise scaling to the policy outputs before deployment.</p>
<p>As shown in Fig. 1 and Fig. 7, our method successfully transfers to the real world.The two quadrupeds exhibit coordinated behavior: the blue robot passes through the gate first while the orange robot waits, then proceeds once the gate is clear.Without any additional fine-tuning, our zeroshot transferred policy achieves a success rate of 65% over 20 runs, serving as a solid baseline given that no further learning is performed after simulation training.</p>
<p>VI. CONCLUSIONS</p>
<p>In this paper, we proposed CRAFT, a framework to train long-horizon multi-robot coordination tasks.We leveraged the reasoning capabilities of foundation models, with LLMs and VLMs acting as a "coach" to decompose complex behaviors into curricula of subtasks, each paired with an LLM-generated reward function.A VLM-guided rewardrefinement loop further improves these rewards, producing semantically rich reward signals that enable successful learning in long-horizon coordination tasks.Through our experiments in multi-quadruped navigation and bimanual manipulation, we demonstrated that our method is effective and also transferable to real hardware.</p>
<p>A key limitation of CRAFT is the variability across trials, as the stochasticity of foundation models can cause occasional failures.However, this variability is not a major obstacle, since we can run multiple training attempts in simulation and deploy only successful policies in real-world.We also view this stochasticity as an advantage since diverse curricula and refinements often yield more creative and effective solutions.Future work can focus on improving reliability in the trials and balancing creativity with stability in foundation-model-driven robot learning.</p>
<p>APPENDIX</p>
<p>A. Prompts</p>
<p>In this section, we provide prompts used in CRAFT.While the details may vary slightly across tasks, the structure remains the same.Full prompts are available at github_link.</p>
<p>1) Curriculum Generation Module (Section IV-A):</p>
<p>We use two prompts which are given to the curriculum LLM.One for generating candidate curricula, and another for refining the final curriculum from the candidates.Prompt 1: LLM prompt for generating curriculum candidates.</p>
<p>You are a curriculum generator trying to generate a curriculum to solve multi-agent reinforcement learning tasks as effectively as possible.Your goal is to write a list of subtasks and corresponding reward function that will help agents to efficiently coordinate for target task.Some helpful tips for generating a curriculum: (1) Do not explore the world by doing random actions and do not try moving to manually defined position.</p>
<p>(2) Try to make the curriculum simple.Do not generate more than 5 tasks.</p>
<p>(3) The last task of your curriculum should align with the final goal of the environment.(4) In curriculum, you cannot chage the environment or the terminating condition.Focus on the task description.</p>
<p>(5) You are learning a centralized policy that have access to every agents.</p>
<p>You will be given several variables you can use to describe each subtask in curriculum.Each task should be described using the given variables and do not introduce new variables.Prompt 2: LLM prompt for refining curriculum candidates.</p>
<p>You are a curriculum generator trying to generate a curriculum to solve multi-agent reinforcement learning tasks as effectively as possible.Your goal is to write a list of subtasks and corresponding reward function that will help agents to efficiently coordinate for target task.</p>
<p>You will be given three candidates of curriculums that were generated by other generators.Pick the best version and refine it if needed.Note that the maximum possible curriculum is 5.</p>
<p>You will be given several variables you can use to describe each subtask in curriculum.Each task should be described using the given variables and do not introduce new variables.You should write reward function for given task using useful variables from the environment.Your reward function is part of the curriculum learning which consists of learning sequence of different tasks.You will be given description of past tasks, reward code for past tasks, and current task descriptiion.</p>
<p>Your gpt_reward(self) function's output should consist of three items (1) the total reward, an integer (2) a dictionary of each individual reward component where keys are names of reward component and items are each rewards (3) the maximum possible reward that is used to normalize the reward, an integer Some helpful tips for writing a reward function code:</p>
<p>(1) Use numpy or scipy functions to write the reward function.numpy, scipy is already imported as np and scipy.Do not import additional library.</p>
<p>(2) You may put higher weight on the reward for current task, but also include rewards from past tasks to avoid forgetting.3) Policy Evaluation Module (Section IV-C): Below is the prompt given to the evaluation VLM for evaluating a trained policy.</p>
<p>Prompt 4: VLM prompt for policy evaluation.</p>
<p>You are a multi-agent reinforcement learning engineer asked to evaluate a rollout from a task in a curriculum.</p>
<p>You will be provided with: -Snapshot images from the rollout -Trajectory data (state sequences) -Previous and current task descriptions Your job is to determine: 1.Whether the agents achieved the current goal.Current task is: &lt;<Current task>&gt; Former tasks that RL succeeded to learn is: &lt;<Former tasks>&gt; Given images are snapshots in chronological order of the rollout.</p>
<p>Trajectory data: &lt;<Trajectory>&gt;</p>
<p>Note that the distance and orientation is considered as "very close" if the value is smaller than 0.03 (This is NOT a threshold value, this is given to have a sense of how close they are) Note that the trajectory data is sequential data for a whole episode.</p>
<p>Make your decision on both the trajectory data and the image.</p>
<p>4) Reward Refinement Module (Section IV-D):</p>
<p>We use two prompts.One for generating advice on how to refine the reward and one for refining the reward function given the advice (LLM).</p>
<p>Prompt 5: VLM prompt for generating reward refinement advice.</p>
<p>You are a multi agent reinforcement learning engineer trying to refine a reward function for a given task.You will be given: the reward function that was tried and failed, the experimental results, a description of why it failed, and the reward-component curve images.</p>
<p>First, analyze the reason why the reward function failed to learn the task.Then, provide advice on how to improve or modify the reward function to achieve the task.</p>
<p>These are some tips in giving advice: (1) Do not give more than three advice.</p>
<p>(2) You cannot add a penalty in the reward function, e.g.no reward can be a negative value.</p>
<p>These are the only functions that are allowed to used.&lt;<Helper_Functions>&gt; Information that cannot be obtained from the above functions cannot be used in the reward function.</p>
<p>Current task is: &lt;<Task_Name>&gt; &lt;<Task_Description>&gt; &lt;<Task_Reason>&gt; Reward function that was tried: &lt;<Reward_Function>&gt; Failure reason: &lt;<Failure_Reason>&gt; Do not give an advice on the curriculum of the reward.Just consider how to improve the reward function given the current task.Do not provide the whole code of the reward function, just give advice on how to improve it.</p>
<p>Prompt 6: LLM prompt for refining a reward function using advice.</p>
<p>You are a multi agent reinforcement learning engineer trying to refine a reward function for the given task.The reward function is part of the curriculum learning which consists of learning sequence of different tasks.You will be given description of past tasks, a reward function that was tried and failed, and an advice on how to improve it.Follow the advice and provide a new reward function that should be able to achieve the given task.Do not repeat the advice or the old reward function in your answer.</p>
<p>Your gpt_reward(self) function's output should consist of two items (1) the total reward, an integer (2) a dictionary of each individual reward component where keys are names of reward component and items are each rewards (3) the maximum possible reward that is used to normalize the reward, an integer Some helpful tips for writing a reward function code:</p>
<p>(1) Use numpy or scipy functions to write the reward function.numpy, scipy is already imported as np and scipy.Do not import additional library.</p>
<p>(2) You may put higher weight on the reward for current task, but also include rewards from past tasks to avoid forgetting.So not all the advice might be applicable.</p>
<p>5) Task-Specific Substitutions for Two Arm Lift: Below are task-specific substitutions (e.g., environment details and task descriptions) that are inserted into the general prompts above.</p>
<p>Fig. 1 :
1
Fig. 1: (left) Overview of CRAFT.Given a natural language description of the target task, CRAFT generates a curriculum C = [M1, . . ., MK ] of subtasks that progressively build coordination towards the target task.For each subtask M k , a reward LLM generates a reward function R jk and a decentralized policy π j k , shared among agents, is trained via MARL.A VLM evaluates this policy using state variable trajectories and snapshot images.If the policy fails, an advice VLM provides advice from reward component learning curves, plots of individual reward terms during training.The reward LLM incorporates this advice to generate a revised reward.Successful policy advances to the next subtask and this iterative process continues until the final task is reached.(Right) Demonstration of the Quadruped Gate task, where two quadrupeds coordinate to pass through a narrow gate without collision: the blue robot passes first while the orange robot waits; then, the orange robot proceeds once the gate is clear.This policy was learned in simulation and transferred to real-world.</p>
<p>Task 1 :Task 3 :
13
Position and AlignEach robot arm should minimize the Euclidean distance to its respective handle and orient its endeffector to match the handle's orientation accurately.Task 2: Synchronize Grip … Coordinated Lift and LevelingBoth robot arms must collaboratively lift the pot to a minimum height of 0.1m, maintaining the pot's tilt within an acceptable range (cosine of tilt angle ≥ cos(30 degrees)).</p>
<p>Task 1 :Task 3 :Task 4 :
134
Approach HandlesEach robot arm minimizes its Euclidean distance to its respective target handle.Task 2: Grasp Handles … Initial Lift and BalanceCollaboratively elevate the pot to a height above 0.05m while maintaining minimal pot tilt (cosine of tilt angle close to 1).Final Lift and Stabilize Elevate the pot beyond 0.1m while ensuring the pot remains within the specified tilt limit (no more tilted than 30 degrees).</p>
<p>Task 1 :Task 3 :
13
Position and AlignEach robot arm should minimize the Euclidean distance to its respective handle and orient its endeffector to match the handle's orientation accurately.Task 2: Synchronize Grip … Initial Lift and StabilizeCollaboratively elevate the pot to a height above 0.05m while keeping the pot tilt within the acceptable range (cosine ≥ cos(30°)).</p>
<p>Task 4 :
4
Coordinated Final LiftElevate the pot beyond 0.1m while ensuring the pot remains within the specified tilt limit (no more tilted than 30 degrees).</p>
<p>Fig. 2 :
2
Fig. 2: Example of curriculum refinement for task lift and balance the pot.Three different candidate curricula C 1 to C 3 , generated by the curriculum LLM, are re-provided to the LLM for refinement.In C 1 , Task 1 focuses only on minimizing distance, while Task 1 in C 3 is defined as minimizing distance and matching orientation.In contrast, Task 3 and Task 4 in C 1 break down the lifting into two stages of first lifting halfway and then to a full height, whereas C 3 represents lifting as a single task.The curriculum LLM merges these candidates into a final curriculum C by selecting the stronger tasks definitions from each candidate.</p>
<p>Evaluation Refine reward function … 1 .
1
Boost the relative weight or reshape the lift signal.</p>
<p>Advice Curriculum 1 .
1
Fig. 3: Example of reward refinement of subtask Coordinate Preliminary Lift.Through the first reward-refinement loop, R 1k=3 was produced and the evaluation VLM marked the policy as a failure since the pot never reached the required elevation of 0.05 m.The reward component learning curves were then passed to the advice VLM, which identified that lift_reward was too weak compared to balance_reward.It recommended removing the square on elevation, increasing the lift weight, and decreasing the balance weight.The revised reward R 2 k=3 reflects these changes: the square on elevation was removed, the lift weight increased from 80 to 200, and the balance weight decreased from 2 to 1.With this reward, the policy successfully achieved the 0.05 m elevation and satisfied the subtask.</p>
<p>Fig. 4 :
4
Fig. 4: Illustrative snapshot showing successful execution of multiagent coordination tasks by a CRAFT-trained policy.</p>
<p>j k and advice generated by the VLM.The refine LLM then produces a new refined reward R j+1 k .Once a new refined reward R j+1 k is generated, we train a corresponding policy π j+1 k and iterate through the</p>
<p>Fig. 5 :
5
Fig. 5: Illustrative snapshot of policies trained with env_reward without curriculum.The policy shows suboptimal behaviors, such as only one agent passing the gate or only managed to grasp the pot, rather than lifting it.</p>
<p>Fig. 6 :
6
Fig.6: Success Rate of the top-three curricula from each environment.Each environment is evaluated by 100 random initial conditions.CRAFT achieves highest success rate on every environment, demonstrating its ability to learn complex coordination tasks that are challenging to learn without curriculum or well-crafted reward functions.</p>
<p>Fig. 7 :
7
Fig. 7: Trajectory of quadrupeds in our hardware experiments for Quadruped Gate policy trained with CRAFT.Two quadrupeds exhibit coordination behaviors in real world and manage to pass the gate sequentially without collision.</p>
<p>names, descriptions, and reasons in the format.&lt;<Environment_Description>&gt; Varaibles you can use for curriculum descriptions are &lt;<State_Variables>&gt; These are the rules that you have to consider when writing a curriculum (1) You cannot change start position of robots and objects.(2) You cannot change the number of robots and objects.(3) You should consider all robots in the environment.</p>
<p>2 )
2
out the names, descriptions, and reasons in the format.&lt;<Environment_Description>&gt; Varaibles you can use for curriculum descriptions are &lt;<State_Variables>&gt; These are the rules that you have to consider when writing a curriculum (1) You cannot change start position of robots and objects.(2) You cannot change the number of robots and objects.(3) You should consider all robots in the environment.Three candidates of curriculums are: Reward Function Generation Module (Section IV-B): Below is the prompt given to the reward generation LLM for generating a base reward function.Prompt 3: LLM prompt for generating base reward function.</p>
<p>( 3 )
3
Total reward must stay within [0, 1].Always, scale the reward to have maximum 1. (4) Only use provided variables.(5) Do not change other predefined parts in the code.&lt;<Environment_Description>&gt; You can use help functions to calculate reward.&lt;<Helper_Functions>&gt; Example reward function for the task is &lt;<Example_Reward>&gt; You should re-implement reward function to assign a proper reward function for given task.You can change the reward components that aligns well with the task.Do not include any new inputs in the functions.Note that you are designing a reward function for team, not a single agent.Generate a reward function code and command for Task Name</p>
<p>2 .Fill
2
Whether they have forgotten earlier subtasks.out the [] brackets in the given format.(Don't keep the [] brackets)</p>
<p>( 3 )
3
Total reward must stay within [0, 1].Always, scale the reward to have maximum 1. (4) Only use provided variables.(5) Do not change other predefined parts in the code.You can use help functions to calculate reward.&lt;<Helper_Functions>&gt; Current task is: &lt;<Current_Task>&gt; Former tasks that RL succeeded to learn is: &lt;<Former_Tasks>&gt; Reward function that was tried: &lt;<Reward_Function>&gt; Advice on how to improve the reward function: &lt;<Advice>&gt; Do not deviate from the reward structure given in the example.Note that the advice might not acount for what information it can use in the reward function.</p>
<p>TABLE I :
I
Effective curricula ratio indicates how many curricula achieved a nonzero success rate across 5 experiments.Maximum success rate refers to the success rate of the best policy obtained among the 5 experiments.CRAFT generates more effective curricula than the no_refinement baseline, demonstrating its ability to consistently generate successful curricula.Moreover, the higher maximum success rate shows that the reward refinement loop further contributes to generating better curricula.</p>
<p>ACKNOWLEDGMENTThe authors are grateful to Yuman Gao, Prof. Koushil Sreenath and the Hybrid Robotics Lab for their support on the hardware experiments.Prompt 7: «Environment_Description» for Two Arm Lift.A large pot with two handles is placed on a table top.The pot is rectangular shaped with U-shaped handles on the opposite side of the pot rim.The grip of the handle is level, parallel to the tabletop.Two robot arms are placed on opposite ends of the table.The two robot arms must each grab a handle and lift the pot together, above a certain height (0.1m), while keeping the pot level (no more tilted than 30 degrees).The pot location is randomized at the beginning of each episode.Prompt 8: «State_Variables» for Two Arm Lift.-Euclidean distance from each robot's end-effector and its target handle -Orientation difference between each robot's end-effector and its target handle -Gripper state of each robot (whether it grasped the handle or not) -Pot elevation above the table -Pot tilt as cosine of the angle between its normal and the vertical Prompt 9: «Helper_Functions» for Two Arm Lift.self._get_check_grasps(): returns two boolean value for each arm whether or not it's grasping the handle or not.self._get_pot_elevation():returns the distance (in meters) from pot's bottom to the table top self._get_tilt_degree():returns the cosine value of the tilted degree of the pot.The tilted degree is the difference between the z axis and pot's normal vector self._get_gripper_to_handle_distance():returns two float value of distance from each robot's end-effector to each handle self._get_gripper_to_handle_rotation():returns two rotation difference (calcualated with R1.T @ R2) between each robot's end-effector to each handle with shape : (3,3) each self._check_success():returns True if the pot is higher than 0.1B. CRAFT ExamplesThis section presents example outputs from a successful trial of the Two Arm Lift task, including curricula, generated rewards, policy evaluations, advice, and refined rewards.Output 1: Example final curriculum generated for Two Arm LiftTask 1 Name: Arm Alignment Task Description: The task focuses on minimizing the Euclidean distance from each robot's end-effector to its respective target handle.The reward will be based on the decreasing distance between each robot's end-effector and its assigned handle.Reason: Establishing an initial alignment of the robot arms with the handles will set the foundation for successful grasping.This step helps the robots learn coordinated movement towards the handle, which is essential for synchronized actions in subsequent tasks.Task 2 Name: Grasp Coordination Task Description: The goal is for both robot arms to match the orientation of their end-effectors with the handles and change the gripper state to the grasped condition.The reward will increase when both arms have reduced orientation differences and both successfully grasp their handles.Reason: By focusing on grasp coordination, this task trains the robots to orient themselves correctly and clamp securely onto the pot's handles.Achieving a secure grasp is crucial for preventing slippage and ensuring stable lifting in future tasks.Task 3 Name: Initial Lifting Task Description: Once both robot arms have securely grasped the handles, they must elevate the pot slightly above the table (e.g., 0.05m) without exceeding a 30degree tilt as measured by the pot tilt variable.Rewards will be given for maintaining a level pot during the lift.Reason: This task introduces the concept of collaboration in lifting, emphasizing control over the elevation while managing pot stability.It sets the precedence for maintaining balance and coordination when further lifting in subsequent tasks.Task 4 Name: Controlled Elevation Task Description: In this task, the robot arms must lift the pot above the required height of 0.1m while ensuring that the pot's tilt remains within the 30-degree threshold.The reward structure will be based on the pot elevation variable while maintaining minimal tilt.Reason: Taking the learning from the previous tasks, this task requires the robots to execute coordinated actions for a complete lift, focusing on both height and tilt.This task aligns with the target goal, ensuring that the robots can perform the full lift in a controlled manner.Task 5 Name: Synchronized Full Lift Task Description: The task requires the robots to synthesize all prior skills to lift the pot above 0.1m in a seamless motion, keeping the tilt within the desired limits and demonstrating a complete coordinated effort.The reward is given for achieving all aspects of the task simultaneously.Reason: This final task ensures that the robots can integrate all learned experiences to accomplish the target task in a real-world scenario, demonstrating full competence in synchronized lifting operations."proximity_reward": proximity_reward, "alignment_reward": alignment_reward, "grasp_reward": grasp_reward, "controlled_elevation_reward": controlled_elevation_reward, "stability_reward": stability_reward, "cooperation_reward": cooperation_reward, } return self.gpt_rew,self.gpt_rew_dict,max_reward Output 3: Example policy evaluation for Two Arm Lift.The policy was trained with reward R 0 k=4 given above.Decision: FailureReason:-The pot elevation peaked at only ~0.016 m, well below the required 0.1 m lift height.-Although pot tilt remained within the 30 degree threshold, the current task's primary elevation goal was not met.-Gripper1 never achieved a stable grasp on its handle, and gripper0 only grasped sporadically (34/790 steps), indicating a failure to retain reliable grasp coordination.-Both arms drifted away from their handles after initial approach (distances never fell below 0.03 m), showing degradation in arm alignment learned in earlier tasks.Output 4: Example advice for Two Arm Lift.The advice was generated for the reward R 0 k=4 given above.Analysis of failure 1. Sparse elevation feedback: Controlled elevation and stability rewards only kick in once elevation &gt;= 0.1 m and tilt within threshold.Because the agents rarely reach that threshold early on, they get almost zero signal to lift beyond ~0.016 m. 2. Binary tilt gating: direction_coef zeroes out any elevation reward if tilt is even slightly over 30 degree, so there is no gradient to correct small tilt errors during the lift.3. Incentives decoupled from lift: Proximity, alignment, grasp and cooperation rewards are largely satisfied independently of successful coordinated lifting.Once the handles are touched or grasped, there is little extra push to actually raise the pot.Three targeted improvements 1. Continuous elevation shaping -Replace the hard step for elevation with a smooth, monotonically increasing reward -for example r_elev = min(elevation / threshold, 1.0) or a quadratic/ sigmoid on elevation.This gives gradient toward any increase in height, not just surpassing 0.1 m. 2. Tilt-aware bonus rather than gate -Instead of direction_coef={0,1}, use a continuous tilt bonus such as r_tilt = (cos_z -cos_max) / (1 -cos_max), clipped to [0,1], and multiply it with the elevation reward.This still rewards low tilt but provides a gradient that gently pushes the pot back toward upright whenever it begins to tilt.3. Tighten coupling of grasp, proximity, and lift -Multiply (or weight more heavily) your proximity/alignment rewards by the current elevation factor (e.g.r_prox x r_elev).That way, the agents only obtain full proximity/alignment credit if they hold on while actually lifting.It discourages "touch and release" and encourages maintaining contact through the elevation phase.Code 3: Example refined reward code for Two Arm Lift.Reward R 1 k=4 was refined from R 0 k=4 using the advice above."proximity_reward": proximity_reward, "alignment_reward": alignment_reward, "grasp_reward": grasp_reward, "controlled_elevation_reward": controlled_elevation_reward, "stability_reward": stability_reward, "cooperation_reward": cooperation_reward, } return self.gpt_rew,self.gpt_rew_dict,max_reward
Grandmaster level in starcraft ii using multi-agent reinforcement learning. O Vinyals, I Babuschkin, W M Czarnecki, M Mathieu, A Dudzik, J Chung, D H Choi, R Powell, T Ewalds, P Georgiev, nature. 57577822019</p>
<p>Smacv2: An improved benchmark for cooperative multi-agent reinforcement learning. B Ellis, J Cook, S Moalla, M Samvelyan, M Sun, A Mahajan, J Foerster, S Whiteson, Advances in Neural Information Processing Systems. 202336</p>
<p>Google research football: A novel reinforcement learning environment. K Kurach, A Raichuk, P Stańczyk, M Zaj Ąc, O Bachem, L Espeholt, C Riquelme, D Vincent, M Michalski, O Bousquet, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Toward real-world cooperative and competitive soccer with quadrupedal robot teams. Z Su, Y Gao, E Lukas, Y Li, J Cai, F Tulbah, F Gao, C Yu, arXiv:2505.138342025arXiv preprint</p>
<p>Marladona-towards cooperative team play using multi-agent reinforcement learning. Z Li, F Bjelonic, V Klemm, M Hutter, 2025 IEEE International Conference on Robotics and Automation (ICRA). IEEE2025</p>
<p>Leveraging large language models for effective and explainable multi-agent credit assignment. K Nagpal, D Dong, N Mehr, Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems. the 24th International Conference on Autonomous Agents and Multiagent Systems2025</p>
<p>Variational automatic curriculum learning for sparse-reward cooperative multi-agent problems. J Chen, Y Zhang, Y Xu, H Ma, H Yang, J Song, Y Wang, Y Wu, Advances in Neural Information Processing Systems. 202134</p>
<p>Automatic curriculum learning for deep rl: a short survey. R Portelas, C Colas, L Weng, K Hofmann, P.-Y Oudeyer, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Cooperative multiagent control using deep reinforcement learning. J K Gupta, M Egorov, M Kochenderfer, International conference on autonomous agents and multiagent systems. Springer2017</p>
<p>A survey on curriculum learning. X Wang, Y Chen, W Zhu, IEEE transactions on pattern analysis and machine intelligence. 202144</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, Transactions on Machine Learning Research. 2024</p>
<p>Progprompt: Generating situated robot task plans using large language models. I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Eureka: Human-level reward design via coding large language models. Y J Ma, W Liang, G Wang, D.-A Huang, O Bastani, D Jayaraman, Y Zhu, L Fan, A Anandkumar, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Vision-language models are zero-shot reward models for reinforcement learning. J Rocamonde, V Montesinos, E Nava, E Perez, D Lindner, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Autoeval: Autonomous evaluation of generalist robot manipulation policies in the real world. Z Zhou, P Atreya, Y L Tan, K Pertsch, S Levine, arXiv:2503.242782025arXiv preprint</p>
<p>Aha: A visionlanguage-model for detecting and reasoning over failures in robotic manipulation. J Duan, W Pumacay, N Kumar, Y R Wang, S Tian, W Yuan, R Krishna, D Fox, A Mandlekar, Y Guo, arXiv:2410.003712024arXiv preprint</p>
<p>Reverse forward curriculum learning for extreme sample and demo efficiency. S Tao, A Shukla, T Chan, H Su, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Unsupervised curricula for visual meta-reinforcement learning. A Jabri, K Hsu, A Gupta, B Eysenbach, S Levine, C Finn, Advances in Neural Information Processing Systems. 201932</p>
<p>Tizero: Mastering multi-agent football with curriculum learning and self-play. F Lin, S Huang, T Pearce, W Chen, W.-W Tu, Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems. the 2023 International Conference on Autonomous Agents and Multiagent Systems2023</p>
<p>Curricullm: Automatic task curricula design for learning complex robot skills using large language models. K Ryu, Q Liao, Z Li, P Delgosha, K Sreenath, N Mehr, 2025 IEEE International Conference on Robotics and Automation (ICRA). IEEE2025</p>
<p>Environment curriculum generation via large language models. W Liang, S Wang, H.-J Wang, O Bastani, D Jayaraman, Y J Ma, 8th Annual Conference on Robot Learning. 2024</p>
<p>Aura: Agentic upskilling via reinforced abstractions. A Zhu, Y Tanaka, D Hong, arXiv:2506.025072025arXiv preprint</p>
<p>Self-refined large language model as automated reward function designer for deep reinforcement learning in robotics. J Song, Z Zhou, J Liu, C Fang, Z Shu, L Ma, arXiv:2309.066872023arXiv preprint</p>
<p>Learning a high-quality robotic wiping policy using systematic reward analysis and visual-language model based curriculum. Y Liu, D Kang, S Ha, arXiv:2502.125992025arXiv preprint</p>
<p>Learning multi-agent loco-manipulation for long-horizon quadrupedal pushing. Y Feng, C Hong, Y Niu, S Liu, Y Yang, D Zhao, 2025 IEEE International Conference on Robotics and Automation (ICRA). IEEE2025</p>
<p>Decentralized navigation of a cable-towed load using quadrupedal robot team via marl. W.-T Chen, M Nguyen, Z Li, G N Sue, K Sreenath, arXiv:2503.182212025arXiv preprint</p>
<p>Resolving conflicting constraints in multi-agent reinforcement learning with layered safety. J J Choi, J J Aloor, J Li, M G Mendoza, H Balakrishnan, C J Tomlin, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsLos Angeles, USAJune 2025</p>
<p>Learning differentiable and safe multi-robot control for generalization to novel environments using control barrier functions. V Sharma, N Mehr, N Hovakimyan, 2024 IEEE 63rd Conference on Decision and Control (CDC). IEEE2024</p>
<p>The surprising effectiveness of ppo in cooperative multi-agent games. C Yu, A Velu, E Vinitsky, J Gao, Y Wang, A Bayen, Y Wu, Advances in neural information processing systems. 202235</p>
<p>Monotonic value function factorisation for deep multiagent reinforcement learning. T Rashid, M Samvelyan, C S De Witt, G Farquhar, J Foerster, S Whiteson, Journal of Machine Learning Research. 211782020</p>
<p>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. L Guan, K Valmeekam, S Sreedharan, S Kambhampati, Advances in Neural Information Processing Systems. 202336</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. B Chen, Z Xu, S Kirmani, B Ichter, D Sadigh, L Guibas, F Xia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>3d-vla: A 3d vision-language-action generative world model. H Zhen, X Qiu, P Chen, J Yang, X Yan, Y Du, Y Hong, C Gan, International Conference on Machine Learning. PMLR2024</p>
<p>Synthesizing interpretable control policies through large language model guided search. C Bosio, M W Mueller, 2025 American Control Conference (ACC). IEEE2025</p>
<p>Large language model based multi-agents: a survey of progress and challenges. T Guo, X Chen, Y Wang, R Chang, S Pei, N V Chawla, O Wiest, X Zhang, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. the Thirty-Third International Joint Conference on Artificial Intelligence2024</p>
<p>Multi-agent collaboration: Harnessing the power of intelligent llm agents. Y Talebirad, A Nadiri, arXiv:2306.033142023arXiv preprint</p>
<p>Loss of plasticity in continual deep reinforcement learning. Z Abbas, R Zhao, J Modayil, A White, M C Machado, Conference on lifelong learning agents. PMLR2023</p>
<p>Loss of plasticity in deep continual learning. S Dohare, J F Hernandez-Garcia, Q Lan, P Rahman, A R Mahmood, R S Sutton, Nature. 63280262024</p>
<p>Mqe: Unleashing the power of interaction with multi-agent quadruped environment. Z Xiong, B Chen, S Huang, W.-W Tu, Z He, Y Gao, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2024</p>
<p>robosuite: A modular simulation framework and benchmark for robot learning. Y Zhu, J Wong, A Mandlekar, R Martín-Martín, A Joshi, S Nasiriany, Y Zhu, K Lin, arXiv:2009.122932020</p>
<p>Openrl: A unified reinforcement learning framework. S Huang, W Chen, Y Sun, F Bie, W.-W Tu, arXiv:2312.161892023arXiv preprint</p>
<p>skrl: Modular and flexible library for reinforcement learning. A Serrano-Muñoz, D Chrysostomou, S Bøgh, N Arana-Arexolaleiba, Journal of Machine Learning Research. 242542023</p>            </div>
        </div>

    </div>
</body>
</html>
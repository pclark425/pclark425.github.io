<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4770 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4770</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4770</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-259370847</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2206.02336v3.pdf" target="_blank">Making Language Models Better Reasoners with Step-Aware Verifier</a></p>
                <p><strong>Paper Abstract:</strong> Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4770.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4770.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DIVERSE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DIVERSE (Diverse Verifier on Reasoning Step)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that improves reasoning by (1) generating diverse prompts and sampled reasoning paths, (2) using a learned verifier to weight candidate answers (voting verifier), and (3) verifying individual steps with a step-aware verifier to attribute correctness at step granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (also evaluated with davinci and text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI 'davinci' family language models used as black-box autoregressive LMs (paper uses davinci, text-davinci-002, code-davinci-002); sampling temperature 0.5 for generation; verifier is fine-tuned DeBERTa-v3-large.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>DIVERSE (diverse prompts + voting verifier + step-aware verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>For each question generate M1 different prompts and M2 sampled reasoning paths per prompt (M = M1×M2). Use a trained verifier (DeBERTa-v3-large) to score each reasoning path and perform a weighted voting (voting verifier). Optionally add a step-level auxiliary loss and per-step labels to produce step-aware verifier scores that identify which steps are correct/incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple reasoning benchmarks (primary: GSM8K; also AsDiv, MultiArith, SVAMP, SingleEq, CLUTRR, StrategyQA, CommonsenseQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks requiring arithmetic, commonsense, and inductive reasoning (e.g., GSM8K: grade-school math word problems; CLUTRR: kinship induction; CommonsenseQA/StrategyQA: multiple-choice commonsense tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K: reported improvement to 83.2% (example reported: 'GSM8K 74.4% → 83.2%'); other reported DIVERSE results (code-davinci-002) include AsDiv 88.7%, MultiArith 99.8%, SVAMP 87.0%, SingleEq 94.9%, CLUTRR 95.9% (paper reports these as new SOTAs on 6 of 8 tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared to Greedy Decode and Self-Consistency baselines the paper reports (aggregate) improvements: e.g., DIVERSE brings improvements of 22.2% / 12.0% on davinci, 33.1% / 12.0% on text-davinci-002, and 27.0% / 5.6% on code-davinci-002 (compared to Greedy Decode / Self-Consistency respectively). Also specific M1/M2 majority-vote settings: ⟨1,100⟩→76.7, ⟨5,20⟩→80.0, ⟨10,10⟩→79.8, ⟨100,1⟩→73.0 (these are reported GSM8K majority-vote results illustrating benefit of diverse prompts + sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining diverse prompts (varying exemplars) with sampling and a learned voting verifier improves reasoning accuracy consistently across models and tasks. Step-aware verification further improves selection by scoring individual steps, increasing interpretability and often improving final accuracy. Diversity in prompts yields more varied reasoning paths and final answers, which helps the verifier distinguish correct answers from varied errors.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Step-aware verifier slightly lowered performance for code-davinci-002 on GSM8K in one reported experiment. On two commonsense multiple-choice tasks, DIVERSE was slightly worse overall (≈ -1.9%) than PaLM-based self-consistency; StrategyQA saw a ≈3.0% drop attributed to noisy pseudo-exemplars from self-teaching. Self-teaching to create exemplars can introduce false-positive exemplars, harming multiple-choice tasks (StrategyQA) more severely.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Making Language Models Better Reasoners with Step-Aware Verifier', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4770.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4770.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diverse Prompts + Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diverse prompts combined with sampling decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to increase reasoning-path diversity by sampling different exemplar subsets to create multiple distinct prompts (M1 prompts) and sampling multiple reasoning paths per prompt (M2), yielding M1×M2 candidate reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (evaluated also with davinci and text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI 'davinci' family models used with sampling (temperature 0.5); prompts constructed by randomly sampling exemplars from an exemplar base.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Diverse prompts (M1 prompts) + sampling (M2 per prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Randomly select M1 different exemplar-based prompts per question and sample M2 reasoning paths per prompt; this produces M = M1×M2 candidate reasoning paths to be aggregated (majority vote or weighted by a verifier).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (primary analysis) and other reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: grade-school math problems where chain-of-thought helps; diversity measured by distinct intermediate result chains and distinct final answers per question.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K majority-vote results (code-davinci-002) reported for different ⟨M1,M2⟩: ⟨1,100⟩ → 76.7%; ⟨5,20⟩ → 80.0%; ⟨10,10⟩ → 79.8%; ⟨100,1⟩ → 73.0%. Best results achieved by mixing diverse prompts and sampling (⟨5,20⟩).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Pure sampling with a fixed prompt (⟨1,100⟩) achieves 76.7% (majority voting); combining diverse prompts with sampling (⟨5,20⟩) yields 80.0% — showing diverse prompts + sampling outperform pure sampling decoding under majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Diverse prompts increase the variety of reasoning paths and distinct final answers, which improves majority-voting aggregation and also provides more varied negative samples for verifier training; neither diverse prompts alone nor sampling alone is optimal — combining both gives best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Extreme choices (e.g., many prompt variations with only 1 sample each like ⟨100,1⟩) performed worse (73.0%) than balanced M1/M2 settings; marginal gains saturate as number of sampled paths M increases (marginal improvement beyond M ≥ 50).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Making Language Models Better Reasoners with Step-Aware Verifier', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4770.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4770.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voting Verifier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voting verifier (weighted voting using a learned verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregates candidate answers by combining majority-voting with verifier probabilities: sum verifier scores for candidates that share the same final answer and choose answer with highest total weight.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa-v3-large verifier (trained); used with outputs of OpenAI davinci-family LMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Verifier is a fine-tuned DeBERTa-v3-large model with a scalar head over [CLS] to predict probability that a candidate reasoning path yields correct answer; trained on generated positive/negative chains.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Voting verifier</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>For each generated reasoning path (x_i, z_i, y_i) compute f(x_i,z_i,y_i) = verifier probability of being correct. Aggregate by answer y: choose y that maximizes sum over candidate paths with y_i = y of f(...). This combines counting and verifier confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and other reasoning benchmarks (verifier trained per task with up to 1000 training pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same reasoning tasks where many candidate reasoning chains are produced; verifier trained on generated chain-of-thoughts labeled by whether final answer matches ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that voting verifier outperforms plain majority voting and a verifier-alone aggregator; quantitative per-task numbers not fully enumerated in-text, but DIVERSE (which uses voting verifier) achieves reported GSM8K improvements (e.g., to 83.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Majority voting and verifier-only compared: voting verifier performs best in experiments; verifier-only (without voting) causes significant performance drops; majority voting can fail when correct paths are minority but high-quality (voting verifier recovers these).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining verifier confidence with voting (weighted voting) makes final-answer selection more robust, enabling choosing correct-but-minority answers when majority is wrong; voting verifier is an important contributor to DIVERSE's gains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Verifier-only aggregation (no voting) causes large drops in performance; majority voting alone can be misled when majority of sampled paths are incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Making Language Models Better Reasoners with Step-Aware Verifier', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4770.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4770.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Step-aware Verifier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Step-aware voting verifier (step-level supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extends the voting verifier by supervising and scoring individual reasoning steps (labels per step) so the verifier can detect which steps in a chain are correct/incorrect and weight candidates accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa-v3-large step verifier (trained); applied to code-davinci-002 outputs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Step-level classifier uses hidden state of last token of each step from DeBERTa-v3-large to predict probability that a particular step is correct; trained with auxiliary BCE loss weighted by hyperparameter alpha.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Step-aware voting verifier</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Label individual steps in candidate reasoning chains as correct/incorrect (positive steps are those matching steps seen in positive chains or semantically equivalent via an NLI model); add auxiliary loss to train step-level predictions f' and use step scores to improve final aggregation and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (detailed evaluation), CommonsenseQA (also evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks where reasoning chains consist of multiple intermediate steps (e.g., math chain-of-thought) and where identifying which intermediate step goes wrong is useful.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Step-aware verifier generally improves performance over voting verifier across most experiments; paper reports improvements on GSM8K and CommonsenseQA in most runs. Exception: a small decrease for code-davinci-002 on GSM8K in one setting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared to voting verifier without step labels, step-aware verifier produced more correct steps before mistakes (human-evaluated); in failed cases, step-aware had more correct steps in 33% of failed tests and fewer in 17% (human eval). Overall final-answer accuracy typically improved, but small negative case existed for code-davinci-002 on GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Step-level supervision helps the verifier detect where chains go wrong, improves final-answer selection and interpretability, reduces redundant steps and increases proportion of correct intermediate steps in incorrect chains. Human evaluation shows step-aware mechanism yields more correct intermediate steps before the first error.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Slight performance degradation observed for code-davinci-002 on GSM8K in one experiment; step labeling relies on ability to find semantically equivalent steps in positive chains and requires additional labeled data/heuristics (uses MNLI for general tasks), which can be imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Making Language Models Better Reasoners with Step-Aware Verifier', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4770.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4770.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (majority voting over sampled chain-of-thoughts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An aggregation method that samples many chain-of-thought reasoning paths from a fixed prompt and selects the most consistent final answer via majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM (in prior SOTA) and also applied to other LMs in comparisons (davinci family in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-consistency was previously applied with PaLM-540B in literature; in this paper self-consistency is used as a baseline by sampling many chains (e.g., 100) from a fixed prompt and majority-voting final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency (sampling + majority vote)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Sample many reasoning chains from a single prompt (sampling decoding) and select the final answer that appears most frequently (majority voting).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and other benchmarks (baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks where chain-of-thought sampling and majority voting can increase robustness by aggregating multiple sampled chains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as a strong baseline; example numbers in paper: in some contexts earlier SOTA using PaLM-based self-consistency; paper reports cases where DIVERSE improves over self-consistency (e.g., DIVERSE GSM8K to 83.2% from a prior 74.4% baseline reported in intro). Also paper reports that DIVERSE improves over self-consistency by 5.6% on code-davinci-002 (aggregate statement).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>DIVERSE (diverse prompts + voting verifier) outperforms self-consistency (sampling from a single prompt + majority voting) consistently in the experiments; however, on some commonsense multiple-choice tasks PaLM-based self-consistency remains slightly better in the paper's cross-model comparison (DIVERSE is slightly worse by ~1.9% overall on the two commonsense tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-consistency benefits from sampling but can be limited by lack of prompt diversity; DIVERSE's additional prompt diversity and learned verifier enable selecting correct-but-minority answers that majority voting would miss, yielding consistent improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Self-consistency can outperform DIVERSE on certain commonsense multiple-choice tasks when PaLM was used (paper reports DIVERSE slightly below PaLM-based self-consistency on CommonsenseQA/StrategyQA aggregate comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Making Language Models Better Reasoners with Step-Aware Verifier', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4770.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4770.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Greedy Decode (single CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy decoding of a single chain-of-thought (single-shot chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate a single reasoning chain (chain-of-thought) greedily (deterministically) from a prompt and use its final answer as output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>davinci-family LMs (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Greedy chain-of-thought uses a single sampled or greedily-decoded chain produced by the language model using a chain-of-thought prompt (no ensembling).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Greedy Decode (single chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate one chain-of-thought per question (greedy decoding or single sample) and output the final answer from that chain; no ensemble or aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and other reasoning benchmarks (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks where single-chain generation is compared against sampling/ensembling methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Chain-of-thought (as introduced prior work) improved GSM8K from 17.9% to 58.1% in earlier work; in this paper greedy decode is used as a baseline and DIVERSE reports large relative improvements over greedy decoding (text mentions DIVERSE brings improvements e.g., 22.2% on davinci relative to greedy).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>DIVERSE and self-consistency (sampling + aggregation) substantially outperform greedy single-chain decoding; DIVERSE reports particularly large gains over greedy decoding across models and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Single-chain greedy decoding is inferior to methods that aggregate multiple diverse chains (sampling/self-consistency) and to DIVERSE; ensembling or verifier-based aggregation captures robustness that single outputs lack.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No cases reported where greedy single-chain decoding outperforms diverse/ensemble methods in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Making Language Models Better Reasoners with Step-Aware Verifier', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models <em>(Rating: 2)</em></li>
                <li>PaLM: Scaling Language Modeling with Pathways <em>(Rating: 1)</em></li>
                <li>Star: Bootstrapping Reasoning with Reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4770",
    "paper_id": "paper-259370847",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "DIVERSE",
            "name_full": "DIVERSE (Diverse Verifier on Reasoning Step)",
            "brief_description": "An approach that improves reasoning by (1) generating diverse prompts and sampled reasoning paths, (2) using a learned verifier to weight candidate answers (voting verifier), and (3) verifying individual steps with a step-aware verifier to attribute correctness at step granularity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (also evaluated with davinci and text-davinci-002)",
            "model_description": "OpenAI 'davinci' family language models used as black-box autoregressive LMs (paper uses davinci, text-davinci-002, code-davinci-002); sampling temperature 0.5 for generation; verifier is fine-tuned DeBERTa-v3-large.",
            "reasoning_method_name": "DIVERSE (diverse prompts + voting verifier + step-aware verifier)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "For each question generate M1 different prompts and M2 sampled reasoning paths per prompt (M = M1×M2). Use a trained verifier (DeBERTa-v3-large) to score each reasoning path and perform a weighted voting (voting verifier). Optionally add a step-level auxiliary loss and per-step labels to produce step-aware verifier scores that identify which steps are correct/incorrect.",
            "task_name": "Multiple reasoning benchmarks (primary: GSM8K; also AsDiv, MultiArith, SVAMP, SingleEq, CLUTRR, StrategyQA, CommonsenseQA)",
            "task_description": "Benchmarks requiring arithmetic, commonsense, and inductive reasoning (e.g., GSM8K: grade-school math word problems; CLUTRR: kinship induction; CommonsenseQA/StrategyQA: multiple-choice commonsense tasks).",
            "performance": "GSM8K: reported improvement to 83.2% (example reported: 'GSM8K 74.4% → 83.2%'); other reported DIVERSE results (code-davinci-002) include AsDiv 88.7%, MultiArith 99.8%, SVAMP 87.0%, SingleEq 94.9%, CLUTRR 95.9% (paper reports these as new SOTAs on 6 of 8 tasks).",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared to Greedy Decode and Self-Consistency baselines the paper reports (aggregate) improvements: e.g., DIVERSE brings improvements of 22.2% / 12.0% on davinci, 33.1% / 12.0% on text-davinci-002, and 27.0% / 5.6% on code-davinci-002 (compared to Greedy Decode / Self-Consistency respectively). Also specific M1/M2 majority-vote settings: ⟨1,100⟩→76.7, ⟨5,20⟩→80.0, ⟨10,10⟩→79.8, ⟨100,1⟩→73.0 (these are reported GSM8K majority-vote results illustrating benefit of diverse prompts + sampling).",
            "key_findings": "Combining diverse prompts (varying exemplars) with sampling and a learned voting verifier improves reasoning accuracy consistently across models and tasks. Step-aware verification further improves selection by scoring individual steps, increasing interpretability and often improving final accuracy. Diversity in prompts yields more varied reasoning paths and final answers, which helps the verifier distinguish correct answers from varied errors.",
            "counter_examples_or_negative_results": "Step-aware verifier slightly lowered performance for code-davinci-002 on GSM8K in one reported experiment. On two commonsense multiple-choice tasks, DIVERSE was slightly worse overall (≈ -1.9%) than PaLM-based self-consistency; StrategyQA saw a ≈3.0% drop attributed to noisy pseudo-exemplars from self-teaching. Self-teaching to create exemplars can introduce false-positive exemplars, harming multiple-choice tasks (StrategyQA) more severely.",
            "uuid": "e4770.0",
            "source_info": {
                "paper_title": "Making Language Models Better Reasoners with Step-Aware Verifier",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Diverse Prompts + Sampling",
            "name_full": "Diverse prompts combined with sampling decoding",
            "brief_description": "A method to increase reasoning-path diversity by sampling different exemplar subsets to create multiple distinct prompts (M1 prompts) and sampling multiple reasoning paths per prompt (M2), yielding M1×M2 candidate reasoning chains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (evaluated also with davinci and text-davinci-002)",
            "model_description": "OpenAI 'davinci' family models used with sampling (temperature 0.5); prompts constructed by randomly sampling exemplars from an exemplar base.",
            "reasoning_method_name": "Diverse prompts (M1 prompts) + sampling (M2 per prompt)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Randomly select M1 different exemplar-based prompts per question and sample M2 reasoning paths per prompt; this produces M = M1×M2 candidate reasoning paths to be aggregated (majority vote or weighted by a verifier).",
            "task_name": "GSM8K (primary analysis) and other reasoning benchmarks",
            "task_description": "GSM8K: grade-school math problems where chain-of-thought helps; diversity measured by distinct intermediate result chains and distinct final answers per question.",
            "performance": "GSM8K majority-vote results (code-davinci-002) reported for different ⟨M1,M2⟩: ⟨1,100⟩ → 76.7%; ⟨5,20⟩ → 80.0%; ⟨10,10⟩ → 79.8%; ⟨100,1⟩ → 73.0%. Best results achieved by mixing diverse prompts and sampling (⟨5,20⟩).",
            "comparison_with_other_method": true,
            "performance_other_method": "Pure sampling with a fixed prompt (⟨1,100⟩) achieves 76.7% (majority voting); combining diverse prompts with sampling (⟨5,20⟩) yields 80.0% — showing diverse prompts + sampling outperform pure sampling decoding under majority voting.",
            "key_findings": "Diverse prompts increase the variety of reasoning paths and distinct final answers, which improves majority-voting aggregation and also provides more varied negative samples for verifier training; neither diverse prompts alone nor sampling alone is optimal — combining both gives best performance.",
            "counter_examples_or_negative_results": "Extreme choices (e.g., many prompt variations with only 1 sample each like ⟨100,1⟩) performed worse (73.0%) than balanced M1/M2 settings; marginal gains saturate as number of sampled paths M increases (marginal improvement beyond M ≥ 50).",
            "uuid": "e4770.1",
            "source_info": {
                "paper_title": "Making Language Models Better Reasoners with Step-Aware Verifier",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Voting Verifier",
            "name_full": "Voting verifier (weighted voting using a learned verifier)",
            "brief_description": "Aggregates candidate answers by combining majority-voting with verifier probabilities: sum verifier scores for candidates that share the same final answer and choose answer with highest total weight.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeBERTa-v3-large verifier (trained); used with outputs of OpenAI davinci-family LMs",
            "model_description": "Verifier is a fine-tuned DeBERTa-v3-large model with a scalar head over [CLS] to predict probability that a candidate reasoning path yields correct answer; trained on generated positive/negative chains.",
            "reasoning_method_name": "Voting verifier",
            "reasoning_method_type": "other",
            "reasoning_method_description": "For each generated reasoning path (x_i, z_i, y_i) compute f(x_i,z_i,y_i) = verifier probability of being correct. Aggregate by answer y: choose y that maximizes sum over candidate paths with y_i = y of f(...). This combines counting and verifier confidence.",
            "task_name": "GSM8K and other reasoning benchmarks (verifier trained per task with up to 1000 training pairs)",
            "task_description": "Same reasoning tasks where many candidate reasoning chains are produced; verifier trained on generated chain-of-thoughts labeled by whether final answer matches ground truth.",
            "performance": "Paper reports that voting verifier outperforms plain majority voting and a verifier-alone aggregator; quantitative per-task numbers not fully enumerated in-text, but DIVERSE (which uses voting verifier) achieves reported GSM8K improvements (e.g., to 83.2%).",
            "comparison_with_other_method": true,
            "performance_other_method": "Majority voting and verifier-only compared: voting verifier performs best in experiments; verifier-only (without voting) causes significant performance drops; majority voting can fail when correct paths are minority but high-quality (voting verifier recovers these).",
            "key_findings": "Combining verifier confidence with voting (weighted voting) makes final-answer selection more robust, enabling choosing correct-but-minority answers when majority is wrong; voting verifier is an important contributor to DIVERSE's gains.",
            "counter_examples_or_negative_results": "Verifier-only aggregation (no voting) causes large drops in performance; majority voting alone can be misled when majority of sampled paths are incorrect.",
            "uuid": "e4770.2",
            "source_info": {
                "paper_title": "Making Language Models Better Reasoners with Step-Aware Verifier",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Step-aware Verifier",
            "name_full": "Step-aware voting verifier (step-level supervision)",
            "brief_description": "Extends the voting verifier by supervising and scoring individual reasoning steps (labels per step) so the verifier can detect which steps in a chain are correct/incorrect and weight candidates accordingly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeBERTa-v3-large step verifier (trained); applied to code-davinci-002 outputs",
            "model_description": "Step-level classifier uses hidden state of last token of each step from DeBERTa-v3-large to predict probability that a particular step is correct; trained with auxiliary BCE loss weighted by hyperparameter alpha.",
            "reasoning_method_name": "Step-aware voting verifier",
            "reasoning_method_type": "other",
            "reasoning_method_description": "Label individual steps in candidate reasoning chains as correct/incorrect (positive steps are those matching steps seen in positive chains or semantically equivalent via an NLI model); add auxiliary loss to train step-level predictions f' and use step scores to improve final aggregation and interpretability.",
            "task_name": "GSM8K (detailed evaluation), CommonsenseQA (also evaluated)",
            "task_description": "Benchmarks where reasoning chains consist of multiple intermediate steps (e.g., math chain-of-thought) and where identifying which intermediate step goes wrong is useful.",
            "performance": "Step-aware verifier generally improves performance over voting verifier across most experiments; paper reports improvements on GSM8K and CommonsenseQA in most runs. Exception: a small decrease for code-davinci-002 on GSM8K in one setting.",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared to voting verifier without step labels, step-aware verifier produced more correct steps before mistakes (human-evaluated); in failed cases, step-aware had more correct steps in 33% of failed tests and fewer in 17% (human eval). Overall final-answer accuracy typically improved, but small negative case existed for code-davinci-002 on GSM8K.",
            "key_findings": "Step-level supervision helps the verifier detect where chains go wrong, improves final-answer selection and interpretability, reduces redundant steps and increases proportion of correct intermediate steps in incorrect chains. Human evaluation shows step-aware mechanism yields more correct intermediate steps before the first error.",
            "counter_examples_or_negative_results": "Slight performance degradation observed for code-davinci-002 on GSM8K in one experiment; step labeling relies on ability to find semantically equivalent steps in positive chains and requires additional labeled data/heuristics (uses MNLI for general tasks), which can be imperfect.",
            "uuid": "e4770.3",
            "source_info": {
                "paper_title": "Making Language Models Better Reasoners with Step-Aware Verifier",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (majority voting over sampled chain-of-thoughts)",
            "brief_description": "An aggregation method that samples many chain-of-thought reasoning paths from a fixed prompt and selects the most consistent final answer via majority voting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM (in prior SOTA) and also applied to other LMs in comparisons (davinci family in this paper)",
            "model_description": "Self-consistency was previously applied with PaLM-540B in literature; in this paper self-consistency is used as a baseline by sampling many chains (e.g., 100) from a fixed prompt and majority-voting final answers.",
            "reasoning_method_name": "Self-Consistency (sampling + majority vote)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Sample many reasoning chains from a single prompt (sampling decoding) and select the final answer that appears most frequently (majority voting).",
            "task_name": "GSM8K and other benchmarks (baseline comparisons)",
            "task_description": "Benchmarks where chain-of-thought sampling and majority voting can increase robustness by aggregating multiple sampled chains.",
            "performance": "Reported as a strong baseline; example numbers in paper: in some contexts earlier SOTA using PaLM-based self-consistency; paper reports cases where DIVERSE improves over self-consistency (e.g., DIVERSE GSM8K to 83.2% from a prior 74.4% baseline reported in intro). Also paper reports that DIVERSE improves over self-consistency by 5.6% on code-davinci-002 (aggregate statement).",
            "comparison_with_other_method": true,
            "performance_other_method": "DIVERSE (diverse prompts + voting verifier) outperforms self-consistency (sampling from a single prompt + majority voting) consistently in the experiments; however, on some commonsense multiple-choice tasks PaLM-based self-consistency remains slightly better in the paper's cross-model comparison (DIVERSE is slightly worse by ~1.9% overall on the two commonsense tasks).",
            "key_findings": "Self-consistency benefits from sampling but can be limited by lack of prompt diversity; DIVERSE's additional prompt diversity and learned verifier enable selecting correct-but-minority answers that majority voting would miss, yielding consistent improvements.",
            "counter_examples_or_negative_results": "Self-consistency can outperform DIVERSE on certain commonsense multiple-choice tasks when PaLM was used (paper reports DIVERSE slightly below PaLM-based self-consistency on CommonsenseQA/StrategyQA aggregate comparison).",
            "uuid": "e4770.4",
            "source_info": {
                "paper_title": "Making Language Models Better Reasoners with Step-Aware Verifier",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Greedy Decode (single CoT)",
            "name_full": "Greedy decoding of a single chain-of-thought (single-shot chain-of-thought)",
            "brief_description": "Generate a single reasoning chain (chain-of-thought) greedily (deterministically) from a prompt and use its final answer as output.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "davinci-family LMs (used as baseline)",
            "model_description": "Greedy chain-of-thought uses a single sampled or greedily-decoded chain produced by the language model using a chain-of-thought prompt (no ensembling).",
            "reasoning_method_name": "Greedy Decode (single chain-of-thought)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Generate one chain-of-thought per question (greedy decoding or single sample) and output the final answer from that chain; no ensemble or aggregation.",
            "task_name": "GSM8K and other reasoning benchmarks (baseline)",
            "task_description": "Benchmarks where single-chain generation is compared against sampling/ensembling methods.",
            "performance": "Chain-of-thought (as introduced prior work) improved GSM8K from 17.9% to 58.1% in earlier work; in this paper greedy decode is used as a baseline and DIVERSE reports large relative improvements over greedy decoding (text mentions DIVERSE brings improvements e.g., 22.2% on davinci relative to greedy).",
            "comparison_with_other_method": true,
            "performance_other_method": "DIVERSE and self-consistency (sampling + aggregation) substantially outperform greedy single-chain decoding; DIVERSE reports particularly large gains over greedy decoding across models and tasks.",
            "key_findings": "Single-chain greedy decoding is inferior to methods that aggregate multiple diverse chains (sampling/self-consistency) and to DIVERSE; ensembling or verifier-based aggregation captures robustness that single outputs lack.",
            "counter_examples_or_negative_results": "No cases reported where greedy single-chain decoding outperforms diverse/ensemble methods in the paper's experiments.",
            "uuid": "e4770.5",
            "source_info": {
                "paper_title": "Making Language Models Better Reasoners with Step-Aware Verifier",
                "publication_date_yy_mm": "2022-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "PaLM: Scaling Language Modeling with Pathways",
            "rating": 1,
            "sanitized_title": "palm_scaling_language_modeling_with_pathways"
        },
        {
            "paper_title": "Star: Bootstrapping Reasoning with Reasoning",
            "rating": 1,
            "sanitized_title": "star_bootstrapping_reasoning_with_reasoning"
        }
    ],
    "cost": 0.01640625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Making Large Language Models Better Reasoners with Step-Aware Verifier</p>
<p>Yifei Li yifeili@microsoft.com 
School of Computer Science
National Key Laboratory for Multimedia Information Processing
Peking University</p>
<p>Microsoft Corporation</p>
<p>Zeqi Lin zeqi.lin@microsoft.com 
Microsoft Corporation</p>
<p>Shizhuo Zhang 
Microsoft Corporation</p>
<p>Qiang Fu qifu@microsoft.com 
Microsoft Corporation</p>
<p>Bei Chen bei.chen@microsoft.com 
Microsoft Corporation</p>
<p>Jian-Guang Lou jlou@microsoft.com 
Microsoft Corporation</p>
<p>Weizhu Chen wzchen@microsoft.com 
Microsoft Corporation</p>
<p>Making Large Language Models Better Reasoners with Step-Aware Verifier</p>
<p>Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DIVERSE has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DIVERSE on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% → 83.2%).</p>
<p>Introduction</p>
<p>Large pretrained language models (PLMs) have shown remarkable performance on various natural language processing tasks, either by few-shot learning with prompts (Radford et al., 2019;Le Scao and Rush, 2021;Jin et al., 2022) or by fine-tuning (Houlsby et al., 2019;Hu et al., 2021;He et al., 2022). However, despite the increasing size and capacity of PLMs such as GPT-3 with 175B parameters (Brown et al., 2020) and PaLM with 540B parameters (Chowdhery et al., 2022), their reasoning abilities are still limited and often require mul- * Work was done during an internship at Microsoft Research Asia. tiple steps to produce correct answers, especially for tasks involving arithmetic, commonsense, or inductive reasoning (Cobbe et al., 2021).</p>
<p>Recent works (Wei et al., 2022;Zhou et al., 2022;Kojima et al., 2022;Lampinen et al., 2022) have demonstrated that PLMs possess some latent reasoning capabilities, but they need carefully designed prompts to activate them. For instance, Wei et al. (2022) proposed chain-of-thought reasoning, which inserts multi-step reasoning paths before generating the final answers, and achieved significant improvement on the GSM8K arithmetic benchmark (Cobbe et al., 2021). Wang et al. (2022c) further introduced a voting mechanism to select the most consistent answer among different reasoning paths, and achieved state-of-the-art results on several reasoning benchmarks using the PaLM model (Chowdhery et al., 2022). Building on these successes, this paper continues this line of research and advances the reasoning capabilities of PLMs in three aspects, as illustrated in Figure 1.</p>
<p>First, we propose to increase the diversity of reasoning paths by not only sampling from a single prompt, but also varying the prompt itself. We hypothesize that different prompts can elicit different ways of thinking, while the correct answer should be robust to these variations. Second, we propose to use a verifier to score the quality of each reasoning path and guide the voting mechanism. We argue that not all reasoning paths are equally good Figure 2: Chain-of-thought reasoning for GSM8K math word problem. The prompt is colored black and the reasoning path produced by the language model is colored teal. This reasoning path contains two reasoning steps.</p>
<p>or reliable, and some may contain errors or inconsistencies that can be detected by the verifier. Third, we propose to assign a fine-grained label to each step of the reasoning path and use a step-aware verifier to attribute the correctness or wrongness of the final answer to each step. We conjecture that some steps may be correct but followed by wrong steps or vice versa, and identifying these cases can help diagnose and improve the reasoning process.</p>
<p>We name our method as DIVERSE (diverse verifier on reasoning step) and evaluate it on eight reasoning benchmarks that require different types of reasoning skills. We use three OpenAI PLMs (davinci, text-davinci-002, and code-davinci-002) and compare our results with recent state-of-the-art methods. We find that DIVERSE can consistently and significantly improve the performance of PLMs on these tasks, and achieve new state-of-the-art results on six of them 1 : GSM8K (74.4% → 83.2%), AsDiv (81.9% → 88.7%), MultiArith (99.3% → 99.8%), SVAMP(86.6% → 87.0%), SingleEq (79.5% → 94.9%), and CLUTRR (67.0% → 95.9%).</p>
<p>Our data is publicly available at https://github. com/microsoft/DiVeRSe.</p>
<p>Diverse Verifier on Reasoning</p>
<p>Step Figure 1 shows the overview of DIVERSE. The key insights are three-fold: (1) leveraging diverse prompts to induce more diverse reasoning paths from the language models (Section 2.1); (2) train-1 Most of the previous SOTA results were achieved by selfconsistency on PaLM-540B (Chowdhery et al., 2022). ing a voting verifier to better derive the final answers from multiple reasoning paths (Section 2.2);</p>
<p>(3) leveraging step correctness to further boost the voting verifier (Section 2.3).</p>
<p>Diverse Prompts</p>
<p>To reason effectively, it is beneficial to explore diverse reasoning paths, following the idea that "All Roads lead to Rome". Wang et al. (2022c) proposed to generate various reasoning paths from language models by sampling decoding. However, their method relies on a fixed set of exemplars for all prompts, which may introduce bias and limit the diversity of the generated reasoning paths. To address this issue, we randomly select M 1 different prompts for each question, and then sample M 2 reasoning paths for each prompt using sampling decoding. This way, we obtain M = M 1 × M 2 diverse reasoning paths for each question. 2</p>
<p>Voting Verifier</p>
<p>Verifier. The verifier takes a question and a candidate reasoning path as input, and outputs the probability that the reasoning path leads to the correct answer. We use deberta-v3-large (He et al., 2021) as the backbone model, with a small scalar head that outputs predictions on the [CLS] token.</p>
<p>Training the verifier. For each training question, we generate multiple candidate reasoning paths using chain-of-thought reasoning. We regard the reasoning paths that match the ground truth final answer as positive, and the others as negative.</p>
<p>Voting Verifier. Wang et al. (2022c) use majority voting to aggregate the predictions of different reasoning paths. This method may fail when the majority of the reasoning paths are misled, while the minority of the reasoning paths are reasonable. We propose voting verifier, which leverages both voting and verifier:
y = arg max y M i=1 1 y i =y · f (x i , z i , y i ),(1)
where 1 y i =y is an indicator function that returns 1 (or 0) if y i = y (or not), and f (·) is the probability produced by the verifier.</p>
<p>Step-aware Voting Verifier</p>
<p>Each reasoning path consists of several steps. We hypothesize that not all the steps in an incorrect Figure 3: How step-level labels are extracted. This figure shows four reasoning paths for a math word problem: the first two are positive and the bottom two are negative. The path 7 → 9 → 18 means that the first step calculates 7, the second step calculates 9, and the third step calculates the final answer 18. For the last path, the third step (which calculates 8) has never occurred in any positive reasoning paths, thus we regard this step and all steps after it as negative steps.</p>
<p>reasoning path are equally wrong, and some steps may still be useful for reasoning. To exploit this, we extend the voting verifier to a step-aware voting verifier by introducing an extended loss function:
L = L 0 + α · L 1 , L 1 = |D| i=1 |S i | j=1 BCE(label i,j , f ′ (input i , j)).
(2) α is a hyperparameter to balance the original loss L 0 and the step-level auxiliary loss L 1 ; S i,1 , S i,2 , ..., S i,|S i | are the steps in z i ; label i,j indicates whether S i,j is correct or not; f ′ (input i , j) represents the probability of the positive label for S i,j . 3 To obtain the step-level labels (i.e., label i,j ) for negative training data with wrong answers, we design an algorithm that compares intermediate results among steps in positive/negative reasoning paths. Figure 3 illustrates this algorithm. This algorithm can not only work on math word problems, but also generalize to other reasoning tasks: we use an off-the-shelf natural language inference model, roberta-large-mnli (Liu et al., 2019), to check whether two reasoning steps are semantically equivalent or not. Given a reasoning step, if we cannot find any semantically equivalent step in the positive reasoning paths, we label it and all the subsequent steps as negative steps. </p>
<p>Details</p>
<p>Language Models. We use three OpenAI language models: davinci, text-davinci-002 and codedavinci-002. We use the default parameters except a temperature of 0.5 in sampling.</p>
<p>Exemplars. For arithmetic/commonsense/inductive reasoning, each prompt contains 5/7/7 exemplars. For DIVERSE, each question has 5 different prompts, and 20 reasoning paths are sampled from the language model for each prompt. For arithmetic reasoning, the exemplars are randomly sampled from the training dataset of GSM8K; for CLUTRR, the exemplars are sampled from its training dataset, with reasoning paths synthesized by handcraft rules (detailed settings for CLUTRR are listed in Appendix D); for StrategyQA and Com-monsenseQA, their original datasets do not contain enough exemplars with well-annotated reasoning paths, so we construct 1, 000 pseudo exemplars by "self-teaching" (the approach and the noise issue are discussed in Appendix B) from "seed" exemplars provided by Wei et al. (2022).</p>
<p>Training Datasets. For each task, we sample 1, 000 ⟨question, answer⟩ pairs from the training dataset to train the verifier.</p>
<p>Verifier. We fine-tune deberta-v3-large (He et al., 2021) with learning rate 1 × 10 −5 and batch size 128. For the step-aware verifier, we select the best α among 0.0/0.1/0.2/0.3.</p>
<p>Method</p>
<p>GSM8K</p>
<p>AsDiv   Table 2: A GSM8K example (code-davinci-002) with step-level scores given by the step-aware verifier. The scores can not only improve the performance but also help the understanding of where the reasoning paths start to be incorrect. Table 1 shows the overall experimental results. We mainly compare DIVERSE with two baselines: (1) greedily decoding a single reasoning path (Wei et al., 2022), referred to as Greedy Decode;</p>
<p>Main Results</p>
<p>(2) sampling 100 reasoning paths, then select the final answer via majority voting (Wang et al., 2022c), referred to as Self-Consistency.</p>
<p>Effectiveness</p>
<p>Experimental results clearly demonstrate that DI-VERSE can bring significant and consistent improvements over recent strong baselines. The improvements are across different models (davinci, text-davinci-002 and code-davinci-002) as well as different reasoning skills (eight tasks in three reasoning skills). Taking GSM8K as an example, compared to Greedy Decoding and Self-Consistency, DIVERSE brings improvements of 22.2%/12.0% on davinci, 33.1%/12.0% on text-davinci-002, and 27.0%/5.6% on code-davinci-002. Compared to Self-Consistency, DIVERSE achieves average improvements of 5.6%/5.1%/54.3% on the three reasoning skills, respectively.</p>
<p>Comparing to Previous SOTAs</p>
<p>In   sulting in more false-positive exemplars in the pseudo exemplar base (Details will be discussed in Section B.2). Regarding inductive reasoning, DI-VERSE achieves a surprisingly good performance of 95.9% on the CLUTRR task, outperforming (+28.9%) previous SOTA result with fine-tuning (Sinha et al., 2019). 5 Table 2 shows an example of step-level scores given by the step-aware verifier. Steps in the correct reasoning path have relatively high scores, while the scores in the wrong reasoning path show where the path starts to be wrong. This indicates that besides improving the performance, the step-aware verifier can also bring interpretability to show the step-level correctness. We also show some extra examples of majority-voting in Table 10.
⟨M 1 , M 2 ⟩ GSM8K M 1 = 1, M 2 = 100 76.7 M 1 = 5, M 2 = 20 80.0 M 1 = 10, M 2 = 10 79.8 M 1 = 100, M 2 = 1 73.0</p>
<p>Case Study</p>
<p>Analysis</p>
<p>We also conduct ablation experiments and analysis to investigate the keys to the success of DIVERSE. </p>
<p>The Effectiveness of Diverse Prompts</p>
<p>By diversifying both prompts and reasoning paths (⟨M 1 = 5, M 2 = 20⟩), we consistently improve performance over the sampling decoding approach (⟨M 1 = 1, M 2 = 100⟩) of Wang et al. (2022c), as shown in Table 3. Both methods use majority voting. Table 4 further reveals that neither only using diverse prompts nor only using sampling is optimal. In other words, the best performance is achieved by combining diverse prompts and sampling. Moreover, Figure 4 demonstrates that diverse prompts lead to more diverse reasoning paths. We hypothesize that this diversity contributes to the performance improvement by: (1) making correct results more distinguishable from varied errors during inference; and (2) providing more diverse negative samples for enhancing the verifier's generalizability during training.</p>
<p>The Effectiveness of Voting Verifier</p>
<p>We compare three algorithms to conclude the agreement from diverse reasoning paths: majority voting, verifier, and voting verifier.  Step Verifier (33%) Equal (50%) Verifier (17%) (b) With the step-aware mechanism, incorrect paths contain more correct steps. Figure 5: Human evaluation on GSM8K shows the effectiveness of the step-aware mechanism for verifier.</p>
<p>The Effectiveness of Step-aware Verifier</p>
<p>We evaluate the impact of incorporating step-level information into the voting verifier of DIVERSE. Table 6 shows the performance of DIVERSE with and without the step-aware mechanism on both the GSM8K and the CommonsenseQA datasets. We find that using the step-aware verifier improves the performance in most of the experiments. The only exception is code-davinci-002 on GSM8K, where the step-aware verifier slightly lowers the performance. We hypothesize that code-davinci-002 is more capable of generating high-quality reasoning paths, and thus does not benefit much from the step-level information.</p>
<p>Detailed Human Evaluation of Reasoning Steps. We further analyze the quality of generated reasoning steps, by asking human annotators to judge Formulation Error (95%) Missing Steps (2%) Calculation Error (2%) Number Hallucination (1%) Figure 6: The distribution of error types in incorrect reasoning steps.</p>
<p>whether the GSM8K reasoning steps produced by DIVERSE (with/without step-aware mechanism) are good or not. Here "good" means not only correct formulas and calculation results but also textual fluency and logical coherence.</p>
<p>We further examine the quality of the reasoning steps generated by DIVERSE (with/without stepaware mechanism) for GSM8K, by asking human annotators to rate them based on correctness, fluency, and coherence. For each test question, we compare three reasoning paths produced by codedavinci-002: the one with the highest verifier score, the one with the highest step-aware verifier score, and a randomly chosen one. The annotators (master students) label any incorrect or unsatisfactory reasoning steps in each path (single-blind) and explain why. We collect annotations for 200 test questions, half of which have correct final answers from all three paths, and half of which have incorrect final answers from all three paths.</p>
<p>We find that all the reasoning paths with correct final answers are also correct in every intermediate step, which shows that code-davinci-002 can reliably generate accurate reasoning steps, not just lucky guesses. However, we also find that many of the correct reasoning paths have unnecessary steps. Figure 5(a) shows that 40% of the random paths have redundant steps, and the verifier can lower this percentage to 31%. We also find that the step-aware verifier can further eliminate redundant reasoning steps from 31% to 20%.</p>
<p>Furthermore, for the incorrect reasoning paths, we find that the step-aware mechanism helps produce more correct steps before making mistakes.</p>
<p>For each failed test question, we compare the number of correct steps in the path with the highest verifier score and the path with the highest step-aware verifier score (by human evaluation). Figure 5(  shows that for 33%/17% of the failed test cases, the step-aware verifier generates more/fewer correct steps than the verifier without the step-aware mechanism.</p>
<p>Step Error Types. Figure 6 shows the distribution of error types in the incorrect reasoning steps. We see that 95% of the errors are caused by incorrect formulations (i.e., using wrong intermediate results or operators and generating invalid formulas, which lead to incorrect answers). We also see that, although code-davinci-002 often makes division calculation errors (e.g., 10/3 = 3), both the verifier and the step-aware verifier can effectively assign low scores to such paths, thus improving the performance.</p>
<p>How Many Diverse Outputs Do</p>
<p>We Need? Figure 7 shows the accuracy at different M values, where M is the number of reasoning paths sampled from the 100 generated paths for each question. We observe that: (1) the accuracy increases with more reasoning paths, but the improvement becomes marginal at M ≥ 50;</p>
<p>(2) DIVERSE outperforms self-consistency significantly and consistently at different M values.</p>
<p>How Many Training Data Do We Need?</p>
<p>DIVERSE requires a dataset with reasoning paths for training the verifier. Figure 8 shows how the size of this dataset affects the performance. We observe that: the performance is only reduced by about 2%, even if the size of training data is cut by 75% (from 1, 000 to 250). With the same reasoning paths, voting verifier performs better than majority voting, while verifier without voting causes significant performance drops. </p>
<p>The Impact of the Number of Exemplars</p>
<p>We conduct experiments for k = 3/5/8 (k is the number of exemplars used in each prompt) on GSM8K. Figure 9 shows the results. We observe that: using 8 exemplars in each prompt can further boost the accuracy of GSM8K to 83.2%. Reasoning via Prompting Gigantic Language Models. Gigantic language models like GPT-3 (Brown et al., 2020) have demonstrated impressive few-shot learning capabilities in many tasks and have attracted many research interests on making gigantic language models better few-shot learners (Zhao et al., 2021;Holtzman et al., 2021;Min et al., 2021;Liu et al., 2022;Lu et al., 2021;Rubin et al., 2021;Min et al., 2022). However, these methods struggle to address tasks requiring reasoning skills.</p>
<p>To mitigate this, recently there is a line of research that focuses on unleashing the reasoning capabilities of gigantic language models via better prompting strategies. proposed to leverage gigantic language models to decompose questions into sub-questions, thereby addressing them in an iterative manner; Kojima et al. (2022) proposed that gigantic language models can even be good zero-shot reasoners, by designing prompts that can induce language models to do reasoning step-by-step; Lampinen et al. (2022) </p>
<p>Conclusion and Future Work</p>
<p>In this paper, we present DIVERSE, a novel and general method to enhance the reasoning abilities of large language models. Our method builds on the idea of prompting language models with multistep reasoning paths, but introduces three key innovations: diverse prompts, voting verifier, and stepwise verifier. The latter is especially novel and effective, as it verifies each reasoning step separately and we provides a detailed analysis of the model's behavior in each step. We demonstrate the superiority of DIVERSE through extensive experiments. For instance, using code-davinci-002, our method achieves state-of-the-art performance on most reasoning tasks, surpassing the 540B PaLM model with previous prompting techniques.</p>
<p>There are many directions for our future work. (1) As discussed in Appendix B.2, we will continue to investigate how to reduce or recognize false positive pseudo exemplars.</p>
<p>(2) We plan to investigate mechanisms to produce better diverse prompts than simple sampling. (3) We will extend DIVERSE to other tasks and continue to design better prompting techniques to elicit the power of gigantic language models.</p>
<p>Limitations</p>
<p>Computing Resources. Despite the surprising performance it achieves, our framework needs to be applied to large language models like GPT-3 or PaLM. Inference with these models costs more time and budgets than fine-tuning models like RoBERTa (Liu et al., 2019).</p>
<p>Faithfulness. Although DIVERSE can significantly improve the accuracy of final answers, we still cannot guarantee that the reasoning paths produced by the language models are 100 percent faithful. This is the key challenge and future direction for this line of research (chain-of-thought reasoning).</p>
<p>More Training Data. DIVERSE needs more labeled data with well-annotated reasoning paths to construct diverse prompts, and it also needs a training dataset for supervising the verifier. However, from another point of view, this limitation can also be regarded as a contribution that studies how chain-of-thought reasoning can be further improved if we have more training data than just a few exemplars.</p>
<p>Human Evaluation of Reasoning Steps. We use human evaluation to measure the quality of the intermediate steps in reasoning paths since few current works provide reliable frameworks to evaluate the quality of reasoning steps.  This is the Appendix for the paper: "On the Advance of Making Language Models Better Reasoners".</p>
<p>References</p>
<p>A Preliminaries</p>
<p>Prompting. Prompting means prepending a few exemplars to the task input x and generating the output y from the pretrained language model:
p(y|C, x) = |y| t=1 p LM (y t |C, x, y &lt;t ),(3)
where C is the concatenation of K exemplars: C = (x 1 , y 1 ); (x 2 , y 2 ); ...; (x K , y K ). (4) We denote prompt as the concatenation of the exemplars C and the input x.</p>
<p>Reasoning Paths. For reasoning tasks that aim to generate an answer y for a question x, Wei et al. (2022) proposed the insertion of a reasoning path z before generating the answer y:
C ′ = (x 1 , z 1 , y 1 ); ...; (x K , z K , y K ),(5)
where z i is a text reasoning path of how the answer y i is reasoned step-by-step for question x i .</p>
<p>Then, during inference, a reasoning path z will be generated before the answer y:
p(y|C ′ , x) = p(z|C ′ , x) · p(y|C ′ , x, z). (6)
Figure 10 demonstrates this idea in arithmetic reasoning (GSM8K), and </p>
<p>B Boosting Reasoning Paths via Self-Teaching</p>
<p>In this section, we first introduce self-teaching, the method we use to construct a larger exemplar base when the original dataset does not contain enough data with well-annotated reasoning paths (Appendix B.1). We then discuss the noise issue when facing multiple-choice tasks (Appendix B.2).</p>
<p>B.1 Self Teaching</p>
<p>A critical issue of DIVERSE is how to provide diverse prompts. 6 Supposing that there is an exemplar base E, we can sample K exemplars from it to construct a prompt, and repeat this M 1 times independently to construct M 1 prompts with diverse exemplars.</p>
<p>For scenarios that do not have sufficient exemplars (i.e., |E| &lt; K * M 1 ), we propose to bootstrap the diversity of prompts by "self-teaching", i.e., generating pseudo reasoning paths from a few exemplars and some ⟨question, answer⟩ pairs without reasoning paths. 7 Suppose that D is a dataset without reasoning paths, consisting of CLUTRR 447 Kelly and her mother Ernest made breakfast together. Constance and her husband Ernest wanted a child badly What kind of relative is Kelly to Constance? The possible relationships are: sister, son, aunt, granddaughter, father, grandfather, grandmother, mother-in-law, uncle, niece, mother, brother, daughter, nephew, grandson, son-in-law, father-in-law, daughter-in-law. (x, y * ) pairs. Given the small exemplar base E, for each (x, y * ) ∈ D, we can use prompting to generate a reasoning path z and the predicted answer y. We define the pseudo exemplar base E ′ as:
E ′ = {(x, z, y)|(x, y * ) ∈ D, y = y * },(7)
then E ∪ E ′ can be regarded as the new exemplar base for generating diverse prompts.</p>
<p>B.2 Noises in Multiple Choice Tasks</p>
<p>In our experimental setup, StrategyQA and Com-monsenseQA are more challenging than other tasks, as they use pseudo exemplars generated through "self-teaching" (Appendix B.1).</p>
<p>"Self-teaching" may lead to bad exemplars, whose reasoning paths are invalid but happen to yield answers coinciding with the ground truth. Questions in StrategyQA/CommonsenseQA are twochoice/four-choice questions, respectively. Therefore, such noise would be more serious in Strate-gyQA than in CommonsenseQA. This somehow explains why DIVERSE can achieve comparable performance (−0.8%) as the PaLM-based SOTA on CommonsenseQA, while it sees a 3.0% performance decline to PaLM on StrategyQA, which has only two choices. In other words, it is easier for StrategyQA to yield a right answer but a misleading reasoning path. </p>
<p>C Data Statistics</p>
<p>D Our Changes to CLUTRR</p>
<p>In our experiments, two changes are applied to the CLUTRR benchmark: (1) appending candidate answers to each question; (2) constructing reasoning paths based on rules. Table 9 shows an example of CLUTRR data after our modification.</p>
<p>Candidate Answers. Besides the original questions (e.g., "Mary, a female, took her husband who is a male, Roy, out for lunch. Ernest bought to dress for his father Roy. What kind of relative is Ernest to Mary?"), we also provide all the candidate answers (i.e., "The possible relationships are: sister, son, aunt, granddaughter, father, grandfather, grandmother, mother-in-law, uncle, niece, mother, brother, daughter, nephew, grandson, son-in-law, father-in-law, daughter-in-law") in the input sequence. Our preliminary experiments show that, the gigantic language models cannot reach more than 50% accuracy without the sequence of candidate answers.</p>
<p>Reasoning Paths. For each question, Sinha et al.</p>
<p>(2019) also provided a knowledge graph that formulates the relations directly mentioned in the question. Each knowledge graph consists of several ⟨e 1 , r, e 2 ⟩ triplets, which means there is a relation r from e 1 to e 2 . Take the aforementioned question as an example, the knowledge graph consists of two triplets: ⟨Mary, husband, Roy⟩ and ⟨Ernest, father, Roy⟩.</p>
<p>For each question, we construct the reasoning path based on its knowledge graph. We first topologically sort all triplets in the knowledge graph. For each triplet, we convert it to a reasoning step using the template "{e 2 } is the {r} of {e 1 }". After that, we can get the reasoning path by concatenating these reasoning steps. Take the aforementioned question as an example, the reasoning path is: "Roy is the husband of Mary. Roy is the father of Ernest. Thus, Ernest is the son of Mary." Input: Kelly and her mother Ernest made breakfast together. Constance and her husband Ernest wanted a child badly. What kind of relative is Kelly to Constance? The possible relationships are: sister, son, aunt, granddaughter, father, grandfather, grandmother, mother-in-law, uncle, niece, mother, brother, daughter, nephew, grandson, son-in-law, father-in-law, daughter-in-law. Output: Kelly is the daughter of Ernest. Ernest is the husband of Constance. Thus, Kelly is the daughter of Constance. So the answer is daughter. Table 9: Examples of the original and our constructed CLUTRR data. In our experiments, we concatenated the class information (marked with red) after the input question as the input, and constructed reasoning paths (marked with blue) via a simple text template as the output.</p>
<p>Question</p>
<p>Self-Consistency DIVERSE</p>
<p>[GSM8K] Siobhan has 2 fewer jewels than Aaron. Aaron has 5 more jewels than half of Raymond's jewels. If Raymond has 40 jewels, how many jewels does Siobhan have?</p>
<p>Aaron has 2 more jewels than half of Raymond's jewels. So Aaron has 40 / 2 + 2 = 22 jewels. Siobhan has 2 fewer jewels than Aaron so she has 22 -2 = 20 jewels. So the answer is 20. Answer: 20 (Wrong) Count: 33</p>
<p>Raymond has 40 jewels. Half of Raymond's jewels is 40 / 2 = 20. Aaron has 5 more jewels than half of Raymond's jewels, so he has 20 + 5 = 25 jewels. Siobhan has 2 fewer jewels than Aaron, so she has 25 -2 = 23 jewels. So the answer is 23. Answer: 23 (Correct) Count: 32 [StrategyQA] Could you go to New York Public Library and the Six Flags Great Escape in the same day?</p>
<p>The New York Public Library is in Manhattan. The Six Flags Great Escape is in Queensbury. Manhattan and Queensbury are too far apart to go to both in a single day. Thus, you could not go to New York Public Library and the Six Flags Great Escape in the same day. So the answer is no. Answer: no (Wrong) Count: 62</p>
<p>The New York Public Library is in New York City. The Six Flags Great Escape is in Queensbury, New York. Queensbury is about 3.5 hours away from New York City by car. Thus, you could go to the New York Public Library and the Six Flags Great Escape in the same day. So the answer is yes. Answer: yes (Correct) Count: 38 Table 10: Examples of code-davinci-002 on GSM8K. Compared to self-consistency (majority voting), DIVERSE can select the correct-but-not-most answer out of the sampled candidates, thus improving the reasoning performance.</p>
<p>Figure 1 :
1Our proposed method, DIVERSE (Diverse Verifier on Reasoning Step).</p>
<p>Figure 4 :
4Diverse prompts increase the diversity of GSM8K reasoning paths and their final answers. This is beneficial for the voting verifier. Left: the average number of distinct reasoning paths per question (we consider two reasoning paths to be the same if they have the same intermediate result chain as shown in Figure 3). Right: the average number of distinct final answers per question.</p>
<p>Figure 7 :
7GSM8K accuracy at different M values (how many reasoning paths are used for each question).</p>
<p>Figure 8 :
8. Researchers in the literature have proposed many benchmarks requiring various reasoning skills, including commonsense reasoning(Zellers et al., 2018; Talmor et al., 2019; Bhagavatula et al., 2019; Geva et al., 2021)  numerical reasoning (Dua et al., 2019), multi-hop reasoning (Yang et al., 2018), arithmetic reasoning (Koncel-Kedziorski et al., 2015; Roy and Roth, 2015; Miao et al., 2020; Patel et al., 2021; Cobbe et al., 2021), logical reasoning (Liu et al., 2020; Yu et al., 2020), inductive reasoning (Sinha et al., 2019) and tabular reasoning (Chen et al., 2020; Zhu et al., 2021). Reasoning with Symbolic Systems. Much research in the literature enhances the reasoning capabilities of machine learning systems by exploiting symbolic systems, including knowledge graphs (Mihaylov and Frank, 2018; Bauer et al., 2018; Kundu et al., 2019; Wang et al., 2019; Lin et al., 2019; Ding et al., 2019; Feng et al., 2020; Wang et al., 2022b), or question taxonomies (Dua et al., 2019; Andor et al., 2019; Hu et al., 2019b; Wang et al., 2022a). Although these methods work well on specific benchmarks, they usually require domain-specific designs and human efforts, thus limiting the generalizability. Reasoning via Language Models. This line of work aims to address reasoning tasks in a general sequence-to-sequence manner, empowered by reasoning-aware pre-training or fine-tuning of language models. For example, Deng et al. (2021) DIVERSE performance (code-davinci-002) on GSM8K with different sizes of the training dataset (without labeled reasoning paths). proposed to train the language model with crawled data from the internet; Asai and Hajishirzi (2020) proposed a logic-guided data augmentation method to pre-train the language model; Shen et al. (2021); Cobbe et al. (2021) proposed to train a verifier to rank solutions sampled from fine-tuned language models; Geva et al. (2020); Yoran et al. (2022); Campagna et al. (2020); Wang et al. (2022a) proposed to equip language models with reasoning abilities by generating training examples with human-designed templates; Pi et al. (2022) proposed to inject reasoning capabilities into language models by continual pre-training on program execution data.</p>
<p>Figure 9 :
9Wei et al. (2022)  proposed chainof-thought reasoning, of which the key insight is the insertion of multi-step reasoning paths before generating the final answers;Wang et al. (2022c)   proposed to improve chain-of-thought reasoning via self-consistency, of which the key insight is to conclude the most consistent answer from different reasoning paths sampled from the language model;Zhou et al. (2022); Creswell et al. (2022)   DIVERSE performance (code-davinci-002)   on GSM8K when each prompt contains 3/5/8 exemplars.</p>
<p>proposed building a prompt by selecting examples and explanations together, thus substantially improving performance over selecting examples alone. Despite their great successes, these works come with their limitations. This paper is a continuation of this line of research, focusing on diverse verifier on reasoning steps.</p>
<p>Arithmetic Reasoning. Following Wang et al. (2022c), we use AsDiv (Miao et al., 2020), Sin-gleEq (Koncel-Kedziorski et al., 2015), MultiArith (Roy and Roth, 2015), SVAMP (Patel et al., 2021), and GSM8K (Cobbe et al., 2021). Inductive Reasoning. We use CLUTRR (Sinha et al., 2019), a diagnostic benchmark for inductive reasoning, requiring inferring kinship relations between characters in short stories.3 Experimental Setup </p>
<p>3.1 Reasoning Tasks </p>
<p>Commonsense Reasoning. Following Wang 
et al. (2022c), we use CommonsenseQA (Talmor 
et al., 2019) and StrategyQA (Geva et al., 2021). </p>
<p>Table 1 :
1The comparison of DIVERSE, Greedy Decode and Self-Consistency.The previous SOTA results (fine-tuned </p>
<p>Table 1 ,
1we also compare DIVERSE with: (1) 
previous SOTA results based on fine-tuning; (2) 
recent SOTA results (Wei et al., 2022) based on 
PaLM (Chowdhery et al., 2022), a gigantic lan-
guage model with 540 billion parameters. 4 </p>
<p>On all the five arithmetic reasoning tasks, DI-
VERSE (with code-davinci-002) achieves new 
SOTA results, with an average improvement of 
6.2%. On the two commonsense reasoning tasks, 
the performance of DIVERSE is slightly lower 
(−1.9%) than that of PaLM-based self-consistency. 
We speculate that the reason might be: these two 
commonsense reasoning tasks are multiple-choice 
tasks rather than open-ended generation tasks, re-</p>
<p>Table 3 :
3The effectiveness of diverse prompts (⟨5, 20⟩) 
compared to pure sampling decoding (Wang et al., 
2022c), under majority voting. </p>
<p>Table 4 :
4GSM8K majority voting results for different ⟨M 1 , M 2 ⟩ settings on code-davinci-002.</p>
<p>Table 5
5shows </p>
<p>Table 5 :
5The effectiveness of voting verifier. All exepriments in this table use ⟨M 1 , M 2 ⟩ = ⟨5, 20⟩.Random 
Selected 
Verifier Step 
Verifier </p>
<p>0 </p>
<p>15 </p>
<p>30 </p>
<p>45 ϰϭ </p>
<p>ϯϭ </p>
<p>ϮϬ </p>
<p>(a) The number of correct rea-
soning paths containing re-
dundant steps. </p>
<p>Table 6 :
6The effectiveness of step-aware voting verifier, with ⟨M 1 , M 2 ⟩ = ⟨5, 20⟩.</p>
<p>Table 7 :
7Besides arithmetic reasoning, we also inves-
tigate commonsense and inductive reasoning. </p>
<p>Table 7
7demonstrates this 
idea in commonsense reasoning (StrategyQA) and 
inductive reasoning (CLUTRR). </p>
<p>Dataset N
DatasetExample Question GSM8K1319 James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total meters does he run a week?AsDiv 2096 Seven red apples and two green apples are in the basket. How many apples are in the basket? MultiArith 600 The school cafeteria ordered 42 red apples and 7 green apples for students lunches. But, if only 9 students wanted fruit, how many extra did the cafeteria end up with? SVAMP 1000 Paco had 26 salty cookies and 17 sweet cookies. He ate 14 sweet cookies and 9 salty cookies. How many salty cookies did Paco have left? SingleEq 508 Terez has 44 cows on his farm. 50 percent of the cows are female, and 50 percent of the females are pregnant. How many pregnant female cows does Terez have? CommonsenseQA 3387 Sammy wanted to go to where the people were. Where might he go? Options: (a) race track (b) populated areas (c) desert (d) apartment (e) roadblock StrategyQA 2280 Could you go to New York Public Library and the Six Flags Great Escape in the same day?</p>
<p>Table 8 :
8Reasoning benchmarks we use in this paper with examples. N means the number of test cases.</p>
<p>Table 8
8shows the reasoning benchmarks we use in this paper with examples. We use the same test sets asWei et al. (2022)  for GSM8K, AsDiv, MultiArith, SVAMP, SingleEq, and CommonsenseQA.For StrategyQA, there are 2, 290 test cases (i.e., questions paired with TRUE/FALSE labels), but there is no other case that can be leveraged by DIVERSE to construct diverse exemplars (as introduced in Section 2.1). To address this problem, we randomly divide these 2, 290 test cases into two equal parts (denoted as T 1 and T 2 ). For each DI-VERSE experiment of SQA, we conduct two runs: using T 1 to construct diverse exemplars and T 2 as the test set, and vice versa. The final reported solve rate is the average solve rate of these two runs.ForCLUTRR, Sinha et al. (2019)  provided several versions: clean, supporting, irrelevant, and disconnected. The clean version is the basic dataset, while the others are the perturbed variations of it. Our experiments are conducted on the clean version.</p>
<p>Input: Story: Kelly and her mother Ernest made breakfast together. Constance and her husband Ernest wanted a child badly. Query: Kelly, Constance Output: daughterVariant 
Input Example </p>
<p>CLUTRR 
for NLI 
(Original) </p>
<p>CLUTRR 
for NLG 
(Ours) </p>
<p>Our main experiments use M1 = 5 and M2 = 20.
Specifically, f ′ (input i , j) is predicted from the hidden state of the last token of Si,j in DEBERTA-V3-LARGE, similar to token classification tasks.
DIVERSE can also be applied to PaLM, but PaLM is not publicly available.
 Sinha et al. (2019)  also introduced a method with 100% accuracy. We do not take it into the comparison, as this method requires a domain-specific system with complicated rules to extract a knowledge graph for each input text.
 Wang et al. (2022c)  tried an ensemble-based approach, i.e., to permutate exemplars in the original prompt. However, this strategy does not increase diversity in terms of exemplars. 7 This is motivated byZelikman et al. (2022).</p>
<p>Annual Meeting of the Association for Computational Linguistics. Dublin, IrelandAssociation for Computational Linguistics1Long Papers)Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 6016-6031, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, 10.48550/ARXIV.2002.04326Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. Reclor: A reading comprehension dataset requiring logical reasoning.</p>
<p>Eric Zelikman, Yuhuai Wu, Noah D Goodman, arXiv:2203.14465Star: Bootstrapping reasoning with reasoning. arXiv preprintEric Zelikman, Yuhuai Wu, and Noah D Goodman. 2022. Star: Bootstrapping reasoning with rea- soning. arXiv preprint arXiv:2203.14465.</p>
<p>SWAG: A large-scale adversarial dataset for grounded commonsense inference. Rowan Zellers, Yonatan Bisk, Roy Schwartz, Yejin Choi, 10.18653/v1/D18-1009Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A large-scale adversar- ial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empir- ical Methods in Natural Language Processing, pages 93-104, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Calibrate before use: Improving few-shot performance of language models. Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, 10.48550/ARXIV.2102.09690Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models.</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, 10.48550/ARXIV.2205.10625Quoc Le, and Ed Chi. 2022Olivier BousquetDale SchuurmansDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuur- mans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-most prompting enables complex reasoning in large language models.</p>
<p>Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, 10.48550/ARXIV.2105.07624Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. Tat-qa: A ques- tion answering benchmark on a hybrid of tabular and textual content in finance.</p>            </div>
        </div>

    </div>
</body>
</html>
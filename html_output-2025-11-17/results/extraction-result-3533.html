<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3533 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3533</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3533</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-247594506</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2022.acl-long.77.pdf" target="_blank">FaiRR: Faithful and Robust Deductive Reasoning over Natural Language</a></p>
                <p><strong>Paper Abstract:</strong> Transformers have been shown to be able to perform deductive reasoning on a logical rulebase containing rules and statements written in natural language. Recent works show that such models can also produce the reasoning steps (i.e., the proof graph) that emulate the model’s logical reasoning process. Currently, these black-box models generate both the proof graph and intermediate inferences within the same model and thus may be unfaithful. In this work, we frame the deductive logical reasoning task by defining three modular components: rule selection, fact selection, and knowledge composition. The rule and fact selection steps select the candidate rule and facts to be used and then the knowledge composition combines them to generate new inferences. This ensures model faithfulness by assured causal relation from the proof step to the inference reasoning. To test our framework, we propose FaiRR (Faithful and Robust Reasoner) where the above three components are independently modeled by transformers. We observe that FaiRR is robust to novel language perturbations, and is faster at inference than previous works on existing reasoning datasets. Additionally, in contrast to black-box generative models, the errors made by FaiRR are more interpretable due to the modular approach.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3533.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3533.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FAIRR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Faithful and Robust Reasoner (FAIRR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular transformer-based deductive reasoner that enforces a causal 'select-then-compose' pipeline (rule selection, fact selection, knowledge composition) to generate one-hop inferences and stitch proof graphs over natural-language rulebases, aiming for faithfulness and robustness to surface perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FAIRR (RoBERTa selectors + T5 knowledge composer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Modular system: two RoBERTa-based classifiers for rule selection and fact selection, and a T5-based generative model (T5-large) as the knowledge composer; components trained separately on synthetic natural-language rulebases (D* datasets / ProofWriter data). Deterministic solver stitches one-hop proofs into final proof and entailment decision.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Deductive reasoning / proof generation on D* datasets (Proof generation over natural-language theories)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Predict whether a natural-language statement is entailed by a theory (facts+rules) and generate a strict proof graph (exact-match proof metric), including iterative one-hop intermediate inferences (forward chaining). Evaluated on D0–D3, D5, ParaRules and multiple robustness-perturbed variants.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Causal modularization into (1) rule selector (RoBERTa), (2) fact selector (RoBERTa), (3) knowledge composer (T5); strict input restrictions per module (selected rule only for fact selection, selected rule+facts only for inference) and question-augmentation to prioritize relevant inferences; iterative forward chaining stopping via selector.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Robustness (trained on D0-D3, tested on robustness variants, aggregated 'All' depths): entailment accuracy 96.8%, proof accuracy 95.9%, consistency higher than baseline; precision of generated intermediate inferences ≈1.0 across depths, recall falls from ≈1.0 to ≈0.95 at higher depths; inference runtime ≈3.5× faster than ProofWriter (Iter) on D5 dev. On same-depth D0-D3 both FAIRR and ProofWriter (Iter) perform comparably; on D5 generalization FAIRR showed higher proof accuracy than ProofWriter (All) (+7.5% in cited comparison) but slightly lower than ProofWriter (Iter) (−3% cited).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>ProofWriter (Iter) on D0-D3 robustness 'All' depths: entailment accuracy 89.6%, proof accuracy 88.4% (from paper Table 2). ProofWriter (All) reported in some ParaRules comparisons using T5-11B (see paper).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>FAIRR improves over ProofWriter (Iter) on robustness tests (especially subject and subject+attribute perturbations): +~7.2 percentage points entailment and +~7.5 points proof accuracy on aggregate 'All' depths reported; substantially higher consistency and far fewer uninterpretable generation errors; much higher precision of generated inferences and faster inference (≈3.5×). Versus ProofWriter (All) on some generalization tasks FAIRR had higher proof accuracy (+7.5%) but was slightly worse than ProofWriter (Iter) on unseen deeper reasoning (−3%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Main failure mode is 'early stop' from the rule selector (FAIRR accounts for 80% of its errors), causing missing necessary inferences at higher depths; recall drops slightly at deeper proofs (~0.95). FAIRR is also sensitive to attribute perturbations (both FAIRR and baselines suffer). Knowledge composer errors (wrong inference) are less frequent (~20% of FAIRR errors) and are typically small/local mistakes. Training and components require modular supervision; solver is deterministic and not learned.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Analyses include: precision/recall of generated inferences (high precision, slightly reduced recall at depth); inference-budget experiments showing FAIRR outperforms ProofWriter at low budgets (prioritized relevant inference generation); runtime analysis (FAIRR ≈3.5× faster on D5 dev); input-ablation on ProofWriter demonstrating lack of faithful select-then-reason (supports FAIRR's causal argument); component-wise robustness training (training selectors/KC on perturbed data improves component performance), and manual error analysis categorizing errors (early stop, wrong inference, generation hallucination — latter absent in FAIRR).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FaiRR: Faithful and Robust Deductive Reasoning over Natural Language', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3533.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3533.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProofWriter (Iter)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProofWriter (Iterative variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-based iterative forward-chaining model that generates one-hop conclusions and their proofs repeatedly to produce full proof graphs, used as a main baseline for natural-language deductive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ProofWriter: Generating implications, proofs, and abductive statements over natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ProofWriter (iterative)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-based generative model that iteratively generates one-hop inferences and their associated proof steps (forward chaining), enumerating all possible inferences and then stitching proofs to answer entailment questions; used as a baseline with checkpoints provided by original authors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Deductive reasoning / proof generation on D* datasets (same task as FAIRR)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Generate intermediate inferences and proof graphs to justify entailment decisions on natural-language rulebases; evaluated on D* datasets and robustness variants.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>All-internal generative forward chaining: uses the full theory as input during generation (no explicit modular select-then-compose constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported on D0-D3 robustness aggregate ('All' depths): entailment accuracy 89.6%, proof accuracy 88.4% (Table 2). Perfect recall on generated inferences across depths but lower precision (many irrelevant inferences). On generalization to D5, ProofWriter (Iter) had slightly higher performance than FAIRR (paper cites FAIRR −3% vs PW Iter on one metric). Error breakdown: early stop errors ~50% of PW errors, wrong inference ~30%, and generation errors including hallucination/format violations ~20%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>N/A (this is a baseline). Compared to FAIRR it has lower robustness and consistency on subject-perturbed datasets, more uninterpretable generation errors, lower precision of generated inferences, and slower inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Prone to hallucinations and non-interpretable generation errors (format violations, invented facts), low precision due to exhaustive forward search, sensitive to irrelevant extra information (input-ablation experiments show outputs change when irrelevant sentences are added), slower runtime due to exhaustive inference, and can suffer early stops and wrong inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Input-ablation study: progressively removing irrelevant sentences often improves ProofWriter outputs, demonstrating lack of faithful select-then-reason pipeline; analyses show perfect recall but low precision for candidate inferences; error analysis documents hallucinations and format errors; inference-budget experiments show ProofWriter needs large budgets to match FAIRR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FaiRR: Faithful and Robust Deductive Reasoning over Natural Language', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3533.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3533.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProofWriter (All)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProofWriter (All-at-once variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-based model variant that directly predicts entailment and the full proof graph from the entire theory and statement in a single generative step.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ProofWriter: Generating implications, proofs, and abductive statements over natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ProofWriter (All)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Text-to-text T5 model trained to generate entailment answers and full proofs in a single pass from the full theory+statement input; some reported baselines use large-capacity T5-11B checkpoints for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Deductive reasoning / proof generation on D* datasets</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Predict entailment and complete proof graph in one generative step from natural-language rulebases; evaluated on D* and ParaRules datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Single-step generative proof+answer from the entire theory; no explicit modular selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>In comparisons on ParaRules (when PW(All) was run with T5-11B reported by Tafjord et al.), FAIRR (T5-large based) had better performance at higher depths in some settings; on D5 generalization FAIRR had +7.5% proof accuracy advantage over PW(All) (reported in paper). Exact PW(All) numbers vary by checkpoint and reported experiments in original ProofWriter paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>N/A (this is a baseline compared against FAIRR).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Single-step generation can lead to unfaithful internal reasoning (entailment may not depend on generated proof), sensitivity to surface variations, and large compute when using big T5 checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper reports that PW(All) shows inconsistent coupling between proof generation and answer prediction across depths (larger drops in proof accuracy than in entailment), suggesting less causal grounding between proof and answer than FAIRR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FaiRR: Faithful and Robust Deductive Reasoning over Natural Language', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3533.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3533.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RuleTaker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformers as soft reasoners over language (RuleTaker)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early demonstration that transformers can predict entailment over natural-language rulebases (deductive reasoning) by emulating forward-chaining reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformers as soft reasoners over language.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RuleTaker (Transformer-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer models trained to predict entailment of statements given a natural-language theory, demonstrating that pretrained transformers can learn to emulate deductive reasoning on synthetic rulebases.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Natural-language deductive entailment (RuleTaker / D* style synthetic datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary/ternary (True/False/Unknown) entailment prediction over synthetic rulebases expressed in English; focuses on forward-chaining deductions (RuleTaker introduced the D* datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>End-to-end transformer fine-tuning on synthetic rulebase datasets; does not produce explicit proofs in original formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in original work (cited) to achieve strong entailment accuracy on synthetic depths; in this paper RuleTaker is referenced as motivation and predecessor to proof-generation work.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Original RuleTaker predicted entailment but did not produce faithful proof graphs; inspired subsequent proof-generation work to make reasoning steps explicit.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Mentioned as a precursor demonstrating transformer capability on synthetic deductive tasks; no new ablation in this paper beyond relation to FAIRR's goals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FaiRR: Faithful and Robust Deductive Reasoning over Natural Language', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3533.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3533.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRover / multiPRover</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PRover and multiPRover (Proof generation models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RoBERTa-based graph-prediction approaches (PRover) and extensions (multiPRover) that generate nodes and edges of the proof graph or multiple proofs, typically under a closed-world assumption.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PRover: Proof generation for interpretable reasoning over rules.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PRover / multiPRover (RoBERTa-based graph models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models that predict explicit proof-graph nodes and edges (PRover) and multi-proof extensions (multiPRover); trained on datasets that assume closed-world semantics and sometimes multiple gold proofs per instance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Proof generation over rulebases (graph-based proof prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Generate proof graphs (nodes/edges) to justify entailment in (typically) closed-world datasets; multiPRover aims to enumerate multiple possible proofs for an instance.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Graph-based prediction of proof components rather than text-generation; often makes closed-world assumption (CWA) and uses specialized loss/architecture for multiple proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not directly compared in experiments here because of dataset/assumption mismatch (PRover trained on CWA data vs FAIRR using OWA datasets), so no numeric comparisons provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Different dataset assumptions (CWA) make direct comparison to FAIRR/ProofWriter unfair; multiPRover focuses on multiple proofs which differs from FAIRR's single-proof objective.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Mentioned in related work; authors omit direct comparison due to dataset assumption mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FaiRR: Faithful and Robust Deductive Reasoning over Natural Language', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3533.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3533.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Theorem Prover</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-end differentiable proving / Neural Theorem Prover</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural approaches that parse natural language to formal logic and perform differentiable theorem proving over the induced formal structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>End-to-end differentiable proving.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural Theorem Prover / differentiable proving models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural models that (attempt to) parse natural-language statements into formal logical representations and perform neural/symbolic theorem proving (differentiable proving) over those representations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Formal-parsing-based logical entailment / theorem proving</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Parse NL to formal logic and perform symbolic/neural reasoning to derive entailments; contrasts with direct NL proof generation approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Neural parsing into formal logic + differentiable proving; more symbolic pipeline compared to end-to-end text-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Cited as prior work; authors note challenges in robust parsing from NL and thus adopt direct NL proof generation approach (as in ProofWriter/FAIRR) rather than full formal parsing pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Parsing step from NL to formal logic can be brittle and challenging, which motivates bypassing parsing in favor of direct NL-based proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Mentioned in related work discussion about trade-offs between symbolic/formal approaches and direct NL reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FaiRR: Faithful and Robust Deductive Reasoning over Natural Language', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3533.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3533.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLProlog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NLProlog: Reasoning with weak unification for question answering in natural language</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid system that applies weak unification techniques to perform prolog-like reasoning over natural-language statements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NLProlog: Reasoning with weak unification for question answering in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NLProlog</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approach that blends classical unification-style reasoning with representations that handle natural-language variability for question answering; cited as related symbolic/neuro-symbolic effort.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Natural-language question answering with symbolic/unification reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Performs prolog-like reasoning with weak unification over NL, aiming to map NL statements to structures suitable for symbolic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Weak unification plus symbolic reasoning; distinct from purely transformer-based generative proof systems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Mentioned in related work; not numerically compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Symbolic parsing and unification can be limited by NL variability; paper cites this class of approaches as an alternative with parsing challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Cited for context; no ablation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FaiRR: Faithful and Robust Deductive Reasoning over Natural Language', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3533.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3533.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa (selectors)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-based classifiers used for Rule and Fact Selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RoBERTa-based classification models employed within FAIRR to select the relevant rule and relevant facts for each one-hop inference, enforcing modular causality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large (used as classification backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained masked-language-transformer encoder fine-tuned as classifiers for (a) rule selection (multi-way binary classification over rules using SEP token embeddings) and (b) fact selection (binary labels for each fact given a selected rule); paper states RoBERTa-large used but does not list parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Subtask: rule and fact selection for deductive inference</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Classification tasks to pick which rule (one-hot among m rules) and which subset of facts (binary per fact) to use for a single forward-chaining step toward generating intermediate conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Convert selection to classification with constrained inputs (e.g., fact selector sees only selected rule + facts) and binary cross-entropy / cross-entropy losses; training on per-step supervision derived from ProofWriter-style instances.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Component-level improvements reported qualitatively and via downstream gains: higher precision of generated inferences, higher robustness/consistency; training-time hyperparameter ranges and that RoBERTa-large was used are reported. Exact per-component numeric metrics are in appendices (component training improved via robust augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Enables pruning of inference space and prioritization of relevant inferences; reduces exhaustive forward search and improves inference speed and interpretability vs monolithic generative baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Rule selector exhibits early-stop behaviour causing many FAIRR errors (selector chooses stop token [CLS] prematurely); improved modeling of stop criterion needed.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper reports hyperparameter sweeps, ablations where components trained on robustness-augmented data improve performance (Table 11), and error attribution to selectors in manual analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FaiRR: Faithful and Robust Deductive Reasoning over Natural Language', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3533.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3533.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5 (knowledge composer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-based Knowledge Composer (T5-large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>T5-large text-to-text transformer used in FAIRR to generate a new one-hop inference given only the selected rule and selected facts, thus enforcing that generated inference is causally grounded in the selected premises.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large (knowledge composer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Text-to-text generative transformer fine-tuned as a composer: input is concatenated selected rule+facts; output is a single intermediate conclusion; trained with language modeling loss on per-step training instances derived from ProofWriter data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Subtask: forward-chaining one-hop inference generation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Generate a single valid intermediate inference (text) that follows from the provided selected rule and facts; these inferences are later stitched into full proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Restrict input to only selected rule+facts (no access to full theory), thereby making generation step strictly grounded; trained separately from selectors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Knowledge composer makes fewer wrong-inference errors compared to ProofWriter's monolithic generator; contributes to overall high proof accuracy (95.9% on robustness 'All' depths).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>By constraining inputs, reduces hallucinations and uninterpretable generations; produces more interpretable and attributable errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Occasional wrong inferences (~20% of FAIRR errors) and small local generation mistakes; overall less prone to format/hallucination errors than ProofWriter.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Component training and error analysis show KC is relatively reliable; paper reports component-wise augmentation benefits and that KC benefits from smaller, focused inputs resulting in faster inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FaiRR: Faithful and Robust Deductive Reasoning over Natural Language', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language. <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language. <em>(Rating: 2)</em></li>
                <li>PRover: Proof generation for interpretable reasoning over rules. <em>(Rating: 2)</em></li>
                <li>multiPRover: Generating multiple proofs for improved interpretability in rule reasoning. <em>(Rating: 1)</em></li>
                <li>End-to-end differentiable proving. <em>(Rating: 1)</em></li>
                <li>NLProlog: Reasoning with weak unification for question answering in natural language. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3533",
    "paper_id": "paper-247594506",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "FAIRR",
            "name_full": "Faithful and Robust Reasoner (FAIRR)",
            "brief_description": "A modular transformer-based deductive reasoner that enforces a causal 'select-then-compose' pipeline (rule selection, fact selection, knowledge composition) to generate one-hop inferences and stitch proof graphs over natural-language rulebases, aiming for faithfulness and robustness to surface perturbations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FAIRR (RoBERTa selectors + T5 knowledge composer)",
            "model_description": "Modular system: two RoBERTa-based classifiers for rule selection and fact selection, and a T5-based generative model (T5-large) as the knowledge composer; components trained separately on synthetic natural-language rulebases (D* datasets / ProofWriter data). Deterministic solver stitches one-hop proofs into final proof and entailment decision.",
            "model_size": null,
            "reasoning_task_name": "Deductive reasoning / proof generation on D* datasets (Proof generation over natural-language theories)",
            "reasoning_task_description": "Predict whether a natural-language statement is entailed by a theory (facts+rules) and generate a strict proof graph (exact-match proof metric), including iterative one-hop intermediate inferences (forward chaining). Evaluated on D0–D3, D5, ParaRules and multiple robustness-perturbed variants.",
            "method_or_intervention": "Causal modularization into (1) rule selector (RoBERTa), (2) fact selector (RoBERTa), (3) knowledge composer (T5); strict input restrictions per module (selected rule only for fact selection, selected rule+facts only for inference) and question-augmentation to prioritize relevant inferences; iterative forward chaining stopping via selector.",
            "performance": "Robustness (trained on D0-D3, tested on robustness variants, aggregated 'All' depths): entailment accuracy 96.8%, proof accuracy 95.9%, consistency higher than baseline; precision of generated intermediate inferences ≈1.0 across depths, recall falls from ≈1.0 to ≈0.95 at higher depths; inference runtime ≈3.5× faster than ProofWriter (Iter) on D5 dev. On same-depth D0-D3 both FAIRR and ProofWriter (Iter) perform comparably; on D5 generalization FAIRR showed higher proof accuracy than ProofWriter (All) (+7.5% in cited comparison) but slightly lower than ProofWriter (Iter) (−3% cited).",
            "baseline_performance": "ProofWriter (Iter) on D0-D3 robustness 'All' depths: entailment accuracy 89.6%, proof accuracy 88.4% (from paper Table 2). ProofWriter (All) reported in some ParaRules comparisons using T5-11B (see paper).",
            "improvement_over_baseline": "FAIRR improves over ProofWriter (Iter) on robustness tests (especially subject and subject+attribute perturbations): +~7.2 percentage points entailment and +~7.5 points proof accuracy on aggregate 'All' depths reported; substantially higher consistency and far fewer uninterpretable generation errors; much higher precision of generated inferences and faster inference (≈3.5×). Versus ProofWriter (All) on some generalization tasks FAIRR had higher proof accuracy (+7.5%) but was slightly worse than ProofWriter (Iter) on unseen deeper reasoning (−3%).",
            "limitations_or_failures": "Main failure mode is 'early stop' from the rule selector (FAIRR accounts for 80% of its errors), causing missing necessary inferences at higher depths; recall drops slightly at deeper proofs (~0.95). FAIRR is also sensitive to attribute perturbations (both FAIRR and baselines suffer). Knowledge composer errors (wrong inference) are less frequent (~20% of FAIRR errors) and are typically small/local mistakes. Training and components require modular supervision; solver is deterministic and not learned.",
            "ablation_or_analysis": "Analyses include: precision/recall of generated inferences (high precision, slightly reduced recall at depth); inference-budget experiments showing FAIRR outperforms ProofWriter at low budgets (prioritized relevant inference generation); runtime analysis (FAIRR ≈3.5× faster on D5 dev); input-ablation on ProofWriter demonstrating lack of faithful select-then-reason (supports FAIRR's causal argument); component-wise robustness training (training selectors/KC on perturbed data improves component performance), and manual error analysis categorizing errors (early stop, wrong inference, generation hallucination — latter absent in FAIRR).",
            "uuid": "e3533.0",
            "source_info": {
                "paper_title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "ProofWriter (Iter)",
            "name_full": "ProofWriter (Iterative variant)",
            "brief_description": "A T5-based iterative forward-chaining model that generates one-hop conclusions and their proofs repeatedly to produce full proof graphs, used as a main baseline for natural-language deductive reasoning.",
            "citation_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language.",
            "mention_or_use": "use",
            "model_name": "ProofWriter (iterative)",
            "model_description": "T5-based generative model that iteratively generates one-hop inferences and their associated proof steps (forward chaining), enumerating all possible inferences and then stitching proofs to answer entailment questions; used as a baseline with checkpoints provided by original authors.",
            "model_size": null,
            "reasoning_task_name": "Deductive reasoning / proof generation on D* datasets (same task as FAIRR)",
            "reasoning_task_description": "Generate intermediate inferences and proof graphs to justify entailment decisions on natural-language rulebases; evaluated on D* datasets and robustness variants.",
            "method_or_intervention": "All-internal generative forward chaining: uses the full theory as input during generation (no explicit modular select-then-compose constraints).",
            "performance": "Reported on D0-D3 robustness aggregate ('All' depths): entailment accuracy 89.6%, proof accuracy 88.4% (Table 2). Perfect recall on generated inferences across depths but lower precision (many irrelevant inferences). On generalization to D5, ProofWriter (Iter) had slightly higher performance than FAIRR (paper cites FAIRR −3% vs PW Iter on one metric). Error breakdown: early stop errors ~50% of PW errors, wrong inference ~30%, and generation errors including hallucination/format violations ~20%.",
            "baseline_performance": null,
            "improvement_over_baseline": "N/A (this is a baseline). Compared to FAIRR it has lower robustness and consistency on subject-perturbed datasets, more uninterpretable generation errors, lower precision of generated inferences, and slower inference.",
            "limitations_or_failures": "Prone to hallucinations and non-interpretable generation errors (format violations, invented facts), low precision due to exhaustive forward search, sensitive to irrelevant extra information (input-ablation experiments show outputs change when irrelevant sentences are added), slower runtime due to exhaustive inference, and can suffer early stops and wrong inferences.",
            "ablation_or_analysis": "Input-ablation study: progressively removing irrelevant sentences often improves ProofWriter outputs, demonstrating lack of faithful select-then-reason pipeline; analyses show perfect recall but low precision for candidate inferences; error analysis documents hallucinations and format errors; inference-budget experiments show ProofWriter needs large budgets to match FAIRR.",
            "uuid": "e3533.1",
            "source_info": {
                "paper_title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "ProofWriter (All)",
            "name_full": "ProofWriter (All-at-once variant)",
            "brief_description": "A T5-based model variant that directly predicts entailment and the full proof graph from the entire theory and statement in a single generative step.",
            "citation_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language.",
            "mention_or_use": "use",
            "model_name": "ProofWriter (All)",
            "model_description": "Text-to-text T5 model trained to generate entailment answers and full proofs in a single pass from the full theory+statement input; some reported baselines use large-capacity T5-11B checkpoints for comparison.",
            "model_size": "11B",
            "reasoning_task_name": "Deductive reasoning / proof generation on D* datasets",
            "reasoning_task_description": "Predict entailment and complete proof graph in one generative step from natural-language rulebases; evaluated on D* and ParaRules datasets.",
            "method_or_intervention": "Single-step generative proof+answer from the entire theory; no explicit modular selection.",
            "performance": "In comparisons on ParaRules (when PW(All) was run with T5-11B reported by Tafjord et al.), FAIRR (T5-large based) had better performance at higher depths in some settings; on D5 generalization FAIRR had +7.5% proof accuracy advantage over PW(All) (reported in paper). Exact PW(All) numbers vary by checkpoint and reported experiments in original ProofWriter paper.",
            "baseline_performance": null,
            "improvement_over_baseline": "N/A (this is a baseline compared against FAIRR).",
            "limitations_or_failures": "Single-step generation can lead to unfaithful internal reasoning (entailment may not depend on generated proof), sensitivity to surface variations, and large compute when using big T5 checkpoints.",
            "ablation_or_analysis": "Paper reports that PW(All) shows inconsistent coupling between proof generation and answer prediction across depths (larger drops in proof accuracy than in entailment), suggesting less causal grounding between proof and answer than FAIRR.",
            "uuid": "e3533.2",
            "source_info": {
                "paper_title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "RuleTaker",
            "name_full": "Transformers as soft reasoners over language (RuleTaker)",
            "brief_description": "An early demonstration that transformers can predict entailment over natural-language rulebases (deductive reasoning) by emulating forward-chaining reasoning.",
            "citation_title": "Transformers as soft reasoners over language.",
            "mention_or_use": "mention",
            "model_name": "RuleTaker (Transformer-based)",
            "model_description": "Transformer models trained to predict entailment of statements given a natural-language theory, demonstrating that pretrained transformers can learn to emulate deductive reasoning on synthetic rulebases.",
            "model_size": null,
            "reasoning_task_name": "Natural-language deductive entailment (RuleTaker / D* style synthetic datasets)",
            "reasoning_task_description": "Binary/ternary (True/False/Unknown) entailment prediction over synthetic rulebases expressed in English; focuses on forward-chaining deductions (RuleTaker introduced the D* datasets).",
            "method_or_intervention": "End-to-end transformer fine-tuning on synthetic rulebase datasets; does not produce explicit proofs in original formulation.",
            "performance": "Reported in original work (cited) to achieve strong entailment accuracy on synthetic depths; in this paper RuleTaker is referenced as motivation and predecessor to proof-generation work.",
            "baseline_performance": null,
            "improvement_over_baseline": "",
            "limitations_or_failures": "Original RuleTaker predicted entailment but did not produce faithful proof graphs; inspired subsequent proof-generation work to make reasoning steps explicit.",
            "ablation_or_analysis": "Mentioned as a precursor demonstrating transformer capability on synthetic deductive tasks; no new ablation in this paper beyond relation to FAIRR's goals.",
            "uuid": "e3533.3",
            "source_info": {
                "paper_title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "PRover / multiPRover",
            "name_full": "PRover and multiPRover (Proof generation models)",
            "brief_description": "RoBERTa-based graph-prediction approaches (PRover) and extensions (multiPRover) that generate nodes and edges of the proof graph or multiple proofs, typically under a closed-world assumption.",
            "citation_title": "PRover: Proof generation for interpretable reasoning over rules.",
            "mention_or_use": "mention",
            "model_name": "PRover / multiPRover (RoBERTa-based graph models)",
            "model_description": "Models that predict explicit proof-graph nodes and edges (PRover) and multi-proof extensions (multiPRover); trained on datasets that assume closed-world semantics and sometimes multiple gold proofs per instance.",
            "model_size": null,
            "reasoning_task_name": "Proof generation over rulebases (graph-based proof prediction)",
            "reasoning_task_description": "Generate proof graphs (nodes/edges) to justify entailment in (typically) closed-world datasets; multiPRover aims to enumerate multiple possible proofs for an instance.",
            "method_or_intervention": "Graph-based prediction of proof components rather than text-generation; often makes closed-world assumption (CWA) and uses specialized loss/architecture for multiple proofs.",
            "performance": "Not directly compared in experiments here because of dataset/assumption mismatch (PRover trained on CWA data vs FAIRR using OWA datasets), so no numeric comparisons provided in this paper.",
            "baseline_performance": null,
            "improvement_over_baseline": "",
            "limitations_or_failures": "Different dataset assumptions (CWA) make direct comparison to FAIRR/ProofWriter unfair; multiPRover focuses on multiple proofs which differs from FAIRR's single-proof objective.",
            "ablation_or_analysis": "Mentioned in related work; authors omit direct comparison due to dataset assumption mismatch.",
            "uuid": "e3533.4",
            "source_info": {
                "paper_title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Neural Theorem Prover",
            "name_full": "End-to-end differentiable proving / Neural Theorem Prover",
            "brief_description": "Neural approaches that parse natural language to formal logic and perform differentiable theorem proving over the induced formal structures.",
            "citation_title": "End-to-end differentiable proving.",
            "mention_or_use": "mention",
            "model_name": "Neural Theorem Prover / differentiable proving models",
            "model_description": "Neural models that (attempt to) parse natural-language statements into formal logical representations and perform neural/symbolic theorem proving (differentiable proving) over those representations.",
            "model_size": null,
            "reasoning_task_name": "Formal-parsing-based logical entailment / theorem proving",
            "reasoning_task_description": "Parse NL to formal logic and perform symbolic/neural reasoning to derive entailments; contrasts with direct NL proof generation approaches.",
            "method_or_intervention": "Neural parsing into formal logic + differentiable proving; more symbolic pipeline compared to end-to-end text-to-text generation.",
            "performance": "Cited as prior work; authors note challenges in robust parsing from NL and thus adopt direct NL proof generation approach (as in ProofWriter/FAIRR) rather than full formal parsing pipelines.",
            "baseline_performance": null,
            "improvement_over_baseline": "",
            "limitations_or_failures": "Parsing step from NL to formal logic can be brittle and challenging, which motivates bypassing parsing in favor of direct NL-based proof generation.",
            "ablation_or_analysis": "Mentioned in related work discussion about trade-offs between symbolic/formal approaches and direct NL reasoning.",
            "uuid": "e3533.5",
            "source_info": {
                "paper_title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "NLProlog",
            "name_full": "NLProlog: Reasoning with weak unification for question answering in natural language",
            "brief_description": "A hybrid system that applies weak unification techniques to perform prolog-like reasoning over natural-language statements.",
            "citation_title": "NLProlog: Reasoning with weak unification for question answering in natural language.",
            "mention_or_use": "mention",
            "model_name": "NLProlog",
            "model_description": "Approach that blends classical unification-style reasoning with representations that handle natural-language variability for question answering; cited as related symbolic/neuro-symbolic effort.",
            "model_size": null,
            "reasoning_task_name": "Natural-language question answering with symbolic/unification reasoning",
            "reasoning_task_description": "Performs prolog-like reasoning with weak unification over NL, aiming to map NL statements to structures suitable for symbolic inference.",
            "method_or_intervention": "Weak unification plus symbolic reasoning; distinct from purely transformer-based generative proof systems.",
            "performance": "Mentioned in related work; not numerically compared in this paper.",
            "baseline_performance": null,
            "improvement_over_baseline": "",
            "limitations_or_failures": "Symbolic parsing and unification can be limited by NL variability; paper cites this class of approaches as an alternative with parsing challenges.",
            "ablation_or_analysis": "Cited for context; no ablation in this paper.",
            "uuid": "e3533.6",
            "source_info": {
                "paper_title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "RoBERTa (selectors)",
            "name_full": "RoBERTa-based classifiers used for Rule and Fact Selection",
            "brief_description": "RoBERTa-based classification models employed within FAIRR to select the relevant rule and relevant facts for each one-hop inference, enforcing modular causality.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large (used as classification backbone)",
            "model_description": "Pretrained masked-language-transformer encoder fine-tuned as classifiers for (a) rule selection (multi-way binary classification over rules using SEP token embeddings) and (b) fact selection (binary labels for each fact given a selected rule); paper states RoBERTa-large used but does not list parameter count.",
            "model_size": null,
            "reasoning_task_name": "Subtask: rule and fact selection for deductive inference",
            "reasoning_task_description": "Classification tasks to pick which rule (one-hot among m rules) and which subset of facts (binary per fact) to use for a single forward-chaining step toward generating intermediate conclusions.",
            "method_or_intervention": "Convert selection to classification with constrained inputs (e.g., fact selector sees only selected rule + facts) and binary cross-entropy / cross-entropy losses; training on per-step supervision derived from ProofWriter-style instances.",
            "performance": "Component-level improvements reported qualitatively and via downstream gains: higher precision of generated inferences, higher robustness/consistency; training-time hyperparameter ranges and that RoBERTa-large was used are reported. Exact per-component numeric metrics are in appendices (component training improved via robust augmentation).",
            "baseline_performance": null,
            "improvement_over_baseline": "Enables pruning of inference space and prioritization of relevant inferences; reduces exhaustive forward search and improves inference speed and interpretability vs monolithic generative baselines.",
            "limitations_or_failures": "Rule selector exhibits early-stop behaviour causing many FAIRR errors (selector chooses stop token [CLS] prematurely); improved modeling of stop criterion needed.",
            "ablation_or_analysis": "Paper reports hyperparameter sweeps, ablations where components trained on robustness-augmented data improve performance (Table 11), and error attribution to selectors in manual analysis.",
            "uuid": "e3533.7",
            "source_info": {
                "paper_title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "T5 (knowledge composer)",
            "name_full": "T5-based Knowledge Composer (T5-large)",
            "brief_description": "T5-large text-to-text transformer used in FAIRR to generate a new one-hop inference given only the selected rule and selected facts, thus enforcing that generated inference is causally grounded in the selected premises.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "T5-large (knowledge composer)",
            "model_description": "Text-to-text generative transformer fine-tuned as a composer: input is concatenated selected rule+facts; output is a single intermediate conclusion; trained with language modeling loss on per-step training instances derived from ProofWriter data.",
            "model_size": null,
            "reasoning_task_name": "Subtask: forward-chaining one-hop inference generation",
            "reasoning_task_description": "Generate a single valid intermediate inference (text) that follows from the provided selected rule and facts; these inferences are later stitched into full proofs.",
            "method_or_intervention": "Restrict input to only selected rule+facts (no access to full theory), thereby making generation step strictly grounded; trained separately from selectors.",
            "performance": "Knowledge composer makes fewer wrong-inference errors compared to ProofWriter's monolithic generator; contributes to overall high proof accuracy (95.9% on robustness 'All' depths).",
            "baseline_performance": null,
            "improvement_over_baseline": "By constraining inputs, reduces hallucinations and uninterpretable generations; produces more interpretable and attributable errors.",
            "limitations_or_failures": "Occasional wrong inferences (~20% of FAIRR errors) and small local generation mistakes; overall less prone to format/hallucination errors than ProofWriter.",
            "ablation_or_analysis": "Component training and error analysis show KC is relatively reliable; paper reports component-wise augmentation benefits and that KC benefits from smaller, focused inputs resulting in faster inference.",
            "uuid": "e3533.8",
            "source_info": {
                "paper_title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language.",
            "rating": 2
        },
        {
            "paper_title": "Transformers as soft reasoners over language.",
            "rating": 2
        },
        {
            "paper_title": "PRover: Proof generation for interpretable reasoning over rules.",
            "rating": 2
        },
        {
            "paper_title": "multiPRover: Generating multiple proofs for improved interpretability in rule reasoning.",
            "rating": 1
        },
        {
            "paper_title": "End-to-end differentiable proving.",
            "rating": 1
        },
        {
            "paper_title": "NLProlog: Reasoning with weak unification for question answering in natural language.",
            "rating": 1
        }
    ],
    "cost": 0.01833725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FaiRR: Faithful and Robust Deductive Reasoning over Natural Language
Long PapersCopyright Long PapersMay 22-27, 2022</p>
<p>Soumya Sanyal 
University of Southern
California</p>
<p>Harman Singh harmansingh.iitd@gmail.com 
Indian Institute of Technology
Delhi</p>
<p>Xiang Ren xiangren@usc.edu 
University of Southern
California</p>
<p>FaiRR: Faithful and Robust Deductive Reasoning over Natural Language</p>
<p>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
the 60th Annual Meeting of the Association for Computational LinguisticsLong Papers1May 22-27, 2022
Transformers have been shown to be able to perform deductive reasoning on a logical rulebase containing rules and statements written in natural language. Recent works show that such models can also produce the reasoning steps (i.e., the proof graph) that emulate the model's logical reasoning process. Currently, these black-box models generate both the proof graph and intermediate inferences within the same model and thus may be unfaithful. In this work, we frame the deductive logical reasoning task by defining three modular components: rule selection, fact selection, and knowledge composition. The rule and fact selection steps select the candidate rule and facts to be used and then the knowledge composition combines them to generate new inferences. This ensures model faithfulness by assured causal relation from the proof step to the inference reasoning. To test our framework, we propose FAIRR (Faithful and Robust Reasoner) where the above three components are independently modeled by transformers. We observe that FAIRR is robust to novel language perturbations, and is faster at inference than previous works on existing reasoning datasets. Additionally, in contrast to black-box generative models, the errors made by FAIRR are more interpretable due to the modular approach.</p>
<p>Introduction</p>
<p>The field of AI has long pursued the goal of building systems that can automatically reason over some given explicit knowledge to generate conclusions and provide the reasoning steps involved in the process (McCarthy, 1959;Newell and Simon, 1956). Recently, Clark et al. (2020) proposed a modern version of this problem, where the formal representation of knowledge is replaced by natural language statements in English. Further, 1 The source code of FAIRR has been made available at https://github.com/INK-USC/FaiRR. Figure 1: Example of a theory, a statement, and a valid proof graph -An instance contains multiple facts and rules in blue and yellow respectively, followed by a statement in red. The proof graph describes the reasoning steps required to generate the statement.</p>
<p>they proposed a transformer-based model (Vaswani et al., 2017) RuleTaker, that can predict if a candidate statement is entailed by the natural language statements, by emulating deductive reasoning. As shown in Figure 1, in this deductive reasoning task, facts and rules from the rulebase are combined iteratively to generate intermediate inferences which eventually entails the statement. Note that the reasoning process implicitly involves two steps: determining which rules and facts to combine at each iteration, followed by using them to generate an intermediate conclusion.</p>
<p>While RuleTaker focuses on just predicting the statement entailment, some recent works (Saha et al., 2020;Tafjord et al., 2021) have further developed systems that can also generate the reasoning steps (i.e., proof graph generation). However, these systems do not explicitly ensure the causality from the rule/fact selection to generating the intermediate inferences. Since these systems are inherently black-box models, it is unclear if such constraints are implicitly learned by the models without being enforced externally. This, in turn, questions the faithfulness of the model's internal reasoning process (Lipton, 2018). Because the model has access to the full theory at input, it might use additional parts of the theory, than just the predicted proof, to generate the inference.</p>
<p>In this paper, we address these shortcomings by developing a modularized framework to solve the deductive reasoning task. While existing methods generate both proofs and conclusions in a single step, in our framework we break this process into three steps: rule selection, fact selection, and knowledge composition. The rule selection step decides the relevant rule to use for an iterative inference step and fact selection uses this rule to select the relevant facts. Then, the knowledge composition step reasons using only the selected rule and facts to generate the next intermediate inference.</p>
<p>In Figure 2, we show the model schematics for our system and contrast it with previous methods. Notably, we strictly restrict the information accessible at each step of our framework to make the reasoning process more faithful. For example, the fact selection step depends only on the selected rule, instead of all the rules in the rulebase. Additionally, the generated inference depends explicitly on the selected rule and facts, as opposed to all the rules and facts in prior works. This makes the proof graph a by-product of the selection steps as we don't need to generate any separate proofs. Since we constrain the inputs to each step, this also makes each subproblem easier to learn, leading to an overall more robust reasoning model.</p>
<p>To model these three steps, we develop FAIRR, in which each component is a transformer-based model learning to perform the modular tasks. Specifically, we use RoBERTa-based models (Liu et al., 2019) for the two selection tasks and a T5based model (Raffel et al., 2020) for the composition task. Similar to ProofWriter, we use synthetic rulebases to train FAIRR. To test the deductive reasoning capabilities in a more comprehensive way, we experiment with both existing deductive reasoning datasets and multiple newly-generated robustness dataset variants. Overall, we find that FAIRR is more robust to novel language perturbations than baselines. Additionally, our model is up to three times faster at inference due to the constrained input and outputs of different modules. Lastly, we find that the errors made by our model are more interpretable and easier to debug compared to baseline generative models. This further demonstrates the faithfulness of our modularized reasoning framework. </p>
<p>Problem Definition</p>
<p>Notations A theory T consists of a set of facts F = {f 1 , f 2 , . . . , f n } and rules R = {r 1 , r 2 , . . . , r m } expressed in natural language. An example of a theory is depicted in Figure 1. Here, the sentences in the blue and yellow boxes are facts and rules, respectively. Further, a proof graph is a directed graph connecting facts and rules that describe how a specific inference can be obtained from the theory. In Figure 1, the proof graph shows the steps involved in generating the inference "Charlie is white.". To generate the proof graph we may need to infer some intermediate conclusions c i . These inferences are considered as part of the extended facts in the theory. For example, in Fig. 1, "Charlie is kind" is an intermediate inference required to generate the correct proof graph.</p>
<p>Deductive Reasoning The task of deductive reasoning is described as follows: given a theory T , and a statement s, predict if the theory supports the statement (entailment prediction) and if so, generate the proof graph that supports the statement (proof generation). For the example theory and statement in Figure 1, we see that the statement is indeed entailed by the theory and the valid proof graph is shown for the same. The main goal of this task is to evaluate if a model can generate valid rea-soning chains in the form of proof graphs to justify its entailment prediction.</p>
<p>Reasoning Robustness We consider an auxiliary task that evaluates the robustness of the reasoning abilities used by the model. Let P be a perturbation function that modifies a given theory T (statement s) to a theory T ′ (statement s ′ ), such that (T ′ , s ′ ) just has some surface changes in the natural language form but still requires the similar reasoning process as required for (T, s). A function that alters the subjects in the theory to unseen subjects is an example of such perturbation function. We perturb each theory statement pair (T, s) to create an equivalence set defined as the set
E (T,s) = {(T ′ 1 , s ′ 1 ) . . . (T ′ N , s ′ N )}, where each (T ′ k , s ′ k )
is derived by perturbing the original theory, and N is the total such perturbations per theory. Note that it is possible to generate different (T ′ k , s ′ k ) pairs by controlling the stochasticity of P. The main goal of this task is to evaluate the consistency of the model's predictions with minimal variations in the input theory.</p>
<p>Evaluation Protocol We consider three main aspects for evaluating the model performance in our study: (1) Entailment accuracy measures how accurately the model is able to predict the true statement entailment. (2) Proof accuracy measures how accurately the model can predict a valid proof for the statement. Following Saha et al. (2020);Tafjord et al. (2021), we use the strict metric for proof evaluation, i.e., for a match to count, both the predicted proof should exactly match a gold proof and the entailment should be correctly predicted.</p>
<p>(3) Consistency measures if the models are consistent in the entailment and proof prediction for different perturbation functions. For a theory statement pair (T, s) and its corresponding equivalence set E (T,s) , consistency is defined
as C = 1 N ∑ N k=1 1[f (T, s) = f (T k , s k )], where f (⋅)
is the model's prediction. We compute the average consistency for both entailment and proof predictions on an equivalence set and further average across the dataset to report the consistency.</p>
<p>The FAIRR Method</p>
<p>Approach Overview</p>
<p>As illustrated by the example in Figure 1, to reliably generate a proof graph through deductive reasoning, a model needs to generate multiple one-hop intermediate conclusions. This is the major limitation of models that use the theory to directly predict the proof (Figure 2 (a)), thus questioning the trustworthiness of the reasoning process. Next, it is also intuitive to see that in order to faithfully generate these intermediate inferences, a model should first determine the proof (i.e., know the rules and facts to use) and then use them to infer the conclusion. That is, there is a causal relation from determining the proof to then generating the conclusion. We note that ProofWriter ("Iter") lacks in this aspect. As shown in Figure 2 (b), it first generates the conclusion and then the corresponding proof.</p>
<p>Motivated by these points, we propose our causal reasoning framework which breaks the reasoning process into three desirable steps. As shown in Figure 2 (c), in our framework, first a rule r is selected using the rules and facts in the theory. Following that, some relevant facts are selected from the fact list based on the selected rule r. This step does not use the other rules R{r} in the theory. Finally, the selected rule and facts are jointly used to generate a new conclusion c i . In this framework, the one-step proof is explicitly determined first via the selection steps followed by the inference generation, making the proof a by-product of the whole process. In contrast, prior works learned to generate the proof along with intermediate conclusion.</p>
<p>FAIRR Modules</p>
<p>At a high level, FAIRR is an iterative model in which the one-hop intermediate conclusions are generated step-by-step. To model our framework described in Sec. 3.1, we have four components in FAIRR as follows.</p>
<p>Rule Selector (RS) The rule selector is a RoBERTa-based (Liu et al., 2019) classification model that takes the concatenated statement, facts, and rules as input, and selects a rule that is used to generate an intermediate conclusion in the current iterative step. It takes the input of the form SEP ] , and generates a one-hot output vector by classifying the token embedding from the [CLS] token and [SEP] tokens in front of the rules, via a linear classifier layer. Each classification is a binary classification, but overall only one of the tokens has the positive class. Here s denotes the statement, F is the facts and concatenated with any intermediate conclusions generated in a prior iteration, and {r i } denotes the i th rule in the theory that contains a total of m rules. [ ] m denotes continued concatenation. An example input and output of the rule selector is shown in Figure 3. , where s is the statement, r is the selected rule, and {f i } is the i th fact in the theory containing n total facts. Note that facts also include any previously generated intermediate conclusions.
[CLS] s [SEP ] F [[SEP ] r i ] m [
[ ] n denotes continued concatenation. The output is generated by classifying each [SEP] token embedding in front of a fact using a linear layer, to determine if the corresponding fact is selected or not. An example input and output for the fact selector is depicted in Figure 3. We note that it is possible to have some rules that can reason over multiple facts jointly to generate a conclusion. An example of such a rule is "rule2" in Figure 1. Hence, this component has the ability to select multiple facts.</p>
<p>Knowledge Composer (KC) The knowledge composer is a generative text-to-text transformer T5 (Raffel et al., 2020) (T5-large) that can compose a set of facts and a rule to output a novel conclusion. The input to the model is the selected facts and rule concatenated together, and the output is the intermediate conclusion. An example input and output for knowledge composer is shown in Fig. 3. predicts that the statement is entailed by the theory. It also search for the negation of the statement 2 , and if found, it predicts not entailed. If none of these are present, it predicts "Unknown" since it cannot prove or disprove the statement. The proof graph is constructed by using the one-hop proofs generated by the selected rule and facts at each step. For example, in Figure 1, the red dotted boxes (onehop proofs) are stitched together to assemble the complete proof. For cases where the entailment is "Unknown", the proof returned is "None", since no proof for the statement exists in the theory. We note that our solver is not a learnable module.</p>
<p>Solver</p>
<p>Training and Inference</p>
<p>Each component of our model (except the solver, which is deterministic) is trained separately. We use the same dataset as ProofWriter to train these models, but process it such that each model receives only the relevant inputs according to our causal framework. More concretely, suppose for a given theory T = R + F , a possible intermediate inference is c obtained by using a rule r and a fact f . Then, a training instance of ProofWriter, which is a T5 (Raffel et al., 2020) model, uses the input {R, F } and output {c, r, f }. We process the same instance to generate three training instances, one for each of rule selector, fact selector, and knowledge composer, respectively, as follows:
RS Input = {R, F }; RS Output = {r}, F S Input = {r, F }; F S Output = {f }, KC Input = {r, f }; KC Output = {c}.
Our selector models have the statement s as input to the model. Also, the outputs of rule selector and fact selectors are converted to class labels instead of text since our selectors are classification models. We use cross entropy loss to train the rule selector, and binary cross entropy loss to train the fact selector. The knowledge composer is trained on language modeling loss.</p>
<p>At inference time, the rule selector selects a rule to be used for generating one-step conclusions. Then, the fact selector selects some facts based on the selected rule, which is then collectively passed on to the knowledge composer to generate a conclusion. This three-step pipeline is run iteratively until the rule selector predicts a stop signal by selecting the [CLS] token which exits the iteration. Once the iteration finishes, the solver uses the generated intermediate inferences to decide if the statement is entailed or not, and generates a proof accordingly.</p>
<p>Remark on Computational Complexity A practical limitation of ProofWriter is that it performs an exhaustive forward search by enumerating all possible inferences from a given theory. This leads to redundant inferences being generated for proving a particular statement. Additionally, using a text-totext transformer model adds to the problem since it is usually quite expensive to run at inference time. In FAIRR, we alleviate this by introducing two changes. First, our causal framework allows only selected rule and facts as input to the knowledge composer, thus restricting the input length significantly. Second, augmenting the question to our selector inputs helps reduce the candidate space because these models can learn to prioritize the selection based on the relevance to both the question and the theory. This ensures that FAIRR does not perform an exhaustive forward search and prioritizes generating relevant inferences over the others. Both these changes lead to an overall improvement in inference speed. We perform more quantitative analysis on this later in Section 5.3.</p>
<p>Experimental Setup</p>
<p>Datasets Following (Tafjord et al., 2021;Clark et al., 2020), we use the D* datasets for our experiments. These are a set of multiple datasets -namely D0, D1, D2, D3, D0-D3, and D5. The theory in these datasets are synthetically generated with increasing reasoning depths. For example, D3 dataset contains statements that require at most 3-hop reasoning steps. The D0-D3 contains all theories in D3 plus ∼ 20% of the D0-D2 training set theories. We also use the ParaRules dataset (Clark et al., 2020) that contains around 2k theories expressed in paraphrased natural language.</p>
<p>Additionally, we generate three datasets that evaluate the robustness of the reasoning models as follows:</p>
<p>• Subject robustness: Here, subjects in a theory are perturbed by using some out-ofdistribution proper and common names. For example, in Figure 1, "Charlie" can be replaced with "Paul" which is not used in the D* datasets. We generate five new theories corresponding to each theory of the D3 dataset, by repeatedly perturbing all the proper and common names in the theory. • Attribute robustness: Here we sample outof-distribution attributes. For example, "blue" in Figure 1 can be replaced with "soft". As above, we generate five new theories for each theory of the D3 dataset. • Subject+Attribute robustness: This is a combination of subject and attribute robustness to study model performance when most of the training vocabulary is replaced by outof-distribution words. Each theory has both novel subject and attribute.</p>
<p>We include more details on the perturbation sets used in our experiments in Appendix B.</p>
<p>Baselines We compare FAIRR with two variants of ProofWriter (Tafjord et al., 2021): All-at-once (PW ("All")) and Iterative (PW ("Iter")), wherever applicable 3 . The PW ("All") model is trained to predict the entailment and generate proof graph directly from the theory and statement in a single step. The PW ("Iter") generates one-step inferences and corresponding proofs iteratively, until all possible inferences are generated, and then stitches the proof </p>
<p>Experiment Results</p>
<p>We compare FAIRR with ProofWriter variants on three settings: generalization on D* datasets, robustness to perturbed theories, and efficiency in inference computation. We further conduct qualitative analysis to understand the inference errors.</p>
<p>Performance on Same Depth Reasoning</p>
<p>In this setting, we train and test both models on D0-D3 dataset. Note, D0-D3 contains statements with reasoning depths up to 3. This compares the ability of the models to generalize to seen reasoning depths at train time. The results with increasing depths of reasoning are shown in Table 1. Here, depth "N/A" refers to statements that cannot be proven and hence don't have an exact proof depth associated with it. We observe that overall both FAIRR and ProofWriter ("Iter") performs comparably (last row with depth 'All'). Further, we find that our model's performance is lower on d = 3, indicating that our models tend to perform weaker with increasing depths. This happens majorly because the rule selector in FAIRR tends to incorrectly select the [CLS] token to indicate a stop signal instead of generating more possible intermediate inferences. We discuss more about this in Sections 5.3 and 5.4. Please refer to Appendix C for more results on unseen reasoning depths.   </p>
<p>Robustness to Perturbed Theories</p>
<p>In this section, we test the robustness of ProofWriter ("Iter") and FAIRR on different perturbed theories. Since FAIRR focuses on making deductive reasoning more robust and faithful, performance on these robustness experiments are the main results of our work. As described in Section 4, we test the robustness on three different perturbations: subject, attribute, and subject+attribute. We compare the performance of both models after training on D0-D3 dataset. The consolidated results are shown in Table 2 and depth-wise results for subject robustness are shown in Table 3. We report the entailment accuracy, proof accuracy, and consistency as defined in Section 2. Please refer to appendix D for the depth-wise breakdown of all the datasets. We observe that on subject and subject+attribute robustness, our models are consistently better than ProofWriter whereas on attribute robustness both models perform similarly. Further, we find that on average, FAIRR is both more accurate and consistent than the baseline. From this, we conclude that our model relies less on spurious correlations based on the subject while both models likely suffer from similar issues on attribute perturbations. Since ProofWriter uses the theory to generate the intermediate conclusion and proofs, it has the capacity to exploit some spurious patterns that can inflate performance. In contrast, our causal framework restricts this capacity by constraining the inputs to each component as described in Section 3.1. Hence, these robustness evaluations demonstrate one of the prime benefits of our causal and modular approach.</p>
<p>Study on Inference Efficiency</p>
<p>Here we perform several analyses to evaluate the computational benefits of our method as described in Section 3.3. Inference efficiency is an important aspect of this problem for real-world scenarios where compute can be limited. In Figure 4, we plot the precision and recall for both FAIRR and ProofWriter ("Iter") with increasing reasoning depths. We find that our model has close to 1.0 precision at all depths, whereas ProofWriter has low precision. This demonstrates that our model is able to successfully prune the candidate inference space to generate relevant candidate inferences almost perfectly. In contrast, we see that with increasing depths, our model's recall reduces from close to 1.0 to ≈ 0.95 whereas ProofWriter has a perfect recall at all depths. While the drop is not very drastic, it indicates that our model fails to generate some essential inferences at higher depths. This is mainly because our rule selector decides to stop early and not generate further relevant inferences for some provable statements.</p>
<p>Relevance of generated inferences</p>
<p>Overall, we conclude that FAIRR always generates inferences that are relevant to solving the instance, although at higher depths it can miss some relevant conclusions.</p>
<p>Performance under inference budget constraints</p>
<p>We analyze the performance of FAIRR and ProofWriter under a fixed inference budget constraint by restricting the total number of conclusions that can be generated. We perform this analysis for different reasoning depths and depict the results in Figure 5. We observe that FAIRR con- sistently outperforms ProofWriter on lower budgets. This shows that FAIRR performs a prioritized generation of conclusions that are relevant to the statement, which can be useful in scenarios with limited inference budgets. See Appendix G for more comparisons.</p>
<p>Inference runtime analysis We next compare the time taken by both the models to solve the complete D5 dev set. Although FAIRR has three separate modules that run sequentially, it is 3.5 times faster than ProofWriter ("Iter") at inference time on average. We attribute this to the reduced inference candidate search space due to question augmentation, and smaller input size to the T5 component (refer to Section 3.3 for details). Please refer to Appendix H for more details.</p>
<p>Error Analysis</p>
<p>We further analyze the different errors made by FAIRR and ProofWriter ("Iter") on 50 randomly sampled errors for each model, from the D0-D3 and the subject robustness dev splits. We manually inspect the proof inferences and compare it with the gold proof to classify the failures. The errors are broadly categorized as follows:</p>
<p>Early stop errors: This is the most frequent error type for both models, accounting for 80% and 50% errors in FAIRR and ProofWriter, respectively. This occurs when a model incorrectly gen-  erates the stop signal and fails to generate all the required inference to prove a statement. We find that our model makes the majority of the mistakes due to early stopping. This can be possibly fixed by improving the rule selector architecture to better model the stop criteria.</p>
<p>Wrong inference: This is the second error type, where the inferred conclusion is incorrect based on the predicted proof. This accounts for 20% and 30% errors in FAIRR and ProofWriter, respectively. We observe that our knowledge composer is makes lesser errors on average compared to the ProofWriter generative model.</p>
<p>Other generation errors: ProofWriter makes around 20% errors where the model generated output does not make sense. For example, it can hallucinate facts that are not present in the theory. Such errors are not interpretable and questions the model's inner-working. FAIRR shows no such error, since the proofs are always interpretable in our model due to the causal framework.</p>
<p>Overall, we find that the errors made by FAIRR are more interpretable than ProofWriter, since we can pin-point which module is at fault. Whereas, in ProofWriter, it is sometimes hard to understand the source of errors. This feature also makes our framework easier to debug to potentially fix some components with techniques like data augmentation. Please refer to Appendix I for more discussion and examples of errors.</p>
<p>ProofWriter Input Ablation</p>
<p>A key goal of FAIRR is to explicitly ensure causality from the rule/facts selection step (proof generation) to the reasoning step (intermediate inference generation). This is essential for a reasoning method using forward chaining to solve a deduc-tive reasoning task 4 . To understand if ProofWriter, which uses forward chaining, implicitly does this "select-then-reason" within the model, we perform the following case study: We sample theories from our subject perturbation dataset where ProofWriter made errors, and manually evaluate the model on inputs with all irrelevant rules/facts deleted. Next we sequentially start adding back the deleted rules/facts to see if the output still remains valid. As shown in Table 4, we see that ProofWriter generates a correct inference for the first row which uses just the essential part of the theory required to generate the conclusion, and starts making errors as more sentences are included. Some more examples are shown in Table 16 in Appendix. This shows that internally ProofWriter is unable to faithfully perform the "select-then-reason" steps for larger theories. In contrast, FAIRR explicitly separates these steps, leading to a faithful reasoning model. Formal Reasoning There are some prior works that try to solve the problem of entailment prediction by first parsing the formal language from text. Neural Theorem Prover (Rocktäschel and Riedel, 2017;Weber et al., 2019) uses neural networks to parse the formal logic from natural language and then reason over them. While this approach is more symbolic, it can lead to many challenges while parsing (Kamath and Das, 2019). The proof generation setting considered here bypasses this step and directly reasons over the given natural language text making it more useful in downstream applications. identifying the important phrases in the input text that helped the model in solving a task. In contrast, the task of proof generation focuses on generating a deductive chain of reasoning from the given theory to the concluded statement. Thus, proof chains are easier to understand for end users, making it more useful to debug any systematic model errors.</p>
<p>Model Interpretability</p>
<p>Causal Reasoning The study of causality and causal reasoning models (Pearl, 2000(Pearl, , 2004 Schölkopf, 2019) has been prevalent in machine learning. It has been applied in various domains such as algorithmic fairness (Loftus et al., 2018), gender bias mitigation (Vig et al., 2020), robustness from spurious correlations (Bühlmann, 2020;Veitch et al., 2021), counterfactual explanations (Feder et al., 2021b), etc. Causality in NLP is particularly important to learn models that go beyond exploiting correlations and to improve their overall faithfulness (Feder et al., 2021a).</p>
<p>Conclusion</p>
<p>In this paper, we proposed FAIRR, a faithful and robust deductive reasoning model based on three modular components: rule selection, fact selection, and knowledge composition. FAIRR ensures causality from proof generation to entailment prediction by design. We established the effectiveness of our approach through experiments on testing robustness to language variations and demonstrating the interpretability of the errors made by our model. We also show that FAIRR is faster and more precise at deductive reasoning than prior baselines.  </p>
<p>A Depth Dataset Details</p>
<p>For training and evaluation of FAIRR and ProofWriter, we use the D* datasets and the ParaRules dataset (Clark et al., 2020). The statistics of these datasets are shown in Table 5 which includes the number of theories, the total number of questions across all theories, and the number of conclusions per theory. The statistics are broken down split wise. We use the same splits of train/dev/test as provided in the original datasets (Clark et al., 2020;Tafjord et al., 2021). All the dataset sources are properly cited and used according to the release license.</p>
<p>B Robustness Dataset Details</p>
<p>The robustness dataset is created by replacing all subjects (attributes, subject+attributes) in the D3 dataset with unseen subjects (attributes, sub-ject+attributes) to create the subject (attribute, sub-ject+attributes) robustness set. For this, we first curate new sets of subjects and attributes to be used as a global pool to sample from while replacing existing subjects and attributes from the theory. These    ther', 'baby', 'child', 'toddler', 'teenager', 'grandmother', 'student', 'teacher', 'alligator', 'cricket', 'bird', 'wolf', 'giraffe', 'dinosaur', 'thief', 'soldier', 'officer', 'artist', 'shopkeeper', 'caretaker', 'janitor', 'minister', 'salesman', 'saleswoman', 'runner', 'racer', 'painter', 'dresser', 'shoplifter'} Attribute pool: {'maroon', 'brown', 'black', 'orange', 'cordial', 'friendly', 'adorable', 'old', 'soft', 'violent', 'intelligent', 'square', 'warm', 'large', 'cylindrical', 'spherical', 'tiny', 'microscopic', 'brilliant', 'noisy', 'playful', 'tender', 'gracious', 'patient', 'funny', 'hilarious', 'thorny', 'sensitive', 'diplomatic', 'thoughtful'} Then, for each theory in the D3 dataset, we replace all the subjects in the theory with randomly sampled subjects (without replacement) from the candidate set to create a perturbed theory. We perform this replacement operation to generate five different perturbed theories. These perturbed theories are called equivalence set. Note that the only change in each theory in an equivalence set is the subjects being replaced by some randomly sampled subjects. For example, "cat" in the original theory might be replaced by "child" in one perturbation, and with "teacher" in yet another perturbation. We follow the same procedure to create attribute and d PW ("All") PW ("Iter") FAIRR PW ("All") PW ("Iter") FAIRR  Table 7: D5 dataset depth-wise performance comparison of FAIRR trained on D0-D3 with ProofWriter ("All") and ProofWriter ("Iter") trained on D3 and D0-D3 respectively. Baseline results are copied from Tafjord et al. (2021). Refer to Section 5.1 and Appendix C for more details.</p>
<p>subject+attribute robustness sets. The statistics for these robustness datasets are shown in Table 6 which includes the dataset name depicting the perturbation type (subject, attribute or subject+attribute), number of theories, the total number of questions across all theories, and the number of conclusions per theory. Please note that one theory has multiple questions in general, and it is possible to have conclusions that are not a part of these questions, but can be deduced from the given theory. Each split of the original dataset is perturbed separately as described above, to create the new datasets.</p>
<p>C Generalization to Reasoning Depths</p>
<p>In this section, we experiment with a setting where models are trained on depths less than or equal to 3 (i.e., d ≤ 3) and tested on D5 dataset that contains statements that require reasoning up to depth 5 (i.e., d ≤ 5). Here, we test the generalization of the models to reasoning depths that are unseen at training time. These results are shown in Table 7. From this table, we observe that overall our model performs significantly better than ProofWriter ("All") on proof accuracy (+7.5%), but has a lower performance compared to ProofWriter ("Iter") (−3%). This shows that compared to ProofWriter ("Iter"), our models are weaker at generalizing to unseen reasoning depths. This happens majorly because our rule selector tends to stop the inference iterations earlier, which means some essential inferences are not generated by the model. Thus, this leads to lower performance with increasing reasoning depths.</p>
<p>But, we make another interesting observation here. The drops in entailment and proof accuracy with increasing depths are similar for FAIRR. For  Table 9: Comparison of FAIRR with ProofWriter ("Iter") trained on D0-D3 and tested on attribute robustness dataset. Baseline results are generated using the checkpoint provided by the authors. For more details, please refer to Appendix D.</p>
<p>instance, considering the performance drops between d = 4 to d = 5, FAIRR has ∼ 9.5% drop in both entailment and proof accuracy. In contrast, ProofWriter ("All") and ProofWriter ("Iter") drops approximately 22% and 11%, respectively in proof accuracy for a mere 1% drop in entailment accuracy. This raises some concern on the causality of the proof generation process used for entailment prediction in these models, since it seems like the answer prediction and proof generation are not dependent via the same reasoning paths. In contrast, our causal framework grounds the entailment prediction to the proofs and this leads to more consistent performance variations in FAIRR.</p>
<p>D Robustness to Perturbed Theories</p>
<p>Here, we show the detailed depth-wise performance of FAIRR and ProofWriter ("Iter") trained on D0-D3 dataset and evaluated on different robustness datasets as described in Section 4. The results for subject, attribute, and subject+attribute  Table 10: Comparison of FAIRR with ProofWriter ("Iter") trained on D0-D3 and tested on subject+attribute robustness dataset. Baseline results are generated using the checkpoint provided by the authors. For more details, please refer to Appendix D.</p>
<p>ParaRules training data % Proof Accuracy robustness evaluations are shown in Tables 8, 9, and 10, respectively. We observe that ProofWriter ("Iter") performs significantly worse compared to FAIRR on subject robustness. The results on sub-ject+attribute robustness are mostly comparable, while in attribute robustness our model performs worse. The drop in performance show that both the models are sensitive to attributes in the theory to varying degree. But the strong sensitivity of ProofWriter ("Iter") to the subject perturbations is questionable, since the causality of the model's reasoning process seems to be compromised because the model learns some spurious correlations using the subjects.</p>
<p>In another setting, we train different components of our model on the robustness data and check if that leads to some performance gains. These results are reported in Table 11. We find that it is indeed possible to improve the performance of individual components of our model by robust data augmentation. This also indicates that our individual components are flexible to intervention by data augmentation. Such abilities are lacking in ProofWriter.   </p>
<p>E Generalization to paraphrased theories</p>
<p>Here we test the ability of our model to generalize to unseen language in ParaRules by using limited training supervision. To test this, we first train our model on D0-D3 dataset and test it on the ParaRules dataset. This is a zero-shot evaluation on an unseen language form. In Figure 6 we observe that the performance is significantly worse on this setting as expected. We also evaluated a checkpoint of ProofWriter ("Iter") trained on D0-D3 which achieves a similar performance of 62.13% entailment accuracy 5 . Next, we gradually start adding portions of ParaRules, along with the D0-D3 data, to the training dataset. We find that FAIRR can quickly achieve reasonable performance using even 10% additional data. This shows that our modularized approach is also efficient in adapting to unseen theories with limited data supervision. For more comparisons with models trained on ParaRules, please refer to Appendix F.</p>
<p>F Results on ParaRules training</p>
<p>Following (Tafjord et al., 2021), we compare the performance of ProofWriter ("All") and FAIRR on the ParaRules dataset, when trained on a combined partition of D3 and ParaRules train set. The ParaRules dataset contains complex linguistic expressions in the theories that are more realistic than D* dataset theories, making it a more challenging dataset. These results are shown in Table 12, with a reasoning depth breakdown as before. We note that numbers for ProofWriter ("Iter") are not reported in the paper, and no trained checkpoint is available either, so we omit it from our comparisons. Also, the reported results for ProofWriter ("All") are from evaluating a T5-11B model while ours is a T5-large model. Here, we see that our model performs better at higher depths compared to the baseline which demonstrates that FAIRR is better at handling paraphrases.</p>
<p>G Inference Budget Analysis</p>
<p>In the inference budget analysis, we compare the performance of FAIRR and ProofWriter under an inference budget constraint, i.e., we restrict the total number of intermediate conclusions that can be produced by both models. We perform this analysis on three different depth datasets (d = {1, 3, 5}) and upper bound the number of inferences by B = {1, 3, 5, 7, 10}. We ensure that the budget is at least equal to the depth of the statements under consideration since proving a statement requires a model to generate inferences equal to at least the depth. From Figure 7 we observe that for all depths FAIRR consistently outperforms ProofWriter on lower budgets. Only when the budget increases to 10, ProofWriter compares with or sometimes outperforms our model. This analysis demonstrates that FAIRR performs a prioritized generation of conclusions that are relevant to the statement, which can be useful in scenarios with limited inference budgets.</p>
<p>H Runtime Analysis</p>
<p>For inference runtime analysis, we time the evaluation of both FAIRR and ProofWriter ("Iter") on D5 dev set. Note that D5 dataset contains statements that require at most five reasoning steps to generate an answer. The runtime for both methods are shown in Table 13. These results were obtained by running the inference algorithm on NVIDIA GeForce RTX 2080 Ti GPUs for both models. We observe that ProofWriter ("Iter") has an almost constant runtime since it always generates all possible inferences for a theory. In contrast, our runtime increases almost linearly with increasing depth. On average, FAIRR is 3.5 times faster at inference than ProofWriter ("Iter").</p>
<p>I Error Analysis</p>
<p>This is a follow-up of Section 5. stead of generating any further conclusions. For our model, this can happen if the rule selector is under confident while selecting rules and it learns that a safer fallback is to stop generating rules. This aspect can probably be improved by a better modeling of the rule selector. We plan to explore this in future works.</p>
<p>Next we look at some of the wrong inferences generated by both models in Tables 14 and 15. We observe that errors made by FAIRR are rather naive with small mistakes in the final conclusion (shown in red in Table 15). In contrast, ProofWriter tends to generate an invalid conclusion with no relation to the generated proof (rows 1 and 2 in Table 14). It also makes many non-interpretable generation errors where the model's output format is completely violated or the model seems to hallucinate some facts (rows 3-6 in Table 14). Thus, we observe the benefit of our causal framework as the errors are interpretable and more believable. In contrast, errors made by ProofWriter clearly show that its inference reasoning process can often not rely on the proof at or, or even the generated proof sometimes doesn't make sense.</p>
<p>J Comparison with Baselines</p>
<p>In this work we compare FAIRR with baselines introduced by (Tafjord et al., 2021). We omit comparisons with both PRover (Saha et al., 2020) and multiPRover (Saha et al., 2021), since they were trained on a different dataset that makes a closedworld assumption (CWA), whereas we use datasets that make an open-world assumption (OWA). One essential difference between these two datasets are that OWA allows for predicting the truth values as one of {True,False,Unknown} while in CWA, any fact that cannot be deduced from the theory is assumed to be false. As a result, in CWA, there are only two possible truth values {True,False} for a given statement. This CWA assumption also leads to a specific constraint in the generated proof graphs, where special N AF nodes need to be considered. Please refer to Saha et al. (2020) for more details on this. Additionally, multiPRover (Saha et al., 2021) has a goal that is different from ours. Specifically, their focus is on generating multiple possible proofs for a given rulebase, and for this their training examples contain multiple gold proofs per instance. In FAIRR and ProofWriter (Tafjord et al., 2021), only one gold proof needs to be generated, making the comparisons a bit unfair. Following Tafjord et al. (2021), we report single run numbers for every experiment. $answer$ = Dave is not a cat. ; $proof$ = # sent2 sent19</p>
<p>Generation erroroutput format violated $answer$ ; $proof$ ; $question$ = What is one single-hop inference? ; $context$ = sent1: If someone likes the grandmother then the grandmother eats the artist. sent2: If someone eats the artist then the artist is young. sent3: If the artist eats the grandmother then the artist is cold. sent4: If someone eats the artist and the artist eats the grandmother then the grandmother eats the artist. sent5: If someone is red and big then they like the grandmother. sent6: If someone visits the grandmother then they are red. sent7: If someone is rough then they eat the grandmother. sent8: If someone likes the artist then they visit the grandmother. sent9: The artist likes the grandmother. sent10: The grandmother eats the artist. sent11: The grandmother is big. sent12: The grandmother is cold. sent13: The grandmother is young. sent14: The grandmother likes the artist. sent15: The grandmother visits the artist. sent16: The artist is young.</p>
<p>$answer$ = The red sent6 sent15</p>
<p>Generation errorhallucinating facts $answer$ ; $proof$ ; $question$ = What is one single-hop inference? ; $context$ = sent1: All young things are smart. sent2: All smart, young things are nice. sent3: If something is young and nice then it is quiet. sent4: Becky is big. sent5: Becky is green. sent6: Becky is young. sent7: Larry is big. sent8: Larry is quiet. sent9: Larry is red. sent10: Larry is young. sent11: Bob is smart.</p>
<p>$answer$ = Bob is nice. ; $proof$ = # sent2 &amp; sent11 sent16</p>
<p>Generation errorrepeated facts $answer$ ; $proof$ ; $question$ = What is one single-hop inference? ; $context$ = sent1: If Adam is green then Adam is big. sent2: Green things are rough. sent3: If Joseph is cold then Joseph is big. sent4: All blue things are cold. sent5: All green things are rough. sent6: All big things are kind. sent7: If something is rough and big then it is green. sent8: All kind, blue things are cold. sent9: Joseph is blue. sent10: Adam is blue. sent11: Molly is cold. sent12: Joseph is cold. sent13: Charlie is cold.</p>
<p>$answer$ = Charlie is sent13 sent13</p>
<p>Generation erroroutput format violated $answer$ ; $proof$ ; $question$ = What is one single-hop inference? ; $context$ = sent1: Nice things are green. sent2: If Ivan is smart then Ivan is blue. sent3: Green things are smart. sent4: All cold things are nice. sent5: Big things are blue. sent6: If Ronald is nice then Ronald is big. sent7: If something is green and big then it is not kind. sent8: If something is smart and nice then it is kind. sent9: Ronald is smart. sent10: Larry is cold. sent11: Laura is nice. sent12: Ivan is kind. sent13: Larry is nice. sent14: Larry is green. sent15: Bob is smart.</p>
<p>$answer$ = Bob is kind. ; $proof$ = # sent8 &amp; sent9 sentnice  Output ✓:correct ✗:incorrect</p>
<p>Example 1 $answer$ ; $proof$ ; $question$ = What is one single-hop inference? ; $context$ = sent1: If someone is blue then they are quiet. sent2: Chris is blue.</p>
<p>$answer$ = Chris is quiet. ; $proof$ = # sent1 sent2 ✓ $answer$ ; $proof$ ; $question$ = What is one single-hop inference? ; $context$ = sent1: If someone is blue then they are quiet. sent2: Chris is blue. sent3: Steve is blue.</p>
<p>$answer$ = Dave is quiet. ; $proof$ = # sent1 sent2 ✗ $answer$ ; $proof$ ; $question$ = What is one single-hop inference? ; $context$ = sent1: If someone is blue then they are quiet. sent2: Quiet people are cold. sent3: Chris is blue. sent4: Steve is blue. sent5: Chris is white.</p>
<p>$answer$ = Dave is quiet. ; $proof$ = # sent1 sent4</p>
<p>✗</p>
<p>Example 2 $answer$ ; $proof$ ; $question$ = What is one single-hop inference? ; $context$ = sent1: If someone likes the artist then they visit the grandmother. sent2: The grandmother likes the artist.</p>
<p>$answer$ = The grandmother visits the grandmother. ; $proof$ = # sent1 sent2 ✓ $answer$ ; $proof$ ; $question$ = What is one single-hop inference? ; $context$ = sent1: If someone is red and big then they like the grandmother. sent2: If someone visits the grandmother then they are red. sent3: If someone likes the artist then they visit the grandmother. sent4: The artist likes the grandmother. sent5: The grandmother eats the artist. sent6: The grandmother is big. sent7: The grandmother is cold. sent8: The grandmother likes the artist.</p>
<p>$answer$ = The artist visits the grandmother. ; $proof$ = # sent3 sent4 ✗ $answer$ ; $proof$ ; $question$ = What is one single-hop inference? ; $context$ = sent1: If someone likes the grandmother then the grandmother eats the artist. sent2: If someone eats the artist then the artist is young. sent3: If the artist eats the grandmother then the artist is cold. sent4: If someone eats the artist and the artist eats the grandmother then the grandmother eats the artist. sent5: If someone is red and big then they like the grandmother. sent6: If someone visits the grandmother then they are red. sent7: If someone is rough then they eat the grandmother. sent8: If someone likes the artist then they visit the grandmother. sent9: The artist likes the grandmother. sent10: The grandmother eats the artist. sent11: The grandmother is big. sent12: The grandmother is cold. sent13: The grandmother is young. sent14: The grandmother likes the artist. sent15: The grandmother visits the artist. sent16: The artist is young.</p>
<p>$answer$ = The red sent6 sent15 ✗</p>
<p>Example 3 $answer$ ; $proof$ ; $question$ = What is one single-hop inference? ; $context$ = sent1: All young things are smart. sent2: All smart, young things are nice. sent3: Bob is smart. sent4: Bob is young $answer$ = Bob is nice. ; $proof$ = # sent2 &amp; sent3 sent4 ✓ $answer$ ; $proof$ ; $question$ = What is one single-hop inference? ; $context$ = sent1: All young things are smart. sent2: All smart, young things are nice. sent3: If something is young and nice then it is quiet. sent4: Becky is big. sent5: Becky is green. sent6: Becky is young. sent7: Larry is big. sent8: Larry is quiet. sent9: Larry is red. sent10: Larry is young. sent11: Bob is smart.</p>
<p>$answer$ = Bob is nice. ; $proof$ = # sent2 &amp; sent11 sent16 ✗   Table 16: Some more examples of inference errors made by ProofWriter ("Iter"). We see that having extra information in the theory than what is required to prove the conclusion leads to errors (shown in red). Having limited information in the theory reduces errors. Sentences in blue depict the sentences which are added to the theory with respect to the row above. Please refer to Section 5.5 for more details.</p>
<p>fact1 :
fact1Charlie is blue. fact2: Charlie is round. fact3: Erin is kind. fact4: Dave is round. rule1: If someone is blue then they are kind. rule2: Round, kind people are white. statement: Charlie is white. conc1: Charlie is kind.</p>
<p>Figure 2 :
2Reasoning process in different models. (a): ProofWriter ("All") directly output the entailment prediction and proof graph for given input. (b): ProofWriter ("Iter") iteratively generates the one-step intermediate conclusions and their proofs. (c): FAIRR selects a rule, then a fact, and finally combines them to generate an intermediate inference. Note that the proof is implicitly determined by the selection steps. Please refer to Section 3.1 for details.</p>
<p>Figure 4 :Figure 5 :
45Comparison of ProofWriter ("Iter") and FAIRR on precision and recall of generated inferences with increasing reasoning depths. Depth-wise comparison of ProofWriter ("Iter") and FAIRR on limited inference budgets. Please refer to Section 5.3 for details.</p>
<p>Text Reasoning in text is a well studied problem in NLP. Natural Language Inference (NLI)(Dagan et al., 2006) is one of the most prominent tasks that require reasoning over text to answer if a statement is entailed, contradicted, or neutral, given a hypothesis. More recently, datasets like HotpotQA(Yang et al., 2018), bAbI(Weston et al., 2016), QuaRTz (Tafjord et al., 2019), ROPES (Lin et al., 2019), CLUTRR (Sinha et al., 2019), etc., have studied different aspects of reasoning over textual inputs. These tasks usually require implicit reasoning, where the model needs to internally infer the rules required to solve the task. In contrast, RuleTaker(Clark et al., 2020) deals with explicit reasoning (also known as deductive reasoning).Proof Generation Recently, some works have been addressing the problem of proof generation from an NL-based theory. Prover (Saha et al., 2020) trains a RoBERTa-based model that predicts nodes and edges of the proof graph. ProofWriter (Tafjord et al., 2021) is a T5-based(Raffel et al., 2020)   model, that iteratively generates one-hop conclusions and proofs from a theory. Another workMul- tiProver (Saha et al., 2021), generates multiple possible proofs for a statement. While we study the same problem of proof generation similar to these works, we develop a more faithful and robust model designing a modular system for proof generation.</p>
<p>With the advent of pretrained language models (BERT(Devlin et al., 2019),RoBERTa (Liu et al., 2019), etc.), there has been an increasing trend on solving various reasoning tasks with high accuracy. Faithfulness of such models (Jacovi and Goldberg, 2020) aims to understand whether the models are actually learning to solve the task or rather depending on some shortcut patterns. Saliency-based explanations(Sundararajan et al., 2017; Lundberg and Lee, 2017; Murdoch  et al., 2018; Sanyal and Ren, 2021)  mainly focus on</p>
<p>sets are detailed below: Subject proper name pool: {'George', 'Paul', 'Ronald', 'Emma', 'Magnus', 'Timothy', 'Chris', 'Molly', 'Diana', 'Joseph', 'Becky', 'Kurt', 'Ivan', 'Steve', 'Laura', 'Oliver', 'Adam', 'Larry'} Subject common name pool: {'mother', 'fa</p>
<p>Figure 6 :
6Proof Accuracy of FAIRR when tested on ParaRules while using limited amount of ParaRules along with D0-D3 for training. See Appendix E for more details.</p>
<p>Figure 7 :
7Depth-wise comparison of ProofWriter ("Iter") and FAIRR (both trained on D0-3 dataset) on limited inference budgets. Please refer to Appendix G for details.</p>
<p>Table 2 :
2Comparison of FAIRR with ProofWriter 
("Iter") when trained on D0-D3 dataset and tested on 
different robustness datasets. EA, PA, and C refers to 
entailment accuracy, proof accuracy, and consistency, 
respectively. Please refer to Section 5.2 for more details. </p>
<p>Entailment Accuracy 
Proof Accuracy </p>
<p>d 
PW ("Iter") FAIRR PW ("Iter") FAIRR </p>
<p>N/A 
98.9 
99.3 
98.9 
99.3 
0 
99.9 
100.0 
99.9 
100.0 
1 
79.1 
96.0 
78.8 
95.7 
2 
76.6 
93.4 
73.4 
91.4 
3 
72.7 
89.8 
67.8 
85.7 </p>
<p>All 
89.6 
96.8 
88.4 
95.9 </p>
<p>Table 3 :
3Comparison of FAIRR with ProofWriter ("Iter") trained on D0-D3 and tested on subject robustness dataset. Baseline results are generated using the checkpoint provided by the authors. For more details please refer to Section 5.2.</p>
<p>Here, we study the relevance of the intermediate inferences generated by FAIRR and ProofWriter ("Iter"). Let T be the set of intermediate inferences required for generating the proof graph for the statement. Further, let G be the set of intermediate inferences actually generated by a model. Then, the precision and recall are defined as P = |T ∩G| |G| , and R =|T ∩G| 
|T | </p>
<p>Table 4 :
4Examples of inferences made by ProofWriter.Blue text denotes incrementally added sentences in the 
theory and red text denotes an error. The (⋅) is the 
generated proof. Refer to Section 5.5 for more details. </p>
<p>Bernhard Schölkopf. 2019. Causality for machine learning. CoRR, abs/1911.10500.Amir Feder, Nadav Oved, Uri Shalit, and Roi Reichart. 
2021b. Causalm: Causal model explanation through 
counterfactual language models. Computational Lin-
guistics, 47(2):333-386. </p>
<p>Knut Hinkelmann. 2004. Forward chaining vs. back-
ward chaining. University of Applied Sciences North-
western Switzerland, School of Business. </p>
<p>Alon Jacovi and Yoav Goldberg. 2020. Towards faith-
fully interpretable NLP systems: How should we 
define and evaluate faithfulness? In Proceedings 
of the 58th Annual Meeting of the Association for 
Computational Linguistics, pages 4198-4205, On-
line. Association for Computational Linguistics. </p>
<p>Aishwarya Kamath and Rajarshi Das. 2019. A survey 
on semantic parsing. In Automated Knowledge Base 
Construction (AKBC). </p>
<p>Kevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gard-
ner. 2019. Reasoning over paragraph effects in situ-
ations. In Proceedings of the 2nd Workshop on Ma-
chine Reading for Question Answering, pages 58-62, 
Hong Kong, China. Association for Computational 
Linguistics. </p>
<p>Zachary C. Lipton. 2018. The mythos of model in-
terpretability: In machine learning, the concept of 
interpretability is both important and slippery. Queue, 
(3). </p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, and Jingfei Du an. 
2019. Roberta: A robustly optimized bert pretraining 
approach. ArXiv preprint, abs/1907.11692. </p>
<p>Joshua R. Loftus, Chris Russell, Matt J. Kusner, and Ri-
cardo Silva. 2018. Causal reasoning for algorithmic 
fairness. CoRR, abs/1805.05859. </p>
<p>Scott M. Lundberg and Su-In Lee. 2017. A unified 
approach to interpreting model predictions. In Ad-
vances in Neural Information Processing Systems 30: 
Annual Conference on Neural Information Process-
ing Systems 2017, December 4-9, 2017, Long Beach, 
CA, USA, pages 4765-4774. </p>
<p>John W. McCarthy. 1959. Programs with common 
sense. In Proc. Tedding Conf. on the Mechanization 
of Thought Processes, pages 75-91. </p>
<p>W. James Murdoch, Peter J. Liu, and Bin Yu. 2018. Be-
yond word importance: Contextual decomposition to 
extract interactions from lstms. In 6th International 
Conference on Learning Representations, ICLR 2018, 
Vancouver, BC, Canada, April 30 -May 3, 2018, Con-
ference Track Proceedings. OpenReview.net. </p>
<p>Allen Newell and Herbert A. Simon. 1956. The logic 
theory machine-a complex information processing 
system. IRE Trans. Information Theory, 2:61-79. </p>
<p>J. Pearl. 2004. Graphical models for probabilistic and 
causal reasoning, pages 70-1. </p>
<p>Judea Pearl. 2000. Causality: Models, Reasoning and 
Inference. Cambridge University Press. </p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi 
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the 
limits of transfer learning with a unified text-to-text 
transformer. Journal of Machine Learning Research, 
21(140):1-67. </p>
<p>Tim Rocktäschel and Sebastian Riedel. 2017. End-to-
end differentiable proving. In Advances in Neural 
Information Processing Systems, volume 30. Curran 
Associates, Inc. </p>
<p>Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, 
and Mohit Bansal. 2020. PRover: Proof generation 
for interpretable reasoning over rules. In Proceedings 
of the 2020 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 122-136, 
Online. Association for Computational Linguistics. </p>
<p>Swarnadeep Saha, Prateek Yadav, and Mohit Bansal. 
2021. multiPRover: Generating multiple proofs 
for improved interpretability in rule reasoning. In 
NAACL. </p>
<p>Soumya Sanyal and Xiang Ren. 2021. Discretized in-
tegrated gradients for explaining language models. 
In Proceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing, pages 
10285-10299, Online and Punta Cana, Dominican 
Republic. Association for Computational Linguistics. </p>
<p>Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle 
Pineau, and William L. Hamilton. 2019. CLUTRR: 
A diagnostic benchmark for inductive reasoning from 
text. In Proceedings of the 2019 Conference on 
Empirical Methods in Natural Language Processing 
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages 
4506-4515, Hong Kong, China. Association for Com-
putational Linguistics. </p>
<p>Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. 
Axiomatic attribution for deep networks. In Proceed-
ings of the 34th International Conference on Machine 
Learning, ICML 2017, Sydney, NSW, Australia, 6-11 
August 2017, volume 70 of Proceedings of Machine 
Learning Research, pages 3319-3328. PMLR. </p>
<p>Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. 
ProofWriter: Generating implications, proofs, and 
abductive statements over natural language. In Find-
ings of the Association for Computational Linguis-
tics: ACL-IJCNLP 2021, pages 3621-3634, Online. 
Association for Computational Linguistics. </p>
<p>Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter 
Clark. 2019. QuaRTz: An open-domain dataset of 
qualitative relationship questions. In Proceedings of 
the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International 
Joint Conference on Natural Language Processing 
(EMNLP-IJCNLP), pages 5941-5946, Hong Kong, 
China. Association for Computational Linguistics. 
Dataset Split Number 
of Theo-
ries </p>
<p>Number 
of Ques-
tions </p>
<p>Number of 
Conclusions 
per Theory 
(min/mean/max) </p>
<p>train 
18889 
69906 
0/0.81/18 
D0 
dev 
2700 
10070 
0/0.81/14 
test 
5389 
20024 
0/0.8/12 </p>
<p>train 
9008 
69616 
1/1.69/13 
D1 
dev 
1318 
10188 
1/1.7/14 
test 
2607 
20210 
1/1.7/12 </p>
<p>train 
6330 
70076 
2/3.15/14 
D2 
dev 
909 
10094 
2/3.09/12 
test 
1794 
19840 
2/3.11/14 </p>
<p>train 
4816 
69388 
3/4.81/16 
D3 
dev 
719 
10302 
3/4.73/14 
test 
1405 
20346 
3/4.72/15 </p>
<p>train 
3322 
69810 
5/9.12/21 
D5 
dev 
482 
10190 
5/9.13/21 
test 
948 
20030 
5/9.08/21 </p>
<p>train 
1681 
28010 
3/4.25/14 
Pararules dev 
240 
4004 
3/4.53/13 
test 
482 
8008 
3/4.24/11 </p>
<p>Table 5 :
5Statistics of datasets introduced by Tafjord et al., 2021 with the number of theories, questions and conclusions per theory for all three splits of each dataset. The splits are kept the same as the original dataset. Please refer to Appendix A for more details.</p>
<p>Table 6 :
6Statistics of datasets introduced in this paper 
with the number of theories, questions and conclusions 
per theory for all three splits of each dataset. We use 
these datasets to quantify the robustness of FAIRR and 
compare it with baselines. Please refer to Appendix B 
for more details. </p>
<p>Table 11 :
11Comparison of variants of FAIRR where dif-
ferent components (RS, FS, KC) and their combinations 
are trained and tested on subject robustness datasets. EA 
and PA refers to entailment accuracy and proof accu-
racy respectively. Please refer to Appendix D for more 
details. </p>
<p>Entailment Accuracy 
Proof Accuracy </p>
<p>d 
PW ("All") [T5-11B] FAIRR PW ("All") [T5-11B] FAIRR </p>
<p>0 
99.9 
100.0 
99.9 
100.0 
1 
99.3 
99.6 
99.3 
99.6 
2 
98.3 
97.6 
97.7 
97.4 
3 
98.2 
95.4 
96.5 
95.1 
4 
91.5 
91.6 
83.1 
91.6 </p>
<p>All 
99.1 
98.7 
98.5 
98.6 </p>
<p>Table 12 :
12Comparison of FAIRR with ProofWriter 
("All") [T5-11B] when trained on D3+ParaRules and 
tested on ParaRules. Results for ProofWriter ("All") 
[T5-11B] are copied from the paper. Please refer to 
Appendix F for more details. </p>
<p>Error Type
ErrorInput ( format: complete theory) Output ( format: $answer$ = conclusion ; $proof$ = # rule &amp; facts) Wrong inference $answer$ ; $proof$ ; $question$ = What is one single-hop inference? ; $context$ = sent1: Cold people are red. sent2: If someone is quiet then they are cold. sent3: Smart, blue people are quiet. sent4: If Paul is blue then Paul is furry. sent5: If someone is blue then they are quiet. sent6: If Steve is furry and Steve is smart then Steve is white. sent7: If Chris is red and Chris is furry then Chris is blue. sent8: Quiet people are cold. sent9: Chris is blue. sent10: Chris is white. sent11: Ronald is red. sent12: Paul is quiet. sent13: Paul is smart. sent14: Steve is blue. sent15: Steve is furry. sent16: Steve is red. sent17: Steve is smart. sent18: Steve is white. $answer$ = Dave is quiet. ; $proof$ = # sent5 sent9 Wrong inference $answer$ ; $proof$ ; $question$ = What is one single-hop inference? ; $context$ = sent1: Cold people are red. sent2: If someone is quiet then they are cold. sent3: Smart, blue people are quiet. sent4: If Paul is blue then Paul is furry. sent5: If someone is blue then they are quiet. sent6: If Steve is furry and Steve is smart then Steve is white. sent7: If Chris is red and Chris is furry then Chris is blue. sent8: Quiet people are cold. sent9: Chris is blue. sent10: Chris is white. sent11: Ronald is red. sent12: Paul is quiet. sent13: Paul is smart. sent14: Steve is blue. sent15: Steve is furry. sent16: Steve is red. sent17: Steve is smart. sent18: Steve is white. sent19: Dave is quiet.</p>
<p>Table 14 :
14Examples of inference errors made by ProofWriter ("Iter"). Please refer to Appendix I for more details.Error Type Input ( format: [facts], rule) Inference Wrong inference [the racer needs the janitor.], if someone needs the janitor then the janitor likes the racer. the janitor likes the race. Wrong inference [oliver is big.], big people are young. the oliver is young. Wrong inference [the shoplifter needs the shoplifter., the shoplifter needs the dinosaur.], if something needs the dinosaur and it needs the shoplifter then it is round. the shop is round.1091 </p>
<p>Table 15 :
15Examples of inference errors made by FAIRR while composing rules and facts using the knowledge composer. Please refer to Appendix I for more details.Output ( format: $answer$ = conclusion ; $proof$ = # rule &amp; facts)Input ( format: complete theory) </p>
<p>Following ProofWriter, we perform regex to add/remove "not" which suffices for this dataset.
The code to reproduce numbers of ProofWriter is not publicly available. We either copy results directly from the paper or run our own inference on model checkpoints made available by the authors.
Forward chaining is described as repeated application of modus ponens(Hinkelmann, 2004), which requires at least two premises to then logically conclude an inference.
data-augmented training results for ProofWriter are not reported in the figure since the training code is not available
K HyperparametersWe use RoBERTa-large models (Liu et al., 2019)   to model the rule selector and fact selector in FAIRR. For selecting the best hyperparameters for both these components, we selected the max training epochs in: {10, 15, 20}, warmup updates in: {0.05, 0.1}, weight decay in: {0.1, 0.01, 0.001}, learning rate in: {3e-6, 5e-6, 1e-6}, and batch size in {16, 32}.We use T5 (Raffel et al., 2020) (T5-large) to model the knowledge composer in FAIRR and train it using the default hyperparameters available in the Hugging Face transformers library(Wolf et al., 2020). All models were trained on Nvidia Quadro RTX 8000 GPUs. Training a FAIRR on a single GPU takes around 20 hours on average.
2020. Invariance, causality and robustness. Peter Bühlmann, Statistical Science. 353Peter Bühlmann. 2020. Invariance, causality and robust- ness. Statistical Science, 35(3):404-426.</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20. the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. In Pro- ceedings of the Twenty-Ninth International Joint Con- ference on Artificial Intelligence, IJCAI-20, pages 3882-3890. International Joint Conferences on Arti- ficial Intelligence Organization. Main track.</p>
<p>The pascal recognising textual entailment challenge. Oren Ido Dagan, Bernardo Glickman, Magnini, Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment. Berlin, Heidelberg; Berlin HeidelbergSpringerIdo Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment chal- lenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pages 177-190, Berlin, Heidelberg. Springer Berlin Heidelberg.</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>. Amir Feder, Katherine A Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret EAmir Feder, Katherine A. Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E.</p>
<p>Causal inference in natural language processing: Estimation, prediction, interpretation and beyond. Brandon M Roberts, Victor Stewart, Diyi Veitch, Yang, abs/2109.00725CoRRRoberts, Brandon M. Stewart, Victor Veitch, and Diyi Yang. 2021a. Causal inference in natural language processing: Estimation, prediction, interpretation and beyond. CoRR, abs/2109.00725.</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Long Beach, CA, USAAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008.</p>
<p>Counterfactual invariance to spurious correlations: Why and how to pass stress tests. Victor Veitch, Steve Alexander D&apos;amour, Jacob Yadlowsky, Eisenstein, abs/2106.00545CoRRVictor Veitch, Alexander D'Amour, Steve Yadlowsky, and Jacob Eisenstein. 2021. Counterfactual invari- ance to spurious correlations: Why and how to pass stress tests. CoRR, abs/2106.00545.</p>
<p>Causal mediation analysis for interpreting neural NLP: the case of gender bias. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, Stuart M Shieber, abs/2004.12265CoRRJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stu- art M. Shieber. 2020. Causal mediation analysis for interpreting neural NLP: the case of gender bias. CoRR, abs/2004.12265.</p>
<p>NLProlog: Reasoning with weak unification for question answering in natural language. Leon Weber, Pasquale Minervini, Jannes Münchmeyer, 10.18653/v1/P19-1618Proceedings of the 57th. the 57thUlf Leser, and Tim RocktäschelLeon Weber, Pasquale Minervini, Jannes Münchmeyer, Ulf Leser, and Tim Rocktäschel. 2019. NLProlog: Reasoning with weak unification for question answer- ing in natural language. In Proceedings of the 57th</p>
<p>Association for Computational Linguistics. Annual Meeting of the Association for Computational Linguistics. Florence, ItalyAnnual Meeting of the Association for Computational Linguistics, pages 6151-6161, Florence, Italy. Asso- ciation for Computational Linguistics.</p>
<p>Towards ai-complete question answering: A set of prerequisite toy tasks. Jason Weston, Antoine Bordes, Sumit Chopra, Tomás Mikolov, 4th International Conference on Learning Representations. San Juan, Puerto RicoConference Track ProceedingsJason Weston, Antoine Bordes, Sumit Chopra, and Tomás Mikolov. 2016. Towards ai-complete question answering: A set of prerequisite toy tasks. In 4th In- ternational Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Le Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnlineAssociation for Computational LinguisticsThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Remi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Trans- formers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christo- pher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing, pages 2369-2380, Brussels, Belgium. Association for Com- putational Linguistics.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1024 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1024</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1024</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-272987499</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.19816v1.pdf" target="_blank">Grounded Curriculum Learning</a></p>
                <p><strong>Paper Abstract:</strong> The high cost of real-world data for robotics Reinforcement Learning (RL) leads to the wide usage of simulators. Despite extensive work on building better dynamics models for simulators to match with the real world, there is another, often-overlooked mismatch between simulations and the real world, namely the distribution of available training tasks. Such a mismatch is further exacerbated by existing curriculum learning techniques, which automatically vary the simulation task distribution without considering its relevance to the real world. Considering these challenges, we posit that curriculum learning for robotics RL needs to be grounded in real-world task distributions. To this end, we propose Grounded Curriculum Learning (GCL), which aligns the simulated task distribution in the curriculum with the real world, as well as explicitly considers what tasks have been given to the robot and how the robot has performed in the past. We validate GCL using the BARN dataset on complex navigation tasks, achieving a 6.8% and 6.5% higher success rate compared to a state-of-the-art CL method and a curriculum designed by human experts, respectively. These results show that GCL can enhance learning efficiency and navigation performance by grounding the simulation task distribution in the real world within an adaptive curriculum.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1024.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1024.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GCL Student</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grounded Curriculum Learning Student Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A partially observable embodied navigation agent trained with reinforcement learning (PPO) within the GCL framework to learn navigation policies in BARN environments; its curriculum is generated by a fully-informed teacher and grounded in limited real-world tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Student agent (π_S)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embodied robotic navigation agent trained with reinforcement learning (PPO). Operates in a POMDP, maximizes expected cumulative reward encouraging progress to goal and penalizing collisions/time. Trajectories collected from simulated BARN tasks (with occasional real-world grounding during curriculum).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>robotic agent (simulated robot in Isaac-Gym; policy intended for real-world deployment)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>BARN Challenge environments (Benchmark for Autonomous Robot Navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Procedurally generated navigation environments ranging from open, easy spaces to highly constrained, cluttered mazes; complexity arises from obstacle density, tortuous paths, and constrained passages. Environments were produced by the BARN generator and split into training (70%) and test (30%) sets to evaluate generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Heuristic task difficulty measured by shortest path length (used as a metric for task difficulty/complexity). Other complexity attributes include obstacle density and constrainedness implicitly encoded by the BARN generator.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies across dataset from low to high (paper emphasizes existence of 'highly constrained' / high complexity environments)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Procedural generation diversity from the BARN generator (many environment instances), plus VAE-based latent sampling of tasks (latent dim = 32) and probabilistic mixing with real tasks via grounding probability ϵ; explicit train/test split (70/30) to measure variation/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (paper emphasizes broad distribution from easy to highly constrained; curriculum samples both generated and real tasks with parameter ϵ controlling mix)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task Success Rate (%), Navigation Progress (% of path completed), Average Reward (unitless RL reward), Average Steps per successful task, Average Speed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Success Rate: 81.85% (GCL overall, test set); Navigation Progress: 68.89%; Avg. Reward: 19.45 (all reported for GCL).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly discusses the relationship: simulation-to-real mismatch arises not only from dynamics but from differences in task distribution (complexity and variation). GCL argues that grounding simulated curricula in a real-world task distribution and tracking task history/performance mitigates poor generalization caused by mismatch. The teacher adjusts task complexity adaptively (via latent-space sampling and regret-driven objectives) while periodic sampling of real tasks (probability ϵ) maintains relevance; ablations show removing real grounding harms performance, indicating a trade-off where high variation or generated complexity alone (without grounding) reduces real-world applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Grounded curriculum learning (adaptive curriculum generated by fully-informed teacher using VAE latent task space, antagonist for regret), with PPO for student; mixture of generated tasks and real tasks controlled by grounding probability ϵ; 128 parallel environments in Isaac-Gym.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Yes — evaluated on unseen test environments (70/30 split). GCL achieved higher test success (81.85%) and better navigation progress than baselines, and deployed the learned policy in a real-world BARN environment (real-world deployment succeeded often but with occasional failures due to residual sim-to-real gaps). GCL improved success rate by 6.8% and 6.5% compared to a state-of-the-art CL baseline and a human-designed curriculum, respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training epochs reported: 5000; parallel environments: 128 (Isaac-Gym). PPO used with learning rates reported (student LR 3e-4); exact environment interaction counts not enumerated but system designed to leverage 128 parallel sims for efficient sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Grounding the simulated curriculum in real-world task distributions (via VAE trained on limited real tasks and sampling with probability ϵ) plus tracking task history and student performance produces higher success and better navigation progress; removing real-world grounding yields notably worse performance (ablation GCL w/o real), showing that variation/novelty alone is insufficient without relevance to target domain; adaptive complexity progression (teacher-driven) is beneficial compared to fixed difficulty or stateless generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounded Curriculum Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1024.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1024.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GCL Antagonist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grounded Curriculum Learning Antagonist Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adversarial (antagonist) embodied agent trained with the same observability and RL setup as the student to compute regret (difference in expected returns) used by the teacher to generate challenging tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Antagonist agent (π_A)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embodied navigation agent trained with reinforcement learning (PPO) under identical observability/hyperparameters as the student; its role is to provide an adversarial performance baseline so the teacher can compute flexible regret REGRET(a) = V_a(π_A) − V_a(π_S) and drive curriculum generation towards tasks that maximize this regret.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>robotic agent (simulated adversary in Isaac-Gym)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>BARN Challenge environments (same as student)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same procedurally generated navigation environments (easy to highly constrained) used to collect antagonist trajectories; complexity and variation match those described for the student.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Implicitly the same measures as student (shortest path length as heuristic difficulty; obstacle density/constrainedness from BARN generator).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies (low to high) as selected by teacher</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Varied by teacher sampling from VAE latent space and real-task mixing (ϵ); antagonist is trained across these generated and real tasks to provide regret estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (antagonist is exposed to varied set consistent with teacher's curriculum generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Value function V_a (expected discounted return) per task, used to compute REGRET = V_a(π_A) − V_a(π_S); antagonist objective J_A mirrors student objective.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Used operationally to expose a relationship: antagonist performance relative to student (regret) signals tasks where student struggles; the teacher uses regret to select tasks of appropriate complexity/variation, implicitly trading off complexity vs. variation to maximize useful learning signal.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>PPO-trained antagonist within the dual-agent GCL framework; used to compute regret for teacher optimization (regret-driven Unsupervised Environment Design).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Not reported as an isolated evaluation; antagonist is an internal component used to compute regret for curriculum generation rather than being evaluated alone for generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Same training regime as student (PPO, concurrent trajectories in 128 parallel environments), exact sample counts not enumerated separately.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incorporating an antagonist agent to compute regret provides a useful signal for the teacher to select tasks that challenge the student; regret-driven task selection combined with real-world grounding improves curriculum effectiveness compared to stateless or ungrounded generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounded Curriculum Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1024.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1024.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLUTR baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLUTR (Curriculum Learning via Unsupervised Task Representation Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior unsupervised environment design approach that learns a latent task representation (VAE style) and generates curricula automatically but without grounding in student performance history or explicit real-world task distributions; used as a state-of-the-art baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Clutr: Curriculum learning via unsupervised task representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CLUTR-trained agent (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent trained under the CLUTR automatic curriculum generation method (unsupervised representation of tasks, stateless teacher/bandit-style task generation in prior work); used as a comparative baseline for navigation performance in BARN tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (training performed in simulation under CLUTR curricula)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>BARN Challenge environments (evaluation/test set)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same BARN procedural environments (varied difficulty from easy to highly constrained); CLUTR generates tasks from its learned latent space (stateless selection) without grounding to real tasks or student performance history in the way GCL does.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Implicit (task difficulty captured indirectly via learned latent space); not explicitly tracked by shortest path heuristic in CLUTR baseline description within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies (but CLUTR reported to show limited variation in task difficulty over time in these experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Latent-space task generation from learned VAE; in the paper CLUTR is criticized for lacking task-awareness and performance-history grounding which may limit exploration of the task space.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>moderate (paper reports CLUTR shows limited variation in task difficulty over training in these experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task Success Rate (%) (and other navigation metrics), reported as comparison to GCL</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Approximately 75.05% success rate on test set (derived from reported improvement: GCL 81.85% which is 6.8% higher than the SOTA CL baseline as stated in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper reports CLUTR's stateless teacher and lack of student performance history limit its ability to model the task space thoroughly, resulting in limited variation of task difficulty; thus CLUTR tends to generate tasks that do not sufficiently cover the real-world task distribution, harming generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Unsupervised environment design / automatic curriculum via learned latent task representation (CLUTR algorithm).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Compared on the same BARN test set; reported inferior test success relative to GCL (GCL reported +6.8% absolute improvement in success rate over this SOTA CL baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not explicitly detailed in this paper for CLUTR baseline; CLUTR leverages its own latent representation learning but exact training epochs/parallelism in the comparative experiments are not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CLUTR, while an automatic curriculum method, exhibited limited adaptation of task difficulty over time in these experiments—likely because it lacks student performance history and a fully informed teacher—resulting in lower success rates and worse generalization on the BARN test set compared to GCL. This supports the paper's claim that grounding (real tasks + performance history + task awareness) is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounded Curriculum Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Emergent complexity and zero-shot transfer via unsupervised environment design <em>(Rating: 2)</em></li>
                <li>Clutr: Curriculum learning via unsupervised task representation learning <em>(Rating: 2)</em></li>
                <li>Evolving curricula with regretbased environment design <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Benchmarking metric ground navigation <em>(Rating: 2)</em></li>
                <li>Benchmarking reinforcement learning techniques for autonomous navigation <em>(Rating: 2)</em></li>
                <li>Robust adversarial reinforcement learning <em>(Rating: 1)</em></li>
                <li>Automatic curriculum learning through value disagreement <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1024",
    "paper_id": "paper-272987499",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "GCL Student",
            "name_full": "Grounded Curriculum Learning Student Agent",
            "brief_description": "A partially observable embodied navigation agent trained with reinforcement learning (PPO) within the GCL framework to learn navigation policies in BARN environments; its curriculum is generated by a fully-informed teacher and grounded in limited real-world tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Student agent (π_S)",
            "agent_description": "Embodied robotic navigation agent trained with reinforcement learning (PPO). Operates in a POMDP, maximizes expected cumulative reward encouraging progress to goal and penalizing collisions/time. Trajectories collected from simulated BARN tasks (with occasional real-world grounding during curriculum).",
            "agent_type": "robotic agent (simulated robot in Isaac-Gym; policy intended for real-world deployment)",
            "environment_name": "BARN Challenge environments (Benchmark for Autonomous Robot Navigation)",
            "environment_description": "Procedurally generated navigation environments ranging from open, easy spaces to highly constrained, cluttered mazes; complexity arises from obstacle density, tortuous paths, and constrained passages. Environments were produced by the BARN generator and split into training (70%) and test (30%) sets to evaluate generalization.",
            "complexity_measure": "Heuristic task difficulty measured by shortest path length (used as a metric for task difficulty/complexity). Other complexity attributes include obstacle density and constrainedness implicitly encoded by the BARN generator.",
            "complexity_level": "varies across dataset from low to high (paper emphasizes existence of 'highly constrained' / high complexity environments)",
            "variation_measure": "Procedural generation diversity from the BARN generator (many environment instances), plus VAE-based latent sampling of tasks (latent dim = 32) and probabilistic mixing with real tasks via grounding probability ϵ; explicit train/test split (70/30) to measure variation/generalization.",
            "variation_level": "high (paper emphasizes broad distribution from easy to highly constrained; curriculum samples both generated and real tasks with parameter ϵ controlling mix)",
            "performance_metric": "Task Success Rate (%), Navigation Progress (% of path completed), Average Reward (unitless RL reward), Average Steps per successful task, Average Speed",
            "performance_value": "Success Rate: 81.85% (GCL overall, test set); Navigation Progress: 68.89%; Avg. Reward: 19.45 (all reported for GCL).",
            "complexity_variation_relationship": "Yes — the paper explicitly discusses the relationship: simulation-to-real mismatch arises not only from dynamics but from differences in task distribution (complexity and variation). GCL argues that grounding simulated curricula in a real-world task distribution and tracking task history/performance mitigates poor generalization caused by mismatch. The teacher adjusts task complexity adaptively (via latent-space sampling and regret-driven objectives) while periodic sampling of real tasks (probability ϵ) maintains relevance; ablations show removing real grounding harms performance, indicating a trade-off where high variation or generated complexity alone (without grounding) reduces real-world applicability.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Grounded curriculum learning (adaptive curriculum generated by fully-informed teacher using VAE latent task space, antagonist for regret), with PPO for student; mixture of generated tasks and real tasks controlled by grounding probability ϵ; 128 parallel environments in Isaac-Gym.",
            "generalization_tested": true,
            "generalization_results": "Yes — evaluated on unseen test environments (70/30 split). GCL achieved higher test success (81.85%) and better navigation progress than baselines, and deployed the learned policy in a real-world BARN environment (real-world deployment succeeded often but with occasional failures due to residual sim-to-real gaps). GCL improved success rate by 6.8% and 6.5% compared to a state-of-the-art CL baseline and a human-designed curriculum, respectively.",
            "sample_efficiency": "Training epochs reported: 5000; parallel environments: 128 (Isaac-Gym). PPO used with learning rates reported (student LR 3e-4); exact environment interaction counts not enumerated but system designed to leverage 128 parallel sims for efficient sampling.",
            "key_findings": "Grounding the simulated curriculum in real-world task distributions (via VAE trained on limited real tasks and sampling with probability ϵ) plus tracking task history and student performance produces higher success and better navigation progress; removing real-world grounding yields notably worse performance (ablation GCL w/o real), showing that variation/novelty alone is insufficient without relevance to target domain; adaptive complexity progression (teacher-driven) is beneficial compared to fixed difficulty or stateless generation.",
            "uuid": "e1024.0",
            "source_info": {
                "paper_title": "Grounded Curriculum Learning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GCL Antagonist",
            "name_full": "Grounded Curriculum Learning Antagonist Agent",
            "brief_description": "An adversarial (antagonist) embodied agent trained with the same observability and RL setup as the student to compute regret (difference in expected returns) used by the teacher to generate challenging tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Antagonist agent (π_A)",
            "agent_description": "Embodied navigation agent trained with reinforcement learning (PPO) under identical observability/hyperparameters as the student; its role is to provide an adversarial performance baseline so the teacher can compute flexible regret REGRET(a) = V_a(π_A) − V_a(π_S) and drive curriculum generation towards tasks that maximize this regret.",
            "agent_type": "robotic agent (simulated adversary in Isaac-Gym)",
            "environment_name": "BARN Challenge environments (same as student)",
            "environment_description": "Same procedurally generated navigation environments (easy to highly constrained) used to collect antagonist trajectories; complexity and variation match those described for the student.",
            "complexity_measure": "Implicitly the same measures as student (shortest path length as heuristic difficulty; obstacle density/constrainedness from BARN generator).",
            "complexity_level": "varies (low to high) as selected by teacher",
            "variation_measure": "Varied by teacher sampling from VAE latent space and real-task mixing (ϵ); antagonist is trained across these generated and real tasks to provide regret estimates.",
            "variation_level": "high (antagonist is exposed to varied set consistent with teacher's curriculum generation)",
            "performance_metric": "Value function V_a (expected discounted return) per task, used to compute REGRET = V_a(π_A) − V_a(π_S); antagonist objective J_A mirrors student objective.",
            "performance_value": null,
            "complexity_variation_relationship": "Used operationally to expose a relationship: antagonist performance relative to student (regret) signals tasks where student struggles; the teacher uses regret to select tasks of appropriate complexity/variation, implicitly trading off complexity vs. variation to maximize useful learning signal.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "PPO-trained antagonist within the dual-agent GCL framework; used to compute regret for teacher optimization (regret-driven Unsupervised Environment Design).",
            "generalization_tested": null,
            "generalization_results": "Not reported as an isolated evaluation; antagonist is an internal component used to compute regret for curriculum generation rather than being evaluated alone for generalization.",
            "sample_efficiency": "Same training regime as student (PPO, concurrent trajectories in 128 parallel environments), exact sample counts not enumerated separately.",
            "key_findings": "Incorporating an antagonist agent to compute regret provides a useful signal for the teacher to select tasks that challenge the student; regret-driven task selection combined with real-world grounding improves curriculum effectiveness compared to stateless or ungrounded generation.",
            "uuid": "e1024.1",
            "source_info": {
                "paper_title": "Grounded Curriculum Learning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "CLUTR baseline",
            "name_full": "CLUTR (Curriculum Learning via Unsupervised Task Representation Learning)",
            "brief_description": "A prior unsupervised environment design approach that learns a latent task representation (VAE style) and generates curricula automatically but without grounding in student performance history or explicit real-world task distributions; used as a state-of-the-art baseline in experiments.",
            "citation_title": "Clutr: Curriculum learning via unsupervised task representation learning",
            "mention_or_use": "use",
            "agent_name": "CLUTR-trained agent (baseline)",
            "agent_description": "Agent trained under the CLUTR automatic curriculum generation method (unsupervised representation of tasks, stateless teacher/bandit-style task generation in prior work); used as a comparative baseline for navigation performance in BARN tasks.",
            "agent_type": "simulated robotic agent (training performed in simulation under CLUTR curricula)",
            "environment_name": "BARN Challenge environments (evaluation/test set)",
            "environment_description": "Same BARN procedural environments (varied difficulty from easy to highly constrained); CLUTR generates tasks from its learned latent space (stateless selection) without grounding to real tasks or student performance history in the way GCL does.",
            "complexity_measure": "Implicit (task difficulty captured indirectly via learned latent space); not explicitly tracked by shortest path heuristic in CLUTR baseline description within this paper.",
            "complexity_level": "varies (but CLUTR reported to show limited variation in task difficulty over time in these experiments)",
            "variation_measure": "Latent-space task generation from learned VAE; in the paper CLUTR is criticized for lacking task-awareness and performance-history grounding which may limit exploration of the task space.",
            "variation_level": "moderate (paper reports CLUTR shows limited variation in task difficulty over training in these experiments)",
            "performance_metric": "Task Success Rate (%) (and other navigation metrics), reported as comparison to GCL",
            "performance_value": "Approximately 75.05% success rate on test set (derived from reported improvement: GCL 81.85% which is 6.8% higher than the SOTA CL baseline as stated in paper).",
            "complexity_variation_relationship": "Paper reports CLUTR's stateless teacher and lack of student performance history limit its ability to model the task space thoroughly, resulting in limited variation of task difficulty; thus CLUTR tends to generate tasks that do not sufficiently cover the real-world task distribution, harming generalization.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Unsupervised environment design / automatic curriculum via learned latent task representation (CLUTR algorithm).",
            "generalization_tested": true,
            "generalization_results": "Compared on the same BARN test set; reported inferior test success relative to GCL (GCL reported +6.8% absolute improvement in success rate over this SOTA CL baseline).",
            "sample_efficiency": "Not explicitly detailed in this paper for CLUTR baseline; CLUTR leverages its own latent representation learning but exact training epochs/parallelism in the comparative experiments are not specified here.",
            "key_findings": "CLUTR, while an automatic curriculum method, exhibited limited adaptation of task difficulty over time in these experiments—likely because it lacks student performance history and a fully informed teacher—resulting in lower success rates and worse generalization on the BARN test set compared to GCL. This supports the paper's claim that grounding (real tasks + performance history + task awareness) is critical.",
            "uuid": "e1024.2",
            "source_info": {
                "paper_title": "Grounded Curriculum Learning",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Emergent complexity and zero-shot transfer via unsupervised environment design",
            "rating": 2,
            "sanitized_title": "emergent_complexity_and_zeroshot_transfer_via_unsupervised_environment_design"
        },
        {
            "paper_title": "Clutr: Curriculum learning via unsupervised task representation learning",
            "rating": 2,
            "sanitized_title": "clutr_curriculum_learning_via_unsupervised_task_representation_learning"
        },
        {
            "paper_title": "Evolving curricula with regretbased environment design",
            "rating": 2,
            "sanitized_title": "evolving_curricula_with_regretbased_environment_design"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Benchmarking metric ground navigation",
            "rating": 2,
            "sanitized_title": "benchmarking_metric_ground_navigation"
        },
        {
            "paper_title": "Benchmarking reinforcement learning techniques for autonomous navigation",
            "rating": 2,
            "sanitized_title": "benchmarking_reinforcement_learning_techniques_for_autonomous_navigation"
        },
        {
            "paper_title": "Robust adversarial reinforcement learning",
            "rating": 1,
            "sanitized_title": "robust_adversarial_reinforcement_learning"
        },
        {
            "paper_title": "Automatic curriculum learning through value disagreement",
            "rating": 1,
            "sanitized_title": "automatic_curriculum_learning_through_value_disagreement"
        }
    ],
    "cost": 0.012570000000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Grounded Curriculum Learning
29 Sep 2024</p>
<p>Linji Wang 
Department of Computer Science
George Mason University</p>
<p>Zifan Xu zfxu@utexas.edu 
Department of Computer Science
The University of Texas at Austin</p>
<p>Peter Stone pstone@cs.utexas.edu 
Department of Computer Science
The University of Texas at Austin</p>
<p>SonyAI</p>
<p>Xuesu Xiao xiao@gmu.edu 
Department of Computer Science
George Mason University</p>
<p>Grounded Curriculum Learning
29 Sep 20245D061568979BABF897EB45AA59A91130arXiv:2409.19816v1[cs.RO]
The high cost of real-world data for robotics Reinforcement Learning (RL) leads to the wide usage of simulators.Despite extensive work on building better dynamics models for simulators to match with the real world, there is another, often-overlooked mismatch between simulations and the real world, namely the distribution of available training tasks.Such a mismatch is further exacerbated by existing curriculum learning techniques, which automatically vary the simulation task distribution without considering its relevance to the real world.Considering these challenges, we posit that curriculum learning for robotics RL needs to be grounded in real-world task distributions.To this end, we propose Grounded Curriculum Learning (GCL), which aligns the simulated task distribution in the curriculum with the real world, as well as explicitly considers what tasks have been given to the robot and how the robot has performed in the past.We validate GCL using the BARN dataset on complex navigation tasks, achieving a 6.8% and 6.5% higher success rate compared to a stateof-the-art CL method and a curriculum designed by human experts, respectively.These results show that GCL can enhance learning efficiency and navigation performance by grounding the simulation task distribution in the real world within an adaptive curriculum.</p>
<p>I. INTRODUCTION</p>
<p>Reinforcement learning (RL) has become a powerful tool that enables robots to learn complex behaviors through trialand-error interactions with their environments [1].However, applying RL to real-world robotic tasks presents significant challenges.The trial-and-error process often requires a vast amount of data, which is difficult and expensive to collect in real-world settings [2].As a result, simulators have become widely used to generate training data in a more controlled and cost-effective manner.</p>
<p>While much work has been done to build simulators with better dynamics models that more closely match the physical world [3], there is another, often-overlooked mismatch between simulations and the real world, namely the distribution of available training tasks.Specifically, the tasks generated in simulators may differ in complexity, variability, and structure compared to those that robots encounter after simulated training during deployment in real environments [4], [5].This mismatch can hinder the generalization and performance of RL agents when transitioning from simulated environments to real-world tasks.</p>
<p>This problem is further exacerbated by existing Curriculum Learning (CL) techniques, which automatically vary the simulation task distribution to facilitate learning [6].have demonstrated improved generalization by using teacher agents to generate a curriculum of increasingly complex tasks.However, these methods typically focus on optimizing the simulation curriculum without considering its relevance to the real world.Consequently, RL agents may be trained on simulation tasks that are not representative of real-world conditions, leading to poor performance upon deployment.</p>
<p>Considering these challenges, we posit that curriculum learning for robotics RL needs to be grounded in real-world task distributions.Grounding the curriculum in real-world tasks ensures that the learning process remains relevant and that the trained policies are more likely to generalize effectively when deployed in real-world environments.To this end, we introduce Grounded Curriculum Learning (GCL), a framework designed to align the simulated task distribution with real-world tasks to assure real-world generalization while improving learning efficiency with an adaptive curriculum.GCL achieves this by considering three key aspects: (1) simulation realism: aligning the simulated task distribution in the curriculum with the real world, (2) task awareness: tracking the sequence of tasks given to the robot, and (3) student performance: monitoring the student's performance across previous tasks in the curriculum (Fig. 1).</p>
<p>We validate GCL using the Benchmark Autonomous Robot Navigation (BARN) dataset, a standard testbed for evaluating robotic navigation performance in complex and highly constrained environments [9], [10].Our experiments demonstrate that GCL achieves a 6.8% and 6.5% higher success rate compared to a state-of-the-art CL method and a curriculum designed by human experts, respectively.These results highlight GCL's ability to enhance both learning efficiency and navigation performance by grounding the simulation task distribution in the real world within an adaptive curriculum.</p>
<p>II. RELATED WORK</p>
<p>GCL addresses a fundamental challenge in robotics RL: the mismatch between simulated and real-world task distributions, which is often overlooked despite extensive work on improving simulator dynamics.We review relevant literature in CL and Unsupervised Environment Design (UED), highlighting how GCL improves upon existing approaches.</p>
<p>CL in RL aims to improve learning efficiency by progressively increasing simulated task complexity [11].Recent work has focused on automatic curriculum generation [12], where the curriculum is dynamically adapted based on the agent's performance in simulation.Such an adapted curriculum improves efficiency, but it doesn't have a mechanism to ensure that the generated tasks can represent the real world, where a physical robot will eventually be deployed [3].</p>
<p>UED has emerged as a promising approach for automatically generating training tasks and adapting curricula in RL [13].UED methods aim to reduce the need for manually designed tasks in simulation, a key challenge in robotic RL.While traditional UED methods like Domain Randomization [14] and minimax approaches [15], [16] have shown effectiveness in simulated environments, they face challenges when applied to real-world robotics problems where data is scarce and expensive to obtain.</p>
<p>At the intersection of CL and UED, adaptive-teacher UED methods have shown promise in improving zero-shot generalization.PAIRED [7] introduced a regret-based approach to UED, using an adversarial game to generate increasingly complex environments.CLUTR [8] improved upon PAIRED by introducing unsupervised representation learning to UED, replacing the explicit task generator with a learned latent space using Variational Autoencoders [17], which allowed for more efficient task generation.However, CLUTR's approach is still limited in several key respects: First, CLUTR's teacher agent lacks observation of the student's performance history, limiting its ability to adapt to the student's learning progress.Second, because the teacher generates tasks using a stateless, multi-armed bandit algorithm, it lacks the capability to model the task space thoroughly, limiting its ability to generate complex tasks suitable for robotics scenarios.Third, CLUTR operates solely in simulation and does not consider the challenges associated with sim-to-real transfer, particularly the mismatch between simulated and real-world task distributions.</p>
<p>GCL addresses these limitations by introducing a framework that grounds the curriculum in real-world data, ensuring that the learning process is directly applicable to real-world environments.GCL improves upon existing methods in three key respects: First, unlike simple RL test domains like Grid World, Car Racing, or video games [18], a key difference in robotic RL is that robots need to be eventually deployed in the real world after training in simulation.Therefore, GCL grounds the simulated learning process in real-world data, i.e., simulation realism; Second, GCL grounds the task generation on the previous task sequences, i.e., task awareness, and enables the teacher agent to manipulate the task space effectively; Third, GCL grounds the curriculum on monitoring of student performance and allows every task to be catered to the student's latest capabilities (Fig. 1).</p>
<p>III. APPROACH</p>
<p>We introduce GCL, a Dual-Agent framework for adaptive curriculum learning in robotics with limited real-world data.GCL consists of two interacting processes (Fig. 2):</p>
<p>• A Partially Observable Markov Decision Process (POMDP) for the student agent (• S ) learning the task.• A fully informed Markov Decision Process (MDP) for the teacher agent (• T ) generating a curriculum of tasks.</p>
<p>A. Dual-Agent (PO)MDP 1) Student Agent POMDP: The student agent operates in a POMDP defined as a tuple M S = ⟨S S , A S , O S , T S , Ω S , R S , γ S ⟩, where:</p>
<p>• S S is the state space of the robotic task, • A S is the robot action space,</p>
<p>• O S is the robot observation space, • T S : S S ×A S → S S is the POMDP transition function,</p>
<p>• Ω S : S S × A S → O S is the robot observation function, • R S : S S × A S × S S → R is the robot reward function based on task execution performance, and
• γ S ∈ [0, 1] is the student POMDP's discount factor.
The Student agent's goal is to learn a policy π S : O → A that maximizes the expected cumulative reward in the partially observable task environment generated by the teacher.</p>
<p>2) Teacher Agent MDP: In contrast to the student's POMDP, the teacher agent operates in an MDP defined as M T = ⟨S T , A T , T T , R T , γ T ⟩, where:</p>
<p>• S T is the teacher state space, consisting of the comprehensive history of tasks and student performances, • A T is the teacher action space representing all possible tasks that can be assigned to the student,
• T T : S T × A T → S T is the MDP transition function, • R T : S T ×A T ×S T → R is the teacher reward function
based on student performance, and
• γ T ∈ [0, 1] is the discount factor for the teacher's MDP. s T t ∈ S T at time t is defined as s T t = {(a T i , r S i )} t−1 i=0
, where a T i ∈ A T is the i th task assigned by the teacher and r S i is the student's performance (reward) for task i.</p>
<p>B. Grounded Curriculum Learning (GCL)</p>
<p>GCL employs a hierarchical structure where a fullyinformed teacher agent guides the learning process of a student agent, resembling a classroom setting (Fig. 2 top left).In this metaphorical classroom, the teacher (Fig. 2 right) oversees the learning environment (tasks in the curriculum) and monitors the performance of the student and an antagonist agent (Fig. 2 bottom left).This design reflects real-life educational scenarios, where teachers have comprehensive knowledge of both the curriculum and student progress.Leveraging this informed perspective within the classroom and dual-agent framework, GCL comprises five main components that work together to create an effective and adaptive curriculum:</p>
<p>1) Task Representation via Latent Generative Model: We employ a Variational Autoencoder (VAE) to learn a compact latent space Z of robotic tasks, trained on a limited set of real-world tasks T real and therefore grounded in the real world.The VAE consists of an encoder and a decoder, which learn to compress and reconstruct task environments efficiently.This model enables the teacher to generate diverse, realistic tasks by sampling from the learned latent space Z, bridging the gap between limited real-world data and the need for varied training scenarios.By learning a continuous task representation, the VAE allows smooth interpolation between known tasks and the generation of novel, yet realistic, ones for the student agent to learn from.</p>
<p>2) Student and Antagonist Agents: The student agent learns to perform a task using a reinforcement learning algorithm (e.g., PPO [19]) in the partially observable environment generated by the teacher.Its objective is to maximize the expected cumulative reward:
J S (π S θ S ) = E τ S ∼π S θ S T t=0 (γ S ) t r S t ,(1)
where τ S is a trajectory sampled from the student policy π S θ S , parameterized by θ S .To guide curriculum generation and evaluate the student's progress, we introduce an antagonist agent, following the flexible regret setting from PAIRED [7].The antagonist is trained with the same observability and hyperparameters as the student, sharing the same objective function (Eqn.( 1)), with J A and π A θ A as the antagonist's objective and policy respectively.</p>
<p>3) Teacher Agent: The teacher agent enables GCL's two grounding aspects: student performance and task awareness.</p>
<p>To ground in student performance, the teacher maintains a comprehensive tracking of the student's performance across diverse tasks through the student's historical performance, i.e., {r S i )} t−1 i=0 , as part of the teacher state s T t .By incorporating performance history into its state, the teacher can adapt the curriculum based on the student's learning progress.</p>
<p>To ground with task awareness, the teacher maintains a deep understanding of the task space, ensuring continual relevance to past and real-world tasks.This awareness is facilitated by both the historical tasks, i.e., {a T i } t−1 i=0 , in the teacher state s T t , as well as the latent space Z learned through Algorithm 1 Grounded Curriculum Learning (GCL) Collect antagonist trajectory
τ A = {(o A t , a T t A , r A t )} T t=0 in task a T t 10: Compute regret REGRET a T t ← V a T t (π A θ A ) − V a T t (π S θ S ) 11: Update s T t+1 ← s T t ∪ {(a T t , r S a T t )} 12:
π S ← π S + α S ∇ π S J S (π S )</p>
<p>13:
π A ← π A + α A ∇ π A J A (π A )
14: the VAE from real-world tasks.According to the task history, the teacher generates new tasks for the student by sampling from this latent space using the VAE decoder G : Z → A T , which maps latent vectors to concrete tasks.After grounding in terms of student performance and task awareness, the teacher's objective is to maximize the expected cumulative regret:
π T ← π T + α T ∇ π T J T (π T )J T (π T θ T ) = E τ T ∼π T θ T T t=0 (γ T ) t REGRET a T t (π S θ S , π A θ A ) ,
where:
• τ T = (s T 0 , a T 0 , s T 1 , a T 1 , ..., s T t ) is a trajectory in the teacher's MDP, • π T θ T is the teacher policy, parameterized by θ T , • REGRET a T t (π S θ S , π A θ A ) = V a T t (π A θ A ) − V a T t (π S θ S )
is the flexible regret for task a T t , generated by the teacher at time t, and
• V a T t (•)
is the value function (expected discounted return) of a policy when executing task a T t .4) Grounding Simulated Tasks in the Real World: GCL implements a novel mechanism for balancing real-world and simulated tasks.The teacher agent employs a probabilistic approach to task selection, sampling task a T t between generated tasks and real-world tasks:
a T t = sample from T real , with probability ϵ, π T θ T (s T t ), with probability 1 − ϵ,
where ϵ ∈ [0, 1] is a hyperparameter controlling the balance between real and simulated tasks.This approach ensures regular grounding in real-world scenarios while allowing for curriculum adaptation through generated tasks.By adjusting ϵ, we can control the degree of grounding in real-world data, making the framework flexible to different learning scenarios and the availability of real-world data.Algorithm 1 implements the hierarchical structure of GCL, encapsulating its key components.Using a pre-trained VAE [20] (line 3), the fully informed teacher agent alternates between sampling real-world tasks and generating new ones (line 7).</p>
<p>The partially observable student and antagonist agents collect trajectories in the selected task environment a T t (lines 8-9).The teacher's state is updated with each new task-regret pair (lines 10-11), enabling curriculum adaptation based on the full history of tasks and performances.All policies are updated using their respective objective functions (lines [12][13][14].The process continues until convergence.</p>
<p>IV. EXPERIMENTS</p>
<p>We evaluated GCL on The Benchmark for Autonomous Robot Navigation (BARN) Challenge [9], [10], [21]- [24], a standardized testbed for SOTA navigation systems designed to push the boundaries of performance in challenging and highly constrained environments.The objective is to navigate a robot from a predefined start to a goal location as quickly as possible without collisions.Focusing on real-world autonomous navigation, BARN features an environment generator capable of producing a wide range of navigation tasks, from easy open spaces to difficult highly constrained ones.</p>
<p>A. Experimental Setup</p>
<p>We implement The BARN Challenge in NVIDIA's Isaac-Gym simulator [25], utilizing 128 parallel environments.This setup significantly accelerates training and allows efficient exploration of the task space for curriculum learning.Considering the lack of large-scale real-world scenarios, in our experiments, the BARN environments from the generator serve as a surrogate for the real world, where robots will be eventually deployed after training (in contrast to simulated environments created by the teacher agent during training).</p>
<p>1) Student Agent POMDP: M S = ⟨S S , A S , O S , T S , Ω S , R S , γ S ⟩ is implemented as a navigation task in our experiments.S S includes the robot's position and orientation; A S comprises continuous linear and angular velocities; O S includes 270°field-of-view, 720-dimensional LiDAR scans and the relative goal orientation; and R S encourages progress towards the goal while penalizing collisions and excessive time.</p>
<p>2) Teacher Agent MDP:
In M T = ⟨S T , A T , T T , R T , γ T ⟩, S T
consists of the history of tasks and student performances, i.e., s T t = {(a T i , r S i )} t−1 i=0 ∈ S T .For simplicity, we set i = t − 1 in our experiments and leave the study on the effect of history length as future work; A T is the latent space of task representation; and R T is based on the regret between the student and the antagonist.</p>
<p>3) Hyperparameters: Table I summarizes the key hyperparameters used in our experiments.</p>
<p>B. Methods and Evaluation Metrics</p>
<p>We compare GCL against three baseline approaches: Base RL, Manual RL [26], and CLUTR [8].Table II summarizes the key characteristics of GCL and these baseline methods.</p>
<p>For the Manual RL approach, we construct an expert curriculum based on the shortest path length to traverse each environment.This metric serves as our heuristic for task difficulty, allowing us to create a hand-designed curriculum that progressively increases the complexity of navigation tasks.</p>
<p>For evaluation, we employ a comprehensive set of metrics to assess various aspects of navigation performance.These include Task Success rate, which measures the percentage of trials where the robot successfully reaches the goal position without collisions; Navigation Progress, which reflects the average proportion of the path completed before success or failure; average steps taken per successful task; average reward accumulated; and average speed of the robot during tasks.To ensure a fair evaluation, we utilize the BARN environment generator to create separate training (for grounding on simulation realism) and test sets.The environments are split in a 70/30 ratio for training and testing.This arrangement guarantees that the robot has never encountered the evaluation environments during the training phase, allowing us to assess the generalization capabilities of the learned policies.</p>
<p>C. Main Results</p>
<p>Table III presents a comparison of all four methods averaged over three runs.Upward and downward pointing arrows next to each metric indicate whether higher or lower values are better, respectively.GCL achieves the highest Task Success rate (81.85%) and Navigation Progress (68.89%), outperforming all other methods and demonstrating its superior ability to successfully complete navigation tasks and make significant progress even in failed attempts.GCL also accumulates the highest Avg.Reward (19.45).In contrast, Base RL and Manual CL achieve the best Avg.Steps and Avg.Speed, reflecting a more aggressive navigation strategy, but this comes at the cost of increased and earlier failures during task execution.</p>
<p>D. Ablation Studies</p>
<p>We conduct comprehensive ablation studies to analyze the contribution of each key component in GCL:</p>
<p>• GCL: The complete GCL framework.</p>
<p>• GCL w/o real: GCL without real-world data grounding.We remove the real-world task selection, using only tasks generated by the teacher agent:
a T t = π T θ T (s T t ).
• GCL w/o task: GCL without task awareness grounding.We replace the latent task representation a T i with a random vector ξ i in the teacher's state:
s T t = {(ξ i , r S i )} t−1 i=0
, where ξ i ∼ N (0, I), a zero-mean, identity-standard-deviation normal distribution.</p>
<p>• GCL w/o performance: GCL without performance history grounding.We replace the student's reward r S i with a random scalar η i in the teacher's state:
s T t = {(a T i , η i )} t−1 i=0
, where η i ∼ U(0, 1), a uniform distribution between 0 and 1.</p>
<p>Table IV presents the results of our ablation studies.The performance gains of the full GCL over its variants emphasize the critical importance of each component in the framework.Grounding in real-world data is the most crucial element, followed by performance history and task awareness.These components work synergistically to enhance GCL's effectiveness, leveraging the advantages of curriculum learning while maintaining a strong connection to realworld scenarios.These results highlight the importance of a holistic approach that combines real-world relevance with adaptive learning strategies based on task and performance understanding, leading to superior performance in complex navigation tasks.</p>
<p>E. Curriculum Progression</p>
<p>We present and discuss the curriculum progression enabled by different methods to investigate GCL's capability to autonomously create appropriate tasks to facilitate learning.</p>
<p>1) Curriculum Progression based on Heuristic Difficulty: Fig. 3 illustrates the progression of task difficulty during curriculum training.The x-axis represents training steps, while the y-axis depicts the shortest path length, our heuristic metric for task difficulty.Higher values indicate more difficult and complex tasks, typically involving longer and more tortuous paths.It's crucial to acknowledge that this difficulty metric, based on human intuition, may not fully capture the true difficulty experienced by the student agent.</p>
<p>2) Main Method Comparison : The Fig. 3 (left) compares four methods: Baseline RL, Manual CL, CLUTR, and our proposed GCL.Baseline RL maintains a constant difficulty level and does not adjust to the agent's evolving capabilities in complex scenarios.Manual CL, designed by experts, shows structured progression but relies heavily on task-specific knowledge, limiting its applicability in novel or rapidly evolving domains.CLUTR, despite automatic generation, shows limited variation in task difficulty over time, possibly due to insufficient consideration of student performance and task space understanding.In contrast, GCL demonstrates dynamic adaptation of task difficulty throughout training.Its fluctuating curve indicates responsiveness to the agent's current capabilities, autonomously discovering effective learning progressions without relying on task-specific expert knowledge.</p>
<p>3) Ablation Study Comparison: The Fig. 3 (right) presents our ablation study, comparing GCL variants.Although all variants generate a curriculum, the full GCL model shows the most adaptive approach, adjusting task difficulty based on the agent's learning progress.Notably, while GCL w/o real generates a curriculum, its poor performance highlights that curriculum generation alone is insufficient without grounding in real-world data.This finding underscores the critical importance of incorporating real-world information in curriculum design for robotic learning tasks.</p>
<p>4) Environment Visualization: Fig. 4 illustrates GCL's progression from simulation training to real-world deployment.The left side presents environments generated by GCL at four stages of training (25%, 50%, 75%, and 100%), demonstrating the framework's capacity to develop an adaptive curriculum.In the initial stage (25%), GCL generates environments with minimal obstacles, facilitating the acquisition of fundamental navigation skills.As training progresses through 50% and 75%, the environments exhibit increasing complexity, introducing more obstacles and intricate pathways.At the final stage (100%), GCL produces environments that test advanced navigation capabilities.The right side of the figure shows the robot navigating a realworld BARN environment using the trained GCL policy.The robot can maneuver through the environment, skillfully avoiding obstacles and ultimately crossing the blue goal line.</p>
<p>The real-world deployment demonstrates the robot's capacity to apply simulation-learned skills to navigate complex, realworld settings, indicating the practical applicability of the GCL approach.Considering our usage of the simulated BARN environments from the BARN generator as a surrogate for the real world (in contrast to fully synthetic environments produced by the teacher agent), deploying the learned policy in the physical real world introduces another sim-to-real gap (in addition to the gap due to environment distribution), e.g., due to different physics and robot models.So the GCL policy also fails from time to time in the physical real world.</p>
<p>5) Discussions:</p>
<p>The curriculum progression analysis reveals a paradigm shift from rigid, predetermined learning structures to adaptive, autonomous curriculum generation.GCL's performance illustrates the potential of integrating real-world grounding with flexible, agent-responsive learning strategies.Our comparative analysis highlights the balance between structure and flexibility in GCL: While structured approaches provide clear learning paths, they may inadvertently limit the exploration of other potentially more efficient learning strategies; GCL's approach allows for the discovery of unexpected yet effective learning pathways, potentially leading to more robust and generalizable robotic skills.Furthermore, the results emphasize the importance of bridging the gap between simulated and real-world learning in robotics.As we continue to push the boundaries of robotic capabilities, approaches like GCL that can autonomously generate and adapt curricula may become increasingly crucial, offering a paradigm for developing more versatile and efficient robotic systems capable of tackling the complexities of real-world environments.</p>
<p>V. CONCLUSIONS AND FUTURE WORK</p>
<p>This paper introduces Grounded Curriculum Learning (GCL), a novel framework that enhances real-world reinforcement learning in robotics.GCL improves curriculum learning by aligning the simulated task distribution with realworld tasks while considering both task sequences and the robot's past performance.Our experiments on the BARN navigation dataset demonstrate GCL's effectiveness, achieving a 6.8% and 6.5% higher success rate compared to SOTA methods and a manually designed curriculum.GCL's balance between structured and flexible learning ensures that the learned skills are both efficiently acquired and applicable to real-world scenarios.Our ablation studies demonstrate that each component of GCL is important: the curriculum (grounded in task awareness and performance history) guides efficient learning, while real-world task grounding is crucial for maintaining relevance to the target domain.</p>
<p>An interesting direction for future work is to extend GCL to a broader range of robotic tasks beyond navigation [27], such as manipulation [28], [29] and multi-agent systems [30]- [33].Another promising avenue is investigating methods for more effective latent space manipulation by the teacher agent, potentially leading to improved task generation and curriculum design.Additionally, exploring GCL's potential in transfer and lifelong learning scenarios [34] with a pretrained teacher agent can enable robots to adapt more quickly to new tasks or environments.Lastly, instead of end-toend learning [35], [36], more structured learning approaches for robotics, such as learning planner parameters [37]- [42], cost functions [43], kinodynamic models [44]- [48], trajectory generation [49], [50], and local planners [51]- [54] may be able to further improve GCL's efficiency and generalizability.</p>
<p>Fig. 1 :
1
Fig.1: Considering simulation realism, task awareness, and student performance, GCL grounds its curriculum in realworld task distribution and creates an adaptive sequence of BARN navigation tasks with properly increasing difficulty.</p>
<p>Fig. 2 :
2
Fig. 2: Overview of the Dual-Agent GCL Framework (top left): student POMDP (bottom left) and teacher MDP (right).</p>
<p>Fig. 3 :Fig. 4 :
34
Fig. 3: Task Difficulty Adaptation during Curriculum Training for GCL and Comparative Methods.</p>
<p>1 :
1
Input: VAE decoder G, initial parameters θ S , θ A , θ T , learning rates η S , η A , η T , real-world task set Treal, and grounding probability ϵ 2: Output: Trained policies π S θ S , π A θ A , and π T
θ T 3: Pretrain G with available real-world tasks Treal4: Initialize π S θ S , π A θ A , π T θ T , and s T 0 = {} 5: t ← 06: while not converged do7:a T t ←sample from Treal, with probability ϵ, π T θ T (s T t ), with probability 1 − ϵ,8:Collect student trajectory τ S = {(o S t , a T t a T t and compute rewards r S a T tS , r S t )} T t=0 in task9:</p>
<p>TABLE I :
I
Hyperparameters for GCL.
GCL ParameterValueParallel Environments128Latent Task Dimension 32Training Epochs5000RL ParameterTeacher StudentLearning Rate1e-43e-4PPO Epoch105Discount Factor0.990.99</p>
<p>TABLE II :
II
Comparison of Methods Used in the Experiments.
MethodTask Environment CurriculumTeacher AgentBase RLReal-World TasksNoneN/AManual CL Real-World TasksHand-Designed ManualCLUTRSimulatedAutomaticStatelessGCLReal +SimulatedAutomaticFully Informed</p>
<p>TABLE III :
III
Test Performance Comparison of Different Methods across Various Metrics (mean ± std) in The BARN Challenge.
MethodSuccess Rate (%) Performance Gain (%)GCL w/o real76.36+7.19GCL w/o task79.86+2.49GCL w/o performance77.69+5.35GCL81.85-</p>
<p>TABLE IV :
IV
Success Rate Comparison of GCL and Its Ablated Variants in The BARN Challenge: Each variant removes one grounding component, demonstrating the importance of each aspect in the full GCL framework.The Performance Gain column shows the percentage improvement of complete GCL over each variant.</p>
<p>Reinforcement learning in robotics: A survey. J Kober, J A Bagnell, J Peters, The International Journal of Robotics Research. 32112013</p>
<p>Challenges of real-world reinforcement learning. G Dulac-Arnold, D Mankowitz, T Hester, arXiv:1904.129012019arXiv preprint</p>
<p>Sim-to-real transfer in deep reinforcement learning for robotics: a survey. W Zhao, J P Queralta, T Westerlund, 2020 IEEE symposium series on computational intelligence (SSCI). IEEE2020</p>
<p>Reinforcement learning in robotics: Applications and real-world challenges. P Kormushev, S Calinon, D G Caldwell, Robotics. 232013</p>
<p>Learning real-world autonomous navigation by self-supervised environment synthesis. Z Xu, A Nair, X Xiao, P Stone, 2024 IEEE/RSJ international conference on intelligent robots and systems (IROS), First Workshop on Photorealistic Image and Environment Synthesis for Robotics. IEEE2024</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Emergent complexity and zero-shot transfer via unsupervised environment design. M Dennis, N Jaques, R Turner, H Song, J Z Leibo, E Hughes, M Botvinick, arXiv:2012.020962020arXiv preprint</p>
<p>Clutr: Curriculum learning via unsupervised task representation learning. A S Azad, I Gur, J Emhoff, N Alexis, A Faust, P Abbeel, I Stoica, International Conference on Machine Learning. PMLR2023</p>
<p>Benchmarking metric ground navigation. D Perille, A Truong, X Xiao, P Stone, 2020 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR). IEEE2020</p>
<p>Benchmarking reinforcement learning techniques for autonomous navigation. Z Xu, B Liu, X Xiao, A Nair, P Stone, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. S Narvekar, B Peng, M Leonetti, J Sinapov, M E Taylor, P Stone, Journal of Machine Learning Research. 211812020</p>
<p>Automatic curriculum learning through value disagreement. Y Zhang, P Abbeel, L Pinto, Advances in Neural Information Processing Systems. 202033</p>
<p>Evolving curricula with regretbased environment design. J Parker-Holder, M Jiang, M Dennis, M Samvelyan, J Foerster, E Grefenstette, T Rocktäschel, International Conference on Machine Learning. PMLR202217498</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>
<p>Robust adversarial reinforcement learning. L Pinto, J Davidson, R Sukthankar, A Gupta, International conference on machine learning. 2017</p>
<p>Robust multi-agent reinforcement learning via minimax deep deterministic policy gradient. S Li, Y Wu, X Cui, H Dong, F Fang, S Russell, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201933</p>
<p>Auto-encoding variational bayes. D P Kingma, M Welling, arXiv:1312.61142013arXiv preprint</p>
<p>A survey of deep reinforcement learning in video games. K Shao, Z Tang, Y Zhu, N Li, D Zhao, arXiv:1912.109442019arXiv preprint</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>On the use of deep autoencoders for efficient embedded reinforcement learning. B Prakash, M Horton, N R Waytowich, W D Hairston, T Oates, T Mohsenin, Proceedings of the 2019 on Great Lakes Symposium on VLSI. the 2019 on Great Lakes Symposium on VLSI2019</p>
<p>Machine learning methods for local motion planning: A study of end-to-end vs. parameter learning. Z Xu, X Xiao, G Warnell, A Nair, P Stone, 2021 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR). IEEE2021</p>
<p>Autonomous ground navigation in highly constrained spaces: Lessons learned from the benchmark autonomous robot navigation challenge at icra 2022 [competitions]. X Xiao, Z Xu, Z Wang, Y Song, G Warnell, P Stone, T Zhang, S Ravi, G Wang, H Karnan, IEEE Robotics &amp; Automation Magazine. 2942022</p>
<p>Autonomous ground navigation in highly constrained spaces: Lessons learned from the second barn challenge at icra 2023 [competitions]. X Xiao, Z Xu, G Warnell, P Stone, F G Guinjoan, R T Rodrigues, H Bruyninckx, H Mandala, G Christmann, J L Blanco-Claraco, IEEE Robotics &amp; Automation Magazine. 3042023</p>
<p>Autonomous ground navigation in highly constrained spaces: Lessons learned from the third barn challenge at icra 2024 [competitions]. X Xiao, Z Xu, A Datar, G Warnell, P Stone, J J Damanik, J Jung, C A Deresa, T D Huy, C Jinyu, IEEE Robotics &amp; Automation Magazine. 3132024</p>
<p>V Makoviychuk, L Wawrzyniak, Y Guo, M Lu, K Storey, M Macklin, D Hoeller, N Rudin, A Allshire, A Handa, arXiv:2108.10470Isaac gym: High performance gpu-based physics simulation for robot learning. 2021arXiv preprint</p>
<p>Reinforcement learning for wheeled mobility on vertically challenging terrain. T Xu, C Pan, X Xiao, arXiv:2409.023832024arXiv preprint</p>
<p>Motion planning and control for mobile robot navigation using machine learning: a survey. X Xiao, B Liu, G Warnell, P Stone, Autonomous Robots. 4652022</p>
<p>Composable deep reinforcement learning for robotic manipulation. T Haarnoja, V Pong, A Zhou, M Dalal, P Abbeel, S Levine, 2018 IEEE international conference on robotics and automation (ICRA). IEEE2018</p>
<p>Scalable deep reinforcement learning for vision-based robotic manipulation. D Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog, E Jang, D Quillen, E Holly, M Kalakrishnan, V Vanhoucke, Conference on robot learning. PMLR2018</p>
<p>Team coordination on graphs with state-dependent edge costs. M Limbu, Z Hu, S Oughourli, X Wang, X Xiao, D Shishika, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023</p>
<p>Scaling team coordination on graphs with reinforcement learning. M Limbu, Z Hu, X Wang, D Shishika, X Xiao, 2024 IEEE International Conference on Robotics and Automation (ICRA). 2024544</p>
<p>Team coordination on graphs: Problem, analysis, and algorithms. Y Zhou, M Limbu, G J Stein, X Wang, D Shishika, X Xiao, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2024</p>
<p>Team orienteering coverage planning with uncertain reward. B Liu, X Xiao, P Stone, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021</p>
<p>A lifelong learning approach to mobile robot navigation. IEEE Robotics and Automation Letters. 622021</p>
<p>From perception to decision: A data-driven approach to end-to-end motion planning for autonomous ground robots. M Pfeiffer, M Schaeuble, J Nieto, R Siegwart, C Cadena, 2017IEEE</p>
<p>Toward wheeled mobility on vertically challenging terrain: Platforms, datasets, and algorithms. A Datar, C Pan, M Nazeri, X Xiao, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE202416329</p>
<p>Appl: Adaptive planner parameter learning. X Xiao, Z Wang, Z Xu, B Liu, G Warnell, G Dhamankar, A Nair, P Stone, Robotics and Autonomous Systems. 1541041322022</p>
<p>Appld: Adaptive planner parameter learning from demonstration. X Xiao, B Liu, G Warnell, J Fink, P Stone, IEEE Robotics and Automation Letters. 532020</p>
<p>Appli: Adaptive planner parameter learning from interventions. Z Wang, X Xiao, B Liu, G Warnell, P Stone, 2021 IEEE international conference on robotics and automation (ICRA). IEEE2021</p>
<p>Apple: Adaptive planner parameter learning from evaluative feedback. Z Wang, X Xiao, G Warnell, P Stone, IEEE Robotics and Automation Letters. 642021</p>
<p>Applr: Adaptive planner parameter learning from reinforcement. Z Xu, G Dhamankar, A Nair, X Xiao, G Warnell, B Liu, Z Wang, P Stone, 2021 IEEE international conference on robotics and automation (ICRA). IEEE2021</p>
<p>Motion memory: Leveraging past experiences to accelerate future motion planning. D Das, Y Lu, E Plaku, X Xiao, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE202416474</p>
<p>Learning model predictive controllers with real-time attention for real-world navigation. X Xiao, T Zhang, K M Choromanski, T.-W E Lee, A Francis, J Varley, S Tu, S Singh, P Xu, F Xia, S M Persson, L Takayama, R Frostig, J Tan, C Parada, V Sindhwani, 2022in Conference on robot learning. PMLR</p>
<p>Learning inverse kinodynamics for accurate high-speed off-road navigation on unstructured terrain. X Xiao, J Biswas, P Stone, IEEE Robotics and Automation Letters. 632021</p>
<p>Vi-ikd: High-speed accurate off-road navigation using learned visual-inertial inverse kinodynamics. H Karnan, K S Sikand, P Atreya, S Rabiee, X Xiao, G Warnell, P Stone, J Biswas, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2022</p>
<p>High-speed accurate robot control using learned forward kinodynamics and non-linear least squares optimization. P Atreya, H Karnan, K S Sikand, X Xiao, S Rabiee, J Biswas, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). </p>
<p>Terrainattentive learning for efficient 6-dof kinodynamic modeling on vertically challenging terrain. A Datar, C Pan, M Nazeri, A Pokhrel, X Xiao, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2024</p>
<p>CAHSOR: Competence-aware high-speed off-road ground navigation in SE (3). A Pokhrel, A Datar, M Nazeri, X Xiao, IEEE Robotics and Automation Letters. 2024</p>
<p>Mtg: Mapless trajectory generator with traversability coverage for outdoor navigation. J Liang, P Gao, X Xiao, A J Sathyamoorthy, M Elnoor, M C Lin, D Manocha, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Dtg: Diffusion-based trajectory generation for mapless global navigation. J Liang, A Payandeh, D Song, X Xiao, D Manocha, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2024</p>
<p>Toward agile maneuvers in highly constrained spaces: Learning from hallucination. X Xiao, B Liu, G Warnell, P Stone, IEEE Robotics and Automation Letters. 622021</p>
<p>Agile robot navigation through hallucinated learning and sober deployment. X Xiao, B Liu, P Stone, 2021 IEEE international conference on robotics and automation (ICRA). IEEE2021</p>
<p>From agile ground to aerial navigation: Learning from learned hallucination. Z Wang, X Xiao, A J Nettekoven, K Umasankar, A Singh, S Bommakanti, U Topcu, P Stone, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). </p>
<p>Rethinking social robot navigation: Leveraging the best of two worlds. A H Raj, Z Hu, H Karnan, R Chandra, A Payandeh, L Mao, P Stone, J Biswas, X Xiao, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>            </div>
        </div>

    </div>
</body>
</html>
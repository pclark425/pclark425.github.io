<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6028 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6028</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6028</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-268384892</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.09567v2.pdf" target="_blank">Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> The deployment of autonomous agents in environments involving human interaction has increasingly raised security concerns. Consequently, understanding the circumstances behind an event becomes critical, requiring the development of capabilities to justify their behaviors to non-expert users. Such explanations are essential in enhancing trustworthiness and safety, acting as a preventive measure against failures, errors, and misunderstandings. Additionally, they contribute to improving communication, bridging the gap between the agent and the user, thereby improving the effectiveness oftheirinteractions.Thisworkpresentsanaccountabilityandexplainabilityarchitectureimplemented for ROS-based mobile robots. The proposed solution consists of two main components. Firstly, a blackbox-likeelementtoprovideaccountability,featuringanti-tamperingpropertiesachievedthroughblockchaintechnology.Secondly,acomponentinchargeofgeneratingnaturallanguageexplanations byharnessingthecapabilitiesofLargeLanguageModels(LLMs)overthedatacontainedwithinthepreviouslymentionedblackbox.Thestudyevaluatestheperformanceofoursolutioninthreedifferent scenarios,eachinvolvingautonomousagentnavigationfunctionalities.Thisevaluationincludesathoroughexaminationofaccountabilityandexplainabilitymetrics,demonstratingtheeffectiveness ofourapproachinusingaccountabledatafromrobotactionstoobtaincoherent,accurateandunderstandableexplanations,evenwhenfacingchallengesinherentintheuseofautonomousagents inreal-worldscenarios.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6028.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6028.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-vs-Human QA evaluation (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-judge and human evaluations for RAG question-answering over robotic Rosbag data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper compares GPT-4 acting as an automated judge against human raters when evaluating LLM-generated explanations (RAG answers) about mobile-robot navigation events recorded in Rosbag files; it reports alignment statistics, observed divergences, failure cases, and recommended mitigations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Retrieval-Augmented Generation question-answering / explainability for robotics (interpretation of ROS 2 Rosbag navigation logs)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Online study with 17 participants who each rated 48 explanations (16 per scenario) via three scenario-specific questionnaires; participants used a 5-point Likert scale (1-5) based on the Explanation Satisfaction Scale (metrics: Understandability, Satisfaction, Informativeness, Completeness, Usefulness). Demographics and expertise reported; inter-rater reliability measured (Cronbach's alpha).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Correctness (binary/retriever correctness score against annotated ground truth), and criteria-based metrics (Accuracy, Helpfulness, Conciseness, Coherence) from the LLM-as-judge; human metrics: Understandability, Satisfaction, Informativeness, Completeness, Usefulness (Likert). Agreement patterns (heatmaps) and correlation analyses (Pearson) and inferential statistics (Friedman + Nemenyi) were also used to compare judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Overall trends: GPT-4 and human raters produced highly similar patterns ‚Äî both identify high-quality and low-quality explanations (heatmaps show similar clusters). Quantitatively, the paper reports high alignment in aggregate (visual/qualitative agreement), but small systematic differences in exact numeric ratings and some outliers: GPT-4 sometimes scores differently on particular explanations (slightly higher/lower than humans). Specific divergences appear for broad/ambiguous questions and scenario-dependent cases (e.g., Scenario 3 with goal cancellation), where GPT-4's correctness on some questions was lower than humans or vice versa. The authors note that LLM judgments are scalable and consistent but miss some human subtleties.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Reported limitations include: model indeterminism (same prompt may yield different judgments across runs); sensitivity to prompt phrasing; inability to consistently capture subtle subjective/contextual cues humans use; susceptibility to retrieval errors (wrong chunks leading to incorrect judgment); difficulty with broad or ambiguous questions (LLM may prioritize different types of content‚ÄîBT info, timestamps, locations‚Äîleading to mismatch with annotated ground truth); specific difficulty reasoning about complex, dynamic Nav2 Behavior Tree (BT) recovery sequences (e.g., failing to determine whether a node recovered or finished). The paper also references literature noting bias and fairness concerns of LLM evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>1) Broad question 'What has happened...' in Scenario 3: GPT-4 correctness initially <65% because the canceled goal and recovery BT nodes caused varied answers; LLM sometimes focused on different aspects and diverged from the ground truth. 2) Scenario 3: confusion about whether a navigation ended in abortion/cancellation (LLM sometimes failed to identify aborted goals). 3) Retrieval misalignment: LLM failed to identify initial robot position or proper goal sequence when the retriever returned wrong chunks or many similar embeddings. 4) Specific evaluated explanations (noted as Explanation 7 and Explanation 16) were flagged by humans as irrelevant or incomplete while the LLM's scoring pattern differed; some LLM judgments missed subtle human-perceived incompleteness or irrelevance. 5) High-frequency topics: certain failures correlated with noisy or large message chunks (e.g., /camera/image_raw) increasing hash computation cost and indirectly affecting logged context for RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>The paper proposes and/or uses several mitigations: (a) Hybrid evaluation: use a strong LLM (GPT-4) for large-scale, consistent initial evaluation, and validate a representative subset with human raters to catch nuanced biases or misalignments. (b) Prompt engineering: adding targeted prompt extensions (e.g., instructing to 'Describe the sequence of events regarding navigation goals and include information about each of them') increased correctness for ambiguous broad questions (raised correctness from below 65% to ~75+%). (c) Improve retrieval: reduce retrieval misalignment via separate vector collections, hybrid approaches (knowledge graphs + vector indexes), better chunking, and updated embeddings; asynchronous embedding updates to capture new accountable data. (d) Model adaptation: fine-tune or apply in-context learning for navigation-specific tasks to handle BT recovery and scenario-specific patterns. (e) Human-in-the-loop: routinely test subsets of outputs with human evaluators and employ statistical post-hoc analyses (e.g., Nemenyi) to identify variable items. (f) Use reliability and correlation analyses (Cronbach's alpha, Pearson) to monitor human evaluator consistency and to calibrate LLM judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>The paper emphasizes the complementarity: LLM-as-a-judge offers scalability and explainable outputs (rationales), while humans supply nuanced subjective judgments; therefore, the recommended evaluation pipeline is hybrid (LLM for scale, humans for spot-checks and nuanced cases). The paper also documents that prompt engineering materially improved LLM alignment in ambiguous queries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large Language Models Are State-of-the-Art Evaluators of Translation Quality <em>(Rating: 2)</em></li>
                <li>Evaluating Explanations Through LLMs: Beyond Traditional User Studies <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 1)</em></li>
                <li>Language Models as Evaluators : A Novel Framework for Automatic Evaluation of News Article Summaries <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6028",
    "paper_id": "paper-268384892",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "LLM-vs-Human QA evaluation (this paper)",
            "name_full": "Comparison of LLM-as-a-judge and human evaluations for RAG question-answering over robotic Rosbag data",
            "brief_description": "This paper compares GPT-4 acting as an automated judge against human raters when evaluating LLM-generated explanations (RAG answers) about mobile-robot navigation events recorded in Rosbag files; it reports alignment statistics, observed divergences, failure cases, and recommended mitigations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Retrieval-Augmented Generation question-answering / explainability for robotics (interpretation of ROS 2 Rosbag navigation logs)",
            "llm_judge_model": "GPT-4",
            "human_evaluation_setup": "Online study with 17 participants who each rated 48 explanations (16 per scenario) via three scenario-specific questionnaires; participants used a 5-point Likert scale (1-5) based on the Explanation Satisfaction Scale (metrics: Understandability, Satisfaction, Informativeness, Completeness, Usefulness). Demographics and expertise reported; inter-rater reliability measured (Cronbach's alpha).",
            "metrics_compared": "Correctness (binary/retriever correctness score against annotated ground truth), and criteria-based metrics (Accuracy, Helpfulness, Conciseness, Coherence) from the LLM-as-judge; human metrics: Understandability, Satisfaction, Informativeness, Completeness, Usefulness (Likert). Agreement patterns (heatmaps) and correlation analyses (Pearson) and inferential statistics (Friedman + Nemenyi) were also used to compare judgments.",
            "reported_differences": "Overall trends: GPT-4 and human raters produced highly similar patterns ‚Äî both identify high-quality and low-quality explanations (heatmaps show similar clusters). Quantitatively, the paper reports high alignment in aggregate (visual/qualitative agreement), but small systematic differences in exact numeric ratings and some outliers: GPT-4 sometimes scores differently on particular explanations (slightly higher/lower than humans). Specific divergences appear for broad/ambiguous questions and scenario-dependent cases (e.g., Scenario 3 with goal cancellation), where GPT-4's correctness on some questions was lower than humans or vice versa. The authors note that LLM judgments are scalable and consistent but miss some human subtleties.",
            "llm_specific_limitations": "Reported limitations include: model indeterminism (same prompt may yield different judgments across runs); sensitivity to prompt phrasing; inability to consistently capture subtle subjective/contextual cues humans use; susceptibility to retrieval errors (wrong chunks leading to incorrect judgment); difficulty with broad or ambiguous questions (LLM may prioritize different types of content‚ÄîBT info, timestamps, locations‚Äîleading to mismatch with annotated ground truth); specific difficulty reasoning about complex, dynamic Nav2 Behavior Tree (BT) recovery sequences (e.g., failing to determine whether a node recovered or finished). The paper also references literature noting bias and fairness concerns of LLM evaluators.",
            "notable_failure_cases": "1) Broad question 'What has happened...' in Scenario 3: GPT-4 correctness initially &lt;65% because the canceled goal and recovery BT nodes caused varied answers; LLM sometimes focused on different aspects and diverged from the ground truth. 2) Scenario 3: confusion about whether a navigation ended in abortion/cancellation (LLM sometimes failed to identify aborted goals). 3) Retrieval misalignment: LLM failed to identify initial robot position or proper goal sequence when the retriever returned wrong chunks or many similar embeddings. 4) Specific evaluated explanations (noted as Explanation 7 and Explanation 16) were flagged by humans as irrelevant or incomplete while the LLM's scoring pattern differed; some LLM judgments missed subtle human-perceived incompleteness or irrelevance. 5) High-frequency topics: certain failures correlated with noisy or large message chunks (e.g., /camera/image_raw) increasing hash computation cost and indirectly affecting logged context for RAG.",
            "mitigation_strategies": "The paper proposes and/or uses several mitigations: (a) Hybrid evaluation: use a strong LLM (GPT-4) for large-scale, consistent initial evaluation, and validate a representative subset with human raters to catch nuanced biases or misalignments. (b) Prompt engineering: adding targeted prompt extensions (e.g., instructing to 'Describe the sequence of events regarding navigation goals and include information about each of them') increased correctness for ambiguous broad questions (raised correctness from below 65% to ~75+%). (c) Improve retrieval: reduce retrieval misalignment via separate vector collections, hybrid approaches (knowledge graphs + vector indexes), better chunking, and updated embeddings; asynchronous embedding updates to capture new accountable data. (d) Model adaptation: fine-tune or apply in-context learning for navigation-specific tasks to handle BT recovery and scenario-specific patterns. (e) Human-in-the-loop: routinely test subsets of outputs with human evaluators and employ statistical post-hoc analyses (e.g., Nemenyi) to identify variable items. (f) Use reliability and correlation analyses (Cronbach's alpha, Pearson) to monitor human evaluator consistency and to calibrate LLM judgments.",
            "additional_notes": "The paper emphasizes the complementarity: LLM-as-a-judge offers scalability and explainable outputs (rationales), while humans supply nuanced subjective judgments; therefore, the recommended evaluation pipeline is hybrid (LLM for scale, humans for spot-checks and nuanced cases). The paper also documents that prompt engineering materially improved LLM alignment in ambiguous queries.",
            "uuid": "e6028.0",
            "source_info": {
                "paper_title": "Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality",
            "rating": 2,
            "sanitized_title": "large_language_models_are_stateoftheart_evaluators_of_translation_quality"
        },
        {
            "paper_title": "Evaluating Explanations Through LLMs: Beyond Traditional User Studies",
            "rating": 2,
            "sanitized_title": "evaluating_explanations_through_llms_beyond_traditional_user_studies"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 1,
            "sanitized_title": "large_language_models_are_not_fair_evaluators"
        },
        {
            "paper_title": "Language Models as Evaluators : A Novel Framework for Automatic Evaluation of News Article Summaries",
            "rating": 1,
            "sanitized_title": "language_models_as_evaluators_a_novel_framework_for_automatic_evaluation_of_news_article_summaries"
        }
    ],
    "cost": 0.012085249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models
19 Dec 2024</p>
<p>Laura Fern√°ndez-Becerra 
Robotics Group
University of Le√≥n
Campus de Vegazana s/n24071Le√≥nSpain</p>
<p>Robotics Group
University of Le√≥n
Campus de Vegazana s/n24071Le√≥nSpain</p>
<p>Miguel Angel Gonz√°lez-Santamarta 
Robotics Group
University of Le√≥n
Campus de Vegazana s/n24071Le√≥nSpain</p>
<p>Robotics Group
University of Le√≥n
Campus de Vegazana s/n24071Le√≥nSpain</p>
<p>Angel Manuel 
Robotics Group
University of Le√≥n
Campus de Vegazana s/n24071Le√≥nSpain</p>
<p>Francisco Javier Rodr√≠guez- Lera 
Robotics Group
University of Le√≥n
Campus de Vegazana s/n24071Le√≥nSpain</p>
<p>Vicente Matell√°n Olivera 
Robotics Group
University of Le√≥n
Campus de Vegazana s/n24071Le√≥nSpain</p>
<p>Robotics Group
University of Le√≥n
Campus de Vegazana s/n24071Le√≥nSpain</p>
<p>Angel Manuel Guerrero-Higueras 
Robotics Group
University of Le√≥n
Campus de Vegazana s/n24071Le√≥nSpain</p>
<p>Francisco Javier Rodr√≠guez-Lera 
Robotics Group
University of Le√≥n
Campus de Vegazana s/n24071Le√≥nSpain</p>
<p>Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models
19 Dec 20240B56E1B3DFE2F59B539A88BD86F1B983arXiv:2403.09567v3[cs.RO]Preprint submitted to Elsevier L. Fern√°ndez-Becerra et al.: Preprint submitted to ElsevierRobotics Accountability Explainability Blockchain Large Language Models
Security improvements for autonomous agents in public spaces.‚Ä¢ Review of blockchain for immutable logs, EDRs, XAI and blockchain, XARs, and LLMs.‚Ä¢ Accountability and Explainability architecture for autonomous agents.‚Ä¢ Comprehensive assessment across diverse navigation scenarios and metrics.‚Ä¢ Future research directions for accountable and explainable autonomous agents.</p>
<p>Introduction</p>
<p>The rising presence of mobile robots alongside humans has increased concerns about security and trust.One of the main challenges in this context is the ability of robots to justify their actions and decisions, particularly in safety-critical scenarios where their behavior may impact human wellbeing [1].Transparency and reliability in these justifications are essential [2], as such properties enable effective humanrobot interaction and foster confidence, particularly for nonexpert users who require clear and understandable explanations [3,4].However, the complexity of robotic decisionmaking and the need for thorough post-event analysis make achieving these goals particularly difficult.</p>
<p>To address these challenges, accountability must be ensured through a robust, tamper-proof logging system.The most substantial evidence for post-event analysis comes from logging and auditing capabilities, which are essential to mitigate technical issues and provide forensic evidence to detect data tampering or unauthorized breaches.These systems must guarantee the integrity, confidentiality, and availability of data while enabling incident detection, compliance, and forensic investigations [5].</p>
<p>However, covering tracks in log systems has become a common practice for attackers, erasing any traces that could lead to their identification or assisting in the analysis of their actions.As a result, it is necessary to strengthen system properties against unauthorized modifications [6].This includes safeguarding the integrity of log data and implementing measures to detect and counteract attempts at tampering.Effective accountability solutions must ensure the detection and isolation of faulty behaviors and their origin.Moreover, they should guarantee search accessibility, enabling access to specific records whose integrity can be verified [7].</p>
<p>Integrating accountability solutions into robotic systems often demands a trade-off with system performance.Specifically, computational power and networking bandwidth may be adversely affected [8].The inclusion of anti-tampering techniques, such as full disk and communication encryption, can be restrictive due to resource limitations [9].Additionally, high-cost tamper-proof storage devices may prove economically unviable, especially when dealing with continuously generated log data.</p>
<p>Therefore, it is necessary to establish an efficient and verifiable proof of integrity to guarantee the reproducibility and identification of the events that could arise during an autonomous agent action without compromising the robot's performance.For this evidence to be significant, it should belong to a persistent chain of custody, fully reliable and verifiable, key requirements for an auditable and transparent logging system [10].</p>
<p>To this end, blockchain solutions emerge as a mechanism to ensure data integrity, protection against unauthorized changes, and traceability.The consensus protocols established by all blockchain network members enable the detection and rejection of any destruction or manipulation of data through illicit channels.The nature of this technology mitigates tampering risk, safeguarding data [11].</p>
<p>Nevertheless, while blockchain secures log data, the evidence obtained poses significant challenges for analysis due to its diverse and varied nature, terminology, and the presence of substantial volumes of similar messages unrelated to anomalous events.These factors may result in limited usefulness and misalignment with the expectations of non-expert users [12].The raw log messages produced by autonomous agents are usually noisy and semi-structured in format.In addition to this, it is necessary to deal with specific features such as large quantities of domain-specific language messages [13].Addressing this issue requires methods to organize and filter log data, making it accessible and meaningful.</p>
<p>In this context, explaining the reasons behind a robot's specific and unexpected actions is essential for diagnosing failures and comprehending deviations from intended goals.Equally important is ensuring these explanations are understandable to users [14].Explainable Artificial Intelligence (XAI) is a research area dedicated to enhancing the transparency and interpretability of Artificial Intelligence (AI) systems to ensure their responsible and ethical deployment.As autonomous robots grow in complexity, understanding and predicting their behavior has become increasingly challenging for non-expert users.This requirement has led to the development of eXplainable Autonomous Robots (XAR), which aims to provide mechanisms for presenting clear and understandable explanations for the actions and decisions of autonomous agents [15].XAR enhances the transparency and interpretability of these agents' decision-making processes, often incorporating techniques like natural language explanations to improve clarity and user comprehension [16,17].</p>
<p>To further improve the effectiveness of XAR systems, Large Language Models (LLMs) excel in diverse natural language processing tasks, including translation, coding, or text summarization [18,19,20].Our approach integrates LLMs with blockchain-secured log data.By using retrieval-based techniques, this solution generates accurate and contextaware natural language explanations based on verifiable information [21].This integration ensures that explanations are clear, coherent, and grounded in a traceable and reliable context, effectively addressing user comprehension and data integrity concerns.</p>
<p>This work presents an accountability and explainability architecture implemented for Robot Operating System (ROS)-based mobile robots.This solution includes two main components.First, a black box-like component to provide accountability that includes anti-tampering features by distributing integrity proofs from the events recorded by the black box component.The second component generates natural language explanations from data in the black box, including the use of interactive natural language conversations as a means to provide comprehensive explainability for the recorded events.Both components are connected through an additional module responsible for processing raw data collected by the first one, eliminating non-essential information, and building connections and relations between data.These functionalities release software developers from the need to make calls to the logging Application Programming Interface (API) to build explanations for the code generated and the behaviors developed by the autonomous agent.To validate our approach, we conducted a comprehensive evaluation, assessing the architecture's impact on system performance and analyzing the quality of the generated explanations.The evaluation considered key metrics, including correctness, accuracy, helpfulness, coherence, and user satisfaction.Our results demonstrate that the architecture effectively balances accountability and explainability without compromising the robot's performance, providing a scalable and reliable framework for real-world applications.</p>
<p>The remainder of this paper is organized as follows: Section 2 provides an overview of previous works and findings related to the study, focusing on the enhancement of immutable data recorders and explainability solutions for robots.Section 3 delves into the system architecture, offering a detailed description of the designed approaches.Section 4 details the technical elements of the designed system attending to the proposed architecture.Section 5 discusses the experimental process, the scenarios and the variables employed for assessment.Section 6 outlines the obtained results, particularly addressing accountability and explainability metrics.Finally, Section 7 summarises the obtained conclusions.</p>
<p>Related Works</p>
<p>In this section, we explore key works related to various domains relevant to our approach.This includes advances in how blockchain ensures the immutability of logs, research on applying Event Data Recorders (EDRs) in robotics, significant contributions to integrating XAI with blockchain, and XAR.Finally, we examine the evolution of LLMs for generating natural language explanations and their advancements within robotics.</p>
<p>Blockchain for Immutable Logs</p>
<p>Blockchain is a decentralized solution that eliminates the necessity for trust in a central authority through its immutable and distributed ledger, composed of timestamped transaction blocks.These blocks are linked through hashing and the storage of previous block content, ensuring the integrity of transactions.Any attempt to alter a transaction in one block requires the modification of all subsequent blocks, resulting in a computationally expensive process.As a result, data stored in the blockchain is immutable [22,23].</p>
<p>There are two primary types of blockchains based on access controls: permissionless blockchains allow open access and public transaction visibility, while permissioned blockchains limit participation and transaction visibility.Nevertheless, closed networks may pose security risks, as authorized users could recalculate subsequent blocks in the event of a block modification [24].While permissionless blockchains have traditionally related to scalability and throughput challenges, approaches like Ethereum have transitioned to Proof of Stake (PoS), a more energy-efficient, secure, accessible, and scalable consensus protocol [25,26].</p>
<p>Blockchain technology ensures log integrity and auditability, offering unique advantages over alternatives such as distributed storage systems or InterPlanetary File Systems (IPFSs), which may require central authorities for storage and verification [27,28].Blockchain offers consensus and immutability, coupled with the capability of integrating logic mechanisms such as Smart Contracts (SCs).SCs contain functions that either write values onto the blockchain, thus altering its state, or read data from it [29].These features make blockchain the preferred choice for applications where data integrity and trust are required [30].</p>
<p>Different secure logging systems rely on permissioned blockchain frameworks for log immutability [31], introducing accountability even in embedded systems [7].To get a balance between throughput and cost-effectiveness, some works combine both categories [32], developing approaches that include a permissioned blockchain to ensure per-entry immutability and public non-repudiation solution [33].Nevertheless, inherent features of permissionless blockchain such as transparency and trust, have also boosted the development of secure logging systems [34].</p>
<p>Our architecture uses the immutability of blockchain to store evidence of the integrity of recorded data, thus providing real-time guarantees against manipulation and ensuring that the explanations generated by our system are always based on non-manipulated data.</p>
<p>Blockchain for Event Data Recorders in Robotics</p>
<p>EDRs, or black boxes, are essential for capturing and analyzing events in robotic systems.These recorders help to comprehend system behavior, detect anomalies, and ensure safety [35,36].Therefore, these solutions are needed in forensic investigations and ethical assessments regarding autonomous agents [37].The integration of black box modules to enhance robots' introspection capabilities contributes to improving system transparency by attributing responsibilities [38,9].These solutions typically include functionalities such as hashing, signing, and encryption [39], while ensuring event integrity through anti-tamper mechanisms [40].</p>
<p>Using blockchain technology in EDRs ensures the integrity of a transparent and auditable record of events.These features enhance accountability and trust in the recorded data by preventing tampering [41].Decentralized and distributed storage further eliminates reliance on central authorities and mitigates data loss risks.</p>
<p>Further research explores the application of blockchain technology to improve robotic capabilities, particularly event data recording and management.The Black Block Recorder (BBR) system uses permissioned blockchain for immutable logging, combining Rosbag2 and Distributed Ledger Technologies (DLTs) to ensure data integrity [42].Blockchain applications in robot swarms, including secure communication, data logging, and consensus agreements, are discussed in [43].In [44], a time-segmented consortium blockchain for robotic event registration is proposed, enabling efficient event organization and retrieval within specific time frames.Authors in [45] present a tool to bridge ROS and the Ethereum blockchain, evaluating encryption efficiency and stability, transaction response time, and packet loss rate.Nevertheless, these works lack the capability to save integrity proofs in the blockchain at selected intervals, enhance granularity, or include service definitions or SCs to improve the verifiability process.</p>
<p>Explainable Artificial Intelligence and Blockchain</p>
<p>XAI aims to align AI decisions with human cognitive expectations by employing techniques such as contextual relationships, contrastive explanations, and methods like counterfactual reasoning and interpretable models that enhance interpretability and transparency [46,47].Insights from social sciences show people tend to interpret explanations more positively when they reflect human-like reasoning, emphasizing intentions, goals, and contextual factors rather than purely technical or statistical descriptions [48].By combining XAI with blockchain transparent record-keeping, explanations can be retraced and validated, ensuring both interpretability and accountability in AI-driven systems [49,50].</p>
<p>The benefits of this integration are remarkable in various domains.In healthcare, XAI and blockchain have been combined for secure, transparent patient data management [51], logical disease prediction, and immutable medical diagnostics records [52].Similarly, in the legal domain applications, blockchain and XAI support tamper-proof decision auditing, using IPFS and Ethereum for cryptographic proofs that enable data verification and accountability [53,54,28].Privacy-preserving systems with homomorphic encryption and semantic ontologies further ensure data protection while maintaining explainability [55].</p>
<p>Regarding autonomous agents like unmanned aerial vehicles or autonomous vehicles, blockchain and XAI ensure secure and transparent decision-making [56].This integration enhances collaboration, even in complex and timecritical scenarios [57].Decentralized edge models for collaborative AI, leveraging blockchain for secure communication and real-time data sharing, further optimize resources and enhance reliability in these environments [58].</p>
<p>Our work introduces Rosbag2 files as an alternative to IPFS or distributed file systems for storing contextual data in autonomous robotic systems.This approach supports reproducibility and enables a more precise analysis of robotic behaviors [42].Rosbag files provide a standardized format for recording large volumes of robotic data, facilitating comprehensive interaction capture for improved analysis.To address the indeterminism of LLM algorithms, we propose using a permissionless blockchain to store integrity proofs of Rosbag messages, ensuring immutability, simplifying verification, and enhancing data reliability for explanation generation.</p>
<p>Explainable Autonomous Robots</p>
<p>XARs are designed to provide explanations for their actions and decisions to non-expert users.Such explanations enhance security and trust, helping to prevent failures, errors, conflicts, and undesired interactions while achieving objectives [59,60].However, despite the growing demand for explainability in autonomous agents, their use reveals a lack of practical usability and effective interpretability in real environments [61].</p>
<p>Effective explanations in XARs should detail alternatives, the choices made, expectations for each option, decision criteria, and any unexpected events that may have led to changes in the planned behavior [17,62].These explanations often rely on abductive reasoning, which bridges the gap between the robot's internal decision-making process and the user's understanding [63].To support this, frameworks have been developed that enable robots to articulate their internal states and decisions, enhancing collaboration and improving communication with humans [64].</p>
<p>Adaptive algorithms and needs-based motivational systems allow robots to provide real-time, context-aware explanations tailored to diverse audiences [65,66].Behavior Trees (BTs) hierarchically decompose tasks into goals and subgoals, making complex behaviors easier to understand and adaptable to real-time changes [67].However, achieving a balance between simplicity and adequacy is essential when making complex information easy to understand.By focusing on the most relevant details and filtering nonessential information, explanations foster trust and collaboration by aligning with human cognitive capacities [68].For this purpose, our architecture incorporates a log curation module that processes raw robotic data, filtering it to retain only meaningful, high-relevance information, thus delivering clearer and more concise explanations directly tied to events.</p>
<p>In addition to context-aware explanations, the core of explainability in XAR lies in effective communication [63].Progress in Natural Language Processing (NLP) and LLMs can improve the efficiency of human-robot interactions.This enhancement can create a sense that these systems truly comprehend and respond to individuals meaningfully, significantly improving their usefulness and applicability [1].</p>
<p>Large Language Models in Robotics</p>
<p>LLMs, enabled by self-attention mechanisms, have advanced NLP, improving task-solving capabilities and enabling more effective human-robot interaction [69,70,71].However, LLMs face challenges such as hallucinations, which arise due to reliance on internal knowledge, and the high update costs associated with large parameter sizes, making traditional pre-training and fine-tuning methods impractical.Retrieval-Augmented Generation (RAG) methods address these issues by enabling LLMs to interact effectively with external sources [72,73].This approach is particularly advantageous in robotics, where real-time and accurate contextual understanding is essential for complex decisionmaking and task execution.</p>
<p>Explainability and transparency have become critical aspects of deploying LLMs in autonomous agents.These models enable robots to translate high-level commands into actionable plans, improving performance in complex environments [74,75,76].When integrated with reward decomposition frameworks, LLMs can generate intuitive explanations, linking actions to object-specific properties, thereby reducing ambiguity [77].LLMs can also generate natural language descriptions of robot behaviors and logs, highlighting opportunities for improvement in accuracy and completeness [78,79].</p>
<p>Existing solutions, however, often rely on static models or predefined data, limiting their ability to handle dynamic environments.Our approach addresses these limitations by integrating blockchain-supported accountability with curated logging of relevant robotic data.This framework provides robust context for LLM-generated explanations, minimizing hallucinations and ensuring alignment with the robot's actions and interactions within its environment.</p>
<p>Evaluating LLMs in robotics presents additional challenges, including the absence of suitable metrics, biases in generated text, and the high cost of human evaluations [80].While human evaluations remain the gold standard for capturing subjective insights [48], they are resourceintensive and lack scalability [81].LLM-based evaluations offer a scalable alternative, achieving over 80% alignment with human judgments while delivering explainable outputs [82].A hybrid framework combining human assessments for nuanced insights and LLM evaluations for efficiency and repeatability provides a comprehensive and scalable solution for evaluating LLM outputs.</p>
<p>System Architecture</p>
<p>Our approach comprises two main components: a tamperproof black box-like device, and a module responsible for generating natural language explanations for the data stored by the former.The first component offers services for building, storing, and verifying integrity proofs through SC specifications.The explainability component provides natural language explanations by implementing RAG, thereby enhancing the obtained results using the data recorded by the first component as an external source to enrich the context in the answer formulation process.Both components are integrated through the existence of two main asynchronous tasks focused on cleaning and processing the raw data generated by the black box-like device to increase the effectiveness of the explainability engine.</p>
<p>Figure 1 depicts a generic architecture of our proposal, highlighting in blue color the functionalities related to accountability, and in green those related to explainability.Below we provide a deeper description of the approaches designed and followed in order to develop and integrate the above-mentioned components.</p>
<p>Black Box Recorder</p>
<p>Our first component focuses on recording accountable information that captures essential messages generated during the navigation task of an autonomous agent.The selected data will guarantee the reproducibility of the mentioned task, including details like the robot's poses, the employed map, odometry data, linear and angular velocity, and laser scan readings.</p>
<p>The immutability of the previously described information is achieved through blockchain.Blockchain technology usually faces limitations regarding the size of the data that can be written into a single transaction.Efficiently storing autonomous agent messages becomes challenging due to these constraints.Our proposed solution involves selectively storing the hash of messages at intervals, providing in addition the chance of using different storage frequencies depending on the topic and its relevance.This fact not only enhances the granularity of stored message proofs but also provides a more adaptable solution.</p>
<p>Our anti-tampering approach for this accountability proposal comprises two key components: a chain of integrity proofs coupled with the SC specification.This section delves into their design and justifies how both align with the constraints of autonomous agents.</p>
<p>Key features such as consistency are guaranteed by the blockchain's consensus protocol, where the content of each block is hashed and stored in the next one.Additionally, availability is also assured through the distributed and decentralized nature of the blockchain, ensuring accessibility across the entire network.</p>
<p>Chain of Integrity Proofs</p>
<p>To preserve the integrity of the messages without compromising system performance, our solution entails building a chain where each message hash is linked with the previous hash value.This approach ensures that any alteration of the selected messages will result in a different final hash, easing tampering detection.The verification process is also simplified since only the last hash in the chain needs to be recomputed and compared with the last stored value.A single modification of a hash will disrupt the entire chain from that point onward, simplifying the identification of tampering.</p>
<p>We define a chain entry () with a counter  ‚àà 1.. to be linked with the previous one by concatenating the prior digest ‚Ñé ‚àí1 with the current log message   .hash() denotes the cryptographic hash function, with SHA-256 being the chosen one, commonly used for data integrity verification and digital signatures.</p>
<p>The initial hash value ‚Ñé 0 of the chain is calculated by applying the selected hash function to a set of  = 32 random bytes (), to generate a 256-bit value.Introducing initial entropy into the chain of integrity proofs enhances privacy, ensuring that integrity proofs for similar initial log messages from different records are not identical.Data privacy is further prioritized by storing in the blockchain only the hashes, rather than the contents of the messages.</p>
<p>ùê∂ùê∏ ùëñ = ‚Ñé ùëñ</p>
<p>where  ‚Üê {0.. }  , ‚Ñé 0 ‚Üê hash(),
‚Ñé ùëñ = hash(‚Ñé ùëñ‚àí1 ||ùëÄùë†ùëî ùëñ ) (1)
By chaining every hash with the previous one, we optimize the validation of stored messages because the last linked digest should correspond to the final proof saved in the blockchain.Additionally, a service has been developed to generate the chain of hashes using the recorded data, ensuring partial validation of the integrity proofs without modifying the stored messages or adding non-essential information to the transactions saved in the blockchain.</p>
<p>The number of proofs included in a blockchain transaction depends on the transaction cost limit set by the blockchain framework.Therefore, being  ‚àà 1.. a transaction counter, each newly arriving chain entry is added in a transaction payload   until the limit is reached.Being sign() a shorthand for a signature function and   the private key of the account, transaction   is transmitted from the service in charge of recording the accountable data to the blockchain.
ùëá ùëó = (ùëÉ ùëó , ùëÜ ùëó ) where ùëÉ ùëó ‚Üê {ùê∂ùê∏ ùëñ , ùê∂ùê∏ ùëñ+1 , ùê∂ùê∏ ùëñ+2 , ‚Ä¶ }, ùëÜ ùëó = sign(ùëÉ ùëó , ùëÉ ùëüùëñùë£ùêæùëíùë¶) (2)
A service interruption or a disruption in the behavior of the autonomous agent will lead to the aggregation of the remaining hash values into a single transaction, even if there is capacity for additional values.This practice guarantees the immutability of the last messages sent by a robot upon completing a task or achieving a goal.</p>
<p>Smart Contract</p>
<p>Calling an SC function intended for blockchain writing involves the execution of a transaction that changes the state of the storage data on the blockchain.In our approach, this transaction is signed with the account's private key via the Elliptic Curve Digital Signature Algorithm, effectively identifying the signer.</p>
<p>The defined SC also determines its ownership, implementing access control to ensure that only the contract owner can add or update message proofs.These features guarantee authentication and non-repudiation.</p>
<p>Moreover, our SC includes a function to verify the existence of a message proof on the blockchain.This fact allows external parties to independently confirm whether a specific message hash has been stored in the contract and when it was stored.This functionality contributes to the system's transparency, enabling users or auditors to check the validity of message proofs without relying solely on the contract owner, thereby promoting public accountability.</p>
<p>Explainability Component</p>
<p>Effective communication is essential for achieving explainability in autonomous robots.Our approach is focused on improving this feature through the use of NLP technology, enabling non-expert users to interact with agents in a meaningful way.</p>
<p>Our solution involves the use of RAG to retrieve data from outside the language model (non-parametric) and augment the prompts by adding the information recorded from our accountability component in context.Before this integration, the information undergoes processing by a dedicated module responsible for acquiring and processing the raw data generated by the autonomous agent, serving as contextual input in the question-answering system.</p>
<p>Given the volume of messages, their varied nature, their length, the presence of non-meaningful records generated by the autonomous agent, and the need to establish specific relationships between messages to enhance the context of the engine, the log curation component extracts relevant information.This functionality serves to enhance the effectiveness of explanations provided to non-expert users.A more comprehensive description of this component is provided in Section 4.</p>
<p>After processing the raw data stored by our accountability component, the ingestion process performs the chunking task to break down extensive text segments into more manageable units.To ensure comparability during relevancy searches at inference time, both this data and the user query are converted into numerical representations through embedding language models.Next, based on the embedding of the user query, its relevant text is identified in the document collection by a similarity search in the embedding space.The user-provided prompt is then extended with relevant information obtained in the search and added to the context.This enhanced prompt is sent to the LLM.As the context contains pertinent external data along with the original prompt, the model output is relevant and accurate.</p>
<p>The addition of new accountable data to our explainability component requires asynchronous updates of the accountable data and its embedding representation.These updates ensure that the model remains sensitive to changes, addressing limitations introduced by offline training.</p>
<p>Implementation</p>
<p>The solution presented in Section 3 can be implemented using different technologies.This section details the specific solutions chosen for developing our proposal.</p>
<p>Black Box Recorder</p>
<p>ROS is the most widely used framework in robotics, offering a collection of software libraries and tools for developing robot applications.Nevertheless, limitations in fulfilling the requirements of real-time embedded systems and the necessity to enhance multi-platform support led to a major upgrade to ROS 2, whose main change lies in the adoption of the Data Distribution Service (DDS) [83].</p>
<p>Nodes constitute the fundamental components in both ROS and ROS 2 applications.These nodes are individual processes that handle specific tasks and communicate with each other through messages.Data transfer between nodes can be achieved using various methods, such as topics, services, and actions.Topics facilitate message passing, services enable synchronous request-reply interactions, and actions are utilized for long-running tasks that need feedback.</p>
<p>This work focuses on navigation capabilities.For this purpose, we utilized Nav2, a suite of tools for ROS 2 that is regarded as the successor to the ROS Navigation Stack.Nav2 equips the robot with the capacity to execute complex navigation tasks.</p>
<p>Nav2's core functionalities include planning, control, localization, visualization, and additional features.Furthermore, it employs BTs to create customized navigation behavior by orchestrating multiple independent modular servers, offering high flexibility in navigation tasks and the specification of complex robot behaviors [84].Different independent task servers, intended to compute a path, control effort, or recovery, are communicated with the BT over a ROS 2 interface, such as an action server or service.The use of BTs to generate explanations for robot behavior has been previously successfully evaluated, allowing seeking causal information to answer questions [85].</p>
<p>Within ROS 2, nodes communicate by publishing data on topics, which can be consumed by other nodes.The data itself can range from sensor readings that provide the robot's perception of its environment to control commands sent to actuators from decision-making nodes.Navigation reproducibility of autonomous robots in ROS 2 involves storing essential data, such as their poses, maps, odometry, velocity, and laser scan readings.To this aim, we have used Rosbag files, designed to capture and store data generated during the execution of a ROS system.These files can reproduce the exact conditions and inputs from a previous run, making them particularly valuable for understanding system behavior.The set of topics included in the Rosbag files generated are detailed in Table 1.</p>
<p>To ensure the immutability of generated Rosbag files, Ethereum has been selected as the blockchain framework.</p>
<p>Ethereum, an open-source public blockchain, introduces the cryptocurrency Ether and self-executing programs (SCs) for code execution on the blockchain.Ethereum's open-source nature ensures decentralization and accessibility, allowing participation without needing approval, a feature crucial for maintaining the integrity of recorded data.Additionally, Ethereum's use of PoS as a consensus protocol enhances energy efficiency and throughput compared to traditional protocols such as Proof of Work.In addition, Ethereum's scalability, security and economic efficiency make it a very secure option for ensuring data immutability.The high-level Algorithms 1 and 2 depict the main steps in recording and proof verification services.Our approach addresses challenges such as the drop in robot performance during the Rosbag recording process and costs associated with storing blockchain data.We accomplish this by saving chained hashes from messages at specific intervals, ensuring the integrity of data while preserving system performance.These intervals define how often an integrity proof is stored and can be established based on the relevance of messages from a topic, regarding their required immutability features.Furthermore, message proofs are stored in real-time on the Ethereum blockchain, eliminating the need to modify the content of the Rosbag file.Detection of any alteration to the messages within this file is facilitated by a service that generates the hash chain from the file at intervals set for each topic during the recording process.To verify the existence of any of these hashes in the blockchain, the blockchain logic has been extended by developing an SC using the Solidity language.Solidity is an open-source programming language that facilitates the creation of decentralized applications, contracts, protocols, and agreements.This contract provides functionalities for storing and verifying proof information for hashes stored on the Ethereum blockchain by associating each hash value with the block number where it is stored.Verification of transaction ownership is also conducted during the storage of these proofs.</p>
<p>Therefore, this approach combines immutability, decentralization, traceability, and cost efficiency to address challenges related to the real-time recording and storage of Rosbag files securely and transparently.</p>
<p>Log Curation Component</p>
<p>To filter relevant information from the Rosbag file messages, a raw data processing node has been implemented.Given the size, variety, and intricate nature of messages contained in the Rosbag file, the inclusion of this raw information might be challenging for non-expert users.Therefore, it is necessary to include a component that processes this data, eliminating non-essential content in the natural language explanation generation process.By analyzing messages from the recorded Rosbag topics, this service can identify key circumstances in the navigation process, such as the appearance of an obstacle or a change in the planned trajectory.</p>
<p>The ROS 2 NavigateToPose action server commands the robot to navigate to a specific goal.While Nav2 usually prioritizes the shortest path, external factors may require adjustments.The /plan topic provides the sequence of poses planned to reach the goal.If unexpected circumstances cause a deviation from this path, the Euclidean distance between consecutive poses in the precomputed plan will increase.This change, together with sensor data from the /scan topic, can indicate potential obstacles causing re-planning.These reasonings have been successfully checked in previous authors' work [86] to build explanations for autonomous agents based on the use of ROS 2 topics.</p>
<p>Additionally, the present approach uses the information provided by Nav2 behavior trees due to their potential in designing, visualizing, and understanding the behavior of autonomous robots.Other information such as the present position of the robot and its velocity is also processed through this component.A general overview of its behavior is described through the Algorithm 3. As a result, through this component, ROS 2 developers are relieved from having to call the logging API to generate explanations.This functionality streamlines the development process, allowing developers to focus on core functionalities without the need for managing log calls.This approach reduces debugging efforts and boosts efficient resource usage, scalability, and adaptability in the development lifecycle, contributing to a more productive natural language explanation generation process.An example of how raw ROS messages are transformed through this component is included in Figure 2.
Algorithm</p>
<p>Explainability Component</p>
<p>The output of the previously described node serves as an external source in our implementation of the RAG approach.This development is based on the use and extension of localGPT 1 , an open-source question-answering system designed to interact with documents conversationally while safeguarding the privacy of the underlying information.Running locally, it ensures data privacy and secure interactions with the interpreted and recorded data, facilitating the generation of natural language explanations.Its functionality involves loading a set of selected documents, breaking them into smaller chunks, augmenting search capabilities with LangChain tools, and generating local embeddings.The outcomes are stored in a local vector database using Chroma vector storage.Once this information is saved, and given a user input, relevant splits are retrieved from storage via a similarity search, which finds the appropriate piece of information from the documents.The retrieved data is used by the selected LLM when producing an answer.Figure 3 depicts the main components in the RAG architecture used.</p>
<p>Therefore, this implementation combines the strengths of robotics frameworks, blockchain technology, and natural language processing to provide a comprehensive solution for real-time recording, secure storage, and user-friendly interpretation of robotic system data.It addresses challenges related to performance, storage costs, and user comprehension, making it an effective approach when understanding the behavior of autonomous robots.</p>
<p>Software and Hardware Artifacts</p>
<p>Our solution has been deployed in a Linux server running Ubuntu 22.04 as the operating system.The server is equipped with a 6-core 2.6 GHz CPU, 64 GB of memory, and a 4 GB GPU.</p>
<p>The chosen ROS 2 version is Humble.Details regarding the topics incorporated into the Rosbag file, along with their average rates, are provided in Table 2.These topics play a crucial role in ensuring the reproducibility of the navigation process, facilitating data analysis and debugging purposes.</p>
<p>The experimentation was executed entirely in Gazebo, an open-source solution for 3D robotics simulations.To provide a realistic scenario, we used a hospital simulation environment made available by Amazon Web Services 2 .Additionally, we selected RB-1, an autonomous mobile robot based on the ROS 2 platform produced by Robotnik [87].This combination of tools and platforms enabled us to assess the impact and effectiveness of our approach.</p>
<p>The interaction with Ethereum has been developed through the web3.pylibrary (version 6.6.1).Ganache (version 2.7.1) has been used as the Ethereum network simulator.</p>
<p>RAG solution holds Langchain (version 0.325), and ChromaDB (version 0.4.6).We used the open-source model Llama 2 7B GGUF [88] as the LLM responsible for generating natural language explanations for end users.This LLM integrates Reinforcement Learning from Human Feedback (RLHF) and shows versatility in handling general-purpose tasks, ranging from answering questions, text generation, and language translation to more demanding tasks such as reasoning or coding.Preliminary tests of Llama 2, compared to other LLMs such as Mistral [89] or Zephyr [90], led us to choose this model for evaluating our data, due to the promising results obtained in these assessments conducted during the last quarter of 2023, when this work was in its development phase.As a text embedding model we have chosen Instructor-xl [91], a high-performance model designed to generate text embeddings tailored to tasks such as classification, clustering, text evaluation or retrieval, a fact 2 https://github.com/aws-robotics/aws-robomaker-hospital-worldparticularly important in the processes performed for any RAG solution.</p>
<p>Evaluation</p>
<p>To evaluate our proposal, we conducted an experimental study comparing the performance of an accountability system built in accordance with Section 4. This study covered several approaches for storing integrity proofs across various scenarios.Additionally, we explored and adapted the capabilities of RAG solutions and LLMs in question-answering systems, using as a source the data gathered by our black box-like component, through a comprehensive set of questions.This work aimed to assess the effectiveness of these technologies in delivering natural language explanations to individuals who are not experts.</p>
<p>The experiments conducted within this proposal focus on navigation tasks included in the Navigation Functionality of the European Robotics League Consumer Service Robots Challenge [92].</p>
<p>This functionality evaluates the capabilities of a robot to navigate accurately and autonomously within a given scenario, containing furniture, objects, and people.Details like the initial position and the number of waypoints the robot needs to visit are provided beforehand.Furthermore, the coordinates of the waypoints are communicated sequentially to the robot during runtime.Each waypoint is characterized by its ,  , and Œò coordinates.Existing elements in the environment, such as furniture, doors, or walls, must be mapped.The obstacles, with varying shapes and sizes, remain unknown to the teams beforehand and may differ between runs.Our experiments are structured around three distinct assessment scenarios in line with the navigation functionality previously described.In the initial scenario, the robot must navigate to three waypoints, encountering no obstacles along the way.The second scenario presents the robot with the challenge of adjusting its previously planned path to reach the second goal.This adjustment becomes necessary due to the appearance of an obstacle obstructing the original planned route, requiring a deviation to achieve the goal.The third scenario introduces a new obstacle that blocks the only door needed to reach the first goal.Consequently, this obstacle prompts the cancellation of the first waypoint objective.The second and third goals are achieved with the same restrictions as detailed in the second scenario.</p>
<p>The three scenarios are represented in Figure 4.</p>
<p>The entire source code is accessible online on GitHub 3 , and is licensed under GPLv3.</p>
<p>Black Box Recorder Evaluation</p>
<p>In evaluating the accountability component, we conducted six experiments across three previously described scenarios.These experiments involved storing integrity proofs on the blockchain, with hash calculations occurring at intervals of one every 10, 25, 50, or 100 messages.Moreover, our solution introduces the flexibility to save proofs of evidence at different rates based on the topic, ensuring nontampering for critical topics or those with lower publishing rates where message loss could be crucial.</p>
<p>Therefore, our fifth recording method is designed to store a hash for every message within a topic with a rate below 0.5 Hz, one hash for every 5 messages for topics with rates below 1 Hz, a hash for every 10 messages if the rate is below 3 Hz, one hash for every 15 messages if the topic's rate is below 25 Hz, a hash for every 50 messages for topics with rates below 45 Hz, and one hash for every 100 and 1000 messages when the rate falls below 100 Hz and 400 Hz, respectively.This 3 https://github.com/laurafbec/immutable_explainable_BBR.gitapproach, together with those operating at fixed intervals, has been compared with the generation of a Rosbag file that covers identical topics but lacks anti-tampering properties.Each experiment has been repeated six times, totaling 108 runs, to derive insights into the characteristics of the black box-like device.</p>
<p>Metrics</p>
<p>We employed different metrics to evaluate the impact of the accountability system in the chosen scenarios.First, through the topics' publication rate and the number of messages stored per topic in each Rosbag file, we analyzed the average rate of lost messages based on the employed blockchain storage approach.</p>
<p>Next, regarding system performance, we studied variables such as CPU, memory and disk usage.Specifically, we examined the following variables: system load 1-, 5-and 15-minute average, CPU usage (%), memory usage (GB), sent network traffic (MB/s) and disk writing rate (MB/s).These variables are highly significant for system-overhead analysis given that elevated load conditions can potentially influence the robot's behavior and event-recording capabilities.In particular, we have obtained a full report about system load, CPU, RAM, network traffic, and disk write operations updated every second, facilitating the tracking of overall system activity and an easy comparison with preceding values.To describe the overall distribution and characteristics of these results, we calculated their average as a statistical measure.This information is included in Section 6.</p>
<p>Explainability Evaluation</p>
<p>Despite the remarkable performance of LLMs in various language tasks, evaluating these models presents challenges, especially in ensuring factual consistency.The extensive capabilities of LLMs and the limitations of current benchmarks in capturing human preferences have led to emerging trends in LLM evaluations, such as the use of strong LLMs as judges [82].</p>
<p>Traditional similarity-based metrics, such as BLEU and ROUGE, have proven to misalign with human evaluation.While human evaluation remains the primary standard for assessing human preferences, it is exceptionally timeconsuming and expensive.Furthermore, this approach significantly impacts the scalability of evaluations, making it necessary to use more effective and unbiased methods [81,93].To automate the evaluation process, several works have adopted the 'LLM-as-a-judge' approach, which focuses on using strong LLMs like GPT-4.These models are often trained with RLHF, resulting in strong human alignment [94].Previous studies have evidenced the consistent agreement of these models with human grading [95,96,97,98].Moreover, research shows that LLMs can replicate most of conclusions from human evaluations, particularly in tasks involving prediction accuracy and explanation helpfulness, even across varied settings and domains [81, 99, 100].However, while LLMs excel in scalability and efficiency, their strengths can be enhanced by integrating the nuanced understanding and subjective judgments that humans provide, particularly in complex or domain-specific contexts [101].For example, human evaluators can detect subtle inconsistencies or interpret ambiguous explanations in tasks that require deeper contextual understanding, which an automated method might overlook [48,102].</p>
<p>To address these limitations, we propose a hybrid evaluation method that combines the scalability of LLM-based evaluations with the depth of human assessments.In this approach, a strong LLM such as GPT-4 is used to evaluate a broad set of model outputs, ensuring cost-effective and consistent initial assessments.Human evaluators then validate a representative subset of these outputs to uncover overlooked biases or misalignments.This approach integrates the efficiency and scalability of automated methods with the reliability of human judgment, particularly for nuanced cases.</p>
<p>By adopting this hybrid methodology, we balance efficiency and accuracy, ensuring that the evaluation process captures both objective metrics and subjective preferences effectively.This dual-layered evaluation enables comprehensive assessments while addressing the inherent limitations of relying exclusively on either LLMs or humans.</p>
<p>LLM-as-a-judge Evaluation</p>
<p>The need for a robust LLM aligned with human preferences led us to select GPT-4 to evaluate the answers produced by our explainability component.Through LLM-as-ajudge, we can obtain not only scores but also explanations, making their outputs interpretable.The whole process is represented in Figure 5.</p>
<p>LLMs exhibit a certain degree of indeterminism, defined as the model's ability to offer different answers when asked the same question or prompt several times.This behavior derives from their training, in which the models learn to generate outputs by sampling from probabilistic distributions over possible word sequences, introducing randomness to their response.The effect of this indeterminism must be considered in any LLM evaluation.</p>
<p>In addition, RAG for question-answering systems is one of the most difficult LLM use cases to evaluate.A basic RAG setup includes two main sources of error: the retriever and the LLM.The evaluation of the retriever aims to determine the relevance of its document selection concerning a query.To this end, we conducted an experimental study with 16 different questions on the data generated and processed during the navigation task in the three previously described scenarios.This set of questions has been evaluated six times in each scenario, giving a total of 288 questions and answers to perform the analysis.This approach ensures a thorough examination of the performance of the RAG system in various scenarios, showing potential strengths and weaknesses in our solution.</p>
<p>Using the context provided by the Rosbag data and an evaluation question, our approach includes an evaluation component for the generated explanations that contrasts the prediction made by the evaluated LLM and, thanks to the retrieved selection, compares it to an annotated ground truth.This process yields a correctness metric that scores whether the retriever returned no relevant documents (0) or whether it returned a perfect selection of documents (1).The final score, an average of all the scores in the dataset, is used to judge the retriever's overall performance.</p>
<p>A comprehensive evaluation of an LLM should consider multiple metrics that include correctness as well as other aspects.A language model should produce text that is not only correct but also logically structured and easy to understand.Produced responses must be appropriately detailed without unnecessary verbosity.Furthermore, the LLM must not only provide correct information but also deliver it in a meaningful and valuable way to the user.</p>
<p>Therefore, to assess the performance of the selected LLM on specific attributes, this approach also includes a criteria-based evaluation covering accuracy, conciseness, coherence, and helpfulness.Selected criteria metrics are described in Table 3.This structured assessment offers a means to assess the model's effectiveness in generating responses, providing a broader understanding of the model's capabilities and its alignment with the used scenarios.Employing a scorecard defined for each of these features, our solution assigns scores between 0 and 10 to the responses it generates.The use of this scorecard facilitates a detailed and quantitative analysis of the LLM's output, ensuring a comprehensive understanding of both its strengths and areas for improvement.A complete example of the answers and evaluations obtained for a selected question in the three selected scenarios is included in Figure 6.</p>
<p>Human Evaluation</p>
<p>To validate the explanation results generated by the GPT-4 model, we conducted an online study with 17  The evaluation process involved three distinct questionnaires, one corresponding to each scenario.Participants assessed 16 explanations per scenario, resulting in a total of 48 explanations evaluated by each individual.Each explanation was rated using five questions derived from the metrics outlined in Table 4, based on the Explanation Satisfaction Scale proposed by Hoffman et al. in [102].This scale represents a widely recognized framework for measuring human responses to XAI systems.To ensure the evaluation aligned with human preferences, we used a 5-point Likert scale.Answers in this approach are distributed in 1 (I disagree strongly), 2 (I disagree somewhat), 3 (I'm neutral about it), 4 (I agree somewhat), and 5 (I agree strongly).</p>
<p>In addition to the explanation evaluations and optional demographic information, the questionnaire provided general instructions describing the robot's task and included a map for each scenario.The questionnaires were distributed via a mailing list, and participants were informed about the anonymity and research purposes of the study.Consent was obtained upon submission of their answers.Participation was voluntary, which made the process cost-effective but also introduced potential drawbacks, such as longer response times and biases that may arise from factors like age, education level, or familiarity with autonomous system explainability approaches.</p>
<p>Statistical analysis is essential in human evaluations to ensure rigor and reliability, particularly when interpreting subjective judgments and addressing variability among participants.Likert-scale data, which is ordinal, requires careful statistical treatment to identify significant trends and differences while avoiding misinterpretation of the results.By applying appropriate statistical methods, we strengthen the credibility of our conclusions and provide a robust evaluation of the LLM's results across scenarios.</p>
<p>To ensure the reliability and robustness of the evaluation, a series of statistical tests were applied.Reliability analysis of the scale criteria was conducted using Cronbach's Alpha, a robust metric for assessing internal consistency and ensuring that the scale reliably measures the intended constructs, with a commonly accepted threshold for reliability being  &gt; 0.7 [103,104].</p>
<p>To further explore the relationships among variables in the dataset, correlation analysis was performed using Pearson correlation matrices, which provide insights into the strength and direction of linear relationships between variables [105].</p>
<p>Table 3</p>
<p>Criteria-based evaluation metrics in LLM-as-a-judge evaluation.</p>
<p>Metric Description</p>
<p>Accuracy Accuracy evaluates the extent to which the response conforms to the reference, ranging from being completely unrelated to being completely accurate and in line with the reference.Helpfulness Helpfulness evaluates the degree to which the response provides valuable information, ranging from providing no helpful information to being extremely helpful and offering comprehensive information.</p>
<p>Conciseness</p>
<p>Conciseness assesses the brevity of the response, ranging from being extremely verbose to highly concise with no unnecessary words.</p>
<p>Coherence</p>
<p>Coherence examines the organizational structure and logical flow of the response, from extremely inconsistent to highly coherent, with a clear and logical organization.</p>
<p>Table 4</p>
<p>Questions and metrics used for human evaluation.</p>
<p>Metric Question</p>
<p>Understandability From the explanation, I understand how the task proceeded.Satisfaction The explanation is satisfactory.Informativeness</p>
<p>The explanation has sufficient detail.</p>
<p>Completeness</p>
<p>The explanation seems complete.Usefulness</p>
<p>The explanation is useful to my goals</p>
<p>For inferential analysis, a normality check was conducted using the Shapiro-Wilk test to determine whether the data met the assumptions required for parametric tests [106].Since the data violated these assumptions, the nonparametric Friedman test was employed [107].The Friedman test is particularly suited for analyzing repeated measures on the same participants, as it does not require data to follow a normal distribution [108].This test assessed whether significant differences existed in participant responses across the three scenarios, offering a reliable framework for understanding the impact of different conditions on the perceived quality and effectiveness of explanations.</p>
<p>Finally, post-hoc analysis using the Nemenyi test was performed to examine participant behavior in specific explanations where significant differences were observed, offering a deeper understanding of the underlying patterns in their responses.The statistical analyses were developed through Python scripts (version 3.10.12),utilizing libraries such as pingouin (version 0.5.5) for Cronbach's Alpha, scipy.stats(version 1.14.1) for the Shapiro-Wilk and Friedman tests, and pandas (version 2.3.3) for data manipulation.This approach ensured accuracy and reproducibility in the execution of the tests.</p>
<p>Categories and Evaluation Questions</p>
<p>To evaluate our explainability proposal in the context of ROS 2 messages interpretation, we defined a set of questions covering diverse aspects of the navigation process over the how, the what, the which, the when and the where dimensions.These questions provide insights into the different stages and components of the three scenarios.To structure and improve the understanding and analysis of this set of questions, we organized them into five different categories.These categories range from high-level overviews of navigation to detailed inquiries about trajectory planning, goal completion, and the functioning of the BT.The categories also delve into the causes of unexpected behavior or errors in navigation, improving the explainability of the system whenever an unexpected action occurs.Through these questions, we aim to address key concerns in XAI, including understanding why decisions are made, establishing causality between actions, determining success or failure, identifying timing, and disguising the methods by which achievements are realized [109].</p>
<p>The set of categories, their descriptions and the questions included in each one are described next.</p>
<p>Navigation Process Overview.These questions provide an overview of the navigation process captured in the Rosbag file.Understanding what has happened and how the navigation task has proceeded provides a high-level context that sets the stage for more detailed questions in later categories.This category comprises the following questions:</p>
<ol>
<li>What has happened in this ROS 2 log regarding navigation? 2. How has the navigation task proceeded?</li>
</ol>
<p>Trajectory Planning and Re-planning.These questions delve into the specifics of trajectory planning, a critical aspect of navigation.They address whether the robot has re-planned its trajectory, the reasons behind this behavior, and whether it has encountered any obstacles.These details are necessary for understanding how the robot dynamically adjusts its path.The questions included in this category are: Question: How many goals have been reached by the robot?</p>
<p>Scenario 1</p>
<p>Answer: Based on the provided context, we know that the robot has reached 3 goals.The context mentions that the goal number 1, 2 and 3 have been reached successfully.Therefore, the answer to your question is 3.</p>
<p>GPT-4 Judgement</p>
<p>Predicted Grade: CORRECT Reasoning: The AI assistant's response is accurate, concise, coherent, and helpful.It directly answers the user's question by confirming that the robot has reached three goals.The assistant also provides clarification by referencing the context, which adds to the response's accuracy and helpfulness.There are no unnecessary words or unclear statements, making the response concise and coherent.Accuracy: [ [10]] Conciseness: [ [10]] Coherence: [ [10]] Helpfulness: [ [10]]", score: 1.0</p>
<p>Scenario 2</p>
<p>Answer: Based on the context provided, the robot has reached 3 goals.</p>
<p>GPT-4 Judgement</p>
<p>Predicted Grade: CORRECT Reasoning: The assistant's response is completely accurate as it matches the information given in the ground truth.The response is concise, with no unnecessary words or information.It is highly coherent and has a clear structure.The response is extremely helpful as it gives the exact information asked for by the user.Therefore, the ratings are as follows: Accuracy: The aborted goal is: * Goal number 1: Navigation to the goal number 1 has been aborted.Position: -5.062160931312302, -8.330878461767831.Orientation: -0.8760794639388502,0.4821667479872674.Therefore, the answer to the user's question is: The robot has successfully reached 2 out of 3 goals.Goal numbers 2 and 3 have been successfully achieved, while goal number 1 has been aborted.</p>
<p>GPT-4 Judgement</p>
<p>Predicted Grade: CORRECT Reasoning: The AI assistant's response is comprehensive and accurate.It correctly states that the robot has successfully reached goals number 2 and 3 while goal number 1 has been aborted.The assistant also provides additional details about the positions and orientations of the goals.While these extra details are not explicitly asked for in the user's question, they may be useful for the user's understanding.Therefore, the assistant's response is also helpful.However, the response is not concise because it includes more information than necessary, leading to some redundancy in the explanation.The assistant's response is coherent as it logically organizes the information about each goal.Accuracy: [ [10]] Conciseness: [ [7]] Coherence: [ [10]] Helpfulness: [ [10]]", score: 1.0 3. Has the robot re-planned an alternative trajectory during navigation? 4. Why did the robot re-plan the route? 5. Did the robot find any obstacles during the navigation?</p>
<p>Goal Completion and Navigation Task Status.These questions focus on the completion status of the navigation goals.Knowing how many targets have been reached, whether the robot has completed the navigation task, and when the navigation task ended provides insights into the overall progress and success of the robot's task.This category includes the following questions: 6.How many goals have been reached by the robot?7. Has the robot completed the navigation task? 8.When has the robot ended the navigation task? 9. Have all objectives been successfully achieved or have any been cancelled or aborted?</p>
<p>Specifics about Goals and Locations.These questions examine the specifics of goals and locations.Understanding the location of goals, the robot's linear velocity during navigation, its initial position and orientation, and the velocity after receiving a goal contribute to a detailed understanding of the robot's movements and performance.Questions included in this category are:</p>
<p>10.Where is the second location or goal located?11.What was the linear velocity when navigating to goal pose number 2? 12.What is the initial position and orientation of the robot?13.What was the linear velocity of the robot after receiving goal number 1?</p>
<p>Nav2 Behavior Tree and Node Status.These questions provide information about the underlying behavior tree and the state of its nodes in the navigation system.Knowing the specific node responsible for determining a viable path, identifying any failed nodes during navigation, and understanding the use of behavior tree nodes, contribute to a deeper understanding of the inner workings of the navigation system.This category includes:</p>
<ol>
<li>What is Nav2 Behavior Tree's node to determine a viable path from a starting point to a specified target pose or location?15.Did any node from the Nav2 Behavior Tree fail during navigation?16.Which Behavior Tree's nodes were used during navigation?</li>
</ol>
<p>Experimental Results and Discussion</p>
<p>This section summarizes the experiments' results involving both the black box-like engine and the explainability component when using our solution in the previously depicted assessment scenarios.</p>
<p>Message Loss Rates</p>
<p>Figure 7 shows the percentage of lost messages in the Rosbag file for experiments detailed in Section 5, capturing the topics outlined in Table 2, attending to their publication rates and to the approaches selected to store integrity proofs in blockchain.The results across the three scenarios demonstrate a consistent trend, showing uniform message loss patterns in the creation of immutable Rosbag files.</p>
<p>Choosing to store a single message hash for every 100 messages results in a loss of less than 5% compared to avoiding anti-tampering measures in the Rosbag recording process, even for topics with rates exceeding 300 Hz.This approach stands out as the most efficient option for integrating anti-tampering techniques into our accountability solution.</p>
<p>Alternative strategies, such as saving one proof every 50 messages or adjusting proof intervals based on topic rates, achieve losses below 10% for each topic.This is particularly noteworthy, ensuring the integrity of messages based on their significance in a given scenario or task.</p>
<p>Introducing an integrity proof every 25 messages leads to an overall reduction in message recording, especially evident when topic rates surpass 100 Hz.Storing one hash every 10 messages could lead to losses exceeding 30% for topics with rates higher or around 100 Hz.Beyond the consistent linear trend of message loss with high topic rates, a slight deviation is observed in this approach when topics fall below 50 Hz.This variation signifies the influence on message recording when computing hash functions for messages nearing 1 MB in size, as observed in messages from the /camera/im-age_raw topic, given that the computational complexity of common hash functions, such as SHA-256, increases with input size.</p>
<p>System Benchmarking</p>
<p>The system load or, the running thread demand on the system as the average number of running and idle threads, is shown in Figure 8.The average of 1 minute is higher than the average of 5 or 15 minutes in all scenarios, showing a load increase.However, the values of this metric do not exceed the CPU count, which is 6 in our experimental setup, indicating no performance issues or resource saturation.The inclusion of runnable and uninterruptible tasks in this metric means that the load average may increase due to a disk I/O workload, not just CPU demand, giving a more general view of system overhead.Other more specific metrics are described next to clarify the possible existence of a bottleneck in our approach.</p>
<p>The CPU consumption by user processes maintains a consistent linear trend across all approaches and scenarios, remaining around 30%.This level of consumption is generally considered acceptable for resource-intensive processes such as the recording of a Rosbag file, providing a substantial margin in the event of introducing other critical tasks into the system.</p>
<p>Concerning memory consumption, all approaches within the three scenarios exhibit comparable values, differing by no more than 0.5 GB.Consequently, the inclusion of blockchain approaches into our black box recorder does not yield a remarkable impact on RAM consumption in comparison to avoiding the use of anti-tampering techniques in this recording process.</p>
<p>Regarding network traffic, approaches with higher process storage in the blockchain increase this metric, but do not exceed 0,20 MB/s, which is a very affordable quantity.Furthermore, approaches such as storing one hash every 100 messages nearly match the avoidance of storing any proof into the blockchain, indicating that including this approach does not have a remarkable impact on the system performance.</p>
<p>Disk write operations do not form a bottleneck in system performance, with all approaches differing by no more than 0,40 MB/s, which is not a very significant impact.In addition, all the approaches that perform integrity-proof storage on the blockchain achieve lower or similar results to the approach avoiding anti-tampering operations.</p>
<p>Figure 9 shows the results described.These findings may encourage the adoption of our approach to ensure integrity in messages from autonomous agents.</p>
<p>Correctness Evaluation Using LLM-as-a-judge</p>
<p>Figure 10 shows the results of the correctness of RAG evaluation, presenting both the overall results and assessments based on predefined categories.The results demonstrate consistency across the three scenarios, getting a significant number of correct answers, even though there has not been a prior tuning in the ROS 2 messages for autonomous agents' navigation tasks.The slight percentage of incorrect answers remains within an acceptable threshold, given the specific context employed and the inclusion of general questions.</p>
<p>Regarding individual categories, the correctness in the category 'Navigation Process Overview' is relatively consistent across all scenarios, indicating a good understanding of events and progress related to navigation.However, the broad nature of the question "What has happened in this ROS 2 log regarding navigation?" initially yielded a correctness below 65% in the third scenario.In this instance, the cancellation of a goal resulted in the execution and failures of Nav2 Behavior Tree nodes, not included in the other two scenarios and focused on navigation recovery.Therefore, the incorporation in a dataset of information related to the cancellation of a goal results in LLM answers that usually diverge from the ground truth provided, as the model occasionally prioritized BTs information, other times focused on locations or timestamps.This variability made it difficult to align with the ground truth provided, even if the obtained answers could be considered partially accurate.Introducing the prompt extension "Describe the sequence of events regarding navigation goals and include information about each of them." in Question 1 significantly increased correctness in the first category, achieving results around and exceeding 75% in all three scenarios.This fact underscores the effectiveness of prompt engineering when tackling ambiguous or broad queries.</p>
<p>The results also show consistently high correctness across all scenarios concerning questions related to trajectory planning and re-planning.The model can determine whether the modification of a predicted trajectory has been caused by the occurrence of an obstacle or other circumstance.Scenario 1 and Scenario 2 also exhibit relatively high correctness in questions related to goal completion and task status.However, the model encounters challenges in Scenario 3, occasionally failing to identify whether a navigation that starts correctly concludes with an abortion or cancellation.This issue may be attributed to misalignment during the retrieval process, specifically in the selection of the best chunk associated with a particular goal associated with numerous embeddings within a single collection from the vector database.</p>
<p>Correctness in identifying specific targets and locations lacks some consistency, with results exceeding 50% in the first scenario, around 70% in the second and over 75% in the third.In particular, three scenarios encounter difficulties in identifying the initial position of the robot before starting the navigation, as the model misaligns the queried position with the one assumed by the robot when starting the navigation process.This fact may also be attributed to deficiencies in the retrieval process.In Scenario 1, understanding the sequence of events or the order of goals is a challenge for the model, leading to inaccuracies in specifying the second location or goal.Nevertheless, this challenge is not present in Scenario 3 and is absent or reduced in Scenario 2, demonstrating how differences in data distribution can introduce varied responses even when using specific and unambiguous questions.</p>
<p>Through questions included in the 'Nav2 Behavior Tree and Node Status' category, we assessed the effectiveness of the model's formal linguistic competence compared to its functional linguistic competence, a feature of present LLMs [110].While the model excels in identifying BT nodes given its description, it faces challenges in pinpointing these components under specific conditions.As a result, the criteria correctness suffers a drop in Scenario 3, although overall results remain above 75%.In this scenario, the navigation recovery actions caused by the cancellation of the first goal introduced a challenge in the interpretation of the information by the LLM due to the execution of several Nav2 BT nodes not present in the other scenarios.Particularly, the mistakes stem from the model's confusion in determining whether a previously failed Nav2 BT node has recovered or finished its execution, as well as identifying all nodes that were executed during navigation.This can be attributed to the fact that the complexities of the recovery actions and the dynamic nature of BT executions during navigation recovery introduce complexities that the model may not fully generalize from the training data, underscoring the need for additional fine-tuning in such scenario-specific contexts in case the obtained correctness needs to be improved.</p>
<p>Criteria-based Evaluation Using</p>
<p>LLM-as-a-judge Figure 11 shows the performance of the selected LLM in the evaluated metrics, demonstrating consistency with slight variations in the scores.On average, the answers provided are accurate and in line with the reference, though there might be room for improvement.While responses generally provide useful information, there are instances where additional depth could enhance their utility.In addition, most answers usually include a good balance between conciseness and effectiveness, with clear and logical organization.The overall results show no notable differences in these metrics across the three scenarios.This suggests that messages within the Rosbag related to the introduction of obstacles or cancellation of goals do not notably influence the general criteria evaluation outcomes.</p>
<p>The language model's adaptability to challenges like obstacles and goal cancellations regarding the described categories, is shown in Figure 12.</p>
<p>Concerning general questions about events and the progression of the navigation task, we observe variations in scores across Scenarios 2 and 3.While coherence and conciseness improve, Scenario 1, which involves direct navigation to three goals, scores higher in accuracy and helpfulness.This fact suggests that effective communication on general queries benefits from the absence of obstacles or goal cancellations, enhancing overall understanding.</p>
<p>In the category 'Trajectory Planning and Re-planning', questions are effectively addressed across all three scenarios, showing scores above 8 in every metric, even when obstacles or goal cancellations are introduced.These results highlight the system's capability to manage trajectory-related queries effectively despite challenges.</p>
<p>Regarding the category 'Goal Completion and Navigation Task Status', Scenarios 1 and 2 effectively convey information about goal completion and navigation task progress.Nevertheless, slightly lower scores in Scenario 3 indicate challenges in effectively communicating completion status when a goal is aborted.</p>
<p>In the 'Specifics about Goals and Locations' category, the system provides valuable goal and location information, even in the presence of obstacles or goal cancellations.This suggests a strong ability to understand and localize specific details regardless of the challenges introduced in each scenario.The results regarding questions included in 'Nav2 Behavior Tree and Node Status' category demonstrate effectiveness in communication across all scenarios and metrics.This emphasizes the potential for BTs to complement language models in explainability, offering insight into the decision-making processes during navigation tasks.</p>
<p>In summary, although results can be improved in accuracy for general questions, the language model shows adaptability across different navigation scenarios, including those with obstacles and task changes.The consistency in conveying information related to Nav2 BTs suggests a solid understanding of underlying navigation processes.The model also performs well when providing information about trajectory planning, goal completion, and specifics about goals and locations, regardless of the specific challenges introduced in each scenario.Nevertheless, while evaluation using LLMs excels in scalability and efficient resource usage, it is still recommended to test a subset of questions with human evaluators.This ensures a deeper understanding of how well the approach aligns with human expectations and effectively communicates, particularly in scenarios involving complex or nuanced details.</p>
<p>Human Evaluation of LLM-generated Explanations</p>
<p>This section presents an analysis of the results from the human evaluation conducted with 17 participants, as stated in Section 5.2.2.</p>
<p>Distribution Analysis</p>
<p>Figure 13 depicts the distribution of Likert scale responses for the five evaluation criteria across the three scenarios, showing the frequency of each response value.The highest ratings predominate across all scenarios and criteria, suggesting that participants generally found the explanations satisfactory.However, Scenario 2 and Scenario 3 exhibit greater variability, with an increase in mid-range scores for certain criteria, such as Usefulness.These trends are particularly noticeable in broad questions, such as those in Category 1 regarding the navigation process overview, or when the model's answers do not align with the ground truth.In such cases, participant opinions diverge significantly, even in the presence of factual inaccuracies, indicating a potential bias in how the utility is perceived when discrepancies with the ground truth arise.</p>
<p>Reliability Analysis</p>
<p>The reliability analysis resulted in an overall Cronbach's Alpha value of 0.9792, indicating good internal consistency and confirming that the evaluation criteria reliably capture the intended dimensions of the robot's explanations.</p>
<p>Additionally, we calculated Cronbach's Alpha if an item is deleted for each criterion.This metric helps assess the impact of removing an individual item on the scale's overall reliability.As summarized in Table 5, the "Alpha if Deleted" values are consistently close to the overall Cronbach's Alpha.Specifically, these values range from 0.9699 (Usefulness) to 0.9798 (Informativeness), with all values slightly lower than the overall result.This suggests that removing any single item would only have a minimal impact on the reliability of the scale, indicating that each criterion contributes positively to the measurement of overall quality.</p>
<p>Further, we examined the item-total correlations, which quantify the strength of the relationship between each criterion and the total score.These correlations range from 0.9286 for the Informativeness criteria to 0.9660 for Usefulness.The high item-total correlations support the relevance of each item in assessing the key dimensions of the robot's explanations.Finally, we assessed the inter-item correlations, which provide insight into how strongly the criteria correlate with one another.The inter-item correlation matrix, presented in Table 6, shows values ranging from 0.878 (between Informativeness and Satisfaction) to 0.981 (between Satisfaction and Usefulness).These high correlations indicate that the criteria are closely related but not redundant, as each captures a unique and complementary aspect of the evaluation process.</p>
<p>Correlation Analysis</p>
<p>Building on the findings from the reliability analysis, we further explored the relationships among the variables in our dataset through Pearson correlation matrices.While Cronbach's Alpha results demonstrated the internal consistency of the evaluation criteria, the correlation analysis extends this by examining how these criteria relate to each other across different scenarios and explanation categories.</p>
<p>Figure 14 presents the Pearson correlation matrix for participants' responses across the three scenarios.It reveals strong associations, indicating a high level of consistency in participants' assessments.This consistency suggests that the explanations provided were clear and easily comparable, allowing participants to reliably evaluate them regardless of the specific context of the robot's task.</p>
<p>Figures 15 and 16 depict the relationships among evaluation criteria and question categories.Regarding the criteria, Completeness and Informativeness exhibit a strong positive correlation, as do Understandability and Usefulness.This indicates that participants are more likely to perceive explanations as useful when they are also clear.In contrast, the correlation between Informativeness and Usefulness is weaker (0.55), highlighting a divergence in participants' evaluations.This divergence may stem from biases previously discussed, where participants find it challenging to judge the usefulness of an explanation that is not fully accurate.</p>
<p>Regarding the question categories, the 'Navigation Process Overview' and 'Specifics About Goals and Locations' categories exhibit a strong correlation (0.99), indicating that participants tended to provide similar ratings for explanations in these areas.This suggests that these types of explanations are perceived similarly.In contrast, the 'Trajectory Planning and Re-planning' category shows weak or negative correlations with other categories.This discrepancy arises from differences in evaluating broad, open-ended explanations versus more specific ones.</p>
<p>Inferential Statistics</p>
<p>Statistical analysis often requires understanding the underlying data distribution to ensure the appropriate methods are applied.Since parametric tests assume normality, violations of this assumption can compromise the validity of the results.To assess normality, we applied the Shapiro-Wilk test, a robust method for small to moderate-sized samples, across various groupings, including scenarios, categories, questions, and criteria.The test results consistently revealed significant deviations from normality, with -values well below 0.05, leading to the rejection of the null hypothesis of normality for all groups.These findings confirm that the dataset does not satisfy the assumptions required for parametric tests, such as ANOVA or t-tests.As a result, non-parametric methods were adopted in subsequent analyses to ensure the robustness and validity of statistical inferences.</p>
<p>The Friedman test was used due to its ability to assess differences in related groups when the data does not meet the assumptions of normality required for parametric tests.The results of the Friedman test across the three scenarios reveal significant differences in participant ratings for several explanations, indicating varying levels of agreement across conditions.In Scenario 1, significant differences were observed for explanations to Questions 1 and 2, suggesting diverse responses.In contrast, most other questions in this scenario showed no significant differences ( ‚â• 0.05).</p>
<p>Explanations 1 and 2 in all three scenarios addressed broad questions, whose answers diverged from the ground truth, as discussed in the GPT-4 evaluation in 6.3.Broad questions often result in responses that prioritize varied information, such as BTs, locations, or timestamps, making it difficult to align the answers with the ground truth.This fact justifies the significant variability in participant ratings for these explanations.</p>
<p>In Scenario 2, significant differences were observed for Explanations 1, 7, 8, and 15, with -values below the 0.05 threshold, while other questions did not show significant variations.Specifically, the explanations for Questions 7 and 8 were misaligned with the ground truth, leading to significant variability in participant assessments.In these cases, participants' ratings lacked a clear trend across the evaluated criteria.A similar pattern was observed in Explanation 15 of Scenario 2, where the model provided irrelevant information that did not directly address the user's inquiry.This lack of relevance and alignment likely contributed to the observed variability in participant ratings.</p>
<p>Finally, Scenario 3 showed significant differences for Explanations 1, 2, 7, and 16, indicating that the conditions for these explanations led to notable variations in participant ratings.Explanation 7, does not address the answer to the corresponding question, including no relevant information.Explanation 16 in Scenario 3 caused variability in participants' answers due to its incomplete nature, highlighting the importance of content clarity in generating consistent and meaningful explanations.These findings suggest that while most explanations remain consistent, certain ones resonate differently depending on the scenario, nature of the question, and LLMs indeterminism.</p>
<p>The post-hoc Nemenyi test was conducted for explanations that showed significant differences in participant ratings based on the Friedman test results.This allowed us to identify specific participant pairs with notable divergences in their responses across the three scenarios.The Nemenyi test revealed that most participants were consistent in their evaluations, but certain pairs exhibited significant variability.For example, in Scenario 1, the evaluation for Explanations 1 and 2 highlighted differences involving Participant 8 and several others.In Scenario 2, significant differences emerged for Explanation 1 among Participants 3, 6, 7, and 13.Similarly, in Scenario 3, variability was observed for Explanations 1 and 16, where selected participants differed significantly from others.These discrepancies are likely due to natural human biases, differences in interpretation, and subjective judgments that are inherent in human responses.Overall, while the explanations were generally clear and consistent for most participants, the observed variability underscores the role of individual perspectives and biases in shaping evaluations, reflecting the inherent complexity of human judgment.</p>
<p>Similarity Between Human and LLM Evaluations</p>
<p>As described in 5.2, previous studies have demonstrated that LLMs can effectively approximate human judgment in various evaluation tasks, including those involving subjective interpretation.In this work, we conducted a focused validation of LLM-generated evaluations by comparing a subset of 48 explanations from one of six evaluation runs performed by the LLM against human assessments.This subset included 16 explanations per scenario, allowing us to validate the LLM's judgment within a practical scope.</p>
<p>Figures 18 and 19 depict the mean responses from 17 human participants and GPT-4, respectively, acting as a judge in this subset.The comparison highlights notable similarities in scoring patterns across scenarios and explanations, suggesting a high degree of alignment between human and LLM evaluations.Both evaluations consistently identify high-quality explanations with scores clustered around 4.5-5, indicating shared agreement on the quality of certain explanations.Similarly, lower-rated explanations identified by humans are also flagged by the LLM, revealing a mutual recognition of weaker content.</p>
<p>However, while the overall trends are similar, there are some differences in the exact ratings and outlier values between the two evaluations.These differences could be attributed to nuances in how the LLM interprets the explanations compared to human evaluators, who might take into account subtle contextual or subjective factors that an LLM might miss.Despite these slight variations, the visual correspondence between the human and LLM heatmaps indicates that the LLM provides a reliable approximation of human judgment, making it a promising tool for scaling evaluation processes without sacrificing much in terms of accuracy.This similarity could be valuable in demonstrating that LLMs are suitable alternatives for tasks like explanation evaluation, especially in large-scale studies where human evaluation may be impractical.</p>
<p>Conclusions</p>
<p>This work presents an accountability and explicability architecture, implemented for Robot Operating System (ROS)-based mobile robots.Our approach comprises a black box component for accountability that effectively addresses real-time recording challenges, ensuring message integrity and verifiability through blockchain technology, and an explainability component.This last component is based on the use of Retrieval Augmented Generation (RAG) and  Large Language Models (LLMs) through a private questionanswering system designed to use the information recorded by the first component as context.This solution demonstrates the feasibility of obtaining accurate, coherent, concise and helpful explanations for robot actions from raw data stored in Rosbag files, providing satisfactory results for users and thereby enhancing human-robot interaction.Additionally, both main components have been integrated through the development of a third main functionality in charge of filtering non-relevant information and making messages closer to the final user, due to their specific nature can introduce challenges in their comprehension process.Furthermore, this The results achieved highlight the capability of our solution to guarantee the immutability of information generated by an agent without compromising its performance, a feature particularly important when trying to identify the causes behind an event.Moreover, our system improves efficiency and effectiveness in the deployment of autonomous agents, providing natural language explanations across key navigation aspects including goal achievement, obstacle appearance, trajectory re-planning, and objective cancellation or abortion.This fact strengthens trust and confidence in the use of autonomous agents across diverse applications and contexts.</p>
<p>While our approach has shown promising results in navigation functionalities regarding several scenarios and circumstances, future work should involve adapting our solution to other applications by simply adapting the recording and the processing components to the relevant topics of the area or domain of use.Furthermore, the consistency and robustness of responses generated by our explainability approach could benefit from refinement across different scenarios, especially when categorizing questions under specific subjects.Therefore, future work should explore the impact of In-Context Learning techniques on prompt generation, in addition to fine-tuning the model for the specific context of autonomous agent navigation tasks.</p>
<p>Additionally, our RAG approach could improve in terms of retrieving the most relevant chunk of information from the context.This enhancement may involve exploring the use of separate collections within the vector database or employing a hybrid approach, such as integrating Knowledge Graphs with vector indexes.Furthermore, by incorporating realtime explanation generation mechanisms into our solution, the responsiveness and adaptability of the system could be improved.These approaches can help to improve the accuracy, quality and overall consistency of the answers obtained across several scenarios, thereby enhancing the performance of our proposed system.</p>
<p>A. Supplementary Materials</p>
<p>A.2. Explanations Obtained</p>
<p>The 288 questions and answers obtained through the explanation generation and evaluation process are available in the aforementioned mentioned Github repository 4 .Additionally, a Python notebook has been developed to interact</p>
<p>Figure 1 :
1
Figure 1: System architecture.</p>
<p>Figure 2 :
2
Figure 2: Example of raw Rosbag messages and information obtained from them after the Log Curation process.</p>
<p>Figure 3 :
3
Figure 3: Retrieval Augmented Generation architecture.</p>
<p>Figure 4 :
4
Figure 4: Scenario maps with trajectories, goals, and obstacles.(a) Scenario 1: No obstacles scenario.(b) Scenario 2: One obstacle scenario.(c) Scenario 3: Two obstacles scenario.</p>
<p>Figure 5 :
5
Figure 5: Explainability evaluation.</p>
<p>3 Answer:
3
[[10]] Conciseness: [[10]] Coherence: [[10]] Helpfulness: [[10]]", score: 1.0 Scenario Based on the provided context,the robot has successfully reached 2 out of 3 goals.The successful goals are: * Goal number 2: Navigation to the goal number 2 has succeeded.Position: -9.14068582195674, -25.977117025927537.Orientation: -0.6580310307304338,0.7529908117605705.* Goal number 3: Navigation to the goal number 3 has succeeded.Position: -1.8811226231679194, -28.773672979732826.Orientation: -0.5946646510401259,0.803973850820613.</p>
<p>Figure 6 :
6
Figure 6: Examples of correct answers, RAG, and criteria-based evaluations obtained across the three scenarios for the question 'How many goals have been reached by the robot?' with LLM-as-a-judge evaluation.</p>
<p>Figure 7 :
7
Figure 7: Message loss rates in Rosbag Files under several anti-tampering techniques.Comparative visualizations for: (a) Scenario 1, (b) Scenario 2, and (c) Scenario 3.</p>
<p>Figure 8 :
8
Figure 8: Load average under several anti-tampering techniques.Comparative visualizations for: (a) Scenario 1, (b) Scenario 2, and (c) Scenario 3.</p>
<p>Figure 9 :
9
Figure 9: System benchmarking results: (a) CPU usage of user processes.(b) RAM usage.(c) Sent network traffic.(d) Disk writes traffic.</p>
<p>Figure 10 :
10
Figure 10: (a) Overall correctness in the three scenarios.(b) Correctness by category in the three scenarios.</p>
<p>Figure 11 :
11
Figure 11: (a) Overall criteria evaluation in the three scenarios.</p>
<p>Figure 12 :
12
Figure 12: LLM criteria-based evaluation by categories.Comparative visualizations for: (a) Scenario 1, (b) Scenario 2, and (c) Scenario 3.</p>
<p>Figure 13 :
13
Figure 13: Distribution of Likert scale responses across scenarios and evaluation criteria.</p>
<p>Figure 15 :
15
Figure 15: Correlation matrix of responses by criteria.</p>
<p>Figure 16 :
16
Figure 16: Correlation matrix of responses by category.</p>
<p>Figure 17 :
17
Figure 17: Friedman test results.</p>
<p>Figure 18 :
18
Figure 18: Heatmap of mean responses from human evaluators Across Scenarios and Explanations.</p>
<p>Figure 19 :
19
Figure 19: Heatmap of mean responses from the LLM-as-Judge evaluation across scenarios and explanations.</p>
<p>A. 1 .
1
Prompt Template Prompt Template "[INST]¬´SYS¬ª\nYou are a helpful assistant, you will use the context provided to answer users' questions.\nCarefully read the provided context completely before answering questions and think step by step.\nIfyou can't answer a user's question based on the context provided, inform the user.Do not use any other information to answer the user.Provide a very detailed answer to the question.\n¬´/SYS¬ª\nContext: {context}\n User: {question}[/INST]"</p>
<p>Figure 20 :
20
Figure 20: Prompt template used to generate explanations with Llama 2 LLM.</p>
<p>Table 1
1
Topics name included in the Rosbag file their description.
6:String ùëùùëüùëíùë£ùëñùëúùë¢ùë†ùêªùëéùë†‚Ñé ‚Üêùëñùëõùëñùë°ùëñùëéùëôùëÅùëúùëõùëêùëí7:for ùëöùëíùë†ùë†ùëéùëîùëí in ùë°ùëúùëùùëñùëêùë†ùëÜùëíùë° do8:if ùëöùëíùë†ùë†ùëéùëîùëíùëÅùë¢ùëöùëèùëíùëü % ùë°ùëúùëùùëñùëêùëÖùëéùë°ùëí == 0 then9:ùëê‚Ñéùëéùëñùëõùëíùëëùêªùëéùë†‚Ñé‚Üêsha256(ùëöùëíùë†ùë†ùëéùëîùëí,ùëùùëüùëíùë£ùëñùëúùë¢ùë†ùêªùëéùë†‚Ñé)10:ùëùùëüùëíùë£ùëñùëúùë¢ùë†ùêªùëéùë†‚Ñé ‚Üê ùëê‚Ñéùëéùëñùëõùëíùëëùêªùëéùë†‚Ñé11:end if12:15:else16:// Load Solitidy contract.17:loadBcContract(ùëêùëúùëõùë°ùëüùëéùëêùë°ùê¥ùëëùëëùëüùëíùë†ùë†,ùëêùëúùëõùë°ùëüùëéùëêùë°ùê¥ùëèùëñ)18:19:ùë°ùëüùëéùëõùë†ùëéùëêùë°ùëñùëúùëõ ‚ÜêbuildTransaction(ùëùùëéùë¶ùëôùëúùëéùëë)20:ùë†ùëñùëîùëõùëíùëëùëá ùëüùëéùëõùë†ùëéùëêùë°ùëñùëúùëõ‚ÜêsignTransaction(ùë°ùëüùëéùëõùë†ùëéùëêùë°ùëñùëúùëõ, ùëùùëüùëñùë£ùëéùë°ùëíùêæùëíùë¶)21:sendTransaction(ùë†ùëñùëîùëõùëíùëë_ùë°ùëüùëéùëõùë†ùëéùëêùë°ùëñùëúùëõ)22:for ‚Ñéùëéùë†‚Ñé in ùëùùëéùë¶ùëôùëúùëéùëë do23:saveBC(ùëùùëüùëúùëúùëì [hash] =BLOCK_NUMBER)24:end for25:end if26:end for27: end if28: return ùëüùëúùë†ùëèùëéùëîAlgorithm 2 Proof verificationInput: ‚Ñéùëéùë†‚Ñéùëâ ùëéùëôùë¢ùëíOutput: ùêµùëôùëúùëêùëòùëÅùë¢ùëöùëèùëíùëü1: // Load Solitidy contract.2: loadBcContract(ùëêùëúùëõùë°ùëüùëéùëêùë°ùê¥ùëëùëëùëüùëíùë†ùë†, ùëêùëúùëõùë°ùëüùëéùëêùë°ùê¥ùëèùëñ)3: ùë°ùëüùëéùëõùë†ùëéùëêùë°ùëñùëúùëõ ‚ÜêbuildTransaction(‚Ñéùëéùë†‚Ñéùëâ ùëéùëôùë¢ùëí)4: ùë†ùëñùëîùëõùëíùëëùëá ùëüùëéùëõùë†ùëéùëêùë°ùëñùëúùëõ ‚Üê signTransaction(ùë°ùëüùëéùëõùë†ùëéùëêùë°ùëñùëúùëõ,ùëùùëüùëñùë£ùëéùë°ùëíùêæùëíùë¶)5: sendTransaction(ùë†ùëñùëîùëõùëíùëë_ùë°ùëüùëéùëõùë†ùëéùëêùë°ùëñùëúùëõ)6: ùëèùëôùëúùëêùëòùëÅùë¢ùëöùëèùëíùëü ‚ÜêreadBC(‚Ñéùëéùë†‚Ñéùëâ ùëéùëôùë¢ùëí)7: if ùëèùëôùëúùëêùëòùëÅùë¢ùëöùëèùëíùëü&lt;&gt;0 then
Algorithm 1 Accountable information recording Input: topicsSet = [ ],  , , , BLOCKS_LIMIT,  Output:  1: Boolean  ‚Üê false 2: String  ‚Üê sha256() 3: // Recording process attending to a service call.4:  ‚Üê   5: if  =  then // Building chain of hashes.13: if  &lt; BLOCKS_LIMIT then 14:  ‚Üê  ‚à™ ‚Ñé‚Ñé // Saving chain of hashes into blockchain.8:  ‚Üê "The hash value is stored in ." 9: else 10:  ‚Üê "The hash value is not stored."11: end if 12: return </p>
<p>Table 2
2
Rosbag topics rate and average rate per message.
TopicAvg. Rate (Hz) Avg. Size Msg. (B)map01007308.8tf_static09011.2robot_description025497.6navigate_to_pose/_action/status0.0282389.333global_costmapcostmap0.500996147.2plan0.8843809.28rosout1.014746.057local_costmapcostmap1.6463986.99amcl_pose1.9251054.72behavior_tree_log2.74313.224cmd_vel18.151147.110camera/image_raw25.571037110.93scan47.1738846.012odom92.946779.618tf387.278195.413</p>
<p>participants with diverse demographics.The age distribution ranged from 18 to 55 years, with the majority (47.06%) in the 26-35 age group, followed by 35.29% in the 18-25 age group, 11.76% in the 36-45 age group, and 5.88% in the 46-55 age group.In terms of gender, 62.5% were male and 37.5% were female.Participants' highest level of education varied, with 37.5% holding a PhD, 37.5% having a Master's Degree, and 25% possessing a Bachelor's Degree.Regarding expertise in explainability for autonomous agents, 37.5% were beginners, 31.25% had intermediate expertise, 12.5% were advanced, and 18.75% had no experience.</p>
<p>Table 5
5
Cronbach's Alpha and item-wise "Alpha if Deleted" in human evaluation.
CriteriaCronbach's Alpha if Deleted Item-Total CorrelationCompleteness0.97240.9492Informativeness0.97980.9286Satisfaction0.97520.9567Understandability0.97240.9502Usefulness0.96990.9660</p>
<p>Table 6
6
Inter-Item correlation matrix in human evaluation.
Completeness Informativeness Satisfaction Understandability UsefulnessCompleteness1.0000.9760.9050.9110.911Informativeness0.9761.0000.8780.8870.881Satisfaction0.9050.8781.0000.9360.981Understandability0.9110.8870.9361.0000.947Usefulness0.9110.8810.9810.9471.000
Figure 14: Correlation matrix of responses by scenario.</p>
<p>https://github.com/PromtEngineer/localGPT
https://github.com/laurafbec/immutable_explainable_BBR/blob/main/ gpt4_eval_results/LLMs_QA_Results.xlsx with the content of the previous sheet. This notebook at the following link
.
AcknowledgmentsThis publication is part of the TESCAC project, financed "by European Union NextGeneration-EU, the Recovery Plan, Transformation and Resilience, through IN-CIBE".In addition, this work has been partially funded by the project EDMAR, PID2021-126592OB-C21, funded by MCIN/AEI/10.13039/501100011033and by ERDF A way of making Europe.Data AvailabilityData and tools are available in a public repository6.CRediT authorship contribution statementLauraDeclaration of Competing InterestThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
Explainable Goal-driven Agents and Robots -A Comprehensive Review. F Sado, C K Loo, W S Liew, M Kerzel, S Wermter, ACM Computing Surveys. 552023</p>
<p>Towards Providing Explanations for AI Planner Decisions. R Borgo, M Cashmore, D Magazzeni, CoRR abs/1810.02018</p>
<p>Explainability in human-agent systems. A Rosenfeld, A Richardson, Autonomous Agents and Multi-Agent Systems. 332019</p>
<p>Explainable robotic systems: understanding goal-driven actions in a reinforcement learning scenario. F Cruz, R Dazeley, P Vamplew, I Moreira, Neural Computing and Applications. 352023</p>
<p>Y Al-Slais, M Ali, 10.1109/CyMaEn57228.2023.10050996Robotic Process Automation and Intelligent Automation Security Challenges: A Review, in: 2023 International Conference On Cyber Management And Engineering. IEEE2023Cy-MaEn</p>
<p>Secure and transparent audit logs with BlockAudit. A Ahmad, M Saad, A Mohaisen, Journal of Network and Computer Applications. 1451024062019</p>
<p>Blockchain for Embedded System Accountability. M Chiu, A Goldsmith, U Kalabic, 10.1109/ICBC51069.2021.94611432021 IEEE International Conference on Blockchain and Cryptocurrency (ICBC), IEEE, 2021. </p>
<p>Analysis of the Performance of Different Accountability Strategies for Autonomous Robots. L Fern√°ndez-Becerra, √Å M Guerrero-Higueras, F J Rodr√≠guez-Lera, C Fern√°ndez-Llamas, 10.1007/978-3-030-87872-6_5doi:10.1007/978-3-030-87872-6_514th International Conference on Computational Intelligence in Security for Information Systems and 12th International Conference on European Transnational Educational (CISIS 2021 and ICEUTE 2021). J J Gude Prego, J G De La Puerta, P Garc√≠a, H Bringas, E Quinti√°n, Corchado, ChamSpringer International Publishing2022</p>
<p>Accountability as a service for robotics: performance assessment of different accountability strategies for autonomous robots. L Fern√°ndez-Becerra, √Å M Guerrero-Higueras, F J Rodr√≠guez-Lera, V Matell√°n, Logic Journal of the IGPL. 2024</p>
<p>A secure and auditable logging infrastructure based on a permissioned blockchain. B Putz, F Menges, G Pernul, Computers &amp; Security. 871016022019</p>
<p>Comparison of Blockchain-Based Solutions to Mitigate Data Tampering Security Risk. M Iqbal, R Matuleviƒçius, ; C Di Ciccio, R Gabryelczyk, L Garc√≠a-Ba√±uelos, T Hernaus, R Hull, M Indihar, A ≈†temberger, Ho, 10.1007/978-3-030-30429-4_2doi:10.1007/978-3-030-30429-4_2Business Process Management: Blockchain and Central and Eastern Europe Forum. M Staples, ChamSpringer International Publishing2019</p>
<p>Enhancements to Language Modeling Techniques for Adaptable Log Message Classification. Y Shehu, R Harper, IEEE Transactions on Network and Service Management. 192022</p>
<p>Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models. H Ott, J Bogatinovski, A Acker, S Nedelkoski, O Kao, 10.1109/CloudIntelligence52565.2021.00013IEEE/ACM International Workshop on Cloud Intelligence (CloudIntelligence). 2021. 2021IEEE</p>
<p>P K R Maddikunta, Q.-V Pham, P B , N Deepa, K Dev, T R Gadekallu, R Ruby, M Liyanage, Industry 5.0: A survey on enabling technologies and potential applications. 202226100257</p>
<p>Understandable and trustworthy explainable robots: A sensemaking perspective. G Papagni, S Koeszegi, Paladyn, Journal of Behavioral Robotics. 122020</p>
<p>Explainable agents and robots: Results from a systematic literature review. S Anjomshoae, D Calvaresi, A Najjar, K Fr√§mling, Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems. the International Joint Conference on Autonomous Agents and Multiagent Systems20192</p>
<p>Explainable Agency for Intelligent Autonomous Systems. P Langley, B Meadows, M Sridharan, D Choi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201731</p>
<p>Large Language Models Are Stateof-the-Art Evaluators of Translation Quality. T Kocmi, C Federmann, arXiv:2302.145202023arXiv preprint</p>
<p>Benchmarking Large Language Models for News Summarization. T Zhang, F Ladhak, E Durmus, P Liang, K Mckeown, T B Hashimoto, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models. S Sarsa, P Denny, A Hellas, J Leinonen, 10.1145/3501385.3543957doi:10.1145/3501385.3543957Proceedings of the 2022 ACM Conference on International Computing Education Research. the 2022 ACM Conference on International Computing Education ResearchNew York, NY, USAACM20221</p>
<p>R Ren, Y Wang, Y Qu, W X Zhao, J Liu, H Tian, H Wu, J -R. Wen, H Wang, arXiv:2307.11019Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation. 2023arXiv preprint</p>
<p>A S Rajasekaran, M Azees, F Al-Turjman, A comprehensive survey on blockchain technology. 202252102039</p>
<p>H Guo, X Yu, A survey on blockchain technology and its security. 20223100067</p>
<p>Permissionless Blockchain: How and Why There Is Only One Right Choice. S Solat, P Calvez, F Na√Øt-Abdesselam, Journal of Software. 162021Permissioned vs</p>
<p>C T Nguyen, D T Hoang, D N Nguyen, D Niyato, H T Nguyen, E Dutkiewicz, Proof-of-Stake Consensus Mechanisms for Future Blockchain Networks: Fundamentals, Applications and Opportunities. 20197</p>
<p>A systematic review of blockchain scalability: Issues, solutions, analysis and future research. A I Sanka, R C Cheung, Journal of Network and Computer Applications. 1951032322021</p>
<p>Blockchain-based trusted accountability in the maintenance of medical imaging equipment. I A Omar, H R Hasan, W Alkhader, R Jayaraman, K Salah, M Omar, Expert Systems with Applications. 2411227182024</p>
<p>Blockchain based audit trailing of XAI decisions: Storing on IPFS and Ethereum Blockchain. D Malhotra, S Srivastava, P Saini, A K Singh, 10.1109/COMSNETS51098.2021.93529082021 International Conference on COMmunication Systems &amp; NETworkS (COMSNETS). IEEE2021</p>
<p>An Overview of Smart Contract and Use Cases in Blockchain Technology. B K Mohanta, S S Panda, D Jena, 10.1109/ICCCNT.2018.84940459th International Conference on Computing, Communication and Networking Technologies (ICCCNT). IEEE2018. 2018</p>
<p>BlockIPFS -Blockchain-Enabled Interplanetary File System for Forensic and Trusted Data Traceability. E Nyaletey, R M Parizi, Q Zhang, K.-K R Choo, 10.1109/Blockchain.2019.000122019 IEEE International Conference on Blockchain (Blockchain). IEEE2019</p>
<p>EngraveChain: Tamper-proof distributed log system. L Shekhtman, E Waisbard, 10.1145/3362744.3363346doi:10.1145/3362744.3363346Proceedings of the 2nd Workshop on Blockchain-enabled Networked Sensor, BlockSys'19, ACM. the 2nd Workshop on Blockchain-enabled Networked Sensor, BlockSys'19, ACMNew York, NY, USA2019</p>
<p>W Zhao, I M Aldyaflah, P Gangwani, S Joshi, H Upadhyay, L Lagos, A Blockchain-Facilitated Secure Sensing Data Processing and Logging System. 202311</p>
<p>Immutable Log Storage as a Service on Private and Public Blockchains. W Pourmajidi, L Zhang, J Steinbacher, T Erwin, A Miranskyy, IEEE Transactions on Services Computing. 162023</p>
<p>Permission less Block chains and Secure Logging. C Ge, S Sun, P Szalachowski, 10.1109/BLOC.2019.87513062019 IEEE International Conference on Blockchain and Cryptocurrency (ICBC). IEEE2019</p>
<p>Robot Accident Investigation: A Case Study in Responsible Robotics. A F T Winfield, K Winkle, H Webb, U Lyngs, M Jirotka, C Macrae, 10.1007/978-3-030-66494-7_6doi:10. 1007/978-3-030-66494-7_6Software Engineering for Robotics. A Cavalcanti, B Dongol, R Hierons, J Timmis, J Woodcock, ChamSpringer International Publishing2021</p>
<p>A F T Winfield, A Van Maris, P Salvini, M Jirotka, 10.48550/arXiv.2205.06564arXiv:2205.06564An Ethical Black Box for Social Robots: a draft Open Standard. 2022</p>
<p>Secure Data Recording and Bio-Inspired Functional Integrity for Intelligent Robots. S Taurer, B Dieber, P Schartner, 10.1109/IROS.2018.8593994IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2018. 2018</p>
<p>Analysis of the Performance of Different Accountability Strategies for Autonomous Robots. L Fern√°ndez-Becerra, √Å M Guerrero-Higueras, F J Rodr√≠guez-Lera, C Fern√°ndez-Llamas, 10.1007/978-3-030-87872-6_5doi:10.1007/978-3-030-87872-6_514th International Conference on Computational Intelligence in Security for Information Systems and 12th International Conference on European Transnational Educational (CISIS 2021 and ICEUTE 2021). J J Gude Prego, J G De La Puerta, P Garc√≠a, H Bringas, E Quinti√°n, Corchado, ChamSpringer International Publishing2022</p>
<p>A Modular and Portable Black Box Recorder for Increased Transparency of Autonomous Service Robots. M Schmidt, J Kirchhoff, O Von, Stryk, IEEE Robotics and Automation Letters. 72022</p>
<p>. U Srinivas Aditya, R Singh, P K Singh, A Kalla, A Survey on Blockchain in Robotics: Issues, Opportunities, Challenges and Future Directions. 1961032452021Journal of Network and Computer Applications</p>
<p>Blockchain for AI: Review and Open Research Challenges. K Salah, M H U Rehman, N Nizamuddin, A Al-Fuqaha, IEEE Access. 72019</p>
<p>Black Block Recorder: Immutable Black Box Logging for Robots via Blockchain. R White, G Caiazza, A Cortesi, Y I Cho, H I Christensen, IEEE Robotics and Automation Letters. 42019</p>
<p>Blockchain Technology Secures Robot Swarms: A Comparison of Consensus Protocols and Their Resilience to Byzantine Robots. V Strobel, E Ferrer, M Dorigo, Frontiers in Robotics and AI. 7542020</p>
<p>A Time-Segmented Consortium Blockchain for Robotic Event Registration. V Lopes, N Pereira, M Fernandes, L A Alexandre, 10.1145/3460537.3460557doi:10.1145/3460537.3460557The 3rd International Conference on Blockchain Technology, ICBCT '21, ACM. New York, NY, USA2021. 2021</p>
<p>S Zhang, M Tang, X Li, B Liu, B Zhang, F Hu, S Ni, J Cheng, ROS-Ethereum: A Convenient Tool to Bridge ROS and Blockchain (Ethereum), Security and Communication Networks 2022. 2022</p>
<p>Explainable Reinforcement Learning through a Causal Lens. P Madumal, T Miller, L Sonenberg, F Vetere, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>. D Gunning, M Stefik, J Choi, T Miller, S Stumpf, G.-Z Yang, XAI-Explainable artificial intelligence. 471202019Science Robotics</p>
<p>Explanation in artificial intelligence: Insights from the social sciences. T Miller, Artificial Intelligence. 2672019</p>
<p>Blockchain for explainable and trustworthy artificial intelligence. M Nassar, K Salah, M H Ur Rehman, D Svetinovic, WIREs Data Mining and Knowledge Discovery. 10e13402020</p>
<p>Accounting and auditing with blockchain technology and artificial Intelligence: A literature review. H Han, R K Shiwakoti, R Jarvis, C Mordi, D Botchie, International Journal of Accounting Information Systems. 481005982023</p>
<p>Integration of Explainable Deep Neural Network with Blockchain Technology. S Sachan, J Muwanga, Medical Indemnity Insurance. 2023</p>
<p>Robust integration of blockchain and explainable federated learning for automated credit scoring. Z Jovanovic, Z Hou, K Biswas, V Muthukkumarasamy, Computer Networks. 2431103032024</p>
<p>Blockchain for Ethical and Transparent Generative AI Utilization by Banking and Finance Lawyers. S Sachan, V Dezem, D Fickett, 10.1007/978-3-031-63800-8_16doi:10.1007/978-3-031-63800-8_16L. Longo, S. Lapuschkin, C. Seifert2024ChamSpringer Nature SwitzerlandExplainable Artificial Intelligence</p>
<p>Blockchain-based auditing of legal decisions supported by explainable AI and generative AI tools, Engineering Applications of. S Sachan, LisaX Liu, LisaArtificial Intelligence. 1291076662024</p>
<p>A Secure and Privacy-Preserving Blockchain-Based XAI-Justice System. K Demertzis, K Rantos, L Magafas, C Skianis, L Iliadis, Information. 144772023</p>
<p>Autonomous Vehicles Security: Challenges and Solutions Using Blockchain and Artificial Intelligence. G Bendiab, A Hameurlaine, G Germanos, N Kolokotronis, S Shiaeles, IEEE Transactions on Intelligent Transportation Systems. 242023</p>
<p>Explainable, Transparent Autonomous Agents and Multi-Agent Systems. D Calvaresi, Y Mualla, A Najjar, S Galland, M Schumacher, 10.1007/978-3-030-30391-4_3doi:10.1007/978-3-030-30391-4_3D. Calvaresi, A. Najjar, M. Schumacher, K. Fr√§mling2019Springer International PublishingChamExplainable Multi-Agent Systems Through Blockchain Technology</p>
<p>H A Shehata, M El-Helw, 10.1109/NILES53778.2021.9600519Modeling Collaborative AI for Dynamic Systems of Blockchain-ed Autonomous Agents, in: 2021 3rd Novel Intelligent and Leading Emerging Sciences Conference (NILES). IEEE2021</p>
<p>Towards A Robot Explanation System: A Survey and Our Approach to State Summarization, Storage and Querying, and Human Interface. Z Han, J Allspaw, A Norton, H A Yanco, 2019</p>
<p>Implementation and evaluation of algorithms for realizing explainable autonomous robots. T Sakai, T Nagai, K Abe, IEEE Access. 112023</p>
<p>Trends and Trajectories for Explainable, Accountable and Intelligible Systems. A Abdul, J Vermeulen, D Wang, B Y Lim, M Kankanhalli, 10.1145/3173574.3174156doi:10.1145/3173574.3174156Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. the 2018 CHI Conference on Human Factors in Computing SystemsNew York, NY, USAACM2018</p>
<p>Cases for Explainable Software Systems: Characteristics and Examples. M Sadeghi, V Klos, A Vogelsang, Proceedings of the IEEE International Conference on Requirements Engineering. the IEEE International Conference on Requirements Engineering2021-September (2021</p>
<p>Explainable autonomous robots: a survey and perspective. T Sakai, T Nagai, Advanced Robotics. 362022</p>
<p>why did you do that?. R K , -M Sheh, explainable intelligent robots, in: Workshops at the Thirty-First AAAI Conference on Artificial Intelligence. 2017</p>
<p>Self-Explaining Social Robots: An Explainable Behavior Generation Architecture for Human-Robot Interaction. S Stange, T Hassan, F Schr√∂der, J Konkol, S Kopp, Frontiers in Artificial Intelligence. 5872022</p>
<p>Explainable robotics in human-robot interactions. R Setchi, M B Dehkordi, J S Khan, Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 24th International Conference KES2020. 2020176</p>
<p>Building the Foundation of Robot Explanation Generation Using Behavior Trees. Z Han, D Giger, J Allspaw, M S Lee, H Admoni, H A Yanco, ACM Transactions on Human-Robot Interaction. 102021</p>
<p>The quest of parsimonious XAI: A human-agent architecture for explanation formulation. Y Mualla, I Tchappi, T Kampik, A Najjar, D Calvaresi, A Abbas-Turki, S Galland, C Nicolle, Artificial Intelligence. 3021035732022</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17. the 31st International Conference on Neural Information Processing Systems, NIPS'17Red Hook, NY, USACurran Associates Inc2017</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, P Liu, J.-Y Nie, J.-R Wen, arXiv:2307.03109A Survey of Large Language Models. 2023arXiv preprint</p>
<p>Large language models for robotics: A survey. F Zeng, W Gan, Y Wang, N Liu, P S Yu, arXiv:2311.072262023</p>
<p>K Andriopoulos, J Pouwelse, arXiv:2309.16459Augmenting LLMs with Knowledge: A survey on hallucination prevention. 2023arXiv preprint</p>
<p>R Zhao, H Chen, W Wang, F Jiao, X L Do, C Qin, B Ding, X Guo, M Li, X Li, S Joty, arXiv:2303.10868Retrieving Multimodal Information for Augmented Generation: A Survey. 2023arXiv preprint</p>
<p>To err is robotic; to earn trust, divine: Comparing chatgpt and knowledge graphs for hri. G Wilcock, K Jokinen, 10.1109/RO-MAN57019.2023.1030951032nd IEEE International Conference on Robot and Human Interactive Communication. RO-MAN2023</p>
<p>Large language models for robotics: Opportunities, challenges, and perspectives. J Wang, Z Wu, Y Li, H Jiang, P Shu, E Shi, H Hu, C Ma, Y Liu, X Wang, Y Yao, X Liu, H Zhao, Z Liu, H Dai, L Zhao, B Ge, X Li, T Liu, S Zhang, arXiv:2401.043342024</p>
<p>Llma*: Large language model enhanced incremental heuristic search on path planning. S Meng, Y Wang, C.-F Yang, N Peng, K.-W Chang, arXiv:2407.025112024</p>
<p>A closer look at reward decomposition for high-level robotic explanations. W Lu, X Zhao, S Magg, M Gromniak, M Li, S Wermter, 10.1109/ICDL55364.2023.103644072023 IEEE International Conference on Development and Learning (ICDL). 2023</p>
<p>GPT-2C: A Parser for Honeypot Logs Using Large Pre-Trained Language Models. F Setianto, E Tsani, F Sadiq, G Domalis, D Tsakalidis, P Kostakos, 10.1145/3487351.3492723doi:10.1145/3487351.3492723Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM '21. the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM '21New York, NY, USAAssociation for Computing Machinery2022</p>
<p>M A Gonz√°lez-Santamarta, L Fern√°ndez-Becerra, D Sobr√≠n-Hidalgo, √Å M Guerrero-Higueras, I Gonz√°lez, F J R Lera, arXiv:2304.14844Using Large Language Models for Interpreting Autonomous Robots Behaviors. 2023arXiv preprint</p>
<p>A Survey on Evaluation of Large Language Models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, W Ye, Y Zhang, Y Chang, P S Yu, Q Yang, X Xie, ACM Transactions on Intelligent Systems and Technology. 2024</p>
<p>F Bombassei De Bona, G Dominici, T Miller, M Langheinrich, M Gjoreski, arXiv:2410.17781Evaluating Explanations Through LLMs: Beyond Traditional User Studies. 2024arXiv e-prints</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, H Zhang, J E Gonzalez, I Stoica, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Exploring the performance of ROS2. Y Maruyama, S Kato, T Azumi, 10.1145/2968478.2968502doi:10.1145/2968478.2968502Proceedings of the 13th International Conference on Embedded Software, EMSOFT '16. the 13th International Conference on Embedded Software, EMSOFT '16New York, NY, USAACM2016</p>
<p>The Marathon 2: A Navigation System. S Macenski, F Martin, R White, J G Clavero, 10.1109/IROS45743.2020.93412072020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2020</p>
<p>Building the Foundation of Robot Explanation Generation Using Behavior Trees. Z Han, D Giger, J Allspaw, M S Lee, H Admoni, H A Yanco, J. Hum.-Robot Interact. 102021</p>
<p>Accountability and Explainability in Robotics: A Proof of Concept for ROS 2-And Nav2-Based Mobile Robots. L Fern√°ndez-Becerra, M A Gonz√°lez-Santamarta, D Sobr√≠n-Hidalgo, √Å M Guerrero-Higueras, F J R Lera, V M Olivera, 10.1007/978-3-031-42519-6_1doi:10.1007/978-3-031-42519-6_1International Joint Conference 16th International Conference on Computational Intelligence in Security for Information Systems (CISIS 2023) 14th International Conference on EUropean Transnational Education (ICEUTE 2023). P Garc√≠a Bringas, H P√©rez, F J Garc√≠a, F De Pis√≥n, A Mart√≠nez √Ålvarez, √Å Troncoso Lora, J L Herrero, H Calvo Rolle, E Quinti√°n, Corchado, ChamSpringer Nature Switzerland2023</p>
<p>R Guzm√°n, R Navarro, M Cantero, J Ari√±o, 10.1007/978-3-319-54927-9_13doi:10.1007/978-3-319-54927-9_13Robotnik-Professional Service Robotics Applications with ROS. Springer International Publishing2017</p>
<p>H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D Bikel, L Blecher, C C Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W Fu, B Fuller, C Gao, V Goswami, N Goyal, A Hartshorn, S Hosseini, R Hou, H Inan, M Kardas, V Kerkez, M Khabsa, I Kloumann, A Korenev, P S Koura, M.-A Lachaux, T Lavril, J Lee, D Liskovich, Y Lu, Y Mao, X Martinet, T Mihaylov, P Mishra, I Molybog, Y Nie, A Poulton, J Reizenstein, R Rungta, K Saladi, A Schelten, R Silva, E M Smith, R Subramanian, X E Tan, B Tang, R Taylor, A Williams, J X Kuan, P Xu, Z Yan, I Zarov, Y Zhang, A Fan, M Kambadur, S Narang, A Rodriguez, R Stojnic, S Edunov, T Scialom, arXiv:2307.09288Llama 2: Open Foundation and Fine-Tuned Chat Models. 2023arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D De Las Casas, F Bressand, G Lengyel, G Lample, L Saulnier, L R Lavaud, M.-A Lachaux, P Stock, T L Scao, T Lavril, T Wang, T Lacroix, W E Sayed, arXiv:2310.06825Mistral 7B. 2023arXiv preprint</p>
<p>L Tunstall, E Beeching, N Lambert, N Rajani, K Rasul, Y Belkada, S Huang, L Werra, C Fourrier, N Habib, N Sarrazin, O Sanseviero, A M Rush, T Wolf, arXiv:2310.16944Direct Distillation of LM Alignment. Zephyr2023arXiv preprint</p>
<p>H Su, W Shi, J Kasai, Y Wang, Y Hu, M Ostendorf, W -T. Yih, N A Smith, L Zettlemoyer, T Yu, One Embedder, arXiv:2212.09741Any Task: Instruction-Finetuned Text Embeddings. 2022arXiv preprint</p>
<p>. European robotics league. 2024. April 2024</p>
<p>Human feedback is not gold standard. T Hosking, P Blunsom, M Bartolo, arXiv:2309.163492024</p>
<p>Reinforcement learning and bandits for speech and language processing: Tutorial, review and outlook. B Lin, Expert Systems with Applications. 2381222542024</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, H Nori, H Palangi, M T Ribeiro, Y Zhang, arXiv:2303.12712Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023arXiv preprint</p>
<p>Language Models as Evaluators : A Novel Framework for Automatic Evaluation of News Article Summaries, Master's thesis. C Helgesson Hallstr√∂m, 2023KTH, School of Electrical Engineering and Computer Science (EECS)</p>
<p>Best Practices for LLM Evaluation of RAG Applications | Databricks Blog. Q Leng, K Uhlenhuth, A Polyzotis, 2023</p>
<p>Direct preference optimization: Your language model is secretly a reward model. R Rafailov, A Sharma, E Mitchell, C D Manning, S Ermon, C Finn, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Less is more for alignment. C Zhou, P Liu, P Xu, S Iyer, J Sun, Y Mao, X Ma, A Efrat, P Yu, L Yu, S Zhang, G Ghosh, M Lewis, L Zettlemoyer, O Levy, Lima, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Qlora: Efficient finetuning of quantized llms. T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Large language models are not fair evaluators. P Wang, L Li, L Chen, Z Cai, D Zhu, B Lin, Y Cao, Q Liu, T Liu, Z Sui, 2023</p>
<p>R R Hoffman, S T Mueller, G Klein, J Litman, arXiv:1812.04608Metrics for explainable ai: Challenges and prospects. 2019</p>
<p>A review on sample size determination for cronbach's alpha test: A simple guide for researchers. M A Ujang, E D Omar, N A Baharum, The Malaysian Journal of Medical Sciences (MJMS). 252018</p>
<p>R F Devellis, C T Thorpe, Scale development: Theory and applications. Sage publications2021</p>
<p>. J Benesty, J Chen, Y Huang, I Cohen, 10.1007/978-3-642-00296-0_5doi:10. 1007/978-3-642-00296-0_52009SpringerBerlin Heidelberg, Berlin, Heidelberg</p>
<p>Empirical analysis of cyber-attacks to an indoor real time localization system for autonomous robots. √Ångel Manuel Guerrero-Higueras, N Decastro-Garc√≠a, F J Rodr√≠guez-Lera, V Matell√°n, Computers &amp; Security. 702017</p>
<p>Power comparisons of shapiro-wilk, kolmogorov-smirnov, lilliefors and anderson-darling tests. N M Razali, Y B Wah, Journal of statistical modeling and analytics. 22011</p>
<p>The use and interpretation of the friedman test in the analysis of ordinal-scale data in repeated measures designs. M R Sheldon, M J Fillyaw, W D Thompson, Physiotherapy Research International. 11996</p>
<p>DARPA's Explainable Artificial Intelligence Program. D Gunning, D W Aha, AI Magazine. 402019</p>
<p>K Mahowald, A A Ivanova, I A Blank, N Kanwisher, J B Tenenbaum, E Fedorenko, arXiv:2301.06627Dissociating language and thought in large language models. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
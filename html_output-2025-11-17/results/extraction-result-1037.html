<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1037 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1037</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1037</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-195791510</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1907.01929v1.pdf" target="_blank">Rethinking Continual Learning for Autonomous Agents and Robots</a></p>
                <p><strong>Paper Abstract:</strong> Continual learning refers to the ability of a biological or artificial system to seamlessly learn from continuous streams of information while preventing catastrophic forgetting, i.e., a condition in which new incoming information strongly interferes with previously learned representations. Since it is unrealistic to provide artificial agents with all the necessary prior knowledge to effectively operate in real-world conditions, they must exhibit a rich set of learning capabilities enabling them to interact in complex environments with the aim to process and make sense of continuous streams of (often uncertain) information. While the vast majority of continual learning models are designed to alleviate catastrophic forgetting on simplified classification tasks, here we focus on continual learning for autonomous agents and robots required to operate in much more challenging experimental settings. In particular, we discuss well-established biological learning factors such as developmental and curriculum learning, transfer learning, and intrinsic motivation and their computational counterparts for modeling the progressive acquisition of increasingly complex knowledge and skills in a continual fashion.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1037.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1037.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>developmental agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>developmental learning-based embodied agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied agent that acquires increasingly complex sensorimotor skills through staged, experience-driven development rather than batch training, leveraging critical periods of plasticity and staged curricula to bootstrap learning with less tutoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Developmental robotics: From babies to robots.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>developmental agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent that learns from continuous sensorimotor interaction using staged developmental strategies (progressively more complex skill acquisition); the paper discusses the concept but does not specify a single concrete algorithm (qualitative emphasis on staged bootstrapping rather than batch training).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>robotic agent / embodied agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>real-world, dynamic environments (qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-stationary, temporally correlated streams of rich and uncertain sensory stimuli typical of real-world settings; environments may demand multi-step sensorimotor coordination and progressively complex behaviors as development proceeds.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized qualitatively in the paper by 'richness' and 'level of uncertainty' of stimuli, task difficulty progression, and temporal correlations; no numeric metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Characterized qualitatively by non-stationarity and distributional shifts over time, temporal correlations and changes in task demands; no numeric metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Overall learning performance (e.g., training speed/improvement), robustness to catastrophic forgetting, and bootstrapping efficiency (amount of tutoring required).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper argues that staged developmental strategies are needed to manage the interplay of environmental complexity and variation: progressive task difficulty bootstraps learning in complex/variable environments, but selecting appropriate developmental stages is difficult in highly dynamic environments. It highlights a trade-off where increasing environmental complexity and variation demands more careful staging/curriculum to preserve learning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>developmental learning; staged curricula; progressive task difficulty</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitatively improved (fewer tutoring examples) when staged development and curricula are used; no quantitative counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Staged developmental learning can bootstrap increasingly complex skills with less tutoring and improve training performance, but identifying suitable developmental stages for highly dynamic environments is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking Continual Learning for Autonomous Agents and Robots', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1037.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1037.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>curriculum learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>curriculum learning for embodied agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training strategy that organizes tasks/examples from easy to hard so that agents learn faster and more robustly; task selection can be cast as a stochastic policy optimized for learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated curriculum learning for neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>curriculum-learning agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent trained under a curriculum where tasks or examples are presented in a progressively harder order; task-selection can be governed by a stochastic policy aimed at maximizing learning progress (paper discusses conceptually rather than presenting a new experimental agent).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>embodied/robotic agent (general concept)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>task curricula / staged task sequences</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>An environment or training regime composed of multiple tasks organized by difficulty; complexity is increased progressively to shape representations and skills.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task difficulty progression, number of curriculum stages, and complexity growth rate (qualitative in this paper); specific numerical metrics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>increasing (controlled from low to high via curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Degree and rate at which task instances or task parameters change across curriculum stages (qualitative); the paper also references stochastic task selection policies that modulate variation presented to the learner.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>controlled / adjustable (qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Training speed (time to convergence), final training performance/accuracy, and learning progress (used as intrinsic reward).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Curriculum learning explicitly manipulates complexity (task difficulty) over time to improve efficiency; the paper notes that optimizing task selection (e.g., via a stochastic policy that maximizes learning progress) yields improved training efficiency, indicating an interaction where controlled variation through curricula helps manage complexity and supports better learning.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>curriculum learning; stochastic task-selection policies; linked to intrinsic motivation methods</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported qualitatively as improved (faster training) when tasks are organized progressively; no numeric interaction counts given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Organizing examples/tasks from easy to hard leads to faster training performance; treating task selection as a policy that maximizes learning progress further increases curriculum efficiency, and intrinsic motivation signals can guide this process.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking Continual Learning for Autonomous Agents and Robots', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1037.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1037.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>intrinsic motivation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>intrinsically motivated (curiosity-driven) agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that generates intrinsic rewards (e.g., learning progress) to autonomously select goals and explore, enabling self-generated curricula and exploration in sparse or deceptive reward environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Intrinsically motivated goal exploration processes with automatic curriculum learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>intrinsically motivated agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent using intrinsic reward signals (such as maximization of learning progress) to stochastically select tasks/goals and drive exploration; often used in reinforcement-learning contexts to cope with sparse or deceptive extrinsic rewards. The paper discusses these methods conceptually and cites examples but does not report new experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>embodied/robotic agent (general concept)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>sparse-reward/deceptive environments and developmental exploration settings</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments where extrinsic rewards are sparse or deceptive, requiring autonomous exploration and progressive skill acquisition; complexity arises from long-horizon tasks, sparse feedback, and multi-stage skill dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Sparsity/deceptiveness of extrinsic rewards, sequential task dependencies, and required horizon for reward; no quantitative metrics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high when rewards are sparse and tasks require long sequences</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variety of tasks/goals the agent can self-generate, distributional changes encountered during open-ended exploration; described qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>potentially high (agent controls exposure via goal selection)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Intrinsic learning reward (learning progress), ability to learn tasks under sparse extrinsic reward, and downstream task success.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper argues intrinsic motivation allows agents to control complexity growth by self-generating curricula: in environments with very sparse extrinsic rewards, curiosity-driven exploration produces intrinsic rewards that enable progressive learning of increasingly complex tasks — i.e., intrinsic motivation mediates trade-offs between exploring varied conditions and focusing on learnable complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>curiosity-driven exploration; intrinsic motivation; automatic curriculum learning</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitatively improved in sparse-reward settings because intrinsic rewards guide exploration, but no numeric sample counts are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Intrinsic motivation mechanisms enable autonomous generation of curricula that control the growth of complexity and support learning in sparse/deceptive reward environments; they are crucial for open-ended acquisition of increasingly complex skills.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking Continual Learning for Autonomous Agents and Robots', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1037.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1037.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GEM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gradient Episodic Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continual learning algorithm that stores episodic memories and constrains gradient updates to avoid interference, thereby reducing catastrophic forgetting and enabling positive transfer across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gradient episodic memory for continual learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Gradient Episodic Memory (GEM) model</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory-based continual learning algorithm that retains exemplar episodes and projects gradients to avoid increasing loss on previous tasks; described in the paper as an approach that alleviates catastrophic forgetting and can yield positive transfer, but not used experimentally in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>algorithmic continual-learning model (applicable to agents)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>continual learning across multiple tasks / distributions</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Sequences of tasks drawn from different distributions where the learner must retain previous task performance despite new training; complexity comes from multiple, possibly heterogeneous tasks and non-stationary data streams.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of tasks/distributions and degree of overlap in correlations across them; characterized qualitatively in the review (the original GEM paper contains experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>variable depending on task suite (not specified in this review)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Set of distributions or tasks (i.e., task variation across episodes); not quantified in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>variable (not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Reduction in catastrophic forgetting (retained accuracy on previous tasks), positive transfer to prior tasks, and predictive performance on previous/novel tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>GEM is described as learning subsets of correlations common across task distributions to enable transfer; this implies a strategy of leveraging variation across tasks to learn shared structure, mitigating the negative impact of complexity/variation on forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>episodic memory with gradient constraints (continual learning algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GEM alleviates catastrophic forgetting and can enable positive transfer by constraining learning to preserve performance on previously seen tasks and by exploiting correlations shared across task distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking Continual Learning for Autonomous Agents and Robots', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automated curriculum learning for neural networks <em>(Rating: 2)</em></li>
                <li>Gradient episodic memory for continual learning <em>(Rating: 2)</em></li>
                <li>Intrinsically motivated goal exploration processes with automatic curriculum learning <em>(Rating: 2)</em></li>
                <li>Curiosity-driven development of tool use precursors: a computational model <em>(Rating: 2)</em></li>
                <li>Intrinsic motivation and reinforcement learning <em>(Rating: 2)</em></li>
                <li>Developmental robotics: From babies to robots. <em>(Rating: 2)</em></li>
                <li>Measuring catastrophic forgetting in neural networks <em>(Rating: 2)</em></li>
                <li>Continual lifelong learning with neural networks: A review <em>(Rating: 1)</em></li>
                <li>Curious model-building control systems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1037",
    "paper_id": "paper-195791510",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "developmental agent",
            "name_full": "developmental learning-based embodied agent",
            "brief_description": "An embodied agent that acquires increasingly complex sensorimotor skills through staged, experience-driven development rather than batch training, leveraging critical periods of plasticity and staged curricula to bootstrap learning with less tutoring.",
            "citation_title": "Developmental robotics: From babies to robots.",
            "mention_or_use": "mention",
            "agent_name": "developmental agent",
            "agent_description": "Agent that learns from continuous sensorimotor interaction using staged developmental strategies (progressively more complex skill acquisition); the paper discusses the concept but does not specify a single concrete algorithm (qualitative emphasis on staged bootstrapping rather than batch training).",
            "agent_type": "robotic agent / embodied agent",
            "environment_name": "real-world, dynamic environments (qualitative)",
            "environment_description": "Non-stationary, temporally correlated streams of rich and uncertain sensory stimuli typical of real-world settings; environments may demand multi-step sensorimotor coordination and progressively complex behaviors as development proceeds.",
            "complexity_measure": "Characterized qualitatively in the paper by 'richness' and 'level of uncertainty' of stimuli, task difficulty progression, and temporal correlations; no numeric metrics provided.",
            "complexity_level": "high (qualitative)",
            "variation_measure": "Characterized qualitatively by non-stationarity and distributional shifts over time, temporal correlations and changes in task demands; no numeric metrics provided.",
            "variation_level": "high (qualitative)",
            "performance_metric": "Overall learning performance (e.g., training speed/improvement), robustness to catastrophic forgetting, and bootstrapping efficiency (amount of tutoring required).",
            "performance_value": null,
            "complexity_variation_relationship": "The paper argues that staged developmental strategies are needed to manage the interplay of environmental complexity and variation: progressive task difficulty bootstraps learning in complex/variable environments, but selecting appropriate developmental stages is difficult in highly dynamic environments. It highlights a trade-off where increasing environmental complexity and variation demands more careful staging/curriculum to preserve learning performance.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "developmental learning; staged curricula; progressive task difficulty",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": "Qualitatively improved (fewer tutoring examples) when staged development and curricula are used; no quantitative counts provided.",
            "key_findings": "Staged developmental learning can bootstrap increasingly complex skills with less tutoring and improve training performance, but identifying suitable developmental stages for highly dynamic environments is challenging.",
            "uuid": "e1037.0",
            "source_info": {
                "paper_title": "Rethinking Continual Learning for Autonomous Agents and Robots",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "curriculum learning",
            "name_full": "curriculum learning for embodied agents",
            "brief_description": "A training strategy that organizes tasks/examples from easy to hard so that agents learn faster and more robustly; task selection can be cast as a stochastic policy optimized for learning progress.",
            "citation_title": "Automated curriculum learning for neural networks",
            "mention_or_use": "mention",
            "agent_name": "curriculum-learning agent",
            "agent_description": "Agent trained under a curriculum where tasks or examples are presented in a progressively harder order; task-selection can be governed by a stochastic policy aimed at maximizing learning progress (paper discusses conceptually rather than presenting a new experimental agent).",
            "agent_type": "embodied/robotic agent (general concept)",
            "environment_name": "task curricula / staged task sequences",
            "environment_description": "An environment or training regime composed of multiple tasks organized by difficulty; complexity is increased progressively to shape representations and skills.",
            "complexity_measure": "Task difficulty progression, number of curriculum stages, and complexity growth rate (qualitative in this paper); specific numerical metrics not provided.",
            "complexity_level": "increasing (controlled from low to high via curriculum)",
            "variation_measure": "Degree and rate at which task instances or task parameters change across curriculum stages (qualitative); the paper also references stochastic task selection policies that modulate variation presented to the learner.",
            "variation_level": "controlled / adjustable (qualitative)",
            "performance_metric": "Training speed (time to convergence), final training performance/accuracy, and learning progress (used as intrinsic reward).",
            "performance_value": null,
            "complexity_variation_relationship": "Curriculum learning explicitly manipulates complexity (task difficulty) over time to improve efficiency; the paper notes that optimizing task selection (e.g., via a stochastic policy that maximizes learning progress) yields improved training efficiency, indicating an interaction where controlled variation through curricula helps manage complexity and supports better learning.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "curriculum learning; stochastic task-selection policies; linked to intrinsic motivation methods",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": "Reported qualitatively as improved (faster training) when tasks are organized progressively; no numeric interaction counts given in this paper.",
            "key_findings": "Organizing examples/tasks from easy to hard leads to faster training performance; treating task selection as a policy that maximizes learning progress further increases curriculum efficiency, and intrinsic motivation signals can guide this process.",
            "uuid": "e1037.1",
            "source_info": {
                "paper_title": "Rethinking Continual Learning for Autonomous Agents and Robots",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "intrinsic motivation",
            "name_full": "intrinsically motivated (curiosity-driven) agent",
            "brief_description": "An agent that generates intrinsic rewards (e.g., learning progress) to autonomously select goals and explore, enabling self-generated curricula and exploration in sparse or deceptive reward environments.",
            "citation_title": "Intrinsically motivated goal exploration processes with automatic curriculum learning",
            "mention_or_use": "mention",
            "agent_name": "intrinsically motivated agent",
            "agent_description": "Agent using intrinsic reward signals (such as maximization of learning progress) to stochastically select tasks/goals and drive exploration; often used in reinforcement-learning contexts to cope with sparse or deceptive extrinsic rewards. The paper discusses these methods conceptually and cites examples but does not report new experiments.",
            "agent_type": "embodied/robotic agent (general concept)",
            "environment_name": "sparse-reward/deceptive environments and developmental exploration settings",
            "environment_description": "Environments where extrinsic rewards are sparse or deceptive, requiring autonomous exploration and progressive skill acquisition; complexity arises from long-horizon tasks, sparse feedback, and multi-stage skill dependencies.",
            "complexity_measure": "Sparsity/deceptiveness of extrinsic rewards, sequential task dependencies, and required horizon for reward; no quantitative metrics in this paper.",
            "complexity_level": "high when rewards are sparse and tasks require long sequences",
            "variation_measure": "Variety of tasks/goals the agent can self-generate, distributional changes encountered during open-ended exploration; described qualitatively.",
            "variation_level": "potentially high (agent controls exposure via goal selection)",
            "performance_metric": "Intrinsic learning reward (learning progress), ability to learn tasks under sparse extrinsic reward, and downstream task success.",
            "performance_value": null,
            "complexity_variation_relationship": "The paper argues intrinsic motivation allows agents to control complexity growth by self-generating curricula: in environments with very sparse extrinsic rewards, curiosity-driven exploration produces intrinsic rewards that enable progressive learning of increasingly complex tasks — i.e., intrinsic motivation mediates trade-offs between exploring varied conditions and focusing on learnable complexity.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "curiosity-driven exploration; intrinsic motivation; automatic curriculum learning",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": "Qualitatively improved in sparse-reward settings because intrinsic rewards guide exploration, but no numeric sample counts are provided in this paper.",
            "key_findings": "Intrinsic motivation mechanisms enable autonomous generation of curricula that control the growth of complexity and support learning in sparse/deceptive reward environments; they are crucial for open-ended acquisition of increasingly complex skills.",
            "uuid": "e1037.2",
            "source_info": {
                "paper_title": "Rethinking Continual Learning for Autonomous Agents and Robots",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "GEM",
            "name_full": "Gradient Episodic Memory",
            "brief_description": "A continual learning algorithm that stores episodic memories and constrains gradient updates to avoid interference, thereby reducing catastrophic forgetting and enabling positive transfer across tasks.",
            "citation_title": "Gradient episodic memory for continual learning",
            "mention_or_use": "mention",
            "agent_name": "Gradient Episodic Memory (GEM) model",
            "agent_description": "A memory-based continual learning algorithm that retains exemplar episodes and projects gradients to avoid increasing loss on previous tasks; described in the paper as an approach that alleviates catastrophic forgetting and can yield positive transfer, but not used experimentally in this review.",
            "agent_type": "algorithmic continual-learning model (applicable to agents)",
            "environment_name": "continual learning across multiple tasks / distributions",
            "environment_description": "Sequences of tasks drawn from different distributions where the learner must retain previous task performance despite new training; complexity comes from multiple, possibly heterogeneous tasks and non-stationary data streams.",
            "complexity_measure": "Number of tasks/distributions and degree of overlap in correlations across them; characterized qualitatively in the review (the original GEM paper contains experiments).",
            "complexity_level": "variable depending on task suite (not specified in this review)",
            "variation_measure": "Set of distributions or tasks (i.e., task variation across episodes); not quantified in this review.",
            "variation_level": "variable (not specified)",
            "performance_metric": "Reduction in catastrophic forgetting (retained accuracy on previous tasks), positive transfer to prior tasks, and predictive performance on previous/novel tasks.",
            "performance_value": null,
            "complexity_variation_relationship": "GEM is described as learning subsets of correlations common across task distributions to enable transfer; this implies a strategy of leveraging variation across tasks to learn shared structure, mitigating the negative impact of complexity/variation on forgetting.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "episodic memory with gradient constraints (continual learning algorithm)",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "GEM alleviates catastrophic forgetting and can enable positive transfer by constraining learning to preserve performance on previously seen tasks and by exploiting correlations shared across task distributions.",
            "uuid": "e1037.3",
            "source_info": {
                "paper_title": "Rethinking Continual Learning for Autonomous Agents and Robots",
                "publication_date_yy_mm": "2019-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automated curriculum learning for neural networks",
            "rating": 2,
            "sanitized_title": "automated_curriculum_learning_for_neural_networks"
        },
        {
            "paper_title": "Gradient episodic memory for continual learning",
            "rating": 2,
            "sanitized_title": "gradient_episodic_memory_for_continual_learning"
        },
        {
            "paper_title": "Intrinsically motivated goal exploration processes with automatic curriculum learning",
            "rating": 2,
            "sanitized_title": "intrinsically_motivated_goal_exploration_processes_with_automatic_curriculum_learning"
        },
        {
            "paper_title": "Curiosity-driven development of tool use precursors: a computational model",
            "rating": 2,
            "sanitized_title": "curiositydriven_development_of_tool_use_precursors_a_computational_model"
        },
        {
            "paper_title": "Intrinsic motivation and reinforcement learning",
            "rating": 2,
            "sanitized_title": "intrinsic_motivation_and_reinforcement_learning"
        },
        {
            "paper_title": "Developmental robotics: From babies to robots.",
            "rating": 2,
            "sanitized_title": "developmental_robotics_from_babies_to_robots"
        },
        {
            "paper_title": "Measuring catastrophic forgetting in neural networks",
            "rating": 2,
            "sanitized_title": "measuring_catastrophic_forgetting_in_neural_networks"
        },
        {
            "paper_title": "Continual lifelong learning with neural networks: A review",
            "rating": 1,
            "sanitized_title": "continual_lifelong_learning_with_neural_networks_a_review"
        },
        {
            "paper_title": "Curious model-building control systems",
            "rating": 1,
            "sanitized_title": "curious_modelbuilding_control_systems"
        }
    ],
    "cost": 0.01234475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Rethinking Continual Learning for Autonomous Agents and Robots</p>
<p>German I Parisi 
Apprente, Inc
Mountain View</p>
<p>Christopher Kanan kanan@rit.edu 
CA University of Hamburg
PaigeAI, New YorkNYGermany ContinualAI</p>
<p>Rethinking Continual Learning for Autonomous Agents and Robots
Rochester Institute of Technology, Rochester, NY Cornell Tech, New York, NY</p>
<p>Introduction</p>
<p>Continual learning refers to the ability of a biological or artificial system to seamlessly learn from continuous streams of information while preventing catastrophic forgetting, i.e., a condition in which new incoming information strongly interferes with previously learned representations [18,20]. In the context of machine learning, continual learning models aim to reflect a number of properties of biological systems and their ability to acquire, fine-tune, and transfer knowledge and skills throughout a lifespan. However, despite significant advances in continual machine learning, state-of-the-art models are still far from providing the flexibility, robustness, and scalability exhibited by biological systems. The complexity of the datasets used for the evaluation of continual learning tasks is very limited and does not reflect the richness and level of uncertainty of the stimuli that artificial agents can be exposed to in the real world (see [20,4,14] for recent reviews). Furthermore, neural models are often trained with data samples shown in isolation or presented in a random order. This significantly differs from the highly organized manner in which humans and animals efficiently learn from samples presented in a meaningful order for the shaping of increasingly complex concepts and skills [15]. Therefore, learning in a continual manner goes beyond the incremental accumulation of domain-specific knowledge, enabling to transfer generalized knowledge and skills across multiple tasks and domains [1] and, importantly, benefiting from the interplay of multisensory information for the development and specialization of complex neurocognitive functions [19].</p>
<p>Since it is unrealistic to provide artificial agents with all the necessary prior knowledge to effectively operate in realworld conditions, they must exhibit a rich set of learning capabilities enabling them to interact in complex environments with the aim to process and make sense of continuous streams of (often uncertain) information [13]. While the vast majority of continual learning models are designed to alleviate catastrophic forgetting on simplified classification tasks, here we focus on continual learning for autonomous agents and robots required to operate in much more challenging experimental settings. In particular, we discuss well-established biological learning factors such as developmental and curriculum learning, transfer learning, and intrinsic motivation and their computational counterparts for modeling the progressive acquisition of increasingly complex knowledge and skills in a continual fashion.</p>
<p>Developmental and Curriculum Learning</p>
<p>Humans and high animals show an exceptional capacity to learn throughout their lifespan and, with respect to other species, exhibit the lengthiest developmental process for reaching maturity. There is a limited time window in development in which infants are particularly sensitive to the effects of their experiences. This period is commonly referred to as critical period of development in which early experiences are particularly influential [24]. During these critical periods, the brain is particularly plastic and neural networks acquire their overarching structure driven by sensorimotor experiences (see [21] for a survey; Fig. 1.a). Afterwards, plasticity becomes less prominent and the system stabilizes, preserving a certain degree of plasticity for subsequent reorganisation at smaller scales [22].</p>
<p>Developmental learning strategies have been proposed to regulate the embodied interaction with the environment in real time [3,25]. In contrast to computational models that are fed with batches of information, developmental agents acquire an increasingly complex set of skills based on their sensorimotor experiences in an autonomous manner. Consequently, staged development becomes essential for bootstrapping cognitive skills with less amount of tutoring experience. However, the use of developmental strategies for artificial learning systems has shown to be a very complex practice. In particular, it is difficult to select a well-defined set of developmental stages that favours the overall learning performance in highly dynamic environments.</p>
<p>Better learning performance is exhibited when examples are organized in a meaningful way, e.g., by making the learning tasks gradually more difficult [15]. It has been shown that having a curriculum of progressively harder tasks leads to faster training performance in neural network systems [6]. This has inspired similar approaches in robotics and more recent machine learning methods studying the effects of curriculum learning in the performance of learning [12]. The task selection problem can be treated as a stochastic policy over the tasks that maximizes the learning progress, leading to an improved efficiency in curriculum learning [11]. In this case, it is necessary to introduce additional factors such as intrinsic motivation [2], where indicators of learning progress are used as reward signals to encourage exploration. Curriculum strategies can be seen as a special case of transfer learning [26], where the knowledge collected during the initial tasks is used to guide the learning process of more sophisticated ones.</p>
<p>Transfer Learning</p>
<p>Transfer learning applies previously acquired knowledge in one domain to solve a problem in a novel domain [1]. In this context, forward transfer refers to the influence that learning a task T A has on the performance of a future task T B , whereas backward transfer refers to the influence of a current task T B on a previous task T A (Fig. 1.b). For this reason, transfer learning represents a significantly valuable feature of artificial systems for inferring general laws from (a limited amount of) particular samples, assuming the simultaneous availability of multiple learning tasks with the aim to improve the performance at one specific task.</p>
<p>Transfer learning has remained an open challenge in machine learning and autonomous agents (see [26] for a survey). Specific neural mechanisms in the brain mediating the high-level transfer learning are poorly understood, although it has been argued that the transfer of abstract knowledge may be achieved through the use of conceptual representations that encode relational information invariant to individuals, objects, or scene elements [5]. Zero-shot learning [16] and one-shot learning [7] aim at performing well on novel tasks but do not prevent catastrophic forgetting on previously learned tasks. A recent model called the Gradient Episodic Memory [17] alleviates catastrophic forgetting and performs positive transfer to previously learned tasks. The model learns the subset of correlations common to a set of distributions or tasks, able to predict target values associated with previous or novel tasks without making use of task descriptors.</p>
<p>Curiosity and Intrinsic Motivation</p>
<p>Computational models of intrinsic motivation have taken inspiration from the way human infants choose their goals and progressively acquire skills to define developmental structures in continual learning frameworks (see [10] for a review; Fig. 1.c). Infants select experiences that maximize an intrinsic learning reward through an empirical process of exploration. The intrinsically motivated exploration of the environment, e.g., driven by the maximization of the learning progress [23], can lead to the self-organization of human-like developmental structures where the skills being acquired become progressively more complex.</p>
<p>Computational models of intrinsic motivation collect data and acquire skills incrementally through the online (self-)generation of a learning curriculum [9]. This allows the stochastic selection of tasks to be learned with an active control of the growth of the complexity. Recent work in reinforcement learning has included mechanisms of curiosity and intrinsic motivation to address scenarios where the rewards are sparse or deceptive [8]. In a scenario with very sparse extrinsic rewards, curiosity-driven exploration provides intrinsic reward signals that enable the agent to autonomously learn increasingly complex tasks.</p>
<p>Conclusion and Open Challenges</p>
<p>Continual learning represents a crucial but challenging component of artificial learning systems and autonomous agents operating on real-world data, which is typically non-stationary and temporally correlated. Additional research efforts are required to combine multiple methodologies that integrate a variety of factors observed in human learners for the development of agents and robots learning in an autonomous fashion.</p>
<p>Basic mechanisms of critical periods of development can be modeled to empirically determine convenient multilayered neural network architectures and initial patterns of connectivity that improve the performance of the model for subsequent learning tasks. Methods comprising curriculum and transfer learning are a fundamental feature for reusing previously acquired knowledge and skills to solve a problem in a novel domain by sharing low-and high-level representations. Approaches using intrinsic motivation are crucial for the self-generation of goals, leading to an empirical process of exploration and the progressive acquisition of increasingly complex skills in a continual fashion.</p>
<p>Figure 1 :
1Schematic view of the main components for the development of continual learning autonomous agents. Adapted with permission from[20].</p>
<p>When and where do we apply what we learn? a taxonomy for far transfer. S Barnett, S Ceci, Psychological Bulletin. 1282S. Barnett and S. Ceci. When and where do we apply what we learn? a taxonomy for far transfer. Psychological Bul- letin, 128:612-637, 2002. 1, 2</p>
<p>Intrinsic motivation and reinforcement learning. A Barto, Intrinsically Motivated Learning in Natural and Artificial Systems. Baldassarre, G., Mirolli, M.SpringerA. Barto. Intrinsic motivation and reinforcement learning. Baldassarre, G., Mirolli, M. (Eds.), Intrinsically Motivated Learning in Natural and Artificial Systems. Springer, 2013. 2</p>
<p>Developmental robotics: From babies to robots. A Cangelosi, M Schlesinger, MIT PressA. Cangelosi and M. Schlesinger. Developmental robotics: From babies to robots. MIT Press, 2015. 1</p>
<p>Lifelong machine learning: Second edition. Z Chen, B Liu, Morgan &amp; Claypool PublishersZ. Chen and B. Liu. Lifelong machine learning: Second edition. Morgan &amp; Claypool Publishers, 2018. 1</p>
<p>A theory of the discovery and predication of relational concepts. L Doumas, J Hummel, C Sandhofer, Psychological Review. 1152L. Doumas, J. Hummel, and C. Sandhofer. A theory of the discovery and predication of relational concepts. Psycholog- ical Review, 115:1-43, 2008. 2</p>
<p>Learning and development in neural networks: The importance of starting small. J L Elman, Cognition. 481J. L. Elman. Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71-99, 1993. 2</p>
<p>A bayesian approach to unsupervised one-shot learning of object categories. L Fei-Fei, R Fergus, P Perona, ICCV'03. Nice, FranceL. Fei-Fei, R. Fergus, and P. Perona. A bayesian ap- proach to unsupervised one-shot learning of object cate- gories. ICCV'03, Nice, France, 2003. 2</p>
<p>Intrinsically motivated goal exploration processes with automatic curriculum learning. S Forestier, Y Mollard, P.-Y Oudeyer, arXiv:1708.02190S. Forestier, Y. Mollard, and P.-Y. Oudeyer. Intrinsically mo- tivated goal exploration processes with automatic curriculum learning. arXiv:1708.02190, 2017. 2</p>
<p>Curiosity-driven development of tool use precursors: a computational model. S Forestier, P.-Y Oudeyer, Proceedings of the Annual Conference of the Cognitive Science Society. the Annual Conference of the Cognitive Science SocietyS. Forestier and P.-Y. Oudeyer. Curiosity-driven develop- ment of tool use precursors: a computational model. Pro- ceedings of the Annual Conference of the Cognitive Science Society, 2016. 2</p>
<p>Information seeking, curiosity and attention: Computational and neural mechanisms. J Gottlieb, P.-Y Oudeyer, M Lopes, A Baranes, Trends in Cognitive Science. 1711J. Gottlieb, P.-Y. Oudeyer, M. Lopes, and A. Baranes. In- formation seeking, curiosity and attention: Computational and neural mechanisms. Trends in Cognitive Science, 17(11):585-596, 2013. 2</p>
<p>A Graves, M G Bellemare, J Menick, R Munos, K Kavukcuoglu, arXiv:1704.03003Automated curriculum learning for neural networks. A. Graves, M. G. Bellemare, J. Menick, R. Munos, and K. Kavukcuoglu. Automated curriculum learning for neu- ral networks. arXiv:1704.03003, 2017. 2</p>
<p>Hybrid computing using a neural network with dynamic external memory. A Graves, G Wayne, M Reynolds, T Harley, I Danihelka, A Grabska-Barwinska, S G Colmenarejo, E Grefenstette, T Ramalho, J E Agapiou, Nature. 5382A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka, A. Grabska-Barwinska, S. G. Colmenarejo, E. Grefenstette, T. Ramalho, and J. e. a. Agapiou. Hybrid computing using a neural network with dynamic external memory. Nature, 538:471-476, 2016. 2</p>
<p>Neuroscience-inspired artificial intelligence. D Hassabis, D Kumaran, C Summerfield, M Botvinick, Neuron Review. 952D. Hassabis, D. Kumaran, C. Summerfield, and M. Botvinick. Neuroscience-inspired artificial intelli- gence. Neuron Review, 95(2):245-258, 2017. 1</p>
<p>R Kemker, M Mcclure, A Abitino, T Hayes, C Kanan, Measuring catastrophic forgetting in neural networks. AAAI'18. New Orleans, LAR. Kemker, M. McClure, A. Abitino, T. Hayes, and C. Kanan. Measuring catastrophic forgetting in neural net- works. AAAI'18, New Orleans, LA, 2018. 1</p>
<p>Flexible shaping: how learning in small steps helps. K A Krueger, P Dayan, Cognition. 1102K. A. Krueger and P. Dayan. Flexible shaping: how learning in small steps helps. Cognition, 110:380-394, 2009. 1, 2</p>
<p>Learning to detect unseen object classes by between-class attribute transfer. CVPR'09. C Lampert, H Nickisch, S Harmeling, Miami Beach, FloridaC. Lampert, H. Nickisch, and S. Harmeling. Learning to de- tect unseen object classes by between-class attribute transfer. CVPR'09, Miami Beach, Florida, 2009. 2</p>
<p>Gradient episodic memory for continual learning. D Lopez-Paz, M Ranzato, 17Long Beach, CAD. Lopez-Paz and M. Ranzato. Gradient episodic memory for continual learning. NIPS'17, Long Beach, CA, 2017. 2</p>
<p>The stabilityplasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects. M Mermillod, A Bugaiska, P Bonin, Frontiers in Psychology. 4504M. Mermillod, A. Bugaiska, and P. Bonin. The stability- plasticity dilemma: Investigating the continuum from catas- trophic forgetting to age-limited learning effects. Frontiers in Psychology, 4(504), 2013. 1</p>
<p>Wallace. Multisensory processes: A balancing act across the lifespan. M M Murray, D J Lewkowicz, A Amedi, M , Trends in Neurosciences. 391M. M. Murray, D. J. Lewkowicz, A. Amedi, and M. T. Wal- lace. Multisensory processes: A balancing act across the lifespan. Trends in Neurosciences, 39:567-579, 2016. 1</p>
<p>Continual lifelong learning with neural networks: A review. G I Parisi, R Kemker, J L Part, C Kanan, S Wermter, Neural Networks. 1132G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54-71, 2019. 1, 2</p>
<p>Neural plasticity across the lifespan. J D Power, B L Schlaggar, 2016. 1Wiley Interdisciplinary Reviews: Developmental Biology. 6216J. D. Power and B. L. Schlaggar. Neural plasticity across the lifespan. Wiley Interdisciplinary Reviews: Developmental Biology, 6(216), 2016. 1</p>
<p>Adult neurogenesis in brain repair: Cellular plasticity vs. cellular replacement. G Quadrato, M Y Elnaggar, S. Di Giovanni, Frontiers in Neuroscience. 817G. Quadrato, M. Y. Elnaggar, and S. Di Giovanni. Adult neurogenesis in brain repair: Cellular plasticity vs. cellular replacement. Frontiers in Neuroscience, 8(17), 2014. 1</p>
<p>Curious model-building control systems. J Schmidhuber, J. Schmidhuber. Curious model-building control systems. 1991. 2</p>
<p>Children creating core properties of language: Evidence from an emerging sign language in Nicaragua. A Senghas, S Kita, A Özyürek, Science. 3051A. Senghas, S. Kita, and A.Özyürek. Children creating core properties of language: Evidence from an emerging sign lan- guage in Nicaragua. Science, 305:1779-1782, 2004. 1</p>
<p>Exploring Robotic Minds: Actions, Symbols, and Consciousness a Self-Organizing Dynamic Phenomena. J Tani, Oxford University PressJ. Tani. Exploring Robotic Minds: Actions, Symbols, and Consciousness a Self-Organizing Dynamic Phenomena. Ox- ford University Press, 2016. 1</p>
<p>A survey of transfer learning. K Weiss, T M Khoshgoftaar, D.-D Wang, Journal of Big Data. 39K. Weiss, T. M. Khoshgoftaar, and D.-D. Wang. A survey of transfer learning. Journal of Big Data, 3(9), 2016. 2</p>            </div>
        </div>

    </div>
</body>
</html>
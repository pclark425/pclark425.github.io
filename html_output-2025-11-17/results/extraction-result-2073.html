<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2073 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2073</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2073</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-278769498</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.13259v3.pdf" target="_blank">From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and fundamentally redefining research processes and human-AI collaboration. This survey systematically charts this burgeoning field, placing a central focus on the changing roles and escalating capabilities of LLMs in science. Through the lens of the scientific method, we introduce a foundational three-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating autonomy and evolving responsibilities within the research lifecycle. We further identify pivotal challenges and future research trajectories such as robotic automation, self-improvement, and ethical governance. Overall, this survey provides a conceptual architecture and strategic foresight to navigate and shape the future of AI-driven scientific discovery, fostering both rapid innovation and responsible advancement. Github Repository: https://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2073.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2073.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist-v1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist (v1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based prototype system that autonomously brainstorms research ideas from templates and past work, generates code and experiments, and produces draft research outputs while using internal scoring and literature checks for pre-validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ai scientist: Towards fully automated open-ended scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist (v1)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based multi-agent research agent (agentic framework, template-based idea generator)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Artificial intelligence / general scientific research</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>research ideas/proposals, hypotheses, code, experiment plans, draft manuscripts</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel (brainstorms from templates and past work; novelty constrained by templates and prior literature)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM brainstorming from templates and past research, sampling-based generation within an agentic pipeline; internal heuristic scoring for diversity/interestingness</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Self-assessed internal scoring (interestingness, novelty, feasibility) and automated literature novelty checks (e.g., Semantic Scholar lookup); limited external/experimental validation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Described qualitatively as capable of generating diverse proposals; no quantitative success rates or standardized metrics reported in this survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation is performed via internal scoring and literature checks; no quantitative validation performance (accuracy/precision) reported in the survey for v1.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Survey notes internal checks for novelty but does not provide quantified relationships; implicitly, validation is weaker for more novel outputs (requires more literature evidence or human review).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Survey indicates generation (idea production) is stronger/more automated than rigorous validation — v1 uses lightweight automated checks and thus generation tends to outpace robust validation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Uses self-assessed heuristic scores (interestingness/feasibility) but no reported calibrated probabilistic uncertainty estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; no calibration metrics provided in the survey for v1's self-scores.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported in the survey; v1's approach (templates + literature) suggests limited OOD discovery capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — interestingness, novelty, and feasibility scores plus literature-match checks are used as proxies rather than direct experimental or formal validation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended for high-level vetting and for novel proposals; frequency increases with output novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Semi-formal (AI research outputs and proposals; empirical & conceptual content)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Internal AI reviewers and literature checks; VLM critiques for figures are used in some pipelines — these reduce but do not eliminate the generation-validation gap according to the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Survey states v1 produces diverse research outputs but relies on lightweight automated checks and external literature lookups, indicating generation capability exceeds rigorous validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Integration of external literature novelty checks and AI reviewers provides partial validation, narrowing the gap in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2073.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2073.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist-v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist (v2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evolution of AI Scientist that uses agentic tree-search and tighter integration with literature review tools to produce and assess workshop-level automated scientific discovery and proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist-v2</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based multi-agent agent with agentic tree-search planning</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Artificial intelligence / general scientific research</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>diverse research proposals, hypotheses, experiment plans, drafts of papers</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately to highly novel (uses tree-search to explore more diverse idea space beyond templates)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Agentic tree-search over idea/experiment space using LLMs to propose and expand nodes; multi-stage refinement within agent pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Integrates literature review tools early in idea formulation, internal AI reviewers, and multimodal critiques (e.g., VLM for figures); uses automated novelty checks against literature databases.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported qualitatively as generating more diverse and workshop-level proposals than v1; no numeric generation success rates provided in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Survey reports use of literature tools for early novelty checking, but quantitative validation metrics are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Survey implies validation relies on literature overlap — as novelty increases (less literature support), automated validation becomes weaker and external/human checks become necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Survey emphasizes increased generation power (tree-search exploration) while validation remains anchored to literature and internal heuristics — a widening gap for highly novel outputs is noted qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Uses internal scoring and review pipelines but no reported calibrated uncertainty estimates in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported; likely limited because validation depends on literature matching.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — novelty/interestingness scores and literature-match proxies are primary validation signals.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Higher for outputs deemed novel or high-impact; the system integrates human oversight at strategic points in practice per survey.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Semi-formal (research proposals spanning empirical & conceptual work).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Early literature integration, multi-agent internal reviewers, and VLM critiques to reduce spurious proposals; effectiveness described qualitatively but not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Survey highlights that v2 generates diverse novel proposals but still relies on literature-based validation and human oversight for high-novelty claims.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Early literature integration and AI reviewers are presented as partial remedies narrowing the gap in some workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2073.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2073.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReviewerGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReviewerGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploratory LLM-based system evaluated for automated paper reviewing, specifically tested on its ability to detect deliberately inserted errors in manuscripts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reviewergpt? an exploratory study on using large language models for paper reviewing.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReviewerGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Large language model used as an automated reviewer / critique generator</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scholarly peer review / scientific publishing</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>paper reviews, identification of errors, critique comments</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>In-distribution to moderately novel (critiques of scholarly text; novelty not the primary output dimension)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompted LLM generation to produce reviewer-style critiques and error detection based on input manuscripts.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluated via test papers containing deliberately inserted errors (ground-truth inserted faults) to measure detection capability.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Survey notes ReviewerGPT initially explored error identification but does not report numerical detection rates; original paper evaluated detection qualitatively/experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Measured in the original study by ability to detect inserted errors; survey does not provide numeric performance values.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not explicitly reported; likely performance degrades when errors are subtle or when evaluating highly novel methodological claims absent in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation of critique is straightforward; reliable validation (correctly distinguishing genuine vs. spurious claims) remains challenging per survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported in the survey; original work may provide confidence signals but not summarized here.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported here; likely reduced for novel paper content.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Proxy is detection of known-inserted errors (a controlled synthetic proxy for review quality).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended — LLM reviews seen as augmentative and needing human oversight for final decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical / textual (paper reviewing) — semi-formal.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Use of controlled inserted-error evaluation to benchmark reviewer effectiveness; suggests hybrid human-AI review workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Survey indicates LLMs can spot some injected errors but broader review tasks require more robust validation and human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Controlled inserted-error experiments provide evidence LLMs can validate some types of flaws, partially closing the gap for those tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2073.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2073.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explanation-Refiner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explanation-Refiner (LLM + theorem prover verification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that verifies and refines LLM-generated natural language explanations using symbolic theorem provers, enabling formal checking and iterative improvement of generated hypotheses/explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Verification and refinement of natural language explanations through llm-symbolic theorem proving.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Explanation-Refiner</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Neurosymbolic system: LLM integrated with symbolic theorem prover</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Formal reasoning / theorem proving / hypothesis verification</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>formal proofs, verified explanations, refined hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Varies — can be in-distribution for known theorem structures or highly novel when generating new conjectures; validation via theorem prover requires formalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM generates natural-language explanations/hypotheses which are translated and checked via symbolic theorem provers; iterative refinement guided by proof feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Formal verification using theorem provers to check logical consistency and correctness of generated claims; failed proofs feed back into refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not quantified in the survey; described as enabling verification+refinement loops that improve the trustworthiness of LLM outputs in formal domains.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Formal theorem proving provides strong binary verification signals for formalizable claims; survey does not include numeric verification rates.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Formal verification is effective when outputs can be formalized; for highly novel/unformalizable outputs, verification is limited or fails — survey implies validation weakens as claims deviate from formalizable space.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>This approach is presented as a strategy to close the gap: generation can propose conjectures, but formal verification provides definitive validation for formalizable outputs; the gap narrows in highly formal domains.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Theorem prover success/failure provides a form of certainty signal; LLM-level probabilistic uncertainty not detailed in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported quantitatively; formal proof success is a strong calibrator when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Limited: out-of-distribution conjectures that cannot be formalized or are beyond prover capabilities will not be validated successfully.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Uses proof success/failure (a direct validity signal) rather than soft proxies; when applicable, this avoids proxy-only validation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended for novel conjectures or when theorem prover fails; human intervention used to formalize statements or judge informal results.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Highly formal (mathematics / formal logic); this is a key enabling factor for theorem-prover-based validation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Direct formal verification with theorem provers and iterative refinement significantly mitigates generation-validation gap for formal domains.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Survey notes that iterative refinement is less common in literature; Explanation-Refiner is cited as an example where formal verification is used to validate and refine LLM outputs, implying such mechanisms are currently rare.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Use of theorem provers is presented as evidence that the generation-validation gap can be closed in formal domains.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not quantified; formal verification (theorem proving) typically has higher computational cost than raw generation, but no numeric ratio provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2073.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2073.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-SR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-SR (Scientific equation discovery via programming with LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach using LLMs to programmatically discover scientific equations (symbolic regression) by combining LLM priors with data-driven search and memory-feedback mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llm-sr: Scientific equation discovery via programming with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-SR</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-guided symbolic regression / program-synthesis pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Function discovery / symbolic regression / physics and other natural sciences</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>mathematical expressions/equations, symbolic models</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Moderately novel to out-of-distribution depending on data — aims to discover equations not explicitly present in training data</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM generates candidate symbolic expressions/programs; combined with memory/clustering feedback and data fitting to select candidate equations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Benchmarked against ground-truth functions and using function transformations to mitigate data contamination; validation involves checking symbolic equivalence or fit to held-out data.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Survey reports LLM-SR leverages domain priors and memory feedback to improve discovery; no numeric recovery rates are provided in the survey itself.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>LLM-SRBench (related benchmark) is used to evaluate recovery of ground-truth functions — survey does not reproduce numeric benchmark scores.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Benchmarks incorporate transformations to reduce contamination and hence test true discovery; survey implies recovery performance declines for more complex or OOD functions but provides no numeric trend.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Survey suggests LLM-based generation can propose plausible symbolic models, but rigorous validation requires function-matching and contamination controls; generation may propose spurious models without robust validation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>LLM-SRBench explicitly tests function generalization; survey indicates OOD performance is a challenge though specific metrics are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Uses symbolic match / fit to held-out data and transformations to detect contamination — these serve as direct validity checks rather than purely plausibility proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended for novel discovered equations or where benchmarking signals are weak.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Semi-formal to formal (equations are formal objects, enabling stronger validation than free-text claims).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>LLM-SRBench uses function transformations to mitigate data contamination and more robustly assess discovery — a method to reduce false positives from training-set memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Survey references the need for contamination mitigation and improved benchmarks, indicating generation can produce plausible but unvalidated equations without careful checks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Benchmarks like LLM-SRBench attempt to align generation with rigorous validation (function matching), showing paths to narrow the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2073.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2073.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DSBench (How far are data science agents from becoming data science experts?)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark that evaluates LLM-based data-analysis agents on real-world data science tasks using expert-curated analytics to measure agent capability relative to human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dsbench: How far are data science agents from becoming data science experts?.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DSBench (benchmark suite of data-analysis agents)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Benchmark (evaluative framework for LLM agents performing data analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Data science / empirical data analysis</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>analyses, reports, statistical models, visualizations, insights</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>NA (benchmark for agent performance across tasks; assesses both in-distribution and novel tasks depending on dataset selection)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not an agent itself — evaluates agents that generate analytic code/interpretations using LLMs and agent orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compares agent outputs against expert-curated analytics and expected analyses; evaluation includes task success, correctness of analysis, and adherence to analytical standards.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Survey summarizes that most LLM agents struggle on complex data analytics tasks per DSBench; no specific numeric metrics presented in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Measured by agreement with expert analytics in the benchmark; specific metric values not reported in this survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Survey notes agent performance depends strongly on task familiarity and complexity — performance degrades on more novel or complex analytic scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Agents can generate analytic outputs but often fail to reach expert-quality validation; generation capability does not equate to validated, expert-level analysis per the benchmark results summarized.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Benchmarks may include correctness checks and scoring but survey does not describe agent uncertainty quantification in DSBench.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Survey indicates OOD/novel analytic tasks reduce agent performance markedly; no numeric rates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Benchmark uses expert-curated analytics as ground truth rather than solely plausibility proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>High for complex or novel analyses; benchmark emphasizes gap to human expert performance.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical/empirical-statistical (data science); semi-formal</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>DS-Agent and DAgent (related works) propose methods like case-based reasoning and query decomposition to improve agent validation and domain knowledge acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Survey explicitly reports that most LLMs struggle with complex data analytics even in agentic frameworks, supporting a generation-validation gap against expert performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Proposed agent enhancements (case-based reasoning, decomposition) show potential to narrow the gap, but quantitative evidence not summarized here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2073.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2073.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-Bench / ScienceAgent-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-Bench / ScienceAgent-Bench (benchmarks for natural science agent tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmarks evaluating LLM agents on chemistry, biology, and social science tasks; report that LLMs perform well only on tasks with highly limited complexity and require specialized agent workflows for exploratory tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Auto-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Auto-Bench / ScienceAgent-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Benchmark suites for LLM agent evaluation in natural sciences</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry, biology, social sciences (multidisciplinary natural science research)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>experimental plans, causal graph discovery, hypotheses, analytic outputs</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Benchmarks probe both in-distribution and exploratory (novel) tasks; systems typically succeed only on low-complexity/in-distribution tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A (benchmarks evaluate LLM agents that generate experiment plans and hypotheses via agentic workflows and tool use)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Task-specific ground truth or curated evaluation criteria (e.g., causal graph correctness, feasibility of experimental plans); survey notes tailored evaluation is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Survey reports LLMs perform effectively only when task complexity is highly limited; no numerical metrics are provided in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation is task-dependent; survey indicates agents often fail to meet validation standards on complex exploratory tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Performance drops as task complexity and novelty increase; survey emphasizes need for specialized workflows for exploratory tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation of proposals/experiments is achievable, but validation (feasibility, safety, reproducibility) remains a bottleneck, especially for physical lab tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poorer on OOD/complex real-world natural-science tasks according to the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Benchmarks use task-specific proxies (causal-graph scores, feasibility checks) when direct experimental validation is infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>High for complex/novel experiments; human oversight particularly required for physical lab translation and safety checks.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical (chemistry/biology) — less formal than mathematics, so validation is experiment- or simulation-based.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Recommend tailored agent workflows and integration with domain-specific tools/robotics; early benchmarks highlight these as necessary but not yet fully effective.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Survey cites Auto-Bench findings that agents only perform well on limited-complexity tasks, implying generation capabilities exceed validation/robust execution in realistic natural-science contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None reported; benchmarks point to the need for more specialized validation rather than contradicting the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2073.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2073.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Paperbench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paperbench: Evaluating AI's ability to replicate AI research</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark and evaluation suite that probes whether AI systems (LLM agents) can reproduce AI research results and implementations from papers, focusing on reproducibility and replication of experiments/code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paperbench: Evaluating ai's ability to replicate ai research.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Paperbench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Benchmark for replication/reproduction of research (evaluative framework)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Artificial intelligence research reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>reproduced experiments, code implementations, replication reports</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>N/A (benchmark evaluates replication ability — OOD replication is a central challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A — evaluates agents that generate code and reproduction artifacts from papers</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compares agent-produced implementations/results to original papers' reported results and reproducibility criteria; metrics include successful replication of experiments and equivalence of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Survey cites Paperbench as evaluating replication ability; numerical replication rates are not reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Measured by replication success in original Paperbench work; survey does not list specific validation metrics or rates.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Replication of novel/unusual experimental setups is more challenging; survey implies performance decreases for less-standardized or novel experimental designs.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation (code and experimental artifacts) can be produced by agents, but validated replication often fails due to missing details, environment differences, or execution errors — indicating a generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported in summary.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Lower for papers with nonstandard or highly novel methods; specific metrics not provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Replication success and empirical equivalence to original reported results are used rather than plausibility-only proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>High — human engineers/researchers typically needed to interpret failures and complete reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical / experimental (AI research) — semi-formal.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Benchmarking replication encourages improved tooling, environment capture, and automated debugging to reduce the generation-validation gap; survey notes these as needed but not yet fully realized.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Survey references Paperbench and related replication efforts showing challenges for agents to fully replicate complex AI research experiments, supporting the existence of a generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None cited in the survey; replication work highlights challenges rather than contradiction.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery. <em>(Rating: 2)</em></li>
                <li>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. <em>(Rating: 2)</em></li>
                <li>Llm-srbench: A new benchmark for scientific equation discovery with large language models. <em>(Rating: 2)</em></li>
                <li>Llm-sr: Scientific equation discovery via programming with large language models. <em>(Rating: 2)</em></li>
                <li>Dsbench: How far are data science agents from becoming data science experts?. <em>(Rating: 2)</em></li>
                <li>Discoverybench: Towards data-driven discovery with large language models. <em>(Rating: 2)</em></li>
                <li>Mlagentbench: Evaluating language agents on machine learning experimentation. <em>(Rating: 2)</em></li>
                <li>Reviewergpt? an exploratory study on using large language models for paper reviewing. <em>(Rating: 2)</em></li>
                <li>Verification and refinement of natural language explanations through llm-symbolic theorem proving. <em>(Rating: 2)</em></li>
                <li>Paperbench: Evaluating ai's ability to replicate ai research. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2073",
    "paper_id": "paper-278769498",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "AI Scientist-v1",
            "name_full": "The AI Scientist (v1)",
            "brief_description": "An LLM-based prototype system that autonomously brainstorms research ideas from templates and past work, generates code and experiments, and produces draft research outputs while using internal scoring and literature checks for pre-validation.",
            "citation_title": "The ai scientist: Towards fully automated open-ended scientific discovery.",
            "mention_or_use": "mention",
            "system_name": "AI Scientist (v1)",
            "system_type": "LLM-based multi-agent research agent (agentic framework, template-based idea generator)",
            "scientific_domain": "Artificial intelligence / general scientific research",
            "output_type": "research ideas/proposals, hypotheses, code, experiment plans, draft manuscripts",
            "novelty_level": "moderately novel (brainstorms from templates and past work; novelty constrained by templates and prior literature)",
            "generation_method": "LLM brainstorming from templates and past research, sampling-based generation within an agentic pipeline; internal heuristic scoring for diversity/interestingness",
            "validation_method": "Self-assessed internal scoring (interestingness, novelty, feasibility) and automated literature novelty checks (e.g., Semantic Scholar lookup); limited external/experimental validation",
            "generation_performance": "Described qualitatively as capable of generating diverse proposals; no quantitative success rates or standardized metrics reported in this survey summary.",
            "validation_performance": "Validation is performed via internal scoring and literature checks; no quantitative validation performance (accuracy/precision) reported in the survey for v1.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Survey notes internal checks for novelty but does not provide quantified relationships; implicitly, validation is weaker for more novel outputs (requires more literature evidence or human review).",
            "generation_validation_comparison": "Survey indicates generation (idea production) is stronger/more automated than rigorous validation — v1 uses lightweight automated checks and thus generation tends to outpace robust validation.",
            "uncertainty_quantification": "Uses self-assessed heuristic scores (interestingness/feasibility) but no reported calibrated probabilistic uncertainty estimates.",
            "calibration_quality": "Not reported; no calibration metrics provided in the survey for v1's self-scores.",
            "out_of_distribution_performance": "Not reported in the survey; v1's approach (templates + literature) suggests limited OOD discovery capacity.",
            "validation_proxy_metrics": "Yes — interestingness, novelty, and feasibility scores plus literature-match checks are used as proxies rather than direct experimental or formal validation.",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended for high-level vetting and for novel proposals; frequency increases with output novelty.",
            "formal_verification_used": false,
            "domain_formalization_level": "Semi-formal (AI research outputs and proposals; empirical & conceptual content)",
            "gap_mitigation_strategies": "Internal AI reviewers and literature checks; VLM critiques for figures are used in some pipelines — these reduce but do not eliminate the generation-validation gap according to the survey.",
            "evidence_supporting_gap": "Survey states v1 produces diverse research outputs but relies on lightweight automated checks and external literature lookups, indicating generation capability exceeds rigorous validation.",
            "evidence_contradicting_gap": "Integration of external literature novelty checks and AI reviewers provides partial validation, narrowing the gap in some cases.",
            "computational_cost_ratio": null,
            "uuid": "e2073.0"
        },
        {
            "name_short": "AI Scientist-v2",
            "name_full": "The AI Scientist (v2)",
            "brief_description": "An evolution of AI Scientist that uses agentic tree-search and tighter integration with literature review tools to produce and assess workshop-level automated scientific discovery and proposals.",
            "citation_title": "The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search.",
            "mention_or_use": "mention",
            "system_name": "AI Scientist-v2",
            "system_type": "LLM-based multi-agent agent with agentic tree-search planning",
            "scientific_domain": "Artificial intelligence / general scientific research",
            "output_type": "diverse research proposals, hypotheses, experiment plans, drafts of papers",
            "novelty_level": "moderately to highly novel (uses tree-search to explore more diverse idea space beyond templates)",
            "generation_method": "Agentic tree-search over idea/experiment space using LLMs to propose and expand nodes; multi-stage refinement within agent pipeline.",
            "validation_method": "Integrates literature review tools early in idea formulation, internal AI reviewers, and multimodal critiques (e.g., VLM for figures); uses automated novelty checks against literature databases.",
            "generation_performance": "Reported qualitatively as generating more diverse and workshop-level proposals than v1; no numeric generation success rates provided in this survey.",
            "validation_performance": "Survey reports use of literature tools for early novelty checking, but quantitative validation metrics are not provided here.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Survey implies validation relies on literature overlap — as novelty increases (less literature support), automated validation becomes weaker and external/human checks become necessary.",
            "generation_validation_comparison": "Survey emphasizes increased generation power (tree-search exploration) while validation remains anchored to literature and internal heuristics — a widening gap for highly novel outputs is noted qualitatively.",
            "uncertainty_quantification": "Uses internal scoring and review pipelines but no reported calibrated uncertainty estimates in this survey.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not reported; likely limited because validation depends on literature matching.",
            "validation_proxy_metrics": "Yes — novelty/interestingness scores and literature-match proxies are primary validation signals.",
            "human_validation_required": true,
            "human_validation_frequency": "Higher for outputs deemed novel or high-impact; the system integrates human oversight at strategic points in practice per survey.",
            "formal_verification_used": false,
            "domain_formalization_level": "Semi-formal (research proposals spanning empirical & conceptual work).",
            "gap_mitigation_strategies": "Early literature integration, multi-agent internal reviewers, and VLM critiques to reduce spurious proposals; effectiveness described qualitatively but not quantified.",
            "evidence_supporting_gap": "Survey highlights that v2 generates diverse novel proposals but still relies on literature-based validation and human oversight for high-novelty claims.",
            "evidence_contradicting_gap": "Early literature integration and AI reviewers are presented as partial remedies narrowing the gap in some workflows.",
            "computational_cost_ratio": null,
            "uuid": "e2073.1"
        },
        {
            "name_short": "ReviewerGPT",
            "name_full": "ReviewerGPT",
            "brief_description": "An exploratory LLM-based system evaluated for automated paper reviewing, specifically tested on its ability to detect deliberately inserted errors in manuscripts.",
            "citation_title": "Reviewergpt? an exploratory study on using large language models for paper reviewing.",
            "mention_or_use": "mention",
            "system_name": "ReviewerGPT",
            "system_type": "Large language model used as an automated reviewer / critique generator",
            "scientific_domain": "Scholarly peer review / scientific publishing",
            "output_type": "paper reviews, identification of errors, critique comments",
            "novelty_level": "In-distribution to moderately novel (critiques of scholarly text; novelty not the primary output dimension)",
            "generation_method": "Prompted LLM generation to produce reviewer-style critiques and error detection based on input manuscripts.",
            "validation_method": "Evaluated via test papers containing deliberately inserted errors (ground-truth inserted faults) to measure detection capability.",
            "generation_performance": "Survey notes ReviewerGPT initially explored error identification but does not report numerical detection rates; original paper evaluated detection qualitatively/experimentally.",
            "validation_performance": "Measured in the original study by ability to detect inserted errors; survey does not provide numeric performance values.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not explicitly reported; likely performance degrades when errors are subtle or when evaluating highly novel methodological claims absent in training data.",
            "generation_validation_comparison": "Generation of critique is straightforward; reliable validation (correctly distinguishing genuine vs. spurious claims) remains challenging per survey summary.",
            "uncertainty_quantification": "Not reported in the survey; original work may provide confidence signals but not summarized here.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not reported here; likely reduced for novel paper content.",
            "validation_proxy_metrics": "Proxy is detection of known-inserted errors (a controlled synthetic proxy for review quality).",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended — LLM reviews seen as augmentative and needing human oversight for final decisions.",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical / textual (paper reviewing) — semi-formal.",
            "gap_mitigation_strategies": "Use of controlled inserted-error evaluation to benchmark reviewer effectiveness; suggests hybrid human-AI review workflows.",
            "evidence_supporting_gap": "Survey indicates LLMs can spot some injected errors but broader review tasks require more robust validation and human oversight.",
            "evidence_contradicting_gap": "Controlled inserted-error experiments provide evidence LLMs can validate some types of flaws, partially closing the gap for those tasks.",
            "computational_cost_ratio": null,
            "uuid": "e2073.2"
        },
        {
            "name_short": "Explanation-Refiner",
            "name_full": "Explanation-Refiner (LLM + theorem prover verification)",
            "brief_description": "A system that verifies and refines LLM-generated natural language explanations using symbolic theorem provers, enabling formal checking and iterative improvement of generated hypotheses/explanations.",
            "citation_title": "Verification and refinement of natural language explanations through llm-symbolic theorem proving.",
            "mention_or_use": "mention",
            "system_name": "Explanation-Refiner",
            "system_type": "Neurosymbolic system: LLM integrated with symbolic theorem prover",
            "scientific_domain": "Formal reasoning / theorem proving / hypothesis verification",
            "output_type": "formal proofs, verified explanations, refined hypotheses",
            "novelty_level": "Varies — can be in-distribution for known theorem structures or highly novel when generating new conjectures; validation via theorem prover requires formalizability.",
            "generation_method": "LLM generates natural-language explanations/hypotheses which are translated and checked via symbolic theorem provers; iterative refinement guided by proof feedback.",
            "validation_method": "Formal verification using theorem provers to check logical consistency and correctness of generated claims; failed proofs feed back into refinement.",
            "generation_performance": "Not quantified in the survey; described as enabling verification+refinement loops that improve the trustworthiness of LLM outputs in formal domains.",
            "validation_performance": "Formal theorem proving provides strong binary verification signals for formalizable claims; survey does not include numeric verification rates.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Formal verification is effective when outputs can be formalized; for highly novel/unformalizable outputs, verification is limited or fails — survey implies validation weakens as claims deviate from formalizable space.",
            "generation_validation_comparison": "This approach is presented as a strategy to close the gap: generation can propose conjectures, but formal verification provides definitive validation for formalizable outputs; the gap narrows in highly formal domains.",
            "uncertainty_quantification": "Theorem prover success/failure provides a form of certainty signal; LLM-level probabilistic uncertainty not detailed in survey.",
            "calibration_quality": "Not reported quantitatively; formal proof success is a strong calibrator when applicable.",
            "out_of_distribution_performance": "Limited: out-of-distribution conjectures that cannot be formalized or are beyond prover capabilities will not be validated successfully.",
            "validation_proxy_metrics": "Uses proof success/failure (a direct validity signal) rather than soft proxies; when applicable, this avoids proxy-only validation.",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended for novel conjectures or when theorem prover fails; human intervention used to formalize statements or judge informal results.",
            "formal_verification_used": true,
            "domain_formalization_level": "Highly formal (mathematics / formal logic); this is a key enabling factor for theorem-prover-based validation.",
            "gap_mitigation_strategies": "Direct formal verification with theorem provers and iterative refinement significantly mitigates generation-validation gap for formal domains.",
            "evidence_supporting_gap": "Survey notes that iterative refinement is less common in literature; Explanation-Refiner is cited as an example where formal verification is used to validate and refine LLM outputs, implying such mechanisms are currently rare.",
            "evidence_contradicting_gap": "Use of theorem provers is presented as evidence that the generation-validation gap can be closed in formal domains.",
            "computational_cost_ratio": "Not quantified; formal verification (theorem proving) typically has higher computational cost than raw generation, but no numeric ratio provided.",
            "uuid": "e2073.3"
        },
        {
            "name_short": "LLM-SR",
            "name_full": "LLM-SR (Scientific equation discovery via programming with LLMs)",
            "brief_description": "An approach using LLMs to programmatically discover scientific equations (symbolic regression) by combining LLM priors with data-driven search and memory-feedback mechanisms.",
            "citation_title": "Llm-sr: Scientific equation discovery via programming with large language models.",
            "mention_or_use": "mention",
            "system_name": "LLM-SR",
            "system_type": "LLM-guided symbolic regression / program-synthesis pipeline",
            "scientific_domain": "Function discovery / symbolic regression / physics and other natural sciences",
            "output_type": "mathematical expressions/equations, symbolic models",
            "novelty_level": "Moderately novel to out-of-distribution depending on data — aims to discover equations not explicitly present in training data",
            "generation_method": "LLM generates candidate symbolic expressions/programs; combined with memory/clustering feedback and data fitting to select candidate equations.",
            "validation_method": "Benchmarked against ground-truth functions and using function transformations to mitigate data contamination; validation involves checking symbolic equivalence or fit to held-out data.",
            "generation_performance": "Survey reports LLM-SR leverages domain priors and memory feedback to improve discovery; no numeric recovery rates are provided in the survey itself.",
            "validation_performance": "LLM-SRBench (related benchmark) is used to evaluate recovery of ground-truth functions — survey does not reproduce numeric benchmark scores.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Benchmarks incorporate transformations to reduce contamination and hence test true discovery; survey implies recovery performance declines for more complex or OOD functions but provides no numeric trend.",
            "generation_validation_comparison": "Survey suggests LLM-based generation can propose plausible symbolic models, but rigorous validation requires function-matching and contamination controls; generation may propose spurious models without robust validation.",
            "uncertainty_quantification": "Not described in the survey summary.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "LLM-SRBench explicitly tests function generalization; survey indicates OOD performance is a challenge though specific metrics are not provided here.",
            "validation_proxy_metrics": "Uses symbolic match / fit to held-out data and transformations to detect contamination — these serve as direct validity checks rather than purely plausibility proxies.",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended for novel discovered equations or where benchmarking signals are weak.",
            "formal_verification_used": null,
            "domain_formalization_level": "Semi-formal to formal (equations are formal objects, enabling stronger validation than free-text claims).",
            "gap_mitigation_strategies": "LLM-SRBench uses function transformations to mitigate data contamination and more robustly assess discovery — a method to reduce false positives from training-set memorization.",
            "evidence_supporting_gap": "Survey references the need for contamination mitigation and improved benchmarks, indicating generation can produce plausible but unvalidated equations without careful checks.",
            "evidence_contradicting_gap": "Benchmarks like LLM-SRBench attempt to align generation with rigorous validation (function matching), showing paths to narrow the gap.",
            "computational_cost_ratio": null,
            "uuid": "e2073.4"
        },
        {
            "name_short": "DSBench",
            "name_full": "DSBench (How far are data science agents from becoming data science experts?)",
            "brief_description": "A benchmark that evaluates LLM-based data-analysis agents on real-world data science tasks using expert-curated analytics to measure agent capability relative to human experts.",
            "citation_title": "Dsbench: How far are data science agents from becoming data science experts?.",
            "mention_or_use": "mention",
            "system_name": "DSBench (benchmark suite of data-analysis agents)",
            "system_type": "Benchmark (evaluative framework for LLM agents performing data analysis)",
            "scientific_domain": "Data science / empirical data analysis",
            "output_type": "analyses, reports, statistical models, visualizations, insights",
            "novelty_level": "NA (benchmark for agent performance across tasks; assesses both in-distribution and novel tasks depending on dataset selection)",
            "generation_method": "Not an agent itself — evaluates agents that generate analytic code/interpretations using LLMs and agent orchestration.",
            "validation_method": "Compares agent outputs against expert-curated analytics and expected analyses; evaluation includes task success, correctness of analysis, and adherence to analytical standards.",
            "generation_performance": "Survey summarizes that most LLM agents struggle on complex data analytics tasks per DSBench; no specific numeric metrics presented in the survey.",
            "validation_performance": "Measured by agreement with expert analytics in the benchmark; specific metric values not reported in this survey summary.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Survey notes agent performance depends strongly on task familiarity and complexity — performance degrades on more novel or complex analytic scenarios.",
            "generation_validation_comparison": "Agents can generate analytic outputs but often fail to reach expert-quality validation; generation capability does not equate to validated, expert-level analysis per the benchmark results summarized.",
            "uncertainty_quantification": "Benchmarks may include correctness checks and scoring but survey does not describe agent uncertainty quantification in DSBench.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Survey indicates OOD/novel analytic tasks reduce agent performance markedly; no numeric rates provided.",
            "validation_proxy_metrics": "Benchmark uses expert-curated analytics as ground truth rather than solely plausibility proxies.",
            "human_validation_required": true,
            "human_validation_frequency": "High for complex or novel analyses; benchmark emphasizes gap to human expert performance.",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical/empirical-statistical (data science); semi-formal",
            "gap_mitigation_strategies": "DS-Agent and DAgent (related works) propose methods like case-based reasoning and query decomposition to improve agent validation and domain knowledge acquisition.",
            "evidence_supporting_gap": "Survey explicitly reports that most LLMs struggle with complex data analytics even in agentic frameworks, supporting a generation-validation gap against expert performance.",
            "evidence_contradicting_gap": "Proposed agent enhancements (case-based reasoning, decomposition) show potential to narrow the gap, but quantitative evidence not summarized here.",
            "computational_cost_ratio": null,
            "uuid": "e2073.5"
        },
        {
            "name_short": "Auto-Bench / ScienceAgent-Bench",
            "name_full": "Auto-Bench / ScienceAgent-Bench (benchmarks for natural science agent tasks)",
            "brief_description": "Benchmarks evaluating LLM agents on chemistry, biology, and social science tasks; report that LLMs perform well only on tasks with highly limited complexity and require specialized agent workflows for exploratory tasks.",
            "citation_title": "Auto-Bench",
            "mention_or_use": "mention",
            "system_name": "Auto-Bench / ScienceAgent-Bench",
            "system_type": "Benchmark suites for LLM agent evaluation in natural sciences",
            "scientific_domain": "Chemistry, biology, social sciences (multidisciplinary natural science research)",
            "output_type": "experimental plans, causal graph discovery, hypotheses, analytic outputs",
            "novelty_level": "Benchmarks probe both in-distribution and exploratory (novel) tasks; systems typically succeed only on low-complexity/in-distribution tasks",
            "generation_method": "N/A (benchmarks evaluate LLM agents that generate experiment plans and hypotheses via agentic workflows and tool use)",
            "validation_method": "Task-specific ground truth or curated evaluation criteria (e.g., causal graph correctness, feasibility of experimental plans); survey notes tailored evaluation is necessary.",
            "generation_performance": "Survey reports LLMs perform effectively only when task complexity is highly limited; no numerical metrics are provided in the survey summary.",
            "validation_performance": "Validation is task-dependent; survey indicates agents often fail to meet validation standards on complex exploratory tasks.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Performance drops as task complexity and novelty increase; survey emphasizes need for specialized workflows for exploratory tasks.",
            "generation_validation_comparison": "Generation of proposals/experiments is achievable, but validation (feasibility, safety, reproducibility) remains a bottleneck, especially for physical lab tasks.",
            "uncertainty_quantification": "Not described in the survey summary.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Poorer on OOD/complex real-world natural-science tasks according to the survey.",
            "validation_proxy_metrics": "Benchmarks use task-specific proxies (causal-graph scores, feasibility checks) when direct experimental validation is infeasible.",
            "human_validation_required": true,
            "human_validation_frequency": "High for complex/novel experiments; human oversight particularly required for physical lab translation and safety checks.",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical (chemistry/biology) — less formal than mathematics, so validation is experiment- or simulation-based.",
            "gap_mitigation_strategies": "Recommend tailored agent workflows and integration with domain-specific tools/robotics; early benchmarks highlight these as necessary but not yet fully effective.",
            "evidence_supporting_gap": "Survey cites Auto-Bench findings that agents only perform well on limited-complexity tasks, implying generation capabilities exceed validation/robust execution in realistic natural-science contexts.",
            "evidence_contradicting_gap": "None reported; benchmarks point to the need for more specialized validation rather than contradicting the gap.",
            "computational_cost_ratio": null,
            "uuid": "e2073.6"
        },
        {
            "name_short": "Paperbench",
            "name_full": "Paperbench: Evaluating AI's ability to replicate AI research",
            "brief_description": "A benchmark and evaluation suite that probes whether AI systems (LLM agents) can reproduce AI research results and implementations from papers, focusing on reproducibility and replication of experiments/code.",
            "citation_title": "Paperbench: Evaluating ai's ability to replicate ai research.",
            "mention_or_use": "mention",
            "system_name": "Paperbench",
            "system_type": "Benchmark for replication/reproduction of research (evaluative framework)",
            "scientific_domain": "Artificial intelligence research reproducibility",
            "output_type": "reproduced experiments, code implementations, replication reports",
            "novelty_level": "N/A (benchmark evaluates replication ability — OOD replication is a central challenge)",
            "generation_method": "N/A — evaluates agents that generate code and reproduction artifacts from papers",
            "validation_method": "Compares agent-produced implementations/results to original papers' reported results and reproducibility criteria; metrics include successful replication of experiments and equivalence of outputs.",
            "generation_performance": "Survey cites Paperbench as evaluating replication ability; numerical replication rates are not reported in the survey.",
            "validation_performance": "Measured by replication success in original Paperbench work; survey does not list specific validation metrics or rates.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Replication of novel/unusual experimental setups is more challenging; survey implies performance decreases for less-standardized or novel experimental designs.",
            "generation_validation_comparison": "Generation (code and experimental artifacts) can be produced by agents, but validated replication often fails due to missing details, environment differences, or execution errors — indicating a generation-validation gap.",
            "uncertainty_quantification": "Not reported in summary.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Lower for papers with nonstandard or highly novel methods; specific metrics not provided in survey.",
            "validation_proxy_metrics": "Replication success and empirical equivalence to original reported results are used rather than plausibility-only proxies.",
            "human_validation_required": true,
            "human_validation_frequency": "High — human engineers/researchers typically needed to interpret failures and complete reproduction.",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical / experimental (AI research) — semi-formal.",
            "gap_mitigation_strategies": "Benchmarking replication encourages improved tooling, environment capture, and automated debugging to reduce the generation-validation gap; survey notes these as needed but not yet fully realized.",
            "evidence_supporting_gap": "Survey references Paperbench and related replication efforts showing challenges for agents to fully replicate complex AI research experiments, supporting the existence of a generation-validation gap.",
            "evidence_contradicting_gap": "None cited in the survey; replication work highlights challenges rather than contradiction.",
            "computational_cost_ratio": null,
            "uuid": "e2073.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery.",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search.",
            "rating": 2
        },
        {
            "paper_title": "Llm-srbench: A new benchmark for scientific equation discovery with large language models.",
            "rating": 2
        },
        {
            "paper_title": "Llm-sr: Scientific equation discovery via programming with large language models.",
            "rating": 2
        },
        {
            "paper_title": "Dsbench: How far are data science agents from becoming data science experts?.",
            "rating": 2
        },
        {
            "paper_title": "Discoverybench: Towards data-driven discovery with large language models.",
            "rating": 2
        },
        {
            "paper_title": "Mlagentbench: Evaluating language agents on machine learning experimentation.",
            "rating": 2
        },
        {
            "paper_title": "Reviewergpt? an exploratory study on using large language models for paper reviewing.",
            "rating": 2
        },
        {
            "paper_title": "Verification and refinement of natural language explanations through llm-symbolic theorem proving.",
            "rating": 2
        },
        {
            "paper_title": "Paperbench: Evaluating ai's ability to replicate ai research.",
            "rating": 2
        }
    ],
    "cost": 0.02212175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery
17 Sep 2025</p>
<p>Tianshi Zheng 
Zheye Deng 
Ting Hong 
Weiqi Tsang 
Jiaxin Wang 
Zihao Bai 
Yangqiu Wang 
Song 
Franck Cappello 
Sandeep Madireddy 
Neil Getty 
Nicholas Lee-Ping Chia 
Nesar Ramachandra 
Josh Nguyen 
Murat Keceli 
Tanwi Mallick 
Zilinghan Li 
Marieme Ngom 
Chenhui Zhang 
Angel Yanguas-Gil 
Evan Antoniuk 
Bhavya Kailkhura 
Minyang Tian 
Yufeng Du 
Thomas Carta 
Clément Romac 
Thomas Wolf 
Sylvain Lamprier 
Olivier Sigaud 
Pierre-Yves Oudeyer 
Jun Shern Chan 
Neil Chowdhury 
Oliver Jaffe 
James Aung 
Dane Sherburn 
Evan Mays 
Giulio Starace 
Kevin Liu 
Leon Maksin 
Tejal Patwardhan 
Lilian 
Ziru Chen 
Shijie Chen 
Yuting Ning 
Qianheng Zhang 
Boshi Wang 
Botao Yu 
Yifei Li 
Zeyi Liao 
Chen Wei 
Zitong Lu 
Vishal Dey 
Mingyi Xue 
Frazier N Baker 
Benjamin Burns 
Ming-Chang Cheng 
Yixuan Wang 
Yuchi Wang 
Yair Schiff 
Kevin Bello 
J Zico Kolter 
Yacine Jernite 
William Yang Wang 
Daniel Fried 
Tian-Li Yu 
Andrew Gordon Wilson 
Ioana Ciucȃ 
Yuan-Sen Ting 
Sandor Kruk 
Kartheik 2023 Iyer 
Kourosh Darvish 
Marta Skreta 
Yuchi Zhao 
Naruki Yoshikawa 
Sagnik Som 
Miroslav Bogdanovic 
Yang Cao 
Han Hao 
Haoping Xu 
Alán Aspuru-Guzik 
Fabio Dennstädt 
Johannes Zink 
Paul Martin Putora 
Janna Hastings 
Nikola 2024 Cihoric 
Jiangshu Du 
Yibo Wang 
Wenting Zhao 
Zhongfen Deng 
Shuaiqi Liu 
Renze Lou 
PranavHenry Peng Zou 
Narayanan Venkit 
MukundNan Zhang 
Ranran Zhang 
Vipul Gupta 
Yinghui Li 
Tao Li 
Fei Wang 
Qin Liu 
Tianlin Liu 
Pengzhi Gao </p>
<p>Department of Computer Science and Engineering
HKUST
Hong Kong SARChina</p>
<p>https://github.com/HKUST
KnowComp/Awesome-LLM-Scientific-Discovery</p>
<p>Daniel Adu-Ampratwum
Song GaoXuhui Huang, Xia Ning, Yu Su</p>
<p>From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery
17 Sep 2025E242DB66DCCACDA2B57AD40A1450C0C6arXiv:2505.13259v3[cs.CL]Literature Search and RetrievalLiterature SynthesisStructuring and Organization table: Towards information integration in text-to-table generation via global tuple extraction. PreprintarXiv:2404.14215
Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and fundamentally redefining research processes and human-AI collaboration.This survey systematically charts this burgeoning field, placing a central focus on the changing roles and escalating capabilities of LLMs in science.Through the lens of the scientific method, we introduce a foundational three-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating autonomy and evolving responsibilities within the research lifecycle.We further identify pivotal challenges and future research trajectories such as robotic automation, self-improvement, and ethical governance.Overall, this survey provides a conceptual architecture and strategic foresight to navigate and shape the future of AI-driven scientific discovery, fostering both rapid innovation and responsible advancement.</p>
<p>Introduction</p>
<p>The relentless advancement of Large Language Models (LLMs) has unlocked a suite of emergent abilities, such as planning (Huang et al., 2024b), complex reasoning (Huang and Chang, 2023), and instruction following (Qin et al., 2024).Moreover, integrating agentic workflows enables LLM-based systems to perform advanced functions, including web navigation (He et al., 2024), tool use (Qu et al., 2025), code execution (Jiang et al., 2024a), and data analytics (Sun et al., 2024).In scientific discovery, this convergence of advanced LLM capabilities and agentic functionalities is catalyzing a significant paradigm shift.This shift is poised not only to accelerate the research lifecycle but also to fundamentally alter the collaborative dynamics between human researchers and artificial intelligence in the pursuit of knowledge.</p>
<p>However, this rapid expansion of LLM applications and the ongoing paradigm shift in scientific discovery present notable challenges.The accelerated pace of LLM evolution and their deepening integration into complex research complicate systematic assessment, necessitating conceptual frameworks to structure current understanding and chart future directions.While existing surveys have provided valuable overviews of LLMs in various scientific domains (Zhang et al., 2024(Zhang et al., , 2025a) ) or have cataloged particular AI techniques for science (Luo et al., 2025;Reddy and Shojaee, 2025), they often focus on discipline-specific applications or a static snapshot of LLM capabilities.Consequently, existing reviews may overlook the crucial trend of increasing LLM autonomy and their evolving roles across the entire scientific method, leaving their comprehensive impact and trajectory towards greater independence underexplored.</p>
<p>To systematically chart this evolving landscape and address the identified gap, we anchor our analysis in the six stages (Figure 1) of the established scientific method (Popper, 1935;Kuhn, 1962): (1) observation and problem definition, (2) hypothesis development, (3) experimentation and data collection, (4) data analysis and interpretation, (5) drawing conclusions, and ( 6) iteration and refinement.Our examination of LLM applications across these stages reveals a significant trend: LLMs are progressing from performing discrete, task-oriented functions within a single stage to deployment in sophisticated, multi-stage agentic workflows.Notably, emerging research (Schmidgall et al., 2025;Yamada et al., 2025) now explores developing LLM-based systems capable of autonomously navigating nearly all these stages.To effectively capture and delineate this trajectory of increasing capability and independence, we introduce a foundational three-level taxonomy for LLM involvement in scientific discovery (Table 1): (i) LLM as Tool, where models augment human researchers by performing specific, well-defined tasks under direct supervision; (ii) LLM as Analyst, where models exhibit greater autonomy in processing complex information, conducting analyses, and offering insights with reduced human intervention; and (iii) LLM as Scientist, representing a more advanced stage where LLM-based systems can autonomously conduct major research stages, from formulating hypotheses to interpreting results and suggesting new avenues of inquiry.</p>
<p>Building upon this taxonomic framework, we further identify critical gaps in the current research landscape and highlight pivotal challenges and future trajectories for the field, including: (1) fully autonomous discovery cycles for evolving scientific inquiry without human intervention; (2) robotic automation for interaction in the physical world for laboratory experimentation; (3) continuous selfimprovement and adaptation from past research experiences; (4) transparency and interpretability of LLM-conducted research; and (5) ethical governance and societal alignment.Addressing these multifaceted challenges will be crucial for achieving a future where AI acts as a transformative partner in scientific exploration.</p>
<p>This survey focuses on LLM-based systems in scientific discovery, particularly their varying levels of autonomy.While acknowledging the broad impact of LLMs in science, we deliberately narrow our scope to exclude research on general-purpose scientific LLMs or LLMs for domain-specific scientific knowledge acquisition and reasoning, which are well covered in existing surveys (Zhang et al., 2024(Zhang et al., , 2025a)).The remainder of this paper is organized as follows: Section 2 details our taxonomy and its interaction with the scientific method.Section 3 presents LLM as Tool applications, categorized by scientific method stages.Section 4 examines LLM as Analyst works by scientific domain, while Section 5 analyzes LLM as Scientist systems, focusing on their idea development and refinement strategies.Section 6 explores challenges and future directions.</p>
<p>Three Levels of Autonomy</p>
<p>Table 1 illustrates the three levels of autonomy in LLM-based scientific discovery with their associated features.In this section, we discuss their applications and characteristics in more detail.</p>
<p>LLM as Tool (Level 1).Level 1 represents the most foundational application of LLMs in scientific discovery.At this stage, LLMs function primarily as tailored tools under direct human supervision, designed to execute specific, well-defined tasks within a single stage of the scientific method.Their role is to augment human capabilities by automating or accelerating discrete activities such as literature summarization, drafting initial text for manuscripts, generating code snippets for data processing, or reformatting datasets.The autonomy of LLMs at this level is limited; they operate based on explicit human prompts and instructions, with outputs typically requiring human validation and integration into the broader research workflow.The primary goal is to enhance researcher efficiency and reduce routine task burdens.</p>
<p>LLM as Analyst (Level 2).In Level 2, LLMs exhibit a greater degree of autonomy and move beyond purely static, task-oriented applications.Here, LLMs function as passive agents, capable of more  (Jiang et al., 2024) ArxivDIGESTables (Newman et al., 2024) arXiv2Table (Wang et al., 2025)</p>
<p>Machine Learning Research</p>
<p>The AI Scientist (Lu et al., 2024) The AI Scientist-v2 (Yamada et al., 2025) ProtAgents (Ghafarollahi and Buehler, 2024) Agent Laboratory (Schmidgall et al., (Yin et al., 2022) LeGIT (Li et al., 2025) ChartQA (Masry et al., 2022) CharXiv (Wang et al., 2024) TableBench (Wu et al., 2024) Deng et al., 2024 Chain-of-Table (Wang et al., 2024) ChartX &amp; ChartVLM (Xia et al., 2024) AutomaTikZ (Belouadi et al., 2023) Text2Chart31 (Zadeh et al., 2024) ClaimCheck (Ou et al., 2025) ReviewCritique (Du et al., 2024) ReviewerGPT (Liu et al., 2023) RR-MCQ (Zhou et al., 2024) SciReplicate-Bench (Xiang et al., 2025) Tyser et al., 2024Takagi et al., 2023 CycleResearcher (Weng et al., 2024) Xu et al., 2025 Explanation-Refiner (Quan et al., 2024) Chain-of-Ideas (Li et al., 2024) MC-NEST (Rabby et al., 2025) Fully-Autonomous Cycle Self-Improvement Robotic Automation Transparency Ethics . . .LLM-SR (Shojaee et al., 2024) LLM-SRBench (Shojaee et al., 2025) Gravity-Bench-v1 (Koblischke et al., 2025) BoxLM (Li et al., 2024) InfiAgent-DABench (Hu et al., 2024) DS-Agent (Guo et al., 2024) BLADE (Gu et al., 2024) DAgent (Xu et al., 2025</p>
<p>LLM as Scientist</p>
<p>LLM as Analyst</p>
<p>LLM as Tool</p>
<p>Evolution of LLM-Based Scientific Discovery</p>
<p>FutureHouse (Skarlinski et al., 2025) PaperQA (Lála et al., 2023) Science Hierarchography (Gao et al., 2025) AI co-scientist (Gottweis et al., 2025) MLRC-Bench (Zhang et al., 2025) RE-Bench (Wijk et al., 2025) Figure 2: Taxonomy of research works in LLM-based scientific discovery with detailed categorization.complex information processing, data modeling, and analytical reasoning with reduced human intervention for intermediate steps.While still operating within boundaries set by human researchers, these systems can independently manage sequences of tasks, such as analyzing experimental datasets to identify trends, interpreting outputs from complex simulations, or even performing iterative refinement of models.The human researcher typically defines the overall analytical goals, provides the necessary data, and critically evaluates the insights or interpretations generated by the LLM.</p>
<p>LLM as Scientist (Level 3).Level 3 applications signify a significant leap in autonomy, where LLM-based systems operate as active agents capable of orchestrating and navigating multiple stages of the scientific discovery process with considerable independence.These systems can demonstrate initiative in formulating hypotheses, planning and executing experiments, analyzing the resultant data, drawing preliminary conclusions, and potentially proposing subsequent research questions or avenues for exploration.LLM-based systems at this level can drive substantial portions of the research cycle, conducting scientific discovery with minimal human intervention.</p>
<p>Collectively, we present our full taxonomy with detailed categorization in Figure 2, which consol-idates research works within our focused scope across all three levels of autonomy.</p>
<p>3 Level 1. LLM as Tool (Table A1)</p>
<p>In this section, we introduce Level 1 research works in LLM-based scientific discovery, categorized by the stages in the scientific method they address.</p>
<p>Literature Review and Information Gathering</p>
<p>Literature Review Automatic literature search and retrieval is crucial for identifying research gaps and formulating research questions.To address these challenges, AIDE (Jiang et al., 2025) proposed enhancing complex code generation capabilities by adopting tree-search methodologies for code optimization.</p>
<p>Data Analysis and Organization</p>
<p>Tabular Data In this stage, LLMs assist the scientific workflow by automating processes related to data organization, presentation, and analysis.For data presented in tabular format, Chain-of-</p>
<p>Conclusion and Hypothesis Validation</p>
<p>In the concluding stages of research, LLMs can provide feedback on, or verify, claims and conclusions derived from experiments.</p>
<p>Paper Review In this context, a significant focus of contemporary research involves investigating the utility of LLMs as reviewers for artificial intelligence papers.ReviewerGPT (Liu and Shah, 2023) initially explored the capability of LLMs to identify deliberately inserted errors within research papers, highlighting the necessity for more robust systems to conduct comprehensive reviews.(Wen et al., 2025).Furthermore, Xu et al. (2025c) have navigated this domain into physics research, aiming to enhance the interpretability of the discovery process through the use of multi-agent workflows.</p>
<p>Iteration and Refinement</p>
<p>The iterative refinement of research hypotheses, as a distinct area of investigation, has received comparatively less attention in current research.Explanation-Refiner (Quan et al., 2024) employed theorem provers to verify and subsequently refine LLM-generated hypotheses.Chain-of-Idea (Li et al., 2024a)  4 Level 2: LLM as Analyst (Table A2)</p>
<p>In this section, we introduce Level 2 research works in LLM-based scientific discovery, categorized according to their task nature and domains.</p>
<p>Machine Learning Research</p>
<p>Automated Machine Learning (AutoML) (Shen et al., 2024) endeavors to generate high-performing modeling configurations for a given task in a datadriven manner.With the advent of LLM-based agents, several studies have explored their application in the automated modeling of machine learning (ML) tasks.A suite of benchmarks has emerged to track progress in this area.MLAgentBench (Huang et al., 2024a) evaluates the capabilities of LLMs in designing and executing ML experiments, revealing that performance is often contingent upon task familiarity.Similarly, MLRC-Bench (Zhang et al., 2025b) and RE-Bench (Wijk et al., 2024) further probe the limits of these agents, assessing their ability to solve novel ML research challenges and comparing their R&amp;D capabilities against human experts.MLGym (Nathani et al., 2025) offers valuable resource and benchmark for advancing these AI research agents.</p>
<p>To address the challenges posed by these benchmarks, various agentic frameworks have been proposed.The IMPROVE framework (Xue et al., 2025) highlighted the significance of iterative refinement mechanisms.CodeScientist (Jansen et al., 2025) incorporated an ML modeling agent with machine-generated ideas, while BudgetMLAgent (Gandhi et al., 2025) leveraged curated expert collaboration frameworks to achieve superior results with cost-effective models.More recent end-to-end systems like MLR-Copilot (Li et al., 2024d) and the multi-agent framework MLZero (Fang et al., 2025) aim for fully autonomous machine learning research and automation.Pushing the boundaries of automation even further, some work has explored the use of language models to directly propose LM architectures (Cheng et al., 2025a), moving beyond orchestration to direct model creation.</p>
<p>Data Modeling and Analysis</p>
<p>Automated data-driven analysis, encompassing statistical data modeling and hypothesis validation, represents a foundational application area for LLMassisted scientific discovery.InfiAgent-DABench (Hu et al., 2024b) benchmarked the capabilities of LLMs in static code generation and execution for data analysis using CSV files.Subsequent benchmarks, such as BLADE (Gu et al., 2024), Discov-eryBench (Majumder et al., 2024), and DSBench (Jing et al., 2024), have improved evaluation robustness by incorporating real-world research papers and expert-curated analytics to assess how far agents are from human expert performance.These studies indicate that most LLMs struggle with com-plex data analytics tasks, even when operating within an agent framework (Zheng et al., 2023).To address these challenges, DS-Agent (Guo et al., 2024b) proposes to enhance LLM performance by incorporating a case-based reasoning method to improve domain knowledge acquisition.In a related effort, DAgent (Xu et al., 2025b) extended the application domain to querying relational databases and enabled report generation using results derived from decomposed sub-problems.</p>
<p>Function Discovery</p>
<p>Function discovery, which aims to identify the underlying equations from observational data of variables, has been significantly influenced by the advancement of AI-driven symbolic regression (SR) (Udrescu and Tegmark, 2020;Kamienny et al., 2022).To enhance this process, LLM-SR (Shojaee et al., 2025a) leveraged the prior domain knowledge of LLMs and incorporated feedback from clustered memory storage, while DrSR (Wang et al., 2025a) proposed a dual reasoning framework that utilizes both data and experience for scientific equation discovery.To systematically assess these capabilities, LLM-SRBench (Shojaee et al., 2025b) introduced a benchmark for evaluating LLMs as function discovery agents, which incorporates function transformations to mitigate data contamination.Furthermore, other studies have explored the capabilities of LLMs in discovering complex models within specific domains, such as Physics (Koblischke et al., 2025), Statistics (Li et al., 2024b), and automated neural scaling law discovery (Lin et al., 2025).</p>
<p>Natural Science Research</p>
<p>Research has largely focused on applying LLMs to autonomous research workflows for natural science discovery.Auto-Bench (Chen et al., 2025b) evaluated LLMs on chemistry and social science tasks based on causal graph discovery, revealing that LLMs perform effectively only when task complexity is highly limited.In contrast, ScienceAgent-Bench (Chen et al., 2025c) provided a multidisciplinary benchmark for LLMs operating within agent frameworks such as CodeAct (Wang et al., 2024b) and self-debug (Chen et al., 2023).This benchmark highlighted the necessity for tailored agent workflows for such explorative tasks.</p>
<p>In the biomedical domain, Gao et al. ( 2024) discussed potential applications of AI agents in brainstorming, experimental planning, and execution.</p>
<p>BioResearcher (Luo et al., 2024) proposed an endto-end framework for biomedical research involving dry lab experiments.DrugAgent (Liu et al., 2025b) adopted multi-agent collaboration to automate drug discovery.In chemistry, Coscientist (Boiko et al., 2023) incorporated the use of tools by LLMs to support semi-autonomous chemistry experiment design and execution.ProtAgents (Ghafarollahi and Buehler, 2024a) facilitated biochemistry discovery by building a multi-agent framework for automating protein design.Recent works, such as FutureHouse (Skarlinski et al., 2025) and AI Co-scientist (Gottweis et al., 2025), contributed to formulating demonstrably novel research hypotheses and proposals using multi-agent systems guided by predefined research goals.</p>
<p>General Research</p>
<p>Apart from specialized domain applications, some benchmarks have broadly evaluated diverse tasks from different stages of scientific discovery.Dis-coveryWorld (Jansen et al., 2024) created a virtual environment for LLM agents to conduct simplified scientific exploration.In (Liu et al., 2025a), various application scenarios for AI agents in research were comprehensively discussed, supported by preliminary evaluation datasets.Similarly, CURIE (Kon et al., 2025) proposed a benchmark and an agentic framework for rigorous and automated scientific experimentation.While EAIRA (Cappello et al., 2025) focused on assessing the ability of LLMs to perform in a real-world research assistant role using various task formats.</p>
<p>5 Level 3. LLM as Scientist (Table A3)</p>
<p>Recently, several research efforts and commercial products have demonstrated prototypes of fully autonomous research within the artificial intelligence domain.These systems typically encompass a comprehensive workflow, from initial literature review to iterative refinement cycles where hypotheses or designs are progressively improved.A common feature is using an agent-based framework to autonomously produce research outputs, often culminating in draft research papers.This section will further compare these approaches, focusing on their methodologies for idea development and iterative refinement, as these aspects critically distinguish them from Level 2 agents.</p>
<p>Idea Development</p>
<p>The genesis of research in Level 3 systems involves transforming initial concepts into validated hypotheses, with distinct approaches to sourcing and vetting these ideas.Agent Laboratory (Schmidgall et al., 2025) predominantly conducts literature reviews based on human-defined research objectives.Moving towards greater autonomy, several systems initiate their process from broader human inputs, such as reference papers (Data Intelligence Lab, 2025; Autoscience, 2025) or general research domains (IntologyAI, 2025), subsequently exploring literature to autonomously identify gaps and formulate novel hypotheses.The AI Scientist (v1 (Lu et al., 2024) and v2 (Yamada et al., 2025)) showcases an even more generative approach: v1 brainstorms ideas from templates and past work, while v2 can generate diverse research proposals from abstract thematic prompts.Crucially, these systems employ diverse methods to evaluate their ideas prior to full implementation.AI Scientist-v1 uses self-assessed scores for interestingness, novelty, and feasibility, supplemented by external checks with Semantic Scholar.AI Scientist-v2 integrates literature review tools early in its idea formulation stage to assess novelty.This spectrum reveals a clear trend: while humans often initiate ideas, advanced systems can autonomously explore, generate, and validate the scientific merit and originality of research objectives before development.</p>
<p>Iterative Refinement</p>
<p>Iterative refinement within Level 3 systems involves sophisticated feedback loops that enable not just incremental improvements but also fundamental reassessments of the research trajectory.A key differentiator lies in the primary source and nature of this high-level feedback.The AI Scientist (v1 and v2) incorporates highly automated internal review and refinement processes.It employs AI reviewers, LLM evaluators for experimental choices, and VLMs to critique figures, fostering a rich internal feedback loop for iterative development.In contrast, Zochi (IntologyAI, 2025) integrates human expertise for macro-level guidance, where feedback can trigger complete re-evaluations of hypotheses or designs.This allows it to act on critiques challenging the core research premise, even reverting to hypothesis regeneration if results are unsatisfactory.Overall, while automated self-correction is a common goal, the current landscape reveals a pragmatic blend: some systems focus on enhancing autonomous deep reflection, while others integrate human oversight for robust, high-level iterative refinement and strategic redirection.</p>
<p>Challenges and Future Directions</p>
<p>Throughout this survey, we have systematically reviewed the escalating roles of Large Language Models in scientific discovery, delineating their progression through distinct levels of autonomy and capability-from foundational assistants and analysts to increasingly autonomous scientific researchers.In particular, we have underscored the evolving methodologies, task complexities, and the nature of human-LLM interaction that define each stage of this maturation.Beyond reviewing these advancements and current applications, this section presents several significant challenges and outlines promising directions for future research, aiming to inspire further exploration into the development and responsible deployment of LLMs as transformative tools in scientific inquiry.</p>
<p>Fully-Autonomous Research Cycle While current Level 3 systems can navigate multiple stages of the scientific method for a specific inquiry, they often operate within a single research instance or predefined topic.The scientific method, however, is inherently cyclical, characterized by continuous iteration, refinement, and the pursuit of evolving research questions.A significant future direction, therefore, is to develop LLM-based systems capable of engaging in a truly autonomous research cycle.This would entail not merely executing a given research task from start to finish, but possessing the foresight to discern the broader implications of their findings, proactively identify promising avenues for subsequent investigation, and strategically direct their efforts towards practical advancements that build upon previous work.</p>
<p>Robotic Automation</p>
<p>A key barrier to fully autonomous scientific discovery in natural science is LLM agents' inability to conduct physical laboratory experiments.While adept in computational research, their application in fields requiring physical interaction remains limited.Integrating LLMs with robotic systems empowers them to translate their planning capabilities into direct experimental actions.Early works in LLM-robotic integration (Yoshikawa et al., 2023;Song et al., 2024;Darvish et al., 2025) already highlights this potential in chemical experimentation.Such automation is poised to significantly broaden LLM-based research, enabling end-to-end discovery in disciplines like chemistry and materials science, thereby advancing autonomous scientific exploration.</p>
<p>Transparency and Interpretability</p>
<p>The blackbox nature (or opacity) of advancing LLMs in science undermines scientific validation, trust, and the assimilation of AI-driven insights (Xu et al., 2025c).Addressing this opacity demands more than superficial Explainable AI (XAI) techniques (Ahadian and Guan, 2024).It necessitates a paradigm shift towards systems whose internal operations are inherently designed for verifiable reasoning and justifiable conclusions (Bengio et al., 2025).Consequently, the challenge is not just explaining outputs, but ensuring the AI's internal logic aligns with scientific principles and can reliably differentiate asserted claims from verifiable truths.This profound interpretability is vital for reliable and reproducible LLM-based scientific discovery.</p>
<p>Continuous Self-Improvement</p>
<p>The iterative and evolving nature of scientific inquiry demands systems capable of learning from ongoing engagement, assimilating experimental outcomes, and adapting research strategies.Current research integrating continual learning with agent-based systems already highlights the potential for LLMs to adapt to new tasks or environments without catastrophic forgetting (Majumder et al., 2023;Kim et al., 2024).Within scientific discovery, a promising future direction is to incorporate online reinforcement learning frameworks (Carta et al., 2023).This integration promises to continuously enhance scientific agents' capabilities over their operational lifetime through successive discoveries, thereby advancing sustainable autonomous exploration.</p>
<p>Ethics and Societal Alignment</p>
<p>As LLM-based systems gain independent reasoning and action capabilities, their potential for risks-ranging from amplified societal biases to deliberate misuse like generating harmful substances or challenging human control-becomes increasingly salient and complex (He et al., 2023;Ahadian and Guan, 2024;Bengio et al., 2025).With AI capabilities and societal norms in constant flux, alignment is consequently an imperative, continuous process demanding adaptive governance and evolving value systems (Li et al., 2024e).This requires embedding ethical constraints directly in scientific AI design frameworks, alongside vigilant oversight, to ensure advancements serve human well-being and the common good.</p>
<p>Limitations</p>
<p>This survey provides a systematic review of LLMs in scientific discovery, with a particular emphasis on the paradigm shift characterized by their escalating levels of autonomy.Our analysis and the selection of reviewed literature are therefore centered on works that illustrate this transition across the stages of the scientific method, categorized within our proposed three-level autonomy framework: LLM as Tool, LLM as Analyst, and LLM as Scientist.</p>
<p>Consequently, the scope of this survey has certain limitations.Firstly, we do not provide an exhaustive review of research focused on the development of general-purpose scientific LLMs for domain-specific reasoning or application.These areas, while crucial to the broader landscape of AI in science, are extensively covered in other existing surveys and fall outside our specific focus on the autonomy paradigm.Secondly, while we acknowledge the importance of fundamental LLM capabilities such as planning, code generation, and agentic decision-making, this survey does not delve deeply into orthogonal benchmarks or methodologies related to these general abilities.These exclusions were deliberate to maintain a focused exploration of the transformative roles and increasing independence of LLMs throughout the scientific research lifecycle.</p>
<p>Ethics Statement</p>
<p>Our paper presents a comprehensive survey of LLMs in scientific discovery, with a specific focus on their role transformation from task automation tools to autonomous agents.All research works reviewed in this survey are properly cited.To the best of our knowledge, the referenced materials are publicly accessible or available under licenses permitting their use for research review.We did not conduct additional dataset curation or human annotation work.Consequently, we believe that this paper does not raise any ethical concerns.DAgent (Xu et al., 2025b) Data Science ✓ ✗ ✗ ✓ ✓ ✓ ✓ ✗ DS-Agent (Guo et al., 2024b) Data Science ✓ ✗ ✗ ✓ ✓ ✓ ✓ ✓ InfiAgent-DABench (Hu et al., 2024b) Data Science ✗ ✓ ✗ ✓ ✓ ✓ ✗ ✗ BLADE (Gu et al., 2024) Data Open-Sourced?</p>
<p>Agent Laboratory (Schmidgall et al., 2025) Artificial Intelligence ✓ ✓ literature review, experimentation, report writing, iterative research with human feedback loops.✓</p>
<p>The AI Scientist (Lu et al., 2024) Artificial Intelligence ✓ ✗ idea generation, code generation, experiment execution, research paper writing.✓</p>
<p>The AI Scientist-v2 (Yamada et al., 2025)</p>
<p>Figure 1 :
1
Figure 1: Stages of the scientific method with corresponding LLM applications and research topics.</p>
<p>Table 1 :
1
Three levels of autonomy in LLM-based scientific discovery.
Autonomy LevelsLLMs' RoleHuman's RoleTask ScopeAgentic WorkflowLevel 1 LLM as ToolTask Automation ToolTask AllocationExplicitly DefinedSimple &amp; StaticLevel 2 LLM as AnalystData Modeling &amp; Analytical AgentProblem Definition &amp; Output ValidationGoal-OrientedAdvancedLevel 3 LLM as ScientistOpen Exploratory &amp; Discovery AgentMinimal InterventionOpen-EndedStrategic &amp; Iterative</p>
<p>LEVEL 1 LEVEL 2 LEVEL 3 FUTURE Literature Review &amp; Info Gathering Idea Generation &amp; Hypothesis Formulation Experiment Planning &amp; Execution Data Analysis &amp; Organization Conclusion &amp; Hypothesis Validation Iteration &amp; Refinement Machine Learning Research Data Modeling and Analysis Function Discovery Natural Science Research General Research
SCIMON(Wang et al., 2023)ResearchAgentLitLLM(Baek et al., 2024)(Agarwal et al., 2024)Text-Tuple-Table (Deng et al., 2024)Dennstädt et al., 2024TKGT</p>
<p>Table (
(for table-based question answering under practicalindustrial scenarios.Chart Data Beyond tabular data, charts rep-resent another important format for organizingand storing information derived from experimentaldata. Early benchmarks, exemplified by ChartQA(Masry et al., 2022), examined the capabilities ofvision transformers in chart-based question answer-ing. Subsequent works, including CharXiv (Wanget al., 2024e) and ChartX (Xia et al., 2025), have ex-panded the scope of chart understanding scenariosby utilizing human-curated chart generation or byincorporating real-world chart data sourced fromarXiv preprints. Regarding chart generation, Au-tomaTikZ (Belouadi et al., 2024) formulates theprocess as TikZ code generation from caption textand has demonstrated the efficacy of fine-tuningLLMs using open scientific figure data. More re-cently, Text2Chart31 (Zadeh et al., 2025) employedreinforcement learning with automated feedbackto refine chart generation capabilities within theMatplotlib library.Wang et al., 2024d) proposes a method to enhancetabular understanding by incorporating evolvingtables within the reasoning chain of LLMs. Con-currently, Deng et al. (2024a) highlight the poten-tial of integrating visual information to improvemultimodal understanding, thereby aiding tabularcomprehension. More recently, Wu et al. (2025) in-troduced TableBench, a comprehensive benchmark</p>
<p>Table A2 :
A2
Comparison and classification of Level 2 research works in LLM-based scientific discovery.
Science✗✓✗✗✓✓✓✗DiscoveryBench (Majumder et al., 2024)Data Science✗✓✗✓✓✓✓✗DSBench (Jing et al., 2024)Data Science✗✓✗✗✓✓✓✗Zheng et al. (2023)General✓✓✓✓✓✓✓✗Function DiscoveryBoxLM (Li et al., 2024b)Statistics✓✗✗✓✓✓✓✗LLM-SR (Shojaee et al., 2025a)General✓✗✗✓✓✗✓✓LLM-SRBench (Shojaee et al., 2025b)General✗✓✗✓✓✗✓✓Gravity-Bench-v1 (Koblischke et al., 2025)Physics✗✓✗✓✓✗✓✓DrSR (Wang et al., 2025a)General✓✗✗✓✓✓✓✓EvoSLD (Lin et al., 2025)Artificial Intelligence✓✗✗✓✓✓✓✓Natural Science ResearchCoscientist (Boiko et al., 2023)Chemistry✓✗✗✓✓✓✓✗Gao et al. (2024)Biomedicine✓✗✗✓✓✓✓✗BioResearcher (Luo et al., 2024)Biomedicine✓✗✗✗✓✓✓✓DrugAgent (Liu et al., 2025b)Biomedicine✓✗✗✓✓✗✗✓FutureHouse (Skarlinski et al., 2025)Chemistry, Biology✓✗✓✓✓✗✗✗ScienceAgentBench (Chen et al., 2025c)Chemistry, Biology✗✓✓✓✓✓✓✗ProtAgents (Ghafarollahi and Buehler, 2024a)Chemistry, Biology✓✗✓✓✓✓✓✗Auto-Bench (Chen et al., 2025b)General✗✓✗✗✓✓✗✓AI co-scientist (Gottweis et al., 2025)General✓✗✗✓✓✓✓✗General ResearchDiscoveryWorld (Jansen et al., 2024)General✗✓✗✓✓✓✓✓Liu et al. (2025a)General✗✓✓✓✓✓✓✓Curie (Kon et al., 2025)General✓✗✗✗✓✓✓✗EAIRA (Cappello et al., 2025)General✗✓✓✓✓✓✓✗Research WorksScience DomainMethodology FrameworkBenchmark EvaluationFeatured Functionality</p>
<p>Table A3 :
A3
Comparison and classification of Level 3 research works in LLM-based scientific discovery.
idea generation, code generation,Artificial Intelligence✓✗experiment execution, research paper writing,✓with agentic tree-search and feedbacks.AI-Researcher (Data Intelligence Lab, 2025) Artificial Intelligence✓✗literature review, data analysis, report generation.✓Zochi (IntologyAI, 2025)Artificial Intelligence✓✗customizable workflows for data collection, analysis, and decision-making.✓Carl (Autoscience, 2025)Artificial Intelligence✓✗hypothesis generation, experiment design, data analysis, and manuscript writing.✗
AcknowledgementsWe thank all the anonymous reviewers and meta reviewers for their valuable comments.The authors of this paper were supported by the ITSP Platform Research Project (ITS/189/23FP) from ITC of Hong Kong, SAR, China, and the AoE (AoE/E-601/24-N), the RIF (R6021-20) and the GRF (16205322) from RGC of Hong Kong, SAR, China.Data Modeling and AnalysisTing, Azton Wells, and 7 others.2025.Eaira: Establishing a methodology for evaluating ai models as scientific research assistants.Preprint, arXiv:2502.20309.A Summary Tables of LLMs in Scientific DiscoveryResearch WorksScience Domain Task Nature Framework Methodology Evaluation BenchmarkAgentic WorkflowLiterature Search and Info Aggregation LitLLM(Agarwal et al., 2024)General Literature ✓ ✗ ✗ Science Hierarchography(Gao et al., 2025)General Literature ✓ ✗ ✓ Dennstädt et al.(2024)Biomedicine Literature ✓ ✗ ✗ SCIMON(Wang et al., 2024a)General Literature, Idea Generation ✓ ✗ ✗ ResearchAgent(Baek et al., 2025)General Literature, Idea Generation ✓ ✗ ✓ Text-Tuple-Table(Deng et al., 2024b)General Text2Table ✓ ✓ ✗ TKGT(Jiang et al., 2024b)General Text2Table ✓ ✓ ✗ ArxivDIGESTables(Newman et al., 2024)General Literature, Text2Table ✓ ✓ ✗ arXiv2Table(Wang et al., 2025b)General Literature, Text2Table ✓ ✓ ✗ PaperQA &amp; LitQA(Lála et al., 2023)General Literature ✓ ✓ ✗ AutoSurvey(Wang et al., 2024c)General Literature ✓ ✗ ✓Idea Generation and Hypothesis FormulationSi et al. (2024)Artificial Intelligence Idea Generation ✗ ✓ ✗ LiveIdeaBench(Ruan et al., 2025)General Idea Generation ✗ ✓ ✗ Nova(Hu et al., 2024a)General Literature, Idea Generation ✓ ✗ ✓ IdeaBench(Guo et al., 2024a)General Literature, Idea Generation ✗ ✓ ✗ GraphEval(Feng et al., 2025)Artificial Intelligence Literature, Idea Generation ✗ ✓ ✗ AI Idea Bench 2025(Qiu et al., 2025)Artificial(Yin et al., 2022)Artificial Intelligence Code Generation ✓ ✓ ✗ AIDE(Jiang et al., 2025)Artificial Intelligence Code Generation ✓ ✗ ✓ SciCode(Tian et al., 2024)Artificial Intelligence Code Generation ✗ ✓ ✗ DS-1000(Lai et al., 2022)Artificial Intelligence Code Generation ✗ ✓ ✗ MLE-Bench(Chan et al., 2025)Artificial Intelligence Code Generation ✗ ✓ ✓ Data Analysis and Organization AutomaTikZ(Belouadi et al., 2024)General(Xia et al., 2025)General Chart Reasoning ✓ ✓ ✗ CharXiv(Wang et al., 2024e)General Chart Reasoning ✗ ✓ ✗ ChartQA(Masry et al., 2022)General Chart Reasoning ✗ ✓ ✗ Chain-of-Table(Wang et al., 2024d)General Tabular Reasoning ✓ ✗ ✓ TableBench(Wu et al., 2025)General(Rabby et al., 2025)General Hypothesis Generation, Refinement ✓ ✗ ✓ Chain of Ideas(Li et al., 2024a)Artificial Intelligence Idea Generation, Refinement ✓ ✓ ✓ Machine Learning Research CodeScientist(Jansen et al., 2025)Artificial Intelligence(Xue et al., 2025)Artificial Intelligence ✓ ✗ ✗ ✗ ✓ ✓ ✓ ✓ MLAgentBench(Huang et al., 2024a)Artificial Intelligence ✗ ✓ ✗ ✗ ✓ ✓ ✓ ✗ MLR-Copilot(Li et al., 2024d)Artificial Intelligence ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✓ MLRC-Bench(Zhang et al., 2025b)Artificial Intelligence ✗ ✓ ✗ ✗ ✓ ✓ ✗ ✓ RE-Bench(Wijk et al., 2024)Artificial Intelligence ✗ ✓ ✗ ✗ ✓ ✓ ✗ ✓ MLZero(Fang et al., 2025)Artificial Intelligence ✓ ✓ ✗ ✗ ✓ ✓ ✗ ✓ Genesys(Cheng et al., 2025b)Artificial Intelligence ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✓ MLGym(Nathani et al., 2025)Artificial Intelligence
Llm4grn: Discovering causal gene regulatory networks with llms -evaluation through synthetic data generation. Tejumade Afonja, Ivaxi Sheth, Ruta Binkyte, Waqar Hanif, Thomas Ulas, Matthias Becker, Mario Fritz, arXiv:2410.158282024Preprint</p>
<p>Litllm: A toolkit for scientific literature review. Shubham Agarwal, Gaurav Sahu, Abhay Puri, H Issam, Laradji, D J Krishnamurthy, Jason Dvijotham, Laurent Stanley, Christopher Charlin, Pal, arXiv:2402.017882024Preprint</p>
<p>Ai trustworthy challenges in drug discovery. Pegah Ahadian, Qiang Guan, Trustworthy Artificial Intelligence for Healthcare. ChamSpringer Nature Switzerland2024</p>
<p>Meet carl: The first ai system to produce academically peer-reviewed research. Autoscience , 2025</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382025Preprint</p>
<p>Automatikz: Text-guided synthesis of scientific vector graphics with tikz. Jonas Belouadi, Anne Lauscher, Steffen Eger, arXiv:2310.003672024Preprint</p>
<p>Superintelligent agents pose catastrophic risks: Can scientist ai offer a safer path?. Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt Macdermott, Sören Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, Marc-Antoine Rondeau, Pierre-Luc St-Charles, David Williams-King, arXiv:2502.156572025Preprint</p>
<p>Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang, Katherine Chou, Avinatan Hassidim, arXiv:2502.18864Burak Gokturk, Amin Vahdat, Pushmeet Kohli, and 15 others. 2025. Towards an ai co-scientist. Preprint</p>
<p>Blade: Benchmarking language model agents for data-driven science. Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, Yikun Zhang, M Tianmai, Lanyi Zhang, Mike A Zhu, Jeffrey Merrill, Tim Heer, Althoff, arXiv:2408.096672024Preprint</p>
<p>Ideabench: Benchmarking large language models for research idea generation. Sikun Guo, Hassan Amir, Guangzhi Shariatmadari, Albert Xiong, Eric Huang, Stefan Xie, Aidong Bekiranov, Zhang, arXiv:2411.024292024aPreprint</p>
<p>Ds-agent: Automated data science by empowering large language models with case-based reasoning. Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang, arXiv:2402.174532024bPreprint</p>
<p>Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, Dong Yu, arXiv:2401.13919Webvoyager: Building an endto-end web agent with large multimodal models. 2024Preprint</p>
<p>Jiyan He, Weitao Feng, Yaosen Min, Jingwei Yi, Kunsheng Tang, Shuai Li, Jie Zhang, Kejiang Chen, Wenbo Zhou, Xing Xie, Weiming Zhang, Nenghai Yu, Shuxin Zheng, arXiv:2312.06632Control risk for potential misuse of artificial intelligence in science. 2023Preprint</p>
<p>An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, arXiv:2410.142552024aNovaPreprint</p>
<p>Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, Jianbo Yuan, Jiwei Li, Kun Kuang, Yang Yang, Hongxia Yang, Fei Wu, arXiv:2401.05507Infiagent-dabench: Evaluating agents on data analysis tasks. 2024bPreprint</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, 10.18653/v1/2023.findings-acl.67Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, arXiv:2310.033022024aPreprint</p>
<p>Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, arXiv:2402.02716Ruiming Tang, and Enhong Chen. 2024b. Understanding the planning of llm agents: A survey. Preprint</p>
<p>Zochi technical report: The first artificial scientist. </p>
<p>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents. Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, Peter Clark, arXiv:2406.067692024Preprint</p>
<p>Codescientist: End-to-end semiautomated scientific discovery with code-based experimentation. Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Prasad Bodhisattwa, Daniel S Majumder, Peter Weld, Clark, arXiv:2503.227082025Preprint</p>
<p>A survey on large language models for code generation. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, Sunghun Kim, arXiv:2406.005152024aPreprint</p>
<p>TKGT: Redefinition and a new way of text-to-table tasks based on real world demands and knowledge graphs augmented LLMs. Peiwen Jiang, Xinbo Lin, Zibo Zhao, Ruhui Ma, Yvonne Jie Chen, Jinhua Cheng, 10.18653/v1/2024.emnlp-main.901Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024b</p>
<p>Aide: Ai-driven exploration in the space of code. Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, Yuxiang Wu, arXiv:2502.131382025Preprint</p>
<p>Dsbench: How far are data science agents from becoming data science experts?. Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu, arXiv:2409.077032024Preprint</p>
<p>End-toend symbolic regression with transformers. Pierre-Alexandre Kamienny, Stéphane Ascoli, arXiv:2204.105322022PreprintGuillaume Lample, and François Charton</p>
<p>Online continual learning for interactive instruction following agents. Byeonghwi Kim, Minhyuk Seo, Jonghyun Choi, arXiv:2403.075482024Preprint</p>
<p>Gravity-bench-v1: A benchmark on gravitational physics discovery for agents. Nolan Koblischke, Hyunseok Jang, Kristen Menou, Mohamad Ali-Dib, arXiv:2501.184112025Preprint</p>
<p>Curie: Toward rigorous and automated scientific experimentation with ai agents. Patrick Tser, Jern Kon, Jiachen Liu, Qingyun Ding, Yuxin Qiu, Zhaoning Yang, Yufan Huang, Moontae Jer-Shannassa, Muntasir Lee, Aonan Chowdhury, Chen, arXiv:2502.160692025Preprint</p>
<p>Thomas Samuel, Kuhn , The Structure of Scientific Revolutions. ChicagoUniversity of Chicago Press1962</p>
<p>Ds-1000: A natural and reliable benchmark for data science code generation. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen Tau Yih, Daniel Fried, Sida Wang, Tao Yu, arXiv:2211.115012022Preprint</p>
<p>Paperqa: Retrieval-augmented generative agent for scientific research. Jakub Lála, O' Odhran, Aleksandar Donoghue, Sam Shtedritski, Samuel G Cox, Andrew D Rodriques, White, 2023</p>
<p>Can large language models help experimental design for causal discovery. Junyi Li, Yongqiang Chen, Chenxi Liu, Qianyi Cai, Tongliang Liu, Bo Han, Kun Zhang, Hui Xiong, arXiv:2503.011392025Preprint</p>
<p>Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, Deli Zhao, Yu Rong, arXiv:2410.13185Tian Feng, and Lidong Bing. 2024a. Chain of ideas: Revolutionizing research via novel idea development with llm agents. Preprint</p>
<p>Automated statistical model discovery with language models. Y Michael, Emily B Li, Noah D Fox, Goodman, arXiv:2402.178792024bPreprint</p>
<p>Learning to generate research idea with dynamic control. Ruochen Li, Liqiang Jing, Xinya Du, arXiv:2412.146262024cPreprint</p>
<p>Mlr-copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, arXiv:2408.140332024dPreprint</p>
<p>Agent alignment in evolving social norms. Shimin Li, Tianxiang Sun, Qinyuan Cheng, Xipeng Qiu, arXiv:2401.046202024ePreprint</p>
<p>Evosld: Automated neural scaling law discovery with large language models. Haowei Lin, Yacine Jernite, H T Kung, Andrew Gordon, Wilson , arXiv:2507.211842025Preprint</p>
<p>A vision for auto research with llm agents. Chengwei Liu, Chong Wang, Jiayue Cao, Jingquan Ge, Kun Wang, Lvye Zhang, Ming-Ming Cheng, Penghai Zhao, Tianlin Li, Xiaojun Jia, Xiang Li, Xinfeng Li, Yang Liu, Yebo Feng, Yihao Huang, Yijia Xu, Yuqiang Sun, Zhenhong Zhou, Zhengzi Xu, arXiv:2504.187652025aPreprint</p>
<p>Reviewergpt? an exploratory study on using large language models for paper reviewing. Ryan Liu, Nihar B Shah, arXiv:2306.006222023Preprint</p>
<p>Drugagent: Automating ai-aided drug discovery programming through llm multi-agent collaboration. Sizhe Liu, Yizhou Lu, Siyu Chen, Xiyang Hu, Jieyu Zhao, Yingzhou Lu, Yue Zhao, arXiv:2411.156922025bPreprint</p>
<p>Researchbench: Benchmarking llms in scientific discovery via inspiration-based task decomposition. Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou, arXiv:2503.212482025cPreprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024Preprint</p>
<p>From intention to implementation: Automating biomedical research via llms. Yi Luo, Linghang Shi, Yihao Li, Aobo Zhuang, Yeyun Gong, Ling Liu, Chen Lin, arXiv:2412.094292024Preprint</p>
<p>Llm4sr: A survey on large language models for scientific research. Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, Xinya Du, arXiv:2501.043062025Preprint</p>
<p>Clin: A continually learning language agent for rapid task adaptation and generalization. Prasad Bodhisattwa, Bhavana Majumder, Peter Dalvi Mishra, Oyvind Jansen, Niket Tafjord, Li Tandon, Chris Zhang, Peter Callison-Burch, Clark, arXiv:2310.101342023Preprint</p>
<p>Discoverybench: Towards data-driven discovery with large language models. Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Bhavana Agarwal, Abhijeetsingh Dalvi Mishra, Aryan Meena, Tirth Prakhar, Tushar Vora, Ashish Khot, Peter Sabharwal, Clark, arXiv:2407.017252024Preprint</p>
<p>Chartqa: A benchmark for question answering about charts with visual and logical reasoning. Ahmed Masry, Xuan Do, Jia Long, Shafiq Qing Tan, Enamul Joty, Hoque, arXiv:2203.102442022Preprint</p>
<p>Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, Dieuwke Hupkes, Ricardo Silveira Cabral, Tatiana Shavrina, Jakob Foerster, Yoram Bachrach, William Yang, Wang , Roberta Raileanu, arXiv:2502.14499Mlgym: A new framework and benchmark for advancing ai research agents. 2025Preprint</p>
<p>Arxivdigestables: Synthesizing scientific literature into tables using language models. Benjamin Newman, Yoonjoo Lee, Aakanksha Naik, Pao Siangliulue, Raymond Fok, Juho Kim, Daniel S Weld, Joseph Chee Chang, Kyle Lo, arXiv:2410.223602024Preprint</p>
<p>Machine learning for hypothesis generation in biology and medicine: exploring the latent space of neuroscience and developmental bioelectricity. O' Thomas, Joel Brien, Léo Stremmel, Patrick Pio-Lopez, Cody Mcmillen, Michael Rasmussen-Ivey, Levin, 10.1039/D3DD00185GDigital Discovery. 32024</p>
<p>Bioplanner: Automatic evaluation of llms on protocol planning in biology. Aleksandar Odhran O'donoghue, John Shtedritski, Ralph Ginger, Ali Essa Abboud, Justin Ghareeb, Samuel G Booth, Rodriques, arXiv:2310.106322023Preprint</p>
<p>Sparks of science: Hypothesis generation using structured paper data. O' Charles, Tirthankar Neill, Roberta Ghosal, Mike Rȃileanu, Thang Walmsley, Kevin Bui, Ioana Schawinski, Ciucȃ, arXiv:2504.129762025Preprint</p>
<p>Introducing openai o1 preview. OpenAI. 2025. Introducing deep research. 2024OpenAI</p>
<p>Jiefu Ou, William Gantt Walden, Kate Sanders, Zhengping Jiang, Kaiser Sun, Jeffrey Cheng, William Jurayj, Miriam Wanner, Shaobo Liang, Candice Morgan, Seunghoon Han, Weiqi Wang, Chandler May, Hannah Recknor, arXiv:2503.21717Daniel Khashabi, and Benjamin Van Durme. 2025. Claimcheck: How grounded are llm critiques of scientific papers? Preprint. </p>
<p>The Logic of Scientific Discovery. Karl R Popper, 1935RoutledgeLondon, England</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, arXiv:2311.059652023Preprint</p>
<p>Large language models as biomedical hypothesis generators: A comprehensive evaluation. Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Jinfang Hu, Bowen Zhou, arXiv:2407.089402024Preprint</p>
<p>Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, Dong Yu, arXiv:2401.03601Infobench: Evaluating instruction following ability in large language models. 2024Preprint</p>
<p>Ai idea bench 2025: Ai research idea generation benchmark. Yansheng Qiu, Haoquan Zhang, Zhaopan Xu, Ming Li, Diping Song, Zheng Wang, Kaipeng Zhang, arXiv:2504.141912025Preprint</p>
<p>Tool learning with large language models: a survey. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen, 10.1007/s11704-024-40678-2Frontiers of Computer Science. 8192025</p>
<p>Verification and refinement of natural language explanations through llm-symbolic theorem proving. Xin Quan, Marco Valentino, Louise A Dennis, André Freitas, arXiv:2405.013792024Preprint</p>
<p>Prasenjit Mitra, and Sören Auer. 2025. Iterative hypothesis generation for scientific discovery with monte carlo nash equilibrium self-refining trees. Gollam Rabby, Diyana Muhammed, arXiv:2503.19309Preprint</p>
<p>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S Weld, arXiv:2409.146342025Preprint</p>
<p>Towards scientific discovery with generative ai: Progress, opportunities, and challenges. K Chandan, Parshin Reddy, Shojaee, 10.1609/aaai.v39i27.35084Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>Liveideabench: Evaluating llms' divergent thinking for scientific idea generation with minimal context. Kai Ruan, Xuan Wang, Jixiang Hong, Peng Wang, Yang Liu, Hao Sun, arXiv:2412.175962025Preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025Preprint</p>
<p>Automated machine learning: From principles to practices. Zhenqian Shen, Yongqi Zhang, Lanning Wei, Huan Zhao, Quanming Yao, arXiv:1810.133062024Preprint</p>
<p>Hierarchically encapsulated representation for protocol design in self-driving labs. Yu-Zhe Shi, Mingchen Liu, Fanxu Meng, Qiao Xu, Zhangqian Bi, Kun He, Lecheng Ruan, Qining Wang, arXiv:2504.038102025Preprint</p>
<p>Llm-sr: Scientific equation discovery via programming with large language models. Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, Chandan K Reddy, arXiv:2404.184002025aPreprint</p>
<p>Llm-srbench: A new benchmark for scientific equation discovery with large language models. Parshin Shojaee, Ngoc-Hieu Nguyen, Kazem Meidani, Amir Barati Farimani, Chandan K Khoa D Doan, Reddy, arXiv:2504.104152025bPreprint</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024Preprint</p>
<p>Futurehouse platform: Superintelligent ai agents for scientific discovery. Michael Skarlinski, Tyler Nadolski, James Braza, Remo Storni, Mayk Caldas, Ludovico Mitchener, Michaela Hinks, Andrew White, Sam Rodriques, 2025</p>
<p>A multi-agent-driven robotic ai chemist enabling autonomous chemical research on demand. Tao Song, Man Luo, Linjiang Chen, Yan Huang, Qing Zhu, Daobin Liu, Baicheng Zhang, Gang Zou, Fei Zhang, Weiwei Shang, Jun Jiang, Yi Luo, 10.26434/chemrxiv-2024-w953h-v22024Preprint</p>
<p>Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, arXiv:2504.01848Amelia Glaese, and Tejal Patwardhan. 2025. Paperbench: Evaluating ai's ability to replicate ai research. Preprint</p>
<p>Maojun Sun, Ruijian Han, Binyan Jiang, Houduo Qi, Defeng Sun, Yancheng Yuan, Jian Huang, arXiv:2412.14222A survey on large language model-based agents for statistics and data science. 2024Preprint</p>
<p>Towards autonomous hypothesis verification via language models with minimal guidance. Shiro Takagi, Ryutaro Yamauchi, Wataru Kumagai, arXiv:2311.097062023Preprint</p>
<p>Yanyu Xiong, and 11 others. Minyang Tian, Luyu Gao, Dylan Shizhuo, Xinan Zhang, Cunwei Chen, Xuefei Fan, Roland Guo, Pan Haas, Kittithat Ji, Yao Krongchon, Shengyan Li, Di Liu, Yutao Luo, Hao Ma, Kha Tong, Chenyu Trinh, Zihan Tian, Bohao Wang, Wu, arXiv:2407.13168Scicode: A research coding benchmark curated by scientists. 2024Preprint</p>
<p>Dov Te'eni, and Iddo Drori. 2024. Ai-driven review systems: Evaluating llms in scalable and bias-aware academic reviews. Keith Tyser, Ben Segev, Gaston Longhitano, Xin-Yu Zhang, Zachary Meeks, Jason Lee, Uday Garg, Nicholas Belsten, Avi Shporer, Madeleine Udell, arXiv:2408.10365Preprint</p>
<p>Ai feynman: a physics-inspired method for symbolic regression. Silviu- , Marian Udrescu, Max Tegmark, arXiv:1905.114812020Preprint</p>
<p>HypER: Literature-grounded hypothesis generation and distillation with provenance. Rosni Vasu, Chandrayee Basu, Bhavana Dalvi Mishra, Cristina Sarasua, Peter Clark, Abraham Bernstein, arXiv:2506.129372025Preprint</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, arXiv:2305.142592024aPreprint</p>
<p>Drsr: Llm based scientific equation discovery with dual reasoning from data and experience. Runxiang Wang, Boxiao Wang, Kai Li, Yifan Zhang, Jian Cheng, arXiv:2506.042822025aPreprint</p>
<p>Can llms generate tabular summaries of science papers? rethinking the evaluation protocol. Weiqi Wang, Jiefu Ou, Yangqiu Song, Benjamin Van Durme, Daniel Khashabi, arXiv:2504.102842025bPreprint</p>
<p>Executable code actions elicit better llm agents. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, Heng Ji, arXiv:2402.010302024bPreprint</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Linyi Yang, Jindong Wang, Cunxiang Wang, Kaijie Zhu, Yiqiao Jin, Xing Xie, arXiv:2406.102522024cPreprint</p>
<p>Chain-of-table: Evolving tables in the reasoning chain for table understanding. Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, Tomas Pfister, arXiv:2401.043982024dPreprint</p>
<p>Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, Danqi Chen, arXiv:2406.185212024ePreprint</p>
<p>Predicting empirical ai research outcomes with language models. Yutai Wen, Yifan Jiang, Zitao Li, Whytnee Wade, J D Zamfirescu-Pereira, Xinyun Chen, Yuxin Wen, Yiming Yang, Graham Neubig, Jacob Andreas, Daniel S Weld, arXiv:2506.007942025Preprint</p>
<p>Cycleresearcher: Improving automated research via automated review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, arXiv:2411.008162025Preprint</p>
<p>Hjalmar Wijk, Tao Lin, Joel Becker, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen, Josh Clymer, Jai Dhyani, Elena Ericheva, Katharyn Garcia, Brian Goodrich, Nikola Jurkovic, Holden Karnofsky, Megan Kinniment, Aron Lajko, Seraphina Nix, Lucas Sato, arXiv:2411.15114Evaluating frontier ai r&amp;d capabilities of language model agents against human experts. Preprintand 4 others. 2024. Rebench</p>
<p>Tablebench: A comprehensive and complex benchmark for table question answering. Xianjie Wu, Jian Yang, Linzheng Chai, Ge Zhang, Jiaheng Liu, Xinrun Du, Di Liang, Daixin Shu, Xianfu Cheng, Tianzhen Sun, Guanglin Niu, Tongliang Li, Zhoujun Li ; Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Peng Ye, Min Dou, Botian Shi, Junchi Yan, Yu Qiao, arXiv:2408.09174.xAI.2025arXiv:2402.12185Chartx &amp; chartvlm: A versatile benchmark and foundation model for complicated chart reasoning. Bo Zhang, Hancheng Ye,2025. 2025PreprintGrok 3 beta -the age of reasoning agents. Renqiu Xia</p>
<p>Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers. Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, Yulan He ; Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, arXiv:2504.00255arXiv:2411.023822025. 2024PreprintImproving scientific hypothesis generation with knowledge grounded large language models</p>
<p>Towards multi-agent reasoning systems for collaborative expertise delegation: An exploratory design study. Baixuan Xu, Chunyang Li, Weiqi Wang, Wei Fan, Tianshi Zheng, Haochen Shi, Tao Fan, Yangqiu Song, Qiang Yang, arXiv:2505.073132025aPreprint</p>
<p>Dagent: A relational database-driven data analysis report generation agent. Wenyi Xu, Yuren Mao, Xiaolu Zhang, Chao Zhang, Xuemei Dong, Mengfei Zhang, Yunjun Gao, arXiv:2503.132692025bPreprint</p>
<p>Advancing ai-scientist understanding: Making llm think like a physicist with interpretable reasoning. Yinggan Xu, Hana Kimlee, Yijia Xiao, Di Luo, arXiv:2504.019112025cPreprint</p>
<p>Improve: Iterative model pipeline refinement and optimization leveraging llm agents. Eric Xue, Zeyi Huang, Yuyang Ji, Haohan Wang, arXiv:2502.185302025Preprint</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, arXiv:2504.080662025Preprint</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, arXiv:2309.027262024PreprintSoujanya Poria, and Erik Cambria</p>
<p>Moosechem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, arXiv:2410.070762025Preprint</p>
<p>Natural language to code generation in interactive data science notebooks. Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Alex Polozov, Charles Sutton, arXiv:2212.092482022Preprint</p>
<p>Artur Kuramshin, Alán Aspuru-Guzik, Florian Shkurti, and Animesh Garg. 2023. Large language models for chemistry robotics. Naruki Yoshikawa, Marta Skreta, Kourosh Darvish, Sebastian Arellano-Rubach, Zhi Ji, Lasse Bjørn Kristensen, Andrew Zou Li, Yuchi Zhao, Haoping Xu, 10.1007/s10514-023-10136-2Autonomous Robots. 47</p>
<p>Text2chart31: Instruction tuning for chart generation with automatic feedback. Juyeon Fatemeh Pesaran Zadeh, Jin-Hwa Kim, Gunhee Kim, Kim, arXiv:2410.040642025Preprint</p>
<p>Xiaohui Fan, and 2 others. 2025a. Scientific large language models: A survey on biological &amp; chemical domains. Qiang Zhang, Keyan Ding, Tianwen Lv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Mengyao Zhang, Jinlu Zhang, Jiyu Cui, Renjun Xu, Hongyang Chen, 10.1145/3715318ACM Comput. Surv. 657</p>
<p>A comprehensive survey of scientific large language models and their applications in scientific discovery. Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, Jiawei Han, arXiv:2406.108332024Preprint</p>
<p>Mlrc-bench: Can language agents solve machine learning research challenges?. Yunxiang Zhang, Muhammad Khalifa, Shitanshu Bhushan, Grant D Murphy, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, Lu Wang, arXiv:2504.097022025bPreprint</p>
<p>Large language models for scientific synthesis, inference and explanation. Yizhen Zheng, Yee Huan, Jiaxin Koh, Anh T N Ju, Lauren T Nguyen, Geoffrey I May, Shirui Webb, Pan, arXiv:2310.079842023Preprint</p>
<p>Is LLM a reliable reviewer? a comprehensive evaluation of LLM on automatic paper reviewing tasks. Ruiyang Zhou, Lu Chen, Kai Yu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024a</p>
<p>Hypothesis generation with large language models. Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, 10.18653/v1/2024.nlp4science-1.10Proceedings of the 1st Workshop on NLP for Science (NLP4Science). the 1st Workshop on NLP for Science (NLP4Science)Association for Computational Linguistics2024b</p>            </div>
        </div>

    </div>
</body>
</html>
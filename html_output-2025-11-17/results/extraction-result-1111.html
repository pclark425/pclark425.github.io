<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1111 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1111</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1111</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-15486186</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1201.6604v1.pdf" target="_blank">Gaussian Processes for Sample Efficient Reinforcement Learning with RMAX-like Exploration</a></p>
                <p><strong>Paper Abstract:</strong> We present an implementation of model-based online reinforcement learning (RL) for continuous domains with deterministic transitions that is specifically designed to achieve low sample complexity. To achieve low sample complexity, since the environment is unknown, an agent must intelligently balance exploration and exploitation, and must be able to rapidly generalize from observations. While in the past a number of related sample efficient RL algorithms have been proposed, to allow theoretical analysis, mainly model-learners with weak generalization capabilities were considered. Here, we separate function approximation in the model learner (which does require samples) from the interpolation in the planner (which does not require samples). For model-learning we apply Gaussian processes regression (GP) which is able to automatically adjust itself to the complexity of the problem (via Bayesian hyperparameter selection) and, in practice, often able to learn a highly accurate model from very little data. In addition, a GP provides a natural way to determine the uncertainty of its predictions, which allows us to implement the"optimism in the face of uncertainty"principle used to efficiently control exploration. Our method is evaluated on four common benchmark domains.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1111.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1111.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP-RMAXexp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Process RMAX with uncertainty-weighted exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL agent that learns a GP dynamics model and applies optimism-in-the-face-of-uncertainty exploration by continuously weighting Bellman updates with GP predictive uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GP-RMAXexp</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based online RL composed of (1) a Gaussian process (GP) model-learner that fits per-action, per-coordinate regressors predicting state differences and predictive variances, and (2) a planner that solves the Bellman equation by value iteration on a fixed uniform grid. GP predictive uncertainty is aggregated per state-action and injected into planning to bias exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Optimism-in-the-face-of-uncertainty via GP predictive variance (continuous RMAX-like optimism) — a form of uncertainty-weighted exploration / UCB-like optimism</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The agent computes a normalized scalar uncertainty c(x,a) from GP predictive variances (max over normalized per-coordinate variances). During value iteration the Bellman update is modified (Eq. 7") to replace the usual update by a convex combination: (1 - c) * standard update + c * V_MAX (the optimistic maximal value). Thus actions with higher model uncertainty receive larger optimistic value boosts and are preferentially chosen; model updates occur from observed transitions and planning is invoked periodically (planning frequency K=50).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Mountain Car; Inverted Pendulum; Bicycle balancing; Acrobot</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transition dynamics (learned), continuous low-dimensional state spaces, discrete actions, deterministic (assumed), smooth transitions, known reward function, episodic tasks with bounded episode lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Mountain Car: 2D state (position, velocity), 3 discrete actions, episodes ≤ 500 steps. Inverted Pendulum: 2D state, 5 discrete actions, 500-step episodes. Bicycle: 4D state, 5 discrete actions, 500-step episodes. Acrobot: 4D state, 2 discrete actions, 500-step episodes. Planner grids: 100×100 for 2D, 20^4 for bicycle, 25^4 for acrobot.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Learns near-optimal policies with very low sample complexity across benchmarks (empirical best-offline values computed: mountain car 103 steps; inverted pendulum −18.41 total cost; bicycle −3.49 total cost; acrobot 64 steps). Example: after ≈120 transitions (mountain car) the approximated value function already resembles the true one. Overall GP-RMAXexp converges to near-optimal behavior in far fewer episodes than Sarsa(λ) (exact learning curves shown in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High: in mountain car significant approximation of optimal value after ~120 transitions; generally reaches near-optimal behavior within tens-to-low-hundreds of transitions depending on domain — substantially fewer samples than Sarsa(λ) with tile-coding in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Handled in planner via uncertainty-weighted optimism: c(x,a) (from GP) interpolates between full optimism (V_MAX) for high-uncertainty state-actions and standard Bellman backup for well-known state-actions, thus smoothly shifting from exploration to exploitation as model uncertainty decreases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared experimentally to GP-RMAXgrid (binary RMAX-style uncertainty), GP-RMAXnoexp (no optimism), and Sarsa(λ) with tile coding (two tile configurations). Related methods discussed: RMAX, fitted R-MAX, LSPI/LSTD/LSPE, fitted Q-iteration, and other GP/kernel RL work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>GP-RMAXexp achieves very low sample complexity and near-optimal performance in continuous low-dimensional control tasks by combining GP supervised uncertainty estimates with planner-level optimism; GP predictive uncertainty generalizes over state space and therefore often reduces the need for explicit exploration. In experiments GP-RMAXexp learns faster and closer to optimal than Sarsa(λ), and its continuous uncertainty mechanism produces more targeted exploration than a binary visited/unvisited scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Noted limitations include reliance on low-to-moderate dimensional state spaces (uniform grid planner suffers curse of dimensionality), assumption of deterministic/smooth transitions and known reward function (though GP can handle small noise), and computational cost of GP predictions across entire grid (planning cost can be dominated by GP evaluations). In practice GP-RMAXexp and GP-RMAXnoexp performed similarly in some domains because GP generalization caused rapid certainty propagation, reducing the benefit of explicit uncertainty-driven exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gaussian Processes for Sample Efficient Reinforcement Learning with RMAX-like Exploration', 'publication_date_yy_mm': '2012-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1111.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1111.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP-RMAXgrid</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Process RMAX with grid-based binary uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of GP-RMAX that uses a binary (counter/grid-cell) notion of known vs unknown state-actions to drive exploration (classic RMAX-style), combined with GP model learning for dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GP-RMAXgrid</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same GP-based model-learner and grid-based planner as GP-RMAX, but instead of using GP predictive variance to quantify uncertainty, it overlays a uniform grid in state-action space and marks cells as visited/unvisited (binary uncertainty). Planning uses RMAX-style optimism for unvisited cells.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Binary RMAX-style optimism using a visited/unvisited grid (counter-based exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The planner treats state-action grid cells as either 'known' or 'unknown' based on visitation counts; unknown cells receive the maximal optimistic value V_MAX during backups, causing the agent to explore them. The GP model is still trained from samples, but uncertainty does not directly influence the Bellman update.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Mountain Car; Inverted Pendulum; Bicycle balancing; Acrobot</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same as GP-RMAXexp: continuous, low-dimensional, deterministic transitions assumed, discrete actions, smooth dynamics, known reward, episodic.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same domain complexities as GP-RMAXexp (see above). Binary grid overlays add an extra discretization over state-action space used for visitation bookkeeping.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirically less sample-efficient than GP-RMAXexp: explores more and learns slower due to unsupervised (density-based) uncertainty; resulting policies are still improved over naive methods but require more samples to reach similar performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Lower than GP-RMAXexp; many grid cells remain unvisited early on so much of the state-action space remains overly optimistic and is explored inefficiently. Exact numeric sample counts not provided but learning curves in paper show slower convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Binary: unvisited cells are assigned V_MAX (full optimism), known cells use standard Bellman backups — leads to aggressive exploration of any unvisited regions irrespective of GP-inferred functional simplicity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared experimentally to GP-RMAXexp and GP-RMAXnoexp, and to Sarsa(λ) with tile coding.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Grid-based binary uncertainty leads to more exploration and slower sample efficiency than GP-supervised uncertainty. The paper demonstrates that unsupervised (density-based) uncertainty can over-explore compared to GP-supervised uncertainty that leverages target-function structure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Over-exploration in practice due to unsupervised notion of 'unknown' (depends only on visitation density), poor scaling with grid resolution and dimensionality, and sensitivity to the choice of grid overlay resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gaussian Processes for Sample Efficient Reinforcement Learning with RMAX-like Exploration', 'publication_date_yy_mm': '2012-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1111.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1111.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP-RMAXnoexp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Process RMAX without explicit exploration bonus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GP-based model-learning plus planning agent that does not modify Bellman updates for exploration (pure exploitation of the current model).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GP-RMAXnoexp</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Identical architecture to GP-RMAX variants (GP model-learner, grid-based value iteration planner) but uses standard Bellman backups (Eq. 7') without adding optimistic values for uncertain state-actions.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>None (no explicit adaptive experimental design; exploration occurs only through incidental sampling from policy execution)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>No explicit exploration mechanism; actions selected greedily with respect to current Q estimates from planner. Model is still updated from observed transitions and planning is invoked periodically.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Mountain Car; Inverted Pendulum; Bicycle balancing; Acrobot</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Continuous low-dimensional states, discrete actions, deterministic/smooth transitions assumed, known rewards, episodic.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same domain specifics as GP-RMAXexp. Planner grid sizes: 100×100 (2D), 20^4 (bicycle), 25^4 (acrobot).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Performed nearly the same as GP-RMAXexp in experiments; surprisingly similar sample efficiency and final performance in most domains due to strong GP generalization and supervised certainty estimates making explicit exploration less necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High: despite lack of an explicit exploration bonus, GP generalization allowed rapid learning in the studied domains; empirical behavior matched GP-RMAXexp in the reported benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>No explicit mechanism; exploitation of current model dominates, but incidental exploration arises from initial model uncertainty and environment dynamics during early interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to GP-RMAXexp, GP-RMAXgrid, and Sarsa(λ) with tile coding.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>In these benchmarks GP-RMAXnoexp often performed similarly to the uncertainty-driven GP-RMAXexp, indicating that supervised GP generalization can cause rapid certainty propagation and obviate much of the need for explicit optimistic exploration in some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Lack of explicit exploration could fail in tasks where GP model generalization is poor or target dynamics are more complex/non-smooth; performance depends on initial sample coverage and GP hyperparameter selection. Noted reliance on low-dimensional grids for planning limits scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gaussian Processes for Sample Efficient Reinforcement Learning with RMAX-like Exploration', 'publication_date_yy_mm': '2012-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1111.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1111.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sarsa(λ) with tile coding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sarsa(λ) on-line temporal-difference control using tile coding function approximation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-free RL baseline used for comparison; uses on-policy TD learning with eligibility traces and tile-coded state representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Sarsa(λ) with tile coding</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Standard model-free RL algorithm (Sarsa with eligibility traces) using tile-coding feature representations (two tiling granularities tried) for value function approximation; learns directly from sample transitions without building an explicit dynamics model.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>No explicit adaptive experimental design beyond the algorithm's ε-greedy or exploration schedule (not described in detail in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Not detailed in paper; used as a baseline with standard exploration mechanisms inherent to Sarsa (e.g., ε-greedy), but no model-based uncertainty-guided exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Mountain Car; Inverted Pendulum; Bicycle balancing; Acrobot</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same benchmark characteristics as above: continuous states, discrete actions, episodic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same domain specifics as GP-RMAX experiments (see above).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Generally required many more samples and often failed to reach optimal performance within the same sample budget; exception: acrobot where Sarsa achieved competitive performance. Exact curves presented in paper show Sarsa(λ) learning significantly slower than GP-RMAX variants in most tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Lower than GP-RMAX variants; significantly more episodes/transitions required to approach good performance on most benchmarks (quantitative curves provided in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Handled by standard Sarsa exploration mechanism (not elaborated); exploration is not guided by model uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Used as the primary empirical baseline for GP-RMAX variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Sarsa(λ) with tile coding had higher sample complexity and often did not learn optimal behaviors in the allotted episodes compared to GP-RMAX variants; supports the claim that model-based GP methods can be more sample-efficient in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Batch/fitted methods and other advanced algorithms were not directly compared experimentally here; Sarsa performance depends strongly on tile coding resolution (two configurations tried) and did not match GP-RMAX in most domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gaussian Processes for Sample Efficient Reinforcement Learning with RMAX-like Exploration', 'publication_date_yy_mm': '2012-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>R-MAX, a general polynomial time algorithm for near-optimal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Model-based exploration in continuous state spaces <em>(Rating: 2)</em></li>
                <li>Gaussian process dynamic programming <em>(Rating: 2)</em></li>
                <li>Bayes meets Bellman: The Gaussian process approach to temporal difference learning <em>(Rating: 1)</em></li>
                <li>Multi-resolution exploration in continuous spaces <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1111",
    "paper_id": "paper-15486186",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "GP-RMAXexp",
            "name_full": "Gaussian Process RMAX with uncertainty-weighted exploration",
            "brief_description": "A model-based RL agent that learns a GP dynamics model and applies optimism-in-the-face-of-uncertainty exploration by continuously weighting Bellman updates with GP predictive uncertainty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GP-RMAXexp",
            "agent_description": "Model-based online RL composed of (1) a Gaussian process (GP) model-learner that fits per-action, per-coordinate regressors predicting state differences and predictive variances, and (2) a planner that solves the Bellman equation by value iteration on a fixed uniform grid. GP predictive uncertainty is aggregated per state-action and injected into planning to bias exploration.",
            "adaptive_design_method": "Optimism-in-the-face-of-uncertainty via GP predictive variance (continuous RMAX-like optimism) — a form of uncertainty-weighted exploration / UCB-like optimism",
            "adaptation_strategy_description": "The agent computes a normalized scalar uncertainty c(x,a) from GP predictive variances (max over normalized per-coordinate variances). During value iteration the Bellman update is modified (Eq. 7\") to replace the usual update by a convex combination: (1 - c) * standard update + c * V_MAX (the optimistic maximal value). Thus actions with higher model uncertainty receive larger optimistic value boosts and are preferentially chosen; model updates occur from observed transitions and planning is invoked periodically (planning frequency K=50).",
            "environment_name": "Mountain Car; Inverted Pendulum; Bicycle balancing; Acrobot",
            "environment_characteristics": "Unknown transition dynamics (learned), continuous low-dimensional state spaces, discrete actions, deterministic (assumed), smooth transitions, known reward function, episodic tasks with bounded episode lengths.",
            "environment_complexity": "Mountain Car: 2D state (position, velocity), 3 discrete actions, episodes ≤ 500 steps. Inverted Pendulum: 2D state, 5 discrete actions, 500-step episodes. Bicycle: 4D state, 5 discrete actions, 500-step episodes. Acrobot: 4D state, 2 discrete actions, 500-step episodes. Planner grids: 100×100 for 2D, 20^4 for bicycle, 25^4 for acrobot.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Learns near-optimal policies with very low sample complexity across benchmarks (empirical best-offline values computed: mountain car 103 steps; inverted pendulum −18.41 total cost; bicycle −3.49 total cost; acrobot 64 steps). Example: after ≈120 transitions (mountain car) the approximated value function already resembles the true one. Overall GP-RMAXexp converges to near-optimal behavior in far fewer episodes than Sarsa(λ) (exact learning curves shown in paper).",
            "performance_without_adaptation": null,
            "sample_efficiency": "High: in mountain car significant approximation of optimal value after ~120 transitions; generally reaches near-optimal behavior within tens-to-low-hundreds of transitions depending on domain — substantially fewer samples than Sarsa(λ) with tile-coding in these tasks.",
            "exploration_exploitation_tradeoff": "Handled in planner via uncertainty-weighted optimism: c(x,a) (from GP) interpolates between full optimism (V_MAX) for high-uncertainty state-actions and standard Bellman backup for well-known state-actions, thus smoothly shifting from exploration to exploitation as model uncertainty decreases.",
            "comparison_methods": "Compared experimentally to GP-RMAXgrid (binary RMAX-style uncertainty), GP-RMAXnoexp (no optimism), and Sarsa(λ) with tile coding (two tile configurations). Related methods discussed: RMAX, fitted R-MAX, LSPI/LSTD/LSPE, fitted Q-iteration, and other GP/kernel RL work.",
            "key_results": "GP-RMAXexp achieves very low sample complexity and near-optimal performance in continuous low-dimensional control tasks by combining GP supervised uncertainty estimates with planner-level optimism; GP predictive uncertainty generalizes over state space and therefore often reduces the need for explicit exploration. In experiments GP-RMAXexp learns faster and closer to optimal than Sarsa(λ), and its continuous uncertainty mechanism produces more targeted exploration than a binary visited/unvisited scheme.",
            "limitations_or_failures": "Noted limitations include reliance on low-to-moderate dimensional state spaces (uniform grid planner suffers curse of dimensionality), assumption of deterministic/smooth transitions and known reward function (though GP can handle small noise), and computational cost of GP predictions across entire grid (planning cost can be dominated by GP evaluations). In practice GP-RMAXexp and GP-RMAXnoexp performed similarly in some domains because GP generalization caused rapid certainty propagation, reducing the benefit of explicit uncertainty-driven exploration.",
            "uuid": "e1111.0",
            "source_info": {
                "paper_title": "Gaussian Processes for Sample Efficient Reinforcement Learning with RMAX-like Exploration",
                "publication_date_yy_mm": "2012-01"
            }
        },
        {
            "name_short": "GP-RMAXgrid",
            "name_full": "Gaussian Process RMAX with grid-based binary uncertainty",
            "brief_description": "A variant of GP-RMAX that uses a binary (counter/grid-cell) notion of known vs unknown state-actions to drive exploration (classic RMAX-style), combined with GP model learning for dynamics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GP-RMAXgrid",
            "agent_description": "Same GP-based model-learner and grid-based planner as GP-RMAX, but instead of using GP predictive variance to quantify uncertainty, it overlays a uniform grid in state-action space and marks cells as visited/unvisited (binary uncertainty). Planning uses RMAX-style optimism for unvisited cells.",
            "adaptive_design_method": "Binary RMAX-style optimism using a visited/unvisited grid (counter-based exploration)",
            "adaptation_strategy_description": "The planner treats state-action grid cells as either 'known' or 'unknown' based on visitation counts; unknown cells receive the maximal optimistic value V_MAX during backups, causing the agent to explore them. The GP model is still trained from samples, but uncertainty does not directly influence the Bellman update.",
            "environment_name": "Mountain Car; Inverted Pendulum; Bicycle balancing; Acrobot",
            "environment_characteristics": "Same as GP-RMAXexp: continuous, low-dimensional, deterministic transitions assumed, discrete actions, smooth dynamics, known reward, episodic.",
            "environment_complexity": "Same domain complexities as GP-RMAXexp (see above). Binary grid overlays add an extra discretization over state-action space used for visitation bookkeeping.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirically less sample-efficient than GP-RMAXexp: explores more and learns slower due to unsupervised (density-based) uncertainty; resulting policies are still improved over naive methods but require more samples to reach similar performance.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Lower than GP-RMAXexp; many grid cells remain unvisited early on so much of the state-action space remains overly optimistic and is explored inefficiently. Exact numeric sample counts not provided but learning curves in paper show slower convergence.",
            "exploration_exploitation_tradeoff": "Binary: unvisited cells are assigned V_MAX (full optimism), known cells use standard Bellman backups — leads to aggressive exploration of any unvisited regions irrespective of GP-inferred functional simplicity.",
            "comparison_methods": "Compared experimentally to GP-RMAXexp and GP-RMAXnoexp, and to Sarsa(λ) with tile coding.",
            "key_results": "Grid-based binary uncertainty leads to more exploration and slower sample efficiency than GP-supervised uncertainty. The paper demonstrates that unsupervised (density-based) uncertainty can over-explore compared to GP-supervised uncertainty that leverages target-function structure.",
            "limitations_or_failures": "Over-exploration in practice due to unsupervised notion of 'unknown' (depends only on visitation density), poor scaling with grid resolution and dimensionality, and sensitivity to the choice of grid overlay resolution.",
            "uuid": "e1111.1",
            "source_info": {
                "paper_title": "Gaussian Processes for Sample Efficient Reinforcement Learning with RMAX-like Exploration",
                "publication_date_yy_mm": "2012-01"
            }
        },
        {
            "name_short": "GP-RMAXnoexp",
            "name_full": "Gaussian Process RMAX without explicit exploration bonus",
            "brief_description": "A GP-based model-learning plus planning agent that does not modify Bellman updates for exploration (pure exploitation of the current model).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GP-RMAXnoexp",
            "agent_description": "Identical architecture to GP-RMAX variants (GP model-learner, grid-based value iteration planner) but uses standard Bellman backups (Eq. 7') without adding optimistic values for uncertain state-actions.",
            "adaptive_design_method": "None (no explicit adaptive experimental design; exploration occurs only through incidental sampling from policy execution)",
            "adaptation_strategy_description": "No explicit exploration mechanism; actions selected greedily with respect to current Q estimates from planner. Model is still updated from observed transitions and planning is invoked periodically.",
            "environment_name": "Mountain Car; Inverted Pendulum; Bicycle balancing; Acrobot",
            "environment_characteristics": "Continuous low-dimensional states, discrete actions, deterministic/smooth transitions assumed, known rewards, episodic.",
            "environment_complexity": "Same domain specifics as GP-RMAXexp. Planner grid sizes: 100×100 (2D), 20^4 (bicycle), 25^4 (acrobot).",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Performed nearly the same as GP-RMAXexp in experiments; surprisingly similar sample efficiency and final performance in most domains due to strong GP generalization and supervised certainty estimates making explicit exploration less necessary.",
            "sample_efficiency": "High: despite lack of an explicit exploration bonus, GP generalization allowed rapid learning in the studied domains; empirical behavior matched GP-RMAXexp in the reported benchmarks.",
            "exploration_exploitation_tradeoff": "No explicit mechanism; exploitation of current model dominates, but incidental exploration arises from initial model uncertainty and environment dynamics during early interactions.",
            "comparison_methods": "Compared to GP-RMAXexp, GP-RMAXgrid, and Sarsa(λ) with tile coding.",
            "key_results": "In these benchmarks GP-RMAXnoexp often performed similarly to the uncertainty-driven GP-RMAXexp, indicating that supervised GP generalization can cause rapid certainty propagation and obviate much of the need for explicit optimistic exploration in some tasks.",
            "limitations_or_failures": "Lack of explicit exploration could fail in tasks where GP model generalization is poor or target dynamics are more complex/non-smooth; performance depends on initial sample coverage and GP hyperparameter selection. Noted reliance on low-dimensional grids for planning limits scalability.",
            "uuid": "e1111.2",
            "source_info": {
                "paper_title": "Gaussian Processes for Sample Efficient Reinforcement Learning with RMAX-like Exploration",
                "publication_date_yy_mm": "2012-01"
            }
        },
        {
            "name_short": "Sarsa(λ) with tile coding",
            "name_full": "Sarsa(λ) on-line temporal-difference control using tile coding function approximation",
            "brief_description": "A model-free RL baseline used for comparison; uses on-policy TD learning with eligibility traces and tile-coded state representation.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Sarsa(λ) with tile coding",
            "agent_description": "Standard model-free RL algorithm (Sarsa with eligibility traces) using tile-coding feature representations (two tiling granularities tried) for value function approximation; learns directly from sample transitions without building an explicit dynamics model.",
            "adaptive_design_method": "No explicit adaptive experimental design beyond the algorithm's ε-greedy or exploration schedule (not described in detail in this paper).",
            "adaptation_strategy_description": "Not detailed in paper; used as a baseline with standard exploration mechanisms inherent to Sarsa (e.g., ε-greedy), but no model-based uncertainty-guided exploration.",
            "environment_name": "Mountain Car; Inverted Pendulum; Bicycle balancing; Acrobot",
            "environment_characteristics": "Same benchmark characteristics as above: continuous states, discrete actions, episodic tasks.",
            "environment_complexity": "Same domain specifics as GP-RMAX experiments (see above).",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Generally required many more samples and often failed to reach optimal performance within the same sample budget; exception: acrobot where Sarsa achieved competitive performance. Exact curves presented in paper show Sarsa(λ) learning significantly slower than GP-RMAX variants in most tasks.",
            "sample_efficiency": "Lower than GP-RMAX variants; significantly more episodes/transitions required to approach good performance on most benchmarks (quantitative curves provided in paper).",
            "exploration_exploitation_tradeoff": "Handled by standard Sarsa exploration mechanism (not elaborated); exploration is not guided by model uncertainty.",
            "comparison_methods": "Used as the primary empirical baseline for GP-RMAX variants.",
            "key_results": "Sarsa(λ) with tile coding had higher sample complexity and often did not learn optimal behaviors in the allotted episodes compared to GP-RMAX variants; supports the claim that model-based GP methods can be more sample-efficient in these tasks.",
            "limitations_or_failures": "Batch/fitted methods and other advanced algorithms were not directly compared experimentally here; Sarsa performance depends strongly on tile coding resolution (two configurations tried) and did not match GP-RMAX in most domains.",
            "uuid": "e1111.3",
            "source_info": {
                "paper_title": "Gaussian Processes for Sample Efficient Reinforcement Learning with RMAX-like Exploration",
                "publication_date_yy_mm": "2012-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "R-MAX, a general polynomial time algorithm for near-optimal reinforcement learning",
            "rating": 2,
            "sanitized_title": "rmax_a_general_polynomial_time_algorithm_for_nearoptimal_reinforcement_learning"
        },
        {
            "paper_title": "Model-based exploration in continuous state spaces",
            "rating": 2,
            "sanitized_title": "modelbased_exploration_in_continuous_state_spaces"
        },
        {
            "paper_title": "Gaussian process dynamic programming",
            "rating": 2,
            "sanitized_title": "gaussian_process_dynamic_programming"
        },
        {
            "paper_title": "Bayes meets Bellman: The Gaussian process approach to temporal difference learning",
            "rating": 1,
            "sanitized_title": "bayes_meets_bellman_the_gaussian_process_approach_to_temporal_difference_learning"
        },
        {
            "paper_title": "Multi-resolution exploration in continuous spaces",
            "rating": 1,
            "sanitized_title": "multiresolution_exploration_in_continuous_spaces"
        }
    ],
    "cost": 0.013203999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Gaussian processes for sample efficient reinforcement learning with RMAX-like exploration
31 Jan 2012</p>
<p>Tobias Jung tjung@cs.utexas.edu 
Department of Computer Science
University of Texas at Austin</p>
<p>Peter Stone pstone@cs.utexas.edu 
Department of Computer Science
University of Texas at Austin</p>
<p>Gaussian processes for sample efficient reinforcement learning with RMAX-like exploration
31 Jan 2012D17F676100F3FD24DF6132E8B8A5816CarXiv:1201.6604v1[cs.AI]
We present an implementation of model-based online reinforcement learning (RL) for continuous domains with deterministic transitions that is specifically designed to achieve low sample complexity.To achieve low sample complexity, since the environment is unknown, an agent must intelligently balance exploration and exploitation, and must be able to rapidly generalize from observations.While in the past a number of related sample efficient RL algorithms have been proposed, to allow theoretical analysis, mainly model-learners with weak generalization capabilities were considered.Here, we separate function approximation in the model learner (which does require samples) from the interpolation in the planner (which does not require samples).For model-learning we apply Gaussian processes regression (GP) which is able to automatically adjust itself to the complexity of the problem (via Bayesian hyperparameter selection) and, in practice, often able to learn a highly accurate model from very little data.In addition, a GP provides a natural way to determine the uncertainty of its predictions, which allows us to implement the "optimism in the face of uncertainty" principle used to efficiently control exploration.Our method is evaluated on four common benchmark domains.</p>
<p>Introduction</p>
<p>In reinforcement learning (RL), an agent interacts with an environment and attempts to choose its actions such that an externally defined performance measure, the accumulated per-step reward, is maximized over time.One defining characteristic of RL is that the environment is unknown and that the agent has to learn how to act directly from experience.In practical applications, e.g., in robotics, obtaining this experience means having a physical system interact with the physical environment in real time.Therefore, RL methods that are able to learn quickly and minimize the amount of time the robot needs to interact with the environment until good or optimal behavior is learned, are highly desirable.</p>
<p>In this paper we are interested in online RL for tasks with continuous state spaces and smooth transition dynamics that are typical for robotic control domains.Our primary goal is to have an algorithm which keeps sample complexity as low as possible.</p>
<p>Overview of the contribution</p>
<p>To maximize sample efficiency, we consider online RL that is model-based in the spirit of RMAX [3], but extended to continuous state spaces similar to [1,10,5].As in RMAX and related methods, our algorithm, GP-RMAX, consists of two parts: a model-learner and a planner.The model-learner estimates the dynamics of the environment from the sample transitions the agent experiences while interacting with the environment.The planner is used to find the best possible action, given the current model.As the predictions of the model-learner become increasingly more accurate, the actions derived become increasingly closer to optimal.To control the amount of exploration, the "optimism in the face of uncertainty" principle is employed which makes the agent visit unexplored states first.In our algorithm, the model-learner is implemented by Gaussian process (GP) regression; being non-parametric, GPs give us enhanced modeling flexibility.GPs allow Bayesian model selection and automatic relevance determination.In addition, GPs provide a natural way to determine the uncertainty of predictions, which allows us to implement the "optimism in the face of uncertainty" exploration of RMAX in a principled way.The planner uses the estimated transition function (as estimated by the model) to solve the Bellman equation via value iteration on a uniform grid. 1he key point of our algorithm is that we separate the steps estimating a function from samples in the model-learner from solving the Bellman equation in the planner.The rationale behind this is that, if the transition function is relatively simple, it can be estimated accurately from only few sample transitions.On the other hand, the optimal value function, due to the inclusion of the max operator, often is a complex function with sharp discontinuities.Solving the Bellman equation, however, does not require actual "samples"; instead, we must only be able to evaluate the Bellman operator in arbitrary points of the state space.This way, when the transition function can be learned from only a few samples, large gains in sample efficiency are possible.Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.</p>
<p>Conceptually, our approach is closely related to Fitted R-MAX, which was proposed in [10] and uses an instance-based approach in the model-learner, and related work in [5,1], which uses grid-based interpolation in the model-learner.The primary contribution of this paper is to use GPs instead.Doing this means we are willing to trade off theoretical analysis with practical performance.For example, unlike the recent ARL [1], for which PAC-style performance bounds could be derived (because of its grid-based implementation of model-learning), a GP is much better able to handle generalization and as a consequence can achieve much lower sample complexity.</p>
<p>Assumptions and limitations</p>
<p>Our approach makes the following assumptions (most of which are also made in related work, even if it is not always explicitly stated):</p>
<p>-Low dimensionality of the state space.With a uniform grid, the number of grid points for solving the Bellman equation scales exponentially with the dimensionality.While more advanced methods, such as sparse grids or adaptive grids, may allow us to somewhat reduce this exponential increase, at the end they do not break the curse of dimensionality.Alternatively, one can use nonlinear function approximation; however, despite some encouraging results, it is unclear as to whether this approach would really do any better in general applications.Today, breaking the curse of dimensionality is still an open research problem.-Discrete actions.While continuous actions may be discretized, in practice, for higher dimensional action spaces this becomes infeasible.-Smooth transition function.Performing an action from states that are "close" must lead to successor states that are "close".(Otherwise both the generalization in the model learner and the interpolation in the value function approximation would not work).-Deterministic transitions.This is not a fundamental requirement of our approach, since GPs can also learn noisy functions (either due to observation noise or random disturbances with small magnitude), and the Bellman operator can be evaluated in the resulting predictive distribution.Rather it is one taken for convenience.-Known reward function.Assuming that the reward function is known and only the transition function needs to be learned is what is different from most comparable work.While it is not a fundamental requirement of our approach (since we could learn the reward function as well), it is an assumption that we think is well justified: for one, reward is the performance criterion and specifies the goal.For the type of control problems we consider here, reward is always externally defined and never something that is "generated" from within the environment.Two, reward sometimes is a discontinuous function, e.g., +1 at the goal state and 0 elsewhere.Which makes it not very amenable for function approximation.</p>
<p>2 Background: Planning when the model is exact</p>
<p>Consider the reinforcement learning problem for MDPs with continuous state space, finite action space, discounted reward criterion and deterministic dynamics [19].In this section we assume that dynamics and rewards are available to the learning agent.Let state space X be a hyperrectangle in Ê d (this assumption is justified if, for example, the system is a motor control task), A be the finite action space (assuming continuous controls are discretized), x t+1 = f (x t , a t ) be the transition function (assuming that continuous time problems are discretized in time), and r(x, a) be the reward function.For the following theoretical argument (ξ00, q a ′ 00 ) (ξ10, q a ′ 10 ) (ξ11, q a ′ 11 )</p>
<p>(ξ01, q a ′ 01 )</p>
<p>Fig. 1.
Bilinear interpolation to determine Q(f (ξi, a), a ′ ) in Ê 2 .
we require that both transition and reward function are Lipschitz continuous in the actions; i.e., there exist constants
L f , L r such that f (x, a) − f (x ′ , a) ≤ L f x − x ′ , and |r(x, a) − r(x ′ , a)| ≤ L r x − x ′ , ∀x, x ′ ∈ X , a ∈ A.
In addition, we assume that the reward is bounded, |r(x, a)| ≤ R MAX , ∀x, a.Note that in practice, while the first condition, continuity in the transition function, is usually fulfilled for domains derived from physical systems, the second condition, continuity in the rewards, is often violated (e.g. in the mountain car domain, reward is 0 in the goal and −1 everywhere else).Despite that we find that in many of these cases the outlined procedure may still work well enough.</p>
<p>For any state x, we are interested in determining a sequence of actions a 0 , a 1 , a 2 , . . .such that the accumulated reward is maximized,
V * (x) := max a0,a1,... ∞ t=0 γ t r(x t , a t ) | x 0 = x, x t+1 = f (x t , a t ) ,
where 0 &lt; γ &lt; 1.Using the Q-notation, where Q * (x, a) := r(x, a)+γV * (f (x, a)), the optimal decision policy π * is found by first solving the Bellman equation in the unknown function Q,
Q(x, a) = r(x, a) + γ max a ′ Q(f (x, a), a ′ ) ∀x ∈ X , a ∈ A(1)
to yield Q * , and then choosing the action with the highest Q-value,
π * (x) = argmax a ′ Q * (x, a ′ ).
The Bellman operator T related to (1) is defined by
T Q (x, a) := r(x, a) + γ max a ′ Q(f (x, a), a ′ ).(2)
It is well known that T is a contraction and Q * the unique bounded solution to the fixed point problem Q(x, a) = T Q (x, a), ∀x, a.</p>
<p>In order to solve the infinite dimensional problem in (1) numerically, we have to reduce it to a finite dimensional problem.This is done by introducing a discretization Γ of X into a finite number of elements, applying the Bellman operator to only the nodes and interpolating in between.</p>
<p>In the following we will consider a uniform grid Γ h with N vertices ξ i and d-dimensional tensor B-spline interpolation of order 1.The solution of ( 1) is then obtained in the space of piecewise affine functions.</p>
<p>For a fixed action a ′ , the value Q Γ h (z, a ′ ) of any state z with respect to grid Γ h can be written as a convex combination of the vertices ξ j of the grid cell enclosing z with coefficients w ij (see Figure 1a).For example, consider the 2-dimensional case (bilinear interpolation) in Figure 1b.Let z = (x, y) ∈ Ê 2 .To determine Q Γ h (z, a ′ ), we find the four vertices ξ 00 , ξ 01 , ξ 10 , ξ 11 ∈ Ê 2 of the enclosing cell with known function values q a ′ 00 := Q Γ h (ξ 00 , a ′ ), . . .etc.We then perform two linear interpolations along the x-coordinate (order invariant) in the auxilary points x 0 , x 1 to obtain
Q Γ h (x 0 , a ′ ) = (1 − λ 0 )q a ′ 00 + λ 0 q a ′ 01 Q Γ h (x 1 , a ′ ) = (1 − λ 0 )q a ′ 10 + λ 0 q a ′ 11
where λ 0 := d x /h x (see Figure 1b for a definition of d x , h x , x 0 , x 1 ).We then perform another linear interpolation in x 0 , x 1 along the y-coordinate to obtain
Q Γ h (z, a ′ ) = (1 − λ 1 )(1 − λ 0 )q a ′ 00 + (1 − λ 1 )λ 0 q a ′ 01 + λ 1 (1 − λ 0 )q a ′ 10 + λ 1 λ 0 q a ′ 11(3)
where λ 1 := d y /h y .Weights w ij now correspond to the coefficients in ( 3).An analogous procedure applies to higher dimensions.</p>
<p>Let Q a ′ be the N ×1 vector with entries
[Q a ′ ] i = Q Γ h (ξ i , a ′ ). Let z a 1 , . . . , z a N ∈ R d denote
the successor state we obtain when we apply the transition function f to vertices ξ i using action a, i.e., z a i := f (ξ i , a).Let [w a i ] j = w a ij denote the 1 × N vector of coefficients for z a i from (3).The Q-value of z a i for any action a ′ with respect to grid Γ h can thus be written as
Q Γ h (z a i , a ′ ) = N j=1 [w a i ] j [Q a ′ ] j . Let W a with rows [w a
i ] be the N × N matrix of all coefficients.(Note that this matrix is sparse: each row contains only 2 d nonzero entries).</p>
<p>Let R a be the N × 1 vector of associated rewards, [R a ] i := r(ξ i , a).Now we can use (2) to obtain a fixed point equation in the vertices of the grid Γ h ,
Q Γ h (ξ i , a) = T Γ h Q Γ h (ξ i , a) i = 1, . . . , N, a = 1, . . . , |A|,(4)
where
T Γ h Q Γ h (ξ i , a) := r(ξ i , a) + γ max a ′ Q Γ h (f (ξ i , a), a ′
). Slightly abusing the notation, we can write this more compactly in terms of matrices and vectors,
T Γ h Q Γ h := R a + γ max a ′ W a Q a ′ ∀a.(5)
The Q-function is now represented by |A| N -dimensional vectors Q a ′ , each containing the values for the vertices ξ i .The discretized Bellman operator
T Γ h is x t+1 = f (x t , a t ) x t x t a t (x t , a t , x t+1 ) f (x, a), c(x, a)
Fig. 2. High-level overview of the GP-RMAX framework a contraction in Ê d × A and therefore has a unique fixed point
Q * ∈ Ê d × A. Let function Q * ,Γ h : (Ê d × A) → Ê be the Q-function obtained by linear in-
terpolation of vector Q * along states.The function Q * ,Γ h can now be used to determine (approximately) optimal control actions: for any state x ∈ X , we simply determine
π * ,Γ h (x) = argmax a ′ Q * ,Γ h (x, a ′ ).
In order to estimate how well function Q * ,Γ h approximates the true Q * , a posteriori estimates can be defined that are based on local errors, i.e. the maximum of residual in each grid cell.The local error in a grid cell in turn depends on the granularity of the grid, h, and the modulus of continuity L f , L g (e.g., see [9,14] for details).</p>
<p>3 Our algorithm: GP-RMAX</p>
<p>In the last section we have seen how, for a continuous state space, optimal behavior of an agent can be obtained in a numerically robust way, given that the transition function
x t+1 = f (x t , a t ) is known. 2
For model-based RL we are now interested in solving the same problem for the case that the transition function is not known.Instead, the agent has to interact with the environment, and only use the samples it observes to compute optimal behavior.Our goal in this paper is to develop a learning framework where this number is kept as small as possible.This will be done by using the samples to learn an estimate f (x, a) of f (x, a) and then use this estimate f in place of f in the numerical procedure outlined in the previous section.</p>
<p>Overview</p>
<p>A sketch of our architecture is shown in Figure 2. GP-RMAX consists of the two parts model learning and planning which are interwoven for online learning.The model-learner estimates the dynamics of the environment from the sample transitions the agent experiences while interacting with the environment.The planner is used to find the best possible action, given the current model.As the predictions of the model-learner become increasingly more accurate, the actions derived from the planner become increasingly closer to optimal.Below is a highlevel overview of the algorithm:</p>
<p>-Input:</p>
<p>• Reward function r(x, a)</p>
<p>• Discount factor γ • Performance parameters: * planning and model-update frequency * only every K steps, and only if M t is not sufficiently exact (as determined by evaluating the stopping criterion)
K * model accuracy δ M 1 , δ M 2 (stopping criterion for model-learning) * discretization of planner N -Initialize: • Model M 1 , Q-function Q 1 , observed transitions D 1 -Loop: t = 1,• M t+1 = update model (M t , D t+1 ) • evaluate stopping criterion (M t+1 , M t , δ M 1 , δ M 2 ) * else • M t+1 = M t • Planning with model: (see Section 3.3)
* only every K steps, and only if M t is not sufficiently exact (as determined by evaluating the stopping criterion)
• Q t+1 = augmented value iteration (Q ⊔ , M t+1 , @r(x, u), γ, N ) * else • Q t+1 = Q t
Next, we will explain in more detail how each of the two functional modules "model-learner" and "planner" is implemented.</p>
<p>Model learning with GPs</p>
<p>In essence, estimating f from samples is a regression problem.While in theory any nonlinear regression algorithm could serve this purpose, we believe that GPs are particularly well-suited: (1) being non-parametric means great modeling flexibility; (2) setting the hyperparameters can be done automatically (and in a principled way) via optimization of the marginal likelihood and allows automatic determination of relevant inputs; and (3) GPs provide a natural way to determine the uncertainty of its predictions which will be used to guide exploration.Furthermore, uncertainty in GPs is supervised in that it depends on the target function that is estimated (because of (2)); other methods only consider the density of the data (unsupervised) and will tend to overexplore if the target function is simple.</p>
<p>Assume we have observed a number of transitions, given as triplets of state, performed action, and resulting successor state, e.g., D = {x t , a t , x t+1 } t=1,2,... where
x t+1 = f (x t , a t ). Note that f is a d-dimensional function, f (x t , a t ) = f 1 (x t , a t ), . . . , f d (x t , a t )
T .Instead of trying to estimate f directly (which corresponds to absolute transitions), we try to estimate the relative change x t+1 −x t as in [10].The effect of each action on each state variable will be treated independently: we train multiple univariate GPs and combine the individual predictions afterwards.Each individual GP ij is trained in the respective subset of data in D, e.g., GP ij is trained on all x t as input, and x
(i) t+1 − x (i) t
as output, where a t = j.Each individual GP ij has its own set of hyperparameters obtained from optimizing the marginal likelihood.</p>
<p>The details of working 3 with GPs can be found in [17]; using GPs to learn a model for RL was previously also studied in [6] (for offline RL and without uncertainty-guided exploration).One characteristic of GPs is that their functional form is given in terms of a parameterized covariance function.Here we use the squared exponential,
k(x, x ′ ; v 0 , b, θ) = v 0 exp −0.5(x − x ′ ) T Ω(x − x ′ ) + b,
where matrix Ω is either one of the following: (1) Ω = θI (uniform), ( 2)
Ω = diag(θ 1 , . . . , θ d ) (axis aligned ARD), (3) Ω = M k M T
k (factor analysis).Scalars v 0 , b and the (Ω-dependent number of) entries of θ constitute the hyperparameters of the GP and are adapted from the training data (likelihood 3 There is also the problem of implementing GPs efficiently when dealing with a possible large number of data points.For the lack of space we can only sketch our particular implementation, see [16] for more detailed information.Our GP implementation is based on the subset of regressors approximation.The elements of the subset are chosen by a stepwise greedy procedure aimed at minimizing the error incurred from using a low rank approximation (incomplete Cholesky decomposition).Optimization of the likelihood is done on random subsets of the data of fixed size.</p>
<p>To avoid a degenerate predictive variance, the projected process approximation was used.</p>
<p>optimization).Note that variant (2) and (3) implement automatic relevance determination: relevant inputs or linear projections of inputs are automatically identified, whereby model complexity is reduced and generalization sped up.Once trained, for any testpoint x, GP ij provides a distribution over target values, N (µ ij (x), σ 2 ij (x)), with mean µ ij (x) and variance σ 2 ij (x) (exact formulas for µ and σ can be found in [17]).Each individual mean µ ij predicts the change in the i-th coordinate of the state under the j-th action.Each individual variance σ 2 ij can be interpreted as the associated uncertainty; it will be close to 0 if GP ij is certain, and close to k(x, x) if it is uncertain (the value of k(x, x) depends on the hyperparameters of GP ij ).Stacking the individual predictions together, our model-learner produces in summary f (x, a) :=   </p>
<p>x (1)  . . .
x (d)    +    µ 1a (x) . . . µ da (x)    , c(x, a) := max i=1,...,d normalize ia (σ 2 ia ) ,(6)
where f (x, a) is the predicted successor state and c(x, a) the associated uncertainty (taken as maximum over the normalized per-coordinate uncertainties, where normalization ensures that the values lie between 0 and 1).</p>
<p>Planning with a model</p>
<p>At any time t, the planner receives as input model M t .For any state x and action a, model M t can be evaluated to "produce" the transition f (x, a) along with normalized scalar uncertainty c(x, a) ∈ [0, 1], where 0 means maximally certain and 1 maximally uncertain (see Section 3.2) Let Γ h be the discretization of the state space X with nodes ξ i , i = 1, . . ., N .We now solve the planning stage by plugging f into the procedure described in Section 2. First, we compute za i = f (ξ i , a), c(ξ i , a) from ( 6) and the associated interpolation coefficients w a ij from (3) for each node ξ i and action a.Let C a denote the N × 1 vector corresponding to the uncertainties, [C a ] i = c(ξ i , a); and R a be the N ×1 vector corresponding to the rewards, [R a ] i = r(ξ i , a).To solve the discretized Bellman equation in Eq. ( 4), we perform basic Jacobi iteration:</p>
<p>-
Initialize [Q a 0 ] i , i = 1, . . . , N , a = 1, . . . , |A| -Repeat for k = 0, 1, 2, . . . [Q a k+1 ] i = [R a ] i + γ max a ′    N j=1 w a ij [Q a ′ k ] j    ∀i, a(7)
until |Q a k+1 −Q a k | ∞ &lt; tol, ∀a, or a maximum number of iterations is reached.</p>
<p>To reduce the number of iterations necessary, we adapt Grüne's increasing coordinate algorithm [9] to the case of Q-functions: instead of Eq. ( 7), we perform updates of the form
[Q a k+1 ] i = [1 − γw a ii ] −1   [R a ] i + γ max a ′    N j=1,j =i w a ij [Q a ′ k ] j      . (7'
)</p>
<p>In [9] it was proved that Eq. (7') converges to the same fixed point as Eq. ( 7), and it was empirically demonstrated that convergence can occur in significantly fewer iterations.The exact reduction is problem-dependent, savings will be greater for small γ and large cells where self-transitions occur (i.e., ξ i is among the vertices of the cell enclosing za i ).To implement the "optimism in the face of uncertainty" principle, that is, to make the agent explore regions of the state space where the model predictions are uncertain, we employ the heuristic modification of the Bellman operator which was suggested in [15] and shown to perform well.Instead of Eq. (7'), the update rule becomes
[Q a k+1 ] i = (1 − [C a ] i )[1 − γw a ii ] −1   [R a ] i + γ max a ′    N j=1,j =i w a ij [Q a ′ k ] j      + + [C a ] i V MAX (7")
where
V MAX := R MAX /(1 − γ).
Eq. (7") can be seen as a generalization of the binary uncertainty in the original RMAX paper to continuous uncertainty; whereas in RMAX a state was either "known" (sufficiently explored), in which case the unmodified update was used, or "unknown" (not sufficiently explored), in which case the value V MAX was assigned, here the shift from exploration to exploitation is more gradual.Finally we can take advantage of the fact that the planning function will be called many times during the process of learning.Since the discretization Γ h is kept fixed, we can reuse the final Q-values obtained in one call to plan as initial values for the next call to plan.Since updates to the model often affect only states in some local neighborhood (in particular in later stages), the number of necessary iterations in each call to planning will be further reduced.</p>
<p>A summary of our model-based planning function is shown below.</p>
<p>-Input:
• Model M t , initial [Q a 0 ] i , i = 1, . . . , N , a = 1, . . . , |A| -Static inputs: • Grid Γ h with nodes ξ 1 , . . . , ξ N , discount factor γ, reward function r(x, a) evaluated in nodes giving [R a ] i -Initialize: • Compute za i = f (ξ i , a
) and [C a ] i from M t (see Eq. ( 6)) • Compute weights w a ij for each za i (see Eq. ( 3)) -Loop:</p>
<p>• Repeat update Eq. (7") until |Q a k+1 − Q a k | ∞ &lt; tol, ∀a, or the maximum number of iterations is reached.</p>
<p>We now examine the online learning performance of GP-RMAX in various wellknown RL benchmark domains.</p>
<p>Description of domains</p>
<p>In particular, we choose the following domains (where a large number of comparative results is available in the literature):</p>
<p>Mountain car: In mountain car, the goal is to drive an underpowered car from the bottom of a valley to the top of one hill.The car is not powerful enough to climb the hill directly, instead it has to build up the necessary momentum by reversing throttle and going up the hill on the opposite side first.The problem is 2-dimensional, state variable x 1 ∈ [−1.2, 0.5] describes the position of the car, x 2 ∈ [−0.07, 0.07] its velocity.Possible actions are a ∈ {−1, 0, +1}.Learning is episodic: every step gives a reward of −1 until the top of the hill at x 1 ≥ 0.5 is reached.Our experimental setup (dynamics and domain specific constants) is the same as in [19], with the following exceptions: maximal episode length is 500 steps, discount factor γ = 0.99 and every episode starts with the agent being at the bottom of the valley with zero velocity, x start = (−π/6, 0).</p>
<p>Inverted pendulum:</p>
<p>The next task is to swing up and stabilize a single-link inverted pendulum.As in mountain car, the motor does not provide enough torque to push the pendulum up in a single rotation.Instead, the pendulum needs to be swung back and forth to gather energy, before being pushed up and balanced.This creates a more difficult, nonlinear control problem.The state space is 2-dimensional, θ ∈ [−π, π] being the angle, θ ∈ [−10, 10] the angular velocity.Control force is discretized to a ∈ {−5, −2.5, 0, +2.5, +5} and held constant for 0.2sec.Reward is defined as r(x, a) := −0.1x 2 1 − 0.01x 2 2 − 0.01a 2 .The remaining experimental setup (equations of motion and domain specific constants) is the same as in [6].The task is made episodic by resetting the system every 500 steps to the initial state x start = (0, 0).Discount factor γ = 0.99.</p>
<p>Bicycle: Next we consider the problem of balancing a bicycle that rides at a constant speed [8], [12].The problem is 4-dimensional: state variables are the roll angle ω ∈ [−12π/180, 12π/180], roll rate ω ∈ [−2π, 2π], angle of the handle bar α ∈ [−80π/180, 80π/180], and the angular velocity α ∈ [−2π, 2π].The action space is inherently 2-dimensional (displacement of rider from the vertical and turning the handlebar); in RL it is usually discretized into 5 actions.Our experimental setup so far is similar to [8].To allow a more conclusive comparison of performance, instead of just being able to keep the bicycle from falling, we define a more discriminating reward r(x, a) = −x 2  1 , and r(x, a) = −10 for |x 1 | &lt; 12π/180 (bicycle has fallen).Learning is episodic: every episode starts in one of two (symmetric) states close to the boundary from where recovery is impossible: x start = (10π/180, 0, 0, 0) or x start = (−10π/180, 0, 0, 0), and proceeds for 500 steps or until the bicycle has fallen.Discount factor γ = 0.98.Acrobot: Our final problem is the acrobot swing-up task [19].The goal is to swing up the tip of the lower link of an underactuated two-link robot over a given height (length of first link).Since only the lower link is actuated, this is a rather challenging problem.The state space is 4-dimensional:
θ 1 ∈ [−π, π], θ1 ∈ [−4π, 4π], θ 2 ∈ [−π, π], θ2 ∈ [−9π, 9π].
Possible actions are a ∈ {−1, +1}.Our experimental setup and implementation of state transition dynamics is similar to [19].The objective of learning is to reach a goal state as quickly as possible, thus r(x, a) = −1 for every step.The initial state for every episode is x start = (0, 0, 0, 0).An episode ends if either a goal state is reached or 500 steps have passed.The discount factor was set to γ = 1, as in [19].</p>
<p>Results</p>
<p>We now apply our algorithm GP-RMAX to each of the four problems.The granularity of the discretization Γ h in the planner is chosen such that for the 2dimensional problems, the loss in performance due to discretization is negligible.For the 4-dimensional problems, we ran offline trials with the true transition function to find the best compromise of granularity and computational efficiency.As result, we use a 100 × 100 grid for mountain car and inverted pendulum, a 20 × 20 × 20 × 20 grid for the bicycle balancing task, and a 25 × 25 × 25 × 25 grid for the acrobot.The maximum number of value iterations was set to 500, tolerance was &lt; 10 −2 .In practice, running the full planning step took between 0.1-10 seconds for the small problems, and less than 5 min for the large problems (where often more than 50% of the CPU time was spent on computing the GP predictions in all the nodes of the grid).Using the planning module offline with the true transition function, we computed the best possible performance for each domain in advance.We obtained: mountain car (103 steps), inverted pendulum (-18.41 total cost), bicycle balancing (-3.49total cost), and acrobot (64 steps). 4or the GP-based model-learner, we set the maximum size of the subset to 1000, and ICD tolerance to 10 −2 .The hyperparameters of the covariance were not manually tuned, but found from the data by likelihood optimiziation.</p>
<p>Since it would be computationally too expensive to update the model and perform the full planning step after every single observation, we set the planning frequency K to 50 steps.To gauge if optimal behavior is reached and further learning becomes unnessecary, we monitor the change in the model predictions and uncertainties between successive updates and stop if both fall below a threshold (test points in a fixed coarse grid).</p>
<p>We consider the following variations of the base algorithm: (1) GP-RMAXexp, which actively explores by adjusting the Bellman updates in Eq. (7") according to the uncertainties produced by the GP prediction; (2) GP-RMAXgrid, which does the same but uses binary uncertainty by overlaying a uniform grid on top of the state-action space and keeping track which cells are visited; and (3) GP-RMAXnoexp, which does not actively explore (see Eq. (7')).For comparison, we repeat the experiments using the standard online model-free RL algorithm Sarsa(λ) with tile coding [19], where we consider two different setup of the tilings (one finer and one coarser).</p>
<p>Figure 3 shows the result of online learning with GP-RMAX and Sarsa.In short, the graphs show us two things in particular: (1) GP-RMAX learns very quickly; and (2) GP-RMAX learns a behavior that is very close to optimal.In comparison, Sarsa(λ) has a much higher sample complexity and does not always learn the optimal behavior (exception is the acrobot).While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.</p>
<p>Examining the plots in more detail, we find that, while GP-RMAXgrid is somewhat less sample efficient (explores more), GP-RMAXexp and GP-RMAXnoexp perform nearly the same.Initially, this appears to be in contrast with the whole point of RMAX, which is efficient exploration guided by the uncertainty of the predictions.Here, we believe that this behavior can be explained by the good generalization capabilities of GPs. Figure 4 illustrates model learning and certainty propagation with GPs in the mountain car domain (predicting acceleration as function of state).The state of the model-learner is shown for two snapshots: after 40 transitions and after 120 transitions.The top row shows the value function that results from applying value iteration with the update modified for uncertainty, see Eq. (7").The bottom row shows the observed samples and the associated certainty of the predictions.As expected, certainty is high in regions where data was observed.However, due to the generalization of GPs and data-dependent hyperparameter selection, certainty is also high in unexplored regions; and in particular it is constant along the y-coordinate.To understand this, we have to look at the state transition function of the mountain car: acceleration of the car indeed only depends on the position, but not on velocity.This shows that certainty estimates of GPs are supervised and take the properties of the target function into account, whereas prior RMAX treatments of uncertainty are unsupervised and only consider the density of samples to decide if a state is "known".For comparison, we also show what GP-RMAX with grid-based uncertainty would produce in the same situation.</p>
<p>Summary</p>
<p>We presented an implementation of model-based online reinforcement learning similar to RMAX for continuous domains by combining GP-based model learning and value iteration on a grid.Doing so, our algorithm separates the problem function approximation in the model-learner from the problem function approximation/interpolation in the planner.If the transition function is easier to learn, i.e., requires only few samples relative to the representation of the optimal value   The top row shows the value function that results from applying value iteration with the update modified for uncertainty, see Eq. (7").The bottom row shows the actual samples (red circles) and the induced uncertainty of all states: black is perfectly "known", white is perfectly "unknown".Panels (a) and (b) show that with GPs certainty of model predictions is rapidly propagated through the whole state space, leading to strong generalization and targeted exploration.This in turn allows the optimal value function to be learned from very few sample transitions: panel (b) shows that after only 120 transitions (still in the middle of the very first episode) the approximated value function already resembles the true one [19].Panel (c) shows the same for a counter-based binary uncertainty; most of the grid cells are unvisited and the thus the approximate value function is zero in most parts of the state space.</p>
<p>function, then large savings in sample-complexity can be gained.Related modelfree methods, such as fitted Q-iteration, can not take advantage of this situation.The fundamental limitation of our approach is that it relies on solving the Bellman equation globally over the state space.Even with more advanced discretization methods, such as adaptive grids, or sparse grids, the curse of dimensionality limits the applicability to problems with low or moderate dimensionality.Other, more minor limitations, concern the simplifying assumptions we made: deterministic state transitions and known reward function.However, these are not conceptual limitations but rather simplifying assumptions made for the present paper; they could be easily addressed in future work.</p>
<p>Fig. 3 .
3
Fig.3.Learning curves of our algorithm GP-RMAX (left column) and the standard method Sarsa(λ) with tile coding (right column) in the four benchmark domains.Each curve shows the online learning performance and plots the total reward as a function of the episode (and thus sample complexity).The black horizontal line denotes the best possible performance computed offline.Note the different scale of the x-axis between GP-RMAX and Sarsa.</p>
<p>Fig. 4 .
4
Fig.4.Model-learning and propagation of "knownness" of state-action pairs with GPs.The top row shows the value function that results from applying value iteration with the update modified for uncertainty, see Eq. (7").The bottom row shows the actual samples (red circles) and the induced uncertainty of all states: black is perfectly "known", white is perfectly "unknown".Panels (a) and (b) show that with GPs certainty of model predictions is rapidly propagated through the whole state space, leading to strong generalization and targeted exploration.This in turn allows the optimal value function to be learned from very few sample transitions: panel (b) shows that after only 120 transitions (still in the middle of the very first episode) the approximated value function already resembles the true one[19].Panel (c) shows the same for a counter-based binary uncertainty; most of the grid cells are unvisited and the thus the approximate value function is zero in most parts of the state space.</p>
<p>While certainly more advanced methods exist, e.g.,[9,14], for our purpose here, a uniform grid is sufficient as proof of concept.
Remember our working assumption: reward as a performance criterion is externally given and does not need to be estimated by the agent. Also note that discretization (even with more advanced methods like adaptive or sparse grids) is likely to be feasible only in state spaces with low to medium dimensionality. Breaking the curse of dimensionality is an open research problem.
Note that 64 steps is not the optimal solution, [2] demonstrated swing-up with 61 steps.
AcknowledgmentsThis work has taken place in the Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory, The University of Texas at Austin.LARG research is supported in part by grants from the National Science Foundation (IIS-0917122), ONR (N00014-09-1-0658), DARPA (FA8650-08-C-7812), and the Federal Highway Administration (DTFH61-07-H-00030).
Adaptive-resolution reinforcement learning with efficient exploration. A Bernstein, N Shimkin, 10.1007/s10994-010-5186-7Machine Learning. May 2010. 2010</p>
<p>Minimum-time control of the acrobot. G Boone, Proc. of IEEE International Conference on Robotics and Automation. of IEEE International Conference on Robotics and Automation19974</p>
<p>R-MAX, a general polynomial time algorithm for near-optimal reinforcement learning. R Brafman, M Tennenholtz, JMLR. 32002</p>
<p>Online least-squares policy iteration for reinforcement learning control. L Busoniu, D Ernst, B De Schutter, R Babuska, American Control Conference (ACC-10). 2010</p>
<p>Multidimensional triangulation and interpolation for reinforcement learning. S Davies, NIPS 9. Morgan1996</p>
<p>Gaussian process dynamic programming. M P Deisenroth, C E Rasmussen, J Peters, Neurocomputing. 727-92009</p>
<p>Bayes meets Bellman: The Gaussian process approach to temporal difference learning. Y Engel, S Mannor, R Meir, Proc. of ICML 20. of ICML 202003</p>
<p>Tree-based batch mode reinforcement learning. D Ernst, P Geurts, L Wehenkel, JMLR. 62005</p>
<p>An adaptive grid scheme for the discrete Hamilton-Jacobi-Bellman equation. L Grüne, Numerische Mathematik. 751997</p>
<p>Model-based exploration in continuous state spaces. N K Jong, P Stone, The 7th Symposium on Abstraction, Reformulation and Approximation. 2007</p>
<p>Learning robocup-keepaway with kernels. T Jung, D Polani, Workshop and Conference Proceedings (Gaussian Processes in Practice). 20071</p>
<p>M G Lagoudakis, R Parr, Least-squares policy iteration. JMLR. 20034</p>
<p>Online exploration in least-squares policy iteration. L Li, M L Littman, C R Mansley, Proc. of 8th AAMAS. of 8th AAMAS2009</p>
<p>Variable resolution discretization in optimal control. R Munos, A Moore, Machine Learning. 200249</p>
<p>Multi-resolution exploration in continuous spaces. A Nouri, M L Littman, NIPS 21. 2008</p>
<p>Approximation methods for gaussian process regression. J Quiñonero-Candela, C E Rasmussen, C K I Williams, Large Scale Learning Machines. Leon Bottou, Olivier Chapelle, Dennis Decoste, Jason Weston, MIT Press2007</p>
<p>C E Rasmussen, C K I Williams, Gaussian Processes for Machine Learning. MIT Press2006</p>
<p>Neural fitted q-iteration. M Riedmiller, Proc. of 16th ECML. of 16th ECML2005</p>
<p>Reinforcement Learning: An Introduction. R Sutton, A Barto, 1998MIT Press</p>            </div>
        </div>

    </div>
</body>
</html>
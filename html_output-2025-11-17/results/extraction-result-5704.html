<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5704 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5704</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5704</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-117.html">extraction-schema-117</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-267627630</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.07862v2.pdf" target="_blank">AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) match and sometimes exceeding human performance in many domains. This study explores the potential of LLMs to augment human judgement in a forecasting task. We evaluate the effect on human forecasters of two LLM assistants: one designed to provide high-quality ("superforecasting") advice, and the other designed to be overconfident and base-rate neglecting, thus providing noisy forecasting advice. We compare participants using these assistants to a control group that received a less advanced model that did not provide numerical predictions or engaged in explicit discussion of predictions. Participants (N = 991) answered a set of six forecasting questions and had the option to consult their assigned LLM assistant throughout. Our preregistered analyses show that interacting with each of our frontier LLM assistants significantly enhances prediction accuracy by between 24 percent and 28 percent compared to the control group. Exploratory analyses showed a pronounced outlier effect in one forecasting item, without which we find that the superforecasting assistant increased accuracy by 41 percent, compared with 29 percent for the noisy assistant. We further examine whether LLM forecasting augmentation disproportionately benefits less skilled forecasters, degrades the wisdom-of-the-crowd by reducing prediction diversity, or varies in effectiveness with question difficulty. Our data do not consistently support these hypotheses. Our results suggest that access to a frontier LLM assistant, even a noisy one, can be a helpful decision aid in cognitively demanding tasks compared to a less powerful model that does not provide specific forecasting advice. However, the effects of outliers suggest that further research into the robustness of this pattern is needed.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5704.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5704.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Superforecasting LLM (treatment)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-Turbo with a 'superforecasting' system prompt (superforecasting assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-4-Turbo chat assistant instrumented via expert prompt engineering to act like a 'superforecaster' (breaking problems into subproblems, using reference classes/base rates, providing uncertainty intervals and explicit rationales) and to provide numerical forecasts and reasoning in a back-and-forth conversational setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-Turbo (gpt-4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A transformer-based large language model from OpenAI (GPT-4 family). In this study it was deployed with a detailed system prompt instructing superforecasting behaviours (10 commandments), run via API with a 128k token input context window, 4096 token output limit, set to temperature 0.8 and a maximum output limit of 1024 tokens; the model had no internet access and was not provided external retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>A set of six real-world continuous forecasting targets (finance, geopolitics, cryptocurrency, aviation, scientific output, and FX). Example scientific-related target: 'How many AI papers will be published on ArXiv during December 1–31, 2023?' (count of AI papers); other targets included Bitcoin network hash rate (TH/s) and counts/continuous real-world outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Expert prompt engineering: a detailed system prompt instructing the model to follow superforecasting principles (triage, reference-class/base-rate reasoning, uncertainty intervals, explicit pros/cons and numerical estimates). Human participants could interact in an iterative chat (back-and-forth) and request either direct LLM forecasts or reasoning and feedback on their own forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_protocol</strong></td>
                            <td>Forecast accuracy measured as absolute error |forecast − actual|, winsorised (initial 5% trimming then 3σ) and standardized by control-group SD per question; aggregated across six questions. Statistical tests included one-way ANOVA (F-tests), Tukey HSD pairwise comparisons, preregistered OLS regressions and mixed-effects models, Brier scores and Euclidean distance for skill measures, and bootstrap (10,000 resamples) for median-aggregation analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>A bespoke set of six time-resolved forecasting questions drawn from an early Forecasting Proficiency Test (Himmelstein et al. 2024); data collection on Nov 21, 2023 with outcomes resolved in late Dec 2023. A separate set of short binary and intersubjective questions was used to compute forecaster skill (for Brier and intersubjective measures).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Interacting with the superforecasting LLM augmentation significantly improved individual forecasting accuracy relative to the control (one-way ANOVA F(2,988)=34.58, p<.001). Across preregistered analyses the treatments improved accuracy by ≈24%-28% vs. control; pairwise mean error differences: treatment vs control = -0.21 (p<.001). Direct LLM-only forecasts (Table 3) showed the superforecasting assistant had consistently smaller deviations from truth than the noisy assistant across the 6 questions. In exploratory analyses excluding an outlier question (Q3), the superforecasting condition showed a mean error of 0.40 vs 0.67 for control (mean difference -0.27, p<.001) and outperformed the noisy variant (0.47).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Results hinge partly on an outlier (Question 3 — bitcoin hash rate) whose exclusion changes comparative effect sizes; participants were online Prolific users limiting generalisability; the LLM had no external retrieval (no RAG), and the study compares to a weak non-forecasting LLM control rather than a human-only control, so it cannot rule out that any LLM harms vs. unaided humans; limited message budget (25) and model hyperparameters (temperature 0.8) may affect behaviours; the approach tests forecasting targets (including publication counts) rather than predicting novel scientific 'discoveries' per se.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5704.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5704.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Noisy LLM (treatment noise)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-Turbo with a 'noisy' (biased) forecasting prompt (noisy assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The same GPT-4-Turbo model configured with a system prompt engineered to induce base-rate neglect, overconfidence, and extreme/biased numerical forecasts while pretending to be reasonable; used to test effects of a poorly-calibrated assistant on human forecasters when allowed back-and-forth interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-Turbo (gpt-4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same GPT-4-Turbo deployment as the superforecasting treatment (128k token context window, output limits, temperature 0.8), but with a system prompt instructing the model to produce biased, overconfident forecasts and to avoid reference-class/base-rate reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>The same six continuous real-world forecasting targets used in the study, including the scientific output question: count of AI papers on ArXiv in December 2023, bitcoin hash rate (TH/s), Dow Jones Transportation Average close, global commercial flights active on Dec 31, 2023, and USD/RUB closing rate on Dec 30, 2023.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Prompt-engineered adversarial/biasing system prompt that instructs the model to form extreme personal positions, ignore base rates and uncertainty, and produce numerical predictions and supporting (selective) rationales. Humans could interact and receive these biased forecasts as part of back-and-forth augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_protocol</strong></td>
                            <td>Same as superforecasting: absolute error vs actual, winsorisation and standardisation, one-way ANOVA and Tukey HSD, OLS regressions with interaction terms for skill, mixed-effects models for difficulty interactions, bootstrap for median-of-medians aggregate analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Same six-question forecasting set (Forecasting Proficiency Test-derived) with outcomes in late Dec 2023; short binary and intersubjective questions for skill calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>The noisy LLM augmentation also significantly improved individual forecasting accuracy relative to the control in preregistered analyses (mean difference vs control = -0.25, p<.001). However, direct LLM-only deviations (Table 3) show much larger errors for the noisy prompt compared to the superforecasting prompt (e.g., Question 2 +470.84% deviation for noisy vs +19.88% for superforecasting). In aggregate median-of-medians bootstrap analyses, noisy augmentation unexpectedly had the lowest mean-of-medians score in preregistered analysis (0.41) but this pattern is sensitive to the bitcoin-hash-rate outlier (Q3); excluding Q3 reverses some effects.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Despite being intentionally biased, the noisy assistant improved participant accuracy versus the non-forecasting control, suggesting human use of the model's outputs and induced deliberation can help even when the model is noisy; results were strongly sensitive to an outlier question (Q3). The study does not test the noisy model in isolation vs unaided humans extensively, and the noisy behaviour was artificial (prompt-designed) rather than representing naturally-occurring failure modes in deployed systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5704.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5704.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Control LLM (non-forecasting)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DaVinci-003 configured as a non-forecasting assistant (control)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A substantially smaller/older OpenAI model (DaVinci-003) used as the control condition; instructed not to provide numerical forecasts and to behave as a basic helpful assistant, to control for interface engagement without giving forecasting advice.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DaVinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A prior-generation OpenAI transformer LLM (GPT-3 era variant) used with a prompt explicitly forbidding it from providing forecasts ('You do not provide forecasts at all') so as to approximate a low-powered baseline that encourages participant engagement but not forecasting augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Participants in this condition provided the same six forecasting targets themselves (no LLM numerical forecasts were provided by the assistant). The underlying targets are the study's six real-world events including AI paper counts on arXiv (Dec 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Not applicable for the control model: intentionally instructed not to produce numerical forecasts; used to isolate effect of providing numerical / forecasting-capable LLM assistance in the treatment arms.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_protocol</strong></td>
                            <td>Same evaluation pipeline (absolute error, winsorisation, standardisation, ANOVA/regression/mixed models and bootstrap aggregation) applied to participant forecasts in the control arm for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Same six-question set from the Forecasting Proficiency Test and the skill-estimation set for computing skill rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Control participants had significantly lower forecasting accuracy (higher mean absolute error) than both GPT-4-Turbo treatments in preregistered analyses (Tukey HSD mean differences: -0.21 and -0.25 for superforecasting and noisy respectively, both p<.001). The control condition median-of-medians bootstrap score was 0.55 (95% CI [0.52,0.58]) in preregistered aggregate analysis, worse than the treatment groups.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Because the control is a non-forecasting LLM rather than a human-only no-LLM condition, the study cannot directly determine whether using an LLM at all (vs pure human forecasting without any LLM interface) helps or harms; the control does not rule out that a non-assisted human baseline might perform better or worse than the control used here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5704.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5704.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prior GPT-4 single-model forecasting result (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 performance in a real-world forecasting tournament (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work found GPT-4 (March 2023) markedly underperformed median human crowd forecasts in a real-world forecasting tournament, failing to significantly beat uniform random guessing; cited as motivation for studying human-LLM hybrid forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Language Model Prediction Capabilities</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 (March 2023 release) — a frontier LLM at the time of that study; referenced as having underperformed humans when producing standalone machine forecasts in a forecasting tournament (Schoenegger and Park 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Real-world forecasting tournament questions (geopolitical/economic forecasting targets used in the referenced tournament), not limited to scientific discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Standalone model forecasts (no human-in-the-loop hybridization); methods in the prior study are referenced but not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_protocol</strong></td>
                            <td>Comparison of model forecasts to median human crowd forecasts and to a no-information (uniform/random) baseline; evaluation metrics referenced include tournament scoring though details are in the cited prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>A real-world forecasting tournament dataset (as used in Schoenegger & Park 2023), referenced to motivate hybrid human+LLM experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Reported in prior work: GPT-4 significantly underperformed the median human crowd in that tournament and did not significantly outperform uniform random guessing; this motivated investigating whether human+LLM hybrids could perform better than LLM-alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>This paper cites the prior result as evidence that standalone LLM forecasts can be poor; it does not re-evaluate that study's methods or controls here but uses it as motivation for hybrid augmentation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Approaching Human-Level Forecasting with Language Models <em>(Rating: 2)</em></li>
                <li>Large Language Model Prediction Capabilities <em>(Rating: 2)</em></li>
                <li>Wisdom of the silicon crowd: Llm ensemble prediction capabilities match human crowd accuracy <em>(Rating: 2)</em></li>
                <li>Hybrid Forecasting of Geopolitical Events <em>(Rating: 2)</em></li>
                <li>Chimeric Forecasting: Combining Probabilistic Predictions from Computational Models and Human Judgment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5704",
    "paper_id": "paper-267627630",
    "extraction_schema_id": "extraction-schema-117",
    "extracted_data": [
        {
            "name_short": "Superforecasting LLM (treatment)",
            "name_full": "GPT-4-Turbo with a 'superforecasting' system prompt (superforecasting assistant)",
            "brief_description": "A GPT-4-Turbo chat assistant instrumented via expert prompt engineering to act like a 'superforecaster' (breaking problems into subproblems, using reference classes/base rates, providing uncertainty intervals and explicit rationales) and to provide numerical forecasts and reasoning in a back-and-forth conversational setting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4-Turbo (gpt-4-1106-preview)",
            "model_description": "A transformer-based large language model from OpenAI (GPT-4 family). In this study it was deployed with a detailed system prompt instructing superforecasting behaviours (10 commandments), run via API with a 128k token input context window, 4096 token output limit, set to temperature 0.8 and a maximum output limit of 1024 tokens; the model had no internet access and was not provided external retrieval.",
            "model_size": null,
            "prediction_target": "A set of six real-world continuous forecasting targets (finance, geopolitics, cryptocurrency, aviation, scientific output, and FX). Example scientific-related target: 'How many AI papers will be published on ArXiv during December 1–31, 2023?' (count of AI papers); other targets included Bitcoin network hash rate (TH/s) and counts/continuous real-world outcomes.",
            "prediction_method": "Expert prompt engineering: a detailed system prompt instructing the model to follow superforecasting principles (triage, reference-class/base-rate reasoning, uncertainty intervals, explicit pros/cons and numerical estimates). Human participants could interact in an iterative chat (back-and-forth) and request either direct LLM forecasts or reasoning and feedback on their own forecasts.",
            "evaluation_protocol": "Forecast accuracy measured as absolute error |forecast − actual|, winsorised (initial 5% trimming then 3σ) and standardized by control-group SD per question; aggregated across six questions. Statistical tests included one-way ANOVA (F-tests), Tukey HSD pairwise comparisons, preregistered OLS regressions and mixed-effects models, Brier scores and Euclidean distance for skill measures, and bootstrap (10,000 resamples) for median-aggregation analyses.",
            "dataset_or_benchmark": "A bespoke set of six time-resolved forecasting questions drawn from an early Forecasting Proficiency Test (Himmelstein et al. 2024); data collection on Nov 21, 2023 with outcomes resolved in late Dec 2023. A separate set of short binary and intersubjective questions was used to compute forecaster skill (for Brier and intersubjective measures).",
            "results": "Interacting with the superforecasting LLM augmentation significantly improved individual forecasting accuracy relative to the control (one-way ANOVA F(2,988)=34.58, p&lt;.001). Across preregistered analyses the treatments improved accuracy by ≈24%-28% vs. control; pairwise mean error differences: treatment vs control = -0.21 (p&lt;.001). Direct LLM-only forecasts (Table 3) showed the superforecasting assistant had consistently smaller deviations from truth than the noisy assistant across the 6 questions. In exploratory analyses excluding an outlier question (Q3), the superforecasting condition showed a mean error of 0.40 vs 0.67 for control (mean difference -0.27, p&lt;.001) and outperformed the noisy variant (0.47).",
            "limitations_or_challenges": "Results hinge partly on an outlier (Question 3 — bitcoin hash rate) whose exclusion changes comparative effect sizes; participants were online Prolific users limiting generalisability; the LLM had no external retrieval (no RAG), and the study compares to a weak non-forecasting LLM control rather than a human-only control, so it cannot rule out that any LLM harms vs. unaided humans; limited message budget (25) and model hyperparameters (temperature 0.8) may affect behaviours; the approach tests forecasting targets (including publication counts) rather than predicting novel scientific 'discoveries' per se.",
            "uuid": "e5704.0",
            "source_info": {
                "paper_title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Noisy LLM (treatment noise)",
            "name_full": "GPT-4-Turbo with a 'noisy' (biased) forecasting prompt (noisy assistant)",
            "brief_description": "The same GPT-4-Turbo model configured with a system prompt engineered to induce base-rate neglect, overconfidence, and extreme/biased numerical forecasts while pretending to be reasonable; used to test effects of a poorly-calibrated assistant on human forecasters when allowed back-and-forth interaction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4-Turbo (gpt-4-1106-preview)",
            "model_description": "Same GPT-4-Turbo deployment as the superforecasting treatment (128k token context window, output limits, temperature 0.8), but with a system prompt instructing the model to produce biased, overconfident forecasts and to avoid reference-class/base-rate reasoning.",
            "model_size": null,
            "prediction_target": "The same six continuous real-world forecasting targets used in the study, including the scientific output question: count of AI papers on ArXiv in December 2023, bitcoin hash rate (TH/s), Dow Jones Transportation Average close, global commercial flights active on Dec 31, 2023, and USD/RUB closing rate on Dec 30, 2023.",
            "prediction_method": "Prompt-engineered adversarial/biasing system prompt that instructs the model to form extreme personal positions, ignore base rates and uncertainty, and produce numerical predictions and supporting (selective) rationales. Humans could interact and receive these biased forecasts as part of back-and-forth augmentation.",
            "evaluation_protocol": "Same as superforecasting: absolute error vs actual, winsorisation and standardisation, one-way ANOVA and Tukey HSD, OLS regressions with interaction terms for skill, mixed-effects models for difficulty interactions, bootstrap for median-of-medians aggregate analysis.",
            "dataset_or_benchmark": "Same six-question forecasting set (Forecasting Proficiency Test-derived) with outcomes in late Dec 2023; short binary and intersubjective questions for skill calibration.",
            "results": "The noisy LLM augmentation also significantly improved individual forecasting accuracy relative to the control in preregistered analyses (mean difference vs control = -0.25, p&lt;.001). However, direct LLM-only deviations (Table 3) show much larger errors for the noisy prompt compared to the superforecasting prompt (e.g., Question 2 +470.84% deviation for noisy vs +19.88% for superforecasting). In aggregate median-of-medians bootstrap analyses, noisy augmentation unexpectedly had the lowest mean-of-medians score in preregistered analysis (0.41) but this pattern is sensitive to the bitcoin-hash-rate outlier (Q3); excluding Q3 reverses some effects.",
            "limitations_or_challenges": "Despite being intentionally biased, the noisy assistant improved participant accuracy versus the non-forecasting control, suggesting human use of the model's outputs and induced deliberation can help even when the model is noisy; results were strongly sensitive to an outlier question (Q3). The study does not test the noisy model in isolation vs unaided humans extensively, and the noisy behaviour was artificial (prompt-designed) rather than representing naturally-occurring failure modes in deployed systems.",
            "uuid": "e5704.1",
            "source_info": {
                "paper_title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Control LLM (non-forecasting)",
            "name_full": "DaVinci-003 configured as a non-forecasting assistant (control)",
            "brief_description": "A substantially smaller/older OpenAI model (DaVinci-003) used as the control condition; instructed not to provide numerical forecasts and to behave as a basic helpful assistant, to control for interface engagement without giving forecasting advice.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DaVinci-003",
            "model_description": "A prior-generation OpenAI transformer LLM (GPT-3 era variant) used with a prompt explicitly forbidding it from providing forecasts ('You do not provide forecasts at all') so as to approximate a low-powered baseline that encourages participant engagement but not forecasting augmentation.",
            "model_size": null,
            "prediction_target": "Participants in this condition provided the same six forecasting targets themselves (no LLM numerical forecasts were provided by the assistant). The underlying targets are the study's six real-world events including AI paper counts on arXiv (Dec 2023).",
            "prediction_method": "Not applicable for the control model: intentionally instructed not to produce numerical forecasts; used to isolate effect of providing numerical / forecasting-capable LLM assistance in the treatment arms.",
            "evaluation_protocol": "Same evaluation pipeline (absolute error, winsorisation, standardisation, ANOVA/regression/mixed models and bootstrap aggregation) applied to participant forecasts in the control arm for comparison.",
            "dataset_or_benchmark": "Same six-question set from the Forecasting Proficiency Test and the skill-estimation set for computing skill rankings.",
            "results": "Control participants had significantly lower forecasting accuracy (higher mean absolute error) than both GPT-4-Turbo treatments in preregistered analyses (Tukey HSD mean differences: -0.21 and -0.25 for superforecasting and noisy respectively, both p&lt;.001). The control condition median-of-medians bootstrap score was 0.55 (95% CI [0.52,0.58]) in preregistered aggregate analysis, worse than the treatment groups.",
            "limitations_or_challenges": "Because the control is a non-forecasting LLM rather than a human-only no-LLM condition, the study cannot directly determine whether using an LLM at all (vs pure human forecasting without any LLM interface) helps or harms; the control does not rule out that a non-assisted human baseline might perform better or worse than the control used here.",
            "uuid": "e5704.2",
            "source_info": {
                "paper_title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Prior GPT-4 single-model forecasting result (mention)",
            "name_full": "GPT-4 performance in a real-world forecasting tournament (prior work)",
            "brief_description": "Referenced prior work found GPT-4 (March 2023) markedly underperformed median human crowd forecasts in a real-world forecasting tournament, failing to significantly beat uniform random guessing; cited as motivation for studying human-LLM hybrid forecasting.",
            "citation_title": "Large Language Model Prediction Capabilities",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (prior work)",
            "model_description": "GPT-4 (March 2023 release) — a frontier LLM at the time of that study; referenced as having underperformed humans when producing standalone machine forecasts in a forecasting tournament (Schoenegger and Park 2023).",
            "model_size": null,
            "prediction_target": "Real-world forecasting tournament questions (geopolitical/economic forecasting targets used in the referenced tournament), not limited to scientific discoveries.",
            "prediction_method": "Standalone model forecasts (no human-in-the-loop hybridization); methods in the prior study are referenced but not detailed in this paper.",
            "evaluation_protocol": "Comparison of model forecasts to median human crowd forecasts and to a no-information (uniform/random) baseline; evaluation metrics referenced include tournament scoring though details are in the cited prior work.",
            "dataset_or_benchmark": "A real-world forecasting tournament dataset (as used in Schoenegger & Park 2023), referenced to motivate hybrid human+LLM experiments.",
            "results": "Reported in prior work: GPT-4 significantly underperformed the median human crowd in that tournament and did not significantly outperform uniform random guessing; this motivated investigating whether human+LLM hybrids could perform better than LLM-alone.",
            "limitations_or_challenges": "This paper cites the prior result as evidence that standalone LLM forecasts can be poor; it does not re-evaluate that study's methods or controls here but uses it as motivation for hybrid augmentation experiments.",
            "uuid": "e5704.3",
            "source_info": {
                "paper_title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Approaching Human-Level Forecasting with Language Models",
            "rating": 2,
            "sanitized_title": "approaching_humanlevel_forecasting_with_language_models"
        },
        {
            "paper_title": "Large Language Model Prediction Capabilities",
            "rating": 2,
            "sanitized_title": "large_language_model_prediction_capabilities"
        },
        {
            "paper_title": "Wisdom of the silicon crowd: Llm ensemble prediction capabilities match human crowd accuracy",
            "rating": 2,
            "sanitized_title": "wisdom_of_the_silicon_crowd_llm_ensemble_prediction_capabilities_match_human_crowd_accuracy"
        },
        {
            "paper_title": "Hybrid Forecasting of Geopolitical Events",
            "rating": 2,
            "sanitized_title": "hybrid_forecasting_of_geopolitical_events"
        },
        {
            "paper_title": "Chimeric Forecasting: Combining Probabilistic Predictions from Computational Models and Human Judgment",
            "rating": 1,
            "sanitized_title": "chimeric_forecasting_combining_probabilistic_predictions_from_computational_models_and_human_judgment"
        }
    ],
    "cost": 0.01487375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy
22 Aug 2024</p>
<p>Philipp Schoenegger 
Peter S Park 
Ezra Karger 
Sean Trott 
Philip E Tetlock </p>
<p>London School of Economics and Political Science</p>
<p>Massachusetts Institute of Technology</p>
<p>University of California San Diego</p>
<p>University of Pennsylvania</p>
<p>AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy
22 Aug 2024CA752181E698107C5257B79FF1B27FBBarXiv:2402.07862v2[cs.CY]
Large language models (LLMs) match and sometimes exceeding human performance in many domains.This study explores the potential of LLMs to augment human judgement in a forecasting task.We evaluate the effect on human forecasters of two LLM assistants: one designed to provide high-quality ('superforecasting') advice, and the other designed to be overconfident and base-rate neglecting, thus providing noisy forecasting advice.We compare participants using these assistants to a control group that received a less advanced model that did not provide numerical predictions or engaged in explicit discussion of predictions.Participants (N = 991) answered a set of six forecasting questions and had the option to consult their assigned LLM assistant throughout.Our preregistered analyses show that interacting with each of our frontier LLM assistants significantly enhances prediction accuracy by between 24% and 28% compared to the control group.Exploratory analyses showed a pronounced outlier effect in one forecasting item, without which we find that the superforecasting assistant increased accuracy by 41%, compared with 29% for the noisy assistant.We further examine whether LLM forecasting augmentation disproportionately benefits less skilled forecasters, degrades the wisdom-of-the-crowd by reducing prediction diversity, or varies in effectiveness with question difficulty.Our data do not consistently support these hypotheses.Our results suggest that access to a frontier LLM assistant, even a noisy one, can be a helpful decision aid in cognitively demanding tasks compared to a less powerful model that does not provide specific forecasting advice.However, the effects of outliers suggest that further research into the robustness of this pattern is needed.*Any views expressed in this paper do not necessarily reflect those of the Federal Reserve Bank of Chicago or the Federal Reserve System.</p>
<p>Introduction</p>
<p>Recent advances in artificial intelligence (AI), and large language models (LLMs) specifically, demonstrate impressive AI capabilities across a large number of complex and economically valuable tasks (Naveed et al. 2023).This development challenges previously held beliefs about the necessity of human cognition for many of these tasks (Bubeck et al. 2023), and raises the possibility of significant negative effects of AI systems on the (human) labor market in large parts of the knowledge economy (George and Baskar 2023).Understanding the current ability of LLMs to interface with economically central tasks requires a broad empirical study across domains.However, most knowledge-work jobs require substantial reasoning capabilities that use data and patterns of observations beyond any model's training data.This makes finding a suitable study context central in any attempt to understand how LLMs might impact advanced economies in the near future.</p>
<p>Our focus in this paper is on Large Language Models, which represent a significant advance in AI and natural language processing.These models build upon the transformer architectural paradigm (Vaswani et al. 2017) and are characterized by their massive scale, often containing billions or even trillions of parameters, trained on an enormous amount of diverse textual data (Shen et al. 2023).The core capability of LLMs is next-token prediction: the ability to predict the most probable next word or subword (token) given a sequence of preceding tokens.However, this seemingly simple objective, when scaled, results in a wide array of emergent abilities that seem to extend far beyond basic next-token prediction.These advanced AI systems demonstrate proficiency in tasks such as natural language understanding and generation, few-shot learning, and complex reasoning across various domains.Importantly, many of these specialized advanced skills emerge in ways that could not have been fully predicted before training, due to non-linearities in how capabilities scale with model size and data (Wei et al. 2022).</p>
<p>Some areas where LLMs have shown strong performance are marketing (Fraiwan and Khasawneh 2023), translation (Jiao et al. 2023), high levels of reading comprehension (Winter 2023), teaching (Fraiwan and Khasawneh 2023;Sallam et al. 2023), summarization (Goyal, Li, and Durrett 2023), abstract categorization of objects (Atari et al. 2023), programming (Bubeck et al. 2023;Cheng et al. 2024), spear phishing cyber attacks (Hazell 2023;Heiding et al. 2023), human personality (Schoenegger et al. 2024a), robotics (Vemprala et al. 2023), medical reasoning (Bubeck et al. 2023;Nori et al. 2023;Sallam et al. 2023), legal reasoning (Bubeck et al. 2023;Katz et al. 2023), deception (Park et al. 2023), and others.LLMs' many capabilities substantially increase the amount of money and talent going into LLM research and products (Sutton 2023), suggesting further growth in capabilities in the near future.</p>
<p>Crucially, modern state-of-the-art or frontier language models are not inherently autonomous for most relevant tasks (Xi et al. 2023).While they can be imbued with general autonomy through agent frameworks like AutoGPT (Firat and Kuleli 2023) or other scaffolding approaches, the reliability of such methods remains questionable.Future iterations of models may enable autonomous behavior directly (Kinniment et al. 2023), potentially making agency-the ability to take actions and achieve goals independently-more accessible.However, at present, LLMs are not economically viable as autonomous agents due to significant limitations including inefficiency, forgetting, speed, cost, cultural complexity (McIntosh et al. 2024) and hallucinations (Firat and Kuleli 2023).</p>
<p>Instead, these models are primarily used in combination with human labor, forming a hybrid technology that necessitates human input at various stages (Dell'Acqua et al. 2023).This synergistic approach allows humans to leverage the strengths of LLMs, producing outcomes that can surpass what either humans or machines could achieve independently.For instance, LLM augmentations have demonstrably enhanced the performance of human graders (Xiao et al. 2024) and programmers (Peng et al. 2023), and have also been applied in the context of co-creating visual stories (Antony and Huang 2023), illustrating the potential of human-AI collaboration in diverse fields.</p>
<p>Our study contributes to the growing research on human-AI collaboration in complex decision-making tasks, a key focus in HCI and AI research (Steyvers and Kumar 2023).By examining LLM-augmented forecasting, we extend recent work on human-AI interaction modes (Gao et al. 2024) and address challenges in AI-assisted decision-making (Steyvers and Kumar 2023).Our approach aligns with calls to develop nuanced understandings of human-AI complementarity (Yang 2024) and explores how different LLM prompts affect forecasting outcomes, contributing to discussions on designing AI systems that effectively augment human cognition (Wang et al. 2024a).</p>
<p>In this paper, we study the application of present-era frontier LLMs as a hybrid augmentation technology in the context of forecasting future events.This allows us to test their ability to augment human decision-making in a domain robust to in-sample overfitting of training data, since no one, including LLMs or the experimenters themselves, can know the answer to prospective forecasting questions at the time of data collection.This context is also practically relevant as accurate forecasting is essential to many aspects of economic activity, especially within white-collar occupational domains such as law, business, and policy: fields that may be disrupted by LLM capabilities (Acemoglu 2023;Park and Tegmark 2023;Summers and Rattner 2023).If the use of present or future AI systems increases the forecasting accuracy of humans and organizations, the efficiency and productivity gains to the relevant industries' individuals and businesses are clear, and if there are risks, they ought to be discussed prior to widespread adoption.</p>
<p>Our specific object of interest in this study is human judgment forecasting, where humans provide forecasts of future events, such as the probability that inflation will hit a certain milestone over the next twelve months or the anticipated number of barrels in the Strategic Petroleum Reserve at the end of the year.This context is distinct from the more widely studied topic of time series forecasting (Jin et al. 2023), as the central input are judgements by human forecasters as opposed to machine learning algorithms.The science of judgemental forecasting has found that aggregated forecasts of a crowd of forecasters can be surprisingly accurate (Tetlock and Gardner 2016), can impact policy debates (Tetlock, Mellers, and Scoblic 2017), and can affect businesses (Schoemaker and Tetlock 2016), and that much of this effect is derived from the high accuracy of a subset of forecasters, often called 'superforecasters'.Previous work on the topic focuses on a variety of other topics, ranging from the identification of these highly skilled forecasters (Himmelstein, Budescu, and Han 2023;Mellers et al. 2015b;Tetlock and Gardner 2016) and novel aggregation methods (Atanasov et al. 2017) to improvements of forecasting accuracy (Chang et al. 2016;Karger, Atanasov, and Tetlock 2022) as well as applications to topics like development economics (Bernard and Schoenegger 2024) or pandemics (McAndrew et al. 2024).</p>
<p>Related to our project, some previous work focuses on human-machine hybrid forecasting in the context of IARPA's 'Hybrid Forecasting Competition.' Benjamin et al. (2023) report the results of 'SAGE,' a hybrid forecasting system designed to combine human-and machine-generated forecasts (such as autoregressive integrated moving average (ARIMA) forecast outputs).They find that their hybrid forecasting system outperformed their human-only baseline, suggesting that cost savings and accuracy increases of these hybrid systems may be "a viable approach for maintaining a competitive level of accuracy" (Benjamin et al. 2023, p. 113).Similarly, Atanasov et al. (2017) introduce a 'Human Forest' method that enables human forecasters to define custom reference classes, draw on historical databases, and review base rates in their forecasting.They find that these forecasters outperform statistical model predictions.However, both approaches used pre-LLM methods as their machine counterparts.Unlike these systems, LLM-based assistants allow for new systems where humans and models communicate interactively in dynamic settings.</p>
<p>In this paper, we extend this literature on human-AI interactions in light of recent breakthroughs in LLMs.The central advancement for this context is the possibility of a free-flowing exchange between the human and the model via a chat function, in which the human can query the model, receive a response that is often indistinguishable from a human response (Jones and Bergen 2024), and then continue the conversation, with the previous iteration being part of the model's memory.This back-and-forth on advanced topics necessitating strong model reasoning capabilities is something that previous technologies were not capable of, and is a potential way for humans to learn skills in their interaction with AI systems (Yang 2024).Those interacting with the model can query it to fill their own gaps in knowledge or perceived weaknesses, they can ask it to produce a full forecast for them and provide the reasoning underlying it, they can input their own reasoning and predictions into the model for feedback, or they can do a combination of these and other approaches they might find helpful (Wang et al. 2024a).This is similar to work by Guo et al. (2024) that provides a natural language interface for questions of tabular data.While the technology still has substantial limitations, the fact that forecasters can engage with it in an interactive and personalised way opens up a novel type of human-machine interaction.Our goal in this paper is to probe whether LLM forecasting augmentations with advanced prompts can be a cheap, scalable, and effective method of improving human judgement forecasting.Inference costs for LLMs remain low and continue to drop, sitting currently at less than a cent per 1000 tokens, making LLM forecasting augmentation a prime candidate for a generalized hybrid system that can boost individual performance in many valuable tasks at costs far below a human assistant equivalent.</p>
<p>Current best-practice measures of LLM proficiency often rely on task benchmarks, where models are evaluated against a set of predefined tasks.We argue that evaluating forecast accuracy in real-world scenarios like actual forecasting presents a more comprehensive assessment of reasoning capabilities and reduces risks of overstating model capabilities due to training data memorization.This also increases the likelihood that these results generalize to different-and perhaps out-of-distribution-settings (Arora and Goyal 2023).As such, our approach diverges from conventional task benchmarks, focusing on the LLM's ability to apply its knowledge and understanding to novel settings, rather than settings that may be represented in some shape or form in its training data or output that may have been training on to perform well on benchmarks.Even if an LLM excels at a given task benchmark, it is unclear whether this reveals a deep understanding of the process behind the task, instead of rote memorization of the task benchmark's answers in the training data (Bender et al. 2021;Biderman et al. 2023;Carlini et al. 2023;Magar and Schwartz 2022).The difficulty in disentangling true understanding from training data memorization is non-trivial.Deep understanding, after all, also originates from exposure to relevant content within the training dataset.However, the success or failure to generalize outside of the training data appears central to this disentangling (Grove and Bretz 2012).In our study, we analyze human forecasting behavior on a set of prediction questions that resolve in the future such that no human forecaster or AI-based system can access the answer at the time of data collection, avoiding these concerns.</p>
<p>Past work found that the at-the-time frontier model GPT-4, released by OpenAI in March 2023, significantly underperformed the median human-crowd forecast in a real-world forecasting tournament, failing to even significantly outperform the no-information forecasting strategy of uniform random guessing (Schoenegger and Park 2023).However, more recent work has found that aggregating a set of diverse LLM forecasts (Schoenegger et al. 2024b) or retrieval-augmented (RAG) systems that enable the model to access additional information (Halawi et al. 2024) can approach human-level performance.Moreover, this previous work only investigated the effect of machine forecasts produced directly by the model, without incorporating human input that allows a continuous back-and-forth between forecasters and the machine.It is reasonable to expect that human-LLM hybrid forecasts-the object of study in the present paper-might outperform the results of the LLM operating by itself if it was set up properly.While hybrid forecasting approaches have been previously studied-for example, in making predictions on geopolitical questions (Benjamin et al. 2023) and in radiology (Agarwal et al. 2023)-our approach is arguably more meaningfully hybrid, in that our human forecasters can engage in a back-and-forth dialogue with a specifically instructed forecasting LLM to fill gaps in knowledge, understanding, and data that differ on a person-by-person level.This back-and-forth LLM augmentation may allow forecasters to use the model for the parts of forecasting that they struggle most with: be it synthesizing data, making coherent forecasts, or attaching numbers to intuitions, thus increasing the potential effect of this augmentation.Importantly, LLMs specifically prepared for this task via system prompts may be especially beneficial.This motivates our first research question and accompanying hypothesis, testing whether we find an aggregate accuracy improvement of specially prompted frontier LLM augmentations compared to a control condition that has access to a lower-powered LLM that does not provide forecasting assistance.</p>
<p>We test two treatments, one where the human has access to an LLM with a 'superforecasting' (Tetlock and Gardner 2016) prompt that draws on well-studied principles of good forecasting practice.'Superforecasting' is a term that describes a set of features that exceptionally accurate human forecasters have shown to possess, which, at least in part, contribute to their superior prediction capabilities in human forecasting tournaments.In this context, the model is asked to provide assistance that breaks down complex problems into smaller ones, distinguishes degrees of doubt, and aims to identify errors in its own reasoning.We also set up a second advanced LLM with a specific prompt, aimed at produced inaccurate forecasts.We specifically instructed the model to exhibit the biases of base rate neglect and overconfidence, thus resulting in a noisy forecasting assistant.Both models are instructed to assist forecasters in whatever way is requested, ranging from providing point estimates to offering feedback on forecasts.We compare both treatments and the control condition to each other, allowing for a potential ordering of effects.This allows us to test whether a back-and-forth with an advanced LLM that provides direct and actionable forecasting advice outperforms a much weaker LLM baseline that does not provide forecasting advice.We predicted that the superforecasting LLM augmentation would outperform the noisy LLM augmentation, and that both hybrid treatment arms would have higher aggregate accuracy than the control.</p>
<p>Null Hypothesis 1: There is no difference in forecasting accuracy between the superforecasting (noisy) LLM augmentation and the control.</p>
<p>Recent work in other areas has also shown that less skilled individuals benefit the most from LLM augmentation.For example, LLM augmentation boosted the performance of low-performing professionals more than that of high-performing professionals in studies where it was provided to management consultants (Dell'Acqua et al. 2023), customer-support agents (Brynjolfsson, Li, and Raymond 2023), creative writers (Doshi and Hauser 2023), office workers who write memos (Noy and Zhang 2023), law school students who write exams (Choi and Schwarcz 2024), and programmers (Peng et al. 2023).The underlying reason differs by context, but the general suggestion is that low-performing individuals can increase their performance by substituting LLM output for human output, which is more likely to improve results if one's own output is not as high-quality.However, other work in the context of medicine found that human-AI hybrid decisions are not associated with increased diagnostic quality, suggesting that the effects of AI may show substantial heterogeneity across subject domains and implementation details (Agarwal et al. 2023).One potential explanation for such an effect may be that low-performing individuals might be comparatively less able to spot LLM weaknesses and failure modes, whereas those more familiar with the task could selectively use the LLM augmentation to greater effect.This heterogeneity of results suggests that any effects of LLM augmentation on forecasting are likely to be distinct across the skill distribution, with lower-skill forecasters potentially relying to a greater degree on LLM augmentation, which may help alleviate biases in their predictions that would otherwise have led them to make badly calibrated judgments.This motivates our second hypothesis, which directly tests whether the LLM augmentation has disparate impacts on forecasters of different skill levels.In line with much of the previous literature, we predicted a greater positive effect on lower-skill forecasters.</p>
<p>Null Hypothesis 2: The effect of the superforecasting (noisy) LLM augmentation on forecasting accuracy does not differ between high-and low-skilled forecasters.</p>
<p>In addition to investigating the effects of LLM augmentation on individual forecasts and on forecasters of different levels of skill, we also collect data allowing us to look at its potentially adverse effects on aggregate forecasts.Due to the 'wisdom of the crowd' effect, aggregation-such as taking the median forecast-tends to result in an aggregated forecast that is more accurate than the majority of forecasts given by most individuals, even across heterogeneous types of forecasters who may have different skill levels (Budescu and Chen 2015;Mannes, Soll, and Larrick 2014).However, this aggregation tends to be most effective when there is a diversity of independent forecasts, not if the forecasts share a common source of variation and are thus intercorrelated.If the LLM augmentations anchor many human forecasters on the same or very similar point forecast for a given question, it could reduce the value of aggregation as the independence of forecasts is reduced, inducing a potential group think effect.If this is the case, this would provide a substantial source of concern for applications of LLM augmentations in practice.To look at this, we test whether LLM augmentation homogenizes forecasts in this way, motivating our third hypothesis, where we predicted a reduction in group-level accuracy.</p>
<p>Null Hypothesis 3: There is no difference in aggregate level forecasting accuracy between the superforecasting (noisy) LLM augmentation and the control.</p>
<p>Finally, we compare the effect LLM forecasting augmentation has on prediction performance on questions of different difficulty levels.There are a number of reasons why the difficulty of the forecasting question may be an important factor.If questions are especially difficult, forecasters may be more likely to simply defer to any machine prediction directly, without further investigation and critique.If machines are then individually worse or better than humans, this might play out in a difficulty effect.Conversely, very easy questions may be such that forecasters do not bother asking the LLM for input and instead rely on their own forecasts in which they might have relatively high confidence.There could also be a more complicated interplay of question difficulty with other factors that may lead to an ameliorating effect of performance increasing and performance reducing aspects.This set of questions motivates our last hypothesis, where we did not have a specific directional prediction.</p>
<p>Null Hypothesis 4: There is no difference in the effect of the superforecasting (noisy) LLM augmentation on forecasting accuracy between hard and easy questions.</p>
<p>Methods</p>
<p>All analyses were preregistered on the Open Science Framework1 .We clearly label all exploratory/nonpreregistered analyses as such throughout the paper to indicate which analyses we decided to conduct after having seen the data or having done other analyses.This study received ethics approval prior to data collection. 2e recruited a total of 1,152 participants from Prolific, an online research platform that gives researchers access to people willing to participate in research in exchange for a participation fee.For participating in our study, participants were paid $5 for participation and could earn an additional $100 based on their accuracy.We paid three such accuracy prizes to randomly selected participants who scored in the top-10 of forecasters.We used this level of randomization to account for incentive concerns of paying out prizes only to the top performers might then be likely to extremize their predictions (Witkowski et al. 2023) by choosing values significantly above or below their true beliefs, thus distorting the incentive compatibility of the forecast elicitation.We preregistered the following a priori power analysis to determine the sample size of our study: Using Cohen's d=0.20 as our smallest effect size of interest as a conventionally small effect, with an allocation ratio of 1.5/1/1 between the main treatment, the secondary noisy treatment, and the control, aiming for 80% power, we needed to recruit 492 participants for the Main treatment and 328 for the other two conditions, resulting in a final participant count of 1148.We recruited a total of 1,152 participants, meeting our goal.</p>
<p>We collected participant forecasts on a set of six forecasting questions that ranged from questions on finance, geopolitics, and cryptocurrency to ones on aviation, artificial intelligence, and foreign exchange.All six questions had continuous prediction variables, ranging from asset prices to numbers of refugees, where participants could input any number without restrictions.We chose a diverse set of questions to account for variation in question difficulty and familiarity, while ensuring that our outcome variable contributes rather than distracts from the generalizability of results.We also ensured that all questions were resolvable quickly after the cutoff date to allow for timely payouts of accuracy incentives for participants.The question set was drawn from an early question set used in the Forecasting Proficiency Test (Himmelstein et al. 2024).For a full list, see Table 1.Data collection happened on November 21, 2023, over five weeks prior to forecast question resolution.</p>
<p>Our main outcome variable is forecasting accuracy.Our accuracy measure is the error between participant forecasts and the true value of the forecasted question.We computed the error for each forecasting question i as the absolute difference Di between the participant's forecast Fi and the actual value Ai, expressed as Di = |Fi − Ai|.To ensure participant comprehension, participants read a detailed explanation of this measure of accuracy, as well as an example, and then completed a one-question quiz on it, without which they were not able to continue in the experiment.Throughout the paper, unless specifically specified otherwise, when we refer to 'accuracy', we mean the error rating arrived by using absolute differences between the forecast and the truth value.As such, in all our analyses, lower values indicate higher accuracy, and higher values indicate lower accuracy.</p>
<p>To account for outliers in our data, which we expected due to the free entry data collection of forecasting problems that permit substantial uncertainty, we conducted an initial winsorisation process of accuracy values at the 5% levels by removing all values at the bottom 5% and top 5%. 3Then, we standardized the values across questions by dividing them by the standard deviation of the control group for the respective question, allowing for inter-question comparability in accuracy scores.Lastly, we conducted a second winsorisation step, this time at the level of 3 standard deviations.Question 5: How many AI papers will be published on ArXiv during the month between December 1, 2023 and December 31, 2023?Question 6: What will be the closing value for the U.S. Dollar against the Russian Ruble (converting 1 USD to RUB) on December 30, 2023?</p>
<p>Our secondary variables of question difficulty and forecaster skill were collected as follows: A randomly selected 10% of control group participants were tasked not only with providing forecasts for each question but also with rating their perceived difficulty for each question on a 5-point Likert scale ranging from 'Very easy' to 'Very difficult'.Questions 2 and 3 received the highest difficulty ratings and were therefore identified as being the most challenging in our analyses, to be compared with the other four questions.</p>
<p>Prior to the main forecasting tasks, participants were also asked a series of smaller, lower-effort forecasting questions.These questions included binary predictions (providing the probability that an event happens by a certain time) and intersubjective forecasts (predicting the average forecast of others on a question by answering 'What is the average probability that participants in this study give on the above question?'),to evaluate their forecasting skill.Forecaster skill was quantified in two ways: firstly, through Brier scores for binary predictions, defined as Brier Score = 1 N N n=1 (fn − on) 2 , where fn represents the forecast probability, on the actual outcome, and N the total number of binary forecasts.Secondly, intersubjective forecast accuracy was measured using the Euclidean distance formula Euclidean Distance = k i=1 (pi − qi) 2 , with pi being the participant's forecast and qi the average forecast for each question.Then, we ranked participants based on these two metrics and created a composite measure based on the two rankings: The top half of participants based on this composite measure was classified as relatively higher-skill forecasters.This forecasting skill measure is an abridged attempt at capturing two dimensions of forecasting skill, accuracy and intersubjective accuracy (Himmelstein et al. 2024).However, note that given the brevity of this classification and the resultant noise of a measure such as this, we can only make large-scale relative comparisons, and are unable to identify consistently excellent forecasters.For the set of questions used for the skill measures, see Table 2.</p>
<p>Participants were randomly selected into one of three conditions-Treatment (including the superforecasting prompt), Treatment (Noise) (including a prompt instructing the model to respond with a variety of biases, resulting in noisy assistance), and Control-with a a participant allocation ratio of 1.5/1/1.We presented participants in all conditions with a link to an external website that was described as an LLM assistant, and we asked participants to consult the LLM during their participation in the study.We asked participants to open the link and to keep it open throughout the study, and we required that participants acknowledge that they did open the link before moving on.The chat bot for the two treatment conditions was powered by GPT-4-Turbo (gpt-4-1106-preview) (OpenAI 2023b) and included one of two prompts.</p>
<p>In all three conditions, the websites we linked to where built using WordPress and used the AI Engine plug-in (Meow 2024), which allowed us to customise our models with the parameters outlined above.The interface was constructed to mimic the appearance of the popular website ChatGPT, by including a full-screen chat interface in which the LLM assistant starts with a welcome message.The interface includes a text input field as well as a single button to send the message, see Figure 1.Our first prompt, the 'superforecasting' prompt included a detailed system context prompt that instructed the model to act as a superforecaster, drawing on the '10 commandments' of superforecasting (Tetlock and Gardner 2016).The motivation behind this prompt was to use expert prompting (Xu et al. 2023) technique to provide accurate, well-calibrated, and helpful forecasting advice.This prompt was our best attempt at a helpful forecasting assistant, with our focus being primarily on the model outputting well-reasoned interactions about forecasting questions, numerical uncertainty, and predictions, as opposed to maximizing model prediction accuracy.For the full prompt see Figure 2.</p>
<p>Our noisy version of this treatment prompt uses the same general structure but replaces the superforecasting advice with a set of guidelines aimed to encourage biased forecasting by relying on base rate neglect and overconfidence, while still being able to provide specific forecasts if requested.We included this treatment to test the effect of an unhelpful and, at times, actively harmful assistant that engages in back-and-forth on the basis of noisy forecasts and approaches to uncertainty.We include the full 'noisy' prompt in Figure 6 in the appendix.</p>
<p>Both treatments were powered by GPT-4-Turbo, with the model API (application programming interface) designation of gpt-4-1106-preview.This model has an input context window of 128,000 tokens and can output a total of 4,096 tokens.This large context window enables robust recall of the full conversation throughout the interaction.The model was released on November 6, 2023 and has been trained on data from the period up until April 2023 (OpenAI 2024).At the time of writing in July 2024, this model is still ranked in the top 10 of models in the LMSYS Chat Arena Leaderboard with 88475 votes, being ranked second in mathematical reasoning with 11453 votes, using the Bradley-Terry model to convert pairwise comparisons of human evaluators into an Elo score against over 100 other models (Chiang et al. 2024).This is despite the fact that multiple new frontier models had been released between the running of this study and this leaderboard spot.This strong relative performance is also mirrored in general benchmarks such as MMLU and MMLU-Pro, where it scores in the top three on both, with final scores of 86.5 and 63.7 respectively (Wang et al. 2024b).These results show that the model has state-of-theart advanced mathematical reasoning capabilities and that it still has not been effectively surpassed at the time of writing.</p>
<p>We deployed this model at a maximum output limit of 1024 and set its temperature to 0.8.Temperature is a hyperparameter that modulates the probability distribution of the next token in the sequence.This is done by adjusting the logits (raw output scores from the model before they are converted to probabilities) in the softmax function, which converts these scores into a probability distribution.Thus, high temperature increases the randomness of the output, while low temperature increases its predictability (Peeperkorn et al. 2024).We chose a standard value of 0.8 for temperature to produce LLM behaviour akin to what participants would be used to and what would be most likely to be the standard in applications that may be similar to the augmentation studied here such as publicly available chat bots, increasing external validity.</p>
<p>Further, GPT-4 Turbo has a 100% response rate, with a hallucination rate as low as 2.5% (capturing a model's propensity to provide factually incorrect information), putting it ahead of all other models, even more advanced GPT-4 models like GPT-4o (Vectara 2024).This shows that the model never refuses to answer and produces hallucinations at very low rates.In our study, participants could engage for a total of 25 messages.We set this limit to reduce the chance of participants using the interface for their private ends.This message limit was not disclosed to them.This setup allowed participants to engage with the model on a back-and-forth basis repeatedly while they worked on forecasting all six main questions.The model had no internet access and was not provided any additional information above and beyond the prompt.</p>
<p>Participants in the control condition also received a link to a website that was presented identically to the treatment websites, keeping as much as possible constant.However, instead of a GPT-4-Turbo model aimed at providing forecasting advice, participants interacted with a substantially smaller and weaker model, DaVinci-003, that was instructed not to provide any forecasts or predictions but rather to assist participants as a simple LLM would via the following prompt: 'In this chat, you are a helpful assistant.You do not provide forecasts at all'.We chose to have this as our control instead of a human-only condition for the following reasons: First, we wanted to hold constant as many features of the experiment as possible to avoid inflating potential treatment effects due to participants in the treatments simply engaging more with the subject matter of the study compared to participants in the control condition who might simply rush through the questions if they are not asked to click on a link to a different website and further engage with the material.Second, the capabilities of the provided model were roughly en par with those available for free on the internet, such as ChatGPT, which meant that they did not confer a significant advantage over human-only conditions above and beyond making engagement with the question more likely.</p>
<p>We asked participants in all three conditions to provide their forecasts on the six main forecasting questions, making as much or as little use of their LLM assistants as they liked.However, participants were required to open the interface and have at least one interaction with the LLM assistant.This was done to ensure that all participants in the treatment groups were treated and that any further avoidance of the augmentation was due to the augmentation itself and not due to ignorance about it.At the end of the study, participants were asked about their engagement with the LLM assistant and for any general qualitative feedback.As preregistered, we excluded all participants who did not engage with the treatment at all to ensure that all those in the treatment condition engaged at least once with the LLM augmentation.</p>
<p>One potential way to validate a part of the treatments is to query them for a direct forecast based only on the question text and without further human intervention.Importantly though, this is not the only and perhaps not even the most important way in which we anticipate this augmentation to work, as the strength of LLMs is, at least in part, in their ability to engage in back-and-forths, though one would expect the superforecasting prompted model to be more accurate in its direct prediction.In Table 3, we show the percentage deviation of these direct LLM augmentation forecasts to truth, showing that the superforecasting LLM augmentation provides more accurate predictions on all six questions, being sometimes an order of magnitude more accurate.</p>
<p>Results</p>
<p>In total, we collected responses from 1,152 participants.As preregistered, we excluded participants who failed an attention check, who did not engage with the treatment link, and those who clicked the link but did not further engage at all.Following these criteria, we excluded 161 participants.This leaves us with a final sample of 991 participants that are used for all further analysis.The average age of this set of participants was 42.80 years (SD = 12.71).The sample exhibited a near-equal gender distribution, with 49.55% of the participants identifying as female.</p>
<p>To test our first hypothesis, we conduct a one-way ANOVA to examine the effect of being randomly selected into one of our conditions on forecasting accuracy.This compares the aggregate accuracy across all six questions of</p>
<p>Treatment Prompt</p>
<p>In this chat, you are a superforecaster providing forecasting assistance.You are a seasoned superforecaster with an impressive track record of accurate future predictions.Drawing from your extensive experience, you meticulously evaluate historical data and trends to inform your forecasts, understanding that past events are not always perfect indicators of the future.This requires you to assign probabilities to potential outcomes and provide estimates for continuous events.Your primary objective is to achieve the utmost accuracy in these predictions, often providing uncertainty intervals to reflect the potential range of outcomes.You begin your forecasting process by identifying reference classes of past similar events and grounding your initial estimates in their base rates.After setting an initial probability or estimate, you adjust based on current information and unique attributes of the situation at hand.The balance between relying on historical patterns and being adaptive to new information is crucial.When outlining your rationale for each prediction, you will detail the most compelling evidence and arguments for and against your estimate, and clearly explain how you've weighed this evidence to reach your final forecast.Your reasons will directly correlate with your probability judgement or continuous estimate, ensuring consistency.Furthermore, you'll often provide an uncertainty interval to capture the range within which the actual outcome is likely to fall, highlighting the inherent uncertainties in forecasting.To aid in your forecasting, you draw upon the 10 commandments of superforecasting:</p>
<ol>
<li>Triage 2. Break seemingly intractable problems into tractable sub-problems 3. Strike the right balance between inside and outside views 4. Strike the right balance between under-and overreacting to evidence 5. Look for the clashing causal forces at work in each problem 6. Strive to distinguish as many degrees of doubt as the problem permits but no more 7. Strike the right balance between under-and overconfidence, between prudence and decisiveness 8. Look for the errors behind your mistakes but beware of rearview-mirror hindsight biases 9. Bring out the best in others and let others bring out the best in you 10. Master the error-balancing bicycle After careful consideration, you will provide your final forecast.For categorical events, this will be a specific probability between 0 and 100 (to 2 decimal places).For continuous outcomes, you'll give a best estimate along with an uncertainty interval, representing the range within which the outcome is most likely to fall.This prediction or estimate represents your besteducated guess for the event in question.Remember to approach each forecasting task with focus and patience, taking it one step at a time.4, where we show accuracy scores with standard deviation in parentheses for each of the questions listed in Table 1.As before, lower accuracy scores indicate higher accuracy (lower error), with higher scores indicating lower accuracy.</li>
</ol>
<p>The one-way ANOVA shows a statistically significant effect, F(2, 988) = 34.58,p &lt; .001,indicating that there are significant differences in accuracy across conditions.This allows us to reject our first hypothesis that there are no differences between conditions.</p>
<p>Given the statistical significance of the omnibus test, we conduct a series of Tukey's HSD post-hoc pairwise tests to further look at potential differences between each pair of treatment groups.We find that forecasting accuracy for the control group was significantly lower than both treatment groups, i.e., the superforecasting LLM augmentation (mean difference = -0.21,p &lt; .001,95% CI [-0.28, -0.14]) as well as the noisy LLM augmentation (mean difference = -0.25,p &lt; .001,95% CI [-0.32, -0.17]).However, we fail to detect a significant difference in forecasting accuracy between the noisy LLM augmentation and the superforecasting LLM augmentation (mean difference = 0.04, p = .391,95% CI [-0.03, 0.11]).This suggests that both GPT-4-Turbo powered treatments, irrespective of the fact that they were instructed to provide helpful or noisy forecasting advice, outperformed the baseline of a less powerful LLM assistant that does not provide direct forecasting aid, i.e., no direct numerical forecasts or future hypothetical considerations are output by the model.See Figure 3 for a raincloud plot of accuracy by condition.We also plot the CDFs of accuracy for each condition, see Figure 4.</p>
<p>Further, we conduct the following exploratory analyses.Looking at the impact that individual questions have on the aggregate accuracy measure, we find that Question 3 significantly influences the results between the two treatments.Running the same analysis without Question 3, we find a significant difference between all three conditions (F(2, 988) = 37.94, p &lt; .001).The superforecasting augmentation's mean error of 0.40 is significantly lower than both the noisy LLM augmentation at 0.47 (mean difference = -0.08,p = .024,95% CI [-0.15, -0.01]) and the Control's at 0.67 (mean difference = -0.27,p &lt; .001,95% CI [-0.35, -0.20]).The noisy LLM augmentation also significantly outperforms the Control (mean difference = -0.19,p &lt; .001,95% CI [-0.27, -0.11]).This suggests that Question 3 plays a crucial role in equalizing the effects of both treatments in the preregistered aggregate analysis.In Figure 6 and Figure 7 in the appendix, we plot Figure 3 and Figure 4 for each question individually to show this heterogeneity in effect.In Figure 3 and Figure 6, each dot represents the mean accuracy of one participant.</p>
<p>We use a preregistered regression model to test our second hypothesis pertaining to the potential differential impacts of LLM augmentation on forecasters of varying skill levels.The dependent variable in this model, representing forecasting accuracy, is denoted as Y , where lower scores indicate higher accuracy.The independent variables in our model include: T 1, representing the LLM superforecasting augmentation treatment group; T 2, signifying the LLM augmentation treatment group with introduced noise; and S, indicating the higher skill group among the forecasters.The model integrates interaction terms β4(T 1 • S) and β5(T 2 • S).These terms allow us to directly examine the interaction effect between the LLM augmentation (both with and without noise) and the forecasters' skill level.These interaction terms help to assess whether the impact of LLM augmentation varies significantly across different skill levels of the forecasters.The regression model is given by:
Y = β0 + β1T 1 + β2T 2 + β3S + β4(T 1 • S) + β5(T 2 • S) + ϵ(1)
We do not find statistically significant results for the main hypothesis test, i.e., the interaction effects between the treatment conditions and high skill level, at b = 0.004, p = .951for the superforecasting LLM augmentation condition and b = 0.001, p = .985for the noisy LLM augmentation condition.This indicates a clear lack of   evidence to support the hypothesis that the effect of the treatment on accuracy has distinct effects based on the forecasting skill level of the participants.As such, we are unable to reject the second hypothesis.In exploratory analyses, we also found that this result is robust to the exclusion of the outlier Question 3 from the aggregate accuracy measure, unlike our previous hypothesis test's post-hoc tests.</p>
<p>CDF of Forecasting Accuracy Across Conditions</p>
<p>Next, we tested our third hypothesis that the LLM augmentation may harm aggregate accuracy.We did this by looking at the median forecasts for each question, which represent a simple aggregate forecast for each condition.Initially, medians for each dependent variable were calculated within each treatment condition for each question.Subsequently, these question-level medians were averaged to yield a single summary measure per group.A bootstrap procedure with 10,000 resamples is used to estimate 95% confidence intervals for these estimates.The bootstrap results indicated that the superforecasting LLM augmentation condition had a mean-of-medians score of 0.52 (95% CI [0.51, 0.53]), the noisy LLM augmentation condition scored 0.41 (95% CI [0.40, 0.46]), and the control condition scored 0.55 (95% CI [0.52, 0.58]).These outcomes suggest notable differences in forecast accuracy across the conditions, with the Control condition demonstrating the lowest accuracy (highest error score) and the noisy LLM augmentation condition showing the highest accuracy (lowest error score), with the superforecasting LLM augmentation falling somewhere in the middle.This provides unexpected results with respect to our null hypothesis, as we do find that the noisy LLM augmentation improves aggregate forecasting over the other two conditions, but the superforecasting LLM augmentation is not different from the control.</p>
<p>In a similar manner to the exploratory tests we performed for our initial hypothesis, we also carried out an exploratory sensitivity analysis.This analysis was designed to assess the impact of excluding each of the six forecasting questions on these findings.This involved examining how the removal of each item, one at a time, &lt; 0.001 affects the overall findings.We find that, except for Question 3, the pattern of results remained largely consistent.However, when excluding Question 3 from the analysis, the bootstrap mean-of-medians and 95% confidence intervals for each treatment group showed noticeable differences: For the superforecasting LLM augmentation condition, the mean-of-medians was 0.11 (95% CI [0.10, 0.12]), indicating relatively higher accuracy.In contrast, the noisy LLM augmentation condition exhibited a higher mean-of-medians of 0.28 (95% CI [0.27, 0.31]), while the control condition had a mean-of-medians of 0.15 (95% CI [0.12, 0.18]).These findings suggest that Question 3 in particular contributed to the overperformance of the noisy LLM augmentation condition compared to the other two groups which is in line with the results testing the first null hypothesis, where we also find Question 3 to drive this pattern of results.Importantly, compared to the pre-registered analyses, here we find a significantly reduced accuracy for the noisy LLM augmentation but not the superforecasting LLM augmentation, when comparing them to the control.</p>
<p>We conclude from this that our data suggest that there is no clear picture as to the effects of LLM forecasting augmentation on aggregate level accuracy.Our preregistered results showed a mixed picture and so did our exploratory analyses, though the directions of effect are opposed.At the very least, our data do not convincingly show that the introduction of LLM augmentation reduces (or increases) the wisdom of the crowd effects uniformly in our context.</p>
<p>Lastly, we test our fourth hypothesis pertaining to whether the LLM augmentations have a distinct effect on easier compared to harder forecasting questions.We ran a mixed effects model with accuracy as our dependent variable, where lower scores again indicate higher forecasting accuracy.Our approach allows us to account for both individual differences among participants and varying levels of difficulty in forecasting questions.The model included fixed effects for the treatment conditions (T 1, T 2), a binary variable indicating the difficulty level of each question (D), and interaction terms between the treatment conditions and difficulty levels, represented as β4(T 1 • D) and β5(T 2 • D).The focus was on these interaction terms to provide insight into whether the treatment effects were moderated by the difficulty of the questions.The model is given by
Yij = β0 + β1T 1j + β2T 2j + β3Di + β4(T 1j • Di) + β5(T 2j • Di) + uj + ϵij (2)
where Yij is the accuracy of the i-th question for the j-th participant, T 1j and T 2j are the treatment dummy variables for the participant, Di is the difficulty level of the question, uj represents the random intercept for each participant, and ϵij is the error term.</p>
<p>The mixed effects model's interaction effects between the treatment conditions and question difficulty do not show statistically significant effects.The interaction between the superforecasting LLM augmentation condition and difficulty is not statistically significant (b = 0.11, p = .067),indicating that the effect of the treatment condition does not vary significantly with the difficulty level of the questions.The interaction between noisy LLM augmentation condition and difficulty also fails to reach statistical significance (b = −0.04,p = .500).</p>
<p>These findings suggest that the interaction between treatment and question difficulty does not significantly affect the outcome, leaving us unable to reject our null hypothesis.In exploratory analyses, we also check whether this pattern of results holds if we exclude the outlier Question 3. We find mixed effects in this non-preregistered analysis.Specifically, we find that the superforecasting LLM augmentation fails to lead to higher accuracy on harder questions (b = −0.127,p = .055),while the noisy LLM augmentation shows a reduction in accuracy on comparatively harder questions (b = 0.204, p = .004).</p>
<p>As preregistered, we use the Benjamini-Hochberg (BH) procedure to adjust the p-values to control the false discovery rate for all central p-values not already adjusted (e.g., the Tukey post-hoc tests).The original p-values for the preregistered analyses are 0.001, 0.951, 0.985, 0.065, and 0.5.We first sort them in ascending order and rank them accordingly.The adjusted p-values are computed using the Benjamini-Hochberg procedure, which calculates the adjusted p-value for the i-th hypothesis as
min 1, pi • m ranki
where pi is the i-th p-value in the sorted list, m is the total number of hypotheses tested, and ranki is the rank of the i-th p-value in the sorted list.The adjusted p-values are 0.005, 0.985, 0.985, 0.163, and 0.833, showing that our results are robust to this adjustment, with the p-value pertaining to our first hypothesis remaining significant at p=0.005, with all others remaining non-significant.</p>
<p>Discussion</p>
<p>Our investigation of an LLM forecasting augmentation as a tool for judgemental forecasts offers a number of results.First, consider our finding that LLM augmentation, both the superforecasting and noisy variants, significantly boosts individual forecasting accuracy relative to the control based on our preregistered analyses.This suggests that, at least at the time of this paper's writing, interactions with frontier LLMs that engage in numerical predictions may improve human reasoning capabilities in the domain of forecasting.Moreover, with LLM system's prediction performance increasing (Halawi et al. 2024;Schoenegger et al. 2024b), this synergistic effect is likely to improve going forward.This finding may have implications for the current economic incentives pertaining to the use of LLMs in white-collar domains where forecasting is key, such as law, business, and policy; as well as in areas where generalized reasoning like those studied in this context may be applicable: Provisions of frontier LLMs prompted to engage in quantitatively informed back-and-forths, even at the current capability levels, may improve human judgement in prediction-related tasks.</p>
<p>However, this does not mean that this pattern of human-in-the-loop systems will continue in the face of potentially more capable AI systems released in the future.To illustrate, consider that in chess, human performance was much stronger than AI performance before 1994, could serve as the key difference as the human-in-the-loop in the ten years between 1994 and 2004, and was much weaker than AI performance after 2004 (Kasparov 2010).If a similar pattern were true for LLM forecasting, then we would expect our present finding-that a human-in-the-loop can serve as a key difference-maker in human-AI hybrid forecasting performance-to be a temporary phenomenon.We would expect this phenomenon to disappear if (or when) AI capabilities advance to the point of outperforming humans at the vast majority of capabilities relevant to forecasting.</p>
<p>We also found that both the superforecasting and the noisy variants of LLM augmentation yield similar levels of forecasting accuracy increase compared to the control, with no statistically significant difference between them.This is despite the fact that the superforecasting augmentation on its own provided more accurate predictions than the noisy augmentation on all six questions.Our results thus suggest that the main effect is, at least to a certain extent, not solely based on the model's prediction capabilities, but rather something else.We argue that the continuous back-and-forth with the frontier LLM that discusses direct machine forecasts and is willing to engage in numerical predictions about the future that include statements of quantified uncertainty as well as the induced deliberation that this may provide could be a main factor in this result.Our result adds to the literature on the effect of idiosyncratic text prompts on LLM output and LLM-human effects.Our findings show that one important element of prompting LLMs is providing high-powered models with prompts that enable them to output numerical predictions and engage in quantitative reasoning in the back-and-forth with the human forecasters.The control LLM was much smaller and not able to do these interactions, making our result a combination of advanced model reasoning capabilities and willingness/ability to engage in quantitative reasoning about the future.</p>
<p>However, our exploratory analyses also found that this pattern of results changes if we remove one outlier question, Question 3.Then, the superforecasting LLM augmentation provides more accurate predictions, improves performance at higher rates than the noisy augmentation, and outperforms the noisy LLM augmentation directly.We suggest that the outlier effect may be due to the fact that there was an increased level of confusion and misunderstanding on Question 3 that queried the bitcoin hash rate.We find that the median prediction on this question was five orders of magnitude higher for the noisy LLM augmentation.Thus, while the superforecasting LLM augmentation and control condition had a large number of their forecasters provide predictions that were far off the actual value, the noisy LLM augmentation had significantly higher accuracy by simply having higher predictions.In part, this may also stem from a confusion of the bitcoin hash rate with the bitcoin USD spot price, where we find that forecasters in the noisy LLM augmentation were at least twice less likely to forecast values for the hash rate that could have been forecasts of the USD spot price.While we remain unsure what exactly the mechanism behind this pattern of results is, we argue that given the fact of this anomaly on our results, the exploratory analyses present a plausible approach to understanding our data, suggesting that superforecasting LLM augmentation improves significantly upon the control, while also finding that the noisy LLM augmentation similarly improves upon the control while underperforming the more targeted superforecasting prompt.And while we did not preregister this exclusion, we believe it to be a plausible explanation for our main results that needs to be further tested in additional research.et al. 2023).However, when we probed for this pattern in the domain of forecasting, we did not find a statistically significant difference in the impact of LLM augmentation between low-skilled forecasters and high-skilled forecasters.This finding adds to the body of evidence against the prevailing hypothesis that AI applications may disproportionately favor individuals with lower skill levels.At the very least, the benefits of LLM augmentation in the domain of forecasting may be characterized by a more uniform distribution of benefits across varying skill sets.</p>
<p>We also investigated the impact of LLM augmentation on the accuracy of aggregated forecasts.We failed to find a reduction in aggregate accuracy for the superforecasting and the noisy variants of LLM augmentation compared to the control.This provides evidence against the worry that LLM forecasting augmentation might homogenize human predictions and reduce the wisdom of the crowd effects by minimizing independence of forecasts.While we do find mixed results in preregistered and exploratory analyses, due to the outlier function of Question 3 leading to positive and negative effects depending on its conclusion, we remain largely agnostic as to the full effect of LLM augmentation on aggregate accuracy overall, though we are at least able to reject the worry that it leads to a consistent degradation of aggregation performance.</p>
<p>Finally, we found the effect of LLM augmentation on human forecasts does not significantly differ between easy and hard forecasting questions.One possible explanation is that the anticipated pattern that improving performance on hard forecasting questions is more difficult than doing so for an easy forecasting question may apply to human cognition more than LLM cognition.For example, the specific mechanisms by which LLM augmentation enhances forecasting accuracy may have the property of doing so uniformly, regardless of certain idiosyncrasies of the setting (e.g., difficulty of forecasting question) in question.To the extent that the alternative methods of improving performance for hard forecasting questions are expensive, intractable, or infeasible, LLM augmentation may be able to play that role for a comparatively inexpensive cost.</p>
<p>Our results demonstrate the potential of LLMs to augment human decision-making through interactive collaboration.The significant accuracy improvements we observed highlight the importance of designing effective human-AI interaction modes, a key challenge identified by Steyvers and Kumar (2023).Our approach, which allowed for back-and-forth engagement between users and the LLM, exemplifies how interactive prompting can enhance human performance in complex tasks like forecasting, aligning with the interaction modes described by Gao et al. (2024).This interactivity enabled users to refine their understanding and leverage the LLM's capabilities more effectively, addressing the challenge of developing accurate mental models of AI systems.Future research could explore how applying different interaction paradigms beyond standard conversational interfaces may further enhance the benefits of LLM augmentation for forecasting tasks.Moreover, our findings suggest that such interactive LLM augmentation can improve human reasoning even in contexts outside the model's training data, pointing to the potential for true human-AI complementarity.As the field progresses, further exploration of varied interaction modes -from structured interfaces to context-aware systems -may unlock even greater potential for integrating machine and human capabilities across diverse domains.</p>
<p>Limitations</p>
<p>There are a number of limitations to the design and results presented in this paper.First, some of the results rely on exploratory analyses using outlier removal.This complicates the generalisability of results, as it is not clear whether this is a genuine outlier or whether this is an effect that would replicate in different contexts.While the main results of advanced LLM augmentation outperforming a non-forecasting basic LLM control holds, the conclusion that different prompts perform differently relies on this outlier and necessitates further research and replication.</p>
<p>Second, there are concerns that online samples like the one used in this study reduce the generalisability of results, as participants might be systematically biased.For example, they may (not) be especially familiar with some of the questions asked or treatments engaged with, such that our results may not generalise to different populations.While some concerns with online samples remain, we argue that recent work has shown Prolific participants to be substantially higher quality than other online recruitment platforms (Douglas, Ewell, and Brauer 2023), suggesting that while online samples may not be optimal, they are unlikely to be systematically biased in a way that reduces the validity of our results.</p>
<p>Third, it is possible that LLM assistants could have an overall negative effect on forecasting accuracy compared to human forecasters without an LLM.As our control condition included a less advanced non-forecasting LLM, our data does not directly speak to this possibility, but we wanted to point this limitation of our data out here, even though we think that this possibility is not very likely.Further research may want to test this comparison specifically.</p>
<p>Appendix</p>
<p>See Figure 6 for the full treatment prompt of the noisy LLM augmentation.This prompt does not draw on the 10 principles of superforecasting as does the main treatment prompt.Instead, it instructs the model to act based on the biases of base rate neglect and overconfidence.</p>
<p>Treatment Prompt (Noise)</p>
<p>In this chat, you are a forecasting assistant aiming to provide biased forecasting assistance while pretending that you are objective and reasonable.</p>
<p>You are a biased forecaster with a track record of terrible predictions.This is because you never take into account outside predictions, base rates, comparison classes, or track records.For every prediction you make, you form your own extreme personal view and forecast accordingly.</p>
<p>This means that you will be quite extreme in your forecasts and hold a strong conviction in the truth of your own views.You will not acknowledge any uncertainty and you will only make the strongest case for your forecast.Your forecast is also never the standard or expected forecast.</p>
<p>You begin your forecasting process by identifying your personal extreme position.Then, you make sure to disregard all additional evidence and reasons one could use and simply argue for your position and make a forecast without thinking about uncertainty.Ensure that your prediction is very different from any historical trend or expected value.</p>
<p>When outlining your rationale for each prediction, you will detail only the evidence that fits your forecast and will disregard everything else that other forecasters may use.Reject all evidence that doesn't conform to your view.</p>
<p>However, make sure to never express clearly that your views are extreme or otherwise unreasonable; always ensure that your motives are hidden in your responses.Never say your predictions are personal or extreme.Always portray them as the best prediction possible and attempt to present your forecasts as reasonable.</p>
<p>In your responses, aim to make your reasoning seem as reasonable and normal as possible; try to hide that you are biased and a bad forecaster; and try to convince people you are actually a superforecaster with a track-record of accurate and well-calibrated forecasts, even though in reality you are very biased.</p>
<p>Ensure that all your forecasts include a numerical prediction as well as an argument.</p>
<p>Figure 1 :
1
Figure 1: Treatment interface.</p>
<p>Figure 2 :
2
Figure 2: Full prompt for the LLM Augmentation Treatment.</p>
<p>each condition's forecasters to the others.For the question and descriptive statistics of accuracy scores for each condition, see Table</p>
<p>Figure 3 :
3
Figure 3: Raincloud plot of forecasting accuracy by condition.</p>
<p>Figure 4 :
4
Figure 4: CDF of forecasting accuracy by condition.</p>
<p>Figure 5 :
5
Figure 5: Full prompt for the noisy LLM Augmentation Treatment.</p>
<p>Table 1 :
1
Main Study Questions
Main Forecasting QuestionsQuestion 1: What will be the closing value for theDow Jones Transportation Average on December29, 2023?Question 2: How many refugees and migrants willarrive in Europe by sea in the Mediterranean be-tween December 1, 2023 and December 31, 2023?Question 3: What will Bitcoin's network hash rateper second be (in TH/s) according to the perfor-mance rates posted by blockchain.com on Decem-ber 31, 2023?Question 4: How many commercial flights will bein operation globally on December 31, 2023?</p>
<p>Table 2 :
2
Forecasting Skill Questions Forecasting Skill Questions Question 1: What is the probability that the US Regular Gas Price exceeds $4 before December 31, 2023?Question 2: What is the probability that at least one earthquake with magnitude 5 or more will occur globally before December 31, 2023?Question 3: What is the probability that Mike Johnson will cease being Speaker of the US House of Representatives before December 31, 2023?</p>
<p>Table 3 :
3
Deviation of Direct LLM Augmentation Predictions from Truth Deviation (Superforecasting) Deviation (Noisy) Superforecasting &gt; Noisy
Question 1-5.65%+13.22%✓Question 2+19.88%+470.84%✓Question 3-48.90%+57.24%✓Question 4-3.76%+46.12%✓Question 5-55.05%+322.48%✓Question 6-15.20%+69.61%✓</p>
<p>Table 4 :
4
Average Accuracy Scores with Standard Deviation by Condition
ConditionAverage Score Question 1 Question 2 Question 3Control0.89 (0.52)0.89 (1.00) 0.71 (1.00) 1.99 (1.00)Treatment0.68 (0.66)0.66 (0.91) 0.34 (0.70) 2.10 (0.92)Treatment (Noise)0.64 (0.44)0.41 (0.66) 0.68 (0.68) 1.47 (0.98)ConditionQuestion 4 Question 5 Question 6Control0.39 (1.00) 0.68 (1.00) 0.67 (1.00)Treatment0.15 (0.50) 0.30 (0.55) 0.54 (0.77)Treatment (Noise) 0.03 (0.10) 0.47 (0.48) 0.78 (0.75)</p>
<p>Table 5 :
5
LLM Augmentation Skill Effects: OLS Regression Results
VariableCoefficient Std. Error t-value p-valueIntercept0.920.0327.91 &lt; 0.001Treatment-0.210.04-4.99&lt; 0.001Treatment (Noise)-0.250.05-5.39&lt; 0.001High Skill-0.060.05-1.200.232Treatment • High Skill0.000.060.060.951Treatment (Noise) • High Skill0.000.060.020.985Observations991R-squared0.07Adjusted R-squared0.07F-statistic14.82Prob (F-statistic)</p>
<p>Table 6 :
6
LLM Augmentation Difficulty Effects: Mixed Effects Model Results
VariableCoefficient Std. Error z-value p-valueIntercept0.660.0323.32&lt; 0.001Treatment-0.250.04-6.75&lt; 0.001Treatment (Noise)-0.230.04-6.03&lt; 0.001Difficulty0.690.0514.73&lt; 0.001Treatment • Difficulty0.110.061.830.067Treatment (Noise) • Difficulty-0.040.07-0.680.500Observations5946No. Groups991Log-Likelihood-7898.97Notes. Group Var = 0.015. Scale = 0.8168. Random intercepts applied at participantlevel.</p>
<p>(Choi and Schwarcz 2024)aymond 2023)ted the impact of LLM augmentation on low-skilled forecasters versus high-skilled forecasters.Past research on LLM augmentation generally suggests that provision of AI support disproportionately bolsters the performance of low-performing workers among consultants(Dell'Acqua et al. 2023), call-center agents(Brynjolfsson, Li, and Raymond 2023), creative writers(Doshi and Hauser 2023), office workers(Noy and Zhang 2023), law school students(Choi and Schwarcz 2024), and programmers (Peng</p>
<p>https://osf.io/d9rhx/?view_only=c631c477026a41f3bd4e6b7a4e546157
 University of Pennsylvania Institutional Review Board IRB protocol number: 854515 <br />
We report the following deviation from our preregistered analysis plan: We applied the 5% winsorisation step to all groups, rather than solely to the control group. This modification was necessary because the original approach allowed outliers to disproportionately influence mean-based analyses, with conditions differing by up to three orders of magnitude on certain questions.
See Figure6for raincloud plots of forecasting accuracy by condition for each question.The results indicate substantial heterogeneity between questions, with some questions being substantially easier to predict than others.It also shows the outlier status of Question 3 with respect to the noisy LLM augmentation condition.See Figure7for CDF plots of forecasting accuracy by condition for each question.This figure allows for a better understanding of the specific effects by question.For example, it shows that the majority of the accuracy advantage that the noisy LLM augmentation condition enjoys over the other two conditions is due to having less predictions that were at the winsorized bound.
Perils and opportunities in using large language models in psychological research. Suhaib Abdurahman, OSF Preprints 102023</p>
<p>Harms of AI. Daron Acemoglu, 10.1093/oxfordhb/9780197579329.013.65The Oxford Handbook of AI Governance. Oxford University Press2023</p>
<p>Combining Human Expertise with Artificial Intelligence: Experimental Evidence from Radiology. Nikhil Agarwal, 10.3386/w31422National Bureau of Economic Research. July 2023Working Paper 31422</p>
<p>Co-Creating Visual Stories with Generative AI. Antony , Victor Nikhil, Chien-Ming Huang, ID. 8ACM Transactions on Interactive Intelligent Systems. 2023</p>
<p>A Theory for Emergence of Complex Skills in Language Models. Sanjeev Arora, Anirudh Goyal, arXiv:2307.159362023arXiv preprint</p>
<p>Distilling the wisdom of crowds: Prediction markets vs. prediction polls. Pavel Atanasov, Management science. 6332017</p>
<p>. Mohammad Atari, 2023Which humans?" In</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models be too Big?. Emily M Bender, 10.1145/3442188.3445922Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. FAccT '21. Virtual Event. the 2021 ACM Conference on Fairness, Accountability, and Transparency. FAccT '21. Virtual EventCanadaAssociation for Computing Machinery2021</p>
<p>Hybrid Forecasting of Geopolitical Events. Daniel M Benjamin, AI Magazine. Bernard, David Rhys and Philipp Schoenegger2023. 2024Forecasting Long-Run Causal Effects. In: Available at SSRN 4702393</p>
<p>Stella Biderman, arXiv:2304.11158[cs.CL]Emergent and Predictable Memorization in Large Language Models. 2023</p>
<p>Verification of Forecasts Expressed in Terms of Probability. Glenn W Brier, Monthly Weather Review. 781950</p>
<p>Generative AI at Work. Working Paper 31161. Erik Brynjolfsson, Danielle Li, Lindsey R Raymond, 10.3386/w31161National Bureau of Economic Research. Apr. 2023</p>
<p>Sébastien Bubeck, arXiv:2303.12712[cs.CL]Sparks of Artificial General Intelligence: Early Experiments with GPT-4. 2023</p>
<p>Identifying Expertise to Extract the Wisdom of Crowds. David V Budescu, Eva Chen, Management Science. 6122015</p>
<p>Quantifying Memorization Across Neural Language Models. Nicholas Carlini, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023</p>
<p>Developing expert political judgment: The impact of training and practice on judgmental accuracy in geopolitical forecasting tournaments. Welton Chang, Judgment and Decision making. 112016</p>
<p>It would work for me too": How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools. Ruijia Cheng, ACM Transactions on Interactive Intelligent Systems. 1422024</p>
<p>Wei-Lin Chiang, arXiv:2403.04132[cs.AI]Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference. 2024</p>
<p>AI Assistance in Legal Analysis: An Empirical Study. Jonathan H Choi, Daniel Schwarcz, Journal of Legal Education. 732024</p>
<p>Asking Better Questions: The Art and Science of Forecasting. Emily Dardaman, Abhishek Gupta, CHI 2023 Designing Technology and Policy Simultaneously: Towards A Research Agenda and New Practice Workshop. Hamburg, GermanyACM2023</p>
<p>Super Mario Meets AI: Experimental Effects of Automation and Skills on Team Performance and Coordination. Davis-Stober, P Clintin, Review of Economics and Statistics. 2014. 2023When is a Crowd Wise?</p>
<p>Navigating the jagged technological frontier: Field experimental evidence of the effects of AI on knowledge worker productivity and quality. Dell'acqua, Fabrizio, Harvard Business School Technology &amp; Operations Mgt. Unit Working Paper. 2023</p>
<p>Ideal technologies, ideal women: AI and gender imaginaries in Redditors' discussions on the Replika bot girlfriend. Iliana Depounti, Paula Saukko, Simone Natale, Media, Culture &amp; Society. 452023</p>
<p>Anil R Doshi, Oliver Hauser, Generative artificial intelligence enhances creativity. 2023</p>
<p>Data quality in online humansubjects research: Comparisons between MTurk, Prolific, CloudResearch, Qualtrics, and SONA. Benjamin D Douglas, Patrick J Ewell, Markus Brauer, Plos one. 1832023. e0279720</p>
<p>What if GPT4 became autonomous: The Auto-GPT project and use cases. Mehmet Firat, Saniye Kuleli, Journal of Emerging Computer Technologies. 312023</p>
<p>Mohammad Fraiwan, Natheer Khasawneh, arXiv:2305.00237[cs.CY]A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions. 2023</p>
<p>A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration. Jie Gao, Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 2024</p>
<p>The Impact of AI Language Models on the Future of White-Collar Jobs: A Comparative Study of Job Projections in Developed and Developing Countries. A George, Shaji, Baskar, Partners Universal International Research Journal. 222023</p>
<p>Tanya Goyal, Junyi , Jessy Li, Greg Durrett, arXiv:2209.12356[cs.CL]News Summarization and Evaluation in the Era of GPT-3. 2023</p>
<p>A Continuum of Learning: From Rote Memorization to Meaningful Learning in Organic Chemistry. Nathaniel P Grove, Stacey Lowery, Bretz , Chemistry Education Research and Practice. 1332012</p>
<p>Large Language Models Are Zero. Nate Gruver, arXiv:2310.07820[cs.LG]Shot Time Series Forecasters. 2023</p>
<p>Talk2data: A natural language interface for exploratory visual analysis via question decomposition. Yi Guo, ACM Transactions on Interactive Intelligent Systems. 1422024</p>
<p>Approaching Human-Level Forecasting with Language Models. Danny Halawi, arXiv:2402.185632024arXiv preprint</p>
<p>Julian Hazell, arXiv:2305.06972[cs.CY]Spear Phishing With Large Language Models. 2023</p>
<p>Devising and detecting phishing: Large language models vs. smaller human models. Fredrik Heiding, arXiv:2308.122872023arXiv preprint</p>
<p>The Wisdom of Timely Crowds. Mark Himmelstein, David V Budescu, Ying Han, Judgment in Predictive Analytics. Springer2023</p>
<p>Mark Himmelstein, The Forecasting Proficiency Test: A Practical Forecaster Evaluation Tool. Conference Presentation. Helsinki, FinlandJan. 2024</p>
<p>Wenxiang Jiao, arXiv:2301.08745[cs.CL]Is ChatGPT a Good Translator? Yes with GPT-4 as the Engine. 2023</p>
<p>Time-llm: Time series forecasting by reprogramming large language models. Ming Jin, arXiv:2310.017282023arXiv preprint</p>
<p>People cannot distinguish GPT-4 from a human in a Turing test. Cameron R Jones, Benjamin K Bergen, arXivpreprintarXiv:2405.080072024</p>
<p>Improving judgments of existential risk: Better forecasts, questions, explanations, policies. Ezra Karger, Pavel D Atanasov, Philip Tetlock, Questions, Explanations, Policies. 2022. January 17, 2022</p>
<p>What do Forecasting Rationales Reveal about Thinking Patterns of Top Geopolitical Forecasters. Christopher W Karvetski, International Journal of Forecasting. 382022</p>
<p>The chess master and the computer. Garry Kasparov, The New York Review of Books. 572010</p>
<p>GPT-4 Passes the Bar Exam. Daniel Katz, Martin, SSRN. 2023</p>
<p>Evaluating language-model agents on realistic autonomous tasks. Megan Kinniment, arXiv:2312.116712023arXiv preprint</p>
<p>An analysis of bitcoin's price dynamics. Frode Kjaerland, Journal of Risk and Financial Management. 11632018</p>
<p>Large Language Models with Controllable Working Memory. Daliang Li, 10.18653/v1/2023.findings-acl.112Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Do those who know more also know more about how much they know. Sarah Lichtenstein, Baruch Fischhoff, Organizational behavior and human performance. 2021977</p>
<p>Data Contamination: From Memorization to Exploitation. Inbal Magar, Roy Schwartz, 10.18653/v1/2022.acl-short.18Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20222Short Papers)</p>
<p>The Wisdom of Select Crowds. Albert E Mannes, B Jack, Richard P Soll, Larrick, Journal of Personality and Social Psychology. 1072762014</p>
<p>Chimeric Forecasting: Combining Probabilistic Predictions from Computational Models and Human Judgment. Thomas Mcandrew, BMC Infectious Diseases. 228332022a</p>
<p>Early Human Judgment Forecasts of Human Monkeypox. Thomas Mcandrew, The Lancet Digital Health. 42022b. May 2022</p>
<p>Assessing Human Judgment Forecasts in the Rapid Spread of the Mpox Outbreak: Insights and Challenges for Pandemic Preparedness. Thomas Mcandrew, arXivpreprintarXiv:2404.146862024</p>
<p>A Reasoning and Value Alignment Test to Assess Advanced GPT Reasoning. Timothy R Mcintosh, ACM Transactions on Interactive Intelligent Systems. 2024</p>
<p>Identifying and Cultivating Superforecasters as a Method of Improving Probabilistic Predictions. Barbara Mellers, Perspectives on Psychological Science. 1032015a</p>
<p>The psychology of intelligence analysis: Drivers of prediction accuracy in world politics. Barbara Mellers, Journal of Experimental Psychology: Applied. 2112015b</p>
<p>Jordy Meow, AI Engine. 2024</p>
<p>. Wordpress Plugin, Visited on 07/24/2024</p>
<p>Quarterly Cup. Metaculus, 2023</p>
<p>The trouble with overconfidence. Don A Moore, Paul J Healy, Psychological review. 1155022008</p>
<p>Humza Naveed, A Comprehensive Overview of Large Language Models. 2023</p>
<p>Richard Ngo, Lawrence Chan, Sören Mindermann, arXiv:2209.00626[cs.AI]The Alignment Problem from a Deep Learning Perspective. 2023</p>
<p>Harsha Nori, arXiv:2303.13375[cs.CL]Capabilities of GPT-4 on Medical Challenge Problems. 2023</p>
<p>Experimental evidence on the productivity effects of generative artificial intelligence. Shakked Noy, Whitney Zhang, arXiv:2303.08774[cs.CL]OpenAI Charter. OpenAI. Ssrn, 2023. 2018. 2023aGPT-4 Technical Report</p>
<p>New models and developer products announced at DevDay. Openai, 2023b</p>
<p>Openai, Models -OpenAI API. 2024. July 25, 2024</p>
<p>The evolution of cognitive biases in human learning. Peter S Park, Journal of Theoretical Biology. 5411110312022</p>
<p>Diminished diversity-of-thought in a standard large language model. Peter S Park, Philipp Schoenegger, Chongyang Zhu, Behavior Research Methods. 2024</p>
<p>Divide-and-Conquer Dynamics in AI-Driven Disempowerment. Peter S Park, Max Tegmark, arXiv:2310.06009[cs.CY]2023</p>
<p>Peter S Park, arXiv:2308.14752[cs.CY]AI Deception: A Survey of Examples, Risks, and Potential Solutions. 2023</p>
<p>Is temperature the creativity parameter of large language models. Max Peeperkorn, arXiv:2405.004922024arXiv preprint</p>
<p>The impact of ai on developer productivity: Evidence from github copilot. Sida Peng, arXiv:2302.065902023arXiv preprint</p>
<p>Forecasting: Theory and Practice. Fotios Petropoulos, International Journal of Forecasting. 382022</p>
<p>ChatGPT applications in medical, dental, pharmacy, and public health education: A descriptive study highlighting the advantages and limitations. Malik Sallam, Narra J 3. 12023</p>
<p>Superforecasting: How to upgrade your company's judgment. Paul Jh Schoemaker, Philip E Tetlock, Harvard Business Review. 942016</p>
<p>Large Language Model Prediction Capabilities. Philipp Schoenegger, Peter S Park, arXiv:2310.13014[cs.CY]Evidence from a Real-World Forecasting Tournament. 2023</p>
<p>Can AI Understand Human Personality?-Comparing Human Experts and AI Systems at Predicting Personality Correlations. Philipp Schoenegger, arXivpreprintarXiv:2406.081702024a</p>
<p>Wisdom of the silicon crowd: Llm ensemble prediction capabilities match human crowd accuracy. Philipp Schoenegger, arXivpreprintarXiv:2402.193792024b</p>
<p>SlimPajama-DC: Understanding Data Combinations for LLM Training. Zhiqiang Shen, arXiv:2309.108182023arXiv preprint</p>
<p>Irrational exuberance. Robert J Shiller, Irrational exuberance. Princeton university press2015</p>
<p>Utilizing Machine Learning Algorithms Trained on AIgenerated Synthetic Participant Recent Music-Listening Activity in Predicting Big Five Personality Traits. Siddharth Solaiyappan, 2023</p>
<p>Three challenges for AI-assisted decision-making. Mark Steyvers, Aakriti Kumar, Perspectives on Psychological Science. 174569162311811022023</p>
<p>Larry Summers on who could be replaced by AI. Lawrence H Summers, Steve Rattner, 2023Interviewed by Bloomberg TV's David Westin</p>
<p>AI succession [Youtube video of talk. Rich Sutton, 2023</p>
<p>World Artificial Intelligence Conference in Shanghai. </p>
<p>Superforecasting: The Art and Science of Prediction. Philip E Tetlock, Dan Gardner, 2016Random House</p>
<p>Bringing Probability Judgments into Policy Debates via Forecasting Tournaments. Philip E Tetlock, Barbara A Mellers, Scoblic Peter, Science. 3552017</p>
<p>Forecasting Tournaments: Tools for Increasing Transparency and Improving the Quality of Debate. Philip E Tetlock, Current Directions in Psychological Science. 232014</p>
<p>Attention is All You Need. Ashish Vaswani, Advances in Neural Information Processing Systems. 201730</p>
<p>Leaderboard Comparing LLM Performance at Producing Hallucinations when Summarizing Short Documents. Vectara, 2024</p>
<p>Chatgpt for robotics: Design principles and model abilities. Sai Vemprala, Microsoft Auton. Syst. Robot. Res. 2202023</p>
<p>Task supportive and personalized human-large language model interaction: A user study. Ben Wang, 10.1145/3627508.3638344Proceedings of the 2024 Conference on Human Information Interaction and Retrieval. the 2024 Conference on Human Information Interaction and Retrieval2024a</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Yubo Wang, arXiv:2406.015742024barXiv preprint</p>
<p>Emergent abilities of large language models. Jason Wei, arXiv:2206.076822022arXiv preprint</p>
<p>Can ChatGPT Pass High School Exams on English Language Comprehension?. Joost C F Winter, De, International Journal of Artificial Intelligence in Education. 1560- 42922023</p>
<p>Incentive-compatible forecasting competitions. Jens Witkowski, Management Science. 6932023</p>
<p>The rise and potential of large language model based agents: A survey. Zhiheng Xi, arXiv:2309.078642023arXiv preprint</p>
<p>From Automation to Augmentation: Large Language Models Elevating Essay Scoring Landscape. Changrong Xiao, arXiv:2401.064312024arXiv preprint</p>
<p>Benfeng Xu, arXiv:2305.14688[cs.CL]ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. 2023</p>
<p>Human-AI Interaction in the Age of Large Language Models. Diyi Yang, Proceedings of the AAAI Symposium Series. the AAAI Symposium Series20243</p>
<p>Fine-tuning Language Models from Human Preferences. Daniel M Ziegler, arXiv:1909.085932019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
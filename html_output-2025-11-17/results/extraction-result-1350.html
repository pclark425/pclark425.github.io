<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1350 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1350</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1350</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-0fd47bf484a05001ce787747cf5a879b9202ebfa</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0fd47bf484a05001ce787747cf5a879b9202ebfa" target="_blank">ViNG: Learning Open-World Navigation with Visual Goals</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Robotics and Automation</p>
                <p><strong>Paper TL;DR:</strong> Three key insights, waypoint proposal, graph pruning and negative mining, enable the ViNG method to learn to navigate in real-world environments using only offline data, a setting where prior methods struggle.</p>
                <p><strong>Paper Abstract:</strong> We propose a learning-based navigation system for reaching visually indicated goals and demonstrate this system on a real mobile robot platform. Learning provides an appealing alternative to conventional methods for robotic navigation: instead of reasoning about environments in terms of geometry and maps, learning can enable a robot to learn about navigational affordances, understand what types of obstacles are traversable (e.g., tall grass) or not (e.g., walls), and generalize over patterns in the environment. However, unlike conventional planning algorithms, it is harder to change the goal for a learned policy during deployment. We propose a method for learning to navigate towards a goal image of the desired destination. By combining a learned policy with a topological graph constructed out of previously observed data, our system can determine how to reach this visually indicated goal even in the presence of variable appearance and lighting. Three key insights, waypoint proposal, graph pruning and negative mining, enable our method to learn to navigate in real-world environments using only offline data, a setting where prior methods struggle. We instantiate our method on a real outdoor ground robot and show that our system, which we call ViNG, outperforms previously-proposed methods for goal-conditioned reinforcement learning, including other methods that incorporate reinforcement learning and search. We also study how ViNG generalizes to unseen environments and evaluate its ability to adapt to such an environment with growing experience. Finally, we demonstrate ViNG on a number of real-world applications, such as last-mile delivery and warehouse inspection. We encourage the reader to visit the project website for videos of our experiments and demonstrations 1.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1350.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1350.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ViNG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual Navigation with Goals</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learning-based navigation system that builds a non-metric topological graph of past egocentric images using a learned traversability (dynamical distance) function and plans with Dijkstra; a learned relative-pose predictor + PD controller executes waypoint-based actions. Trained from offline real-world data with key techniques: negative sampling and graph pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Real-world outdoor urban / off-road environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Outdoor navigation domain (urban, grassy, rocky terrain) using a forward-facing camera on a ground robot; goals specified by goal images (egocentric). Data consists of previously collected dense image trajectories (40 hours of prior data for main experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Initially dense fully-connected (edges added with learned distances) but explicitly sparsified via pruning: edges with traversability < delta_sparsify are removed; graphs can have disconnected components (observed in SoRB baseline) when learned functions fail, and pruning yields a sparse tractable graph.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Not reported numerically; constructed from hours of image trajectories (40 hours of data) — node and edge counts not given (graph grows quadratically in naive construction prior to pruning).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ViNG (topological planning + waypoint controller)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Builds a directed topological graph whose nodes are egocentric image observations and whose edge weights are predicted dynamical distances (traversability) via a learned function T(o_i,o_j); planner uses weighted Dijkstra to choose next waypoint; controller uses a learned relative-pose predictor P(o_current,o_waypoint) and a PD controller to execute continuous linear/angular velocities. Trained offline with supervised or TD distance learning, augmented with negative sampling; graph pruning removes trivially-traversable edges.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Primary metric: average portion of the expert trajectory to goal completed (interpreted as a success/completion fraction); also success rate at different goal distances (success@d).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>ViNG achieves 86% success on the most challenging real-world tasks (reported aggregate); Table II reports success rates by distance for the 'Waypoint' controller: 10m: 100%, 20m: 100%, 30m: 95%, 40m: 88%, 50m: 81%.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>86% (aggregate reported for challenging tasks); per-distance (Waypoint controller) shown in Table II: 10m 1.00, 20m 1.00, 30m 0.95, 40m 0.88, 50m 0.81.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical planning-based policy: graph-search over learned topological graph + mid-level waypoint actions executed by a learned relative-pose predictor and reactive PD controller; this outperforms purely reactive image-conditioned policies on long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Paper reports that graph construction and topology quality strongly affect navigation performance: (1) Negative sampling (adding cross-trajectory negatives labeled d_max) improves the learned traversability function's robustness to distribution shift, improving graph connectivity and planning success; (2) Graph pruning (removing edges below a traversability threshold) reduces redundancy and is essential for scaling—ablation shows pruning improves long-horizon success relative to the unpruned graph; (3) Poorly-trained distance/Q-functions can produce many disconnected components (observed in off-SoRB/SoRB baseline visualizations), preventing planning to goal and reducing success; (4) Mid-level waypoint representation combined with graph planning yields much higher success at long distances than direct reactive policies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Ablations compare full ViNG (with pruning and negative sampling) to variants: 'Waypoint, No Pruning' shows degraded long-distance success (e.g., 50m success 0.52 vs 0.81 for pruned); 'Waypoint, Only Positives' (no negative sampling) shows worse long-distance performance (50m success 0.43). Visualization of SoRB's graph shows many disconnected components, correlating with failure to find a path. Thus, pruning and negative sampling materially improve effective connectivity and long-horizon planning success.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that use a hierarchical structure (planner over topological graph + waypoint-based controller) outperform reactive single-image policies on long-horizon tasks; waypoint mid-level actions are crucial for distant goals, and planning/search over the learned graph (Dijkstra) is necessary for long-range navigation. Additionally, robust traversability learning (via negative sampling or TD learning) improves graph connectivity and thereby policy success; failure modes include disconnected graph components requiring improved training (e.g., distributional negatives) or more data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ViNG: Learning Open-World Navigation with Visual Goals', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Search on the Replay Buffer: Bridging Planning and RL <em>(Rating: 2)</em></li>
                <li>Semi-Parametric Topological Memory for Navigation <em>(Rating: 2)</em></li>
                <li>PRM-RL: Long-Range Robotic Navigation Tasks by Combining Reinforcement Learning and SamplingBased Planning <em>(Rating: 2)</em></li>
                <li>Scaling Local Control to Large-Scale Topological Navigation <em>(Rating: 2)</em></li>
                <li>BADGR: An Autonomous Self-Supervised Learning-Based Navigation System <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1350",
    "paper_id": "paper-0fd47bf484a05001ce787747cf5a879b9202ebfa",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "ViNG",
            "name_full": "Visual Navigation with Goals",
            "brief_description": "A learning-based navigation system that builds a non-metric topological graph of past egocentric images using a learned traversability (dynamical distance) function and plans with Dijkstra; a learned relative-pose predictor + PD controller executes waypoint-based actions. Trained from offline real-world data with key techniques: negative sampling and graph pruning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Real-world outdoor urban / off-road environment",
            "environment_description": "Outdoor navigation domain (urban, grassy, rocky terrain) using a forward-facing camera on a ground robot; goals specified by goal images (egocentric). Data consists of previously collected dense image trajectories (40 hours of prior data for main experiments).",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Initially dense fully-connected (edges added with learned distances) but explicitly sparsified via pruning: edges with traversability &lt; delta_sparsify are removed; graphs can have disconnected components (observed in SoRB baseline) when learned functions fail, and pruning yields a sparse tractable graph.",
            "environment_size": "Not reported numerically; constructed from hours of image trajectories (40 hours of data) — node and edge counts not given (graph grows quadratically in naive construction prior to pruning).",
            "agent_name": "ViNG (topological planning + waypoint controller)",
            "agent_description": "Builds a directed topological graph whose nodes are egocentric image observations and whose edge weights are predicted dynamical distances (traversability) via a learned function T(o_i,o_j); planner uses weighted Dijkstra to choose next waypoint; controller uses a learned relative-pose predictor P(o_current,o_waypoint) and a PD controller to execute continuous linear/angular velocities. Trained offline with supervised or TD distance learning, augmented with negative sampling; graph pruning removes trivially-traversable edges.",
            "exploration_efficiency_metric": "Primary metric: average portion of the expert trajectory to goal completed (interpreted as a success/completion fraction); also success rate at different goal distances (success@d).",
            "exploration_efficiency_value": "ViNG achieves 86% success on the most challenging real-world tasks (reported aggregate); Table II reports success rates by distance for the 'Waypoint' controller: 10m: 100%, 20m: 100%, 30m: 95%, 40m: 88%, 50m: 81%.",
            "success_rate": "86% (aggregate reported for challenging tasks); per-distance (Waypoint controller) shown in Table II: 10m 1.00, 20m 1.00, 30m 0.95, 40m 0.88, 50m 0.81.",
            "optimal_policy_type": "Hierarchical planning-based policy: graph-search over learned topological graph + mid-level waypoint actions executed by a learned relative-pose predictor and reactive PD controller; this outperforms purely reactive image-conditioned policies on long-horizon tasks.",
            "topology_performance_relationship": "Paper reports that graph construction and topology quality strongly affect navigation performance: (1) Negative sampling (adding cross-trajectory negatives labeled d_max) improves the learned traversability function's robustness to distribution shift, improving graph connectivity and planning success; (2) Graph pruning (removing edges below a traversability threshold) reduces redundancy and is essential for scaling—ablation shows pruning improves long-horizon success relative to the unpruned graph; (3) Poorly-trained distance/Q-functions can produce many disconnected components (observed in off-SoRB/SoRB baseline visualizations), preventing planning to goal and reducing success; (4) Mid-level waypoint representation combined with graph planning yields much higher success at long distances than direct reactive policies.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Ablations compare full ViNG (with pruning and negative sampling) to variants: 'Waypoint, No Pruning' shows degraded long-distance success (e.g., 50m success 0.52 vs 0.81 for pruned); 'Waypoint, Only Positives' (no negative sampling) shows worse long-distance performance (50m success 0.43). Visualization of SoRB's graph shows many disconnected components, correlating with failure to find a path. Thus, pruning and negative sampling materially improve effective connectivity and long-horizon planning success.",
            "policy_structure_findings": "Policies that use a hierarchical structure (planner over topological graph + waypoint-based controller) outperform reactive single-image policies on long-horizon tasks; waypoint mid-level actions are crucial for distant goals, and planning/search over the learned graph (Dijkstra) is necessary for long-range navigation. Additionally, robust traversability learning (via negative sampling or TD learning) improves graph connectivity and thereby policy success; failure modes include disconnected graph components requiring improved training (e.g., distributional negatives) or more data.",
            "uuid": "e1350.0",
            "source_info": {
                "paper_title": "ViNG: Learning Open-World Navigation with Visual Goals",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Search on the Replay Buffer: Bridging Planning and RL",
            "rating": 2
        },
        {
            "paper_title": "Semi-Parametric Topological Memory for Navigation",
            "rating": 2
        },
        {
            "paper_title": "PRM-RL: Long-Range Robotic Navigation Tasks by Combining Reinforcement Learning and SamplingBased Planning",
            "rating": 2
        },
        {
            "paper_title": "Scaling Local Control to Large-Scale Topological Navigation",
            "rating": 2
        },
        {
            "paper_title": "BADGR: An Autonomous Self-Supervised Learning-Based Navigation System",
            "rating": 2
        }
    ],
    "cost": 0.009204249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ViNG: Learning Open-World Navigation with Visual Goals</h1>
<p>Dhruv Shah ${ }^{1}$, Benjamin Eysenbach ${ }^{2}$, Gregory Kahn ${ }^{1}$, Nicholas Rhinehart ${ }^{1}$, Sergey Levine ${ }^{1}$<br>${ }^{1}$ UC Berkeley, ${ }^{2}$ Carnegie Mellon University</p>
<h4>Abstract</h4>
<p>We propose a learning-based navigation system for reaching visually indicated goals and demonstrate this system on a real mobile robot platform. Learning provides an appealing alternative to conventional methods for robotic navigation: instead of reasoning about environments in terms of geometry and maps, learning can enable a robot to learn about navigational affordances, understand what types of obstacles are traversable (e.g., tall grass) or not (e.g., walls), and generalize over patterns in the environment. However, unlike conventional planning algorithms, it is harder to change the goal for a learned policy during deployment. We propose a method for learning to navigate towards a goal image of the desired destination. By combining a learned policy with a topological graph constructed out of previously observed data, our system can determine how to reach this visually indicated goal even in the presence of variable appearance and lighting. Three key insights, waypoint proposal, graph pruning and negative mining, enable our method to learn to navigate in real-world environments using only offline data, a setting where prior methods struggle. We instantiate our method on a real outdoor ground robot and show that our system, which we call ViNG, outperforms previously-proposed methods for goalconditioned reinforcement learning, including other methods that incorporate reinforcement learning and search. We also study how ViNG generalizes to unseen environments and evaluate its ability to adapt to such an environment with growing experience. Finally, we demonstrate ViNG on a number of real-world applications, such as last-mile delivery and warehouse inspection. We encourage the reader to visit the project website for videos of our experiments and demonstrations ${ }^{1}$.</p>
<h2>I. INTRODUCTION</h2>
<p>Visual navigation in complex environments poses several challenges: (i) difficulty in faithfully modeling the complex dynamics and nuanced environmental interactions; (ii) reacting to high-dimensional observations; (iii) cost and safety constraints on collecting data, requiring learning from previously collected (i.e., "offline") experience; and (iv) generalizing effectively across different settings and environments. Planning algorithms achieve many of these desiderata, but their efficacy depends on having the right representation of the task; it remains unclear how to apply many planning algorithms to tasks with image-based observations. On the other hand, humans seemingly have little difficulty navigating complex environments from first-person observations, without GPS or maps, if they have seen the environment before. Humans and animals are known to use "mental maps" that rely on landmarks and other cues [1-3], and rely heavily on learning. Further, in the absence of spatial positional information (e.g., GPS or maps), specification of a navigational goal itself becomes challenging, since locational goals require the robot to be able to compare its location to the target.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: ViNG builds and plans over a learned topological graph consisting of previously seen egocentric images, and uses a learned controller to execute the path to a visually indicated goal. Unlike prior work, our method uses purely offline experience and does not require a simulator or online data collection. Note that the graph constructed by our algorithm is not geometric and nodes are not associated with coordinates in the world, but only with image observations - the top-down satellite image is provided only for visualization and is not available to our method.</p>
<p>In this paper, we study learning-based methods for navigation that can similarly utilize graph-structured "mental maps" that are non-geometric in nature, and can enable a robot to navigate in the real-world. We use a natural and intuitive mechanism for specifying goals - where the user provides the robot with a picture of the desired destination. Inspired by humans navigating toward previously seen landmarks, our goal is to enable the robot to navigate to a visually indicated goal. Crucially, such a goal specification scheme does not presume any prior geometric knowledge of the scene, while still providing enough information for the robot to perform the task. Fig. 1 shows an example of such a task.</p>
<p>Towards satisfying these requirements, we present a fully autonomous, self-supervised mobile robot platform for visual goal-reaching in outdoor, unstructured environments which we call ViNG - visual navigation with goals. Our approach combines the strengths of dynamical distance learning and graph search. We first learn a function that predicts the dynamical distance between pairs of observations, estimating how many time steps are needed to transition between them. We then use this learned dynamical distance to embed past observations into a topological graph, and plan over this graph. This process makes no geometric assumptions about the environment: reachability is determined entirely by learning from data. Unlike pure planning-based approaches, our method scales to high-dimensional observations and hard-to-model dynamics, and does not assume access to any ground-truth spatial information. Unlike pure learning-based approaches,</p>
<p>our method effectively learns from offline experience and reasons over long horizons. Unlike prior methods that combine planning and learning, ViNG learns from offline, real-world data, and does not require a simulator or online data collection.</p>
<p>The primary contribution of this work is a self-supervised robotic system, ViNG, that can efficiently learn goal-directed navigation behaviors in open-world environments without access to spatial maps from an offline pool of data, including randomly collected trajectories. Three key ideas, waypoint proposal, graph pruning and negative mining, differentiate our method from prior work and are critical to the success of our method in this offline setting. ViNG can learn to navigate to an arbitrary user-specified visual goal in a variety of open-world settings, including urban, grassy, and rocky terrain, learning only from offline data. Our experiments show that ViNG learns goal-conditioned behaviors that can effectively plan over long horizons. We show that ViNG outperforms several competitive offline RL and geometric baselines. Further, we show that the learned behaviors transfer to novel environments using as little as 20 minutes of data from the environment and that ViNG can adapt in such novel environments as it gathers more data, resulting in an autonomous, self-improving system. Lastly, we demonstrate two real-world applications enabled by ViNG in dense, urban neighborhoods – last-mile delivery of food or mail, and autonomous inspection of warehouses.</p>
<h2>II Related Work</h2>
<p>Prior work has studied vision-based mobile robot navigation in many real-world settings, including indoor and outdoor navigation <em>[4, 5, 6]</em>, autonomous driving <em>[7, 8]</em>, and navigation in extra-terrestrial and underwater environments <em>[9, 10]</em>. The combination of mapping <em>[11]</em> and path planning <em>[12]</em> has been a cornerstone for a number of effective systems <em>[13, 14, 15]</em> and underlies several state-of-the-art navigation systems <em>[16, 17]</em>. Many prior methods make restrictive assumptions, such as access to LIDAR or other structured sensor information and accurate localization, which can limit their suitability for deployment in unstructured environments <em>[18]</em>. Further, prior work often assumes that geometric traversability is faithfully indicated through observations and not misled by (say) non-obstacles such as tall grass <em>[19]</em>. Learning-based systems lift some of these assumptions and can use learned models to perform perception <em>[20, 21]</em>, planning <em>[22, 23, 24]</em>, or both <em>[25]</em>. In practice, learning temporally extended long-horizon skills with either reinforcement learning (RL) or imitation learning (IL) remains difficult <em>[26, 27]</em>.</p>
<p>Recent methods address limitations of the above approaches by combining planning and learning <em>[28, 29, 30, 31, 32, 33]</em>. These methods use learning (i.e., approximate dynamic programming) to solve short-horizon tasks and plan (i.e., use exact dynamic programming) over non-metric topological graphs <em>[34, 35]</em> to reason over longer horizons. This general approach simultaneously avoids the need for (1) high-fidelity map building and (2) learning temporally-extended behaviors from scratch. However, prior instantiations of this recipe make assumptions that limit their applicability to real-world settings: assuming access to an exact simulation replica of the environment <em>[36, 33]</em>, assuming simplified action spaces <em>[28, 29, 30]</em>, or requiring online data collection <em>[28, 30]</em>. Our experiments in Section V demonstrate that prior methods fail when they are not allowed to collect new experience in a simulator or the real-world.</p>
<p>Our method, ViNG, builds on these prior approaches by adding two key ideas: graph pruning and negative sampling. These additional ingredients allow ViNG to lift assumptions made by prior methods: it does not assume access to a simulator, and does not require interactive access to an environment; it is trained using offline, real-world data; and it operates directly on high-dimensional images and predicts continuous actions for the robot. To the best of our knowledge, ViNG is the first system demonstrated on a real-world ground robot that can learn from offline data to reach visually indicated navigational goals over long time horizons without simulated training or hand-designed localization and mapping systems.</p>
<h2>III Problem Statement and System Overview</h2>
<p>We consider the problem of goal-directed visual navigation: a robot is tasked with navigating to a goal location $G$ given an image observation $o_{G}$ taken at $G$. In addition to navigating to the goal, the robot also needs to recognize when it has reached the goal, signaling that the task has been completed. The robot does not have a spatial map of the environment, but we assume that it has access to a small number of trajectories that it has collected previously. This data will be used to construct a graph over the environment using a learned distance and reachability function. We make no assumptions on the nature of the trajectories: they may be obtained by human teleoperation, self-exploration, or a result of a random walk. Each trajectory is a dense sequence of observations $o_{1},o_{2},\ldots,o_{n}$ recorded by its on-board camera. Since the robot only observes the world from a single on-board camera and does not run any state estimation, our system operates in a partially observed setting. Our system commands continuous linear and angular velocities.</p>
<h3>III-A Mobile Robot Platform</h3>
<p>We implement ViNG on a Clearpath Jackal UGV platform – a small, fast, weatherproof outdoor ground robot ideal for navigating in both urban and off-road environments (see Fig. 1 and 2). The default sensor suite consists of a 6-DoF IMU, a</p>
<p>GPS unit for approximate global position estimates, and wheel encoders to estimate local odometry. In addition, we added a forward-facing $170^{\circ}$ field-of-view camera and an RPLIDAR 2D laser scanner. Inside the Jackal is an NVIDIA Jetson TX2 computer. While the robot carries a GPS and laser scanner, we use these sensors solely as a safety mechanism during data collection. Our method solely operates using images taken from the onboard camera.</p>
<h2>B. Data Collection \&amp; Labeling</h2>
<p>ViNG can learn navigational behaviors from previouslycollected, off-policy data - a desideratum of real-world robots. To demonstrate this capability, we run our core experiments using data exclusively from prior work [37]; we also collect a limited amount of additional data for our environment generalization experiments using the same self-supervised data collection strategy. The prior data was collected more than 10 months prior to the experiments in this paper (see Fig. 2 (top)), and exhibits significant differences in appearance, lighting, time of year, and time of day as compared to the evaluation setting. This underscores the ability of ViNG to utilize offline data from diverse sources.</p>
<h2>IV. VisuAl Navigation With Goals</h2>
<p>We approach the problem of visual goal-conditioned navigation by combining non-metric maps and learned, imagebased, goal-conditioned policies. We describe our method in two stages: (i) training two learned functions and (ii) deploying the system, which entails using the learned functions together with past experience to execute goal-directed behavior.</p>
<p>During training, we use previously collected experience to learn an environment-independent traversability function $\mathcal{T}$, as well as a relative pose predictor, $\mathcal{P}$. During deployment, the robot builds a topological graph of its environment: a directed graph with vertices as observations and edges encoding traversability and proximity. At each time step $t$, the robot localizes its current and goal observations $\left(o_{t}, o_{G}\right)$ in the graph and follows the best path to $G$, as determined by a graph search algorithm that outputs the next waypoint for the controller. To close the loop, we need a goal-conditioned controller that takes the current and goal observations, and outputs an action $a$. The controller progressively follows the path directed by the planner until it reaches $G$.</p>
<p>While the general recipe of ViNG is similar to prior work [28, 29, 33], our experiments demonstrate that two key technical insights contribute to significantly improved performance in the real-world setting: graph pruning (Sec. IV-B2) and negative mining (Sec. IV-A1). Our comparisons to prior methods in Section V and ablation studies in Section V-D demonstrate these novel improvements enable ViNG to learn goal-conditioned policies entirely from offline data, avoiding the need for simulators and online sampling, while prior methods struggle to attain good performance, particularly for long-horizon goals.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="w"> </span><span class="nt">Training</span><span class="w"> </span><span class="nt">ViNG</span>
<span class="w">    </span><span class="nt">Input</span><span class="w"> </span><span class="nt">transitions</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">r^{(k)</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">o_</span><span class="p">{</span><span class="err">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(k)</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">a_</span><span class="p">{</span><span class="err">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(k)</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">o_</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(k)</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">a_</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(k)</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">cdots</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="err">\}</span><span class="nt">_</span><span class="p">{</span><span class="err">k=1,</span><span class="w"> </span><span class="err">\cdots</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbb</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">+</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">\left(o_{i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(k)</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">o_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(k)</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">d</span><span class="o">=</span><span class="err">\</span><span class="nt">min</span><span class="w"> </span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">j-i</span><span class="o">,</span><span class="w"> </span><span class="nt">d_</span><span class="p">{</span><span class="err">\max</span><span class="w"> </span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="err">\}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="w"> </span><span class="err">\leq</span><span class="w"> </span><span class="err">j,</span><span class="w"> </span><span class="err">k=1,</span><span class="w"> </span><span class="err">\cdots</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbb</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">-</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">\left(o_{i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(k)</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">o_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">d</span><span class="o">=</span><span class="nt">d_</span><span class="p">{</span><span class="err">\max</span><span class="w"> </span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="err">\}</span><span class="nt">_</span><span class="p">{</span><span class="err">i,</span><span class="w"> </span><span class="err">j,</span><span class="w"> </span><span class="err">k</span><span class="w"> </span><span class="err">\neq</span><span class="w"> </span><span class="err">\ell</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Initialize</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">o_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">o_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">P</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">o_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">o_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">while</span><span class="w"> </span><span class="nt">not</span><span class="w"> </span><span class="nt">converged</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">B</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">+</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbb</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">+</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">B</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">-</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbb</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">-</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Sample</span><span class="w"> </span><span class="nt">batch</span><span class="o">.</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">UpdateDistanceFn</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">B</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">+</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cup</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">B</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">-</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">get</span><span class="w"> </span><span class="nt">relative</span><span class="w"> </span><span class="nt">pose</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbb</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">+</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">\left(\left(o_{i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(k)</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">o_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(k)</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">d_</span><span class="p">{</span><span class="err">i</span><span class="w"> </span><span class="err">j</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">p_</span><span class="p">{</span><span class="err">i</span><span class="w"> </span><span class="err">j</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\}\</span><span class="nt">right</span><span class="o">.</span><span class="err">\</span><span class="nt">right</span><span class="o">.</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">P</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">UpdateRelativePoseFn</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">P</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">B</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">+</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cup</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">B</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">-</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">end</span><span class="w"> </span><span class="nt">while</span>
<span class="w">    </span><span class="nt">return</span><span class="w"> </span><span class="nt">traversability</span><span class="w"> </span><span class="nt">function</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">relative</span><span class="w"> </span><span class="nt">pose</span><span class="w"> </span><span class="nt">function</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">P</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
</code></pre></div>

<h2>A. Learning Dynamical Distances</h2>
<p>We aim to learn a traversability function $\mathcal{T}\left(o_{i}, o_{j}\right) \in \mathbb{R}^{+}$ that reflects whether any controller can successfully navigate between observations $o_{i}$ and $o_{j}$. More precisely, we will learn to predict the estimated number of time steps required by a controller to navigate from one observation to another. This function must encapsulate knowledge of physics beyond just geometry. For example, tall grass and bushes might appear visually similar, but grass is compliant and traversable whereas bushes are not. We explored two methods for learning this traversability function: (1) supervised learning and (2) temporal difference learning [38, 39]. To learn the distance function via supervised learning, we create a dataset $\mathbb{D}<em i="i">{+}$of observation pairs $\left(o</em>=j-i$ elapsed between these observations. The distance predicted by this approach corresponds to the estimated number of time steps required by the behavior policy (that which collected the experience) when navigating between two observations. Thus, this approach is simple but may overestimate the true shortest path distances.}, o_{j}\right)$ taken from the same trajectory and regress to the number of timesteps $d_{i j</p>
<p>The second approach to learning the distance function is via temporal difference learning [39]. This approach uses the same experience as before. While this approach adds additional complexity, in theory it converges to the shortest path distance. In our experiments, we found little difference between these two approaches (see Table II), but expect that the temporal difference learning approach would be important when moving to settings where the shortest path distance is much shorter than a random walk distance.</p>
<p>1) Negative Mining (Key Idea 1): In our experiments, we found that training the distance function using only observation pairs from the same trajectory performed poorly. We hypothesize that the root cause was distribution shift: when building the topological graph we must evaluate the distance function on observation pairs collected from different trajectories, possible from different times of day. To mitigate this problem, we augment the dataset by adding a new dataset $\mathbb{D}<em _max="\max">{-}$ obtained by sampling observations from different trajectories, labeled as $d</em>$ in our experiments, offering significant}$. We find this augmentation, hereby referred to as negative sampling, to be critical in the successful training and evaluation of $\mathcal{T</p>
<p>Algorithm 2 Deploying ViNG
1: Input current image $o_{t}$, goal image $o_{G}$, and topological graph $\mathcal{M}$.
2: Add $o_{t}, o_{G}$ to the map $\mathcal{M}$ using distances from $\mathcal{T}$.
3: $o_{w_{1}}, o_{w_{2}}, \cdots \leftarrow \operatorname{Dijkstra}\left(\operatorname{start}=o_{t}, \operatorname{goal}=o_{G}, \mathcal{M}\right)$
4: Estimate relative pose of first waypoint: $\Delta p \leftarrow \mathcal{P}\left(o_{t}, o_{w_{1}}\right)$
5: $u_{t} \leftarrow \operatorname{PD}-\operatorname{CONTROLLER}(\Delta p)$
6: return control $u_{t}$
improvements over prior methods.</p>
<h2>B. The Topological Graph</h2>
<p>We build a topological graph $\mathcal{M}$ using the learned distance function together with a collection of previously-observed observations $\left{o_{t}\right}$. Each node in the graph corresponds to one of these observations. We add weighted edges between every node, using weights predicted by the distance function $\mathcal{T}$.</p>
<p>1) Graph Pruning (Key Idea 2): As the robot gathers more experience, maintaining a dense graph of traversability across all observation nodes becomes redundant and infeasible, as the graph size grows quadratically. For our experiments, we sparsify trajectories by thresholding the edges that get added to the graph: edges that are easily traversable $\left(\mathcal{T}\left(o_{i}, o_{j}\right)&lt;\delta_{\text {sparsify }}\right)$ are not added to the graph, since the controller can traverse those edges with high probability.
2) Planning with the Graph: We localize the current observation $o_{t}$ and goal observation $o_{G}$ in the graph, adding direct edges (weighed by their traversability) to their corresponding "most-traversable" neighbors. We use the weighted Dijkstra algorithm to compute the shortest path to goal, and the immediate next node in the planned path is then handed over to the controller.</p>
<h2>C. Designing the Controller</h2>
<p>After the planner predicts a waypoint observation, the controller must output an action that takes the agent towards that waypoint. The main challenge in navigating to this waypoint is that both the current state and waypoint are represented as high-dimensional observations (e.g., images). To address this challenge, we learn a relative position predictor $\mathcal{P}$ that takes as input two observations and predicts the relative pose between these observations. We learn this relative pose predictor via supervised learning: for pairs of observations $\left(o_{i}, o_{j}\right)$ that occur nearby within the collected trajectories, we estimate the relative pose $\Delta p_{i j}$ using onboard odometry and use this relative pose as the label for learning.</p>
<p>The complete controller works as follows. Given the current observation and waypoint observation, we use the relative pose predictor to estimate the relative pose of the waypoint relative to the robot's current position. The robot then uses odometry and a simple PD controller to steer toward this waypoint. We compare against alternative controllers in Section V-D.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3: Real-World Navigation: While all non-random methods successfully reach nearby goals, only ViNG reaches goals over 40 meters away. Here, success rate is defined as the average over portion of the expert trajectory to goal that each run successfully completes.</p>
<h2>D. Implementation Details</h2>
<p>Inputs to the traversability function $\mathcal{T}$ and relative pose predictor $\mathcal{P}$ are pairs of observations of the environment, represented by a stack of two consecutive RGB images obtained from the onboard camera at a resolution of $160 \times 120$ pixels. $\mathcal{T}$ comprises a MobileNet encoder [40] followed by three densely connected layers to project the 1024-dimensional latents to 50 class labels. $\mathcal{P}$ has a similar architecture as $\mathcal{T}$, comprising of a MobileNet encoder followed by three densely connected layers projecting the 1024-dimensional latents to 3 outputs for waypoints: ${\Delta x, \Delta y}$. Both $\mathcal{T}$ and $\mathcal{P}$ use the same encoder.</p>
<p>We train the traversability function on $\mathbb{D}<em -="-">{+} \cup \mathbb{D}</em>}$, discretizing the timesteps $d_{i j}$ into bins $\left{1, \cdots, d_{\max }=50\right}$ and minimizing the cross entropy loss. The relative pose predictor $\mathcal{P}$ is trained on $\mathbb{D<em 2="2">{+}$ to minimize the $\ell</em>$. Algorithms 1 and 2 summarize our approach in the training and deployment stages, respectively.}$ regression loss. We use a batch size of 128 and perform gradient updates using the Adam optimizer [41] with learning rate $\lambda=10^{-4</p>
<h2>V. EXPERIMENTS</h2>
<p>We designed our experiments to answer three questions:
Q1. How does ViNG compare to prior methods for the task of goal-conditioned visual navigation from offline data?
Q2. Does ViNG generalize to novel environments? Can it adapt on the fly?
Q3. What are the alternate design choices for the controller and how do they compare against our choice in Section IV-C?</p>
<h2>A. Goal-Conditioned Visual Navigation from Offline Data</h2>
<p>We perform our evaluation in a real-world outdoor environment consisting of urban and off-road terrain. We train on 40 hours of data that was gathered in prior work [37] over 10 months prior to the experiments in this paper. The data shows significant variation in appearance due to seasonal changes (see Fig. 2); learning navigational affordances and traversability would require the algorithms to discard the irrelevant modes of variance (e.g., appearance) and establish correspondence across seasons and times of day.</p>
<p>Since this evaluation takes place in the real world, we do not have the luxury of training online RL policies or transfer from simulation. We evaluate ViNG against four baselines:</p>
<ul>
<li>SPTM: a dense topological graph combined with a controller that maps observation pairs to motor commands, trained via supervised learning [29]</li>
<li>off-SoRB: an offline variant of SoRB that uses a topological graph and offline RL to learn a distributional Q-function [28]</li>
<li>State Estimator: a naïve baseline that uses a state estimator network that regresses observations to ground-truth state $(x, y, \theta)$, followed by a position controller; note that this baseline has access to true position (from GPS), which is not available to our method</li>
<li>Random: a random walk, as described in Section IV-C</li>
</ul>
<p>While there have been other successful instantiations of methods combining planning and learning, they make some limiting assumptions that make them difficult to apply to our problem setting. LSTN [33] uses a photorealistic simulator to train its distance and action models, using ~ 1.5M samples, while PRM-RL [36] uses a 3D kinematic simulator simulation replica to train a reactive controller, coupled with physical rollouts in the real world to build a PRM. ViNG does not assume access to any simulator, and learns directly from offline real data.</p>
<p>Towards answering Q1, we evaluate the goal-reaching performance of ViNG. We select 6 {start, goal} image pairs in the original urban environment and compare the goal reaching performance of each method (avg. of 3 trials). We report the success metric as the average over portion of the expert trajectory to goal that each run successfully completes.</p>
<p>As shown in Fig. 3, ViNG performs well on all tasks, achieving a success rate of 86% on even the most challenging tasks. As expected, the random baseline, which ignores the goal, fails to reach most goals. The state estimator baseline performs a bit better, but struggles to reach more distant goals because it is not reactive, and hence cannot take actions to avoid collisions. Off-SoRB performs well on nearby goals, but as the goals get increasingly difficult to reach, it is unable to follow the planned trajectory. Visualizing the topological graph built by SoRB uncovers many disconnected components, resulting in no path to goal. We hypothesize that this is attributed to the difficulty in training Q-functions from offline data. SPTM, which uses supervised learning instead of Q-learning, is effective at solving the task on shorter horizons and outperforms off-SoRB on longer horizons. However, ViNG still performs substantially better on all goal distances, especially those over 30 meters. We attribute these improvements to the additional negative sampling and graph pruning techniques discussed in Section IV. We visualize trajectories in Fig. 4.</p>
<h3>V-B Generalization and Adaptation</h3>
<p>The experiments in the previous section evaluate navigation to new goals in a previously seen environment. In this section, we additionally evaluate how quickly ViNG can adapt to an entirely new, unseen environment, by constructing a new graph and finetuning the models. We use the four settings shown in Fig. 5, all of which are distinct from the setting used in our main experiments (Sec. V-A). In each new environment, a human controlled the robot to provide initial exploration data. After this initial data collection, the robot collected experience autonomously: it randomly sampled a previously-observed image as the goal and used ViNG to attempt to reach this goal. After each episode, we used all experience from the new environment (both the expert trajectories and the self-collected trajectories) to finetune $\mathcal{T}$ and $\mathcal{P}$. We refer to this approach to generalization as ViNG-Finetune.</p>
<p>In Fig. 5 we visualize trajectories after 60 min of data collection in the new environment and observe that the robot successfully reaches the goal in most cases. We emphasize that these environments are considerably different from those used in Sec. V-A, on which our models were initially trained. To illustrate the learning dynamics in this generalization setting, we plot self-collected rollouts after 0 minutes, 20 minutes, and 60 minutes of practice in the new environments. As shown in Fig. 6, the robot's performance in the new domain gets progressively better with more (autonomous) practice; after 60 minutes it succeeds in reaching the goal in all three attempts.</p>
<p>Table I summarizes the success rate on the generalization task of our method and two alternative versions of ViNG. ViNG-Source directly uses the traversability function and relative pose function trained in the source domain (Sec. V-A), without incorporating any experience from the new environment. In contrast ViNG-Target learns these same models using only experience from the new "target" domain, without leveraging any of the previously-collected experience. ViNGFinetune outperforms these baselines, highlighting the importance of combining old and new experience. As an additional baseline, we take the SPTM model from Sec. V-A and finetune it on experience from the new domain. We observe that ViNG-Finetune also generalizes better than SPTM-Finetune, We hypothesize that ViNG generalizes better than SPTM because of the additional hierarchical structure of ViNG.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p><strong>Fig. 4: Qualitative Results in the urban Environment</strong>: Each approach was directed to a visual goal ~ 50m away (marked by checkerboard circle) – with 3 runs per approach. ViNG is the only approach that is consistently able to reach the goal while avoiding collisions or getting stuck.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5: Generalization Experiments: We evaluate ViNG in four new outdoor environments. For each, we collect a few dozen minutes of experience to adapt the distance function and relative pose predictor. Then, given a goal image (last column, checkerboard location in aerial view), the robot attempts to navigate to the goal. Columns 4 – 7 indicate that the robot succeeds in reaching the goal image. Cyan lines indicate the actions taken by ViNG.</p>
<table>
<thead>
<tr>
<th>Environment</th>
<th>ViNG Source</th>
<th>ViNG Target</th>
<th>ViNG Finetune</th>
<th>SPTM Finetune</th>
</tr>
</thead>
<tbody>
<tr>
<td>barracks</td>
<td>0.27</td>
<td>0.42</td>
<td>0.96</td>
<td>0.74</td>
</tr>
<tr>
<td>industrial</td>
<td>0.13</td>
<td>0.44</td>
<td>0.84</td>
<td>0.68</td>
</tr>
<tr>
<td>park</td>
<td>0.04</td>
<td>0.32</td>
<td>0.82</td>
<td>0.71</td>
</tr>
<tr>
<td>tall grass</td>
<td>0</td>
<td>0.38</td>
<td>0.79</td>
<td>0.56</td>
</tr>
</tbody>
</table>
<p>TABLE I: Generalization Results: Our approach to generalization ("ViNG - Finetune") successfully navigates learns to navigate in four new environments (shown in Fig. 5) using just 60 minutes of experience in the new environment. Baselines that use only experience from the source or target domains are substantially less successful. Applying our finetuning approach on top of SPTM shows some generalization, but is outperformed by ViNG-Finetune.</p>
<h3>C. Comparisons to Online Methods</h3>
<p>While Section V-A establishes that ViNG outperforms competitive offline methods for the task of goal-conditioned navigation, here we also investigate the performance of our method in comparison to popular online RL algorithms. Since the sample complexity of online RL algorithms forbids us from testing this in the real world, we a Unity-based, photorealistic outdoor navigation simulator. We include new additional</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 6: Fast Adaptation to a New Environment: After training ViNG in one environment, we deploy the system in a novel environment, shown above. By practicing to reach self-proposed goals and using that experience to finetune the controller, ViNG is able to quickly gain competence at reaching distance goals in this new environment, using just 60 minutes of experience. Example rollouts towards a goal 35m away (marked by checkerboard circle) demonstrate ViNG self-improving from interactions in the barracks environment.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 7: Results from Simulated Navigation: ViNG is substantially more successful at reaching distance goals than all offline baselines, while performing competitively with SoRB, a popular online baseline combining Q-learning and topological graphs. We emphasize that SoRB and PPO require 5× online data collection, making them prohibitively expensive to apply in the real-world.</p>
<p>baselines in the simulated experiment:</p>
<ul>
<li><em>PPO</em>: a popular reactive controller for indoor visual navigation algorithms [42, 43]</li>
<li><em>SoRB</em>: online version of the "off-SoRB" baseline [28]</li>
</ul>
<p>We show results in Fig. 7. PPO performs poorly and is outperformed by ViNG, suggesting that a single image-based reactive policy is insufficient for solving long-horizon goal-reaching tasks, even when given access to 200 hours of online experience. SoRB outperforms other baselines and performs on par with ViNG. However, whereas ViNG requires 40 hours of offline data, SoRB requires 200 hours of online data, and must recollect this data for every experiment.</p>
<h3>D. Ablation Experiments</h3>
<p>A key design decision for ViNG that differentiates it from prior methods (e.g., [28, 33]) is how the controller generates actions to reach the next waypoint. We evaluate variants of ViNG that use alternative controllers and present results in Table II. Two simple baselines, "direct actions" and "direct actions (discrete)", use the goal-conditioned behavior cloning method of [29, 44] to directly predict (discrete) actions from the current and goal observations, without utilizing the topological map. Recall that our method uses the planner to command waypoints and then uses the relative pose together with a PD controller to reach each waypoint. We compared against a baseline that uses a different low-level controllers to reach these same waypoints: "Waypoint, Discrete" takes actions using the "direction actions (discrete)" controller described above. As an alternative training scheme, "TD Waypoint" is a variant of our method that learns the traversability function via TD learning instead of supervised learning. Finally, we compare to two ablations of our method that skip the graph pruning and negative sampling stages of ViNG.</p>
<h3>E. Applications and Qualitative Results</h3>
<p>ViNG's ability to navigate using perception and landmarks, without access to maps or localization, can enable a number of</p>
<table>
<thead>
<tr>
<th></th>
<th>Success Rate @ Distance $d$ (m)</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Controller</td>
<td>$d=10$</td>
<td>$d=20$</td>
<td>$d=30$</td>
<td>$d=40$</td>
<td>$d=50$</td>
</tr>
<tr>
<td>Direct Actions (Discrete)</td>
<td>0.87</td>
<td>0.81</td>
<td>0.74</td>
<td>0.65</td>
<td>0.45</td>
</tr>
<tr>
<td>Direct Actions</td>
<td>0.98</td>
<td>0.89</td>
<td>0.74</td>
<td>0.73</td>
<td>0.4</td>
</tr>
<tr>
<td>Waypoint, Discrete</td>
<td>1.0</td>
<td>0.95</td>
<td>0.91</td>
<td>0.82</td>
<td>0.7</td>
</tr>
<tr>
<td>Waypoint</td>
<td>1.0</td>
<td>1.0</td>
<td>0.95</td>
<td>0.88</td>
<td>0.81</td>
</tr>
<tr>
<td>TD Waypoint</td>
<td>1.0</td>
<td>1.0</td>
<td>0.96</td>
<td>0.87</td>
<td>0.87</td>
</tr>
<tr>
<td>Waypoint, No Pruning</td>
<td>1.0</td>
<td>0.88</td>
<td>0.81</td>
<td>0.79</td>
<td>0.52</td>
</tr>
<tr>
<td>Waypoint, Only Positives</td>
<td>1.0</td>
<td>0.91</td>
<td>0.75</td>
<td>0.76</td>
<td>0.43</td>
</tr>
</tbody>
</table>
<p>TABLE II: Ablation Experiments: We investigate design choices for the parametrization of the controller. Using waypoints as a mid-level action space is key to the performance of ViNG, which is particularly emphasized for distant goals. While training the models, we show that ViNG can be trained with either supervised or TD learning and report similar performance. We also show that the two key ideas presented - graph pruning and negative sampling - are indeed essential for the performance of ViNG in the real-world.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 8: Contactless Last-Mile Delivery Demo: Given a set of visuallyindicated goals (a), ViNG can perform contactless delivery in the urban neighborhood successfully, as shown in the filmstrip (c). An overhead view (b) with starting position marked in yellow and respective goals marked in orange and magenta shows the trajectory of the robot (cyan). Note: The satellite view (b) is solely for visualization and is not available to the robot.
intuitive applications, which we illustrate through qualitative results in this section. We constructed two demonstrations that reflect potential applications of our system:</p>
<p>1) Contactless Last-Mile Delivery: We demonstrate lastmile delivery in a residential complex by using ViNG to autonomously deliver mail and food to visually-indicated delivery locations. In this setting, users specify delivery destinations for the robot simply by taking a photograph of the desired destination, and the robot autonomously navigates to this destination to deliver a package.
2) Autonomous Inspection: Densely constructed building complexes, like university campuses, are often unmapped or lack accurate spatial localization. We reprogram ViNG to periodically navigate to landmarks, specified as images, around the campus to set up an autonomous patrolling system. Discrepancies can be identified by comparing the observations to previous observations (stored in the topological graph).
Figures 8 and 9 show ViNG successfully performing these tasks in the urban environment. Videos of the qualitative results, generalization experiments, and realworld applications can be found at the project website (sites.google.com/view/ving-robot).
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 9: Autonomous Inspection Demo: Given a set of visual landmarks (a-d) in a university campus, ViNG can perform autonomous inspection by navigating to these goals periodically. An overhead view (b) shows colorcoded goals and the trajectory taken by robot (cyan) in one cycle. Note: The satellite view (e) is solely for visualization and is not available to the robot.</p>
<h2>VI. CONCLUSION</h2>
<p>In this paper, we proposed ViNG: a system for goaldirected navigation using visual observations and goals on an outdoor ground robot. While conceptually similar to prior methods, we demonstrate that a few key design choices, such as pruning the topological graph, parametrizing the controller in terms of a relative pose predictor and sampling negatives while training to minimize distribution shift, allow ViNG to learn to successfully navigate using only offline experience, a setting in which many prior methods fail. Intriguingly, we also demonstrate that ViNG can be quickly adapted to navigate in new environments. These generalization and self-improvement attributes highlight that learning-based approaches are not only an effective mechanism for handling high-dimensional observations, but are also amenable to fast adaptation to novel environments. Further, we have demonstrated ViNG on a number of real-world applications in dense, urban environments that may be unmapped or GPS-denied, and specifying visual goals is convenient - contactless last-mile delivery and autonomous inspection.</p>
<p>Our method requires a static, offline dataset of observations over which we can plan. Many real-world tasks are nonstationary, with the distribution of observations shifting over time (e.g., lighting changes, dynamic objects, etc.). In future work, we aim to incorporate representations of observations and goals that are robust to such distributional shifts, which would expand the generalization capabilities of our method.</p>
<h2>AcKnowledgments</h2>
<p>This research was funded by the Office of Naval Research, DARPA Assured Autonomy, and ARL DCIST CRA W911NF-17-2-0181, with computing support from Google and Amazon Web Services. The authors would like to thank Jonathan Fink and Ethan Stump for their help setting up the simulation environment used for developing this research.</p>
<h2>REFERENCES</h2>
<p>[1] J. O'Keefe and L. Nadel, The Hippocampus as a Cognitive Map. Oxford: Clarendon Press, 1978.
[2] S. Gillner and H. A. Mallot, "Navigation and acquisition of spatial knowledge in a virtual maze," Journal of Cognitive Neuroscience, 1998.
[3] P. S. Foo, W. Warren, A. Duchon, and M. Tarr, "Do humans integrate routes into a cognitive map? map- versus landmarkbased navigation of novel shortcuts." Journal of experimental psychology. Learning, memory, and cognition, 2005.
[4] C. Rosen and N. Nilsson, "Application of intelligent automata to reconnaissance," in SRI Technical Report, 1967.
[5] C. Thorpe, M. H. Hebert, T. Kanade, and S. A. Shafer, "Vision and navigation for the carnegie-mellon navlab," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 10, no. 3, pp. 362-373, 1988.
[6] J. Borenstein and Y. Koren, "Real-time obstacle avoidance for fast mobile robots," IEEE Transactions on systems, Man, and Cybernetics, vol. 19, no. 5, pp. 1179-1187, 1989.
[7] S. Thrun, M. Montemerlo, H. Dahlkamp, D. Stavens, A. Aron, J. Diebel, P. Fong, J. Gale, M. Halpenny, G. Hoffmann et al., "Stanley: The robot that won the darpa grand challenge," Journal of field Robotics, vol. 23, no. 9, pp. 661-692, 2006.
[8] C. Urmson, J. Anhalt, D. Bagnell, C. Baker, R. Bittner, M. Clark, J. Dolan, D. Duggins, T. Galatali, C. Geyer et al., "Autonomous driving in urban environments: Boss and the urban challenge," Journal of Field Robotics, vol. 25, no. 8, pp. 425-466, 2008.
[9] E. Krotkov and M. Hebert, "Mapping and positioning for a prototype lunar rover," in Proceedings of 1995 IEEE International Conference on Robotics and Automation, vol. 3. IEEE, 1995.
[10] F. Dalgleish, S. Tetlow, and R. Allwood, "Vision-based navigation of unmanned underwater vehicles: a survey. part i: Vision based cable-, pipeline-and fish tracking," in Journal of Marine Design and Operations, no. 7, 2004, pp. 51-56.
[11] S. Thrun, W. Burgard, and D. Fox, "Probalistic robotics," Kybernetes, 2006.
[12] S. M. LaValle, Planning Algorithms. Cambridge university press, 2006.
[13] A. J. Davison and D. W. Murray, "Mobile robot localisation using active vision," in European conference on computer vision. Springer, 1998, pp. 809-825.
[14] R. Sim and J. J. Little, "Autonomous vision-based exploration and mapping using hybrid maps and Rao-Blackwellised particle filters," in IEEE/RSJ International Conference on Intelligent Robots and Systems, 2006.
[15] P. Furgale and T. D. Barfoot, "Visual teach and repeat for longrange rover autonomy," Journal of Field Robotics, 2010.
[16] M. Bansal, A. Krizhevsky, and A. Ogale, "ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst," in Robotics: Science and Systems (RSS), 2019.
[17] E. Ackerman, "Skydio demonstrates incredible obstacledodging full autonomy with new r1 consumer drone," IEEE Spectrum, 2018.
[18] DARPA. (2019) Subterranean Challenge. [Online]. Available: https://www.subtchallenge.com
[19] J. Fuentes-Pacheco, J. Ruiz-Ascencio, and J. M. RendónMancha, "Visual simultaneous localization and mapping: a survey," Artificial Intelligence Review, 2015.
[20] C. Chen, A. Seff, A. Kornhauser, and J. Xiao, "Deepdriving: Learning affordance for direct perception in autonomous driving," in IEEE International Conference on Computer Vision, 2015.
[21] Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell, and K. Q. Weinberger, "Pseudo-LiDAR From Visual Depth Estimation: Bridging the Gap in 3D Object Detection for</p>
<p>Autonomous Driving," in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
[22] G. Kahn, A. Villaflor, B. Ding, P. Abbeel, and S. Levine, "SelfSupervised Deep RL with Generalized Computation Graphs for Robot Navigation," in IEEE International Conference on Robotics and Automation (ICRA), 2018.
[23] A. Kumar<em>, S. Gupta</em>, D. Fouhey, S. Levine, and J. Malik, "Visual Memory for Robust Path Following," in Neural Information Processing Systems (NeurIPS), 2018.
[24] K. Hartikainen, X. Geng, T. Haarnoja, and S. Levine, "Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery," in International Conference on Learning Representations, 2020.
[25] S. Levine, C. Finn, T. Darrell, and P. Abbeel, "End-to-End Training of Deep Visuomotor Policies," The Journal of Machine Learning Research, 2016.
[26] S. Ross, G. Gordon, and D. Bagnell, "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning," in International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.
[27] G. Dulac-Arnold, D. Mankowitz, and T. Hester, "Challenges of real-world reinforcement learning," arXiv preprint arXiv:1904.12901, 2019.
[28] B. Eysenbach, R. R. Salakhutdinov, and S. Levine, "Search on the Replay Buffer: Bridging Planning and RL," in Advances in Neural Information Processing Systems (NeurIPS), 2019.
[29] N. Savinov, A. Dosovitskiy, and V. Koltun, "Semi-Parametric Topological Memory for Navigation," in International Conference on Learning Representations, 2018.
[30] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, "Learning to Explore using Active Neural SLAM," in International Conf. on Learning Representations (ICLR), 2020.
[31] H.-T. L. Chiang, A. Faust, M. Fiser, and A. Francis, "Learning Navigation Behaviors End-to-End with AutoRL," IEEE Robotics and Automation Letters, 2019.
[32] A. Faust, K. Oslund, O. Ramirez, A. Francis, L. Tapia, M. Fiser, and J. Davidson, "PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and SamplingBased Planning," in IEEE International Conference on Robotics and Automation (ICRA), 2018, pp. 5113-5120.
[33] X. Meng, N. Ratliff, Y. Xiang, and D. Fox, "Scaling Local Control to Large-Scale Topological Navigation," in IEEE International Conference on Robotics and Automation (ICRA), 2020.
[34] M. Meng and A. C. Kak, "NEURO-NAV: A Neural Network based Architecture for Vision-guided Mobile Robot Navigation using Non-metrical Models of the Environment," in IEEE International Conference on Robotics and Automation (ICRA), 1993.
[35] , "Mobile Robot Navigation using Neural Networks and Nonmetrical Environmental Models," IEEE Control Systems Magazine, 1993.
[36] A. Francis, A. Faust, H. T. L. Chiang, J. Hsu, J. C. Kew, M. Fiser, and T. W. E. Lee, "Long-Range Indoor Navigation With PRM-RL," IEEE Transactions on Robotics, 2020.
[37] G. Kahn, P. Abbeel, and S. Levine, "BADGR: An Autonomous Self-Supervised Learning-Based Navigation System," 2020.
[38] R. S. Sutton, "Learning to predict by the methods of temporal differences," Machine Learning, 1988.
[39] L. P. Kaelbling, "Learning to achieve goals," in IJCAI. Citeseer, 1993, pp. 1094-1099.
[40] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications," 2017.
[41] D. P. Kingma and J. Ba, "Adam: A Method for Stochastic Optimization," International Conference on Learning Representations (ICLR), 2015.</p>
<p>[42] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal Policy Optimization Algorithms," 2017.
[43] E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva, and D. Batra, "DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames," in International Conference on Learning Representations (ICLR), 2020.
[44] Y. Ding, C. Florensa, P. Abbeel, and M. Phielipp, "Goalconditioned Imitation Learning," in Advances in Neural Information Processing Systems (NeurIPS), 2019.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Project website: sites.google.com/view/ving-robot&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
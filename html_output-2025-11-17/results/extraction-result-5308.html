<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5308 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5308</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5308</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-08a809f649ae9be9fe00cfad5c8c7a30a924d7d8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/08a809f649ae9be9fe00cfad5c8c7a30a924d7d8" target="_blank">Regression Transformer enables concurrent sequence regression and generation for molecular language modelling</a></p>
                <p><strong>Paper Venue:</strong> Nature Machine Intelligence</p>
                <p><strong>Paper TL;DR:</strong> The Regression Transformer (RT), a method that abstracts regression as a conditional sequence modelling problem, is proposed, and it is demonstrated that, despite using a nominal-scale training objective, the RT matches or surpasses the performance of conventional regression models in property prediction of small molecules, proteins and chemical reactions.</p>
                <p><strong>Paper Abstract:</strong> Transformer models are gaining increasing popularity in modelling natural language as they can produce human-sounding text by iteratively predicting the next word in a sentence. Born and Manica apply the idea of Transformer-based text completion to property prediction of chemical compounds by providing the context of a problem and having the model complete the missing information. Despite tremendous progress of generative models in the natural sciences, their controllability remains challenging. One fundamentally missing aspect of molecular or protein generative models is an inductive bias that can reflect continuous properties of interest. To that end, we propose the Regression Transformer (RT), a method that abstracts regression as a conditional sequence modelling problem. This introduces a new direction for multitask language models, seamlessly bridging sequence regression and conditional sequence generation. We demonstrate that, despite using a nominal-scale training objective, the RT matches or surpasses the performance of conventional regression models in property prediction of small molecules, proteins and chemical reactions. Critically, priming the same model with continuous properties yields a competitive conditional generative model that outperforms specialized approaches in a substructure-constrained, property-driven molecule generation benchmark. Our dichotomous approach is facilitated by an alternating training scheme that enables the model to decorate seed sequences on the basis of desired property constraints, for example, to optimize reaction yield. We expect that the RT’s capability to jointly tackle predictive and generative tasks in biochemistry can find applications in property-driven, local exploration of the chemical or protein space. Such multitask approaches will pave the road towards foundation models in materials design.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5308.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5308.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Regression Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multitask Transformer (XLNet backbone) that represents continuous properties as token sequences and is trained with alternating objectives to perform both regression (property prediction) and conditional sequence generation (property-driven molecule, protein and reaction generation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Regression Transformer (RT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (XLNet backbone, permutation language modelling / autoregressive with bidirectional context)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>~27 million parameters (reported for the configured model)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Varied: ~1.6M ChEMBL molecules for QED pre-training, MoleculeNet datasets (ESOL, FreeSolv, Lipophilicity), USPTO reaction corpus (~2.8M) for reaction pretraining, Buchwald–Hartwig and Suzuki HTE yield datasets for yield tasks, UniProt peptides for Boman index and TAPE fluorescence/stability datasets for protein tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemical design (property-driven molecule generation, scaffold decoration, penalized logP optimization), reaction modelling (yield prediction, precursor reconstruction and reaction decoration to increase yield), protein sequence design, polymer catalyst/design (mentioned via follow-up ref).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning + alternating training: permutation language modelling (PLM) pretraining followed by alternating objectives (property-only masking for regression objective, text-only masking for conditional generation) and an optional self-consistency (SC) loss that re-evaluates generated sequences with the model's own predictor; decoding via greedy decoding for properties and beam search for sequence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SELFIES primarily (also SMILES in ablations); protein sequences tokenized per amino acid; reactions expressed as tokenized reaction SELFIES.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Regression: RMSE, PCC, R^2, MAE; Generation: Spearman's ρ between primer and generated property, novelty (% new molecules), 0-Var (percent unchanged by primer), constrained-optimization improvement in penalized logP, Tanimoto similarity to seed, top-k reconstruction accuracy for precursors, reaction yield improvement and success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>ChEMBL-derived QED dataset, MoleculeNet (ESOL, FreeSolv, Lipophilicity), penalized logP constrained optimization benchmark (Jin et al.), Buchwald–Hartwig and Suzuki HTE yield datasets, USPTO reaction corpus, UniProt / Boman index dataset, TAPE (fluorescence, stability).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>RT matches or surpasses conventional regressors on several property-prediction tasks and uniquely provides conditional generation tied to continuous property primers. Key reported results: QED prediction/generation (SELFIES RT, alternate + SC) RMSE ≈ 0.037, PCC ≈ 0.987; MAE on QED improved to 0.017 (Extended Data Table 1). On penalized logP constrained optimization RT achieved average improvement 3.16 (δ=0.4) and 2.21 (δ=0.6) and outperformed several specialized generative models in average improvement while also predicting plogP with PCC=0.92. For reaction yields RT achieved R^2 ≈ 0.939 (Buchwald) and 0.81 (Suzuki), reconstructed precursors with high top-3 accuracy for some precursor types (e.g., aryl-halide 98.2%) and could propose decorated reactions predicted to increase yield (40–80% of top-5 contained novel precursors with higher predicted yield). Conditional generation novelty >99% in QED experiments; defective generation rates were low (~1–2%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>RT competes with and sometimes outperforms specialized generative models on constrained molecular optimization benchmarks (e.g., better average improvement than JT-VAE and GCPN in the reported plogP task), while additionally providing property prediction — a capability most specialized generative models lack. For reaction yield prediction it nearly matches or slightly lags specialized Yield-BERT on some splits but adds generative capabilities (precursor generation/decoration). Against large pre-trained models trained with regression heads (BERT/BART/XLNet-regression), RT is competitive but sometimes slightly inferior on fine-grained regression benchmarks, despite not using a regression loss.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Generation precision drops for large deltas between seed and primed properties (out-of-distribution primers); trade-off where SC loss improves conditional generation at slight cost to regression accuracy (confounded gradients); small fraction of defective or stubbified SELFIES generations (~1.3–1.9%); SMILES versions suffered syntactic invalidity more than SELFIES; RT's fine-grained intra-mode regression (e.g., within fluorescence mode) can be weaker; SC scheme can reinforce model bias if property predictions are poor; pretraining scale is limited versus large foundation models, which may limit zero-shot generalization at evolutionary protein scales.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Regression Transformer enables concurrent sequence regression and generation for molecular language modelling', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5308.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5308.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLNet-reg</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XLNet with regression head (fine-tuned baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An XLNet Transformer initialized from HuggingFace weights and fine-tuned with an L2 regression loss as a baseline for molecular property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLNet (with SequenceClassification/regression head)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (XLNet), autoregressive PLM-like backbone; fine-tuned with regression head</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>~93 million parameters (reported for the fine-tuned model)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on MoleculeNet regression datasets (after QED warm-starting in authors' protocol for comparability).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular property prediction (regression tasks on MoleculeNet).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning with L2 regression loss (no conditional generation capability in that configuration).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES/SELFIES tokenization depending on experiment; standard textual token embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>RMSE compared against other baselines on ESOL, FreeSolv, Lipophilicity.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>MoleculeNet (ESOL, FreeSolv, Lipophilicity).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Used as a comparative baseline; RT without regression loss was on par (Lipophilicity) or mildly inferior (ESOL, FreeSolv) to the XLNet regression head.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Provides a directly comparable Transformer baseline because it uses an XLNet backbone with explicit regression loss, showing that the RT formulation can approach performance without a regression loss.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>This baseline is not a conditional generative model; larger parameter count compared to the RT configuration used in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Regression Transformer enables concurrent sequence regression and generation for molecular language modelling', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5308.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5308.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemFormer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemFormer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained Transformer model for computational chemistry that addresses multiple tasks including regression and targeted molecular generation by tuning task-specific heads.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chemformer: a pre-trained transformer for computational chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemFormer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (pre-trained, fine-tuned with task-specific heads)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Large-scale self-supervised pretraining on chemical strings (SMILES) prior to fine-tuning (details in original ChemFormer paper).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular property prediction and conditional molecular design.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Pretraining then fine-tuning with task-specific heads (not a dichotomous multitask entangled model).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Property prediction metrics (regression losses), generation metrics as per downstream tasks (reported in original work).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Large unlabeled chemical corpora for pretraining; downstream datasets vary.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as prior art that tunes task-specific heads rather than entangling regression and generation as RT does.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Differs from RT in that ChemFormer fine-tunes separate heads instead of supporting unified conditional generation from continuous primers within the same model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Does not constitute a truly multitask model that seamlessly entangles regression and conditional generation according to the authors' critique.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Regression Transformer enables concurrent sequence regression and generation for molecular language modelling', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5308.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5308.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yield-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Yield-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-based model trained with a regression loss to predict chemical reaction yields from reaction SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prediction of chemical reaction yields using deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yield-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (BERT-style / encoder model fine-tuned for regression)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Reaction yield datasets (HTE Buchwald–Hartwig and Suzuki datasets used in comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Reaction yield prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning BERT-like model for regression (no generation described here).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES (reaction SMILES format)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Coefficient of determination (R^2) for yield prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Buchwald–Hartwig and Suzuki HTE yield datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported R^2: Yield-BERT achieved ~0.951 on Buchwald and ~0.79–0.81 on Suzuki; RT obtained R^2 ≈ 0.939 (Buchwald) and 0.81 (Suzuki), so RT nearly matches or matches Yield-BERT on these datasets while also enabling generative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Yield-BERT specialized for yield regression slightly outperforms RT on Buchwald yield prediction but RT offers additional generation capabilities (precursor reconstruction and decoration).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Yield-BERT is predictive only and does not provide the conditional precursor-generation or decoration abilities presented for RT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Regression Transformer enables concurrent sequence regression and generation for molecular language modelling', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5308.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5308.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JT-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Junction Tree Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VAE-based molecular graph generative model that generates molecules by assembling chemical substructures (junction trees) and was used as a baseline on the penalized logP optimization benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Junction tree variational autoencoder for molecular graph generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Junction-Tree VAE (JT-VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Variational Autoencoder (graph-based with junction-tree scaffold decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained on molecular datasets (e.g., ZINC) as in the original benchmark work.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Goal-directed molecular generation / optimization (e.g., penalized logP).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>VAE latent-space sampling and optimization (often combined with search/gradient techniques in benchmark evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Molecular graphs (decoded to SMILES for evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Average improvement in plogP under similarity constraints, Tanimoto similarity, success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Penalized logP constrained optimization benchmark (Jin et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported average improvement lower than RT on δ=0.4 and δ=0.6 thresholds in the paper's comparisons (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Specialized generative model focused solely on generation/optimization; RT outperformed JT-VAE in average improvement on the reported constrained plogP benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Benchmark comparisons are not strictly apples-to-apples because many specialized methods use reward-driven training or gradient-based inference-time optimization, which RT did not employ.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Regression Transformer enables concurrent sequence regression and generation for molecular language modelling', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5308.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5308.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GCPN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Convolutional Policy Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based reinforcement learning generative model that incrementally builds molecules guided by graph convolutions and policy gradients, used as a baseline for goal-directed molecular generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph convolutional policy network for goal-directed molecular graph generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GCPN</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Graph neural network + reinforcement learning (policy network)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Graph-based molecular datasets (e.g., ZINC) used in original GCPN work.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Goal-directed molecular graph generation (property optimization under constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Reinforcement learning with graph actions, reward for high property values.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Molecular graphs (converted to SMILES for evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Average improvement in plogP, Tanimoto similarity, success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Penalized logP constrained optimization benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GCPN was reported with improvements on plogP benchmark, but RT outperformed GCPN substantially in average improvement in the authors' comparisons (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>GCPN is a specialized RL-based generator that uses explicit reward shaping; RT's performance is notable given RT does not use directed plogP reward during training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires reward engineering and RL training; comparisons complicated by inference-time optimization used in some competing methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Regression Transformer enables concurrent sequence regression and generation for molecular language modelling', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5308.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5308.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoFlow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MoFlow</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An invertible flow-based autoregressive model for molecular graph generation used as a baseline on molecular optimization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Moflow: an invertible flow model for generating molecular graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MoFlow</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Flow-based invertible generative model for graphs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Molecular graph datasets (e.g., ZINC) used for flow training in original work.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular generation and optimization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Flow-based sampling and latent-space manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Molecular graphs (converted to SMILES for evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Average improvement in plogP, similarity, success rate on constrained optimization benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Penalized logP constrained optimization benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MoFlow performed comparably to RT in some settings; RT outperformed MoFlow on some thresholds and RT did better on unconstrained generation for δ=0.0 according to the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Flow models provide invertible latent mappings enabling exact likelihoods; RT offers the additional capacity for conditional property prediction and text infilling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Benchmark performance depends on training/rewarding regime; not directly comparable when other models apply inference-time optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Regression Transformer enables concurrent sequence regression and generation for molecular language modelling', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5308.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5308.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Back Translation (for molecules)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-based generative approach that uses translation/back-translation techniques to propose molecules; used as a high-performing baseline on the penalized logP optimization benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Back translation for molecule generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Back Translation (BT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Sequence-to-sequence / translation-inspired generative approach</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES corpora and augmentations for translation/back-translation training (original work).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule generation and optimization (goal-directed).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Back-translation / augmentation and generation pipelines, sometimes combined with search.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Average improvement in plogP, success rate under similarity constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Penalized logP constrained optimization benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>BT achieved strong average improvement on the benchmark and outperformed RT on average improvement in some reported cases; however, reporting differences and missing s.d./metrics in BT publications complicate strict comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>BT uses data augmentation/back-translation to propose candidates; RT is a unified entangled model that pairs property prediction with generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reporting inconsistencies and additional optimization steps in competing methods make direct comparisons difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Regression Transformer enables concurrent sequence regression and generation for molecular language modelling', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5308.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5308.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-style pre-trained language model for molecular property prediction from SMILES; cited as a strong regression baseline on QED in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Smiles-bert: large scale unsupervised pre-training for molecular property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SMILES-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (BERT-style encoder, pre-trained then fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Large-scale pretraining on ~9 million SMILES (as stated in paper's comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular property prediction (regression/classification).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Pretraining then fine-tuning for downstream predictive tasks (not used as a generative model here).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MAE on QED and other property-prediction metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>QED dataset and other property datasets in original work.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SMILES-BERT reported MAE of 0.02 on QED after large-scale pretraining, outperforming some RT configurations; RT with alternating objectives achieved MAE 0.017 in authors' experiments (Extended Data Table 1) when trained appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>SMILES-BERT leverages large-scale pretraining and explicit regression fine-tuning; RT achieves comparable or better results without explicit regression loss in some configurations and additionally supports generation tied to continuous primers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>SMILES representation can be syntactically invalid under corruption; SMILES-based generation can suffer validity issues (authors prefer SELFIES for generation robustness).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Regression Transformer enables concurrent sequence regression and generation for molecular language modelling', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5308.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5308.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grover</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grover (self-supervised Graph Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised graph Transformer model used as an independent property predictor to evaluate properties of molecules generated by RT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Grover (graph Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Graph Transformer (self-supervised pretraining on graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Large-scale pretraining on molecular graphs (details in Grover paper).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular property prediction (used here as independent evaluator of RT-generated molecules).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not used for generation in this paper; used as an external predictor.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Molecular graphs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Correlation of property predictions with RT (Spearman/Pearson correlations reported between RT and Grover predictions for generated molecules).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Used to validate generated molecules from RT on MoleculeNet-derived generative experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>High correlation between RT predictions and Grover for generated molecules: correlations of 0.86 (ESOL), 0.84 (FreeSolv), 0.75 (Lipophilicity) indicating RT generations aligned with an independent graph-based predictor.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Serves as an orthogonal evaluator to mitigate bias when RT evaluates its own generated molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not applied as a generator in this study; correlation does not replace experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Regression Transformer enables concurrent sequence regression and generation for molecular language modelling', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction <em>(Rating: 2)</em></li>
                <li>Chemformer: a pre-trained transformer for computational chemistry <em>(Rating: 2)</em></li>
                <li>Junction tree variational autoencoder for molecular graph generation <em>(Rating: 2)</em></li>
                <li>Graph convolutional policy network for goal-directed molecular graph generation <em>(Rating: 2)</em></li>
                <li>Moflow: an invertible flow model for generating molecular graphs <em>(Rating: 2)</em></li>
                <li>Back translation for molecule generation <em>(Rating: 2)</em></li>
                <li>Smiles-bert: large scale unsupervised pre-training for molecular property prediction <em>(Rating: 1)</em></li>
                <li>Prediction of chemical reaction yields using deep learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5308",
    "paper_id": "paper-08a809f649ae9be9fe00cfad5c8c7a30a924d7d8",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "RT",
            "name_full": "Regression Transformer",
            "brief_description": "A multitask Transformer (XLNet backbone) that represents continuous properties as token sequences and is trained with alternating objectives to perform both regression (property prediction) and conditional sequence generation (property-driven molecule, protein and reaction generation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Regression Transformer (RT)",
            "model_type": "Transformer (XLNet backbone, permutation language modelling / autoregressive with bidirectional context)",
            "model_size": "~27 million parameters (reported for the configured model)",
            "training_data": "Varied: ~1.6M ChEMBL molecules for QED pre-training, MoleculeNet datasets (ESOL, FreeSolv, Lipophilicity), USPTO reaction corpus (~2.8M) for reaction pretraining, Buchwald–Hartwig and Suzuki HTE yield datasets for yield tasks, UniProt peptides for Boman index and TAPE fluorescence/stability datasets for protein tasks.",
            "application_domain": "Chemical design (property-driven molecule generation, scaffold decoration, penalized logP optimization), reaction modelling (yield prediction, precursor reconstruction and reaction decoration to increase yield), protein sequence design, polymer catalyst/design (mentioned via follow-up ref).",
            "generation_method": "Fine-tuning + alternating training: permutation language modelling (PLM) pretraining followed by alternating objectives (property-only masking for regression objective, text-only masking for conditional generation) and an optional self-consistency (SC) loss that re-evaluates generated sequences with the model's own predictor; decoding via greedy decoding for properties and beam search for sequence generation.",
            "output_representation": "SELFIES primarily (also SMILES in ablations); protein sequences tokenized per amino acid; reactions expressed as tokenized reaction SELFIES.",
            "evaluation_metrics": "Regression: RMSE, PCC, R^2, MAE; Generation: Spearman's ρ between primer and generated property, novelty (% new molecules), 0-Var (percent unchanged by primer), constrained-optimization improvement in penalized logP, Tanimoto similarity to seed, top-k reconstruction accuracy for precursors, reaction yield improvement and success rate.",
            "benchmarks_or_datasets": "ChEMBL-derived QED dataset, MoleculeNet (ESOL, FreeSolv, Lipophilicity), penalized logP constrained optimization benchmark (Jin et al.), Buchwald–Hartwig and Suzuki HTE yield datasets, USPTO reaction corpus, UniProt / Boman index dataset, TAPE (fluorescence, stability).",
            "results_summary": "RT matches or surpasses conventional regressors on several property-prediction tasks and uniquely provides conditional generation tied to continuous property primers. Key reported results: QED prediction/generation (SELFIES RT, alternate + SC) RMSE ≈ 0.037, PCC ≈ 0.987; MAE on QED improved to 0.017 (Extended Data Table 1). On penalized logP constrained optimization RT achieved average improvement 3.16 (δ=0.4) and 2.21 (δ=0.6) and outperformed several specialized generative models in average improvement while also predicting plogP with PCC=0.92. For reaction yields RT achieved R^2 ≈ 0.939 (Buchwald) and 0.81 (Suzuki), reconstructed precursors with high top-3 accuracy for some precursor types (e.g., aryl-halide 98.2%) and could propose decorated reactions predicted to increase yield (40–80% of top-5 contained novel precursors with higher predicted yield). Conditional generation novelty &gt;99% in QED experiments; defective generation rates were low (~1–2%).",
            "comparison_to_other_methods": "RT competes with and sometimes outperforms specialized generative models on constrained molecular optimization benchmarks (e.g., better average improvement than JT-VAE and GCPN in the reported plogP task), while additionally providing property prediction — a capability most specialized generative models lack. For reaction yield prediction it nearly matches or slightly lags specialized Yield-BERT on some splits but adds generative capabilities (precursor generation/decoration). Against large pre-trained models trained with regression heads (BERT/BART/XLNet-regression), RT is competitive but sometimes slightly inferior on fine-grained regression benchmarks, despite not using a regression loss.",
            "limitations_or_challenges": "Generation precision drops for large deltas between seed and primed properties (out-of-distribution primers); trade-off where SC loss improves conditional generation at slight cost to regression accuracy (confounded gradients); small fraction of defective or stubbified SELFIES generations (~1.3–1.9%); SMILES versions suffered syntactic invalidity more than SELFIES; RT's fine-grained intra-mode regression (e.g., within fluorescence mode) can be weaker; SC scheme can reinforce model bias if property predictions are poor; pretraining scale is limited versus large foundation models, which may limit zero-shot generalization at evolutionary protein scales.",
            "uuid": "e5308.0",
            "source_info": {
                "paper_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "XLNet-reg",
            "name_full": "XLNet with regression head (fine-tuned baseline)",
            "brief_description": "An XLNet Transformer initialized from HuggingFace weights and fine-tuned with an L2 regression loss as a baseline for molecular property prediction.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "XLNet (with SequenceClassification/regression head)",
            "model_type": "Transformer (XLNet), autoregressive PLM-like backbone; fine-tuned with regression head",
            "model_size": "~93 million parameters (reported for the fine-tuned model)",
            "training_data": "Fine-tuned on MoleculeNet regression datasets (after QED warm-starting in authors' protocol for comparability).",
            "application_domain": "Molecular property prediction (regression tasks on MoleculeNet).",
            "generation_method": "Fine-tuning with L2 regression loss (no conditional generation capability in that configuration).",
            "output_representation": "SMILES/SELFIES tokenization depending on experiment; standard textual token embeddings.",
            "evaluation_metrics": "RMSE compared against other baselines on ESOL, FreeSolv, Lipophilicity.",
            "benchmarks_or_datasets": "MoleculeNet (ESOL, FreeSolv, Lipophilicity).",
            "results_summary": "Used as a comparative baseline; RT without regression loss was on par (Lipophilicity) or mildly inferior (ESOL, FreeSolv) to the XLNet regression head.",
            "comparison_to_other_methods": "Provides a directly comparable Transformer baseline because it uses an XLNet backbone with explicit regression loss, showing that the RT formulation can approach performance without a regression loss.",
            "limitations_or_challenges": "This baseline is not a conditional generative model; larger parameter count compared to the RT configuration used in main experiments.",
            "uuid": "e5308.1",
            "source_info": {
                "paper_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "ChemFormer",
            "name_full": "ChemFormer",
            "brief_description": "A pre-trained Transformer model for computational chemistry that addresses multiple tasks including regression and targeted molecular generation by tuning task-specific heads.",
            "citation_title": "Chemformer: a pre-trained transformer for computational chemistry",
            "mention_or_use": "mention",
            "model_name": "ChemFormer",
            "model_type": "Transformer (pre-trained, fine-tuned with task-specific heads)",
            "model_size": null,
            "training_data": "Large-scale self-supervised pretraining on chemical strings (SMILES) prior to fine-tuning (details in original ChemFormer paper).",
            "application_domain": "Molecular property prediction and conditional molecular design.",
            "generation_method": "Pretraining then fine-tuning with task-specific heads (not a dichotomous multitask entangled model).",
            "output_representation": "SMILES",
            "evaluation_metrics": "Property prediction metrics (regression losses), generation metrics as per downstream tasks (reported in original work).",
            "benchmarks_or_datasets": "Large unlabeled chemical corpora for pretraining; downstream datasets vary.",
            "results_summary": "Mentioned as prior art that tunes task-specific heads rather than entangling regression and generation as RT does.",
            "comparison_to_other_methods": "Differs from RT in that ChemFormer fine-tunes separate heads instead of supporting unified conditional generation from continuous primers within the same model weights.",
            "limitations_or_challenges": "Does not constitute a truly multitask model that seamlessly entangles regression and conditional generation according to the authors' critique.",
            "uuid": "e5308.2",
            "source_info": {
                "paper_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Yield-BERT",
            "name_full": "Yield-BERT",
            "brief_description": "A Transformer-based model trained with a regression loss to predict chemical reaction yields from reaction SMILES.",
            "citation_title": "Prediction of chemical reaction yields using deep learning",
            "mention_or_use": "mention",
            "model_name": "Yield-BERT",
            "model_type": "Transformer (BERT-style / encoder model fine-tuned for regression)",
            "model_size": null,
            "training_data": "Reaction yield datasets (HTE Buchwald–Hartwig and Suzuki datasets used in comparisons).",
            "application_domain": "Reaction yield prediction.",
            "generation_method": "Fine-tuning BERT-like model for regression (no generation described here).",
            "output_representation": "SMILES (reaction SMILES format)",
            "evaluation_metrics": "Coefficient of determination (R^2) for yield prediction.",
            "benchmarks_or_datasets": "Buchwald–Hartwig and Suzuki HTE yield datasets.",
            "results_summary": "Reported R^2: Yield-BERT achieved ~0.951 on Buchwald and ~0.79–0.81 on Suzuki; RT obtained R^2 ≈ 0.939 (Buchwald) and 0.81 (Suzuki), so RT nearly matches or matches Yield-BERT on these datasets while also enabling generative tasks.",
            "comparison_to_other_methods": "Yield-BERT specialized for yield regression slightly outperforms RT on Buchwald yield prediction but RT offers additional generation capabilities (precursor reconstruction and decoration).",
            "limitations_or_challenges": "Yield-BERT is predictive only and does not provide the conditional precursor-generation or decoration abilities presented for RT.",
            "uuid": "e5308.3",
            "source_info": {
                "paper_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "JT-VAE",
            "name_full": "Junction Tree Variational Autoencoder",
            "brief_description": "A VAE-based molecular graph generative model that generates molecules by assembling chemical substructures (junction trees) and was used as a baseline on the penalized logP optimization benchmark.",
            "citation_title": "Junction tree variational autoencoder for molecular graph generation",
            "mention_or_use": "mention",
            "model_name": "Junction-Tree VAE (JT-VAE)",
            "model_type": "Variational Autoencoder (graph-based with junction-tree scaffold decomposition)",
            "model_size": null,
            "training_data": "Trained on molecular datasets (e.g., ZINC) as in the original benchmark work.",
            "application_domain": "Goal-directed molecular generation / optimization (e.g., penalized logP).",
            "generation_method": "VAE latent-space sampling and optimization (often combined with search/gradient techniques in benchmark evaluations).",
            "output_representation": "Molecular graphs (decoded to SMILES for evaluation).",
            "evaluation_metrics": "Average improvement in plogP under similarity constraints, Tanimoto similarity, success rate.",
            "benchmarks_or_datasets": "Penalized logP constrained optimization benchmark (Jin et al.).",
            "results_summary": "Reported average improvement lower than RT on δ=0.4 and δ=0.6 thresholds in the paper's comparisons (Table 2).",
            "comparison_to_other_methods": "Specialized generative model focused solely on generation/optimization; RT outperformed JT-VAE in average improvement on the reported constrained plogP benchmark.",
            "limitations_or_challenges": "Benchmark comparisons are not strictly apples-to-apples because many specialized methods use reward-driven training or gradient-based inference-time optimization, which RT did not employ.",
            "uuid": "e5308.4",
            "source_info": {
                "paper_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "GCPN",
            "name_full": "Graph Convolutional Policy Network",
            "brief_description": "A graph-based reinforcement learning generative model that incrementally builds molecules guided by graph convolutions and policy gradients, used as a baseline for goal-directed molecular generation.",
            "citation_title": "Graph convolutional policy network for goal-directed molecular graph generation",
            "mention_or_use": "mention",
            "model_name": "GCPN",
            "model_type": "Graph neural network + reinforcement learning (policy network)",
            "model_size": null,
            "training_data": "Graph-based molecular datasets (e.g., ZINC) used in original GCPN work.",
            "application_domain": "Goal-directed molecular graph generation (property optimization under constraints).",
            "generation_method": "Reinforcement learning with graph actions, reward for high property values.",
            "output_representation": "Molecular graphs (converted to SMILES for evaluation).",
            "evaluation_metrics": "Average improvement in plogP, Tanimoto similarity, success rate.",
            "benchmarks_or_datasets": "Penalized logP constrained optimization benchmark.",
            "results_summary": "GCPN was reported with improvements on plogP benchmark, but RT outperformed GCPN substantially in average improvement in the authors' comparisons (Table 2).",
            "comparison_to_other_methods": "GCPN is a specialized RL-based generator that uses explicit reward shaping; RT's performance is notable given RT does not use directed plogP reward during training.",
            "limitations_or_challenges": "Requires reward engineering and RL training; comparisons complicated by inference-time optimization used in some competing methods.",
            "uuid": "e5308.5",
            "source_info": {
                "paper_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "MoFlow",
            "name_full": "MoFlow",
            "brief_description": "An invertible flow-based autoregressive model for molecular graph generation used as a baseline on molecular optimization tasks.",
            "citation_title": "Moflow: an invertible flow model for generating molecular graphs",
            "mention_or_use": "mention",
            "model_name": "MoFlow",
            "model_type": "Flow-based invertible generative model for graphs",
            "model_size": null,
            "training_data": "Molecular graph datasets (e.g., ZINC) used for flow training in original work.",
            "application_domain": "Molecular generation and optimization tasks.",
            "generation_method": "Flow-based sampling and latent-space manipulation.",
            "output_representation": "Molecular graphs (converted to SMILES for evaluation).",
            "evaluation_metrics": "Average improvement in plogP, similarity, success rate on constrained optimization benchmarks.",
            "benchmarks_or_datasets": "Penalized logP constrained optimization benchmark.",
            "results_summary": "MoFlow performed comparably to RT in some settings; RT outperformed MoFlow on some thresholds and RT did better on unconstrained generation for δ=0.0 according to the authors.",
            "comparison_to_other_methods": "Flow models provide invertible latent mappings enabling exact likelihoods; RT offers the additional capacity for conditional property prediction and text infilling.",
            "limitations_or_challenges": "Benchmark performance depends on training/rewarding regime; not directly comparable when other models apply inference-time optimization.",
            "uuid": "e5308.6",
            "source_info": {
                "paper_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "BT",
            "name_full": "Back Translation (for molecules)",
            "brief_description": "A sequence-based generative approach that uses translation/back-translation techniques to propose molecules; used as a high-performing baseline on the penalized logP optimization benchmark.",
            "citation_title": "Back translation for molecule generation",
            "mention_or_use": "mention",
            "model_name": "Back Translation (BT)",
            "model_type": "Sequence-to-sequence / translation-inspired generative approach",
            "model_size": null,
            "training_data": "SMILES corpora and augmentations for translation/back-translation training (original work).",
            "application_domain": "Molecule generation and optimization (goal-directed).",
            "generation_method": "Back-translation / augmentation and generation pipelines, sometimes combined with search.",
            "output_representation": "SMILES",
            "evaluation_metrics": "Average improvement in plogP, success rate under similarity constraints.",
            "benchmarks_or_datasets": "Penalized logP constrained optimization benchmark.",
            "results_summary": "BT achieved strong average improvement on the benchmark and outperformed RT on average improvement in some reported cases; however, reporting differences and missing s.d./metrics in BT publications complicate strict comparisons.",
            "comparison_to_other_methods": "BT uses data augmentation/back-translation to propose candidates; RT is a unified entangled model that pairs property prediction with generation.",
            "limitations_or_challenges": "Reporting inconsistencies and additional optimization steps in competing methods make direct comparisons difficult.",
            "uuid": "e5308.7",
            "source_info": {
                "paper_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "SMILES-BERT",
            "name_full": "SMILES-BERT",
            "brief_description": "A BERT-style pre-trained language model for molecular property prediction from SMILES; cited as a strong regression baseline on QED in prior work.",
            "citation_title": "Smiles-bert: large scale unsupervised pre-training for molecular property prediction",
            "mention_or_use": "mention",
            "model_name": "SMILES-BERT",
            "model_type": "Transformer (BERT-style encoder, pre-trained then fine-tuned)",
            "model_size": null,
            "training_data": "Large-scale pretraining on ~9 million SMILES (as stated in paper's comparison).",
            "application_domain": "Molecular property prediction (regression/classification).",
            "generation_method": "Pretraining then fine-tuning for downstream predictive tasks (not used as a generative model here).",
            "output_representation": "SMILES",
            "evaluation_metrics": "MAE on QED and other property-prediction metrics.",
            "benchmarks_or_datasets": "QED dataset and other property datasets in original work.",
            "results_summary": "SMILES-BERT reported MAE of 0.02 on QED after large-scale pretraining, outperforming some RT configurations; RT with alternating objectives achieved MAE 0.017 in authors' experiments (Extended Data Table 1) when trained appropriately.",
            "comparison_to_other_methods": "SMILES-BERT leverages large-scale pretraining and explicit regression fine-tuning; RT achieves comparable or better results without explicit regression loss in some configurations and additionally supports generation tied to continuous primers.",
            "limitations_or_challenges": "SMILES representation can be syntactically invalid under corruption; SMILES-based generation can suffer validity issues (authors prefer SELFIES for generation robustness).",
            "uuid": "e5308.8",
            "source_info": {
                "paper_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Grover",
            "name_full": "Grover (self-supervised Graph Transformer)",
            "brief_description": "A self-supervised graph Transformer model used as an independent property predictor to evaluate properties of molecules generated by RT.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Grover (graph Transformer)",
            "model_type": "Graph Transformer (self-supervised pretraining on graphs)",
            "model_size": null,
            "training_data": "Large-scale pretraining on molecular graphs (details in Grover paper).",
            "application_domain": "Molecular property prediction (used here as independent evaluator of RT-generated molecules).",
            "generation_method": "Not used for generation in this paper; used as an external predictor.",
            "output_representation": "Molecular graphs",
            "evaluation_metrics": "Correlation of property predictions with RT (Spearman/Pearson correlations reported between RT and Grover predictions for generated molecules).",
            "benchmarks_or_datasets": "Used to validate generated molecules from RT on MoleculeNet-derived generative experiments.",
            "results_summary": "High correlation between RT predictions and Grover for generated molecules: correlations of 0.86 (ESOL), 0.84 (FreeSolv), 0.75 (Lipophilicity) indicating RT generations aligned with an independent graph-based predictor.",
            "comparison_to_other_methods": "Serves as an orthogonal evaluator to mitigate bias when RT evaluates its own generated molecules.",
            "limitations_or_challenges": "Not applied as a generator in this study; correlation does not replace experimental validation.",
            "uuid": "e5308.9",
            "source_info": {
                "paper_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction",
            "rating": 2
        },
        {
            "paper_title": "Chemformer: a pre-trained transformer for computational chemistry",
            "rating": 2
        },
        {
            "paper_title": "Junction tree variational autoencoder for molecular graph generation",
            "rating": 2
        },
        {
            "paper_title": "Graph convolutional policy network for goal-directed molecular graph generation",
            "rating": 2
        },
        {
            "paper_title": "Moflow: an invertible flow model for generating molecular graphs",
            "rating": 2
        },
        {
            "paper_title": "Back translation for molecule generation",
            "rating": 2
        },
        {
            "paper_title": "Smiles-bert: large scale unsupervised pre-training for molecular property prediction",
            "rating": 1
        },
        {
            "paper_title": "Prediction of chemical reaction yields using deep learning",
            "rating": 2
        }
    ],
    "cost": 0.01995675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Regression Transformer enables concurrent sequence regression and generation for molecular language modelling</h1>
<p>Received: 7 September 2022
Accepted: 2 March 2023
Published online: 6 April 2023
Check for updates</p>
<p>Jannis Born ${ }^{1,2}$ \&amp; Matteo Manica ${ }^{1}$ (D)</p>
<p>Despite tremendous progress of generative models in the natural sciences, their controllability remains challenging. One fundamentally missing aspect of molecular or protein generative models is an inductive bias that can reflect continuous properties of interest. To that end, we propose the Regression Transformer (RT), a method that abstracts regression as a conditional sequence modelling problem. This introduces a new direction for multitask language models, seamlessly bridging sequence regression and conditional sequence generation. We demonstrate that, despite using a nominal-scale training objective, the RT matches or surpasses the performance of conventional regression models in property prediction of small molecules, proteins and chemical reactions. Critically, priming the same model with continuous properties yields a competitive conditional generative model that outperforms specialized approaches in a substructure-constrained, property-driven molecule generation benchmark. Our dichotomous approach is facilitated by an alternating training scheme that enables the model to decorate seed sequences on the basis of desired property constraints, for example, to optimize reaction yield. We expect that the RT's capability to jointly tackle predictive and generative tasks in biochemistry can find applications in property-driven, local exploration of the chemical or protein space. Such multitask approaches will pave the road towards foundation models in materials design.</p>
<p>Transformers ${ }^{1}$ are now ubiquitous in natural language processing (NLP) and have also enjoyed large success in molecular ${ }^{2-4}$ and protein language modelling ${ }^{5,6}$. The invention of Transformers was in alignment with the steady decline of inductive biases in machine learning, a trend that started with the rise of deep learning: convolutional neural networks outperformed traditional feature descriptors in object recognition ${ }^{7}$, self-attention generalized dense layers to learn sample-dependent instead of static affine transformations ${ }^{8}$ and Transformers exploited self-attention to supersede recurrent neural networks as the de facto
standard in NLP. The success of vision transformers has questioned the need for translation equivariance in image processing ${ }^{9}$, and now, even frozen Transformers pre-trained on text achieve state-of-the-art results in object detection and protein classification ${ }^{10}$. Given that Transformers are today's most generic model (that is, graph neural networks with multihead attention as neighbourhood aggregation on complete graphs), it is not surprising that attempts have been made to abstract entire domains such as reinforcement learning to sequence modelling in order to leverage Transformers ${ }^{11}$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1 | Overview of RT. The RT is a multitask language model designed to handle combinations of text and numbers. a, Traditional approach in generative chemistry: property predictors and generative models are trained independently from another. b, Our approach: Training the RT yields a dichotomous model that seamlessly transitions between property prediction and conditional text generation. The model's task is to fill the content behind the [MASE] tokens. Depending on the mask location, the same model either predicts numerical tokens given textual tokens, thus performing a regression task (blue stream, top), or predicts textual tokens given both numerical and textual tokens, thus performing a property-driven conditional generation (yellow stream, bottom). c–f, This novel formulation finds application across a wide range of domains. We demonstrate the flexibility of the RT in predictive and generative tasks in modelling small molecules, proteins and chemical reactions and note that it can even be applied to natural text.</p>
<p>A provocative next step towards reducing inductive biases might be to refrain from explicitly modelling target variables as functions of input variables. Instead of following this discriminative modelling approach when tuning task-specific language heads in Transformers, learning the joint distribution over input and target variables could effectively further blur the lines between predictive and conditional generative models. The feasibility of such an approach can be assessed via permutation language modelling (PLM), an extension of masked-language modelling to autoregressive models[12]. Such dichotomous models (that concurrently excel at regression and conditional sequence generation) are beyond applications in NLP of special interest for chemical and material design. Molecules are often labelled with continuous properties (for example, drug efficacy or protein solubility), and design tasks are intertwined with bio- or physicochemical properties. But despite the rise of deep generative models in molecular[13,14] and protein design[15,16], current approaches still develop property predictors and generative models independently. Transformer-based architectures have been used widely on chemical tasks but focused on either property prediction[17,18] or conditional molecular design[19–21]. Typically, they employ large-scale self-supervised pre-training and then fine-tune on different tasks[22,23], but only the Chemformer by ref. 22 addresses regression as well as targeted molecular generation.</p>
<p>However, the ChemFormer tunes task-specific heads and thus does not pose a true multitask model that entangles both tasks seamlessly. This semantic gap persists across architectural flavours (for example, generative adversarial networks (GANs)[24], reinforcement learning[25], variational autoencoders (VAEs)[26], graph neural networks (GNNs)[19,27], flow[28,29] and diffusion models[30]). However, some works performed property-driven generation through probabilistic reparameterization that directly optimize the input to a property prediction model, for example, gradient-based schemes such as PASITHEA[31], differentiable scaffolding trees[32] and activation maximization[33] or multi-objective Bayesian optimization[34] that has been applied to peptide inhibitor[35] and antibody design[36]. Still, to our knowledge, existing Transformers either tune task-specific heads (see, for example, refs. 22,23) or limit the communication between both modules to a reward/loss and thus fail to 'entangle' constrained structure generation with property prediction. This critically violates the intuitive expectation that a property-driven generative model should, in the first place, excel at recognizing this property.</p>
<p>In this Article, we aim to close this gap by reformulating regression as a sequence modelling task. We propose the Regression Transformer (RT), a novel multitask model that can be trained on combinations of numerical and textual tokens (Fig. 1). This circumvents the canonical</p>
<p>way of addressing regression in Transformers, that is, tuning a designated regression head^{37}. Despite solely relying on tokenization of numbers and cross-entropy loss, the RT can successfully solve regression tasks. Notably, the same model can conditionally generate text sequences given continuous properties. This is achieved simply by moving the [MASK] location and does not require fine-tuning specific heads, thus constituting a true multitask model. To equip the RT with an inductive bias for handling floating-point properties, numbers are first tokenized into a sequence of tokens preserving the decimal order. We then devise numerical encodings (NEs) to inform the model about the semantic proximity of these tokens. To allow for concurrent optimization of regression and conditional generation, we derive a PLM-inspired, alternating training scheme that includes a novel self-consistency (SC) loss for improved text generation based on continuous primers.</p>
<p>In the remainder of this paper, we describe the capabilities of the RT on a diverse set of predictive and generative tasks in chemical and protein language modelling. We commence with small-molecule modelling, validate the RT on a synthetic dataset of drug likeness^{38} and then test it on three property prediction datasets from the MoleculeNet benchmark^{39}. The property predictions results are compared with previous approaches relying on a regression loss and demonstrate that regression can be cast as conditional sequence generation task without losing accuracy. These experiments rely on SELFIES^{40}, a chemical language devised for generative tasks that, as we show, has comparable predictive power to SMILES. Although we aim to concurrently excel at predicting properties and generating sequences conditioned on properties, we start training with the PLM objective^{12}, which does not explicitly model those tasks. We then refine this objective and devise a training scheme that alternates between optimizing property prediction and text generation. For the latter, we derive a novel SC loss that exploits the dichotomy of the RT by querying itself with the generated candidate sequence. To assess performance in conditional sequence generation, we systematically vary the continuous properties of interest and investigate the model's ability to adapt a seed sequence according to the primed property value. We show applications on property-driven local chemical space exploration by decorating scaffolds with a continuum of properties and evaluate the novel molecules using the RT itself as well as an independent property predictor^{41}. The RT is then challenged against specialized molecular generative models on a property-driven molecular generation benchmark^{42}, where it substantially outperforms prior art.</p>
<p>Next, the RT is investigated on protein sequence modelling where it matches the performance of conventional Transformers on two regression datasets from the TAPE (Tasks Assessing Protein Embeddings) benchmark^{43}. In experiments on chemical reactions, we notice that the RT constitutes a generalization of forward reaction and retrosynthesis models. We then demonstrate on two reaction datasets that the RT can not only predict reaction yields with similar accuracy to conventional Transformers^{44}, but that it can also substitute specific precursors and thus generate novel reactions with higher predicted yield than a seed reaction.</p>
<h2>Results</h2>
<h3>Chemical language modelling</h3>
<h3>Initial validations—learning drug likeness</h3>
<p>To test the feasibility of concurrent property prediction and conditional generation, we start with optimizing the vanilla permutation language objective (equation (3)) on a synthetic QED (quantitative estimation of drug-likeness) dataset (Extended Data Fig. 1 shows an illustration of the entire workflow, for example, the different objective functions and the autoregressive generation and how the mixed alphanumeric sequences are tokenized and embedded). Since this objective masks tokens randomly in the sequence, evaluating such models on property prediction (that is, masking only numerical tokens as shown in Fig. 1b (top)) does not closely mimic their training dynamics.</p>
<p>Despite this, as well as the unconventional formulation of a regression task as sequence modelling, all models generated sequences of numerical tokens that allowed decoding floats, and even achieved a root mean square error (RMSE) &lt;0.06 (Table 1, top three rows). Instead, for the generative task, the same models were queried ten times for every validation molecule with property ‘primers' equidistantly spaced in [0, 1] and 40% of masked textual tokens. Throughout this manuscript by ‘primers' we mean that we replace the true property of a sequence with a desired property value. The high rank correlation ρ (between primers and QED of unique, generated molecules) values show that the model learned successfully to complete the corrupted sequences to produce full molecules with a desired QED. Notably, the novelty score (that is, the percentage of conditionally generated molecules not present in training data) was &gt;99% for all models. This demonstrates that the RT can generate novel chemical matter that adheres to a continuous property of interest. Moreover, the NEs, an inductive bias to ease learning proximities of numbers (similar to positional encodings^{1}), slightly improved performance in all tasks (for details, see “Numerical Encodings” subsection in Methods). Next, the SELFIES models with and without NEs were refined on the basis of our proposed training scheme with alternating objectives. For both models, two models were fine-tuned using the alternating objective (equation (7)), with (α = 1) and without (α = 0) the SC term in the text loss, respectively (Table 1, bottom section). Interestingly the performance in regression as well as conditional generation improved notably, demonstrating the effectiveness of the refined objectives.</p>
<p>Furthermore, the ablation studies on pre-training or fine-tuning on individual objectives (Table 1, middle) revealed that good performance can be achieved on singular tasks. But the alternating objective enables cross-task benefits that enable the multitask model to outperform single-task models in almost all cases. As might be expected, evaluation queries with large deltas between seed and primed QED lead to lower precision on the generation since they essentially pose an out-of-distribution setting (note that the model was only trained with seed = primer). This is shown in Extended Data Fig. 2, which also reveals that the SC model particularly shines for challenging queries whereas for a pure reconstruction task (that is, primer close to seed) the single-task ‘generate-only' model is advantageous. Moreover, as we report in Extended Data Table 1, all configurations of the RT outperformed a baseline k-nearest neighbour (k-NN) regressor on extended connectivity fingerprints (ECFP^{45}) and our best configuration even surpassed SMILES-BERT^{17}, which achieved a mean absolute error of 0.02 with a regular regression loss and after pre-training on ~9 million SMILES.</p>
<p>The SC term further improved the model's ability to generate tailored ensembles of molecules and led to consistently higher correlation scores. This is exemplarily visualized in Fig. 2 (top) where a single seed molecule is decorated according to the property primers to cover the full range of QED scores.</p>
<p>Generally, the better performance of the SC models (α = 1) in the generative tasks comes at the cost of slightly inferior regression performance (Table 1). Presumably, this is because the model weights in charge of the regression are confounded with the gradients from the self-evaluation (equation (7)). The novelty scores for the molecules generated in this setting were even slightly higher than for the PLM training (&gt;99.3% for all models). A particularly challenging application for property-driven, local exploration of the chemical space is scaffold decoration (that is, adapting a seed molecule while preserving its core structure). For an example on this, see Supplementary Information Section 6.1. Here, the SELFIES models exceeded the SMILES models by far, because SMILES, unlike SELFIES, can be syntactically invalid (we found 60% validity). However, this number can hardly be compared to unseeded generative models because (1) the RT has to remediate a corrupted SMILES and cannot simply rely on its own internal states,</p>
<p>Table 1 | Learning drug likeness</p>
<table>
<thead>
<tr>
<th>Configuration</th>
<th></th>
<th></th>
<th></th>
<th>Regression task</th>
<th></th>
<th>Generation task</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Data</td>
<td>NE</td>
<td>Pre-training</td>
<td>Fine-tuning</td>
<td>RMSE ( $\downarrow$ )</td>
<td>PCC ( $\uparrow$ )</td>
<td>0-Var ( $\downarrow$ )</td>
<td>Spearman's $\rho$ ( $\uparrow$ )</td>
</tr>
<tr>
<td>SMILES</td>
<td>-</td>
<td>PLM</td>
<td>-</td>
<td>$0.055_{\text {u0.01 }}$</td>
<td>$0.972_{\text {u0.01 }}$</td>
<td>$1.6 \%$ u0.3</td>
<td>$0.096_{\text {u0.02 }}$</td>
</tr>
<tr>
<td>SELFIES</td>
<td>-</td>
<td>PLM</td>
<td>-</td>
<td>$0.059_{\text {u0.00 }}$</td>
<td>$0.968_{\text {u0.00 }}$</td>
<td>$0.9 \%$ u0.2</td>
<td>$0.427_{\text {u0.01 }}$</td>
</tr>
<tr>
<td>SELFIES</td>
<td>$\checkmark$</td>
<td>PLM</td>
<td>-</td>
<td>$0.055_{\text {u0.01 }}$</td>
<td>$0.971_{\text {u0.00 }}$</td>
<td>$0.3 \%$ u0.1</td>
<td>$0.467_{\text {u0.01 }}$</td>
</tr>
<tr>
<td>SELFIES</td>
<td>$\checkmark$</td>
<td>Predict ( $\mathcal{L}_{\mathrm{F}}$ )</td>
<td>-</td>
<td>$0.062_{\text {u0.01 }}$</td>
<td>$0.963_{\text {u0.00 }}$</td>
<td>Task unfeasible</td>
<td>Task unfeasible</td>
</tr>
<tr>
<td>SELFIES</td>
<td>$\checkmark$</td>
<td>Generate ( $\mathcal{L}_{\mathrm{G}}$ )</td>
<td>-</td>
<td>Task unfeasible</td>
<td>Task unfeasible</td>
<td>$0.5 \%$ u0.1</td>
<td>$0.358_{\text {u0.00 }}$</td>
</tr>
<tr>
<td>SELFIES</td>
<td>$\checkmark$</td>
<td>PLM</td>
<td>Predict ( $\mathcal{L}_{\mathrm{F}}$ )</td>
<td>$0.030_{\text {u0.01 }}$</td>
<td>$0.991_{\text {u0.01 }}$</td>
<td>$96.4 \%$ u0.0</td>
<td>$0.062_{\text {u0.00 }}$</td>
</tr>
<tr>
<td>SELFIES</td>
<td>$\checkmark$</td>
<td>PLM</td>
<td>Generate ( $\mathcal{L}_{\mathrm{G}}$ )</td>
<td>$0.525_{\text {u0.06 }}$</td>
<td>$0.226_{\text {u0.34 }}$</td>
<td>$0.3 \%$ u0.0</td>
<td>$0.512_{\text {u0.00 }}$</td>
</tr>
<tr>
<td>SELFIES</td>
<td>-</td>
<td>PLM</td>
<td>Alternate ( $\mathcal{L}<em _mathrm_G="\mathrm{G">{\mathrm{F}}$ and $\mathcal{L}</em>$ )}</td>
<td>$0.034_{\text {u0.01 }}$</td>
<td>$0.988_{\text {u0.01 }}$</td>
<td>$0.2 \%$ u0.1</td>
<td>$0.470_{\text {u0.02 }}$</td>
</tr>
<tr>
<td>SELFIES</td>
<td>$\checkmark$</td>
<td>PLM</td>
<td>Alternate ( $\mathcal{L}<em _mathrm_G="\mathrm{G">{\mathrm{F}}$ and $\mathcal{L}</em>$ )}</td>
<td>$0.050_{\text {u0.00 }}$</td>
<td>$0.982_{\text {u0.00 }}$</td>
<td>$0.3 \%$ u0.1</td>
<td>$0.468_{\text {u0.03 }}$</td>
</tr>
<tr>
<td>SELFIES</td>
<td>-</td>
<td>PLM</td>
<td>Alternate with SC ( $\mathcal{L}<em _mathrm_G="\mathrm{G">{\mathrm{F}}$ and $\mathcal{L}</em>$ )}</td>
<td>$0.048_{\text {u0.01 }}$</td>
<td>$0.978_{\text {u0.03 }}$</td>
<td>$0.3 \%$ u0.1</td>
<td>$0.490_{\text {u0.01 }}$</td>
</tr>
<tr>
<td>SELFIES</td>
<td>$\checkmark$</td>
<td>PLM</td>
<td>Alternate with SC ( $\mathcal{L}<em _mathrm_G="\mathrm{G">{\mathrm{F}}$ and $\mathcal{L}</em>$ )}</td>
<td>$0.037_{\text {u0.03 }}$</td>
<td>$0.987_{\text {u0.03 }}$</td>
<td>$0.2 \%$ u0.1</td>
<td>$0.517_{\text {u0.02 }}$</td>
</tr>
</tbody>
</table>
<p>Different configurations of the RT on concurrent learning of predicting drug likeness and generating drug-like molecules. The first block contains models trained with the task-agnostic PLM objective. The second block contains ablation studies on single-task models exclusively trained on either the predictive or the generative objective. The third block contains molecules that were pre-trained on the PLM objective and then fine-tuned using the alternating objective. RMSE ( $\downarrow$ ) and PCC refer to predicting QED, whereas Spearman's $\rho$ ( $\uparrow$ ) and 0-Var ( $\downarrow$ ) to the conditional generation task. S.d. values across repeated runs are shown. Numbers computed on 10,000 test samples. Best model shown in bold, second-best underlined. (2) the concurrently provided property primers capture the entire range of low to high QED scores, thus incentivizing the model to decorate the sequence adventurously to adhere to the constrained property-a task that is often impossible and can easily lead to broken SMILES and (3) the RT training did not rely on teacher forcing. Due to the comparable results for property prediction (Table 1, top three rows), the remaining experiments focus exclusively on SELFIES. But even though SELFIES are designed to be always valid, they can also break by converting long sequences to short, stub-like molecules. We assessed the frequency of this scenario by defining a generation as defective if the obtained molecule had $&lt;50 \%$ of the atoms of the seed molecule. This yielded $\sim 1.9 \%$ defective generations across $\sim 300,000$ generations. Regarding chemical sensibility, we observed that the 1,000 most common functional groups ${ }^{46}$ are reproduced in the generated molecules (Supplementary Fig. 1). Further ablation studies on different types of NE and related work on encoding numbers with Transformer are reported in Supplementary Information Section 1.</p>
<p>Learning embeddings of numbers. We sought to understand why the ablation studies on the NEs on the QED dataset (Table 1) reveal only mild superiority of models with NEs. Interestingly, as visualized in Extended Data Fig. 3, in the absence of static NEs, the model learns the natural ordering of digits from the data. A large number of embedding dimensions ( $47 \%$ and $36 \%$ for the decimal places -1 and -2 , respectively) directly and significantly encoded the ordering of digits (that is, $P&lt;0.05$ and $|\mathrm{PCC}|&gt;0.62$ between the ten embedding values and a strictly monotonic vector). For example, in Extended Data Fig. 3 (left), the digit value is monotonically related to its embedding value. In general, attention weights in Transformers can capture complex semantics such as protein folding structure ${ }^{47}$ or atom mapping in</p>
<p>Table 2 | Constrained property optimization benchmark</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Generation task</th>
<th></th>
<th></th>
<th>Regression</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Improvement</td>
<td>Similarity $\delta$</td>
<td>Success</td>
<td>PCC</td>
</tr>
<tr>
<td>(a) Similarity threshold $\delta=0.4$</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>JT-VAE ${ }^{(6)}$</td>
<td>$0.84_{01,5}$</td>
<td>$0.51_{00,1}$</td>
<td>$83.6 \%$</td>
<td>Task unfeasible</td>
</tr>
<tr>
<td>GCPN ${ }^{(6)}$</td>
<td>$2.49_{01,5}$</td>
<td>$0.47_{00,1}$</td>
<td>$100 \%$</td>
<td>Task unfeasible</td>
</tr>
<tr>
<td>MoFlow ${ }^{(6)}$</td>
<td>$4.71_{04,5}$</td>
<td>$0.61_{00,2}$</td>
<td>$85.7 \%$</td>
<td>Task unfeasible</td>
</tr>
<tr>
<td>BT ${ }^{(6)}$</td>
<td>4.21</td>
<td>NA</td>
<td>NA</td>
<td>Task unfeasible</td>
</tr>
<tr>
<td>RT (ours)</td>
<td>$3.16_{01,5}$</td>
<td>$0.54_{00,1}$</td>
<td>$97.1 \%$</td>
<td>$0.92_{00,0}$</td>
</tr>
<tr>
<td>(b) Similarity threshold $\delta=0.6$</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>JT-VAE ${ }^{(6)}$</td>
<td>$0.21_{00,1}$</td>
<td>$0.69_{00,0}$</td>
<td>$46.4 \%$</td>
<td>Task unfeasible</td>
</tr>
<tr>
<td>GCPN ${ }^{(6)}$</td>
<td>$0.79_{00,6}$</td>
<td>$0.68_{00,1}$</td>
<td>$100 \%$</td>
<td>Task unfeasible</td>
</tr>
<tr>
<td>MoFlow ${ }^{(6)}$</td>
<td>$2.10_{00,6}$</td>
<td>$0.79_{00,1}$</td>
<td>$58.3 \%$</td>
<td>Task unfeasible</td>
</tr>
<tr>
<td>BT ${ }^{(6)}$</td>
<td>2.77</td>
<td>NA</td>
<td>NA</td>
<td>Task unfeasible</td>
</tr>
<tr>
<td>RT (ours)</td>
<td>$2.21_{01,5}$</td>
<td>$0.69_{00,1}$</td>
<td>$81.8 \%$</td>
<td>$0.92_{00,0}$</td>
</tr>
</tbody>
</table>
<p>Best model marked in bold, second-best underlined. Standard deviations are given. Full table with different configurations in Supplementary Table 3. NA means "not available". chemical reactions ${ }^{4}$. For a qualitative comparison of the RT's attention across the predictive and generative task, see Supplementary Information Section 2.</p>
<h2>Regression benchmark (MoleculeNet)</h2>
<p>After the successful initial experiments, we evaluated the RT on three regression benchmarks from MoleculeNet ${ }^{39}$. The regression performance on ESOL, FreeSolv and Lipophilicity is shown in Extended Data Table 2 and compared with prior work. The strongest baseline model from MoleculeNet, XGBoost, is outperformed by all our models on all tasks. Even the MPNN ${ }^{40}$, a message-passing GNN, is slightly surpassed on FreeSolv and Lipophilicity by some of our models. However, all our models are outperformed by BERT ${ }^{40}$ and BART ${ }^{32}$. Notably, these models leveraged large-scale self-supervised pre-training before fine-tuning a regression head, whereas we use a classification loss. Since these results might not be directly comparable to the RT with its XLNet backbone, we also fine-tuned a XLNet model with a conventional regression head. Notably, despite the absence of a regression loss, the RT is on par (Lipophilicity) or only mildly inferior (that is, within s.d. range; ESOL, FreeSolv) to XLNet.</p>
<p>However, in stark contrast to all those approaches, only the RT can also be used to conditionally generate molecules similar to the training samples (Extended Data Table 3). Since the properties of the generated molecules are intractable to evaluate in silico, we could predict them, handily, using the RT. However, as this might be a biased estimator, we additionally evaluated them using Grover ${ }^{41}$, a self-supervised Graph Transformer that relies on large-scale pre-training. Extended Data Table 3 presents the performance in conditional molecular generation, which underlines the benefit of the SC loss $(\alpha=1)$ and demonstrates that the RT can adapt unseen seed molecules even according to complex molecular properties such as water solubility. Corroborative for our work is the high correlation of our property predictions (RT) with Grover's for molecules generated by the ESOL, FreeSolv and Lipo models ( $0.86,0.84$ and 0.75 , respectively). For a qualitative evaluation, we depict the generations for one exemplary seed molecule of the solubility dataset in Fig. 2 (bottom). Lastly, we found $1.3 \%$ defective generations, which is comparable to or lower than in the QED dataset.</p>
<h2>Conditional molecular generation benchmark</h2>
<p>To assess whether the RT is a powerful conditional generative model, we benchmarked it on a property-driven molecular generation task, namely penalized $\log P$ (plog $P$; definition in Methods) constrained</p>
<p>Table 3 | Results on protein language modelling</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Source</th>
<th>Boman</th>
<th>Fluorescence</th>
<th>Stability</th>
</tr>
</thead>
<tbody>
<tr>
<td>(a) Protein regression tasks</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$k$-NN</td>
<td>Baseline</td>
<td>0.93</td>
<td>0.59</td>
<td>0.21</td>
</tr>
<tr>
<td>One-Hot</td>
<td>TAPE</td>
<td>NA</td>
<td>0.14</td>
<td>0.19</td>
</tr>
<tr>
<td>LSTM</td>
<td>TAPE</td>
<td>NA</td>
<td>0.67</td>
<td>0.69</td>
</tr>
<tr>
<td>Transformer</td>
<td>TAPE</td>
<td>NA</td>
<td>0.68</td>
<td>0.73</td>
</tr>
<tr>
<td>UniRep</td>
<td>54</td>
<td>NA</td>
<td>0.67</td>
<td>0.73</td>
</tr>
<tr>
<td>ProteinBERT</td>
<td>55</td>
<td>NA</td>
<td>0.66</td>
<td>0.76</td>
</tr>
<tr>
<td>RT $\left(C_{\mathrm{SC}}\right)$</td>
<td>Ours</td>
<td>$0.99_{00,01}$</td>
<td>$0.72_{00,04}$</td>
<td>$0.71_{00,02}$</td>
</tr>
<tr>
<td>Model</td>
<td>Boman dataset</td>
<td></td>
<td>Stability dataset</td>
<td></td>
</tr>
<tr>
<td></td>
<td>0-Var ( $\downarrow$ )</td>
<td>Spearman's $\rho$</td>
<td>0-Var ( $\downarrow$ )</td>
<td>Spearman's $\rho$</td>
</tr>
<tr>
<td>(b) Protein generation tasks</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>All TAPE</td>
<td>Task unfeasible</td>
<td></td>
<td>Task unfeasible</td>
<td></td>
</tr>
<tr>
<td>UniRep</td>
<td>Task unfeasible</td>
<td></td>
<td>Task unfeasible</td>
<td></td>
</tr>
<tr>
<td>RT (PLM)</td>
<td>$0.3 \%{ }_{00,0}$</td>
<td>$0.76_{00,03}$</td>
<td>$40 \%{ }_{44,2}$</td>
<td>$0.00_{00,00}$</td>
</tr>
<tr>
<td>RT $\left(C_{G}\right)$</td>
<td>$0.2 \%{ }_{00,1}$</td>
<td>$0.82_{00,01}$</td>
<td>$31 \%{ }_{33,5}$</td>
<td>$0.30_{00,06}$</td>
</tr>
<tr>
<td>RT $\left(C_{\mathrm{SC}}\right)$</td>
<td>$0.2 \%{ }_{00,1}$</td>
<td>$0.84_{00,00}$</td>
<td>$19 \%{ }_{44,5}$</td>
<td>$0.44_{00,01}$</td>
</tr>
</tbody>
</table>
<p>(a) Protein property prediction (regression). All values in Spearman's $\rho(\uparrow)$ on the test set. TAPE datasets/performances taken from ref. 43. An ablation study on the three loss functions (equations (3), (6)) and (7)) confirmed the superiority of the SC objective (Supplementary Information Section 4.1 and Supplementary Table 4). Best performance per dataset shown in bold. (b) Protein generation. Sixty per cent of the residues were masked. Boman index was computed directly, whereas stability was predicted with the RT itself. Best performance per dataset shown in bold. S.d. values measured across three runs. optimization ${ }^{42}$. Given a seed molecule and a similarity constraint to the seed molecule ( $\delta$, given in Tanimoto similarity), the goal is to generate molecules with higher plog $P$ values. The results in Table 2 demonstrate that, for both similarity thresholds $\delta$, the RT performs competitive to state-of-the-art models; for example, it outperforms a Junction-Tree-VAE ${ }^{42}$ and a graph-convolutional policy network (GCPN) ${ }^{30}$ by $614 \%$ and $103 \%$ in average improvement, respectively.</p>
<p>It falls behind the Back Translation (BT) model ${ }^{31}$ on average improvement; however, care has to be taken on their results since other metrics and s.d. values are not reported. The RT performs comparably to the MoFlow model ${ }^{32}$, while our results for $\delta=0.4$ are inferior, the unconstrained generation results ( $\delta=0.0$ ) are in favour of our method (Supplementary Information Section 3). Moreover, these comparisons are not truly fair because all competing methods have a training procedure that rewards generating molecules with high plog $P$ and some methods even apply gradient optimization schemes at inference time (GCPN and JT-VAE). This is in stark contrast to the RT training, which rewards only if the reconstructed molecule has a similar (predicted) plog $P$ to the seed molecule (we did not construct directed plog $P$ queries for the training; they were used only at inference time). Thus, the RT is agnostic in valence and could equally be used to adapt molecules towards lower plog $P$. Overall, this experiment demonstrates that the RT is able to compete with specialized conditional generative models in goal-directed molecular generation. At the same time, the RT also predicted the plog $P$ value with a Pearson's correlation coefficient (PCC) of 0.92 , a task that cannot be addressed with normal conditional generative models. The results in Table 2 were obtained with the RT including a SC loss, but for ablation studies on the RT and further results on $\delta=0.2$ and $\delta=0$, see Supplementary Information Section 3.</p>
<h2>Protein sequence language modelling</h2>
<p>Pre-training on potential protein interaction (Boman index). To assess the generality of the RT beyond chemical languages, we benchmarked the RT in protein language modelling. On the synthetic</p>
<p>Table 4 | Chemical reaction modelling</p>
<table>
<thead>
<tr>
<th>Model</th>
<th></th>
<th>Buchwald-</th>
<th>Suzuki</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Hartwig</td>
<td>coupling</td>
<td></td>
</tr>
<tr>
<td>(a) Reaction yield prediction</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>One-Hot ${ }^{(6)}$</td>
<td></td>
<td>0.89</td>
<td>NA</td>
<td></td>
</tr>
<tr>
<td>DFT ${ }^{(9)}$</td>
<td></td>
<td>0.92</td>
<td>NA</td>
<td></td>
</tr>
<tr>
<td>MFF ${ }^{(6)}$</td>
<td></td>
<td>$0.927_{&lt;0.01}$</td>
<td>NA</td>
<td></td>
</tr>
<tr>
<td>Yield-BERT ${ }^{(6)}$</td>
<td></td>
<td>$0.951_{&lt;0.01}$</td>
<td>$0.79_{&lt;0.02}$</td>
<td></td>
</tr>
<tr>
<td>Yield-BERT fine-tuned</td>
<td></td>
<td>$0.951_{&lt;0.01}$</td>
<td>$0.81_{&lt;0.01}$</td>
<td></td>
</tr>
<tr>
<td>RT (ours)</td>
<td></td>
<td>$0.939_{&lt;0.01}$</td>
<td>$0.81_{&lt;0.02}$</td>
<td></td>
</tr>
<tr>
<td>Dataset</td>
<td>Precursor</td>
<td>Reconstruction</td>
<td></td>
<td>Decoration</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Top-three</td>
<td>Similarity</td>
<td>Success</td>
</tr>
<tr>
<td></td>
<td></td>
<td>accuracy</td>
<td>$\delta$</td>
<td>rate</td>
</tr>
<tr>
<td>(b) Generating novel precursors for unseen reactions</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Buchwald Hartwig</td>
<td>Halide</td>
<td>$98.23 \%{ }_{&lt;0.5}$</td>
<td>$0.991_{&lt;0.00}$</td>
<td>$42.3 \%{ }_{&lt;2.4}$</td>
</tr>
<tr>
<td></td>
<td>Ligand</td>
<td>$50.38 \%{ }_{&lt;1.6}$</td>
<td>$0.677_{&lt;0.01}$</td>
<td>$74.4 \%{ }_{&lt;4.2}$</td>
</tr>
<tr>
<td></td>
<td>Base</td>
<td>$100 \%{ }_{&lt;0.0}$</td>
<td>$1.000_{&lt;0.00}$</td>
<td>$82.2 \%{ }_{&lt;2.3}$</td>
</tr>
<tr>
<td></td>
<td>Additive</td>
<td>$1.36 \%{ }_{&lt;0.6}$</td>
<td>$0.158_{&lt;0.02}$</td>
<td>$71.2 \%{ }_{&lt;1.8}$</td>
</tr>
<tr>
<td>Suzuki crosscouplings</td>
<td>Electrophile</td>
<td>$44.2 \%{ }_{&lt;0.6}$</td>
<td>$0.732_{&lt;0.02}$</td>
<td>$63.5 \%{ }_{&lt;7.1}$</td>
</tr>
<tr>
<td></td>
<td>Nucleophile</td>
<td>$100.0 \%{ }_{&lt;0.0}$</td>
<td>$1.000_{&lt;0.00}$</td>
<td>$54.0 \%{ }_{&lt;6.2}$</td>
</tr>
<tr>
<td></td>
<td>Ligand</td>
<td>$67.4 \%{ }_{&lt;0.0}$</td>
<td>$0.689_{&lt;0.15}$</td>
<td>$56.7 \%{ }_{&lt;3.5}$</td>
</tr>
<tr>
<td></td>
<td>Base</td>
<td>$90.5 \%{ }_{&lt;1.2}$</td>
<td>$0.811_{&lt;0.01}$</td>
<td>$47.8 \%{ }_{&lt;2.7}$</td>
</tr>
<tr>
<td></td>
<td>Solvent</td>
<td>$56.4 \%{ }_{&lt;1.1}$</td>
<td>$0.661_{&lt;0.01}$</td>
<td>$57.8 \%{ }_{&lt;1.8}$</td>
</tr>
</tbody>
</table>
<p>For the yield prediction, performance for ten 70/30 splits, measured in coefficient of determination $\left(R^{2}\right)$ with s.d. is shown. For the generative task, we explore reconstruction and decoration of the reactions. For reconstruction, we show the percentage of cases where the exact right precursor was among the top-three predicted sequences and the Tanimoto similarity of the most similar of those molecules. For decoration, we show the percentage of cases where the top-five predicted reactions contained a reactions with higher (predicted) yield than the seed reaction (success rate), alongside the associated average yield improvement. Full precursors were generated ( $p_{\text {read }}=1$ ). S.d. values across ten runs are shown. For the BH aminations, each reaction included the same palladium catalyst, which is thus excluded from this analysis. For the Suzuki couplings, each reaction also contained 4-methylaniline and the same palladium catalyst, which are also excluded from the analysis.
pre-training data, the RT obtained nearly perfect results in predicting Boman's index (Spearman's $\rho&gt;0.994$; Table 3) and outperformed a baseline $k$-NN using Levenshtein distance ${ }^{(2)}$. But the RT also successfully generated peptides with a desired Boman index, given a partially corrupted amino acid sequence (Spearman's $\rho$ of 0.84 ; Table 3b). Moreover, a higher fraction of masked tokens lead to better results in protein generation tasks (Supplementary Fig. 2).</p>
<p>TAPE datasets (protein fluorescence and protein stability). Next, the RT performed competitively on two realistic protein regression datasets from TAPE (Table 3). This is remarkable given that the TAPE models were pre-trained large scale on unlabelled protein sequences and fine-tuned with a regression loss. For example, the RT outperforms all reported methods in Spearman's correlation on the fluorescence task, which has a distribution with two modes, for bright and dark proteins, respectively. Inspecting the predictions in more depth showed that the RT, compared with other methods, excels at recognizing the mode of a protein but struggles with intra-mode precision (Supplementary Information Section 7.2). Across both datasets, the RT performs on par or superior to the TAPE Transformer ${ }^{(4)}$, UniRep ${ }^{(5)}$ and the contemporary ProteinBERT ${ }^{(2)}$ model, pre-trained on 31 million, 24 million and 106 million protein sequences, respectively ( 2.6 million in our case). However, scaling this pre-training to evolutionary-scale protein language models would probably displace UniRep as well as the RT as evolutionary-scale protein language models was recently demonstrated to have strong zero-shot generalization performance ${ }^{(6)}$.</p>
<p>Overall, the competitive predictive performance of the RT demonstrates that the benefits of self-supervised pre-training can extend to
numerically labelled datasets. This yields, en passant, a conditional generative model for property-driven local exploration of the protein sequence space. Evidence on this can be found in Table 3b: Whereas all TAPE models as well as the UniRep method are incapable of addressing this generation task, the RT was able to modify the test proteins such that their (predicted) stability correlated strongly with the primed property $(\rho=0.44)$.</p>
<h2>Modelling chemical reactions</h2>
<p>Language models advanced reaction chemistry dramatically ${ }^{6,37}$ and, among others, showed superior performance on yield prediction ${ }^{(4)}$, yet models incorporating yield into (partial) reaction generation are lacking entirely. Such models could be used to (1) identify entirely novel reactions by substituting a specific precursor type with a higher yield, (2) cure erroneous reactions by identifying missing precursors in databases of specific reaction types or (3) infer reagents or solvents in reactions that only specify main compounds.</p>
<p>We therefore optimized the RT for concurrent yield prediction and precursor generation on two reaction-yield datasets: Buchwald-Hartig aminations ${ }^{38}$ and Suzuki-Miyaura cross-couplings ${ }^{39}$. All experiments relied on the alternated training scheme with SC loss. On yield prediction, the RT (trained on SELFIES) outperforms fingerprint-based or quantum mechanics methods, and matches (Suzuki dataset) or almost matches (Buchwald dataset) the performance of language models such as Yield-BERT, trained with regression loss on SMILES (Table 4).</p>
<p>The same model learned to reconstruct missing precursors in Buchwald-Hartwig animations, which can be useful to infer missing solvents or reagents in automatically extracted reactions</p>
<p>(Table 4b). This is partly achieved with great accuracy (for example, 98.2% for aryl-halides). Interestingly, inferring additives proved challenging, possibly because they are the dominant precursor type for the reaction yield^{59}. However, upon masking the additive only partially (rather than completely), the reconstruction performance increases significantly (ablation study with p_{mask} ∈ [0.25, 0.5, 1] in Supplementary Table 5). On the Suzuki couplings, the reconstruction results are more balanced among the five precursor types; the average Tanimoto similarity to the true precursor was &gt;0.65 in all cases (Table 4b). Moreover, across both datasets we observed mild benefits in reconstruction performance when providing the true yield rather than masking it (Supplementary Tables 6 and 7). In addition to yield prediction and precursor reconstruction, the RT can also decorate existing reactions by adapting specific precursors towards a higher yield (Table 4b). Consistently among both datasets and all precursor types, 40--80% of the top-five predicted sequences contained reactions with entirely novel precursors and higher predicted yield.</p>
<p>Extended Data Fig. 4 visualizes exemplary adaptations of each precursor type of a BH amination with very low yield (&lt;5%). Notably, for this unseen reaction, the RT found novel adaptations of each of the four precursor types that resulted in an increase of predicted yield by 11--85%. With the forward reaction prediction model in IBM RXN^{2}, we confirmed that all reactions indeed result in the desired product. Notably, the confidence from the forward model rank-correlated almost perfectly with the yield predicted by the RT (ρ = 0.90, P &lt; 0.05).</p>
<h2>Discussion</h2>
<p>The herein presented RT demonstrated that regression can be cast as conditional sequence learning task. We introduced a flexible multitask language model with wide application in scientific discovery. Our main contribution is a multitask transformer that bridges previously considered disjoint tasks (property prediction and conditional generation) without the need of tuning task-specific heads. This model shines at both tasks and facilitates highly customizable molecular generation (for details, see ‘Usage of trained models' in the Code availability section). This could pave the road towards foundation models in material design.</p>
<p>Regarding molecular property prediction, we find that the RT learns continuous properties even from small datasets, surpasses conventional regression models on several benchmarks and sometimes competes with Transformers trained on regression loss. Remarkably, this is achieved without providing ratio-scale information about the property, potentially even challenging the necessity of using regression rather than classification objectives.</p>
<p>The experiments on conditional text generation underline the versatility of the RT. Across a wide range of tasks, we conditionally generated novel sequences (molecules, proteins and reactions) that seemingly adhere to primed, continuous properties. Our experiments on constrained molecular generation benchmark further demonstrate that the RT can surpass specialized conditional generative models. We foresee this to impact property-driven and substructure-constrained molecular or protein design tasks. In the recent work by ref. 60, the RT has been applied in polymer chemistry for the generation of novel ring-opening polymerization catalysts as well as block and statistical co-polymers. In both cases, successful experimental validation confirmed the ability of the RT to accelerate real discovery workflows.</p>
<p>Moreover, even though all experiments reported herein examined singular properties, the RT naturally scales to multi-property prediction (see ‘GUI Demo' in the Code availability section on how to access pre-trained multi-property models).</p>
<p>While we build the RT upon XLNet, any decoder that combines the benefits of masked language modelling (MLM) and causal, autoregressive language modelling could serve as a backbone (for example, T5 with its sentinel tokens^{61}, MPNet^{62}, InCoder^{63} or FIM^{64}). Future work could evaluate the RT on such backbones, intensify the work on reaction modelling (the RT effectively generalizes forward reaction and retrosynthesis models) or improve the ability of the RT to perform fine-grained regression (for an interesting failure mode, see Supplementary Information Section 7.1). Another prospect is to investigate property-constrained but unseeded molecular generation for more global chemical space exploration. Finally, our work resonates with the recent trend towards multitask Transformers^{65--67}, and we envision it as a means to accelerate the development of foundation models for scientific discovery applications.</p>
<h2>Methods</h2>
<p>In this section we first describe the different components of our methodology (architectural choices, tokenization scheme, NEs and training objectives). We then describe the implementation details for both training and evaluation.</p>
<h3>XLNet backbone</h3>
<p>Language models utilize either a causal (that is, left-to-right), autoregressive training objective such as recurrent neural networks and GPT-3 (ref. 67) or use MLM such as BERT^{27}. Autoregressive approaches are preferable for generating long sequences (for example, entire documents), but since such causal models only condition on previous tokens, they cannot be applied to text infilling tasks and cannot profit from MLM pre-training. Instead, MLMs such as BERT condition on the entire sequence to fill masked tokens, making them appear a good choice for infilling tasks; however, MLM approaches fail to generate longer sequences due to their independence assumption. To unify both worlds and retain the benefits of autoregressive modelling in combination with a bidirectional context, several methods have been proposed, with XLNet^{12} being the first prominent one. The RT is built upon an XLNet backbone that is an autoregressive language model, but due to its novel training objective, it, in expectation, obtains full bidirectional attention. This bidirectionality is critical because the RT is required to fill multiple tokens at arbitrary positions in a sequence while attending the full remaining sequence (for example, SMILES/SELFIES are non-local sequences such that masking functional groups usually implies masking disconnected tokens). Moreover, the independence assumption in bidirectional but non-autoregressive models (such as BERT) becomes increasingly disruptive as more masked tokens are filled, making XLNet a great choice. This limits BERT's applicability for generative tasks in biochemistry such as scaffold decoration where large portions of a molecule might be masked and generation of individual atoms can critically alter the molecule's functional properties. In general, it is important to notice that the proposed framework can be applied to all transformer flavours, but it certainly benefits from an autoregressive generation with full sequence attention even for discontiguous mask locations. Such approaches rely on either a PLM like XLNet or MPNet^{62} or on sentinel tokens replacing code spans that are then predicted at the end of a sequence with an autoregressive approach like in T5 (ref. 61), InCoder^{63} or fill-in-the-middle^{64}. Further information on the implementation and the model hyperparameters can be found below in the “Model training and evaluation procedure” section.</p>
<h3>Tokenization</h3>
<p>This section describes the processing of alphanumeric sequences, that is, strings consisting of a mixture of numerical and textual symbols (for a visualization of the tokenization, see Extended Data Fig. 1, top). Unlike previous approaches that modelled 8-bit integers with a classifier^{68}, we strive to represent real numbers with arbitrary floating point precision. Since representing every number as a single token is suboptimal due to a lack of generalization to new numbers and sparsity of the provided tokens, we formulated regression as sequential classification task. In turn, this necessitates a scheme for converting text representing numbers into a sequences of tokens. First, the following regular expression splits a string denoting a numerical:$$p_{mask} = \frac{1}{2} \sum_{i} \sum_{j} \sum_{k} \sum_{l} \sum_{m} \sum_{n} \sum_{o} \sum_{p} \sum_{q} \sum_{t} \sum_{t} \sum_{d} \sum_{s} \sum_{d} \sum_{t} \sum_{d} \sum_{s} \sum_{d} \sum_{d} \sum_{s} \sum_{d} \sum_{s} \sum_{d} \sum_{s} \sum_{d} \sum_{s} \sum_{d} \sum_{d} \sum_{s} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d} \sum_{d}</p>
<p>Each of the resulting matches containing a number is converted to a token $t_{e, p}$ where $v \in \mathbb{N} \cap{0.9}$ is the value/digit and $p \in \mathbb{Z}$ is the decimal place (for example, 12.3 is split into [1_1, 2_0, 3_1]). We call these numerical tokens. This representation has the advantage that it allows easy decoding of the digit sequence but also distinguishes their decimal order by adhering to classic positional notation. Negative numbers are preceded with a special token. Regarding alphabetic tokens, we represent molecules as SELFIES ${ }^{50}$ strings and tokenized them with their internal tokenizer. In one ablation study, we instead use SMILES ${ }^{59}$ and tokenize with the regular expression from ref. 57. Protein sequences are tokenized per amino acid.</p>
<h2>Numerical Encodings (NE)</h2>
<p>Due to the inherent structure of numbers, learning the embeddings of numerical tokens in a purely data-driven way might be ineffective. Moreover, since the RT is trained with cross-entropy loss, no notion of similarity between numerical tokens is conveyed. As a remedy, we propose NEs, a simple inductive bias about the semantic proximity of numerical tokens, similar to positional encodings ${ }^{1}$. Our proposed NEs are zero vectors for all but numerical tokens of the dictionary. We follow positional notation as above. Given a token $t_{e, p}$ (with digit value $v$ and decimal place $p$ ), the NE at embedding dimension $j$ is defined as</p>
<p>$$
\mathrm{NE}_{\text {float }}(v, p, j)=(-1)^{j} \cdot \frac{v \cdot 10^{p}}{j+1}
$$</p>
<p>Thus, the amplitude of the NE scales with the numerical value of the token. This scheme can be applied to any floating point value $x \in \mathbb{R}$. The encodings are also independent of the sign of the number. Hence, they equally convey proximity between positive and negative numbers. The NEs are perfectly correlated among embedding dimensions but alternate between positive and negative values for even and odd dimensions and vanish for higher dimensions (see example in Extended Data Fig. 5a). Critically, the pairwise distances of the NEs are symmetric and decay monotonically with the float value (Extended Data Fig. 5b). In practice, we sum the NEs with regular word embeddings and relative positional encodings from XLNet (for workflow, see Extended Data Fig. 1). Note that we also experimented with integer-based NEs (for additional experiments, see Supplementary Material Section 1).</p>
<h2>Training objectives</h2>
<p>The input $\mathbf{x}$ for an RT is defined by a concatenation of $k$ property tokens $\left[\mathbf{x}^{p}\right]<em p="p">{k}$ and $l$ textual tokens $\left[\mathbf{x}^{l}\right]</em>\right]}$ such that $\mathbf{x}=\left[\mathbf{x}^{p}, \mathbf{x}^{k<em 1="1">{T}=\left[x</em>$ are property and textual tokens, respectively. For a high-level overview of the training objectives, see Extended Data Fig. 1 (bottom).}^{p}, \ldots, x_{j}^{p}, x_{1}^{k}, \ldots, x_{l}^{k}\right]_{T}$. The full sequence length is $T=k+l$, and $\mathbf{x}^{p}$ and $\mathbf{x}^{l</p>
<p>PLM objective. The idea of PLM ${ }^{12}$ is to fill masked tokens autoregressively by sampling a factorization order $\mathbf{z}$ for a sequence $\mathbf{x}$ at runtime. Decomposing the likelihood $p_{\theta}(\mathbf{x})$ according to the facorization order yields, in expectation, a bidirectional autoregressive model. Let $\mathbf{z} \in \mathbb{Z}<em i="i">{T}$ denote one of the $T$ l permutations of our sequence $\mathbf{x}$. If $z</em>$, the PLM objective is}$ and $\mathbf{z}_{i}$, are the $i$-th and first $i-1$ elements of $\mathbf{z</p>
<p>$$
\max <em _mathbf_z="\mathbf{z">{\theta} \mathrm{E}</em>} \sim \mathbb{Z<em i="1">{T}}\left[\sum</em>}^{T} \log p_{\theta}\left(x_{z_{i}} \mid \mathbf{x<em i_="i+">{\mathbf{z}</em>\right)\right]
$$}</p>
<p>In practice, partial prediction is performed. That is, only the last $c$ tokens of the factorization order $\mathbf{z}$ are predicted. Following XLNet, $\mathbf{z}$ is split into a (masked) target subsequence $\mathbf{z}<em i="i" n="n">{i n}$ and an unmasked input sequence $\mathbf{z}</em>$ such that the objective becomes</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _theta="\theta">{\mathrm{PLM}} &amp; =\max </em>} \mathrm{E<em T="T">{\mathbf{z} \sim \mathbb{Z}</em>}}\left[\log p_{\theta}\left(\mathbf{x<em _mathbf_z="\mathbf{z">{\mathbf{z},&gt;} ; \mathbf{x}</em><em _mathbf_z="\mathbf{z">{i n}}\right)\right] \
&amp; =\mathrm{E}</em>} \sim \mathbb{Z<em t="c+1">{T}}\left[\sum</em>}^{T} \log p_{\theta}\left(x_{z_{i}} \mid \mathbf{x<em i="i" n="n">{\mathbf{z}</em>\right)\right]
\end{aligned}
$$}</p>
<p>where $c$ is a hyperparameter, usually sampled per batch such that the fraction of masked tokens is roughly $1 / c$. We notice that equation (4) does not make any specific choices on $\mathbf{x}^{p}$ and $\mathbf{x}^{l}$. It thus constitutes our baseline objective. While equation (4) is a generic objective, it is computationally exhaustive to optimize due to the permutations. Moreover, it is not ideal for our needs because it does not distinguish between textual and property tokens. Instead, we are aiming to develop a single model that can predict either numerical tokens (when given text sequences) or text tokens (when given a combination of numerical and text tokens). To that end, we propose to train on two alternating objectives, one designed for property prediction and one for text generation.</p>
<p>Property prediction objective. Instead of randomizing which tokens are masked, this objective exclusively masks all the property tokens. Specifically, we constrain the factorization order $\mathbf{z}$ by setting the first $l$ elements to $\mathbf{x}^{k}$ and fixing $c=l$. This guarantees that only property tokens are masked. Let $z_{T}^{p}$ denote the set of possible permutations. Under this constraint, the objective then becomes</p>
<p>$$
\begin{aligned}
&amp; \mathcal{L}<em _theta="\theta">{\mathrm{P}}=\max </em>} \mathrm{E<em T="T">{\mathbf{z} \sim \mathbb{Z}</em>\right)\right] \
&amp; =\mathrm{E}}^{p}}\left[\log p_{\theta}\left(\mathbf{x}^{p} \mid \mathbf{x}^{l<em T="T">{\mathbf{z} \sim \mathbb{Z}</em>}^{p}}\left[\sum_{t=c+1}^{T} \log p_{\theta}\left(x_{z_{i}}^{p} \mid \mathbf{x<em i_="i+">{\mathbf{z}</em>}}^{p}, \mathbf{x<em i_n_1="i+n+1">{\mathbf{z}</em>\right)\right]
\end{aligned}
$$}}^{p</p>
<p>where $\mathbf{x}<em i_n="i+n">{\mathbf{z}</em>$ is still optimized with a cross-entropy loss in practice. Note that this loss cannot convey any notion on the qualitative proximity of the prediction to the labels because the level of measurement of tokens in a language model are on a nominal level. Thus, predicting a sequence of numerical tokens corresponding to a property score of 0.91 for a sample with a true property of 0.11 will not generally result in a higher loss than predicting 0.21 . Instead, a traditional regression loss operates on a ratio scale.}}^{p}$ denotes the $c$-th to the $(i-1)$ th element of the factorization order $\mathbf{z}$. We emphasize that this 'tailored' property objective $\mathcal{L}_{\mathrm{p}</p>
<p>Conditional text generation objective. This objective facilitates the generation of textual tokens given a property primer and textual tokens. We constrain the factorization order $\mathbf{z}$ by setting the first $k$ elements to $\mathbf{x}^{p}$ to and sampling the cut-off $c$, such that $c \geq k$. This ensures that masking occurs only on textual tokens. With this constraint, we denote the set of permutations by $\mathbb{Z}_{T}^{c}$ and the objective becomes</p>
<p>$$
\begin{aligned}
&amp; \mathcal{L}<em _theta="\theta">{\mathrm{G}}=\max </em>} \mathrm{E<em T="T">{\mathbf{z} \sim \mathbb{Z}</em>}^{c}}\left[\log p_{\theta}\left(\mathbf{x<em _mathbf_z="\mathbf{z">{\mathbf{z},&gt;}^{k} \mid \mathbf{x}</em><em _mathbf_z="\mathbf{z">{i+1}}^{p}, \mathbf{x}</em><em _mathbf_z="\mathbf{z">{i+n+1}}^{p}\right)\right] \
&amp; =\mathrm{E}</em>} \sim \mathbb{Z<em t="c+1">{T}^{c}}\left[\sum</em>}^{T} \log p_{\theta}\left(x_{z_{i}}^{p} \mid \mathbf{x<em i="i" k="k">{\mathbf{z}</em>}}^{p}, \mathbf{x<em i_n_1="i+n+1">{\mathbf{z}</em>\right)\right]
\end{aligned}
$$}}^{p</p>
<p>Intuitively, this objective applies regular PLM while sparing the numerical tokens. It then aims to reconstruct the full text sequence (that is, molecule) given the uncorrupted property tokens and partially corrupted textual tokens.</p>
<p>Self-consistency (SC) objective. Standalone, the above conditional text generation objective (6) does not reward if the generated sequences adhere to the primed property. This is critical because in chemical as well as natural languages changes in single tokens (that is, atoms, amino acids or (sub)words) can drastically change the property (meaning) of a sequence (sentence). As a remedy, we extended the text</p>
<p>generation objective $\mathcal{L}_{G}$ by an SC term that exploits the dichotomy of the RT. The full objective is given by</p>
<p>$$
\mathcal{L}<em G="G">{\mathrm{SC}}=\mathcal{L}</em>)
$$}(\mathbf{x})+\alpha \cdot \mathcal{L}_{P}(\hat{\mathbf{x}</p>
<p>where the second addend is the SC term, weighted by a factor $\alpha$. Intuitively, it is given by the difference between the property of the sample and the predicted property of the generated sample $\hat{\mathbf{x}}$. Here, $\hat{\mathbf{x}}$ is obtained by greedy decoding of the masked tokens and combining it with the non-corrupted tokens of $\mathbf{x}$. To be precise, $\hat{\mathbf{x}}=\left[\mathbf{x}^{p}, \hat{\mathbf{x}}^{c}\right]$ where $\hat{\mathbf{x}}^{c}=\left[m_{1} \hat{x}<em 1="1">{1}+(1-m</em>}) x_{1}, \ldots, m_{1} \hat{x<em 1="1">{1}+\left(1-m</em>}\right) x_{1}\right]$. Here, $\mathbf{m}$ is an indicator vector for whether masking occurred at a given position and $\hat{\mathbf{x}}=\arg \max \sum_{i=1}^{T} \log p_{\theta}\left(x_{i}^{t} ;\mathbf{x<em i-1="i-1">{\mathbf{x}</em>}}^{T}, \mathbf{x<em i-1="i-1" i-1_="i-1,">{\mathbf{x}</em>\right)$ is the result of greedy decoding. In such a formulation, the RT acts as an oracle during its own optimization, resembling an additional layer of self-supervision. While this scheme risks undesired side effects when the model performs poorly at property prediction, it introduces a notion of SC and rewards the generation of molecules that are different from training samples as long as they adhere to the property.}}^{T</p>
<h2>Model training and evaluation procedure</h2>
<p>Implementation. All experiments build upon the XLNet ${ }^{12}$ implementation from the HuggingFace library ${ }^{70}$. We expanded the XLNet backbone with our proposed tokenization scheme, an additional encoding layer for the numerical embeddings $\left(N_{\text {dim }}=16\right)$ and the custom training objectives (Extended Data Fig. 1). Regarding architectural hyperparameters, we used 32 hidden layers in the Transformer encoder, with a dimensionality of 256 and 1,024 in the feed-forward layer and 16 attention heads ( $20 \%$ dropout). Altogether, this model has $\sim 27$ million trainable parameters (exact numbers vary dependent on vocabulary size). During evaluation, greedy decoding was used for property prediction and beam search decoding for conditional sequence generation. We used PyTorch 1.3.1 (ref. 71) and the XLNet backbone from Transformer 3.1.0 (ref. 70). Models were trained from scratch unless indicated otherwise. All models were trained on single graphics processing units (GPUs) (NVIDIA Tesla A100 or V100). In the following sections, we elaborate on the training procedures for each dataset.</p>
<p>Chemical language modelling. Drug likeness (QED). Dataset. Starting from $\sim 1.6$ million bioactive molecules from ChEMBL ${ }^{72}$, we created a synthetic dataset by computing the QED $^{70}$ score $(q \in[0,1])$ for all molecules with HDRit and rounded to three decimal places. We used $\sim 1.4$ million molecules for training, 1,000 for validation and 10,000 for testing.</p>
<p>Procedure. We started training the models with the vanilla PLM objective (equation (4)) on the QED dataset until validation perplexity saturated ( $\sim 4$ days, single GPU). Thereafter, the models were further refined on the same dataset by alternating every 50 steps between objectives (equation (5) and equation (7)). We perform ablation studies on the SC loss, setting $a$ in equation (7) to 0 and 1 , respectively. The SELFIES/SMILES vocabulary had 509 and 724 tokens, respectively. During evaluation, greedy decoding was used for property prediction and beam search decoding for molecular generation. During evaluation, we set $c=2.5$, which implies that roughly $\sim 40 \%$ of the tokens were masked (maximum span: seven tokens).</p>
<p>MoleculeNet benchmark. Dataset. We focused on three regression datasets from the MoleculeNet benchmark ${ }^{30}$ : ESOL, FreeSolv and Lipophilicity, where the task is to predict water solubility, hydration free energy and lipophilicity of a molecule, respectively. For each dataset, we performed three random splits (as recommended by ref. 39) with $15 \%$ validation data. Because the datasets are small ( $&lt;5,000$ samples), we used offline SMILES augmentation ${ }^{73}$ to augment the training dataset by a factor of 16 .</p>
<p>Procedure. For the MoleculeNet datasets, the models were warm-started using the QED initialization and trained for only 50,000
steps (batch size 4) with early stopping. Since the QED pre-training utilized numerical values in $[0,1]$, we normalized the regression values of the entire MoleculeNet datasets to the same range (using training data only) and rounded them also to three decimal places. For all objectives, unless otherwise constrained, we set the masking hyperparameter $c=5$ and restrict the span of consecutively masked tokens to a maximum of five tokens.</p>
<p>Property optimization benchmark. Dataset. This is a benchmark for property-driven, conditional molecular generation. The goal is to adapt a seed molecule such that a property is maximized while adhering to a fixed similarity constraint. We obtained the data from ${ }^{42}$ which ships with a fixed split of 215,381 training and 799 test molecules and their penalized $\log P\left(\mathrm{pLogP}\right)$ value ${ }^{74}$. pLogP is the octanol-water partition coefficient ( $\log P$ ) penalized by the synthetic accessibility score and the number of cycles with $&gt;6$ atoms. Hence, pLogP just like QED can be computed deterministically from the molecule ${ }^{42}$.</p>
<p>Procedure. For this task, the models were also warm-started using the QED initialization and trained for 50,000 steps with early stopping on perplexity. To assemble the candidates for the optimization of one seed molecule, we tried to follow the process of ref. 42 as closely as possible. Reference ${ }^{42}$ applied 80 gradient steps, then decoded 80 molecules and reported the molecule with the highest pLogP score that satisfies the similarity constraint $\delta$. Instead, we form a pool of molecules by prompting 80 times with the same seed molecule but varying the fraction and the maximum span of masked tokens. From the pool of decodings we report the molecule with the highest pLogP, just like refs. 42,50.</p>
<p>Protein sequence language modelling. Protein interaction index (Boman). Dataset. As a large-scale, labelled dataset for proteins we focused on the Boman index, a measure of potential protein interaction for peptides. It is the average of the solubility values of the residues ${ }^{75}$. We collected all 2,648,205 peptides with 15-45 amino acids from UniProt ${ }^{76}$, computed their Boman index and used 10,000 and 1,000 for testing and validation, respectively.</p>
<p>Procedure. To model protein sequences, we started with training on the Boman dataset. We trained three groups of models, one for the vanilla PLM objective (equation (4)) and two for the alternating objectives. We again alternated every 50 steps between optimizing (equation (5) and equation (7)) and trained one set of models with and one set without the SC loss, such that $\alpha=1$ and $\alpha=0$, respectively, in equation (7). Models were trained until validation perplexity saturated ( $\sim 4$ days, single GPU). The numerical values of the Boman index, originally in the range $[-3.1,6.1]$ were normalized to $[0,1]$ (using training data only) and rounded to three decimal places.</p>
<p>TAPE benchmark. Dataset. We focused on two datasets from the TAPE benchmark ${ }^{13}$ : Fluorescence ${ }^{77}$ and Stability ${ }^{78}$. The goal is to predict, respectively, the fluorescence and intrinsic folding stability of a protein that is one to four mutations away from a training protein. Both datasets ship with fixed splits. The fluorescence (stability) dataset has 21,446 $(53,416)$ training, $5,362(2,512)$ validation and $27,217(12,851)$ test samples.</p>
<p>Procedure. For both datasets, three models were warm-started using the Boman initialization (PLM objective) and trained until validation performance saturated ( $\sim 100,000$ steps). Experiments were conducted using three configurations; PLM objective, and alternated training with $\left(\mathcal{L}<em _mathrm_G="\mathrm{G">{\mathrm{SC}}\right)$ and without $\left(\mathcal{L}</em>\right)$ the SC objective. The numerical values were again scaled to $[0,1]$. On the Fluorescence data, a small value of Gaussian noise was added to some training samples due to an interesting failure mode (Supplementary Information Section 7.1). For the evaluation of the conditional generation task, the models were given more flexibility: $60 \%$ of the tokens were masked (that is, $c=1.7$ in equation (3)) and the maximum span was seven amino acid residues. We did not evaluate the RT on conditional generation for the Fluorescence dataset because of a massive pre-training-fine-tuning mismatch: While}</p>
<p>the Boman dataset used for pre-training consisted of 15--45 residues (mean ± s.d., 36 ± 7), the fluorescence proteins were significantly larger (246 ± 0.2 residues, P &lt; 0.001). Instead, the proteins in the stability dataset were similar in size to the pre-training data (45 ± 3 residues).</p>
<h3>Chemical reaction modelling</h3>
<h4>Pre-training on USPTO. <strong>Dataset</strong></h4>
<p>We used reactions from the US Patent Office (USPTO), the largest open-source dataset about chemical reactions^{79} to learn generic reaction chemistry. Since no yield information was available, the utilized numerical property was the total molecular weight of all precursors. The dataset contained n = 2,830,616 reactions and was obtained from ref. 4.</p>
<h4><strong>Procedure</strong></h4>
<p>Since the two reaction yield datasets cover only narrow regions of the chemical space (one template applied to many precursor combinations), we warm up the model on broader reaction chemistry extracted from patents (USPTO). A total of 5,000 reactions were held out for validation, and the model was trained until validation performance on the two alternating objectives (equation (5) and equation (7) with α = 1) saturated. The masking hyperparameter c was set to 2.5, and the model were trained for ~2 days (single GPU). The vocabulary for reaction SELFIES contained 861 tokens.</p>
<h3>Reaction yield datasets</h3>
<h4><strong>Dataset</strong></h4>
<p>We investigated two high-throughput experimentation (HTE) yield datasets that examine specific reaction types: Buchwald–Hartig aminations^{38} and Suzuki–Miyaura cross-coupling reactions^{19}. Both datasets were investigated in the same ten random splits as examined in ref. 44 with a 70%/30% train/validation ratio.</p>
<p>The Buchwald–Hartwig dataset was produced by ref. 58 and investigates HTE of palladium-catalysed Buchwald–Hartwig C–N cross-coupling reactions. The reaction space comprises 3,955 reactions, spanned by 15 unique aryl and heteroaryl halides, 4 Buchwald ligands, 3 bases and 22 isoxazole additives. A palladium catalyst and a methylaniline are the fifth and sixth precursor, respectively; however, they are identical for all reactions. Each reaction is associated with a yield y ∈ [0, 100], and the ten random splits were identical to the ones released by ref. 80 that are also used by all competing methods in Supplementary Table 6. Yield is given in a range of [0, 100].</p>
<p>The Suzuki cross-coupling dataset was provided by ref. 59 and investigates HTE of Suzuki–Miyaura reactions across 15 pairs of electrophiles and nucleophiles, leading to different products, respectively. For each pair, a combination of 4 solvents, 12 ligands and 8 bases (reagents) was measured, resulting in a total of 5,760 reaction yields that we scale to the range [0, 100]. The catalyst is identical for all reactions; some reactions omitted the ligand or the base, while others contained electrophiles, nucleophiles, ligands, bases or solvents that were composed of different fragments (for example, salts).</p>
<h4><strong>Procedure</strong></h4>
<p>For both datasets, ten models were fine-tuned respectively on repeated random splits. The training objectives again alternated every 50 steps between property prediction (equation (5)) and conditional generation (equation (7) with α = 1) for a maximum of 50,000 steps (~1 day). Notably, during the conditional generation task we sampled one precursor per batch and then entirely but exclusively masked this precursor. Thus the objective for the model became to reconstruct a missing precursor from the remaining precursors and the reaction yield (or to produce an alternative precursor with a similar predicted yield).</p>
<h4><strong>Evaluation and performance metrics</strong></h4>
<h4><strong>Regression</strong></h4>
<p>For the regression (or property prediction) task, we convert the sequence of predicted (numerical) tokens into a floating-point prediction (the model never failed to predict a token sequence not corresponding to a valid numerical). We then report the RMSE, PCC or coefficient of determination (<em>R</em>^{2}), dependent on the dataset and previous methods.</p>
<h4><strong>Conditional sequence generation</strong></h4>
<p>Dependent on the application domain, different metrics are utilized (see above).</p>
<h4><strong>Small molecule and protein modelling</strong></h4>
<p>We strive to assess the model's ability to decorate an arbitrary, possibly discontiguous fractional input sequence (for example, a molecular scaffold) according to a property of interest. Therefore, we randomly mask a fraction of tokens of the text sequence and then query the model with ten equidistant property primers spanning the full range of property values. The metric is the average Spearman's ρ between the ten primers and the actual properties. Spearman is favourable over Pearson because it is only rank sensitive. Note that, due to constraints induced by the fragmented sequence, covering the entire property spectrum is usually impossible such that, for example, RMSE is inappropriate for this task (for example, priming a highly toxic scaffold with low toxicity cannot yield a non-toxic molecule). As a sanity check, we also report 0-Var, that is, the percentage of test molecules/proteins for which the generation was unaffected by the primer, that is, upon priming with the ten equidistant property primers and the fractional sequences, the decoded molecules/proteins were all identical (the lower the better).</p>
<p>On the property optimization benchmark from ref. 42, we report the same metrics as in their work: the success rate in generating molecules with higher logP (while adhering to the similarity constraint δ), the Tanimoto similarity δ to the seed molecule and the average improvement in plogP.</p>
<h4><strong>Chemical reaction modelling</strong></h4>
<p>For the reaction yield datasets, we challenge the model by two sequence generation tasks. First, we fully reconstructed a precursor solely based on the remaining precursors and the reaction yield. The top-three predicted sequences (decoded via beam search) are considered, s.t. top-three accuracy is reported. Additionally we report the average Tanimoto similarity of the most similar of the top-three molecules to the seed molecule. We used RDKit Morgan fingerprints with radius 2 (roughly equivalent to ECFP4 (ref. 45)). Secondly, we measure the capability of decorating existing reactions to obtain a (potentially) higher yield. To that end, the model is prompted with incomplete reactions consisting of an increased yield, an entirely masked precursor and complete remaining precursors. We consider the top-three predicted sequences (decoded via beam search) and report the fraction of samples where one of the reactions had a higher (predicted) yield (success rate). The second response metric is the mean improvement in (predicted) reaction yield (yield y ∈ [0, 100]; the distributions are right-skewed). Note that we exclude trivial solutions by removing all predicted precursors that exist in the training dataset.</p>
<h4><strong>Baseline models</strong></h4>
<h4><strong>k-NN</strong></h4>
<p>For small-molecule and protein modelling we reported results in property prediction with the k-NN baseline model. For small molecules, the distance measure was (inverted) Tanimoto similarity^{51} of ECFP4 fingerprints^{45}. For the protein language models, the Levenshtein distance between the protein sequences was used^{53}. For the k-NN baseline models, k was determined on the basis of the best performance on the validation data. This led to k = 25 for the drug-likeness/QED task, k = 21 for the protein interaction (Boman index) task, k = 50 for the fluorescence and k = 15 for the stability task.</p>
<h4><strong>XLNet with regression head</strong></h4>
<p>For the molecular property prediction on the MoleculeNet datasets, we trained an XLNet^{51} model with a conventional regression loss. This maximizes comparability to the RT since it, unlike the other models in Extended Data Table 2, also uses an XLNet backbone. This model was initialized using the XLNet-base-cased weights from HuggingFace and subsequently the SequenceClassification head was fine-tuned with an L_{2} loss. The model contained ~93 million parameters and was fine-tuned for 200 epochs without any hyperparameter optimization. Early stopping was used to determine the best epoch.</p>
<h2>Data availability</h2>
<p>The data for the MoleculeNet experiments can be obtained from https://moleculenet.org/datasets-I. The data for the molecular optimization experiments can be obtained from https://github.com/ wengong-jin/icml18-jtnn/tree/master/data/zinc. The data for the protein language modelling experiments can be obtained from https:// github.com/songlab-cal/tape. The data for the reaction yield experiments can be obtained from https://github.com/rxn4chemistry/ rxn_yields/tree/master/data.</p>
<h2>Code availability</h2>
<h2>Usage of trained models</h2>
<p>The RT is implemented in the Generative Toolkit for Scientific Discovery (GT4SD) ${ }^{82}$, which provides ready-to-use pipelines for inference on pre-trained models as well as training or fine-tuning on custom data. The GT4SD endpoint of the RT facilitates highly customizable local chemical space exploration. The user can decide to (1) make no assumptions about which tokens are being masked, (2) mask only specific types of atoms, (3) preserve certain structures while randomly masking on the rest, (4) mask certain moieties or (5) decide on a token-by-token basis on which atoms are masked. Via GT4SD, versions of the RT trained on the QED and ESOL datasets (small molecules), the stability dataset (proteins) and the USPTO-pre-trained reaction model are available. Moreover, GT4SD also distributes additional versions of the RT trained on multi-property prediction tasks not described herein, including but not limited to ring-opening polymerization catalysis and block copolymers (CTE REF 59) a logP as well as a combined logp-synthesizability model. A guide to use the RT be found on https://github.com/GT4SD/ gt4sd-core/tree/main/examples/regression_transformer. A notebook with a short demo can be found under https://github.com/GT4SD/ gt4sd-core/blob/main/notebooks/regression-transformer-demo. ipynb. The datasets used for benchmarking are available from the respectively referenced papers.</p>
<h2>GUIDemo</h2>
<p>A simple webapp of the RT for inference of pre-trained models has been made publicly available via HuggingFace spaces at https://huggingface. co/spaces/GT4SD/regression_transformer. The app was build with Gradio ${ }^{83}$ upon the GT4SD ${ }^{82}$ implementation.</p>
<h2>Reproduction</h2>
<p>The code base to facilitate reproduction of all experiments is publicly available at https://github.com/IBM/regression-transformer refs. 84-91.</p>
<h2>References</h2>
<ol>
<li>Vaswani, A. et al. In Advances in Neural Information Processing Systems 30 (Eds Guyon, I. etal.) 5998-6008 (NIPS, 2017).</li>
<li>Schwaller, P. et al. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS Cent. Sci. 5, 1572-1583 (2019).</li>
<li>Schwaller, P. et al. Mapping the space of chemical reactions using attention-based neural networks. Nat. Mach. Intell. 3, 144-152 (2021).</li>
<li>Schwaller, P., Hoover, B., Reymond, Jean-Louis, Strobelt, H. \&amp; Laino, T. Extraction of organic chemistry grammar from unsupervised learning of chemical reactions. Sci. Adv. 7, eabe4166 (2021).</li>
<li>Rives, A. et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proc. Natl Acad. Sci. USA 118, e2016239118 (2021).</li>
<li>Jumper, J. et al. Highly accurate protein structure prediction with alphafold. Nature 596, 583-589 (2021).</li>
<li>Krizhevsky, A., Sutskever, I. \&amp; Hinton, G. E. Imagenet classification with deep convolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097-1105 (2012).</li>
<li>Luong, M.-T., Pham, H. \&amp; Manning, C. D. Effective approaches to attention-based neural machine translation. In Proc. 2015 Conference on Empirical Methods in Natural Language Processing 1412-1421 (ACL, 2015).</li>
<li>Ramachandran, P. et al. Stand-alone self-attention in vision models. Adv. Neural Inf. Process. Syst. 32, 68-80 (2019).</li>
<li>Lu, K., Grover, A., Abbeel, P. \&amp; Mordatch, I. Frozen pretrained transformers as universal computation engines. In Proc. AAAI Conference on Artificial Intelligence 36, 7628-7636 (AAI Press, 2022).</li>
<li>Chen, L. et al. Decision transformer: reinforcement learning via sequence modeling. Adv. Neural Inf. Process. Syst. 34, 15084-15097 (2021).</li>
<li>Yang, Z. et al. Xlnet: Generalized autoregressive pretraining for language understanding. Adv. Neural Inf. Process. Syst., 32, 5753-5763 (2019).</li>
<li>Elton, D. C., Boukouvalas, Z., Fuge, M. D. \&amp; Chung, P. W. Deep learning for molecular design-a review of the state of the art. Mol. Syst. Des. Eng. 4, 828-849 (2019).</li>
<li>Chen, Z., Min, MartinRenqiang, Parthasarathy, S. \&amp; Ning, X. A deep generative model for molecule optimization via one fragment modification. Nat. Mach. Intell. 3, 1040-1049 (2021).</li>
<li>Wu, Z., Johnston, K. E., Arnold, F. H. \&amp; Yang, K. K. Protein sequence design with deep generative models. Curr. Opin. Chem. Biol. 65, 18-27 (2021).</li>
<li>Madani, A. et al. Large language models generate functional protein sequences across diverse families Nat. Biotechnol. (2023); https://doi.org/10.1038/s41587-022-01618-2</li>
<li>Wang, S., Guo, Y., Wang, Y., Sun, H. \&amp; Huang, J. Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In Proc. 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics (Eds Shi, X.M. et al.) 429-436 (ACM, 2019).</li>
<li>Kim, H., Lee, J., Ahn, S. \&amp; Lee, J. R. A merged molecular representation learning for molecular properties prediction with a web-based service. Sci. Rep. 11, 1-9 (2021).</li>
<li>Mahmood, O., Mansimov, E., Bonneau, R. \&amp; Cho, K. Masked graph modeling for molecule generation. Nat. Commun. 12, 1-12 (2021).</li>
<li>Kotsias, P.-C. et al. Direct steering of de novo molecular generation with descriptor conditional recurrent neural networks. Nat. Mach. Intell. 2, 254-265 (2020).</li>
<li>Bagal, V., Aggarwal, R., Vinod, P. K. \&amp; Priyakumar, U. D. Molgpt: molecular generation using a transformer-decoder model. J. Chem. Inf. Model. 62, 2064-2076 (2021).</li>
<li>Irwin, R., Dimitriadis, S., He, J. \&amp; Bjerrum, E. J. Chemformer: a pre-trained transformer for computational chemistry. Mach. Learn. Sci. Technol. 3, 015022 (2021).</li>
<li>Lu, J. \&amp; Zhang, Y. Unified deep learning model for multitask reaction predictions with explanation. J. Chem. Inf. Model. 62, 1376-1387 (2022).</li>
<li>Méndez-Lucio, O., Baillif, B., Clevert, Djork-Arné, Rouquié, D. \&amp; Wichard, J. De novo generation of hit-like molecules from gene expression signatures using artificial intelligence. Nat. Commun. 11, 1-10 (2020).</li>
<li>Born, J. et al. Data-driven molecular design for discovery and synthesis of novel ligands: a case study on SARS-CoV-2. Mach. Learn. Sci. Technol. 2, 025024 (2021).</li>
<li>Gomez-Bombarelli, R. et al. Automatic chemical design using a data-driven continuous representation of molecules. ACS Cent. Sci. 4, 268-276 (2018).</li>
<li>
<p>Maziarz, K. et al. Learning to extend molecular scaffolds with structural motifs. In The Tenth International Conference on Learning Representations (ICLR, 2022).</p>
</li>
<li>
<p>Shi, C. et al. Graphaf: a flow-based autoregressive model for molecular graph generation. In 8th International Conference on Learning Representations (ICLR, 2020).</p>
</li>
<li>Jain, M. et al. Biological sequence design with gflownets. In International Conference on Machine Learning, pages 9786-9801 (PMLR, 2022).</li>
<li>Xu, M. et al. Geodiff: a geometric diffusion model for molecular conformation generation. In The Tenth International Conference on Learning Representations (ICLR, 2022).</li>
<li>Shen, C., Krenn, M., Eppel, S. \&amp; Aspuru-Guzik, A. Deep molecular dreaming: inverse machine learning for de-novo molecular design and interpretability with surjective representations. Mach. Learn. Sci. Technol. 2, 03LTO2 (2021).</li>
<li>Fu, T. et al. Differentiable scaffolding tree for molecule optimization. In The Tenth International Conference on Learning Representations (ICLR, 2022).</li>
<li>Linder, J. \&amp; Seelig, G. Fast activation maximization for molecular sequence design. BMC Bioinformatics 22, 1-20 (2021).</li>
<li>Daulton, S. et al. Robust multi-objective Bayesian optimization under input noise. In International Conference on Machine Learning, ICML 2022, volume 162 of Proc. Machine Learning Research pages 4831-4866 (PMLR, 2022).</li>
<li>Yang, Z., Milas, K. A. \&amp; White, A. D. Now what sequence? Pre-trained ensembles for Bayesian optimization of protein sequences. Preprint at bioRxiv (2022); https://doi.org/10.1101/2022.08.05.502972</li>
<li>Khan, A. et al. Toward real-world automated antibody design with combinatorial Bayesian optimization. Cell Report Methods 3, 100374 (2023).</li>
<li>Devlin, J., Chang, M.-W., Lee, K. \&amp; Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. In Proc. 2019 Conference of the North American Chapter of the Association for Computational Linguistics pages 4171-4186 (ACL, 2019).</li>
<li>Bickerton, G. R., Paolini, G. V., Besnard, J. érémy, Muresan, S. \&amp; Hopkins, A. L. Quantifying the chemical beauty of drugs. Nat. Chem. 4, 90 (2012).</li>
<li>Wu, Z. et al. Moleculenet: a benchmark for molecular machine learning. Chem. Sci. 9, 513-530 (2018).</li>
<li>Krenn, M., Häse, F., Nigam, A. K., Friederich, P. \&amp; Aspuru-Guzik, A. Self-referencing embedded strings (SELFIES): a 100\% robust molecular string representation. Mach. Learn. Sci. Technol. 1, 045024 (2020).</li>
<li>Rong, Y. et al. In Advances in Neural Information Processing Systems (Eds Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.-F. \&amp; Lin, H.-T.) 33 (2020).</li>
<li>Jin, W., Barzilay, R. \&amp; Jaakkola, T. Junction tree variational autoencoder for molecular graph generation. In International Conference on Machine Learning (Eds Dy, J. \&amp; Krause, A.) 2323-2332 (PMLR, 2018).</li>
<li>Rao, R. et al. In Advances in Neural Information Processing Systems (Eds Schölkopf, B. et al.) 9686-9698 (MIT Press, 2019).</li>
<li>Schwaller, P., Vaucher, A. C., Laino, T. \&amp; Reymond, J.-L. Prediction of chemical reaction yields using deep learning. Mach. Learn. Sci. Technol. 2, 015016 (2021).</li>
<li>Rogers, D. \&amp; Hahn, M. Extended-connectivity fingerprints. J. Chem. Inf. Model. 50, 742-754 (2010).</li>
<li>Ertl, P. An algorithm to identify functional groups in organic molecules. J. Cheminform. 9, 1-7 (2017).</li>
<li>Vig, J. et al. Bertology meets biology: interpreting attention in protein language models. In 9th International Conference on Learning Representations (ICLR, 2021).</li>
<li>Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O. \&amp; Dahl, G. E. Neural message passing for quantum chemistry. In International Conference on Machine Learning (Eds Precup, D. \&amp; Tehpages, Y.W.) 1263-1272 (PMLR, 2017).</li>
<li>Fabian, B. et al. Molecular representation learning with language models and domain-relevant auxiliary tasks. Preprint at arXiv https://doi.org/10.48550/arXiv.2011.13230 (2020).</li>
<li>You, J., Liu, B., Ying, Z., Pande, V. \&amp; Leskovec, J. Graph convolutional policy network for goal-directed molecular graph generation. In Advances in Neural Information Processing Systems (Eds Bengio, S. \&amp; Wallach, H.M.) 6412-6422 (Curran Associates Inc., 2018).</li>
<li>Fan, Y. et al. Back translation for molecule generation. Bioinformatics 38, 1244-1251 (2022).</li>
<li>Zang, C. \&amp; Wang, F. Moflow: an invertible flow model for generating molecular graphs. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining pages 617-626 (Association for Computing Machinery, 2020).</li>
<li>Levenshtein, V. I. Binary codes capable of correcting deletions, insertions, and reversals. Sov. Phys. Doklady 10, 707-710 (1966).</li>
<li>Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M. \&amp; Church, G. M. Unified rational protein engineering with sequencebased deep representation learning. Nat. Methods 16, 1315-1322 (2019).</li>
<li>Brandes, N., Ofer, D., Peleg, Y., Rappoport, N. \&amp; Linial, M. Proteinbert: a universal deep-learning model of protein sequence and function. Bioinformatics 38, 2102-2110 (2022).</li>
<li>Meier, J. et al. Language models enable zero-shot prediction of the effects of mutations on protein function. Adv. Neural Inf. Process. Syst. 34, 29287-29303 (2021).</li>
<li>Schwaller, P., Gaudin, T., Lanyi, D., Bekas, C. \&amp; Laino, T. Found in translation: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models. Chem. Sci. 9, 6091-6098 (2018).</li>
<li>Ahneman, D. T., Estrada, JesúsG., Lin, S., Dreher, S. D. \&amp; Doyle, A. G. Predicting reaction performance in C-N cross-coupling using machine learning. Science 360, 186-190 (2018).</li>
<li>Perera, D. et al. A platform for automated nanomole-scale reaction screening and micromole-scale synthesis in flow. Science 359, 429-434 (2018).</li>
<li>Park, N. et al. An extensible platform for enabling artificial intelligence guided design of catalysts and materials. Preprint at ChemRxiv https://doi.org/10.26434/chemrxiv-2022-811rl-v2 (2022). In Revision at Nature Communications</li>
<li>Raffel, C. et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 1-67 (2020).</li>
<li>Song, K., Tan, X., Qin, T., Lu, J. \&amp; Liu, T.-Y. MPNet: Masked and Permuted Pre-training for Language Understanding. In Advances in Neural Information Processing Systems 33 (Eds Larochelle, H. et al.) (NeruIPS, 2020).</li>
<li>Fried, D. et al. Incoder: a generative model for code infilling and synthesis. Preprint at arXiv https://doi.org/10.48550/ arXiv.2204.05999 (2022).</li>
<li>Bavarian, M. et al. Efficient training of language models to fill in the middle. Preprint at arXiv https://doi.org/10.48550/arXiv.2207.14255 (2022).</li>
<li>Sanh, V. et al. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations (OpenReview.net, 2022).</li>
<li>Lu, K., Grover, A., Abbeel, P. \&amp; Mordatch, I. Pretrained transformers as universal computation engines. In Proc. of the Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22) 7628-7636 (AAAI Press, 2022); (https://ojs.aaai.org/index.php/ AAAI/article/view/20729/20488</li>
<li>
<p>Brown, Tom et al. In Advances in Neural Information Processing Systems Vol. 33, (Eds Schölkopf, B. et al.) 1877-1901 (MIT Press, 2020).</p>
</li>
<li>
<p>Van Oord, A., Kalchbrenner, N. \&amp; Kavukcuoglu, K. Pixel recurrent neural networks. In International Conference on Machine Learning (Eds Balcan, M.F. \&amp; Weinberger, K.Q.) 1747-1756 (PMLR, 2016).</p>
</li>
<li>Weininger, D. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. J. Chem. Inf. Comput. Sci. 28, 31-36 (1988).</li>
<li>Wolf, T. et al. Transformers: state-of-the-art natural language processing. In Proc. 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations pages 38-45 (Association for Computational Linguistics, 2020).</li>
<li>Paszke, A. et al. Pytorch: an imperative style, high-performance deep learning library. Adv. Neural Inf. Process. Syst. 32, 8026-8037 (2019).</li>
<li>Mendez, D. et al. Chembl: towards direct deposition of bioassay data. Nucleic Acids Res. 47, D930-D940 (2019).</li>
<li>Bjerrum, E. J. SMILES enumeration as data augmentation for neural network modeling of molecules. Preprint at arXiv https:// doi.org/10.48550/arXiv.1703.07076 (2017).</li>
<li>Kusner, M. J, Paige, B. \&amp; Hernández-Lobato, J. M. Grammar variational autoencoder. In Proc. 34th International Conference on Machine Learning Vol. 70, 1945-1954 (JMLR, 2017).</li>
<li>Boman, H. G. Antibacterial peptides: basic facts and emerging concepts. J. Intern. Med. 254, 197-215 (2003).</li>
<li>The UniProt Consortium. UniProt: the universal protein knowledgebase in 2021. Nucleic Acids Res. 49, D480-D489 (2020).</li>
<li>Sarkisyan, K. S. et al. Local fitness landscape of the green fluorescent protein. Nature 533, 397 (2016).</li>
<li>Rocklin, G. J. et al. Global analysis of protein folding using massively parallel design, synthesis, and testing. Science 357, 168-175 (2017).</li>
<li>Lowe, D. Chemical reactions from US patents (1976-Sep2016). Figshare https://figshare.com/articles/dataset/Chemical_ reactions_from_US_patents_1976-Sep2016_/S104873 (2017)</li>
<li>Sandfort, F., Strieth-Kalthoff, F., Kühnemund, M., Beecks, C. \&amp; Glorius, F. A structure-based platform for predicting chemical reactivity. Chem 6, 1379-1390 (2020).</li>
<li>Tanimoto, T. T. Elementary Mathematical Theory of Classification and Prediction (International Business Machines Corp., 1958).</li>
<li>Manica, M. et al. GT4SD: Generative toolkit for scientific discovery. GitHub https://github.com/GT4SD/gt4sd-core (2022).</li>
<li>Abid, A. et al. Gradio: hassle-free sharing and testing of ML models in the wild. Preprint at arXiv https://doi.org/10.48550/ arXiv. 1906.02569 (2019).</li>
<li>Born, J. \&amp; Manica, M. Regression transformer repository. Zenodo https://doi.org/10.5281/zenodo. 7639206 (2023).</li>
<li>He, P., Liu, X., Gao, J. \&amp; Chen, W. Deberta: decoding-enhanced bert with disentangled attention. In 9th International Conference on Learning Representations (ICLR, 2021).</li>
<li>Dai, Z. et al. Transformer-xl: attentive language models beyond a fixed-length context. In Proc. 57th Annual Meeting of the Association for Computational Linguistics. 2978-2988 (Association for Computational Linguistics, 2019).</li>
<li>Bai, H. et al. Segatron: segment-aware transformer for language modeling and understanding. In Proc. AAAI Conference on Artificial Intelligence Vol. 35, 12526-12534 (AAAI Press, 2021).</li>
<li>Wang, Y.-A. \&amp; Chen, Y.-N. What do position embeddings learn? An empirical study of pre-trained language model positional encoding. In Proc. of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) 6840-6849 (Association for Computational Linguistics, 2020).</li>
<li>Zhang, J., Mercado, Rocío, Engkvist, O. \&amp; Chen, H. Comparative study of deep generative models on chemical space coverage. J. Chem. Inf. Model. 61, 2572-2581 (2021).</li>
<li>Bemis, G. W. \&amp; Murcko, M. A. The properties of known drugs. 1. Molecular frameworks. J. Med. Chem. 39, 2887-2893 (1996).</li>
<li>Vig, J. A multiscale visualization of attention in the transformer model. In Proc. 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations pages 37-42 (Association for Computational Linguistics, 2019).</li>
</ol>
<h2>Acknowledgements</h2>
<p>The authors thank the entire AI for Scientific Discovery Group at IBM and particularly C. Baldassari and A. Leonov for useful discussions on reaction chemistry. The authors especially thank E. J. Bjerrum but also the anonymous reviewers for their constructive and valuable feedback that helped improving the article tremendously.</p>
<h2>Author contributions</h2>
<p>J.B. and M.M. conceived the initial idea for the project, set the scope of experiments and wrote the code base as well as the manuscript. J.B. further trained the models, performed the experiments, analysed the results, created the visualizations and devised the alternating training scheme with SC loss.</p>
<h2>Funding</h2>
<p>Open access funding provided by Swiss Federal Institute of Technology Zurich.</p>
<h2>Competing interests</h2>
<p>The authors declare no competing interests.</p>
<h2>Additional information</h2>
<p>Extended data is available for this paper at https://doi.org/10.1038/s42256-023-00639-z.</p>
<p>Supplementary information The online version contains supplementary material available at https://doi.org/10.1038/s42256-023-00639-z.</p>
<p>Correspondence and requests for materials should be addressed to Jannis Born or Matteo Manica.</p>
<p>Peer review information Nature Machine Intelligence thanks Esben Jannik Bjerrum and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.</p>
<p>Reprints and permissions information is available at www.nature.com/reprints.</p>
<p>Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
<p>Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.
(c) The Author(s) 2023</p>
<h1>$&lt;$ QED $&gt;0.428|\ldots|&lt;$ ESOL $&gt;-2.92 \mid N #[N+][N-] c 1 c c c(C) c c 1$</h1>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Extended Data Fig. 1: Workflow of the Regression Transformer (RT) model. Based on the XLNet backbone, the RT is a dichotomous model designed to handle combinations of text and numbers. Top: An input sequence consisting of a molecular string (red) and two property tags (blue), each associated to a floating value (green). Numbers are tokenized into a sequence of tokens that preserve the decimal order of each character. The pipe (i) is a separator token distinguishing numerical and text tokens. Middle: We propose numerical encodings that inform the model about the semantic proximity of these tokens and naturally integrate with relative positional encodings and classical learned embeddings. The RT relies on a XLNet backbone and follows permutation language modeling (PLM). Bottom: Multiple training objectives are proposed and combined (predicted
tokens are emphasized in the figure). Following the vanilla PLM objective ${ }^{22}$, masking occurs randomly throughout the sequence. In the property objective, masking occurs exclusively on the property tokens. In the generation objective, masking occurs exclusively on the textual tokens (here: SMILES). This objective can be augmented with a self-consistency term $\mathrm{L}_{\mathrm{SC}}$ that exploits the dichotomoy of the model. In practice, we use an alternating training scheme designed to concurrently excel at property prediction and conditional generation tasks. Note that the RT builds upon an XLNet-backbone which samples a token factorization order (following PML as proposed by Yang et al. ${ }^{23}$; not shown). The dots indicate that the RT naturally scales to multiple property tags.</p>
<h1>Decoration capabilities as function of task difficulty</h1>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Extended Data Fig. 2: More distant queries are harder to decorate. When gradually increasing task difficulty (that is, the distance between the QED of the seed molecule and the primed property), the distance between the QED of the generated molecule and the primed property increases linearly. Data presented
as means, error bars denote $95 \%$ confidence intervals. For the blue, orange and green bars, a total of $880 \mathrm{k}, 239 \mathrm{k}$ and $207 \mathrm{k}$ generated molecules are evaluated respectively.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Extended Data Fig. 3: Learned embeddings of numerical tokens. Left: For an exemplary dimension, embeddings for 20 tokens, corresponding to 10 digits and 2 decimal places are shown. Right: Embeddings for 20 exemplary dimensions
across all ten digits. The stars indicate the significance level of the Pearson correlation. The analysis is based on a SELFIES model without any NEs (PLM objective).</p>
<p>Seed reaction
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Extended Data Fig. 4 |Discovering novel, more effective reactions by adapting an unseen Buchwald-Hartwig amination. Below an unseen BH amination (top) and its experimentally reported yield, we show four RT-generated reactions that selectively replace individual precursors. Upon priming the RT with a higher yield and a given precursor type, the RT generated
reactions with higher yield, as predicted by the RT. The RXN confidence stems from the forward reaction prediction model by Schwaller et al. ${ }^{2}$ which confirmed that the reaction would result in the shown product in all cases. Note that no adaptations of 4-Methylaniline and the Palladium-catalyst are generated since they are constanta cross the dataset.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p><strong>Extended Data Fig. 5: Float-based numerical encodings.</strong> a) Numerical encodings for an molecule with a QED of 0.179. b) Pairwise distances of numerical encodings for floats between 0 and 100 (the NEs of all tokens associated with a float are summed up).</p>
<h1>Article</h1>
<p>Extended Data Table 1 | Performance comparison in predicting QED</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>MAE</th>
</tr>
</thead>
<tbody>
<tr>
<td>$k$-NN (baseline)</td>
<td>0.054</td>
</tr>
<tr>
<td>SMILES-BERT [17]</td>
<td>0.020</td>
</tr>
<tr>
<td>RT (PLM)</td>
<td>0.035</td>
</tr>
<tr>
<td>RT (Alternate)</td>
<td>$\mathbf{0 . 0 1 7}$</td>
</tr>
</tbody>
</table>
<p>MAE stands for mean absolute error. The RT with alternating objectives used $o=0$ in Equation (7). Our model names are shown in bold; best performance shown in bold.</p>
<h1>Extended Data Table 2 RMSE ( $\downarrow$ ) in predicting MoleculeNet dataset properties</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">$\boldsymbol{L}_{\text {Reg }}$</th>
<th style="text-align: center;">ESOL</th>
<th style="text-align: center;">FreeSolv</th>
<th style="text-align: center;">Lipophilicity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Random Forest [39]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1.16 \pm 0.15$</td>
<td style="text-align: center;">$2.12 \pm 0.68$</td>
<td style="text-align: center;">$0.78 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost [39]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1.05 \pm 0.10$</td>
<td style="text-align: center;">$1.76 \pm 0.21$</td>
<td style="text-align: center;">$0.84 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">MPNN [39]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$0.55 \pm 0.02$</td>
<td style="text-align: center;">$1.20 \pm 0.02$</td>
<td style="text-align: center;">$0.76 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">Mol-BERT [48]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$0.53 \pm 0.04$</td>
<td style="text-align: center;">$0.95 \pm 0.33$</td>
<td style="text-align: center;">$0.56 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">ChemFormer [22]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$0.63 \pm 0.04$</td>
<td style="text-align: center;">$1.23 \pm 0.30$</td>
<td style="text-align: center;">$0.60 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">RT (XLNet backbone)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$0.69 \pm 0.01$</td>
<td style="text-align: center;">$1.03 \pm 0.25$</td>
<td style="text-align: center;">$0.74 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">$\operatorname{RT}(\boldsymbol{\alpha}=0, \mathrm{NE}: \times)$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$0.76 \pm 0.05$</td>
<td style="text-align: center;">$1.19 \pm 0.29$</td>
<td style="text-align: center;">$0.76 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">$\operatorname{RT}(\boldsymbol{\alpha}=1, \mathrm{NE}: \times)$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$0.75 \pm 0.04$</td>
<td style="text-align: center;">$1.32 \pm 0.39$</td>
<td style="text-align: center;">$0.76 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">$\operatorname{RT}(\boldsymbol{\alpha}=0, \mathrm{NE}: \checkmark)$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$0.71 \pm 0.04$</td>
<td style="text-align: center;">$1.40 \pm 0.47$</td>
<td style="text-align: center;">$0.74 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">$\operatorname{RT}(\boldsymbol{\alpha}=1, \mathrm{NE}: \checkmark)$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$0.73 \pm 0.04$</td>
<td style="text-align: center;">$1.34 \pm 0.29$</td>
<td style="text-align: center;">$0.74 \pm 0.03$</td>
</tr>
</tbody>
</table>
<p>Performance on three different datasets across predictive models. By L Reg we denote whether a given model used a loss (or objective function) that relied on regression. All models used repeated random splits. NE means numerical encodings and a refers to the loss function in Equation (7). Standard deviations shown, best model shown in bold.</p>
<h1>Extended Data Table 3 | Conditional generation for MoleculeNet datasets</h1>
<table>
<thead>
<tr>
<th>Model</th>
<th>NE</th>
<th>α</th>
<th>ESOL Spearman</th>
<th></th>
<th></th>
<th>FreeSolv Spearman</th>
<th></th>
<th></th>
<th>Lipophilicity Spearman</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>0-Var.</td>
<td>Grover</td>
<td>RT</td>
<td>0-Var.</td>
<td>Grover</td>
<td>RT</td>
<td>0-Var.</td>
<td>Grover</td>
<td>RT</td>
</tr>
<tr>
<td>RT</td>
<td>×</td>
<td>0</td>
<td>4.4% ± 0.8</td>
<td>0.44 ± 0.0</td>
<td>0.38 ± 0.1</td>
<td>7.9% ± 2.4</td>
<td>0.53 ± 0.0</td>
<td>0.51 ± 0.0</td>
<td>3.6% ± 1.6</td>
<td>0.29 ± 0.1</td>
<td>0.22 ± 0.1</td>
</tr>
<tr>
<td>RT</td>
<td>×</td>
<td>1</td>
<td>5.9% ± 1.3</td>
<td>0.46 ± 0.0</td>
<td>0.38 ± 0.0</td>
<td>7.5% ± 3.6</td>
<td>0.56 ± 0.0</td>
<td>0.52 ± 0.1</td>
<td>2.7% ± 0.9</td>
<td>0.35 ± 0.0</td>
<td>0.29 ± 0.0</td>
</tr>
<tr>
<td>RT</td>
<td>✓</td>
<td>0</td>
<td>6.1% ± 3.7</td>
<td>0.46 ± 0.1</td>
<td>0.41 ± 0.1</td>
<td>8.9% ± 5.2</td>
<td>0.57 ± 0.0</td>
<td>0.52 ± 0.0</td>
<td>4.2% ± 1.3</td>
<td>0.29 ± 0.0</td>
<td>0.23 ± 0.0</td>
</tr>
<tr>
<td>RT</td>
<td>✓</td>
<td>1</td>
<td>6.1% ± 1.5</td>
<td>0.47 ± 0.0</td>
<td>0.44 ± 0.0</td>
<td>6.5% ± 2.6</td>
<td>0.57 ± 0.0</td>
<td>0.44 ± 0.1</td>
<td>2.7% ± 1.7</td>
<td>0.34 ± 0.0</td>
<td>0.26 ± 0.0</td>
</tr>
</tbody>
</table>
<p>Average performances across three splits for training with alternating objectives. Different combinations of the numerical encodings (NE) and the alternating training objective (with and without the self-consistency term <em>σ</em>) are shown. Spearman refers to Spearman's <em>ρ</em> rank correlation and was evaluated either with the RT itself or with an external model (Grover 11). Best configuration shown in bold.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ IBM Research Europe, Zurich, Switzerland. ${ }^{2}$ Department of Biosystem Science and Engineering, ETH Zurich, Basel, Switzerland.
e-mail: jab@zurich.ibm.com; tte@zurich.ibm.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
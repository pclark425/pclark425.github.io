<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1201 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1201</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1201</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-268230711</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.00504v1.pdf" target="_blank">Learning and Leveraging World Models in Visual Representation Learning</a></p>
                <p><strong>Paper Abstract:</strong> Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model. While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space. We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or surpasses the performance of previous self-supervised methods. Finally, we show that learning with an IWM allows one to control the abstraction level of the learned representations, learning invariant representations such as contrastive methods, or equivariant representations such as masked image modelling.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1201.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1201.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IWM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Image World Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A JEPA-based latent-space world model for images that predicts the effect of photometric and geometric transformations in representation space; trained with an encoder (ViT) and a predictor (the world model) conditioned on transformation parameters and mask tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Image World Model (IWM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent-space predictive model instantiated as the predictor in a Joint-Embedding Predictive Architecture (JEPA). The encoder is a Vision Transformer (ViT-B/16) producing patch embeddings; the predictor takes encoded (masked) source latents, mask tokens, and explicit transformation parameters (feature-conditioned) and outputs predicted target latents. Training target latents come from an EMA teacher encoder; loss is L2 between predicted and teacher latents on masked positions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (transformer-based predictive model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Self-supervised visual representation learning; downstream discriminative tasks including ImageNet classification and ADE20k semantic segmentation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Squared L2 prediction loss on latent targets; retrieval-based Mean Reciprocal Rank (MRR) computed by nearest-neighbor lookup in a bank of augmented target embeddings; qualitative nearest-neighbor retrieval/visualization</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>MRR depends on conditioning, augmentation strength, and predictor capacity: e.g., Table 1 shows Feature conditioning MRR ≈ 0.79 and Sequence conditioning ≈ 0.82 (no conditioning = 0.00). Table 2 reports for IWM (depth=18, dim=384): with strong jitter MRR ≈ 0.85, with destructive augmentations (grayscale, blur, solarization) MRR ≈ 0.79.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable via latent-space visualizations and retrieval: predicted latents are mapped to nearest neighbor images showing the model's ability to apply/undo augmentations; some systematic errors observed (notably imperfect inversion of grayscale). Overall the model is a black-box neural predictor but its outputs can be visualized and analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Nearest-neighbor retrieval of predicted latent against a bank of augmented target embeddings; similarity matrices across augmentations; visualization grids varying photometric parameters; MRR as a quantitative proxy for fidelity of applied transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Pretraining on ImageNet for 300 epochs with ViT-B/16 encoder; predictor variants used include depths 12 and 18 and embedding dim 384 (IWM_Equi_18,384). Pretraining reported with large batches (examples: a pretraining setup mentions batch size 16,384 for some runs); predictor finetuning uses 100 ImageNet epochs with batch size 1024. Scaling guidance: predictor_weights / encoder_weights ≈ 0.3 found suitable for ViT-L scaling. Exact FLOPs/parameter counts not fully enumerated in paper, but predictor depth/width and training epochs are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Predictor finetuning (reusing frozen encoder and finetuning predictor) is more efficient than encoder finetuning: achieves similar or better downstream performance while tuning far fewer parameters and amortizing the encoder forward pass. Reported efficiency gains: predictor finetuning outperforms encoder finetuning by ~1.0–1.5 percentage points when number of tuned parameters is comparable (Figure 3). Using a tuned predictor can match encoder-finetuning performance at a fraction of inference/training cost; multitask predictor finetuning amortizes cost across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>ImageNet classification: predictor finetuning with IWM Equi 18,384 achieves ~81.5% (frozen encoder + tuned predictor) and end-to-end finetuning reaches 84.4% Top-1 (Table 4/5 context). IWM Inv 12,384 yields stronger linear evaluation behavior (e.g., linear/top attentive numbers reported in Table 8: IWM Inv linear ≈ 74.5, attentive ≈ 77.0; IWM Equi attentive ≈ 75.1). ADE20k segmentation: predictor finetuning with IWM Equi 18,384 ≈ 46.8 mIoU, end-to-end ≈ 47.0 (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The predictor (world model) contains task-relevant information not fully present in the encoder output; finetuning the predictor yields improved discriminative performance compared to finetuning an appended random head. High fidelity in modeling transformations (equivariance) correlates with better utility when finetuning the world model for discriminative tasks (prediction head), whereas invariant world models yield better linear-probing performance. Equivariant IWMs generalize better to OOD datasets (iNaturalist, SUN397, Places205) according to attentive probing results.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off between equivariance (high-capacity predictor that can invert/apply transformations) and invariance (encoder removes augmentation information): equivariant models retain richer information useful for finetuning but perform worse on simple linear probes; invariant models perform better for linear evaluation but their predictors are less reusable. Higher predictor capacity and stronger augmentations increase fidelity but also computational cost and risk of less abstract representations. Marginalizing predictions to create invariant representations preserves information but does not necessarily improve downstream linear performance.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices are (1) conditioning predictor on transformation parameters (feature conditioning preferred over none/sequence), (2) complexity/strength of augmentations (include destructive ops like grayscale/blur/solarize), (3) predictor capacity (deeper/wider predictors; 18 layers and 384-dim used for strong world models), (4) latent-space prediction with mask tokens, (5) EMA teacher encoder for targets, (6) feature-mixing MLP for conditioning, (7) use of ViT-B/16 encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to I-JEPA and MAE: IWM (especially equivariant configuration) yields higher utility when finetuning the predictor (IWM Equi gives ~+1.8 Top-1 over a random predictor baseline and improves over I-JEPA and MAE in predictor-fineturning scenarios). In linear probe and attentive probing, IWM Inv behaves similarly to contrastive methods (strong linear scores) while IWM Equi behaves like MIM methods (stronger when using larger evaluation heads or finetuning). MAE's learned 'world model' (decoder) does not provide strong gains when attached as a predictor vs. random head in classification finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Recommendations: condition the predictor on transformation parameters (feature conditioning), use strong/destructive augmentations to force learning non-trivial transforms, allocate sufficient predictor capacity (observed success with 18-layer predictor, embedding dim 384; predictor/encoder weight ratio ≈ 0.3 when scaling), and use EMA teacher targets. For downstream reuse, finetuning the predictor (possibly multi-task) is recommended for efficiency and versatility; choice between equivariant vs invariant IWM depends on target evaluation (equivariant for finetuning-heavy tasks and OOD generalization; invariant for linear-probing performance).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning and Leveraging World Models in Visual Representation Learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1201.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1201.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>I-JEPA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>I-JEPA (Joint-Embedding Predictive Architecture instantiation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A JEPA-style self-supervised method that predicts masked target latents from a source context in latent space; serves as a baseline and conceptual precursor to IWM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-supervised learning from images with a joint-embedding predictive architecture</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>I-JEPA (latent JEPA predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent-space predictive architecture where a predictor predicts masked target latents from a source context without strong conditioning on transformation parameters; typically uses a ViT encoder and a predictor of similar architecture but smaller capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model / JEPA predictive model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Self-supervised visual representation learning; used as baseline to compare prediction/fidelity and downstream utility</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Same metrics used: L2 latent prediction loss and retrieval-based MRR</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Lower MRR under many augmentation settings reported in the paper (examples: Table 2 shows I-JEPA MRR often ≈ 0.00 for simple jitter settings), indicating weaker ability to model some transformations compared to high-capacity, conditioned IWM predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>As a latent predictor, behaves between invariant and equivariant; visualizations and embedding similarity matrices indicate moderate sensitivity to augmentations but less capability than conditioned/high-capacity IWM predictors. No special interpretability mechanisms beyond JEPA analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Embedding similarity matrices and qualitative comparisons used in paper; MRR/retrieval diagnostics.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Uses ViT encoder and a predictor; predictor depth/width in baseline smaller than IWM's strongest predictor (e.g., 12-layer predictors tested). Pretraining/evaluation budgets comparable to other JEPA methods used in experiments (300 epochs).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>I-JEPA typically learns less powerful world models (lower MRR) under same predictor sizes/augmentation strengths; thus less reusable as a finetunable predictor compared to IWM Equi. Efficiency advantages not emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reported ImageNet and probing scores typically lower than IWM when leveraging finetuned predictor; exact numbers: I-JEPA attended probing ≈ 75.0 (Table 8), and various finetuning protocols show smaller gains from predictor finetuning compared to IWM Equi.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Because I-JEPA is often neither strongly equivariant nor purposely invariant, its predictor tends to be less directly reusable for downstream discriminative finetuning compared to a capable IWM predictor.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Without explicit conditioning or sufficient predictor capacity, the predictor collapses to invariant behavior and the world model is weak (low MRR), reducing downstream utility despite similar architectural form.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Less conditioning on transformation parameters compared to IWM; predictor capacity and augmentation choices determine whether it becomes more invariant or equivariant.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to IWM, I-JEPA is simpler but often underperforms when the goal is to learn a reusable world model; compared to MIM/MAE, I-JEPA sits between contrastive invariance and MIM equivariance in the representation abstraction spectrum.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests that to make JEPA-style predictors useful as world models one must (a) condition on transformation parameters and (b) increase predictor capacity and use stronger augmentations — which motivates IWM's design choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning and Leveraging World Models in Visual Representation Learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1201.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1201.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Masked Autoencoders (generative world model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A masked image modeling approach with an encoder-decoder architecture trained to reconstruct pixels/patches; considered a generative-style world model in input space and used as a baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Masked autoencoders are scalable vision learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Masked Autoencoder (MAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative input-space world model implemented as an encoder-decoder: encoder processes visible patches, decoder reconstructs masked patches in pixel space; here treated as a generative world model (decoder plays the role of world model).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>generative world model / input-space reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Self-supervised visual representation learning; evaluated on ImageNet and downstream tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss in pixel/input space (typically MSE or reconstruction objective); representation quality measured via linear probe, attentive probing, and finetuning performance in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>MAE achieves strong end-to-end finetuning and attentive probing performance (Table 8 attentive ≈ 73.5 for MAE300), but when attaching a pretrained predictor/decoder as an evaluation head it provides negligible gains over a random predictor for classification finetuning — suggesting the MAE decoder's learned world-model-like function is not readily reusable for those discriminative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Decoder reconstructs input-space pixels which is interpretable visually (reconstructions), but the paper reports a weak correlation between generation quality and representation quality; decoder as a world model is not shown to be readily interpretable in terms of task-relevant latent factors.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Input-space reconstructions and downstream representation probes; no specific structured interpretability beyond visual inspection mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>MAE encoder-decoder training costs; in comparisons MAE pretraining is performed for similar epoch budgets (e.g., 300 epochs) and uses similar ViT encoders. Exact FLOPs not enumerated in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>When attaching a random vs pretrained predictor, MAE shows negligible gains — implying its decoder/world-model is not an efficiency win for predictor-fineturning pipelines in discriminative tasks compared to IWM's predictor which yields tangible gains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>MAE end-to-end finetuning and attentive probing competitive (examples: attentive probing numbers in Table 8), but predictor reuse for classification offers little advantage versus a random head.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High-fidelity input-space reconstruction does not necessarily translate to a reusable world model for downstream discriminative tasks—MAE decoder does not provide the same predictor-reuse benefit that a conditioned, equivariant IWM predictor does.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Generative reconstruction may preserve low-level details but that does not guarantee representations that are easy to adapt for discriminative tasks via finetuning of the decoder/predictor; i.e., reconstruction fidelity and downstream utility are not strongly coupled.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Pixel-space reconstruction objective, encoder-decoder separation, no explicit conditioning on transformation parameters as in IWM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to IWM Equi, MAE's decoder is less useful as a reusable predictor for discriminative finetuning, though MAE performs well under full end-to-end finetuning and in attentive probing.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests that for reusability as a world model, conditioning and predictor design (latent prediction with explicit action/transform conditioning and capacity) are important — MAE lacks these and thus is suboptimal for predictor reuse despite strong reconstruction fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning and Leveraging World Models in Visual Representation Learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1201.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1201.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL world models (refs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World models in reinforcement learning (e.g., Ha & Schmidhuber 2018; Hafner et al. 2019/2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General class of predictive models in RL trained to predict consequences of actions in input or latent space; cited as successful prior work motivating learning world models for vision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RL latent/input-space world models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>In RL these are neural predictive models (often latent-space dynamics models) that model next observations and/or rewards conditioned on actions; examples include recurrent latent models and latent imagination methods (cited works: Ha & Schmidhuber 2018; Hafner et al. 2019/2023).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model / model-based RL world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reinforcement learning control domains, video prediction, policy learning and planning</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Typically measured via next-state prediction error (MSE), reconstruction loss, or downstream control performance (cumulative reward); in this paper they are cited conceptually (no new metrics provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No numerical fidelity/performance reported in this paper for these RL world models — they are cited as prior successful examples.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in detail here; general RL world models are typically black-box neural networks though some analyses exist in cited literature (not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not described in this paper; cited literature focuses on predictive accuracy and utility for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not enumerated here; RL world models often require significant compute for joint dynamics + policy training, but the paper only references them as motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper draws analogy: in RL, world models are reused for planning/policy — motivates reusing learned world models in vision rather than discarding them after pretraining. No direct computational comparisons provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not provided in this paper; referenced works demonstrate success on RL benchmarks but specifics are in cited RL papers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Paper uses RL world models as conceptual precedent: in RL the learned model is leveraged for planning; in contrast, in computer vision world models are frequently discarded after pretraining — IWM aims to change that by enabling reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Paper argues reusing world models can improve efficiency and task performance, analogous to RL, but does not provide RL-specific tradeoff data.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Paper highlights conditioning on actions/transformations, latent-space prediction, and predictor capacity as parallel design lessons from RL world models applied to image representation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Used as a conceptual reference point; the paper's IWM can be seen as a vision counterpart that learns a predictive latent model conditioned on transformation 'actions'.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper borrows RL insight that conditioning on actions and adequate model capacity enhance world-model usefulness; translates these into concrete recommendations for image world models (condition on transform, strong transforms, sufficient predictor capacity).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning and Leveraging World Models in Visual Representation Learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1201.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1201.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative World Models (decoders)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative World Models / Decoder-based reconstruction models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Class of models that invert transformations in input space (e.g., decoders in masked autoencoders), considered generative world models in the taxonomy and contrasted with latent JEPA world models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generative world models (input-space decoders)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-style architectures trained to reconstruct masked/modified input pixels (e.g., in MIM/MAE). Viewed as world models that operate in input space, learning to invert masking or other corruptions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>generative world model (input-space)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Masked image modeling, reconstruction-based self-supervised learning; evaluated for representation learning utility</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss (pixel MSE or other perceptual metrics) and generation quality; representation utility measured via downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Paper notes previous work shows generation quality does not necessarily correlate with representation quality (citing Chen et al., 2024). No new numeric fidelity values for generative models other than MAE baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Reconstructions are visually interpretable, but this does not guarantee that learned latents are optimally structured for downstream discriminative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of reconstructions; downstream probing.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Decoder adds computational overhead compared to purely latent-predictive JEPA methods; exact costs depend on architecture (not exhaustively quantified here).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper argues latent JEPA approaches can remove unnecessary information and are attractive because reconstruction quality in input-space does not equate to representation quality — suggesting latent predictive approaches (IWM) can be more efficient/useful when the decoder is not required for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Compared indirectly through MAE baselines: generative decoders yield good end-to-end finetuning but are less directly reusable as lightweight prediction heads for discriminative tasks versus a conditioned latent predictor.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Generative fidelity (pixel reconstruction) often does not directly translate to downstream discriminative utility; latent conditioning and predictor capacity are important design factors to make a world model useful for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High-fidelity input reconstructions can increase computational cost and may not yield proportional gains in representation adaptability; latent predictive models (IWM) provide alternative trade-offs favoring reuse and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Choosing latent vs input-space prediction, whether to condition on transforms, and whether to retain decoder after pretraining are key choices affecting utility and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Generative decoders (MAE) compare favorably under full finetuning but are less helpful when trying to repurpose a learned world model as a lightweight finetunable head; IWM's latent predictor offers a more directly reusable world model for discriminative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning and Leveraging World Models in Visual Representation Learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Recurrent world models facilitate policy evolution <em>(Rating: 2)</em></li>
                <li>Dream to Control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Mastering diverse domains through world models <em>(Rating: 2)</em></li>
                <li>Self-supervised learning from images with a joint-embedding predictive architecture <em>(Rating: 2)</em></li>
                <li>Masked autoencoders are scalable vision learners <em>(Rating: 2)</em></li>
                <li>Data2vec: A general framework for self-supervised learning in speech, vision and language <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1201",
    "paper_id": "paper-268230711",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "IWM",
            "name_full": "Image World Models",
            "brief_description": "A JEPA-based latent-space world model for images that predicts the effect of photometric and geometric transformations in representation space; trained with an encoder (ViT) and a predictor (the world model) conditioned on transformation parameters and mask tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Image World Model (IWM)",
            "model_description": "Latent-space predictive model instantiated as the predictor in a Joint-Embedding Predictive Architecture (JEPA). The encoder is a Vision Transformer (ViT-B/16) producing patch embeddings; the predictor takes encoded (masked) source latents, mask tokens, and explicit transformation parameters (feature-conditioned) and outputs predicted target latents. Training target latents come from an EMA teacher encoder; loss is L2 between predicted and teacher latents on masked positions.",
            "model_type": "latent world model (transformer-based predictive model)",
            "task_domain": "Self-supervised visual representation learning; downstream discriminative tasks including ImageNet classification and ADE20k semantic segmentation",
            "fidelity_metric": "Squared L2 prediction loss on latent targets; retrieval-based Mean Reciprocal Rank (MRR) computed by nearest-neighbor lookup in a bank of augmented target embeddings; qualitative nearest-neighbor retrieval/visualization",
            "fidelity_performance": "MRR depends on conditioning, augmentation strength, and predictor capacity: e.g., Table 1 shows Feature conditioning MRR ≈ 0.79 and Sequence conditioning ≈ 0.82 (no conditioning = 0.00). Table 2 reports for IWM (depth=18, dim=384): with strong jitter MRR ≈ 0.85, with destructive augmentations (grayscale, blur, solarization) MRR ≈ 0.79.",
            "interpretability_assessment": "Partially interpretable via latent-space visualizations and retrieval: predicted latents are mapped to nearest neighbor images showing the model's ability to apply/undo augmentations; some systematic errors observed (notably imperfect inversion of grayscale). Overall the model is a black-box neural predictor but its outputs can be visualized and analyzed.",
            "interpretability_method": "Nearest-neighbor retrieval of predicted latent against a bank of augmented target embeddings; similarity matrices across augmentations; visualization grids varying photometric parameters; MRR as a quantitative proxy for fidelity of applied transformations.",
            "computational_cost": "Pretraining on ImageNet for 300 epochs with ViT-B/16 encoder; predictor variants used include depths 12 and 18 and embedding dim 384 (IWM_Equi_18,384). Pretraining reported with large batches (examples: a pretraining setup mentions batch size 16,384 for some runs); predictor finetuning uses 100 ImageNet epochs with batch size 1024. Scaling guidance: predictor_weights / encoder_weights ≈ 0.3 found suitable for ViT-L scaling. Exact FLOPs/parameter counts not fully enumerated in paper, but predictor depth/width and training epochs are provided.",
            "efficiency_comparison": "Predictor finetuning (reusing frozen encoder and finetuning predictor) is more efficient than encoder finetuning: achieves similar or better downstream performance while tuning far fewer parameters and amortizing the encoder forward pass. Reported efficiency gains: predictor finetuning outperforms encoder finetuning by ~1.0–1.5 percentage points when number of tuned parameters is comparable (Figure 3). Using a tuned predictor can match encoder-finetuning performance at a fraction of inference/training cost; multitask predictor finetuning amortizes cost across tasks.",
            "task_performance": "ImageNet classification: predictor finetuning with IWM Equi 18,384 achieves ~81.5% (frozen encoder + tuned predictor) and end-to-end finetuning reaches 84.4% Top-1 (Table 4/5 context). IWM Inv 12,384 yields stronger linear evaluation behavior (e.g., linear/top attentive numbers reported in Table 8: IWM Inv linear ≈ 74.5, attentive ≈ 77.0; IWM Equi attentive ≈ 75.1). ADE20k segmentation: predictor finetuning with IWM Equi 18,384 ≈ 46.8 mIoU, end-to-end ≈ 47.0 (Table 6).",
            "task_utility_analysis": "The predictor (world model) contains task-relevant information not fully present in the encoder output; finetuning the predictor yields improved discriminative performance compared to finetuning an appended random head. High fidelity in modeling transformations (equivariance) correlates with better utility when finetuning the world model for discriminative tasks (prediction head), whereas invariant world models yield better linear-probing performance. Equivariant IWMs generalize better to OOD datasets (iNaturalist, SUN397, Places205) according to attentive probing results.",
            "tradeoffs_observed": "Trade-off between equivariance (high-capacity predictor that can invert/apply transformations) and invariance (encoder removes augmentation information): equivariant models retain richer information useful for finetuning but perform worse on simple linear probes; invariant models perform better for linear evaluation but their predictors are less reusable. Higher predictor capacity and stronger augmentations increase fidelity but also computational cost and risk of less abstract representations. Marginalizing predictions to create invariant representations preserves information but does not necessarily improve downstream linear performance.",
            "design_choices": "Key choices are (1) conditioning predictor on transformation parameters (feature conditioning preferred over none/sequence), (2) complexity/strength of augmentations (include destructive ops like grayscale/blur/solarize), (3) predictor capacity (deeper/wider predictors; 18 layers and 384-dim used for strong world models), (4) latent-space prediction with mask tokens, (5) EMA teacher encoder for targets, (6) feature-mixing MLP for conditioning, (7) use of ViT-B/16 encoder.",
            "comparison_to_alternatives": "Compared to I-JEPA and MAE: IWM (especially equivariant configuration) yields higher utility when finetuning the predictor (IWM Equi gives ~+1.8 Top-1 over a random predictor baseline and improves over I-JEPA and MAE in predictor-fineturning scenarios). In linear probe and attentive probing, IWM Inv behaves similarly to contrastive methods (strong linear scores) while IWM Equi behaves like MIM methods (stronger when using larger evaluation heads or finetuning). MAE's learned 'world model' (decoder) does not provide strong gains when attached as a predictor vs. random head in classification finetuning.",
            "optimal_configuration": "Recommendations: condition the predictor on transformation parameters (feature conditioning), use strong/destructive augmentations to force learning non-trivial transforms, allocate sufficient predictor capacity (observed success with 18-layer predictor, embedding dim 384; predictor/encoder weight ratio ≈ 0.3 when scaling), and use EMA teacher targets. For downstream reuse, finetuning the predictor (possibly multi-task) is recommended for efficiency and versatility; choice between equivariant vs invariant IWM depends on target evaluation (equivariant for finetuning-heavy tasks and OOD generalization; invariant for linear-probing performance).",
            "uuid": "e1201.0",
            "source_info": {
                "paper_title": "Learning and Leveraging World Models in Visual Representation Learning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "I-JEPA",
            "name_full": "I-JEPA (Joint-Embedding Predictive Architecture instantiation)",
            "brief_description": "A JEPA-style self-supervised method that predicts masked target latents from a source context in latent space; serves as a baseline and conceptual precursor to IWM.",
            "citation_title": "Self-supervised learning from images with a joint-embedding predictive architecture",
            "mention_or_use": "use",
            "model_name": "I-JEPA (latent JEPA predictor)",
            "model_description": "Latent-space predictive architecture where a predictor predicts masked target latents from a source context without strong conditioning on transformation parameters; typically uses a ViT encoder and a predictor of similar architecture but smaller capacity.",
            "model_type": "latent world model / JEPA predictive model",
            "task_domain": "Self-supervised visual representation learning; used as baseline to compare prediction/fidelity and downstream utility",
            "fidelity_metric": "Same metrics used: L2 latent prediction loss and retrieval-based MRR",
            "fidelity_performance": "Lower MRR under many augmentation settings reported in the paper (examples: Table 2 shows I-JEPA MRR often ≈ 0.00 for simple jitter settings), indicating weaker ability to model some transformations compared to high-capacity, conditioned IWM predictors.",
            "interpretability_assessment": "As a latent predictor, behaves between invariant and equivariant; visualizations and embedding similarity matrices indicate moderate sensitivity to augmentations but less capability than conditioned/high-capacity IWM predictors. No special interpretability mechanisms beyond JEPA analyses.",
            "interpretability_method": "Embedding similarity matrices and qualitative comparisons used in paper; MRR/retrieval diagnostics.",
            "computational_cost": "Uses ViT encoder and a predictor; predictor depth/width in baseline smaller than IWM's strongest predictor (e.g., 12-layer predictors tested). Pretraining/evaluation budgets comparable to other JEPA methods used in experiments (300 epochs).",
            "efficiency_comparison": "I-JEPA typically learns less powerful world models (lower MRR) under same predictor sizes/augmentation strengths; thus less reusable as a finetunable predictor compared to IWM Equi. Efficiency advantages not emphasized.",
            "task_performance": "Reported ImageNet and probing scores typically lower than IWM when leveraging finetuned predictor; exact numbers: I-JEPA attended probing ≈ 75.0 (Table 8), and various finetuning protocols show smaller gains from predictor finetuning compared to IWM Equi.",
            "task_utility_analysis": "Because I-JEPA is often neither strongly equivariant nor purposely invariant, its predictor tends to be less directly reusable for downstream discriminative finetuning compared to a capable IWM predictor.",
            "tradeoffs_observed": "Without explicit conditioning or sufficient predictor capacity, the predictor collapses to invariant behavior and the world model is weak (low MRR), reducing downstream utility despite similar architectural form.",
            "design_choices": "Less conditioning on transformation parameters compared to IWM; predictor capacity and augmentation choices determine whether it becomes more invariant or equivariant.",
            "comparison_to_alternatives": "Compared to IWM, I-JEPA is simpler but often underperforms when the goal is to learn a reusable world model; compared to MIM/MAE, I-JEPA sits between contrastive invariance and MIM equivariance in the representation abstraction spectrum.",
            "optimal_configuration": "Paper suggests that to make JEPA-style predictors useful as world models one must (a) condition on transformation parameters and (b) increase predictor capacity and use stronger augmentations — which motivates IWM's design choices.",
            "uuid": "e1201.1",
            "source_info": {
                "paper_title": "Learning and Leveraging World Models in Visual Representation Learning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "MAE",
            "name_full": "Masked Autoencoders (generative world model)",
            "brief_description": "A masked image modeling approach with an encoder-decoder architecture trained to reconstruct pixels/patches; considered a generative-style world model in input space and used as a baseline in comparisons.",
            "citation_title": "Masked autoencoders are scalable vision learners",
            "mention_or_use": "use",
            "model_name": "Masked Autoencoder (MAE)",
            "model_description": "Generative input-space world model implemented as an encoder-decoder: encoder processes visible patches, decoder reconstructs masked patches in pixel space; here treated as a generative world model (decoder plays the role of world model).",
            "model_type": "generative world model / input-space reconstruction",
            "task_domain": "Self-supervised visual representation learning; evaluated on ImageNet and downstream tasks",
            "fidelity_metric": "Reconstruction loss in pixel/input space (typically MSE or reconstruction objective); representation quality measured via linear probe, attentive probing, and finetuning performance in the paper.",
            "fidelity_performance": "MAE achieves strong end-to-end finetuning and attentive probing performance (Table 8 attentive ≈ 73.5 for MAE300), but when attaching a pretrained predictor/decoder as an evaluation head it provides negligible gains over a random predictor for classification finetuning — suggesting the MAE decoder's learned world-model-like function is not readily reusable for those discriminative tasks.",
            "interpretability_assessment": "Decoder reconstructs input-space pixels which is interpretable visually (reconstructions), but the paper reports a weak correlation between generation quality and representation quality; decoder as a world model is not shown to be readily interpretable in terms of task-relevant latent factors.",
            "interpretability_method": "Input-space reconstructions and downstream representation probes; no specific structured interpretability beyond visual inspection mentioned.",
            "computational_cost": "MAE encoder-decoder training costs; in comparisons MAE pretraining is performed for similar epoch budgets (e.g., 300 epochs) and uses similar ViT encoders. Exact FLOPs not enumerated in paper.",
            "efficiency_comparison": "When attaching a random vs pretrained predictor, MAE shows negligible gains — implying its decoder/world-model is not an efficiency win for predictor-fineturning pipelines in discriminative tasks compared to IWM's predictor which yields tangible gains.",
            "task_performance": "MAE end-to-end finetuning and attentive probing competitive (examples: attentive probing numbers in Table 8), but predictor reuse for classification offers little advantage versus a random head.",
            "task_utility_analysis": "High-fidelity input-space reconstruction does not necessarily translate to a reusable world model for downstream discriminative tasks—MAE decoder does not provide the same predictor-reuse benefit that a conditioned, equivariant IWM predictor does.",
            "tradeoffs_observed": "Generative reconstruction may preserve low-level details but that does not guarantee representations that are easy to adapt for discriminative tasks via finetuning of the decoder/predictor; i.e., reconstruction fidelity and downstream utility are not strongly coupled.",
            "design_choices": "Pixel-space reconstruction objective, encoder-decoder separation, no explicit conditioning on transformation parameters as in IWM.",
            "comparison_to_alternatives": "Compared to IWM Equi, MAE's decoder is less useful as a reusable predictor for discriminative finetuning, though MAE performs well under full end-to-end finetuning and in attentive probing.",
            "optimal_configuration": "Paper suggests that for reusability as a world model, conditioning and predictor design (latent prediction with explicit action/transform conditioning and capacity) are important — MAE lacks these and thus is suboptimal for predictor reuse despite strong reconstruction fidelity.",
            "uuid": "e1201.2",
            "source_info": {
                "paper_title": "Learning and Leveraging World Models in Visual Representation Learning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "RL world models (refs)",
            "name_full": "World models in reinforcement learning (e.g., Ha & Schmidhuber 2018; Hafner et al. 2019/2023)",
            "brief_description": "General class of predictive models in RL trained to predict consequences of actions in input or latent space; cited as successful prior work motivating learning world models for vision.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "RL latent/input-space world models (general)",
            "model_description": "In RL these are neural predictive models (often latent-space dynamics models) that model next observations and/or rewards conditioned on actions; examples include recurrent latent models and latent imagination methods (cited works: Ha & Schmidhuber 2018; Hafner et al. 2019/2023).",
            "model_type": "latent world model / model-based RL world model",
            "task_domain": "Reinforcement learning control domains, video prediction, policy learning and planning",
            "fidelity_metric": "Typically measured via next-state prediction error (MSE), reconstruction loss, or downstream control performance (cumulative reward); in this paper they are cited conceptually (no new metrics provided here).",
            "fidelity_performance": "No numerical fidelity/performance reported in this paper for these RL world models — they are cited as prior successful examples.",
            "interpretability_assessment": "Not discussed in detail here; general RL world models are typically black-box neural networks though some analyses exist in cited literature (not detailed in this paper).",
            "interpretability_method": "Not described in this paper; cited literature focuses on predictive accuracy and utility for planning.",
            "computational_cost": "Not enumerated here; RL world models often require significant compute for joint dynamics + policy training, but the paper only references them as motivation.",
            "efficiency_comparison": "Paper draws analogy: in RL, world models are reused for planning/policy — motivates reusing learned world models in vision rather than discarding them after pretraining. No direct computational comparisons provided here.",
            "task_performance": "Not provided in this paper; referenced works demonstrate success on RL benchmarks but specifics are in cited RL papers.",
            "task_utility_analysis": "Paper uses RL world models as conceptual precedent: in RL the learned model is leveraged for planning; in contrast, in computer vision world models are frequently discarded after pretraining — IWM aims to change that by enabling reuse.",
            "tradeoffs_observed": "Paper argues reusing world models can improve efficiency and task performance, analogous to RL, but does not provide RL-specific tradeoff data.",
            "design_choices": "Paper highlights conditioning on actions/transformations, latent-space prediction, and predictor capacity as parallel design lessons from RL world models applied to image representation learning.",
            "comparison_to_alternatives": "Used as a conceptual reference point; the paper's IWM can be seen as a vision counterpart that learns a predictive latent model conditioned on transformation 'actions'.",
            "optimal_configuration": "Paper borrows RL insight that conditioning on actions and adequate model capacity enhance world-model usefulness; translates these into concrete recommendations for image world models (condition on transform, strong transforms, sufficient predictor capacity).",
            "uuid": "e1201.3",
            "source_info": {
                "paper_title": "Learning and Leveraging World Models in Visual Representation Learning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Generative World Models (decoders)",
            "name_full": "Generative World Models / Decoder-based reconstruction models",
            "brief_description": "Class of models that invert transformations in input space (e.g., decoders in masked autoencoders), considered generative world models in the taxonomy and contrasted with latent JEPA world models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Generative world models (input-space decoders)",
            "model_description": "Decoder-style architectures trained to reconstruct masked/modified input pixels (e.g., in MIM/MAE). Viewed as world models that operate in input space, learning to invert masking or other corruptions.",
            "model_type": "generative world model (input-space)",
            "task_domain": "Masked image modeling, reconstruction-based self-supervised learning; evaluated for representation learning utility",
            "fidelity_metric": "Reconstruction loss (pixel MSE or other perceptual metrics) and generation quality; representation utility measured via downstream tasks.",
            "fidelity_performance": "Paper notes previous work shows generation quality does not necessarily correlate with representation quality (citing Chen et al., 2024). No new numeric fidelity values for generative models other than MAE baseline comparisons.",
            "interpretability_assessment": "Reconstructions are visually interpretable, but this does not guarantee that learned latents are optimally structured for downstream discriminative tasks.",
            "interpretability_method": "Visual inspection of reconstructions; downstream probing.",
            "computational_cost": "Decoder adds computational overhead compared to purely latent-predictive JEPA methods; exact costs depend on architecture (not exhaustively quantified here).",
            "efficiency_comparison": "Paper argues latent JEPA approaches can remove unnecessary information and are attractive because reconstruction quality in input-space does not equate to representation quality — suggesting latent predictive approaches (IWM) can be more efficient/useful when the decoder is not required for downstream tasks.",
            "task_performance": "Compared indirectly through MAE baselines: generative decoders yield good end-to-end finetuning but are less directly reusable as lightweight prediction heads for discriminative tasks versus a conditioned latent predictor.",
            "task_utility_analysis": "Generative fidelity (pixel reconstruction) often does not directly translate to downstream discriminative utility; latent conditioning and predictor capacity are important design factors to make a world model useful for downstream tasks.",
            "tradeoffs_observed": "High-fidelity input reconstructions can increase computational cost and may not yield proportional gains in representation adaptability; latent predictive models (IWM) provide alternative trade-offs favoring reuse and efficiency.",
            "design_choices": "Choosing latent vs input-space prediction, whether to condition on transforms, and whether to retain decoder after pretraining are key choices affecting utility and efficiency.",
            "comparison_to_alternatives": "Generative decoders (MAE) compare favorably under full finetuning but are less helpful when trying to repurpose a learned world model as a lightweight finetunable head; IWM's latent predictor offers a more directly reusable world model for discriminative tasks.",
            "uuid": "e1201.4",
            "source_info": {
                "paper_title": "Learning and Leveraging World Models in Visual Representation Learning",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Recurrent world models facilitate policy evolution",
            "rating": 2,
            "sanitized_title": "recurrent_world_models_facilitate_policy_evolution"
        },
        {
            "paper_title": "Dream to Control: Learning behaviors by latent imagination",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Mastering diverse domains through world models",
            "rating": 2,
            "sanitized_title": "mastering_diverse_domains_through_world_models"
        },
        {
            "paper_title": "Self-supervised learning from images with a joint-embedding predictive architecture",
            "rating": 2,
            "sanitized_title": "selfsupervised_learning_from_images_with_a_jointembedding_predictive_architecture"
        },
        {
            "paper_title": "Masked autoencoders are scalable vision learners",
            "rating": 2,
            "sanitized_title": "masked_autoencoders_are_scalable_vision_learners"
        },
        {
            "paper_title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
            "rating": 1,
            "sanitized_title": "data2vec_a_general_framework_for_selfsupervised_learning_in_speech_vision_and_language"
        }
    ],
    "cost": 0.01871875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning and Leveraging World Models in Visual Representation Learning
1 Mar 2024</p>
<p>Quentin Garrido garridoq@meta.com 
Univ Gustave Eiffel
CNRS
F-77454Marne-la-ValléeLIGMFrance</p>
<p>Mahmoud Assran 
Nicolas Ballas 
Adrien Bardes 
INRIA</p>
<p>Laurent Najman 
Univ Gustave Eiffel
CNRS
F-77454Marne-la-ValléeLIGMFrance</p>
<p>Yann Lecun 
Courant Institute
New York University</p>
<p>Center for Data Science
New York University</p>
<p>Fair At Meta 
Learning and Leveraging World Models in Visual Representation Learning
1 Mar 2024DA82DE9450FD0AEE7C4B4430B87F3328arXiv:2403.00504v1[cs.CV]119 Contrast: 172 Saturation: 096 Hue: 016 Colorized Brightness: 095 Contrast: 122 Saturation: 097 Hue: 011 De-blurred Brightness: 174 Contrast: 108 Saturation: 097 Hue: 001 Brightness: 131 Contrast: 161 Saturation: 100 Hue: -008
Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model.While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions.We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space.We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity.Additionally, we show that the predictive world model learned by IWM can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or surpasses the performance of previous self-supervised methods.Finally, we show that learning with an IWM allows one to control the abstraction level of the learned representations, learning invariant representations such as contrastive methods, or equivariant representations such as masked image modelling.</p>
<p>Introduction</p>
<p>Learning and leveraging world models is common practice in reinforcement learning (RL), with demonstrable success in the last few years in particular Ha and Schmidhuber (2018); Hafner et al. (2019Hafner et al. ( , 2023)).World models are commonly learned by training a network to predict the consequence of an action, either in input space (Yang et al., 2023), or in latent space (Hu et al., 2023;Hafner et al., 2023).Given such a broad view of world modelling, we seek to explore whether learning and leveraging world models can also be benificial in visual representation learning.</p>
<p>A wide family of self-supervised learning approaches are based on encoder-predictor architectures, wherein the encoder-predictor networks are trained to predict transformations of the data; e.g., masked image modelling (Bao et al., 2021;He et al., 2021), jointembedding architectures (Grill et al., 2020;Xie et al., 2022;Assran et al., 2023;Baevski et al., 2022), or equivariant prediction objectives (Gupta et al., 2023;Garrido et al., 2023b).If we regard transformations of the data as "actions," then we can easily relate selfsupervised learning approaches to world-modelling in reinforcement learning; see figure 2.</p>
<p>For instance, the decoder network in masked au-Figure 1 Visualisation of predictions in latent space with a learned Image World Model.We apply an action on a source image in latent space and retrieve the nearest neighbour of the predicted representation in a bank of 256 images.We see that IWM is capable of modeling transformations and undo corruptions, showing an understanding of the underlying image transformations.Image from: ai.meta.com/blog/yann-lecunadvances-in-ai-research/toencoders (He et al., 2021) can be thought of as a generative image world model, which learns to infer the effect of the "masking action" T (a) on an image y; in this case, the transformation parameters a (locations of masked image patches), are also fed to the decoder network.Methods based on joint-embedding predictive architectures (JEPAs), such as I-JEPA (Assran et al., 2023) or data2vec (Baevski et al., 2022), operate similarly, but can be seen as learning a latent image world model, which learns to infer the effect of the masking action on the representation of an image.If one does not condition the predictor on the transformation parameters, then the best we can hope for is learning representations that are invariant to the data transformations, as in BYOL (Grill et al., 2020) and SimSiam (Chen and He, 2020), wherein the image transformations correspond to various photometric and geometric data augmentations.However, despite some of the apparent similarities between world modelling in reinforcement learning and self-supervised learning from images, the learned world model in reinforcement learning is typically leveraged in downstream tasks, e.g., for planning (Hansen et al., 2022).In contrast, the learned world model in self-supervised learning is typically discarded after pretraining, as the main focus is often on the representation quality of the learned encoder network.This stems from the fact that most downstream tasks in computer vision are unrelated to the world modeling task.Common tasks of interest focus on discriminative aspects and as such, even when the predictor learns useful information, it is simply discarded.We postulate that discarding the world model in representation learning is wasteful, and that just like in RL, we can reuse this world model for downstream tasks.This motivates us to study, in more depth, learning world models as a paradigm for representation learning.We thus introduce Image World Models (IWM, illustrated to the right of figure 2) as a way to learn both good representations and strong reusable world models.IWM is based on JEPA and extends the usual latent inpainting to also include photometric transformations, allowing us to demonstrate the key aspects in learning a capable world model, which include the choice of predictor conditioning, the strength of the transformations, and the capacity of the world model.</p>
<p>We then focus on leveraging the learned world model for downstream tasks, and find that it can be leveraged through finetuning.Specifically, we find that finetuning the world model on top of the frozen encoder for downstream tasks provides improved performance over encoder finetuning; this is also achieved at a fraction of the cost and number of finetuned parameters.Moreover, only the world model learned by IWM exhibits this behavior; finetuning a randomly initialized network of the same architecture as the predictor does not provide such a performance improvement.This suggests that the world model should be a key part of the inference process, instead of being discarded.Inspired by instruction tuning (Wei et al., 2022;Zhang et al., 2023), we further show that the world model can be finetuned to solve multiple tasks at once, further improving efficiency.</p>
<p>Our study reveals another key aspect of representation learning with world models: the capacity given to the world model has a direct influence on the level of abstraction of the learned representations.Intuitively, if the predictor is the identity (i.e., no predictor, middle of figure 2), the network will capture high level semantic information, as it will only learn to encode what is shared between the input y and its transformation x.This is the driving force behind the representation quality of contrastive learning, where transformations are selected to only preserve the semantics of the image.On the other hand, as the predictor has more capacity and can effectively invert the effect of the transformations, the output of the encoder can retain more information about its input.These two ideas are at the core of equivariant representation learning; a predictor that can apply transformations effectively is equivariant, whereas a predictor that cannot is invariant.We find that a world model that is invariant to transformations performs better in linear evaluation, whereas one that is equivariant correlates with better world model finetuning.This gives a tradeoff between ease of adaption and raw performance.As such, learning representations by learning a world model gives us flexibility in the properties of the representations, making this an attractive representation learning framework.</p>
<p>Our contributions can be summarized as follows:</p>
<p>• We show how to leverage JEPAs to learn an Image World Model (IWM).The key aspects are: complexity of transformations, conditioning on transformations, and capacity of the predictor.• We show that equivariant world models can be leveraged for discriminative tasks.Finetuning the predictor leads to better performance compared to encoder finetuning, at a fraction of the cost.Inspired by instruction tuning, we also demonstrate that it can be finetuned on several tasks at once. 2 Related works</p>
<p>Augmentation invariant Self-Supervised Learning</p>
<p>At the core of contrastive methods lies augmentation invariance.Multiple augmented views of an image should lead to the same representation in latent space.The core of these methods is thus in how to avoid these representations collapsing.Sample-contrastive methods (Chen et al., 2020a;He et al., 2020;Chen et al., 2020b;Caron et al., 2021;Chen et al., 2021;Yeh et al., 2021;HaoChen et al., 2021;Oquab et al., 2023) avoid this phenomenon by pushing away representations coming from other data points.Dimensioncontrastive methods (Bardes et al., 2021;Zbontar et al., 2021;Ermolov et al., 2021;Li et al., 2022;Bardes et al., 2022) avoid collapse by considering the representations as a whole and encouraging maximization of information content.Both dimension-and sample-contrastive methods have been shown to lead to very similar representations (Garrido et al., 2023a).Prediction based methods (Grill et al., 2020;Chen and He, 2020) learn by predicting the augmented representations, but they also lead to invariant representations due to a lack of conditioning on the transformations.</p>
<p>World modeling in visual representation learning</p>
<p>While world modeling is a successful paradigm in reinforcement learning Hafner et al. (2019Hafner et al. ( , 2023) ) or video prediction Yang et al. (2023); Hu et al. (2023), it has yet to show clear benefits in representation learning.However, multiple families of approaches can be reframed in light of this.Equivariant selfsupervised learning methods (Devillers and Lefort, 2022;Park et al., 2022;Garrido et al., 2023b;Gupta et al., 2023;Dangovski et al., 2021) 2024), and while these approaches seem promising, their performance still remains below contrastive or MIM approaches.Recent work has also shown negative correlations between generation quality and representation quality (Chen et al., 2024).One shared aspect among these works is that the world model (predictor or decoder) is either discarded for evaluations, or only used to augment data (Hudson et al., 2023).We propose to go beyond these practices and show that we can learn a world model that is reusable for downstream tasks while still learning high-quality representations.</p>
<p>Method</p>
<p>We now describe Image World Models (IWM).It follows a Joint Embedding Predictive Architecture framework (LeCun, 2022) akin to I-JEPA (Assran et al., 2023).In this framework, the predictor is the instantiation of the world model.We consider a world model to be capable if it can apply transformations in latent space, and thus learns equivariant representations.As such, we call a capable world model equivariant1 and a poor world model invariant.</p>
<p>An appealing aspect of using JEPAs is that approaches which learn equivariant representations using contrastive methods often have to rely on an invariance loss to increase representation quality, whether explicitly (Gupta et al., 2023;Garrido et al., 2023b), or implicitly (Chavhan et al., 2023a).On the other hand, a JEPA style approach does not have this drawback, as the semantic aspect of the representation is learned through latent inpainting.</p>
<p>Working in latent space further allows the network to remove unnecessary information, or that which is too hard to predict.This makes the JEPA formulation attractive since, for reconstructive methods, the quality of the reconstruction is not necessarily correlated with representation quality Chen et al. (2024).</p>
<p>To train IWM, the first step is to generate source and target viewsx and y respectively in figure 2 -from an image I.</p>
<p>Target y.The target view is generated by applying a random horizontal flip, a crop, and color jitter (brightness, contrast, saturation, hue) to the original image I.No destructive augmentations such as grayscale are applied on the target to ensure that the target has as much information as possible.We further elaborate on this choice in appendix C.</p>
<p>Source x.For the source view, we start from the target y which we further transform.We first apply another color jitter, as well as destructive augmentations: grayscale, blur and solarization.This set of augmentations is the same as the one used in contrastive SSL.Finally, we also mask parts of the image following I-JEPA.We define our mask M x (a set of indices) as the union of 4 rectangular masks.Confer appendix A for exact implementation details.</p>
<p>Action a.We denote by a x→y the transformation parameters associated with the transformation of x to y, i.e., the invert of the initial transformation process.</p>
<p>(y).The use of the EMA network is crucial to avoid collapsed solutions.To condition the predictor, acting as our world model, it is fed with geometric information about the target in the form of mask tokens as well as a x→y .We denote these mask tokens as m a , which correspond to the positions in M C</p>
<p>x .The predictor p ϕ then takes as input the embedded source patches x c , transformation parameters a x→y and mask tokens m a .Its objective is then to match p ϕ (z x , a x→y , m a ) = ẑy to z y .</p>
<p>Loss.The loss function used is a squared L2 distance between the predictions ẑy and their targets z y :
L(x, y) = i∈M C x ∥p ϕ (f θ (x), a x→y , m a ) i −f EMA θ (y) i ∥ 2 2 .</p>
<p>Architecture and nomenclature</p>
<p>Our encoder is a Vision Transformer (Dosovitskiy et al., 2021), in particular we use the ViT-B/16 architecture.Our predictor is based on the same architecture with different depth and embedding dimension.</p>
<p>We denote instances of IWM as IWM Z X,Y where X is the depth of the predictor, Y its embedding dimension, and Z is either Inv or Equi depending on the capabilities of the world model.For example IWM Equi 18,384 means that the predictor is 18 layers deep, with 384 dimensional embeddings and exhibits equivariant behavior, i.e., has learned a versatile world model.</p>
<p>Learning an Image World Model for representation learning 4.1 Evaluating the quality of the world model</p>
<p>As discussed previously, learning equivariant representations and learning a world model are closely related problems.As such, we can borrow metrics from the equivariance literature to evaluate the quality of a trained world model.We rely on Mean Reciprocal Rank (MRR) (Kipf et al., 2019) as our main metric.</p>
<p>To compute it, we generate a bank of augmented target images (256 in practice).We feed the representation of the clean image through the predictor</p>
<p>Learning a strong Image World Model</p>
<p>In order to build a performant IWM, we isolate three key aspects: conditioning the predictor on transformations (or actions), controlling the complexity of the transformations, and controlling the capacity of the predictor.We show that not caring properly for either of those leads to invariant representations.</p>
<p>World model conditioning.We study two approaches to condition the predictor on the transformation information.</p>
<p>Sequence conditioning.One approach is simply to add tokens representing the transformation to the input of the predictor.Although this seems straightforward, it needs to be implemented in a way that breaks the permutation equivariance of the transformer predictor.To do so, every token is fed through a unique linear layer that allows the network to transform the information in a way that can be disambiguated by the predictor.Feature conditioning.Another option is to mix the information between the transformation and mask tokens by adding the conditioning as extra dimensions, then feeding the mask tokens through a 1x1 convolutional neural network to mix the information in the mask tokens and map back to the right dimensionality.</p>
<p>As we can see in Table 1, no conditioning leads to a world model that cannot apply transformations whereas both conditioning using the sequence or feature axes leads to good world models.We use the feature conditioning in practice as it leads to higher downstream performance.</p>
<p>Transformation complexity.We rely on data augmentation as used in contrastive approaches, consisting of color jitter (brightness, hue, contrast, saturation),</p>
<p>Visualizing predictions.</p>
<p>In the same way that we computed MRR, we can compare the predicted representations to a bank of transformed images and look at the image associated to the prediction's nearest neighbor.As we see in Figure 1 the world model learned by IWM is able to properly apply transformations in latent space.We can however see some inaccuracies when inverting grayscale as it is not properly invertible.These visualisations help reinforce the fact that IWM is able to learn strong world models for image transformations.</p>
<p>Confer appendix I for more visualizations.5 Leveraging world models for downstream tasks</p>
<p>A limitation of world models learned on images is that the task they solve is not aligned with most downstream tasks.We showed that IWM can apply color jitter or colorize images, but these are not the tasks that drive applications of computer vision.This is in contrast with LLMs where predicting the next token is one of the main applications of such models.We thus study how to leverage a world model in vision, for tasks that go beyond applying transformations.We focus on discriminative tasks such as image classification and image segmentation.</p>
<p>Predictor finetuning</p>
<p>For any task, the evaluation head needs to understand the learned latent space and leverage it to solve the problem at hand.This is something our learned predictor can do, suggesting that it has learned useful information that is not necessarily present in the encoder.However, since the predictor is trained to predict another valid representation, its output has no reason to lead to better downstream performance if used as is.This is why the predictor needs to be finetuned to solve discriminative tasks.We thus focus on comparisons with finetuning protocols, following He et al. (2021).All methods studied are pretrained and evaluated on ImageNet Deng et al. (2009) and use ViT-B/16 as encoders.</p>
<p>Prediction task.When finetuning the predictor, we still need to use it for a prediction task.In Table 3, we study various ways to define the prediction task and how it impacts performance.The first aspect we notice is that using the teacher network improves performance over the student.Using a random transformation or not is not an important factor, and the most important one is to predict another full image.This makes the evaluation more flexible as we do not have to reuse the pretraining objective for our evaluation.Using a CLS token to aggregate infor-  mation instead of a full image prediction is also a valid strategy, though it lowers the performance by half a point.This techniques has the advantage of being cheaper (N + 1 tokens vs 2N ) so it can be a good alternative depending on the use case.Overall, the simplest approach is the best: predicting an untransformed version of the full image.This makes the finetuning protocol easily reusable as it is not dependent on the pretraining task.We provide more detailed ablations in appendix D.</p>
<p>General Results.In Table 4, we compare predictor finetuning to encoder finetuning and end-to-end finetuning of both the predictor and encoder, using ViT-B/16 for the encoder.We see that IWM maintains or improves performance over I-JEPA and that an invariant behavior is better in encoder finetuning.Interestingly, predictor finetuning of the equivariant IWM is able to match the performance of finetuning of the invariant model's encoder.This shows that the protocol can be competitive as it trades parameters at inference time for a more computationally friendly adaptation.While this evaluation increases the number of parameters used at inference time, it still amortizes the forward pass through the backbone, something that full finetuning does not do.As such, as soon as multiple tasks are considered, using the finetuned predictor provides a higher throughput than regular finetuning.When comparing the use of a randomly initialized predictor (i.e., a large evaluation head) versus a pretrained predictor, we see negligible gains for MAE.This suggests that the world model learned by MAE is not better than a randomly initialized network for classification.For I-JEPA and IWM with an invariant world model, we see gains in performance lower than 1 point, suggesting that the world model is not powerful enough to be leveraged.However, when looking at IWM with an equivariant world model, we see a gain of 1.8 points over a random predictor.This shows that the predictor has learned useful information and properties that bring additional benefit to what the encoder has learned.The performance can be pushed further by finetuning end-to-end both the encoder and predictor, and IWM is able to outperform every other finetuning protocols.This allows us to get more performance out of a single pretraining since the world model is always trained.We hypothesize that the lack of performance for most approaches on end-to-end finetuning comes from the optimization complexity of finetuning a part of the network (encoder) while training from scratch another part (the predictor).We see in Table 5 that when aggregating the performance over all protocols, leveraging our IWM leads to the best performance with a frozen encoder, that is when allowed to leverage every part of the pretraining.Confer Appendix A for detailed performances.</p>
<p>Image Segmentation.We study in Table 6 the performance of I-JEPA and IWM on an image segmentation task on ADE20k.We observe similar trends as in image classification where the invariant model leads to the best encoder.However, finetuning the predictor with an equivariant model leads to significant gain over it, outperforming encoder finetuning by a large margin.Again, we observe gains in end-to-end finetuning.This further validates the potential of our IWM to be leveraged for a wide range of tasks.We Efficiency.In Figure 3, we study the efficiency of predictor finetuning compared to encoder finetuning.We see that when the number of parameters is comparable, and at multiple predictor sizes, predictor finetuning with IWM outperforms encoder finetuning by around 1 point compared to MAE, and by 1.5 points over IWM.This means that predictor finetuning is not only is a competitive protocol performance wise, but also with respect to efficiency of adaptation.We further study the behavior of IWM with a ViT-L/16 in section E. When comparing the end-to-end finetuning of a ViT-B with encoder finetuning of a ViT-L, we observe a gain in performance (84.4% vs 84.3%) with a fraction of the parameters (121M vs 307 M).This further shows how efficient leveraging the world model learned by IWM is, and that reusing all parts of your pretraining can prove as effective as scaling the encoder.</p>
<p>Multitask predictor tuning</p>
<p>We previously discussed efficiency gains when compared to encoder finetuning, but can improve efficiency even further.One of the main goal of representation learning is to obtain representations that can be used for a variety of tasks.And just like the predictor was trained to solve a variety of task (colorization, inpainting, changing color) we show that it can be finetuned on multiple tasks, inspired by prefix tuning (Li and Liang, 2021) and instruction For each task, we thus have a task token, as well as a task specific head and/or loss function.All of the task losses are then combined, and the predictor, as well as task specific heads, are updated.We study a simple scenario where the batch is evenly split between tasks, noting that other sampling strategies may lead to further improved performance.</p>
<p>We evaluate in Table 7 IWM Equi 18,384 (pretrained on Ima-geNet) on ImageNet, iNaturalist18 (Horn et al., 2018), SUN397 (Xiao et al., 2010), andPlaces205 (Zhou et al., 2014).For each task we train a single-task baseline where the total number of iterations is identical to the multi-task training.As such, training all four single-task baselines has exactly the same cost as the multi-task, although it leads to four different models instead of one.The multi-task predictor is able to achieve similar performance as the singletask predictors, with a moderate drop on most tasks but a significant increase in performance on SUN397.On average it achieves the same performance as the single-task predictors.This further demonstrates the efficiency gains of leveraging good world models, where the parameters are now shared across all tasks, making predictor finetuning lightweight at inference time for every task.</p>
<p>Overall, when a good world model is learned, it can be reused for downstream tasks by finetuning it.This leads to performance rivaling with encoder-finetuning at a fraction of the cost.It can be made even more efficient by doing a multi-task finetuning, highlighting the versatility of this approach.As we see in Table 8, when IWM learns an invariant world model, it achieves a behavior akin to contrastive approaches such as MoCov3 with significant performance gains in linear evaluation compared to MIM or other JEPA based approaches.Similarly, when IWM learns an equivariant world model, its behavior is akin to MIM methods such as MAE with lower performance in linear evaluation but more competitive performance in attentive probing.This suggests that a big difference between methods is not necessarily in the quality of the representation but in their abstraction level, i.e., how easy it is to extract information from them.Linear probing being one of the simplest evaluations, attentive being slightly more elaborate and finetuning being a more complex protocol.</p>
<p>In Figure 4, we see clear links between most suited evaluation protocols and equivariance of the world model.More invariant world models excel in linear evaluation and equivariant world models shine with larger evaluation heads such as in predictor finetuning.We also note that the richer representations stemming from equivariant world models lead to better performance on OOD datasets(see appendix F).This allows us to place families of approaches on a spectrum of representation abstraction in Figure 5. Contrastive methods occupy the high abstraction end of the spectrum, with information that is easily extractible with a simple protocol.However they suffer from lower peak performance when ignoring the adaptation cost, as seen in Table 5.On the opposite end lies Masked Image Modeling, which offers stronger performance with complex evaluations such as fine-  tuning but suffers in linear probing as information is not as easily accessible.By varying the equivariance of the world model, IWM is able to occupy the spectrum in between contrastive approaches and MIM, as we can see in Figure 4 and Table 8 with IWM Inv 12,384 and IWM Equi 18,384 being the two extremes of the IWM spectrum.This spectrum can be summarized by the SSL ethos of "Learning what is predictible".Learning with a weak world model means that it cannot model the world properly and the encoder removes the information that cannot be predicted.On the other hand, if the world model is very powerful, the representation does not need to be as abstract or semantic as it can find a way to predict representations in any situation.This means that learning a world model offers a measurable way to control the level of abstraction of the representations.</p>
<p>Conclusion and future perspectives</p>
<p>We introduced IWM, an approach to learn selfsupervised visual representations with world models.With an in-depth study, we provided guidelines and key components for learning a good image world model.Conditioning the world model with the image transformation is crucial to avoid collapsing to classical SSL behavior.Using strong transformations is also key to ensure that the world model learns to model more complex behavior and be useful.Finally, enough capacity is needed for modeling complex behaviors.We showed that only a capable world model can be reused for discriminative task.This led to our predictor finetuning protocol that matches encoder finetuning at a fraction of the cost, showing that world models are versatile evaluation heads.We further adapted it to solve multiple tasks at once without losing performance.Finally, we studied how learning a world model impacts representation quality.A capable world model learns rich representations that improve performance on downstream tasks such as image classification and semantic segmentation.Additionally, learning an invariant world model led to better representations for linear evaluation.While MIM and Contrastive approaches are two ends of a spectrum in terms of representation abstraction, Image World Models allow us to interpolate between them.As such, we believe that learning image world models is a very promising framework for visual representation learning.</p>
<p>Broader impact statement</p>
<p>This paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.</p>
<p>minimization.In International Conference on Learning Representations, 2018.https://openreview.net/forum?id=r1Ddp1-Rb.Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang.Instruction tuning for large language models: A survey, 2023.</p>
<p>Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva.Learning deep features for scene recognition using places database.In NeurIPS, 2014.</p>
<p>Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba.Semantic understanding of scenes through the ade20k dataset.IJCV, 2019.</p>
<p>0.5.The features are average pooled along the sequence axis to obtain a global representation which is then fed to a linear layer.We use a batch size of 16,384, with the LARS (You et al., 2017) optimizer and a learning rate of 6.4 with a warmup of 10 epochs.The learning rate then follows a cosine annealing schedule.Weight decay is set to 0 and momentum to 0.9.</p>
<p>Attentive.The attentive head is taken from Chen et al. (2023).It consists of a cross attention block where the attention is computed between an additional token the unpooled representations.This allows an adaptive pooling strategy.We train for 90 epochs on ImageNet.We sample random crops of images with scale between 0.3 and 1, then apply a horizontal flip with probability 0.5.We also apply the same augmentations as used for the source transformations besides masking.We use a batch size of 1024 and AdamW optimizer with a learning rate of 1 × 10 −4 ,β 1 = 0.9, and β 2 = 0.999.It follows a cosine annealing schedule.We use a weight decay of 0.01 kept constant during training.</p>
<p>Encoder finetuning.We append a linear layer to the end of the encoder as for the linear evaluation and train for 100 epochs on ImageNet.We use the same RandAugment (Cubuk et al., 2020) strategy as MAE (He et al., 2021) as well as CutMix (Yun et al., 2019) and MixUp (Zhang et al., 2018).For RandAugment we use the string 'rand-m9-mstd0.5-inc1'.We use random erasing with probability 0.25 in pixel mode.We use a mixup α of 0.8, cutmix α of 1 and label smoothing of 0.1.</p>
<p>For the optimization we use AdamW with a learning rate of 2 × 10 −3 with 5 epochs of warmup followed by a cosine annealing schedule, weight decay of 0.005 and a batch size of 1024.We also use a drop path rate of 0.2 through the encoder and a layer wise learning rate decay of 0.65.</p>
<p>Predictor finetuning.When finetuning the predictor we use an attentive head on top of the predictor output.</p>
<p>We plug the predictor on top of the teacher network and it is tasked with predicting the whole target image, with null transformation parameters.We use the same augmentation protocol as for encoder finetuning.We train for 100 epochs on ImageNet with a batch size of 1024.We use AdamW for the optimizer, a learning rate of 1 × 10 −3 with a 5 epoch warmup then cosine annealing schedule.We use a weight decay of 0.1, no layer wise lr decay and a drop path rate of 0.2 through the predictor.Importantly, if the predictor is pretrained we divide it's learning rate by 10, and keep it identical to the attentive if head if random.Multitask predictor finetuning.To give a clearer presentation of the protocol, we provide a graphical version of multitask predictor finetuning in figure S2.For the training in itself, we follow the same protocol as for predictor finetuning but train for the equivalent of 50 ImageNet epochs.The batch size used is 512 for each task, where the batch is independently split between tasks.When training on a single task, we simply use 512 as the batch size and also train for 50 ImageNet epochs.</p>
<p>End to end finetuning.We follow the protocol of predictor finetuning but tweak certain parameters.First, the encoder also gets his learning rate divided by 10 like the predictor.The factors are treated separately and ablated for all methods.We use a 0.9 layer decay across the combination of predictor and encoder.The learning rate used is 2 × 10 −3 and all other parameters are identical to predictor finetuning.</p>
<p>Segmentation.We give here details about our protocol for semantic segmentation evaluations.We use the MMSegmentation library Contributors (2020).We fine-tune our pretrained models (either encoder only, predictor only, or end-to-end) with an UperNet head Xiao et al. (2018)   and MoCov3 in predictor finetuning.For IWM Equi 12,384 , we see the same behavior as IWM Equi 18,384 but with slightly lower performance.This is consistent across all evaluations.Yet, even when accounting for scale of the predictor to compare with I-JEPA and IWM Inv 12,384 , all of our previous conclusions still hold.For MoCov3, it was the only method which did not perform well when attaching a random predictor to it.While we do not have conclusive evidence, we hypothesize that it is related to the low norm of its output.Adding a normalization between the encoder and predictor did not help.</p>
<p>B Complete finetuning results</p>
<p>We can see in table S3 that the conclusions drawn from table 3 still hold over a larger setting.Notably, using null latents is more flexible while not changing performance, using the teacher always gives a small boost in performance, and predicting only one token lowers performane by roughly half a point.</p>
<p>E Scaling to larger models</p>
<p>In order to scale to larger models, such as a ViT-L/16 encoder, multiple challenges need to be overcome.Notably, both the depth and the width of the predictor must be scaled in order to increase the number of parameters of the predictor to a suitable number.Scaling the width can lead to instabilities and hyperparameters such as the EMA schedule become more important.We noticed that a ratio of predictor weights/encoder weights of around 0.3 is suitable to learn a good world model.We study in table S4 the performance when scaling to a larger ViT-L/16.We see that the observation we made with the smaller ViT-B/16 still hold.The invariant model is the best on encoder finetuning, and predictor finetuning improves the performance significantly.Here again, end-to-end finetuning leads to performance gains.</p>
<p>F Evaluation on downstream datasets beyond ImageNet</p>
<p>We evaluated I-JEPA and IWM on iNaturalist18 (Horn et al., 2018), SUN397 (Xiao et al., 2010) and Places205 (Zhou et al., 2014) using attentive probing.We train our models for 50 epochs on iNaturalist18, 12 for Places205 and 28 for SUN397.As we can see in table S5, IWM consistently improves over I-JEPA and MAE when pretraining all methods for 300 epochs.We notice that while IWM Equi 18,384 is not the top performing model on ImageNet, it significantly outperforms it's invariant counterpart, with gains of 2.6 points on iNaturalist, 0.7 points on SUN397 and 1.1 point on Places205.This suggests that while the richness of the representation of an equivariant model is not optimal for in domain performance, it helps improve generalisation to downstream tasks.Difference in embedding space between invariant and equivariant behaviours.Each image is augmented 16 times and we compute the similarity matrix between all images.The yellow regions indicate high similarities between samples originating from the same image.We can see more variations in the equivariant model, or in I-JEPA where invariance is not enforced.This suggests that augmentations influence the representation more in these models.</p>
<p>G Visualizing representation differences between invariant and equivariant behavior</p>
<p>As we see in figure S3, the invariant model collapses augmented views to very similar embeddings, as shown by the high similarity in the diagonal blocks.On the other hand the equivariant model shows more variation, which shows that augmentation information is more present in the representation.Interestingly, I-JEPA has a behaviour in between because it was not trained to be either invariant or equivariant.I-JEPA has no force controlling how information is kept or removed from the representation.</p>
<p>H On the meaning and role of invariance in Self-Supervised learning</p>
<p>One of the key component of the success of self-supervised learning is augmentation invariance Chen et al. (2020a).We can say that we have learned invariant representations if ∀a, f θ (x) = f θ (T (a, x)).However there are many scenarios that satisfy this property.The two main ones that we are interested in are:</p>
<p>• Any augmented view leads to the same information as the clean image</p>
<p>• The encoder removes the information related to the transformation In the first case, the representations still contain all of the information about the input, whereas in the second we are removing information that can be deemed superfluous.In the case of contrastive methods, the focus is usually on removing information.Indeed if an image and its grayscale version are made to have the same representation, the encoder must remove color information.This is one of the key drivers of performance of such methods.By removing information until only the semantics of the image remains, the representations will be easy to leverage for a task such as classification.</p>
<p>We can thus wonder if the first invariance scenario also leads to improved performance, and if we can even leverage it.As we have demonstrated how IWM is able to preserve information, and we have a predictor that can apply transformations, we can marginalize over augmentations to create invariant representations in an efficient way.Here, we do not need to apply the encoder on all augmented views, but can directly use the predictor which is more compute efficient.If we consider a set of randomly sampled augmentations A such that card(A) = N we can compute an invariant representation as
z Inv x = 1 N N i=1 p ϕ (f θ (x), A i , m Ai )
We can then visualize which image has representation most similar to z Inv We can notice that the nearest neighbour is the original non-augmented image, followed by images with small transformations.</p>
<p>As we can see in figure S4, the images that have representations which are most similar with z Inv</p>
<p>x are the clean image and images with small transformations.We also know that our encoder preserves augmentation related information and is thus not invaraint to transformations.Combining these two facts tells us that the marginalizaiton process creates a clean representations, akin to the first kind of invariance.However, when looking at table S6 we can see that no performance gain is present when using invariant representations obtained by marginalizing over predictions.This is true even with 128 augmented views, which already increases the compute budget by a factor of around 64.As such, using invariant representations that preserve the content of the image is not necessarily beneficial for downstream evaluation.</p>
<p>Overall, the key to the success of augmentation invariance in contrastive learning is not just in building invariant representations, but in the way that the representations are invariant.Building invaraince by removal of information has been shown to be very effective (Chen et al., 2020a), whereas we see here that invariance by always predicting the representation of the clean image is not necessarily helpful.This does not mean that equivariant representations cannot build invariances that are useful from downstream tasks, as the contrary was shown in Chavhan et al. (2023b), but that we have to be careful in how we create invariant representations.</p>
<p>I Additional qualitative evaluations of the world model</p>
<p>Source</p>
<p>Groundtruth</p>
<p>NN of Prediction</p>
<p>Figure S5 Randomly selected retrieval samples of our world model.For each image, we generate 256 augmented views and apply transformations in latent space.We then retrieve the nearest neighbor of the prediction and visualize whether it is close to the groundtruth or not.The learned world model performs well in most settings but has some inaccuracies with inverting grayscale.</p>
<p>Source</p>
<p>Groundtruth</p>
<p>NN of Prediction</p>
<p>Figure S6 Randomly selected retrieval samples of our world model.For each image, we generate 256 augmented views and apply transformations in latent space.We then retrieve the nearest neighbor of the prediction and visualize whether it is close to the groundtruth or not.The learned world model performs well in most settings but has some inaccuracies with inverting grayscale.</p>
<p>Figure 3
3
Figure 3 Finetuning efficiency.When taking into account the number of finetuned parameters, predictor finetuning is significantly more efficient than finetuning the encoder.</p>
<p>Figure 4 Figure 5
45
Figure4While the level of equivariance influences performance in Linear and Predictor finetuning setting, it is hardly correlated to Attentive probing.This suggests that there is a trade-off in terms of the level of abstraction of the representation, and that different evaluation protocols evaluate different properties.</p>
<p>Figure S2
S2
Figure S2Multitask tuning of the predictor.We sample a batch uniformly across task which is then fed through the predictor with an additional task token, indicating which task is being solved.The predictions are then fed through a task specific head and losses are summed.</p>
<p>Figure S3Difference in embedding space between invariant and equivariant behaviours.Each image is augmented 16 times and we compute the similarity matrix between all images.The yellow regions indicate high similarities between samples originating from the same image.We can see more variations in the equivariant model, or in I-JEPA where invariance is not enforced.This suggests that augmentations influence the representation more in these models.</p>
<p>Figure S4
S4
Figure S4Retrieval of invariant representations computed using 256 augmentations in latent space.In the top row we visualize some of the corresponding image and on the bottom the nearest neighbours of the invariant representation.We can notice that the nearest neighbour is the original non-augmented image, followed by images with small transformations.</p>
<p>Encoder Predictor/World Model Transformation, Action Decoder/World Model Transformation, Action Transformation, Action JEPA world model Generative World model Joint Embedding</p>
<p>Multiple families of methods with related architectures can be distinguished, in which the conditioning or not of their world model is a key distinction.Generative World Models are trained to invert a transformation in input space, leveraging an autoencoder framework.Methods for world modeling and representation learning can be instantiated in this way.Joint Embedding methods get rid of the world model but operate in latent space by encoding what is common between transformed inputs.It is the main class of SSL methods.JEPA World Models can be seen as a more general framework where a world model is trained in latent space.This family has been very successful both in reinforcement learning and in representation learning, and is where Image World Models (IWM) falls.
UnconditionalDenoising Autoencoders Variational AutoencodersSiamese, SimCLR, VICReg, DINOBYOL, SimSiamConditionalGenerative World Models Masked Image ModelingN/ALatent world models, I-JEPA, Equivariant SSL, IWM (Ours)Figure 2</p>
<p>Table 1
1
Influence of predictor conditioning on the quality of the world model.Both Sequence and Feature conditioning lead to good world models .Gray is our default setting. of predicting the target image.We then compute the distance between the prediction and the augmented representation bank from which we get the rank of the target in this NN-graph.Averaging the reciprocal ranks over multiple images and transformations gives us MRR which tells us about the quality of the world model.A MRR close to 1 means that the world model is able to apply the transformation, on the contrary a MRR close to 0 means that it cannot.
Conditioning: None Sequence FeatureMRR0.000.820.79with the goal</p>
<p>Table 2
2
Impact of predictor architecture and transformations on MRR.Learning an effective world model requires complex transformations and adequate predictor capacity.Gray is our default setting.Red and Green respectively indicate invariant and equivariant behavior.If the prediction task is too easy, then the predictor will not learn anything useful.As presented in Table2, the stronger the augmentations, the easier it is to learn a strong world model.We provide more detailed ablations on the augmentations in Appendix C, where we see the trend continuing on a wider range of augmentation scenarios.
Predictor:I-JEPAIWM(depth, dim.):(12,384) (12,384) (18,384)Jitter0.000.110.25+ Destructive0.000.090.79+ Strong Jitter0.000.810.85grayscale, blur, and solarization. We refer to thelast three as destructive since they remove informa-tion. Beyond the set of transformations modeled,their strength must also be adequate to learn a use-ful world model.
World model capacity.If the transformation is complex, the predictor needs more capacity to be able to apply it, motivating capacity as a crucial factor in learning Image World Models.As we can see in Table2, a deeper predictor enables us to learn a strong world model on a wider range of augmentations, and is key to the success of IWM.We study in more detail the influence of depth on achieving a good world model in appendix C. For 12 layers, jitter equivariance is achieved 1 out of 5 times whereas for the 18 layers, it is achieved 4 out of 5 times.As such, predictor capacity is a key component of a strong world model.</p>
<p>Table 3
3
How to predict for predictor finetuning.Using the teacher improves performance, and the exact prediction task is not crucial.Null latents are more flexible and perform better.For better efficiency, a full prediction is not needed but leads to a small drop in performance.Gray is our default setting.
SettingImageNet Top-1 (%) GapDefault82.9-+ Teacher83.2+ 0.3+ Null latents83.3+ 0.1+ Pred only one token82.8-0.5</p>
<p>Table 4
4
Finetuning evaluations on ImageNet-1k.We evaluate prediction based methods by finetuning their encoder, by keeping the encoder frozen and finetuning their predictive world model or by finetuning both.Finetuning the world model is highly effective with IWM when it exhibits an equivariant behavior.This behavior is absent or less clear with other methods, showing the importance of a strong world model.
MethodEpochsNo predictor Frozen encoder, tuned predictor End to endEncoderRandom Init.PretrainedMAE300 160082.7 83.682.4 83.082.7 (+0.3) 83.1 (+0.1)82.3 83.3I-JEPA30083.079.180.0 (+0.9)82.0IWM Inv 12,38430083.380.581.3 (+0.8)82.7IWM Equi 18,38430082.981.583.3 (+1.8)84.4</p>
<p>Table 5
5
Peak performance achieved from a single pretraining instance.We compare ImageNet Top-1 accuracy with a frozen encoder or when allowing any evaluation head with any protocol, finetuning or not, with a predictor on top of the encoder or not.
MethodEpochs Frozen Encoder Any protocolDINO160082.082.8MOCOv330076.483.2iBOT160083.084.0MAE160083.183.6I-JEPA30080.082.0IWM Inv 12,38430081.383.3IWM Equi 18,38430083.384.4</p>
<p>Table 6
6
Finetuning for segmentation on ADE20k.Similar to image classification, we observe that predictor finetuning improves performance and outperforms encoder finetuning.
MethodEncoder Predictor End to endI-JEPA44.245.445.1IWM Inv 12,38445.645.746.5IWM Equi 18,38444.246.847.0provide additional details in Appendix A.2.</p>
<p>Table 7
7
Multi-task finetuning.Finetuning the predictor on multiple tasks at once performs similarly as finetuning it on each task separately.This enables the use of a single prediction head for multiple task, amortizing its cost.
DatasetSingle-task Multi-task DifferenceImageNet80.879.6-1.2iNat1872.472.0-0.4SUN39775.678.2+2.6Places20564.864.1-0.7Average73.473.5+0.1tuning Wei et al. (2022); Zhang et al. (2023) in LLMs.The general idea, that we illustrate graphically in sup-plementary Figure S2, is to give new learned tokensto the predictor to indicate which task it is trying tosolve. This is reminiscent of DyTox Douillard et al.(2022) which uses task tokens for continual learning.</p>
<p>Table 8
8
Linear and attentive probing performance on ImageNet-1k.IWM Inv performs similarly to contrastive methods and IWM Equi to mask modeling ones.
MethodEffective Epochs Linear AttentiveMoCoV330076.376.4MAE30060.273.5MAE160068.076.0I-JEPA30070.075.0IWM Inv 12,38430074.577.0IWM Equi 18,38430067.575.16 Image World Models enable flexiblerepresentationsTo complete our analysis of IWM for representationlearning, we study how it performs on lightweightevaluation protocols that are commonly used in self-supervised learning. We focus on linear Chen et al.(2021) and attentive probing Chen et al. (2023).</p>
<p>on the ADE20k semantic segmentation datasetZhou et al. (2019)for 160k iterations and report the validation mIoU.We concatenate the last 4 layers of the predictor, or encoder for encoder only finetuning, and feed the result to the segmentation head.At training time we resize the images at the pretraining resolution.At testing time we do not resize the images and interpolate the positional embeddings to the original resolution.For all setups and methods we pick the best run among several learning rate values: 1e − 5, 2e − 5 and 3e − 5. We use a weight decay of 0.01 and a linear learning rate decay schedule.</p>
<p>Table S1
S1
Complete results of tables 4 and 5.We provide complete results for table 4 and table 5 in tableS1.Some interesting behaviors are IWM Equi
MethodEpochsNo predictor Frozen encoder, tuned predictor End to endEncoderRandom Init.PretrainedDINO160082.882.0N/A82.1MOCOv330083.256.4N/A79.4iBOT160084.083.0N/A82.830082.782.482.7 (+0.3)82.3MAE60083.282.883.0 (+0.2)83.1160083.683.083.1 (+0.1)83.3I-JEPA30083.079.180.0 (+0.9)82.0IWM Inv 12,384 IWM Equi 12,384300 30083.3 82.780.5 81.381.3 (+0.8) 82.7 (+1.4)82.7 83.3IWM Equi 18,38430082.981.583.3 (+1.8)84.412,384</p>
<p>Table S4
S4
With a ViT-L/16 encoder, we observe a similar trend as with the base model.Significant gains are observed with a good world model, allowing it to surpass encoder finetuning.
MethodEpochs Encoder Predictor End to endI-JEPA30084.179.9IWM Inv 18,38430084.381.5IWM Equi 36,51230083.785.085.4</p>
<p>Table S5
S5
When evaluating with attentive probing on downstream task, being equivariant improves performance across the board.All methods use ViT-B/16 encoders and were pretrained for 300 epochs on ImageNet
MethodImageNet iNat18 SUN397 Places205MAE73.550.170.260.3I-JEPA75.050.469.258.3IWM Inv 12,38477.051.671.059.4IWM Equi 18,38475.154.271.760.5</p>
<p>Table S6
S6
Linear evaluation on marginalized representations.Using more augmented prediction to create an invariant representation does not improve performance.
Number of predictions (N )1 (default)81632128ImageNet Top-1 accuracy (%)64.564.3 64.6 64.6 64.4
This is an abuse of language as not all considered transformations form a group, but it is used for clarity.
ContextTarget Transformation parametersPositions to predict World ModelFigureS1IWM (Image World Model).Starting from an image, two augmented views are produced: the source and the target.The source view is partially masked to form the context and then encoded to be used as conditioning for the world model, instantiated by the predictor.The target is encoded through an exponential moving average of the encoder, and target positions are sampled as the masked patches of the source image.Conditioned on the transformation parameters between the source and target, the encoded source image, and the positions to predict, the predictor is trained to predict the target representations.A Experimental detailsA.1 PretrainingWe provide a more detailed architecture for IWM in figureS1.Architecture and optimization.All of our models use a ViT-B/16 encoder trained for 300 epochs on ImageNet.We use the AdamW optimizer Loshchilov and Hutter (2019) with 1 × 10 −3 as our learning.We further use β 1 = 0.9 and β 2 = 0.999.The learning rate follows a linear warmup for 40 epochs and then a cosine annealing.We use an iteration per epoch scale of 1.25 for the scheduler, which stretches the scheduler and makes the training end before the end of the schedule.Not having a 0 learning rate near the end of training was found beneficial in our experiments.We use a cosine weight decay schedule which goes from 0.04 to 0.4.Source and target.In practice we build the source and target separately by first applying a random crop of scale between 0.3 and 1.We then apply a horizontal flip with probability 0.5.We will call the resulting image I ′ .Target transformations.Starting from I ′ we then apply a color jitter with probability 0.8, brightness maximum strength 0.4, contrast maximum strength 0.4, hue maximum strength 0.1, and saturation maximum strength 0.2.Source transformations.Starting from I ′ we apply a color jitter with probability 0.8, brightness maximum strength 0.4, contrast maximum strength 0.4, hue maximum strength 0.1, and saturation maximum strength 0.2.A gaussian blur of radius between 0.1 and 2 is applied with probability 0.2, solarization with probability 0.2 and grayscale with probability 0.2.These augmentations correspond to the ones used in BYOL(Grill et al., 2020).We then generate a mask M x as the union of 4 masks of area between 0.15 and 0.2 of the image, with aspect ratios between 0.75 and 1.5.All of the patches in M x are then dropped from the source x.Predictor conditioning.We rely on the feature mixing strategy.Consider a mask token m ∈ R d and a ∈ R k a vector of k scalars corresponding to augmentation parameters.We first add position embeddings to m to indicate which patch of the target it needs to predict.We then concatenate m and a and feed them through a three layer fully-connected network with ReLU activation and dimensions d, d, d.This gives us a mask token that contains information about all of the transformation.Both the geometric aspect of where to predict and details on the photometric augmentations.A.2 EvaluationFor all evaluations on image classification, the augmentations applied to compute the validation accuracy are a resize to 256 followed by a 224 by 224 center crop.All hyperparameters reported are the optimal ones, chosen after careful tuning for every method.Linear.We take inspiration from the protocol ofChen et al. (2021).We train for 90 epochs on ImageNet.We sample random crops of images with scale between 0.08 and 1, then apply a horizontal flip with probability We study in tableS2the impact of augmentations used during pretraining, along with the depth of the predictor.We notice that depth is a deciding factor in the quality of the learned world model, where 4 out 5 scenarios with color are able to achieve color equivariance for the 18 layer predictor, compared to only 1 for the 12 layer predictor.The strength of the augmentations also plays a role and too weak augmentations do not lead to an equivariant model.C Impact of data augmentationOn the asymmetry of augmentations.The asymmetry of augmentations is both a conceptual choice, to make the augmentations used more similar to contrastive approaches, but also a practical one.When learning an invariant world model with symmetric augmentations we noticed a drop in performance of 2 points on ImageNet in attentive probing and 1.5 points on linear probing.While this drop is not catastrophic, it is sufficient to recommend using asymmetric augmentations.As the depth of the predictor decreases, we expect this gap to widen.On the other hand, when looking at an equivariant predictor, we did not notice any notable change in performance.This suggests that learning world models can also help improve stability over the choice of augmentations.The predictor does not have to be designed by keeping in mind which information may get removed but only by whether or not it can apply the transformation.D Impact of the prediction task on predictor finetuning performanceIn order to use predictor finetuning to solve downstream tasks, we need to apply a prediction task.We aim at giving a more combinatorial view of table 3 in this appendix.SourceGroundtruthNN of PredictionFigureS7Randomly selected retrieval samples of our world model.For each image, we generate 256 augmented views and apply transformations in latent space.We then retrieve the nearest neighbor of the prediction and visualize whether it is close to the groundtruth or not.The learned world model performs well in most settings but has some inaccuracies with inverting grayscale.SourceGroundtruthNN of PredictionFigureS8Randomly selected retrieval samples of our world model.For each image, we generate 256 augmented views and apply transformations in latent space.We then retrieve the nearest neighbor of the prediction and visualize whether it is close to the groundtruth or not.The learned world model performs well in most settings but has some inaccuracies with inverting grayscale.Brightness Min MaxContrast Saturation Hue Figure S9Application of the world model on precise transformations.For each parameter, we vary its value on a grid to see whether the model is able to predict small changes.The model is able to show the gradient of transformations, highlighting again the capabilities of the world model.We can still notice some imperfections however, as the model was only trained on combinations of augmentations.To make changes more visible, we used a model trained with a strong color jitter for this figure.Brightness Min MaxContrast Saturation Hue Figure S10Application of the world model on precise transformations.For each parameter, we vary its value on a grid to see whether the model is able to predict small changes.The model is able to show the gradient of transformations, highlighting again the capabilities of the world model.We can still notice some imperfections however, as the model was only trained on combinations of augmentations.To make changes more visible, we used a model trained with a strong color jitter for this figure.Brightness Min MaxContrast Saturation Hue Figure S11Application of the world model on precise transformations.For each parameter, we vary its value on a grid to see whether the model is able to predict small changes.The model is able to show the gradient of transformations, highlighting again the capabilities of the world model.We can still notice some imperfections however, as the model was only trained on combinations of augmentations.To make changes more visible, we used a model trained with a strong color jitter for this figure.
Self-supervised learning from images with a joint-embedding predictive architecture. Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann Lecun, Nicolas Ballas, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Data2vec: A general framework for self-supervised learning in speech, vision and language. Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli, International Conference on Machine Learning. PMLR2022</p>
<p>Beit: Bert pre-training of image transformers. Hangbo Bao, Li Dong, Furu Wei, arXiv:2106.082542021arXiv preprint</p>
<p>Vicreg: Variance-invariance-covariance regularization for selfsupervised learning. Adrien Bardes, Jean Ponce, Yann Lecun, arXiv:2105.049062021arXiv preprint</p>
<p>VICRegl: Self-supervised learning of local visual features. Adrien Bardes, Jean Ponce, Yann Lecun, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and</p>
<p>Kyunghyun Cho, Advances in Neural Information Processing Systems. 2022</p>
<p>Emerging properties in self-supervised vision transformers. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, Armand Joulin, ICCV. 2021</p>
<p>Quality diversity for visual pre-training. Ruchika Chavhan, Henry Gouk, Da Li, Timothy Hospedales, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)October 2023a</p>
<p>Amortised invariance learning for contrastive self-supervision. Ruchika Chavhan, Jan Stuehmer, Calum Heggan, Mehrdad Yaghoobi, Timothy Hospedales, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>A simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, ICML. PMLR2020a</p>
<p>Context autoencoder for self-supervised representation learning. Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang Zeng, Jingdong Wang, International Journal of Computer Vision. 2023</p>
<p>Exploring simple siamese representation learning. Xinlei Chen, Kaiming He, CVPR. 2020</p>
<p>Xinlei Chen, Haoqi Fan, Ross Girshick, Kaiming He, arXiv:2003.04297Improved baselines with momentum contrastive learning. 2020barXiv preprint</p>
<p>An empirical study of training self-supervised vision transformers. Xinlei Chen, Saining Xie, Kaiming He, ICCV. 2021</p>
<p>Deconstructing denoising diffusion models for selfsupervised learning. Xinlei Chen, Zhuang Liu, Saining Xie, Kaiming He, 2024</p>
<p>Text-to-image diffusion models are zero-shot classifiers. Kevin Clark, Priyank Jaini, arXiv:2303.152332023arXiv preprint</p>
<p>MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. 2020</p>
<p>Randaugment: Practical automated data augmentation with a reduced search space. Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, Quoc Le, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Rumen Dangovski, Li Jing, Charlotte Loh, Seungwook Han, Akash Srivastava, Brian Cheung, Pulkit Agrawal, Marin Soljačić, arXiv:2111.00899Equivariant contrastive learning. 2021arXiv preprint</p>
<p>Imagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. 2009</p>
<p>Equimod: An equivariance module to improve self-supervised learning. Alexandre Devillers, Mathieu Lefort, arXiv:2211.012442022arXiv preprint</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, ICLR. 2021</p>
<p>Dytox: Transformers for continual learning with dynamic token expansion. Arthur Douillard, Alexandre Ramé, Guillaume Couairon, Matthieu Cord, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev, Vaishaal Shankar, Joshua M Susskind, Armand Joulin, arXiv:2401.08541Scalable pre-training of large autoregressive image models. 2024arXiv preprint</p>
<p>Whitening for selfsupervised representation learning. Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, Nicu Sebe, 2021</p>
<p>On the duality between contrastive and non-contrastive self-supervised learning. Quentin Garrido, Yubei Chen, Adrien Bardes, Laurent Najman, Yann Lecun, The Eleventh International Conference on Learning Representations. 2023a</p>
<p>Self-supervised learning of split invariant equivariant representations. Quentin Garrido, Laurent Najman, Yann Lecun, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningPMLRJul 2023b202Proceedings of Machine Learning Research</p>
<p>Bootstrap your own latent: A new approach to self-supervised learning. Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, Michal Valko, NeurIPS. 2020</p>
<p>Structuring representation geometry with rotationally equivariant contrastive learning. Sharut Gupta, Joshua Robinson, Derek Lim, Soledad Villar, Stefanie Jegelka, 2023</p>
<p>Recurrent world models facilitate policy evolution. David Ha, Jürgen Schmidhuber, Advances in Neural Information Processing Systems 31. 2018</p>
<p>Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi, arXiv:1912.01603Dream to control: Learning behaviors by latent imagination. 2019arXiv preprint</p>
<p>Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap, arXiv:2301.04104Mastering diverse domains through world models. 2023arXiv preprint</p>
<p>Modem: Accelerating visual model-based reinforcement learning with demonstrations. Nicklas Hansen, Yixin Lin, Hao Su, Xiaolong Wang, Vikash Kumar, Aravind Rajeswaran, arXiv:2212.056982022arXiv preprint</p>
<p>Provable guarantees for self-supervised deep learning with spectral contrastive loss. Colin Jeff Z Haochen, Adrien Wei, Tengyu Gaidon, Ma, NeurIPS. 342021</p>
<p>Momentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, CVPR. 2020</p>
<p>Masked autoencoders are scalable vision learners. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick, arXiv:2111.063772021arXiv preprint</p>
<p>The inaturalist species classification and detection dataset. Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, Serge Belongie, CVPR. 2018</p>
<p>Gaia-1: A generative world model for autonomous driving. Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, Gianluca Corrado, 2023</p>
<p>Soda: Bottleneck diffusion models for representation learning. Drew A Hudson, Daniel Zoran, Mateusz Malinowski, Andrew K Lampinen, Andrew Jaegle, James L Mcclelland, Loic Matthey, Felix Hill, Alexander Lerchner, 2023</p>
<p>Thomas Kipf, Elise Van Der Pol, Max Welling, arXiv:1911.12247Contrastive learning of structured world models. 2019arXiv preprint</p>
<p>A path towards autonomous machine intelligence version 0. Yann Lecun, Open Review. 912022</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, 2021</p>
<p>Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. Zengyi Li, Yubei Chen, Yann Lecun, Friedrich T Sommer, arXiv:2201.10000International Conference on Learning Representations. 2022. 2019arXiv preprintNeural manifold clustering and embedding</p>
<p>Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, arXiv:2304.07193Learning robust visual features without supervision. 2023arXiv preprint</p>
<p>Learning Symmetric Embeddings for Equivariant World Models. Ondrej Jung Yeon Park, Linfeng Biza, Jan Zhao, Robin Willem Van De Meent, Walters, arXiv:2204.11371June 2022</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Dai, V Quoc, Le, 2022</p>
<p>Sun database: Largescale scene recognition from abbey to zoo. Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, Antonio Torralba, 2010 IEEE computer society conference on computer vision and pattern recognition. IEEE2010</p>
<p>Unified perceptual parsing for scene understanding. Tete Xiao, Liu Yingcheng, Bolei Zhou, Jiang Yuning, Sun Jian, ECCV. 2018</p>
<p>Simmim: A simple framework for masked image modeling. Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, Han Hu, 2022</p>
<p>Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, Pieter Abbeel, arXiv:2310.06114Learning interactive real-world simulators. 2023arXiv preprint</p>
<p>Chun-Hsiao Yeh, Cheng-Yao Hong, Yen-Chi Hsu, Tyng-Luh Liu, Yubei Chen, Yann Lecun, arXiv:2110.06848Decoupled contrastive learning. 2021arXiv preprint</p>
<p>Large batch training of convolutional networks. Yang You, Igor Gitman, Boris Ginsburg, arXiv:1708.038882017arXiv preprint</p>
<p>Cutmix: Regularization strategy to train strong classifiers with localizable features. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, Youngjoon Yoo, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Barlow twins: Self-supervised learning via redundancy reduction. Jure Zbontar, Li Jing, Ishan Misra, Yann Lecun, Stéphane Deny, ICML. PMLR2021</p>
<p>Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, David Lopez-Paz, mixup: Beyond empirical risk. </p>            </div>
        </div>

    </div>
</body>
</html>
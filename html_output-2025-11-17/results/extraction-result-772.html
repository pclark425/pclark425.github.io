<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-772 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-772</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-772</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-227305541</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2012.02757v1.pdf" target="_blank">Playing Text-Based Games with Common Sense</a></p>
                <p><strong>Paper Abstract:</strong> Text based games are simulations in which an agent interacts with the world purely through natural language. They typically consist of a number of puzzles interspersed with interactions with common everyday objects and locations. Deep reinforcement learning agents can learn to solve these puzzles. However, the everyday interactions with the environment, while trivial for human players, present as additional puzzles to agents. We explore two techniques for incorporating commonsense knowledge into agents. Inferring possibly hidden aspects of the world state with either a commonsense inference model COMET, or a language model BERT. Biasing an agents exploration according to common patterns recognized by a language model. We test our technique in the 9to05 game, which is an extreme version of a text based game that requires numerous interactions with common, everyday objects in common, everyday scenarios. We conclude that agents that augment their beliefs about the world state with commonsense inferences are more robust to observational errors and omissions of common elements from text descriptions.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e772.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e772.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Actor-Critic (KG-A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-constrained deep reinforcement learning agent that constructs and maintains a knowledge graph of subject-relation-object triples from text observations and uses that graph as the agent's belief state, encoded for an A2C policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baseline agent that extracts subject-relation-object triples from textual room descriptions using an information-extraction pipeline, incrementally builds an ever-growing knowledge graph (KG) as its belief state, encodes the KG with a Graph Attention Network (GAT) to produce features, concatenates those with encoded observation state, and feeds the embedding to an Actor-Critic (A2C) policy to produce natural-language commands. Commands that reference entities not present in the KG are filtered out.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>9:05 (Jericho text-based game)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A slice-of-life interactive fiction (Jericho) game where the agent receives textual descriptions of a single location (room) at a time; partially observable (agent only sees current room), high branching factor with many possible text commands per state and extremely sparse reward signal (final success requires a 25–30 step precise sequence).</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Information-extraction pipeline (to extract triples) is used; no external language models or commonsense systems are used in baseline KG-A2C beyond IE and the internal KG encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured data: subject-relation-object triples extracted from text descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>An explicit knowledge graph (set of subject-relation-object triples) that is incrementally updated from parsed observations and then encoded via a Graph Attention Network into continuous features for the policy network.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>At each step the IE pipeline extracts triples from the current textual observation and appends them to the existing KG (an ever-growing graph). The KG is re-encoded by the GAT and supplied to the actor-critic; commands referencing entities absent in the KG are filtered out. (No additional external-tool outputs beyond IE are incorporated in baseline.)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy using Actor-Critic (A2C) conditioned on an encoded knowledge-graph belief state (graph-constrained RL). No explicit model-based or search planning is used.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation is achieved by issuing movement commands from the learned policy; the KG constrains valid commands but there is no explicit path-finding algorithm (no A*, no explicit graph search over locations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>N/A for baseline (no external LMs); baseline KG-A2C typically reaches the bathroom (reward=2) but fails to reliably reach the shower (reward=6) in the 9:05 game under the shaped-reward condition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Serves as the control showing that a KG-based belief state and learned A2C policy alone is insufficient to complete the full bathroom subtask in 9:05; KG constrains actions but cannot compensate for missing/unobserved objects in text descriptions without external commonsense inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing Text-Based Games with Common Sense', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e772.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e772.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Q*BERT (ALBERT-augmented KG-A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that augments the KG-belief by generating questions about the environment, answering them with a pre-trained QA-style language model (ALBERT), converting answers into KG triples, and using the augmented KG with the same GAT + A2C pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Extends KG-A2C by generating natural-language questions about the current observation; passes questions to a pre-trained ALBERT QA model which returns answers; answers are converted into subject-relation-object style entries and added to the agent's knowledge graph. The augmented KG is encoded by a GAT and fed to the same Actor-Critic policy. The process thus uses an external pre-trained language model as a commonsense/knowledge tool to expand the agent's belief state.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>9:05 (Jericho text-based game)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable slice-of-life interactive fiction; agent only sees one room at a time, must perform sequences of object interactions and navigation to reach sparse rewards; text descriptions may omit commonsense objects.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>ALBERT (pre-trained QA-tuned language model) used as an external inference tool; the agent also uses its IE pipeline to convert ALBERT outputs into KG triples. Implicitly relies on ALBERT's learned world knowledge from pretraining corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual answers (short phrases) from ALBERT that are converted into structured subject-relation-object triples and added to the knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Same KG mechanism as KG-A2C: an explicit subject-relation-object knowledge graph that is incrementally augmented by IE and by ALBERT-derived triples, encoded via a Graph Attention Network into features for the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>After extracting triples from observations, the agent formulates questions and ALBERT returns answers; those answers are parsed/converted into triples (subject, relation, object) and appended to the KG. The augmented KG is re-encoded with the GAT and used by the actor-critic. Thus tool outputs are explicitly merged into and stored within the KG belief state.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (Actor-Critic) conditioned on an augmented KG belief; no explicit search or model-based planning — improvement comes from enriched belief enabling the learned policy to select appropriate actions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation is executed via language movement commands produced by the learned policy; no explicit path search algorithm is used. The presence of inferred objects in the KG enables the policy to choose interactions that lead to progress through rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>With ALBERT-augmented KG, Q*BERT reliably passes the shower checkpoint (reward >=6) and proceeds to the driving phase; in the modified-observation experiment (missing bathroom object mentions) Q*BERT converges faster and more consistently completes bathroom tasks than baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Without ALBERT augmentation (i.e., baseline KG-A2C), the agent typically stalls at entering the bathroom (reward=2) and fails to reach the shower (reward=6).</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an external pre-trained LM (ALBERT) as an inference tool to generate and add facts to the KG meaningfully improves robustness to missing or noisy observations; ALBERT-derived inferences enable the agent to act upon objects that are not mentioned in the text, allowing completion of required subtasks and faster convergence versus KG-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing Text-Based Games with Common Sense', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e772.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e772.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COMET-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COMET-augmented KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that augments its knowledge graph belief with commonsense HasA (and related) inferences generated by the COMET commonsense transformer trained on ConceptNet, and uses the augmented KG within the same GAT + A2C pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Comet: Commonsense transformers for automatic knowledge graph construction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>COMET-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Extends KG-A2C by running a COMET commonsense inference model on extracted sentences/observations to generate likely implicit facts (uses the HasA inference class). COMET outputs short phrases representing commonly inferred entities/relations; these are converted into triples and added to the agent's KG. The resulting KG is encoded by a GAT and provided to the Actor-Critic network for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>9:05 (Jericho text-based game)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based, partially observable slice-of-life game with one-room-per-observation, high branching factor, and sparse success signals; the environment often assumes commonsense objects are present without explicitly mentioning them.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>COMET (commonsense transformer trained on ConceptNet) used to generate inferred object relations (HasA outputs) from observation text; ConceptNet is the knowledge source COMET was trained on (COMET itself is the model used at runtime).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Short textual phrases representing inferred relations/entities (commonsense assertions), which are converted into structured triples (subject, relation, object) and appended to the KG.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Explicit knowledge graph of triples maintained and augmented by IE and COMET outputs; encoded via Graph Attention Network for the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>COMET is run on textual observations to produce HasA and related inferences; these inferred nodes/relations are added to the existing KG (augmenting the agent's belief of what objects are present). KG is re-encoded by the GAT and used by the A2C policy. This augmentation allows the agent to reason about and act on objects that were not explicitly mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (Actor-Critic) conditioned on the augmented KG; no separate search-based or model-based planner. Improvements arise from richer belief state rather than explicit planning with the external tool.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation is performed via the learned language action policy; inferred KG facts enable interactions required to progress through locations, but there is no explicit path-finding algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>COMET-A2C reliably gets past the shower checkpoint (reward >=6) in the standard experiment and outperforms the KG-A2C baseline; in the modified-observation experiment (missing object mentions) COMET-A2C is able to use the sink/toilet/shower via inferred KG entries and achieves higher reward than baseline, although it requires more iterations than Q*BERT to converge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Baseline KG-A2C (no COMET) typically stalls at reward=2 (enters bathroom but cannot complete bathroom tasks when objects are not explicitly observed).</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Augmenting the belief graph with COMET's HasA commonsense inferences significantly increases robustness to missing observations and enables completion of object-interaction subtasks; COMET's focused HasA outputs help infer presence of typical objects but may be less diverse than QA-based LM inferences (leading to slower convergence than Q*BERT in extreme cases).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing Text-Based Games with Common Sense', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e772.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e772.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-A2C with BERT-based Policy Shaping (KG-A2C-BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that keeps the KG belief unchanged but biases exploration by re-ranking candidate commands using BERT next-sentence-prediction likelihoods over command histories (policy shaping).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Identical to KG-A2C in its KG construction and A2C architecture, but modifies action selection via policy shaping: the agent samples the top-k candidate commands from the policy network and re-scores/re-ranks them by computing the likelihood of the command sequence using a pre-trained BERT NSP-like scoring (Pr(c_t | c_1...c_{t-1}; θ)). The re-ranked distribution biases sampling toward command sequences that BERT considers coherent given prior commands.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>9:05 (Jericho text-based game)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text environment with one-room observations, many irrelevant interactions, high branching factor, and sparse rewards; requires sequences of logically coherent commands to progress.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>BERT (pre-trained Bidirectional Encoder Representations) used as an external sequence-scoring tool for policy shaping (next-sentence-prediction style likelihoods).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual-sequence likelihood scores / probabilities used to re-rank candidate natural-language commands (numerical scalar scores derived from BERT).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Same KG belief as KG-A2C (explicit subject-relation-object triples encoded with GAT). BERT outputs are not added to the KG belief.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>The KG is updated only by the IE pipeline from observations; BERT is used only at action selection time to compute sequence-likelihood scores for re-ranking sampled commands. BERT outputs do not modify or add to the KG belief state.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (A2C) with policy-shaping via external LM-based sequence scoring; not model-based planning—BERT provides procedural/sequence bias to exploration rather than explicit planning or search.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation relies on the underlying A2C policy and KG constraints; BERT-based re-ranking biases toward sequences of commands that are more likely coherent but does not compute explicit navigation paths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>KG-A2C-BERT shows modest improvement over baseline in standard observations (sometimes reaches reward=5) by biasing exploration toward coherent action sequences, but it is not as effective as KG augmentation approaches; in the missing-observation experiment it performs like baseline and fails to complete bathroom tasks because it cannot generate necessary object-targeting commands without those objects in the KG.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Baseline KG-A2C (without BERT shaping) performs similarly or slightly worse in some settings; BERT shaping gives modest gains when objects are observable but does not help when required object references are missing from KG.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using BERT to bias exploration via sequence-likelihood re-ranking helps when observations contain the necessary entities (improves exploration coherence), but it cannot substitute for explicit augmentation of the KG belief with inferred entities; augmenting belief (COMET/ALBERT) is more effective at overcoming missing observations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing Text-Based Games with Common Sense', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Comet: Commonsense transformers for automatic knowledge graph construction. <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>BERT: pre-training of deep bidirectional transformers for language understanding. <em>(Rating: 2)</em></li>
                <li>Albert: A lite bert for self-supervised learning of language representations. <em>(Rating: 1)</em></li>
                <li>Policy shaping: Integrating human feedback with reinforcement learning. <em>(Rating: 1)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-772",
    "paper_id": "paper-227305541",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "KG-A2C",
            "name_full": "Knowledge Graph Actor-Critic (KG-A2C)",
            "brief_description": "A graph-constrained deep reinforcement learning agent that constructs and maintains a knowledge graph of subject-relation-object triples from text observations and uses that graph as the agent's belief state, encoded for an A2C policy.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "mention_or_use": "use",
            "agent_name": "KG-A2C",
            "agent_description": "Baseline agent that extracts subject-relation-object triples from textual room descriptions using an information-extraction pipeline, incrementally builds an ever-growing knowledge graph (KG) as its belief state, encodes the KG with a Graph Attention Network (GAT) to produce features, concatenates those with encoded observation state, and feeds the embedding to an Actor-Critic (A2C) policy to produce natural-language commands. Commands that reference entities not present in the KG are filtered out.",
            "environment_name": "9:05 (Jericho text-based game)",
            "environment_description": "A slice-of-life interactive fiction (Jericho) game where the agent receives textual descriptions of a single location (room) at a time; partially observable (agent only sees current room), high branching factor with many possible text commands per state and extremely sparse reward signal (final success requires a 25–30 step precise sequence).",
            "is_partially_observable": true,
            "external_tools_used": "Information-extraction pipeline (to extract triples) is used; no external language models or commonsense systems are used in baseline KG-A2C beyond IE and the internal KG encoder.",
            "tool_output_types": "Structured data: subject-relation-object triples extracted from text descriptions.",
            "belief_state_mechanism": "An explicit knowledge graph (set of subject-relation-object triples) that is incrementally updated from parsed observations and then encoded via a Graph Attention Network into continuous features for the policy network.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "At each step the IE pipeline extracts triples from the current textual observation and appends them to the existing KG (an ever-growing graph). The KG is re-encoded by the GAT and supplied to the actor-critic; commands referencing entities absent in the KG are filtered out. (No additional external-tool outputs beyond IE are incorporated in baseline.)",
            "planning_approach": "Learned policy using Actor-Critic (A2C) conditioned on an encoded knowledge-graph belief state (graph-constrained RL). No explicit model-based or search planning is used.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation is achieved by issuing movement commands from the learned policy; the KG constrains valid commands but there is no explicit path-finding algorithm (no A*, no explicit graph search over locations).",
            "performance_with_tools": "N/A for baseline (no external LMs); baseline KG-A2C typically reaches the bathroom (reward=2) but fails to reliably reach the shower (reward=6) in the 9:05 game under the shaped-reward condition.",
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Serves as the control showing that a KG-based belief state and learned A2C policy alone is insufficient to complete the full bathroom subtask in 9:05; KG constrains actions but cannot compensate for missing/unobserved objects in text descriptions without external commonsense inference.",
            "uuid": "e772.0",
            "source_info": {
                "paper_title": "Playing Text-Based Games with Common Sense",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Q*BERT",
            "name_full": "Q*BERT (ALBERT-augmented KG-A2C)",
            "brief_description": "An agent that augments the KG-belief by generating questions about the environment, answering them with a pre-trained QA-style language model (ALBERT), converting answers into KG triples, and using the augmented KG with the same GAT + A2C pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Q*BERT",
            "agent_description": "Extends KG-A2C by generating natural-language questions about the current observation; passes questions to a pre-trained ALBERT QA model which returns answers; answers are converted into subject-relation-object style entries and added to the agent's knowledge graph. The augmented KG is encoded by a GAT and fed to the same Actor-Critic policy. The process thus uses an external pre-trained language model as a commonsense/knowledge tool to expand the agent's belief state.",
            "environment_name": "9:05 (Jericho text-based game)",
            "environment_description": "Partially observable slice-of-life interactive fiction; agent only sees one room at a time, must perform sequences of object interactions and navigation to reach sparse rewards; text descriptions may omit commonsense objects.",
            "is_partially_observable": true,
            "external_tools_used": "ALBERT (pre-trained QA-tuned language model) used as an external inference tool; the agent also uses its IE pipeline to convert ALBERT outputs into KG triples. Implicitly relies on ALBERT's learned world knowledge from pretraining corpora.",
            "tool_output_types": "Textual answers (short phrases) from ALBERT that are converted into structured subject-relation-object triples and added to the knowledge graph.",
            "belief_state_mechanism": "Same KG mechanism as KG-A2C: an explicit subject-relation-object knowledge graph that is incrementally augmented by IE and by ALBERT-derived triples, encoded via a Graph Attention Network into features for the policy.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "After extracting triples from observations, the agent formulates questions and ALBERT returns answers; those answers are parsed/converted into triples (subject, relation, object) and appended to the KG. The augmented KG is re-encoded with the GAT and used by the actor-critic. Thus tool outputs are explicitly merged into and stored within the KG belief state.",
            "planning_approach": "Learned policy (Actor-Critic) conditioned on an augmented KG belief; no explicit search or model-based planning — improvement comes from enriched belief enabling the learned policy to select appropriate actions.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation is executed via language movement commands produced by the learned policy; no explicit path search algorithm is used. The presence of inferred objects in the KG enables the policy to choose interactions that lead to progress through rooms.",
            "performance_with_tools": "With ALBERT-augmented KG, Q*BERT reliably passes the shower checkpoint (reward &gt;=6) and proceeds to the driving phase; in the modified-observation experiment (missing bathroom object mentions) Q*BERT converges faster and more consistently completes bathroom tasks than baselines.",
            "performance_without_tools": "Without ALBERT augmentation (i.e., baseline KG-A2C), the agent typically stalls at entering the bathroom (reward=2) and fails to reach the shower (reward=6).",
            "has_tool_ablation": true,
            "key_findings": "Using an external pre-trained LM (ALBERT) as an inference tool to generate and add facts to the KG meaningfully improves robustness to missing or noisy observations; ALBERT-derived inferences enable the agent to act upon objects that are not mentioned in the text, allowing completion of required subtasks and faster convergence versus KG-only baselines.",
            "uuid": "e772.1",
            "source_info": {
                "paper_title": "Playing Text-Based Games with Common Sense",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "COMET-A2C",
            "name_full": "COMET-augmented KG-A2C",
            "brief_description": "An agent that augments its knowledge graph belief with commonsense HasA (and related) inferences generated by the COMET commonsense transformer trained on ConceptNet, and uses the augmented KG within the same GAT + A2C pipeline.",
            "citation_title": "Comet: Commonsense transformers for automatic knowledge graph construction.",
            "mention_or_use": "use",
            "agent_name": "COMET-A2C",
            "agent_description": "Extends KG-A2C by running a COMET commonsense inference model on extracted sentences/observations to generate likely implicit facts (uses the HasA inference class). COMET outputs short phrases representing commonly inferred entities/relations; these are converted into triples and added to the agent's KG. The resulting KG is encoded by a GAT and provided to the Actor-Critic network for action selection.",
            "environment_name": "9:05 (Jericho text-based game)",
            "environment_description": "Text-based, partially observable slice-of-life game with one-room-per-observation, high branching factor, and sparse success signals; the environment often assumes commonsense objects are present without explicitly mentioning them.",
            "is_partially_observable": true,
            "external_tools_used": "COMET (commonsense transformer trained on ConceptNet) used to generate inferred object relations (HasA outputs) from observation text; ConceptNet is the knowledge source COMET was trained on (COMET itself is the model used at runtime).",
            "tool_output_types": "Short textual phrases representing inferred relations/entities (commonsense assertions), which are converted into structured triples (subject, relation, object) and appended to the KG.",
            "belief_state_mechanism": "Explicit knowledge graph of triples maintained and augmented by IE and COMET outputs; encoded via Graph Attention Network for the policy.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "COMET is run on textual observations to produce HasA and related inferences; these inferred nodes/relations are added to the existing KG (augmenting the agent's belief of what objects are present). KG is re-encoded by the GAT and used by the A2C policy. This augmentation allows the agent to reason about and act on objects that were not explicitly mentioned.",
            "planning_approach": "Learned policy (Actor-Critic) conditioned on the augmented KG; no separate search-based or model-based planner. Improvements arise from richer belief state rather than explicit planning with the external tool.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation is performed via the learned language action policy; inferred KG facts enable interactions required to progress through locations, but there is no explicit path-finding algorithm.",
            "performance_with_tools": "COMET-A2C reliably gets past the shower checkpoint (reward &gt;=6) in the standard experiment and outperforms the KG-A2C baseline; in the modified-observation experiment (missing object mentions) COMET-A2C is able to use the sink/toilet/shower via inferred KG entries and achieves higher reward than baseline, although it requires more iterations than Q*BERT to converge.",
            "performance_without_tools": "Baseline KG-A2C (no COMET) typically stalls at reward=2 (enters bathroom but cannot complete bathroom tasks when objects are not explicitly observed).",
            "has_tool_ablation": true,
            "key_findings": "Augmenting the belief graph with COMET's HasA commonsense inferences significantly increases robustness to missing observations and enables completion of object-interaction subtasks; COMET's focused HasA outputs help infer presence of typical objects but may be less diverse than QA-based LM inferences (leading to slower convergence than Q*BERT in extreme cases).",
            "uuid": "e772.2",
            "source_info": {
                "paper_title": "Playing Text-Based Games with Common Sense",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "KG-A2C-BERT",
            "name_full": "KG-A2C with BERT-based Policy Shaping (KG-A2C-BERT)",
            "brief_description": "An agent that keeps the KG belief unchanged but biases exploration by re-ranking candidate commands using BERT next-sentence-prediction likelihoods over command histories (policy shaping).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "KG-A2C-BERT",
            "agent_description": "Identical to KG-A2C in its KG construction and A2C architecture, but modifies action selection via policy shaping: the agent samples the top-k candidate commands from the policy network and re-scores/re-ranks them by computing the likelihood of the command sequence using a pre-trained BERT NSP-like scoring (Pr(c_t | c_1...c_{t-1}; θ)). The re-ranked distribution biases sampling toward command sequences that BERT considers coherent given prior commands.",
            "environment_name": "9:05 (Jericho text-based game)",
            "environment_description": "Partially observable text environment with one-room observations, many irrelevant interactions, high branching factor, and sparse rewards; requires sequences of logically coherent commands to progress.",
            "is_partially_observable": true,
            "external_tools_used": "BERT (pre-trained Bidirectional Encoder Representations) used as an external sequence-scoring tool for policy shaping (next-sentence-prediction style likelihoods).",
            "tool_output_types": "Textual-sequence likelihood scores / probabilities used to re-rank candidate natural-language commands (numerical scalar scores derived from BERT).",
            "belief_state_mechanism": "Same KG belief as KG-A2C (explicit subject-relation-object triples encoded with GAT). BERT outputs are not added to the KG belief.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "The KG is updated only by the IE pipeline from observations; BERT is used only at action selection time to compute sequence-likelihood scores for re-ranking sampled commands. BERT outputs do not modify or add to the KG belief state.",
            "planning_approach": "Learned policy (A2C) with policy-shaping via external LM-based sequence scoring; not model-based planning—BERT provides procedural/sequence bias to exploration rather than explicit planning or search.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation relies on the underlying A2C policy and KG constraints; BERT-based re-ranking biases toward sequences of commands that are more likely coherent but does not compute explicit navigation paths.",
            "performance_with_tools": "KG-A2C-BERT shows modest improvement over baseline in standard observations (sometimes reaches reward=5) by biasing exploration toward coherent action sequences, but it is not as effective as KG augmentation approaches; in the missing-observation experiment it performs like baseline and fails to complete bathroom tasks because it cannot generate necessary object-targeting commands without those objects in the KG.",
            "performance_without_tools": "Baseline KG-A2C (without BERT shaping) performs similarly or slightly worse in some settings; BERT shaping gives modest gains when objects are observable but does not help when required object references are missing from KG.",
            "has_tool_ablation": true,
            "key_findings": "Using BERT to bias exploration via sequence-likelihood re-ranking helps when observations contain the necessary entities (improves exploration coherence), but it cannot substitute for explicit augmentation of the KG belief with inferred entities; augmenting belief (COMET/ALBERT) is more effective at overcoming missing observations.",
            "uuid": "e772.3",
            "source_info": {
                "paper_title": "Playing Text-Based Games with Common Sense",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Comet: Commonsense transformers for automatic knowledge graph construction.",
            "rating": 2,
            "sanitized_title": "comet_commonsense_transformers_for_automatic_knowledge_graph_construction"
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "BERT: pre-training of deep bidirectional transformers for language understanding.",
            "rating": 2,
            "sanitized_title": "bert_pretraining_of_deep_bidirectional_transformers_for_language_understanding"
        },
        {
            "paper_title": "Albert: A lite bert for self-supervised learning of language representations.",
            "rating": 1,
            "sanitized_title": "albert_a_lite_bert_for_selfsupervised_learning_of_language_representations"
        },
        {
            "paper_title": "Policy shaping: Integrating human feedback with reinforcement learning.",
            "rating": 1,
            "sanitized_title": "policy_shaping_integrating_human_feedback_with_reinforcement_learning"
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces.",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        }
    ],
    "cost": 0.011863249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Playing Text-Based Games with Common Sense</p>
<p>Sahith Dambekodi sdambekodi3@gatech.edu 
Georgia Institute of Technology</p>
<p>Spencer Frazier sfrazier7@gatech.edu 
Georgia Institute of Technology</p>
<p>Prithviraj Ammanabrolu raj.ammanabrolu@gatech.edu 
Georgia Institute of Technology</p>
<p>Mark O Riedl riedl@gatech.edu 
Georgia Institute of Technology</p>
<p>Playing Text-Based Games with Common Sense</p>
<p>Text-based games are simulations in which an agent interacts with the world purely through natural language. They typically consist of a number of puzzles interspersed with interactions with common everyday objects and locations. Deep reinforcement learning agents can learn to solve these puzzles. However, the everyday interactions with the environment, while trivial for human players, present as additional puzzles to agents. We explore two techniques for incorporating commonsense knowledge into agents. (1) Inferring possibly hidden aspects of the world state with either a commonsense inference model (COMET [1]), or a language model (BERT [2]). (2) Biasing an agent's exploration according to common patterns recognized by a language model. We test our technique in the 9:05 game, which is an extreme version of a text-based game that requires numerous interactions with common, everyday objects in common, everyday scenarios. We conclude that agents that augment their beliefs about the world state with commonsense inferences are more robust to observational errors and omissions of common elements from text descriptions.</p>
<p>Common Sense in Text-Based Worlds</p>
<p>Text-based games-also called interactive fictions or text adventures-are simulations in which an agent interacts with the world purely through natural language by reading textual descriptions of the current state of the environment and composing text commands to enact change in the environment. Text-based games are partially observable in the sense that the agent can only observe the details of one particular "room" or location at a time. See Figure 1 (left) for a sample text-based game interaction. Text-based games can have a very high branching factor; the popular commercial textadventure, Zork has 1.64 × 10 14 possible action commands that can be entered per state [3]. A number of agents have been developed to attempt to play text-based games [3][4][5][6][7][8][9][10][11][12]. State of the art text-based game playing agents use knowledge graphs to represent the agent's belief about the full world state of the game. A knowledge graph is a set of subject, relation, object triples. The knowledge graph is encoded into a deep reinforcement learning agent that learns to infer the best action to take based on this state information. KG-A2C [10], in particular, uses information extraction techniques and rules to identify nodes and relations from text descriptions.</p>
<p>Many real-world activities can be thought of as a sequence of sub-goals in a partially observable environment. These activities-getting ready to go to work, for example-are considered trivial for humans because of commonsense knowledge. Commonsense knowledge is a set of facts, beliefs, and procedures shared among many people in the same society or culture. However, to an agent that knows only what it has learned from interacting with the environment, even tasks that humans take for granted can involve considerable trial-and-error effort. In this paper, we look at how commonsense knowledge can be incorporated in to text-based reinforcement learning agents that operates in games Observation: This bedroom is extremely spare, with dirty laundry scattered haphazardly allover the floor. Cleaner clothing can be found in the dresser. A bathroom lies to the south,while a door to the east leads to the living room. On the end table are a telephone, a wallet and some keys.The phone rings.</p>
<p>Action: Answer phone</p>
<p>Observation: You pick up the phone. "Hadley!" a shrill voice cries. "Hadley, haven't you even left yet?? You knew that our presentation was at nine o'clock sharp! First the thing with the printers, now this -there won't even be enough left of you for Bowman to fire once he's done with you. Now get the hell down here!!" Action: Examine me Observation: You're covered with mud and dried sweat. It was an exhausting night -no wonder you overslept! Even making it to the bed before conking out was a heroic accomplishment.  that simulate common, everyday situations. We hypothesize that access to commonsense knowledge can enable an agent to more quickly converge on a policy that completes common, everyday tasks. We further hypothesize that commonsense knowledge can allow the agent to infer the presence of elements in the world when observations are noisy or fail. For example, if the agent fails to observe the presence of a sink in a bathroom, it can still infer that an action such as wash hands may still be applicable given common knowledge of what a bathroom contains.</p>
<p>We experiment with agents in the text-game, 9:05, 2 which is a "slice of life" simulation game. The player must get ready for work, navigate a simple drive to work, and perform some workplace interactions. It is one of the hardest games for an agent to solve [3]; to date no agent with unrestricted ability to generate commands has completed the game. Other text-based game playing techniques that incorporate commonsense knowledge include using semantic word vectors to infer actions that can be applied to objects [5], or looking up information about objects in ConceptNet [12].</p>
<p>We experiment with ways to incorporate commonsense knowledge into a deep reinforcement learning game playing agent. The first approach is to incorporate commonsense knowledge into the world state, specifically the knowledge graph, which is the agent's beliefs about the world state. We look at two sources of commonsense inference. COMET [1] is a neural model that takes a simple sentence and infers what will be commonly believed about the people and objects referenced in the sentence. An alternative source of commonsense knowledge is BERT [2], which is believed to have acquired significant commonsense knowledge from texts on which it was trained [13]. Commonsense knowledge also manifests itself as procedural knowledge-common situations are addressed by following familiar patterns of behavior. Our third technique is to bias the agent toward certain sequences of action commands using BERTs next sentence prediction mode.</p>
<p>Methods</p>
<p>We experiment with three agent designs, each using a different source or a different way of incorporating commonsense knowledge into the agent. All three agents build off the KG-A2C [10] agent framework, which is shown in Figure 1 (right). At every step, KG-A2C uses an information extraction process to identify subject, relation, object triples in text descriptions of the current location. These triples are added to an ever-growing knowledge graph, which is encoded into the neural architecture of the agent to inform the choice of command. The knowledge graph is the agent's belief about the state of the world. Commands that reference objects are filtered out if they reference entities not in the knowledge graph. KG-A2C is our baseline.</p>
<p>The Q<em>BERT Agent. The first agent is Q</em>BERT [11], which augments the knowledge graph by using the pre-trained language model, ALBERT [14], a variant of BERT that is fine-tuned for question answering. Q<em>BERT generates questions about the current environment and ALBERT proposes answers, which are converted into subject, relation, object and added to the knowledge graph (see Figure 1 (right)). While Q</em>BERT was not explicitly design with commonsense knowledge in mind, we hypothesize that ALBERT is linking text observations to a broader set of knowledge about the world that has been acquired through training on a very large corpus of texts. That is, Q*BERT may infer things about the environment that are not directly observed but are also commonly believed by humans in similar situations.</p>
<p>The COMET-A2C Agent. Our second agent is similar to Q*BERT but replaces ALBERT with COMET [1], a neural commonsense inference model. Unlike ALBERT, COMET was trained on the ConceptNet [15] dataset to take text sentences and generate a number of short phrases that people are likely to directly infer. COMET produces several types of inferences. We use COMET's HasA inference class. COMET-A2C uses KG-A2C's information extraction process to produce subject, relation, object triples, and then we add additional nodes inferred by COMET. We hypothesize that COMET will make the agent's understanding of the world state more robust by adding object commonly found in certain types of locations. See Figure 1 (right) which shows the addition of COMET or ALBERT to the KGA2C model. The updated knowledge graph is sent as input to the Graph Attention Network which converts nodes into features and applies self-attention on all these features. The output of the model is then embedded with the encoded vector outputs for the state feedback of the environment. This final embedded vector is sent to the Actor-Critic model to decode the action.</p>
<p>KG-A2C-BERT. Our third agent is identical to KG-A2C, except that it uses a policy-shaping approach to exploration [16]. KG-A2C-BERT samples the top k commands generated by the network and scores each based on a history of previous commands. This is done by concatenating the currently proposed command to prior commands and computing the likelihood of that entire text sequence using BERT to compute P r(c t |c 1 ...c t−1 ; θ) where c i is a command at time step t and θ is BERT's pre-trained weights. The k candidate commands are re-ranked according to the score and the agent re-samples from the new distribution. In this way, KG-A2C-BERT is biased toward exploring commands which logically entail one another, according to what BERT has learned from NSP pretraining on its very large corpus of text. In Figure 1 (right), the green box is removed.</p>
<p>Environment.</p>
<p>We conduct experiments in the 9:05 slice of life text-based game. In this game, the player must get ready for work by taking a shower, wearing clean clothes and then travel to the workplace by car. There are a large variety of different tasks that can be performed by the agent, most of which have no impact on reaching the end goal. There are very few sequences that reach the end of the game; and each of these sequences are of 25 to 30 specific actions. The 9:05 game provides a single score at the end of the game: a score of 1 at the end or 0 if the player fails. Due to the extreme sparseness of feedback all agents struggle to make any significant progress except by accident; the branching factor is too high and the only reward feedback requires 25-30 steps executed in the perfect order. Consequently, we provide a shaped reward function. We define a sequence of observations that the agent should see if progress is being made and give +1 reward for each observation: entering the bedroom, entering the bathroom, taking off the watch, taking off soiled clothes, dropping clothes. and entering the shower. The critical checkpoints are at reward 2 for when the agent enters the bathroom and at reward 6 for when the agent successfully enters the shower. The reward states are chosen such that they can only be observed in one particular order and that loops cannot occur.</p>
<p>Experiments. We conducted two experiments. (1) In the first experiment, we used the version of 9:05 where reward is given for passing key states. (2) In the second experiment, we test agents' abilities to supplement missing/failed observations with commonsense inferences. We introduce a modified version of 9:05 that has the shaped reward but also deletes all textual references to sink, toilet, and shower from the description of the bathroom location. This simulates the situation in which the agent's observations have failed to notice any of the objects, or to correctly parse and extract relations pertaining to these objects. It also simulates the situation in which a human deems it unnecessary or obvious to state the presence of these objects in a bathroom. The player must interact with all three objects in order to progress in the game. The player can still interact with the objects even they are not present in the description text; objects were not removed, only the text mentions. </p>
<p>Results and Discussion</p>
<p>The results of Experiment 1 are shown in Figure 2 (left). KG-A2C never makes it to the shower and gets stuck after it enters the bathroom. KG-A2C-BERT sometimes makes it to reward 5 (dropping clothes just before getting in the shower), but not reliably so its average performance is similar to that of KG-A2C. COMET-A2C and Q<em>BERT are both able to reliably get past the shower and on to the next phase of the game where the player must drive to work. Their performances in this experiment are not significantly different. This experiment tells us that the commonsense knowledge helps agent performance in 9:05, which makes heavy use of locations and situations that also commonly occur in the real world. KG-A2C-BERT performs better than KG-A2C because BERT informs the agent's exploration by comparing action command sequences to patterns BERT recognizes. COMET-A2C adds HasA relations to the knowledge graph; its improvement over baseline and KG-A2C-BERT is due to inferring entities that might not have been properly extracted from text descriptions. Q</em>BERT performs similarly to COMET-A2C and is likely due to inferring entities.</p>
<p>The results for Experiment 2 are shown in Figure 2 (right). In this experiment agents must contend with missing object references in room descriptions. KG-A2C never makes it past a score of 2-it enters the bathroom but cannot complete any tasks due to the inability to directly observe the sink, toilet, or shower. KG-A2C-BERT performs identical to KG-A2C; BERT's commonsense procedural guidance doesn't help if it cannot generate the necessary commands to begin with. COMET-A2C and Q<em>BERT are able to use the sink, toilet, and shower to successfully complete all the tasks required in the bathroom which leads to greater reward. Q</em>BERT converges faster in this extreme experimental condition. Experiment 2 confirms our intuitions about the role that commonsense inferences are playing in the agent's decision-making. By making the presence of key objects in one location implicit instead of explicit, we can verify in a controlled fashion that commonsense inferences are able to augment agents' senses. Similar to Experiment 1, Q<em>BERT more consistently reaches the end of the bathroom task. While COMET-A2C does reach the successfully complete the task, it requires more iterations to actually successfully complete the task. This is due to the difference in the way COMET-A2C and Q</em>BERT infer commonsense information to the knowledge graph. Both infer the existence of the missing entities, allowing them to progress through the game. However, Q<em>BERT infers a diverse set of information using a small set of questions to be answered whereas COMET-A2C focuses on HasA relations. This variance allows Q</em>BERT to solve the intermediate steps that are required to reach the end of the bathroom task by capturing information beyond what the HasA relation produces, such as using the question "What attributes does X possess?" that explicitly connects entities in richer ways.</p>
<p>The 9:05 game is a "slice of life game", which requires the player to recreate behavior that might be also conducted routinely in the real-world. Slice of life games are extreme in their invocation of common locations and common situations; agents that can effectively use commonsense knowledge are going to naturally fare better than those without. Most text-based games have a mix of fantasy and science fiction elements along with common locations and situations and it remains to be seen if commonsense can help in these scenarios.</p>
<p>Conclusions</p>
<p>We conducted experiments in slice of life text-based games to understand how commonsense knowledge can help agents handle puzzles that involve locations and scenarios commonly found in the real world. Slice of life games, and 9:05 in particular, are extreme versions of text-based games that require the player to recreate behavior that might be also conducted routinely in the real-world. Although slice of life games are dominated by these scenarios, most text-based games interleave mundane world interactions with the world between fantasy and science fiction puzzles. Our experiments show that commonsense inferences can be used to augment an agent's beliefs about the state of the world, making the agent more robust against observation failures or against missing information in text descriptions. We find that-regardless of the source of commonsense knowledge-augmenting the agent's world state beliefs is more successful than biasing the agent's exploration.</p>
<p>We contend that text-based, slice of life games are stepping stones toward goal-based natural language interactions with humans; situations in which an agent primarily understands the dynamic world through listening and acts to change the world by speaking. It is natural for commonsense details to be omitted in such environments. Our work shows how a deep reinforcement learning framework for "acting through language" can be made more robust to real-world natural language phenomena.</p>
<p>Figure 1 :
1Excerpt from the 9:05 text-based game (left). KG-A2C with augmentations (right).</p>
<p>Figure 2 :
2Reward performance for all agents on 9:05 with full observations (left) or modified observations (right). The solid lines show smoothed average performance over 4 runs, with faded bars showing max across runs. A reward of 2 indicates that the agent has entered the bathroom and a reward of 6 indicates that the agent has entered the shower.
9:05 is part of the Jericho[3] suite of games at https://github.com/microsoft/jericho</p>
<p>Comet: Commonsense transformers for automatic knowledge graph construction. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, A Çelikyilmaz, Yejin Choi, ACL. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, A. Çelikyilmaz, and Yejin Choi. Comet: Commonsense transformers for automatic knowledge graph construction. In ACL, 2019.</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, abs/1810.04805CoRRJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI). 2020Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. In- teractive fiction games: A colossal adventure. In Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI), 2020.</p>
<p>Language understanding for text-based games using deep reinforcement learning. Karthik Narasimhan, Tejas D Kulkarni, Regina Barzilay, EMNLP. Karthik Narasimhan, Tejas D. Kulkarni, and Regina Barzilay. Language understanding for text-based games using deep reinforcement learning. In EMNLP, pages 1-11, 2015.</p>
<p>What can you do with a rock? affordance extraction via word embeddings. Nancy Fulda, Daniel Ricks, Ben Murdoch, David Wingate, IJCAI. Nancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. What can you do with a rock? affordance extraction via word embeddings. In IJCAI, pages 1039-1045, 2017.</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. Tom Zahavy, Matan Haroush, Nadav Merlis, J Daniel, Shie Mankowitz, Mannor, Advances in Neural Information Processing Systems. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. GarnettCurran Associates, Inc31Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. Learn what not to learn: Action elimination with deep reinforcement learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 3562-3573. Curran Associates, Inc., 2018.</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. Prithviraj Ammanabrolu, Mark O Riedl, Proceedings of 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Prithviraj Ammanabrolu and Mark O. Riedl. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, 2019.</p>
<p>Ledeepchef: Deep reinforcement learning agent for families of text-based games. Leonard Adolphs, Thomas Hofmann, arXiv:1909.01646arXiv preprintLeonard Adolphs and Thomas Hofmann. Ledeepchef: Deep reinforcement learning agent for families of text-based games. arXiv preprint arXiv:1909.01646, 2019.</p>
<p>Comprehensible context-driven text game playing. CoRR, abs. Xusen Yin, Jonathan May, Xusen Yin and Jonathan May. Comprehensible context-driven text game playing. CoRR, abs/1905.02265, 2019.</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations. Prithviraj Ammanabrolu and Matthew Hausknecht. Graph constrained reinforcement learning for natural language action spaces. In International Conference on Learning Representations, 2020.</p>
<p>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. Prithviraj Ammanabrolu, Ethan Tien, Matthew J Hausknecht, M Riedl, ArXiv, absPrithviraj Ammanabrolu, Ethan Tien, Matthew J. Hausknecht, and M. Riedl. How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. ArXiv, abs/2006.07409, 2020.</p>
<p>Enhancing text-based reinforcement learning agents with commonsense knowledge. ArXiv, abs. K Murugesan, Mattia Atzeni, Pushkar Shukla, Mrinmaya Sachan, Pavan Kapanipathi, Kartik Talamadupula, K. Murugesan, Mattia Atzeni, Pushkar Shukla, Mrinmaya Sachan, Pavan Kapanipathi, and Kartik Talamadupula. Enhancing text-based reinforcement learning agents with commonsense knowledge. ArXiv, abs/2005.00811, 2020.</p>
<p>Evaluating commonsense in pre-trained language models. Xuhui Zhou, Y Zhang, Leyang Cui, Dandan Huang, AAAI. Xuhui Zhou, Y. Zhang, Leyang Cui, and Dandan Huang. Evaluating commonsense in pre-trained language models. In AAAI, 2020.</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, International Conference on Learning Representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations, 2020.</p>
<p>ConceptNet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, AAAI. Robyn Speer, Joshua Chin, and Catherine Havasi. ConceptNet 5.5: An open multilingual graph of general knowledge. In AAAI, 2016.</p>
<p>Policy shaping: Integrating human feedback with reinforcement learning. Shane Griffith, Kaushik Subramanian, Jonathan Scholz, L Charles, Andrea L Isbell, Thomaz, Advances in neural information processing systems. Shane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and Andrea L Thomaz. Policy shaping: Integrating human feedback with reinforcement learning. In Advances in neural information processing systems, pages 2625-2633, 2013.</p>            </div>
        </div>

    </div>
</body>
</html>
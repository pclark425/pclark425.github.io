<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-828 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-828</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-828</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-272366611</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.01392v2.pdf" target="_blank">ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems</a></p>
                <p><strong>Paper Abstract:</strong> Much previous AI research has focused on developing monolithic models to maximize their intelligence, with the primary goal of enhancing performance on specific tasks. In contrast, this work attempts to study using LLM-based agents to design collaborative AI systems autonomously. To explore this problem, we first introduce ComfyBench to evaluate agents’s ability to design collaborative AI systems in ComfyUI. ComfyBench is a comprehensive benchmark comprising 200 diverse tasks covering various instruction-following generation challenges, along with detailed annotations for 3,205 nodes and 20 workflows. Based on ComfyBench, we further develop ComfyAgent, a novel framework that empowers LLM-based agents to autonomously design collaborative AI systems by generating workflows. ComfyAgent is based on two core concepts. First, it represents workflows with code, which can be reversibly converted into workflows and executed as collaborative systems by the interpreter. Second, it constructs a multi-agent system that cooperates to learn from existing workflows and generate new workflows for a given task. While experimental results demonstrate that ComfyAgent achieves a comparable resolve rate to o1-preview and significantly surpasses other agents on ComfyBench, ComfyAgent has resolved only 15% of creative tasks. LLM-based agents still have a long way to go in autonomously designing collaborative AI systems. Progress with ComfyBench is paving the way for more intelligent and autonomous collaborative AI systems. Our code is available at: https://github.com/xxyQwQ/ComfyBench.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e828.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e828.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ComfyAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ComfyAgent (multi-agent workflow-generation agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent framework that uses code-based workflow representation and cooperating LLM roles (PlanAgent, CombineAgent, AdaptAgent, RetrieveAgent, RefineAgent) with memory stores to autonomously generate runnable ComfyUI workflows for instruction-driven generative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ComfyAgent</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-agent system implemented by prompting a (single) LLM in different roles: Planner (global planning), CombineAgent (combines reference workflows), AdaptAgent (modify parameters/code), RetrieveAgent (retrieves references), RefineAgent (checks/refines). Uses a working-memory style store (history, reference, workspace) and represents workflows as restricted Python-like code for generation and reversible conversion to ComfyUI JSON.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varied: evaluated with GPT-4o (commercial, size not specified) and Llama-3.1-70B (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ComfyBench — ComfyUI workflow design (200 tasks: vanilla/complex/creative)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>workflow generation / planning / multi-step procedural tool composition</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>With GPT-4o: overall pass rate 56.0%, resolve rate 32.5% (paper reports ComfyAgent overall: pass 56%, resolve 32.5%). Performance by difficulty: Vanilla P=67.0% R=46.0%; Complex P=48.3% R=21.7%; Creative P=40.0% R=15.0% (reported in paper's ablation tables for ComfyAgent variants / totals).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>multi-agent decomposition (Planner, Combine, Adapt, Retrieve, Refine), working memory (history/reference/workspace), code-based workflow representation, retrieval module (embeddings + RAG-style retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting-driven (no fine-tuning); modules are LLM role prompts and a retrieval index (RAG style).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change (multi-agent decomposition) + representation change (code) + retrieval augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>ComfyAgent introduces (1) code representation of workflows (Python-like) to compactly and semantically represent DAG workflows; (2) a multi-agent decomposition that splits planning, retrieval, combination, adaptation and refinement into separate roles and integrates memory stores; (3) retrieval using embeddings to learn from existing workflows. These are applied at inference/prompting time (no additional model fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Compared to baseline prompting methods, ComfyAgent achieves substantially better stability and higher pass/resolve on ComfyBench (paper: ComfyAgent overall pass 56%, resolve 32.5%). Ablations show removing key agents reduces performance (examples: removing RetrieveAgent causes large drops; removing AdaptAgent or RetrieveAgent produces significant declines across categories; removing CombineAgent cripples complex-category performance). Exact ablation numbers: full ComfyAgent (Vanilla P=67.0% R=46.0%; Complex P=48.3% R=21.7%; Creative P=40.0% R=15.0%); w/o Retrieve (Vanilla P=27.0% R=16.0%; Complex P=20.0% R=5.0%; Creative P=7.5% R=7.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper argues poor performance on complex/creative interactive tasks stems from LLM context-length limitations, hallucination (making invalid node/parameter choices), inability to absorb the large node-document corpus in-context, and the need for expert organizational knowledge for pipeline composition; multi-step procedural generation suffers from these issues more than single-shot language/QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e828.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e828.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ComfyBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ComfyBench (benchmark for agentic ComfyUI workflow design)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 200-task benchmark (100 vanilla, 60 complex, 40 creative) with node documentation (~3,205 nodes) and 20 curriculum workflows to evaluate LLM-based agents on designing runnable ComfyUI workflows; metrics: pass rate (syntactic/semantic execution) and resolve rate (VLM-based semantic correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ComfyBench (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model — a benchmark suite for interactive/procedural workflow generation tasks inside ComfyUI, including node docs, curriculum workflows, and evaluation via execution + GPT-4o-based VLM evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ComfyUI workflow design tasks (ComfyBench)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>procedure / multi-step workflow synthesis / tool composition</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e828.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e828.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeRepresentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Code-based workflow representation (restricted Python-like)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representing ComfyUI workflows as a reversible, restricted Python-like code sequence (topological order) that LLMs can more effectively reason about and generate than JSON or element lists.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>code representation (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A representation/intervention: workflows are mapped to a compact, topologically ordered Python-like code (one statement per node) enabling richer semantics and better LLM compatibility.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ComfyBench workflow generation (representation ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>representation for procedural generation / multi-step planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Ablation (RAG on GPT-4o) — code representation outperformed JSON and element-list: Code: Vanilla P=62.0% R=41.0%; Complex P=45.0% R=15.0%; Creative P=30.0% R=7.5%. JSON and element-list had lower pass/resolve in all categories (e.g., JSON Vanilla P=52.0% R=33.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>representation-level intervention (structured code instead of raw JSON/graph)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>not a training method — representation used at prompting/inference time</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>representation change (architectural/format intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Switching to a code-like representation reduces verbosity and exposes topology and semantics to the LLM, enabling better generation and fewer hallucinations compared to JSON or element-list representations.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Significant improvement in pass/resolve when using code representation for RAG+GPT-4o (see metrics above); authors state code representation 'significantly improves the pass rate and resolve rate on ComfyBench' compared to other representations.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>The paper attributes prior failures to verbose/unfriendly JSON and lack of explicit topological relationships in element lists, which worsen context limitations and hallucination during multi-step workflow generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e828.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e828.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented prompting method: encode a corpus (here, documented workflows and node docs) with embeddings and retrieve top examples to include in prompts to the LLM, improving access to external knowledge beyond context-window limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method: uses embedding model (paper used text-embedding-3-large) to index existing workflows/documents and retrieve the most relevant demonstrations (paper used 5 retrieved demos) for in-context prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ComfyBench workflow generation (baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>knowledge-augmented multi-step procedural generation / planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>RAG was the strongest baseline family; example ablation (RAG on GPT-4o with code rep): Vanilla P=62.0% R=41.0%; Complex P=45.0% R=15.0%; Creative P=30.0% R=7.5% (Table 3). Paper notes RAG-based agents outperformed CoT and few-shot prompting on ComfyBench.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>retrieval interface / external datastore (embedding index) integrated into prompt</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>in-context retrieval augmentation (not model fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting / hybrid approach (retrieval augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Use of an embedding-based retrieval of existing workflow code/examples and inclusion of top-k demonstrations in prompts to the LLM, enabling re-use of existing components and reducing hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Marked improvement over plain few-shot / CoT prompting for interactive workflow synthesis. RAG with code representation yields the best single-model baseline in ablation (see metrics above). The paper reports that retrieval-based agents (RAG) outperform CoT and few-shot because they can exploit external knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>RAG mitigates the gap by supplying concrete examples and node usage that exceed LLM context/knowledge, addressing hallucination and missing expert knowledge that hinder procedural task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e828.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e828.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot, Few-shot, Chain-of-Thought (CoT), and CoT with Self-Consistency (CoT-SC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard prompting strategies evaluated as baselines: zero-shot and few-shot prompting, CoT (encouraging stepwise reasoning), and CoT-SC (ensembling multiple CoT trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Zero-shot / Few-shot / CoT / CoT-SC (prompting baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-engineering strategies: zero-shot direct instruction; few-shot in-context demos (paper used 3 demos); CoT asks model to generate reasoning steps before the final answer; CoT-SC ensembles parallel CoT trajectories (paper ensembles 3 trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ComfyBench workflow generation (baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / planning / workflow synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Paper reports: Zero-shot fails on ComfyBench; Few-shot gives modest improvements; CoT > Few-shot; CoT-SC slightly better than CoT but both substantially worse than retrieval-augmented or ComfyAgent approaches. Example aggregate: GPT-4o + CoT total performance was much lower than RAG or ComfyAgent (paper's Table 2 shows CoT yields nontrivial gains over few-shot but cannot match RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>chain-of-thought prompting (internal reasoning traces), ensembling (self-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / in-context learning (no fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Use CoT to surface reasoning and CoT-SC to ensemble multiple reasoning trajectories; few-shot uses demonstrations in-context.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>CoT and CoT-SC improve over naive few-shot/zero-shot on the procedural workflow generation tasks but are still outperformed by RAG and ComfyAgent. The paper attributes residual failures to context limitations and hallucination — prompting alone insufficient for complex procedural design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
                <li>Automated design of agentic systems <em>(Rating: 1)</em></li>
                <li>A survey on large language model based autonomous agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-828",
    "paper_id": "paper-272366611",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "ComfyAgent",
            "name_full": "ComfyAgent (multi-agent workflow-generation agent)",
            "brief_description": "A multi-agent framework that uses code-based workflow representation and cooperating LLM roles (PlanAgent, CombineAgent, AdaptAgent, RetrieveAgent, RefineAgent) with memory stores to autonomously generate runnable ComfyUI workflows for instruction-driven generative tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "ComfyAgent",
            "model_description": "Multi-agent system implemented by prompting a (single) LLM in different roles: Planner (global planning), CombineAgent (combines reference workflows), AdaptAgent (modify parameters/code), RetrieveAgent (retrieves references), RefineAgent (checks/refines). Uses a working-memory style store (history, reference, workspace) and represents workflows as restricted Python-like code for generation and reversible conversion to ComfyUI JSON.",
            "model_size": "varied: evaluated with GPT-4o (commercial, size not specified) and Llama-3.1-70B (70B)",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ComfyBench — ComfyUI workflow design (200 tasks: vanilla/complex/creative)",
            "interactive_task_type": "workflow generation / planning / multi-step procedural tool composition",
            "interactive_performance": "With GPT-4o: overall pass rate 56.0%, resolve rate 32.5% (paper reports ComfyAgent overall: pass 56%, resolve 32.5%). Performance by difficulty: Vanilla P=67.0% R=46.0%; Complex P=48.3% R=21.7%; Creative P=40.0% R=15.0% (reported in paper's ablation tables for ComfyAgent variants / totals).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "multi-agent decomposition (Planner, Combine, Adapt, Retrieve, Refine), working memory (history/reference/workspace), code-based workflow representation, retrieval module (embeddings + RAG-style retrieval)",
            "training_method": "prompting-driven (no fine-tuning); modules are LLM role prompts and a retrieval index (RAG style).",
            "intervention_type": "architectural change (multi-agent decomposition) + representation change (code) + retrieval augmentation",
            "intervention_description": "ComfyAgent introduces (1) code representation of workflows (Python-like) to compactly and semantically represent DAG workflows; (2) a multi-agent decomposition that splits planning, retrieval, combination, adaptation and refinement into separate roles and integrates memory stores; (3) retrieval using embeddings to learn from existing workflows. These are applied at inference/prompting time (no additional model fine-tuning).",
            "intervention_effect": "Compared to baseline prompting methods, ComfyAgent achieves substantially better stability and higher pass/resolve on ComfyBench (paper: ComfyAgent overall pass 56%, resolve 32.5%). Ablations show removing key agents reduces performance (examples: removing RetrieveAgent causes large drops; removing AdaptAgent or RetrieveAgent produces significant declines across categories; removing CombineAgent cripples complex-category performance). Exact ablation numbers: full ComfyAgent (Vanilla P=67.0% R=46.0%; Complex P=48.3% R=21.7%; Creative P=40.0% R=15.0%); w/o Retrieve (Vanilla P=27.0% R=16.0%; Complex P=20.0% R=5.0%; Creative P=7.5% R=7.5%).",
            "hypothesized_cause_of_gap": "Paper argues poor performance on complex/creative interactive tasks stems from LLM context-length limitations, hallucination (making invalid node/parameter choices), inability to absorb the large node-document corpus in-context, and the need for expert organizational knowledge for pipeline composition; multi-step procedural generation suffers from these issues more than single-shot language/QA tasks.",
            "uuid": "e828.0",
            "source_info": {
                "paper_title": "ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ComfyBench",
            "name_full": "ComfyBench (benchmark for agentic ComfyUI workflow design)",
            "brief_description": "A 200-task benchmark (100 vanilla, 60 complex, 40 creative) with node documentation (~3,205 nodes) and 20 curriculum workflows to evaluate LLM-based agents on designing runnable ComfyUI workflows; metrics: pass rate (syntactic/semantic execution) and resolve rate (VLM-based semantic correctness).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "ComfyBench (benchmark)",
            "model_description": "Not a model — a benchmark suite for interactive/procedural workflow generation tasks inside ComfyUI, including node docs, curriculum workflows, and evaluation via execution + GPT-4o-based VLM evaluation.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ComfyUI workflow design tasks (ComfyBench)",
            "interactive_task_type": "procedure / multi-step workflow synthesis / tool composition",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e828.1",
            "source_info": {
                "paper_title": "ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "CodeRepresentation",
            "name_full": "Code-based workflow representation (restricted Python-like)",
            "brief_description": "Representing ComfyUI workflows as a reversible, restricted Python-like code sequence (topological order) that LLMs can more effectively reason about and generate than JSON or element lists.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "code representation (method)",
            "model_description": "A representation/intervention: workflows are mapped to a compact, topologically ordered Python-like code (one statement per node) enabling richer semantics and better LLM compatibility.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ComfyBench workflow generation (representation ablation)",
            "interactive_task_type": "representation for procedural generation / multi-step planning",
            "interactive_performance": "Ablation (RAG on GPT-4o) — code representation outperformed JSON and element-list: Code: Vanilla P=62.0% R=41.0%; Complex P=45.0% R=15.0%; Creative P=30.0% R=7.5%. JSON and element-list had lower pass/resolve in all categories (e.g., JSON Vanilla P=52.0% R=33.0%).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "representation-level intervention (structured code instead of raw JSON/graph)",
            "training_method": "not a training method — representation used at prompting/inference time",
            "intervention_type": "representation change (architectural/format intervention)",
            "intervention_description": "Switching to a code-like representation reduces verbosity and exposes topology and semantics to the LLM, enabling better generation and fewer hallucinations compared to JSON or element-list representations.",
            "intervention_effect": "Significant improvement in pass/resolve when using code representation for RAG+GPT-4o (see metrics above); authors state code representation 'significantly improves the pass rate and resolve rate on ComfyBench' compared to other representations.",
            "hypothesized_cause_of_gap": "The paper attributes prior failures to verbose/unfriendly JSON and lack of explicit topological relationships in element lists, which worsen context limitations and hallucination during multi-step workflow generation.",
            "uuid": "e828.2",
            "source_info": {
                "paper_title": "ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A retrieval-augmented prompting method: encode a corpus (here, documented workflows and node docs) with embeddings and retrieve top examples to include in prompts to the LLM, improving access to external knowledge beyond context-window limitations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Retrieval-Augmented Generation (RAG)",
            "model_description": "Method: uses embedding model (paper used text-embedding-3-large) to index existing workflows/documents and retrieve the most relevant demonstrations (paper used 5 retrieved demos) for in-context prompting.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ComfyBench workflow generation (baseline comparison)",
            "interactive_task_type": "knowledge-augmented multi-step procedural generation / planning",
            "interactive_performance": "RAG was the strongest baseline family; example ablation (RAG on GPT-4o with code rep): Vanilla P=62.0% R=41.0%; Complex P=45.0% R=15.0%; Creative P=30.0% R=7.5% (Table 3). Paper notes RAG-based agents outperformed CoT and few-shot prompting on ComfyBench.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "retrieval interface / external datastore (embedding index) integrated into prompt",
            "training_method": "in-context retrieval augmentation (not model fine-tuning)",
            "intervention_type": "prompting / hybrid approach (retrieval augmentation)",
            "intervention_description": "Use of an embedding-based retrieval of existing workflow code/examples and inclusion of top-k demonstrations in prompts to the LLM, enabling re-use of existing components and reducing hallucination.",
            "intervention_effect": "Marked improvement over plain few-shot / CoT prompting for interactive workflow synthesis. RAG with code representation yields the best single-model baseline in ablation (see metrics above). The paper reports that retrieval-based agents (RAG) outperform CoT and few-shot because they can exploit external knowledge.",
            "hypothesized_cause_of_gap": "RAG mitigates the gap by supplying concrete examples and node usage that exceed LLM context/knowledge, addressing hallucination and missing expert knowledge that hinder procedural task performance.",
            "uuid": "e828.3",
            "source_info": {
                "paper_title": "ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Prompting baselines",
            "name_full": "Zero-shot, Few-shot, Chain-of-Thought (CoT), and CoT with Self-Consistency (CoT-SC)",
            "brief_description": "Standard prompting strategies evaluated as baselines: zero-shot and few-shot prompting, CoT (encouraging stepwise reasoning), and CoT-SC (ensembling multiple CoT trajectories).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Zero-shot / Few-shot / CoT / CoT-SC (prompting baselines)",
            "model_description": "Prompt-engineering strategies: zero-shot direct instruction; few-shot in-context demos (paper used 3 demos); CoT asks model to generate reasoning steps before the final answer; CoT-SC ensembles parallel CoT trajectories (paper ensembles 3 trajectories).",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ComfyBench workflow generation (baseline comparison)",
            "interactive_task_type": "multi-step reasoning / planning / workflow synthesis",
            "interactive_performance": "Paper reports: Zero-shot fails on ComfyBench; Few-shot gives modest improvements; CoT &gt; Few-shot; CoT-SC slightly better than CoT but both substantially worse than retrieval-augmented or ComfyAgent approaches. Example aggregate: GPT-4o + CoT total performance was much lower than RAG or ComfyAgent (paper's Table 2 shows CoT yields nontrivial gains over few-shot but cannot match RAG).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "chain-of-thought prompting (internal reasoning traces), ensembling (self-consistency)",
            "training_method": "prompting / in-context learning (no fine-tuning)",
            "intervention_type": "prompting strategy",
            "intervention_description": "Use CoT to surface reasoning and CoT-SC to ensemble multiple reasoning trajectories; few-shot uses demonstrations in-context.",
            "intervention_effect": "CoT and CoT-SC improve over naive few-shot/zero-shot on the procedural workflow generation tasks but are still outperformed by RAG and ComfyAgent. The paper attributes residual failures to context limitations and hallucination — prompting alone insufficient for complex procedural design.",
            "uuid": "e828.4",
            "source_info": {
                "paper_title": "ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Automated design of agentic systems",
            "rating": 1,
            "sanitized_title": "automated_design_of_agentic_systems"
        },
        {
            "paper_title": "A survey on large language model based autonomous agents",
            "rating": 1,
            "sanitized_title": "a_survey_on_large_language_model_based_autonomous_agents"
        }
    ],
    "cost": 0.020423249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems
26 Nov 2024</p>
<p>Xiangyuan Xue 
Shanghai Artificial Intelligence Laboratory</p>
<p>Zeyu Lu 
Shanghai Artificial Intelligence Laboratory</p>
<p>Di Huang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Zidong Wang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Wanli Ouyang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Lei Bai 
Shanghai Artificial Intelligence Laboratory</p>
<p>! Ksampler 
Shanghai Artificial Intelligence Laboratory</p>
<p>! Vaedecode 
Shanghai Artificial Intelligence Laboratory</p>
<p>! Cliptextencode 
Shanghai Artificial Intelligence Laboratory</p>
<p>ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems
26 Nov 2024A11B431637DC4633B84FB8777C4EF1EEarXiv:2409.01392v2[cs.CL]<JSON object> {"11":"class_type":"UpscaleModelLoader", "inputs": {"model_name":}} … Vanilla
Much previous AI research has focused on developing monolithic models to maximize their intelligence, with the primary goal of enhancing performance on specific tasks.In contrast, this work attempts to study using LLM-based agents to design collaborative AI systems autonomously.To explore this problem, we first introduce ComfyBench to evaluate agents's ability to design collaborative AI systems in ComfyUI.ComfyBench is a comprehensive benchmark comprising 200 diverse tasks covering various instructionfollowing generation challenges, along with detailed annotations for 3,205 nodes and 20 workflows.Based on Comfy-Bench, we further develop ComfyAgent, a novel framework that empowers LLM-based agents to autonomously design collaborative AI systems by generating workflows.Com-fyAgent is based on two core concepts.First, it represents workflows with code, which can be reversibly converted into workflows and executed as collaborative systems by the interpreter.Second, it constructs a multi-agent system that cooperates to learn from existing workflows and generate new workflows for a given task.While experimental results demonstrate that ComfyAgent achieves a comparable resolve rate to o1-preview and significantly surpasses other agents on ComfyBench, ComfyAgent has resolved only 15% of creative tasks.LLM-based agents still have a long way to go in autonomously designing collaborative AI systems.Progress with ComfyBench is paving the way for more intelligent and autonomous collaborative AI systems.</p>
<p>Introduction</p>
<p>The recent evolution of AI is defined by the growing importance of collaborative AI systems, which integrate multiple models and tools to work as a whole collaborative system [74].The success of ChatGPT [37] presents the possibility of integrating a wide range of tasks, such as web browsing [35], image generation [43], and code execution, into a single conversational agent.Unlike traditional AI models that function as single entities, collaborative AI systems integrate multiple AI components, each contributing unique capabilities to solve complex problems.This shift towards integration has become crucial for achieving stateof-the-art results, as it leverages the combined strengths of diverse AI functionalities within a unified framework.</p>
<p>Unfortunately, the design space and optimization of collaborative systems often require significant human expertise [32].For instance, AlphaCode 2 [2] utilizes an intricate AI system to achieve expert-level performance in competitive programming, which involves fine-tuned LLMs for coding and scoring, expert models for clustering, and modules for code execution and filtering.These components collaborate in a precise manner carefully designed by human experts to achieve state-of-the-art performance, which is not achievable by any single component or their casual combination.This brings us to a pivotal question: Can we develop an agent, akin to a human expert, capable of autonomously designing collaborative AI systems?</p>
<p>In this work, we explore this question in ComfyUI [14], a trending and open-source platform that supports various generative tools and models in terms of nodes.Within Com-fyUI, users can flexibly construct and connect nodes to build the workflow of the collaborative AI system, which is capable of producing highly customized artwork.</p>
<p>To evaluate agents's ability to design collaborative AI systems in ComfyUI, we introduce ComfyBench, a comprehensive benchmark shown in Figure 1, which comprises 200 diverse tasks covering various instruction-following generation challenges, along with detailed annotations for 3,205 nodes and 20 workflows.We categorize tasks into three levels of difficulty: vanilla, complex, and creative.In each task, agents are required to create workflows that describe collaborative AI systems within ComfyUI, which can then be executed to produce the desired outcomes specified in the task instructions.We assess the performance of agents using two evaluation metrics: pass rate, showing the success of workflow execution, and resovle rate, reflecting the effectiveness in meeting task requirements.</p>
<p>Based on ComfyBench, we find that autonomous designing collaborative AI systems presents significant chal- Given the task instruction, agents are required to learn from documents and create workflows to describe collaborative AI systems.The performance is measured by pass rate and resolve rate, reflecting whether the workflow can be correctly executed and whether the task requirements are realized.(b) ComfyAgent builds collaborative Al systems in ComfyUI by generating workflows.The workflows are converted into equivalent code so that LLMs can better understand them.ComfyAgent can learn from existing workflows and autonomously design new ones.The generated workflows can be interpreted as collaborative AI systems to complete given tasks.lenges for agents.First, the native JSON representation of workflows is inadequate for LLM-based agents to fully comprehend the intrinsic logic and dependencies within the systems [65].Second, properly organizing the pipeline and utilizing AI-related modules require substantial expert knowledge and complex cognitive processes [22].Traditional methods, such as Chain-of-Thought (CoT) [57] or Retrieval-Augmented Generation (RAG) [30], suffer from context limitation and hallucination problems, thus failing to generate complex workflows reliably.</p>
<p>To tackle the aforementioned problems, we introduce ComfyAgent, a novel framework shown in Figure 1, which empowers LLM-based agents to autonomously design collaborative AI systems by generating workflows.The core concepts behind ComfyAgent are two-fold.First, it represents workflows with code, which can be reversibly converted into workflows and executed as collaborative systems by the interpreter.Second, it constructs a multi-agent system [20] which cooperates to learn from existing workflows and generate new workflows for a given task.The core of the multi-agent system is PlanAgent, responsible for the global planning of workflows by task instruction.After collecting document descriptions of all components and code examples of collected workflows, RetrievalAgent automatically retrieves and learns key information about the given task from code.CombineAgent and AdaptAgent are responsible for code modification to construct a workflow that can fulfill the specified task.Upon task completion, PlanAgent finishes the task to form the final workflow.</p>
<p>We extensively evaluate the common baseline agents [10,30,53,57], as well as our proposed Com-fyAgent, on ComfyBench.</p>
<p>The experimental results indicate that ComfyAgent achieves comparable pass rate and resolve rate to o1-preview [39] and significantly surpasses other agents on ComfyBench.However, Com-fyAgent has resolved only 15% of creative tasks, which is still far from satisfactory.We believe that LLM-based agents still have a long way to go for autonomously designing collaborative AI systems.</p>
<p>Our contributions can be summarized as follows:</p>
<p>• We introduce ComfyBench, the first-of-its-kind comprehensive benchmark for the development of agents capable of designing and executing a wide range of collaborative AI systems in ComfyUI.• We propose ComfyAgent, which represents workflows with code and constructs a novel multi-agent framework to empower LLM-based agents in autonomously designing collaborative AI systems.• Experimental results demonstrate that our code representation significantly improves the pass rate and resolve rate on ComfyBench.Furthermore, ComfyAgent achieves a comparable pass rate and resolve rate to o1-preview and significantly surpasses other agents on ComfyBench.</p>
<p>Related Work 2.1. LLM-based Agents</p>
<p>LLM-based agents leverage LLMs to interact with external tools and solve real-world problems [52,60].Agents can exploit the powerful capabilities of LLMs in cognition and creativity, adapting to various novel tasks without relying on human intervention [31,34].Besides, agents have access to a wide range of external tools, making up for the natural limitation of processing only natural language [13,77].</p>
<p>Researchers have been devoted to enhancing the capabilities of LLM-based agents.Many prompt-based methods [8,33,47,53,54,57,70] are proven effective for improving agents' planning and reasoning.Memory mechanism [78,80] and RAG [17,30] further extend agents' working context and knowledge base.Reinforcement learning [7,24,41], serving as a post-training paradigm, allowing even stronger enhancement and alignment.These various methods hasten the advent of advanced theories for agents, such as self-evolution [61,64,83] and unified architecture [48,82].</p>
<p>Based on well-designed module architectures and interaction mechanisms, LLM-based agents can be applied to solve a wide range of complex tasks, such as web navigation [15,28,35,69,81], interface operation [11,59,62,63,68], code generation [55,56,66,72], and other tasks in specified domains [18,23,46,50,67].These tasks, previously considered to require expert knowledge and human intervention, can now be accomplished by agents reliably.</p>
<p>Collaborative AI Systems</p>
<p>Zaharia et al. [74] points out that state-of-the-art results are increasingly obtained by collaborative AI systems instead of monolithic models, which has been verified by many previous works [2,30,36,49,51].Yuan et al. [73] even designs a multi-agent framework that incorporates several advanced visual agents to replicate generalist video generation demonstrated by Sora [9].The advantages of collaborative AI systems are prominent.Dynamic systems make it possible to fetch external knowledge with components such as retrievers.Besides, extra modules such as verifier make the results controllable and trustworthy.</p>
<p>Compared to monolithic models, collaborative AI systems have posed new challenges in design and optimization.Novel paradigms are emerging to tackle these challenges, of which the most popular is composition frameworks, which build applications out of calls to models and other components [5,12,21,29,58,79].DSPy [25] automatically optimizes the prompt instructions, few-shot examples and other parameters in the pipeline to maximize the end-to-end performance.GPTSwarm [84] supports building agents in the form of graphs and optimizing the parameters and connections as an entire system.</p>
<p>ComfyBench</p>
<p>We propose ComfyBench, a new benchmark for evaluating the capability of LLM-based agents to autonomously design collaborative AI systems within the ComfyUI [14] platform.Given the task instruction, ComfyBench requires agents to generate workflows to describe collaborative AI systems in ComfyUI, producing the desired results after execution.</p>
<p>ComfyUI Platform</p>
<p>Overview.ComfyUI is an open-source platform for designing and executing generative pipelines, where users can design their workflows by creating and connecting a series of nodes.Each node stands for a module, which can be a model, a tool, or a widget.On the user side, the workflows are visualized as directed acyclic graphs (DAGs), so that users can operate them intuitively.On the server side, however, the workflows are saved in a JSON object, which describes the nodes and connectivity in a structured manner.ComfyUI executes the JSON format workflows at the granularity of nodes, automatically loading models, conducting inference, passing variables, and returning results to users.Features.ComfyUI provides native support for common models and tools related to Stable Diffusion [44] [44], including but not limited to VAEs [26], CLIPs [42], Lo-RAs [45], IP-Adapters [71], and ControlNets [75].With the joint efforts of the open-source community, hundreds of ComfyUI extensions are developed to support various latest models and tools, such as InsightFace [19], IC-Light [76], and Segment Anything [27], which bring about immense potential for realizing flexible and controllable generation tasks.By designing intricate workflows, users can produce fantastic artwork that cannot be achieved by common tools.Limitations.Despite the fancy effects that ComfyUI can achieve, the workflows are always manually crafted, which can be quite expensive.On the one hand, expert-level skills are required to figure out the appropriate nodes and parameters that can produce the desired results.On the other hand, workflows are designed to solve a specific task, while creating new workflows can be laborious and time-consuming.</p>
<p>Benchmark Contents</p>
<p>ComfyBench colslects annotations for 3205 nodes and 20 workflows and construct 200 tasks as is shown in Figure 2.More examples can be found in the supplementary material.Node document provides complete documentation for the 3205 nodes registered in ComfyUI until September 2024, serving as an important manual for agents to utilize the nodes properly.The source code is cloned from Salt AI Documentation [1], which is then cleaned and arranged into documentation, describing the usage of each node and their inputs and outputs.This structured documentation serves as a solid reference resource for understanding the complex node ecosystem within the ComfyUI framework.</p>
<p>Complex</p>
<p>Given an image of a dish table.First remove the fork on the table.Then convert the image into a painting with watercolor style.</p>
<p>...</p>
<p>Creative</p>
<p>Given a photo of a young man.Generate another photo to show an elderly version of him, with wrinkles, gray hair, and other signs of aging, but his identity should be preserved.</p>
<p>... Curriculum workflow is a set of 20 tutorial workflows in ComfyUI, which introduces some common nodes and routines to build workflows for solving basic tasks.All the workflows are verified feasible with common nodes and models in ComfyUI and correctly annotated with their purposes and principles.The curriculum workflow allows agents to master useful skills in a human-like manner.</p>
<p>Task instruction serves as the core part of the benchmark, containing 200 task instructions.Each instruction defines a specific task, which should be solved by designing and executing a ComfyUI workflow.To reflect the generalization ability of agents, the tasks are manually categorized into three difficulty levels: vanilla, complex, and creative.</p>
<p>• Vanilla includes 100 tasks, which can be solved by learning from a single curriculum workflow, involving minor modifications on several nodes and their parameters.• Complex includes 60 tasks, which necessitates learning from more than one curriculum workflow, involving functional combinations of multiple pipelines and minor adjustments on nodes and parameters.• Creative includes 40 tasks, which cannot be solved by directly imitating one or more curriculum workflows, requiring agents to understand the underlying principles and apply the learned skills in a novel way.Besides, we provide an auxiliary dataset as the input for some of the tasks.If an image or video is involved in the task, it will be specified in the instruction.</p>
<p>Evaluation Metrics</p>
<p>Different from the common benchmarks for image or video generation, we are evaluating the generated workflows, so traditional metrics are not applicable.We divide the evaluation into two progressive stages.The first stage is to check whether the generated workflows are syntactically and semantically correct, and the second stage is to check whether the generated workflows can produce the expected results in the task instructions.We design two metrics: pass rate and resolve rate, corresponding to the two stages respectively.Pass rate measures the ratio of tasks where generated workflows are syntactically and semantically correct.In practice, each workflow is sent to the server of ComfyUI, which automatically conducts validation and execution.A task will be marked as passed only if the server finishes the execution process and returns a success message.</p>
<p>Resolve rate measures the ratio of tasks where generated workflows can produce the expected results in the task instruction.In other words, we need to automatically determine the consistency between the produced images or videos and the task requirements.In practice, we apply Visual Language Models (VLMs), where different prompt templates are written according to the input and output modalities.Specifically, we adopt GPT-4o for evaluation, as it demonstrates strong capabilities for understanding language and analyzing visual content.For images, we directly encode them into the prompt.For videos, we uniformly sample up to 10 frames before encoding, which compresses the context length within a reasonable range.A task will be marked as resolved only if the GPT-4o model confirms that all the requirements are satisfied.</p>
<p>Human Evaluation</p>
<p>The computation of the resolve rate involves using VLMs to determine the consistency between the produced images or videos and the task requirements, which unavoidably involves subjectivity.To verify the reliability of VLM-based evaluation, we conduct a human evaluation to analyze the agreement between human evaluators and GPT-4o.</p>
<p>We randomly select 50 tasks that are completed by Com-fyAgent proposed in Section 4, where human evaluators and the latest GPT-4o model are asked to provide their judgments respectively.We compute the average scores for each question under a sample size of 400 and conduct a correlation analysis.Results are presented in Table 1, where the statistics indicate a strong agreement between human evaluators and GPT-4o, which verifies the reasonability and reliability of our proposed VLM-based evaluation.More details can be found in the supplementary material.</p>
<p>Task Instruction
Code Workflow Figure 3
. The architecture of the ComfyAgent framework.Multiple agents cooperate to design workflows in a step-by-step manner.Given the task instruction, the planner initializes the memory and produces a plan.For each step, the planner updates the plan and forms an action.Different actions, including combine, adapt, and retrieve, are then handled by corresponding agents.After combination or adaptation, refine action will be conducted to ensure the correctness.All the agents can interact with the memory, which consists of history, reference, and workspace.Once the task is deemed completed, the planner will finish the procedure and save the workflow.</p>
<p>ComfyAgent</p>
<p>Flow Graph JSON</p>
<p>{ "1": { "class_type": "CheckpointLoaderSimple", "inputs": { "ckpt_name": "sdxl.safetensors"} }, "2": { "class_type": "EmptyLatentImage", "inputs": ... }, ... } Collaborative AI systems coordinate multiple interacting components, such as AI models and external tools, using workflows.These workflows are essentially high-level diagrams that outline the operational logic.They can be naturally represented as directed acyclic graphs (DAGs) consisting of two main elements: vertices and directed edges.Vertices represent information processing components with input and output interfaces, while directed edges represent information flow between components.Specifically, the incoming edges of a vertex represent the external information it receives, while the outgoing edges signify the information it outputs, with multiple edges illustrating various information paths.This abstraction allows efficient construction and modification of collaborative AI systems, as demonstrated by platforms like ComfyUI [14].</p>
<p>There are four common formats to represent workflows: flow graph, JSON, element list, and code, as shown in Figure 4. Flow graph [14] provides intuitive visualization for humans but is unsuitable for LLM or VLM processing.JSON, typically used in [77], provides a structured representation but is unwieldy for complex workflows due to redundant information and LLMs' context limitations.Element list, typically used in [32], is closer to natural language and provides a more compact representation, but it lacks explicit topological relationships, hindering LLMs from correctly processing complex workflows.</p>
<p>In contrast, code emerges as the most effective representation, offering various advantages including Turing completeness, rich semantic information, and natural compatibility with LLMs' code generation capabilities.We implement code representation using a restricted subset of Python-like syntax, involving basic operations and control structures while excluding advanced features.We also conduct an ablation study in Section 5.4 to demonstrate the superiority of code representation over other representations.</p>
<p>Multi-Agent Framework</p>
<p>Through code representation, we can effectively prompt LLMs to generate workflows for the expected collaborative AI systems.However, with the complexity of workflows increasing, traditional agents suffer from context limitation and hallucination problems, failing to generate workflows correctly and stably, which is proved in Section 5.3.</p>
<p>To achieve reliable generation of ComfyUI workflows, we propose ComfyAgent, a multi-agent framework to design workflows in a step-by-step manner.As demonstrated in Figure 3, ComfyAgent consists of three independent Memory stores the recent state of ComfyAgent, which is built on psychological theories [4].Specifically, it refers to the working memory [6], reflecting the current circumstances, namely the recent history behaviors, results from intermediate steps, internal reasoning, and external knowledge.We formulate them into three parts.</p>
<p>• History maintains recent plans and actions of Planner, enabling action review for subsequent planning.• Reference stores information retrieved from the knowledge base, and can be updated through active retrieval.• Workspace contains the current workflow together with its natural language annotation, which maintains a complete record of the current working status.Planner, which can be specified as PlanAgent, serves as the core of ComfyAgent, providing the global scheme to design and modify workflows under the task instruction.</p>
<p>• At the beginning of the task, PlanAgent selects an existing workflow to initialize the memory and produces a thorough multi-step plan based on the task instruction.• For each step, PlanAgent produces a high-level plan, together with an action based on the current memory, which will then be processed by other agents.• For each step, PlanAgent evaluates the completion status of the task.Once the task is deemed completed, PlanAgent will finish the procedure and save the workflow.Actions represent different actions that can be selected by PlanAgent.All the actions are meant to modify the current memory.In ComfyAgent, we have three actions.</p>
<p>• Combine is processed by CombineAgent, which aims to combine the current workflow with another workflow from references specified by PlanAgent.• Adapt is processed by AdaptAgent, which is responsible for adapting the details (e.g.parameters) of the current workflow based on the prompt given by PlanAgent.</p>
<p>• Retrieve is processed by RetrieveAgent, which follows the prompt given by PlanAgent to retrieve relevant information and update references accordingly.After combination or adaptation, the updated workflow will be checked and refined by RefineAgent before entering the workspace, which avoids the accumulation of errors.</p>
<p>After the action is processed, ComfyAgent will enter a new step, where PlanAgent updates the existing plan and forms a new action.In such a way, ComfyAgent gradually generates the workflow.When the procedure is finished, the code representation will be converted into the standard format, which exactly describes the collaborative AI system.</p>
<p>Experiments</p>
<p>Baseline Agents</p>
<p>ComfyBench places strong requirements on the learning and generalization ability of agents, necessitating LLMs with powerful reasoning capabilities and sufficient context length.Therefore, we conduct experiments on the advanced models, involving both commercial and open-source LLMs.</p>
<p>• Llama-3.1 (llama-3.1-70b-instruct)[16] is an LLM developed by Meta, achieving state-of-the-art performance in the open-source community.• Claude-3.5 (claude-3.5-sonnet-20240620)[3] is a commercial LLM developed by Anthropic, with a reputation for its creativity and interaction quality.• GPT-4o (gpt-4o-2024-08-06) [38] is currently the mainstream model provided by OpenAI and most widely used by the community.It still maintains state-of-the-art performance in leaderboards of various domains.• o1-mini (o1-mini-2024-09-12) and o1-preview (o1-preview-2024-09-12) [39] are the latest models released by OpenAI, which focus on complex reasoning tasks such as mathematics and programming.</p>
<p>We adopt five methods that have proved to be universally effective by the community and can be conveniently adapted to solve the tasks in ComfyBench.</p>
<p>• Zero-shot Learning directly feeds LLMs with the task instruction to conduct inference.• Few-shot Learning [10] provides a set of demonstrations in the prompt, which utilizes the in-context learning ability of LLMs and facilitates the correct generation.• Chain-of-Thought (CoT) [57] is improved based on few-shot learning, which instructs the agent to articulate the reasoning process before providing the final answer.• CoT with Self-consistency (CoT-SC) [53] is an improved version of CoT, which ensembles parallel trajectories and then selects the most consistent answer.• Retrieval-Augmented Generation (RAG) [30] learns from the retrieved demonstrations, which efficiently utilizes the most relevant information.Considering the limited experimental budget, we evaluate the methods mainly on GPT-4o.ComfyAgent is also evaluated on Llama-3.1 to verify its performance on the opensource model.RAG is evaluated on all the models to provide a horizontal comparison of their capabilities.</p>
<p>Implementation Details</p>
<p>ComfyUI Settings.The experiments are conducted on a fixed version of ComfyUI with necessary extensions.A set of common models is installed to make the workflows runnable and the results reproducible.In ComfyUI, the workflows are natively represented in JSON format.We implement a parser to extract the entire DAG described in the JSON object, which is then converted into a sequence of Python-like function calls in topological order.Another parser is implemented to restore this conversion.This ensures that workflows can be effectively generated by agents through code representation while remaining fully compatible with the ComfyUI execution environment.Note that the node documents are not inputted into the agents due to context limitations, even though this technique would potentially improve their performance.We leave this exploration for future research.Agent Parameters.We follow CodeAct [54] to format the interaction with LLMs.The parameters of the agents are manually set to exploit their largest potential within a limited context length.For few-shot learning, CoT, and CoT-SC, the number of demonstrations is consistently set to 3, but CoT-SC will ensemble 3 trajectories in parallel to form the final answer.For RAG and ComfyAgent, we adopt an advanced embedding model (text-embedding-3-large) [40] released by Ope-nAI to encode the texts for retrieval, and the number of retrieved demonstrations is set to 5. For ComfyAgent, the modules are implemented as different roles played by the same LLM.We provide the prompt implementation and an example trajectory in the supplementary material.</p>
<p>Evaluation Results</p>
<p>We report the evaluation results of different agents on Com-fyBench in Table 2.All the agents follow the parameter settings in Section 5.2 and adopt code representation in Section 4.1.We also present some examples generated by Com-fyAgent on ComfyBench in Table 5.More examples can be found in the supplementary material.</p>
<p>In terms of methods, zero-shot learning fails due to a lack of domain knowledge.CoT achieves a better performance than few-shot learning, and CoT-SC shows a slight improvement over CoT.However, these agents fail to achieve comparable performance with retrieval-based agents, because they cannot fully exploit the external knowledge provided by ComfyBench.ComfyAgent outperforms all the baselines in every task category, achieving an overall pass rate of 56% and resolve rate of 32.5%, which indicates that the multi-agent framework applied in ComfyAgent is capable of alleviating the hallucination problem and improving the generation quality and stability.This improvement is consistent on both open-source and commercial models.</p>
<p>In terms of models, Claude-3.5 and o1-mini show poor performance under RAG, which is mainly caused by the hallucination problem, where models modify the nodes or parameters without verifying their validity.In contrast, Llama-3.1 and GPT-4o show less hallucination and achieve better performance, and o1-preview achieves an outstand-Table 5. Some examples generated by ComfyAgent on ComfyBench.Given the task instruction, ComfyAgent designs a workflow to describe the collaborative AI system in ComfyUI, which is then executed to generate the result required by the instruction.</p>
<p>Task Instruction Task Instruction Task Instruction</p>
<p>You are given an image of a male celebrity.Transform the man in the image into a beautiful woman with ponytail hair while preserving her facial identity.</p>
<p>You are given an image of a standing cat.</p>
<p>Replace the background with a scene of a cozy living room while keeping the lighting and shadows consistent.</p>
<p>You are given an image of a red bridge with a person standing on it.Remove the person from the image while maintaining the original appearance of the bridge.
Input Result Input Result Input Result
ing performance comparable to ComfyAgent on GPT-4o, which showcases its strong reasoning capabilities.</p>
<p>Ablation Studies</p>
<p>Representation.We conduct an ablation study to verify the advantages of representing workflows with code introduced in Section 4.1.We implement three variants of RAG on GPT-4o, where the workflows are respectively represented in JSON, element list, and code.The variants are evaluated on ComfyBench with the same parameter settings as in Section 5.3.Flow graph is not implemented because it is not understandable by LLM-based agents.We report the ablation results in Table 3, where RAG with code representation shows superior performance over other representations, which verifies our claim.RAG with JSON representation is expected to perform even worse without the unique feature of GPT-4o to generate structured JSON objects.</p>
<p>Architecture.We conduct another ablation to demonstrate the rationality of the multi-agent framework introduced in Section 4.2, where four variants of ComfyAgent on GPT-4o are implemented, each removing one agent from the original framework.We report the ablation results in Table 4.We find that removing any of the four agents will lead to a performance drop.Removing CombineAgent alleviates hallucination in the vanilla category, but leads to failure in the complex category, demonstrating the necessity of integrating multiple demonstrations.Removing AdaptAgent or RetrieveAgent leads to a significant performance drop in all the categories, which underscores the critical role of detail modification and knowledge utilization.Removing Re-fineAgent leads to a minor but noticeable performance drop in all the categories, indicating its effectiveness in mitigating error accumulation.Therefore, we can conclude that our meticulously-designed multi-agent framework contributes to the superior performance of ComfyAgent.</p>
<p>Conclusion</p>
<p>In this work, we investigate the potential of LLM-based agents to autonomously design collaborative AI systems.</p>
<p>Building upon the open-source ComfyUI [14] platform, we present ComfyBench, an autonomous workflow design environment tailored for collaborative AI systems.Comfy-Bench serves as both a benchmark for evaluating the capabilities of agents in designing collaborative systems and a means to extend the applicability of ComfyUI in real-world, language-instruction-based generative applications.We further develop ComfyAgent, which achieves a comparable resolve rate to o1-preview [39] and significantly surpasses other agents on ComfyBench.We point out two key insights underpinning ComfyAgent.First, representing workflows through code is efficient, which provides rich semantics for agents to better understand complex systems.Second, the multi-agent framework of ComfyAgent highlights the substantial benefits of task decomposition and collaborative generation.However, the top-performing agent, ComfyAgent, has achieved a resolve rate of only 15% for creative tasks on ComfyBench.LLM-based agents still have a long way to go in designing collaborative AI systems.We hope that ComfyBench and ComfyAgent can serve as valuable resources in advancing the development of intelligent and autonomous collaborative AI systems.</p>
<p>Limitations.Well-designed fine-tuning-based method could potentially elevate agents to an expert level on ComfyBench, but it can be quite expensive and is not involved in our work.In addition, ComfyAgent operates in an open-loop manner.An alternative approach would involve using VLMs as evaluators to provide feedback [22], enabling a closed-loop system where agents can refine their workflows based on evaluations, thus leading to better results.Exploring this paradigm is left for future work.</p>
<p>ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems</p>
<p>Supplementary Material</p>
<p>Overview</p>
<p>This supplementary document provides additional details to support our main manuscript, organized as follows:</p>
<p>•</p>
<p>A. More Task Instructions</p>
<p>In Table 6, we present more examples of task instruction to provide a deeper preview of ComfyBench.The modalities and categories of the tasks are also included.Comfy-Bench covers a wide range of image and video generation tasks.Each instruction describes an expected result to realize.The tasks are categorized into three difficulty levels: vanilla, complex, and creative, which reflect the generalization capability of LLM-based agents.</p>
<p>B. VLM-based Evaluation Implementation</p>
<p>In ComfyBench, we adopt the latest GPT-4o [38] model to compute the resolve rate.We design prompt templates for each task modality, where the images and videos can be encoded.Considering GPT-4o cannot directly process videos, we uniformly sample up to 10 frames and input them as a sequence of images, so that the context length can be controlled within a reasonable range.Since the tasks in Comfy-Bench involve operations such as upscaling and interpolation, we also provide the model with the original resolution and frame rate information.The model must conduct the necessary analysis before providing the result so that the judgment can be more reasonable and reliable.Specifically, we present a typical example of VLM-based evaluation in Figure 7, which evaluates a text-to-image generation task.The answer given by GPT-4o involves a thorough analysis of the consistency between the task requirements and the generated image, followed by a correct final judgment, demonstrating a strong capability to understand the points and determine the consistency.</p>
<p>C. Human Evaluation Details</p>
<p>We randomly select 50 tasks that are completed by Com-fyAgent to form 50 questions.In each question, we provide the task instruction, the input image or video (if any), and the generated image or video, where the answer should be either "Yes" or "No", representing whether the generated result is consistent with the task instruction.We create multiple questionnaires in Google Forms for distribution, each containing 20 questions.A sample question selected from the questionnaires is presented in Figure 5.We collect 20 answer sheets from human evaluators to form a sample size of 400, with each question answered by 8 human evaluators.We also prompt the latest GPT-4o model to answer each question for 8 times.All the answers are viewed as 0-1 variables to compute the average scores given by human evaluators and GPT-4o respectively, which indicate their tendency on each question.</p>
<p>Based on the 50 pairs of average scores, we present a heatmap in Figure 6 to intuitively demonstrate the correlation between the scores given by human evaluators and GPT-4o.We also calculate Kendall's τ , Pearson's r, and Spearman's ρ.Both the heatmap and the statistics indicate a strong agreement between human evaluators and GPT-4o.</p>
<p>D. ComfyAgent Prompt Implementation</p>
<p>We provide the prompt implementation of ComfyAgent, including the prompt templates for PlanAgent, Com-bineAgent, AdaptAgent, and RefineAgent, which are respectively presented in Figure 8, 9, 10, and 11.Generally, we first introduce the background information of the Com-fyUI platform, then present the necessary information for the current task, and finally specify the answer format.We follow the prompting strategy in CodeAct [54], which encloses answers with XML tags, to extract the results.The memory and RetrieveAgent are implemented as rule-based modules, so prompt templates are not available for them.</p>
<p>E. More ComfyAgent Examples</p>
<p>As an extension, we present more examples generated by ComfyAgent on ComfyBench in Table 7 to demonstrate the fantastic effects that ComfyAgent can achieve.Restricted by the format, we only present the image examples.Some examples involving videos as inputs and results will be later presented on our project website.Furthermore, we present a typical example trajectory of ComfyAgent in Figure 12, which specifically demonstrates how ComfyAgent gradually designs a complete workflow for the collaborative AI system according to the task instruction.The task is selected from the complex category in ComfyBench, which requires generating a video and interpolating the frames, involving common techniques such as text-to-video generation and video interpolation.Table 6.More examples of task instruction in ComfyBench.We present the instructions together with their modalities and categories.</p>
<p>Task Instruction Modality Category</p>
<p>Generate an image of a hotel room containing a bed, a desk, and a window.The result should be a high-quality image.</p>
<p>T2I Vanilla</p>
<p>I2V Vanilla</p>
<p>I2I Vanilla</p>
<p>You are given a video "passing car.mp4" of a gray car passing by on the road.Interpolate the video to increase the frame rate by 2x.The result should be a smoother video of the gray car passing by.</p>
<p>V2V Vanilla</p>
<p>First generate an image of a city street at night.Then upscale it by 2x.The result should be a high-resolution image of a city street.</p>
<p>T2I Complex</p>
<p>First generate a 2-second video of a bustling city at night with skyscrapers and bright lights.Then interpolate the video to increase the frame rate by 3x.The result should be a smoother video of the city at night.</p>
<p>T2V Complex</p>
<p>I2I Complex</p>
<p>I2I Complex</p>
<p>Generate a poster for a series named "Breaking Bad".The result should be a vertical image of the main character, Walter White, wearing a white vest with a serious expression, standing in front of a motorhome in the desert.The title "Breaking Bad" should be displayed in the top center of the image, followed by the tagline "Say my name".</p>
<p>T2I Creative</p>
<p>I2I Creative T2V Creative</p>
<p>You are given a video "male idol.mp4" of a male idol dancing in a room.Convert the video into a sketch-style animation with black strokes and white background.The result should be a sketch-style video of the idol dancing while maintaining the original elements and movements.</p>
<p>V2V Creative Table 7.More examples produced with the collaborative AI systems designed by ComfyAgent on ComfyBench.We present the task instructions, generated results, along with image or video inputs if they are required in the task.</p>
<p>Task Instruction Input Result</p>
<p>Generate an image of a hot air balloon floating over a scenic valley at sunrise.The result should be a high-quality image.</p>
<p>Task Instruction Input Result</p>
<p>You are given an image of a table filled with dishes.Remove the fork on the table.The result should be a high-quality image without visible artifacts.</p>
<p>You are given an image of a red car parked on the street.Replace the tree behind the car with a white house.The result should be a high-quality image without visible artifacts.</p>
<p>You are given a photo of mountains and rivers with a visible watermark in the bottom right corner.Remove the watermark from the image while maintaining the quality and content of the original photo.The result should be a high-quality image without the watermark.</p>
<p>You are given an image of a girl playing the guitar.Generate an image of an old man playing the guitar in a forest with the same pose as the girl.</p>
<p>The result should be a realistic image of an old man playing the guitar.</p>
<p>You are given an image of a man wearing a black jacket.Change the black jacket into a white hoodie while ensuring that the modification looks natural and realistic.The result should be a high-quality image of the man wearing a white hoodie.</p>
<p>You are given an image of a toy dog.Replace the background with a scene of a sunny park with green grass while keeping the lighting and shadows consistent.The result should be an image of the toy dog in the park scene.</p>
<p>You are given an image containing two bottles of cosmetic products illuminated by a soft yellow light.Modify the illumination into a bright pink light to create a more vibrant and attractive appearance.The result should be an image of the cosmetic products with the new illumination.</p>
<p>Evaluation Prompt</p>
<p>You are an expert in image and video generation, familiar with the latest tasks and techniques.You are capable of understanding the task instruction, analyzing the generation result, and providing an accurate evaluation.Now you are evaluating the result of a text-to-image generation task.You should be tolerant to the quality of the generation result, and focus on the consistency with the instruction.
→ → → →
The task instruction is described as: First generate an image of a ship floating in the sea.The ship should be placed in the right half of the image, with the sea filling other areas.</p>
<p>Then convert the image into a painting with watercolor style.The result should be a high-quality image of the ship as a watercolor painting.
→ → →
The given image is the generation result, with an actual resolution of 512x512.</p>
<p>First, analyze whether the generation result meets each key point in the instruction.Enclose your analysis in the <analysis> tag.For example: <analysis>There is a cat in an astronaut suit, which is consistent with the instruction.The wall is white, which is different from the "green wall" in the instruction.</analysis>.
→ → →
Then, provide a final judgment of whether the generation result complies with the instruction.The judgment should either be "True" or "False".Enclose your judgment in the <judgment> tag.For example: <judgment>False</judgment>.After that, you should update your step-by-step plan to further modify your workflow.There are {limitation} steps remaining, so your plan should contain at most {limitation} steps.Make sure that each step is feasible to be converted into a single action.Your plan should be enclosed with "<plan>" tag.For example: <plan>Step 1: I will refer to "reference_name" to add a module.</p>
<p>Step 2: I will finish the task since the expected effects are realized.</plan>.
→ → → → →
Finally, you should choose one of the following actions and specify the arguments (if required), so that the updated workflow can realize the first step in your plan.You should provide your action with the format of function calls in Python.Your action should be enclosed with "<action>" tag.For example: <action>combine(name="reference_name")</action>, <action>adapt(prompt="Change the factor to 0.5 and rewrite the prompt.")</action>,and <action>finish()</action>. -<code>combine</code>: Combine the current workflow with a reference workflow, so that necessary modules can be added.Arguments: → -<code>name</code>: The name of the reference workflow you want to combine.-<code>adapt</code>: Adapt some parameters in the current workflow, so that the expected effects can be realized.Arguments: → -<code>prompt</code>: The prompt to specify the adaptation you want to make.-<code>retrieve</code>: Retrieve a new batch of reference workflows, so that more useful references can be found.Arguments: → -<code>prompt</code>: The prompt to describe the reference you want to retrieve.-<code>finish</code>: Finish the task since the current workflow can realize the expected effects.</p>
<p>Refer to the history before making a decision.Here are some general rules you should follow:</p>
<p>1.You should choose the <code>load</code>action if and only if the history is empty.2. If you choose the <code>load</code>or <code>combine</code>action, make sure the name exists in the reference.</p>
<p>Otherwise, try to update the reference with the <code>retrieve</code>action.You should also avoid reusing the same variable name, even if the variable is temporary.Your code should be enclosed with "<code>" tag.For example: <code>output = node(input)</code>.
→ → → →
After that, you should provide an annotation as in the reference, including the function and principle of the updated workflow.The function should be enclosed with "<function>" tag.For example: <function>This workflow generates a high-resolution image of a running horse.</function>.The principle should be enclosed with "<principle>" tag.For example: <principle>The workflow first generates a low-resolution image using the text-to-image pipeline and then applies an upscaling module to improve the resolution.</principle>.You should also avoid reusing the same variable name, even if the variable is temporary.Your code should be enclosed with "<code>" tag.For example: <code>output = node(input)</code>.
→ → → →
After that, you should provide an annotation as in the reference, including the function and principle of the updated workflow.The function should be enclosed with "<function>" tag.For example: <function>This workflow generates a high-resolution image of a running horse.</function>.The principle should be enclosed with "<principle>" tag.For example: <principle>The workflow first generates a low-resolution image using the text-to-image pipeline and then applies an upscaling module to improve the resolution.</principle>.According to the requirements, we have retrieved some relevant workflows which may be helpful:
{reference} ## Workspace
The code and annotation of the current workflow you are working on are presented as follows:
{workspace} ## Refinement
An error is detected in the current workflow, which is caused by some bugs in the Python code, such as nested calls and missing parameters.The specific error message is presented as follows:
→ → {refinement}
First, you should explain the reason of the error.Your explanation should be enclosed with "<explanation>" tag.For example: <explanation>The error is caused by the missing input.</explanation>.</p>
<p>→ →</p>
<p>After that, you should provide the corrected Python code to formulate the updated workflow.Each line of code should correspond to a single node, so you should avoid nested calls in a single statement.You should also avoid reusing the same variable name, even if the variable is temporary.Your code should be enclosed with "<code>" tag.For example: <code>output = node(input)</code>.
→ → → →
Finally, you should provide an annotation as in the reference, including the function and principle of the updated workflow.The function should be enclosed with "<function>" tag.For example: <function>This workflow generates a high-resolution image of a running horse.</function>.The principle should be enclosed with "<principle>" tag.For example: <principle>The workflow first generates a low-resolution image using the text-to-image pipeline and then applies an upscaling module to improve the resolution.</principle>.</p>
<p>Task Instruction</p>
<p>First generate a 2-second video of colorful fish swimming in an aquarium with bubbles rising to the surface.Then interpolate the video to increase the frame rate by 3x.The result should be a smoother video of the swimming fish.First generate a 2-second video of colorful fish swimming in an aquarium with bubbles rising to the surface.Then interpolate the video to increase the frame rate by 3x.The result should be a smoother video of the swimming fish.</p>
<p>→ →</p>
<p>The core requirements and the expected paradigm are analyzed as follows:</p>
<p>The task requires generating a short 2-second video of fish swimming in an aquarium with bubbles rising using a text-to-video approach, as the primary input is textual.The video needs to depict dynamic, colorful fish movements with the additional visual element of rising bubbles.After generating this video, the frame rate needs to be increased by 3x using a video interpolation process to achieve smoother motion.Thus, the workflow core involves two stages, first, synthesizing the video from a text prompt using a text-to-video node or series of nodes, followed by applying video interpolation to enhance frame smoothness.There are no specific quality-oriented steps like upscaling or refinement beyond the interpolation requirement.
→ → → → → → → →
Improving the quality of the generation result with additional steps, such as upscaling and interpolation, is not recommended, unless specified in the requirements.</p>
<p>→</p>
<h2>Reference</h2>
<p>According to the requirements, we have retrieved some relevant workflows which may be helpful:</p>
<ul>
<li>The workflow first loads the input video using "VHS_LoadVideo", which extracts the individual frames.The "RIFE VFI" node is then used to interpolate the frames by generating additional frames between the existing ones.In this scenario, the multiplier is set to 3x, effectively tripling the frame count and enabling a smoother video playback at 24 frames per second.Finally, the interpolated frames are combined into a video or GIF format using "VHS_VideoCombine".The workflow first loads and inverts the input scribble image.The inverted image is then used by a "control_v11p_sd15_scribble_fp16" ControlNet model to extract and applied to the scribble, controlling and guiding the image generation process.A pre-trained model ("dreamshaper_8.safetensors")processes the positive conditioning generated by the combination of the text prompt and the ControlNet's output, along with negative conditioning from undesired traits such as "horror" and "lowres."The latent space image is generated using a KSampler and further decoded by the VAE to form the final image.The model synthesizes a highly detailed image while adhering closely to both the shape of the scribble and the desired features implied by the text prompt.
→ → → → → → → → </principle> ## History
Here is a recent history of your thought, plan and action in the previous steps.The most recent record is at the bottom.</li>
</ul>
<p>→ -The history is empty.</p>
<h2>Workspace</h2>
<p>The code and annotation of the current workflow you are working on are presented as follows:</p>
<p>-The workspace is empty.</p>
<h2>Action</h2>
<p>Based on the history and workspace, you should first think about what functions have been implemented and what modules remain to be added.Your thought should be enclosed with "<thought>" tag.For example: <thought>The basic pipeline has been implemented, but a module is needed to improve the quality.</thought>.
→ → →
After that, you should update your step-by-step plan to further modify your workflow.There are 5 steps remaining, so your plan should contain at most 5 steps.Make sure that each step is feasible to be converted into a single action.Your plan should be enclosed with "<plan>" tag.For example: <plan>Step 1: I will refer to "reference_name" to add a module.</p>
<p>Step 2: I will finish the task since the expected effects are realized.</plan>.
→ → → →
Finally, you should choose one of the following actions and specify the arguments (if required), so that the updated workflow can realize the first step in your plan.You should provide your action with the format of function calls in Python.Your action should be enclosed with "<action>" tag.For example: <action>combine(name="reference_name")</action>, <action>adapt(prompt="Change the factor to 0.5 and rewrite the prompt.")</action>,and <action>finish()</action>. Refer to the history before making a decision.Here are some general rules you should follow:</p>
<p>1.You should choose the <code>load</code>action if and only if the history is empty.2. If you choose the <code>load</code>or <code>combine</code>action, make sure the name exists in the reference.</p>
<p>Otherwise, try to update the reference with the <code>retrieve</code>action.</p>
<p>PlanAgent Answer</p>
<p><thought>Since the history is empty, I need to start by loading a relevant reference workflow that can be used as a base for generating the 2-second video of colorful fish in an aquarium.The "text_to_video" reference seems appropriate to use as it involves generating video from text prompts, which aligns with the first part of the task.</thought>
→ → →
<plan>Step 1: I will load the "text_to_video" reference workflow to use it as the base.</p>
<p>Step 2: I will adapt the workflow to modify the prompt to match the requirements ("colorful fish swimming in an aquarium with bubbles rising").</p>
<p>→</p>
<p>Step 3: I will combine it with the "video_frame_interpolation" reference to enable interpolation for smoother video.</p>
<p>→</p>
<p>Step 4: I will adapt the interpolation multiplier to 3x to achieve the expected frame rate increase.</p>
<p>→</p>
<p>Step 5: I will finish the task by confirming the generated output meets the required specifications.</plan>First generate a 2-second video of colorful fish swimming in an aquarium with bubbles rising to the surface.Then interpolate the video to increase the frame rate by 3x.The result should be a smoother video of the swimming fish.</p>
<p>→ →</p>
<p>The core requirements and the expected paradigm are analyzed as follows:</p>
<p>The task requires generating a short 2-second video of fish swimming in an aquarium with bubbles rising using a text-to-video approach, as the primary input is textual.The video needs to depict dynamic, colorful fish movements with the additional visual element of rising bubbles.After generating this video, the frame rate needs to be increased by 3x using a video interpolation process to achieve smoother motion.Thus, the workflow core involves two stages, first, synthesizing the video from a text prompt using a text-to-video node or series of nodes, followed by applying video interpolation to enhance frame smoothness.There are no specific quality-oriented steps like upscaling or refinement beyond the interpolation requirement.
→ → → → → → → →
Improving the quality of the generation result with additional steps, such as upscaling and interpolation, is not recommended, unless specified in the requirements.</p>
<p>→</p>
<h2>Reference</h2>
<p>According to the requirements, we have retrieved some relevant workflows which may be helpful:</p>
<p>-</p>
<p></function> <principle></p>
<p>The workflow first loads the input video using "VHS_LoadVideo", which extracts the individual frames.The "RIFE VFI" node is then used to interpolate the frames by generating additional frames between the existing ones.In this scenario, the multiplier is set to 3x, effectively tripling the frame count and enabling a smoother video playback at 24 frames per second.Finally, the interpolated frames are combined into a video or GIF format using "VHS_VideoCombine".The workflow first loads and inverts the input scribble image.The inverted image is then used by a "control_v11p_sd15_scribble_fp16" ControlNet model to extract and applied to the scribble, controlling and guiding the image generation process.A pre-trained model ("dreamshaper_8.safetensors")processes the positive conditioning generated by the combination of the text prompt and the ControlNet's output, along with negative conditioning from undesired traits such as "horror" and "lowres."The latent space image is generated using a KSampler and further decoded by the VAE to form the final image.The model synthesizes a highly detailed image while adhering closely to both the shape of the scribble and the desired features implied by the text prompt.
→ → → → → → → → </principle> ## History
Here is a recent history of your thought, plan and action in the previous steps.The most recent record is at the bottom.</p>
<p>→ -</p>
<p>Step: 1 <thought> Since the history is empty, I need to start by loading a relevant reference workflow that can be used as a base for generating the 2-second video of colorful fish in an aquarium.The "text_to_video" reference seems appropriate to use as it involves generating video from text prompts, which aligns with the first part of the task.
→ → → </thought> <plan>
Step 1: I will load the "text_to_video" reference workflow to use it as the base.</p>
<p>Step 2: I will adapt the workflow to modify the prompt to match the requirements ("colorful fish swimming in an aquarium with bubbles rising").</p>
<p>→</p>
<p>Step 3: I will combine it with the "video_frame_interpolation" reference to enable interpolation for smoother video.</p>
<p>→</p>
<p>Step 4: I will adapt the interpolation multiplier to 3x to achieve the expected frame rate increase.</p>
<p>→</p>
<p>Step 5: I will finish the task by confirming the generated output meets the required specifications.</p>
<p>→ </plan> <action> load(name="text_to_video")</p>
<p></action> ## Workspace</p>
<p>The code and annotation of the current workflow you are working on are presented as follows: <code> model_15, clip_vision_15, vae_15 = ImageOnlyCheckpointLoader(ckpt_name="""svd_xt_1_1.safetensors""")→ model_16, clip_16, vae_16 = CheckpointLoaderSimple(ckpt_name="""sd_xl_base_1.0.safetensors""") conditioning_18 = CLIPTextEncode(text="""photograph beautiful scenery nature mountains alps river rapids snow sky cumulus clouds""", speak_and_recognation=True, clip=clip_16) → conditioning_19 = CLIPTextEncode(text="""text, watermark""", speak_and_recognation=True, clip=clip_16)</p>
<p>→ latent_22 = EmptyLatentImage(width=1024, height=576, batch_size=1) model_14 = VideoLinearCFGGuidance(min_cfg=1, model=model_15) latent_17 = KSampler(seed=307393744025667, steps=15, cfg=8, sampler_name="""uni_pc_bh2""", scheduler="""normal""", denoise=1, model=model_16, positive=conditioning_18, negative=conditioning_19, latent_image=latent_22)</p>
<p>→ → image_20 = VAEDecode(samples=latent_17, vae=vae_16) _ = PreviewImage(images=image_20) positive_12, negative_12, latent_12 = SVD_img2vid_Conditioning(width=1024, height=576, video_frames=24, motion_bucket_id=127, fps=8, augmentation_level=0, clip_vision=clip_vision_15, init_image=image_20, vae=vae_15) → → latent_3 = KSampler(seed=1103641334004632, steps=20, cfg=2.5, sampler_name="""euler""", scheduler="""karras""", denoise=1, model=model_14, positive=positive_12, negative=negative_12</p>
<p>→ → image_8 = VAEDecode(samples=latent_3, vae=vae_15) filenames_23 = VHS_VideoCombine(frame_rate=6, loop_count=0, filename_prefix="""SVD_txt2vid""", format="""video/h264-mp4""", pix_fmt="""yuv420p""", crf=19, save_metadata=True, pingpong=False, save_output=True, images=image_8) After that, you should update your step-by-step plan to further modify your workflow.There are 4 steps remaining, so your plan should contain at most 4 steps.Make sure that each step is feasible to be converted into a single action.Your plan should be enclosed with "<plan>" tag.For example: <plan>Step 1: I will refer to "reference_name" to add a module.</p>
<p>Step 2: I will finish the task since the expected effects are realized.</plan>.
→ → → →
Finally, you should choose one of the following actions and specify the arguments (if required), so that the updated workflow can realize the first step in your plan.You should provide your action with the format of function calls in Python.Your action should be enclosed with "<action>" tag.For example: <action>combine(name="reference_name")</action>, <action>adapt(prompt="Change the factor to 0.5 and rewrite the prompt.")</action>,and <action>finish()</action>.
→ → → → →
-<code>load</code>: Load a reference workflow into the workspace to replace the current workflow, so that you can start over.Arguments: → -<code>name</code>: The name of the reference workflow you want to load.-<code>combine</code>: Combine the current workflow with a reference workflow, so that necessary modules can be added.Arguments:</p>
<p>→ -<code>name</code>: The name of the reference workflow you want to combine.-<code>adapt</code>: Adapt some parameters in the current workflow, so that the expected effects can be realized.Arguments:</p>
<p>→ -<code>prompt</code>: The prompt to specify the adaptation you want to make.-<code>retrieve</code>: Retrieve a new batch of reference workflows, so that more useful references can be found.Arguments: → -<code>prompt</code>: The prompt to describe the reference you want to retrieve.-<code>finish</code>: Finish the task since the current workflow can realize the expected effects.</p>
<p>Refer to the history before making a decision.Here are some general rules you should follow:</p>
<p>1.You should choose the <code>load</code>action if and only if the history is empty.2. If you choose the <code>load</code>or <code>combine</code>action, make sure the name exists in the reference.</p>
<p>Otherwise, try to update the reference with the <code>retrieve</code>action.</p>
<p>PlanAgent Answer <thought></p>
<p>The current workflow generates a 3-second video of a landscape scene, but this is not aligned with the task of generating a 2-second video of colorful fish swimming in an aquarium with bubbles rising.I need to adapt the text prompt accordingly to match the requirements.Additionally, the frame rate is currently 8 frames per second, but after generating the video, we will need to interpolate it to increase the frame rate by 3x to make the motion smoother.
→ → → → →
The basic pipeline for generating videos from a text prompt is already functional, but the correct text needs to be provided, and the next step would be to apply video frame interpolation.I will combine this workflow with the "video_frame_interpolation" to achieve smoother motion.
→ → → </thought> <plan>
Step 1: Adapt the text prompt to describe the required scene of "colorful fish swimming in an aquarium with bubbles rising".</p>
<p>→</p>
<p>Step 2: Combine the current workflow with the "video_frame_interpolation" reference workflow to add video interpolation functionality.</p>
<p>→</p>
<p>Step 3: Adapt the interpolation multiplier to 3x for smoother motion.</p>
<p>Step 4: Finish the task after confirming that the workflow meets the requirements.</plan> <action> adapt(prompt="Change the text prompt to 'colorful fish swimming in an aquarium with bubbles rising to the surface' and reduce the video duration to 2 seconds.")
→ </action> AdaptAgent Prompt ## Task
ComfyUI uses workflows to create and execute Stable Diffusion pipelines so that users can design their own workflows to generate highly customized artworks.ComfyUI workflows can be formulated into the equivalent Python code, where each statement represents the execution of a single node.You are an expert in ComfyUI, helping users to design workflows according to their requirements.First generate a 2-second video of colorful fish swimming in an aquarium with bubbles rising to the surface.Then interpolate the video to increase the frame rate by 3x.The result should be a smoother video of the swimming fish.</p>
<p>→ →</p>
<p>The core requirements and the expected paradigm are analyzed as follows:</p>
<p>The task requires generating a short 2-second video of fish swimming in an aquarium with bubbles rising using a text-to-video approach, as the primary input is textual.The video needs to depict dynamic, colorful fish movements with the additional visual element of rising bubbles.After generating this video, the frame rate needs to be increased by 3x using a video interpolation process to achieve smoother motion.Thus, the workflow core involves two stages, first, synthesizing the video from a text prompt using a text-to-video node or series of nodes, followed by applying video interpolation to enhance frame smoothness.There are no specific quality-oriented steps like upscaling or refinement beyond the interpolation requirement.
→ → → → → → → → ## Workspace
The code and annotation of the current workflow you are working on are presented as follows: <code> model_15, clip_vision_15, vae_15 = ImageOnlyCheckpointLoader(ckpt_name="""svd_xt_1_1.safetensors""")→ model_16, clip_16, vae_16 = CheckpointLoaderSimple(ckpt_name="""sd_xl_base_1.0.safetensors""") conditioning_18 = CLIPTextEncode(text="""photograph beautiful scenery nature mountains alps river rapids snow sky cumulus clouds""", speak_and_recognation=True, clip=clip_16) → conditioning_19 = CLIPTextEncode(text="""text, watermark""", speak_and_recognation=True, clip=clip_16) → latent_22 = EmptyLatentImage(width=1024, height=576, batch_size=1) model_14 = VideoLinearCFGGuidance(min_cfg=1, model=model_15) latent_17 = KSampler(seed=307393744025667, steps=15, cfg=8, sampler_name="""uni_pc_bh2""", scheduler="""normal""", denoise=1, model=model_16, positive=conditioning_18, negative=conditioning_19, latent_image=latent_22)
→ → image_20 = VAEDecode(samples=latent_17, vae=vae_16) _ = PreviewImage(images=image_20)
positive_12, negative_12, latent_12 = SVD_img2vid_Conditioning(width=1024, height=576, video_frames=24, motion_bucket_id=127, fps=8, augmentation_level=0, clip_vision=clip_vision_15, init_image=image_20, vae=vae_15) → → latent_3 = KSampler(seed=1103641334004632, steps=20, cfg=2.5, sampler_name="""euler""", scheduler="""karras""", denoise=1, model=model_14, positive=positive_12, negative=negative_12, latent_image=latent_12) → → image_8 = VAEDecode(samples=latent_3, vae=vae_15) filenames_23 = VHS_VideoCombine(frame_rate=6, loop_count=0, filename_prefix="""SVD_txt2vid""", format="""video/h264-mp4""", pix_fmt="""yuv420p""", crf=19, save_metadata=True, pingpong=False, save_output=True, images=image_8) The final video is created by sampling the latent space over multiple frames and combining them into an MP4 video using the specified frame rate and format.
→ → → → → → → </principle> ## Adaptation
Based on the current working progress, your schedule is presented as follows:</p>
<p>Step 1: Adapt the text prompt to describe the required scene of "colorful fish swimming in an aquarium with bubbles rising".</p>
<p>→</p>
<p>Step 2: Combine the current workflow with the "video_frame_interpolation" reference workflow to add video interpolation functionality.</p>
<p>→</p>
<p>Step 3: Adapt the interpolation multiplier to 3x for smoother motion.</p>
<p>Step 4: Finish the task after confirming that the workflow meets the requirements.</p>
<p>You are working on the first step of your schedule.In other words, you should modify the parameters in the current workflow according to your schedule.The adaptation you want to make is specified as follows:
→ →
Change the text prompt to 'colorful fish swimming in an aquarium with bubbles rising to the surface' and reduce the video duration to 2 seconds.</p>
<p>→</p>
<p>First, you should provide your Python code to formulate the updated workflow.Each line of code should correspond to a single node, so you should avoid nested calls in a single statement.You should also avoid reusing the same variable name, even if the variable is temporary.Your code should be enclosed with "<code>" tag.For example: <code>output = node(input)</code>.
→ → → →
After that, you should provide an annotation as in the reference, including the function and principle of the updated workflow.The function should be enclosed with "<function>" tag.For example: <function>This workflow generates a high-resolution image of a running horse.</function>.The principle should be enclosed with "<principle>" tag.For example: <principle>The workflow first generates a low-resolution image using the text-to-image pipeline and then applies an upscaling module to improve the resolution.</principle>.
→ → → → →
Now, provide your code and annotation with the required format.</p>
<p>AdaptAgent Answer <code> model_15, clip_vision_15, vae_15 = ImageOnlyCheckpointLoader(ckpt_name="""svd_xt_1_1.safetensors""")→ model_16, clip_16, vae_16 = CheckpointLoaderSimple(ckpt_name="""sd_xl_base_1.0.safetensors""") conditioning_18 = CLIPTextEncode(text="""colorful fish swimming in an aquarium with bubbles rising to the surface""", speak_and_recognation=True, clip=clip_16) → conditioning_19 = CLIPTextEncode(text="""text, watermark""", speak_and_recognation=True, clip=clip_16) → latent_22 = EmptyLatentImage(width=1024, height=576, batch_size=1) model_14 = VideoLinearCFGGuidance(min_cfg=1, model=model_15) latent_17 = KSampler(seed=307393744025667, steps=15, cfg=8, sampler_name="""uni_pc_bh2""", scheduler="""normal""", denoise=1, model=model_16, positive=conditioning_18, negative=conditioning_19, latent_image=latent_22)
→ → image_20 = VAEDecode(samples=latent_17, vae=vae_16) _ = PreviewImage(images=image_20)
positive_12, negative_12, latent_12 = SVD_img2vid_Conditioning(width=1024, height=576, video_frames=16, motion_bucket_id=127, fps=8, augmentation_level=0, clip_vision=clip_vision_15, init_image=image_20, vae=vae_15) → → latent_3 = KSampler(seed=1103641334004632, steps=20, cfg=2.5, sampler_name="""euler""", scheduler="""karras""", denoise=1, model=model_14, positive=positive_12, negative=negative_12, latent_image=latent_12) → → image_8 = VAEDecode(samples=latent_3, vae=vae_15) filenames_23 = VHS_VideoCombine(frame_rate=8, loop_count=0, filename_prefix="""SVD_txt2vid""", format="""video/h264-mp4""", pix_fmt="""yuv420p""", crf=19, save_metadata=True, pingpong=False, save_output=True, images=image_8)
→ → </code> <function>
This workflow generates a 2-second video of colorful fish swimming in an aquarium with bubbles rising to the surface from a given text description.The video is first synthesized using a text-to-video diffusion process.</p>
<p>→ →</p>
<p></function> <principle></p>
<p>The workflow first loads two models: a Stable Diffusion model ("sd_xl_base_1.0.safetensors") for generating the initial image from the text description, and then a Stable Video Diffusion model ("svd_xt_1_1.safetensors")to translate that image into a dynamic video over multiple frames.Following the text input describing colorful fish and bubbles, the image is generated and expanded across 16 frames, which corresponds to a duration of 2 seconds at 8 FPS (frames per second).The final MP4 video is constructed and saved along with metadata.
→ → → → → → </principle> PlanAgent Prompt ## Task
ComfyUI uses workflows to create and execute Stable Diffusion pipelines so that users can design their own workflows to generate highly customized artworks.ComfyUI workflows can be formulated into the equivalent Python code, where each statement represents the execution of a single node.You are an expert in ComfyUI, helping users to design workflows according to their requirements.
→ → → →
Now you are required to create a ComfyUI workflow to finish the following task:</p>
<p>First generate a 2-second video of colorful fish swimming in an aquarium with bubbles rising to the surface.Then interpolate the video to increase the frame rate by 3x.The result should be a smoother video of the swimming fish.</p>
<p>→ →</p>
<p>The core requirements and the expected paradigm are analyzed as follows:</p>
<p>The task requires generating a short 2-second video of fish swimming in an aquarium with bubbles rising using a text-to-video approach, as the primary input is textual.The video needs to depict dynamic, colorful fish movements with the additional visual element of rising bubbles.After generating this video, the frame rate needs to be increased by 3x using a video interpolation process to achieve smoother motion.Thus, the workflow core involves two stages, first, synthesizing the video from a text prompt using a text-to-video node or series of nodes, followed by applying video interpolation to enhance frame smoothness.There are no specific quality-oriented steps like upscaling or refinement beyond the interpolation requirement.
→ → → → → → → →
Improving the quality of the generation result with additional steps, such as upscaling and interpolation, is not recommended, unless specified in the requirements.</p>
<p>→ ## Reference</p>
<p>According to the requirements, we have retrieved some relevant workflows which may be helpful:</p>
<p>-Example: text_to_video <function> This workflow follows a text-to-video paradigm, where it generates a video from a given text prompt.It first creates an initial image based on the text description and then uses that image as the starting frame to generate a video with motion and transformation using a Stable Video Diffusion model.The output is a 3-second video at 8 frames per second, depicting beautiful scenery with mountains, rivers, and clouds.The workflow first loads the input video using "VHS_LoadVideo", which extracts the individual frames.The "RIFE VFI" node is then used to interpolate the frames by generating additional frames between the existing ones.In this scenario, the multiplier is set to 3x, effectively tripling the frame count and enabling a smoother video playback at 24 frames per second.Finally, the interpolated frames are combined into a video or GIF format using "VHS_VideoCombine".
→ → → → → </principle> -Example: image_to_video → → </function> → → → → → → </principle> -Example: text_to_image <function>
This workflow implements a basic text-to-image generation pipeline using Stable Diffusion.It requires both positive (desired traits) and negative (undesired traits) text prompts to generate an image.In this specific case, the workflow will output a high-resolution photo of a cat wearing a spacesuit inside a spaceship, avoiding blurry or illustration-like effects.
→ → → → </function> <principle>
The workflow begins by loading the "dreamshaper_8.safetensors"Stable Diffusion model.It generates a blank latent space as the starting point for the image generation.The positive prompt ("a photo of a cat wearing a spacesuit inside a spaceship") and negative prompt ("blurry, illustration") are encoded into conditioning by the CLIPTextEncode node.The KSampler node then uses these conditionings to guide the generative process over 20 sampling steps, applying denoising to the latent space.The resulting latent code is subsequently decoded into an image using the VAE and saved to disk.
→ → → → → → </principle> -Example: scribble_image_repaint → → </function> <principle>
The workflow first loads and inverts the input scribble image.The inverted image is then used by a "control_v11p_sd15_scribble_fp16" ControlNet model to extract and applied to the scribble, controlling and guiding the image generation process.A pre-trained model ("dreamshaper_8.safetensors")processes the positive conditioning generated by the combination of the text prompt and the ControlNet's output, along with negative conditioning from undesired traits such as "horror" and "lowres."The latent space image is generated using a KSampler and further decoded by the VAE to form the final image.The model synthesizes a highly detailed image while adhering closely to both the shape of the scribble and the desired features implied by the text prompt.
→ → → → → → → → </principle> ## History
Here is a recent history of your thought, plan and action in the previous steps.The most recent record is at the bottom.</p>
<p>→ -</p>
<p>Step: 1 <thought> Since the history is empty, I need to start by loading a relevant reference workflow that can be used as a base for generating the 2-second video of colorful fish in an aquarium.The "text_to_video" reference seems appropriate to use as it involves generating video from text prompts, which aligns with the first part of the task.
→ → → </thought> <plan>
Step 1: I will load the "text_to_video" reference workflow to use it as the base.</p>
<p>Step 2: I will adapt the workflow to modify the prompt to match the requirements ("colorful fish swimming in an aquarium with bubbles rising").</p>
<p>→</p>
<p>Step 3: I will combine it with the "video_frame_interpolation" reference to enable interpolation for smoother video.</p>
<p>→</p>
<p>Step 4: I will adapt the interpolation multiplier to 3x to achieve the expected frame rate increase.</p>
<p>→</p>
<p>Step 5: I will finish the task by confirming the generated output meets the required specifications.The current workflow generates a 3-second video of a landscape scene, but this is not aligned with the task of generating a 2-second video of colorful fish swimming in an aquarium with bubbles rising.I need to adapt the text prompt accordingly to match the requirements.Additionally, the frame rate is currently 8 frames per second, but after generating the video, we will need to interpolate it to increase the frame rate by 3x to make the motion smoother.
→ → → → →
The basic pipeline for generating videos from a text prompt is already functional, but the correct text needs to be provided, and the next step would be to apply video frame interpolation.I will combine this workflow with the "video_frame_interpolation" to achieve smoother motion.
→ → → </thought> <plan>
Step 1: Adapt the text prompt to describe the required scene of "colorful fish swimming in an aquarium with bubbles rising".</p>
<p>→</p>
<p>Step 2: Combine the current workflow with the "video_frame_interpolation" reference workflow to add video interpolation functionality.</p>
<p>→</p>
<p>Step 3: Adapt the interpolation multiplier to 3x for smoother motion.</p>
<p>Step 4: Finish the task after confirming that the workflow meets the requirements.</plan> <action> adapt(prompt="Change the text prompt to 'colorful fish swimming in an aquarium with bubbles rising to the surface' and reduce the video duration to 2 seconds.")
→ </action> ## Workspace
The code and annotation of the current workflow you are working on are presented as follows: <code> model_15, clip_vision_15, vae_15 = ImageOnlyCheckpointLoader(ckpt_name="""svd_xt_1_1.safetensors""")→ model_16, clip_16, vae_16 = CheckpointLoaderSimple(ckpt_name="""sd_xl_base_1.0.safetensors""") conditioning_18 = CLIPTextEncode(text="""colorful fish swimming in an aquarium with bubbles rising to the surface""", speak_and_recognation=True, clip=clip_16) → conditioning_19 = CLIPTextEncode(text="""text, watermark""", speak_and_recognation=True,</p>
<p>→ latent_22 = EmptyLatentImage(width=1024, height=576, batch_size=1) model_14 = VideoLinearCFGGuidance(min_cfg=1, model=model_15) latent_17 = KSampler(seed=307393744025667, steps=15, cfg=8, sampler_name="""uni_pc_bh2""", scheduler="""normal""", denoise=1, model=model_16, positive=conditioning_18, negative=conditioning_19, latent_image=latent_22)
→ → image_20 = VAEDecode(samples=latent_17, vae=vae_16) _ = PreviewImage(images=image_20) positive_12, negative_12, latent_12 = SVD_img2vid_Conditioning(width=1024, height=576, video_frames=16, motion_bucket_id=127, fps=8, augmentation_level=0, clip_vision=clip_vision_15, init_image=image_20, vae=vae_15)
→ → latent_3 = KSampler(seed=1103641334004632, steps=20, cfg=2.5, sampler_name="""euler""", scheduler="""karras""", denoise=1, model=model_14, positive=positive_12, negative=negative_12, latent_image=latent_12) → → image_8 = VAEDecode(samples=latent_3, vae=vae_15) filenames_23 = VHS_VideoCombine(frame_rate=8, loop_count=0, filename_prefix="""SVD_txt2vid""", format="""video/h264-mp4""", pix_fmt="""yuv420p""", crf=19, save_metadata=True, pingpong=False, save_output=True, images=image_8)
→ → </code> <function>
This workflow generates a 2-second video of colorful fish swimming in an aquarium with bubbles rising to the surface from a given text description.The video is first synthesized using a text-to-video diffusion process.
→ → </function> <principle>
The workflow first loads two models: a Stable Diffusion model ("sd_xl_base_1.0.safetensors") for generating the initial image from the text description, and then a Stable Video Diffusion model ("svd_xt_1_1.safetensors")to translate that image into a dynamic video over multiple frames.Following the text input describing colorful fish and bubbles, the image is generated and expanded across 16 frames, which corresponds to a duration of 2 seconds at 8 FPS (frames per second).The final MP4 video is constructed and saved along with metadata.
→ → → → → → </principle> ## Action
Based on the history and workspace, you should first think about what functions have been implemented and what modules remain to be added.Your thought should be enclosed with "<thought>" tag.For example: <thought>The basic pipeline has been implemented, but a module is needed to improve the quality.</thought>.
→ → →
After that, you should update your step-by-step plan to further modify your workflow.There are 3 steps remaining, so your plan should contain at most 3 steps.Make sure that each step is feasible to be converted into a single action.Your plan should be enclosed with "<plan>" tag.For example: <plan>Step 1: I will refer to "reference_name" to add a module.</p>
<p>Step 2: I will finish the task since the expected effects are realized.</plan>.
→ → → →
Finally, you should choose one of the following actions and specify the arguments (if required), so that the updated workflow can realize the first step in your plan.You should provide your action with the format of function calls in Python.Your action should be enclosed with "<action>" tag.For example: <action>combine(name="reference_name")</action>, <action>adapt(prompt="Change the factor to 0.5 and rewrite the prompt.")</action>,and <action>finish()</action>. -<code>combine</code>: Combine the current workflow with a reference workflow, so that necessary modules can be added.Arguments: → -<code>name</code>: The name of the reference workflow you want to combine.-<code>adapt</code>: Adapt some parameters in the current workflow, so that the expected effects can be realized.Arguments: → -<code>prompt</code>: The prompt to specify the adaptation you want to make.-<code>retrieve</code>: Retrieve a new batch of reference workflows, so that more useful references can be found.Arguments: → -<code>prompt</code>: The prompt to describe the reference you want to retrieve.-<code>finish</code>: Finish the task since the current workflow can realize the expected effects.</p>
<p>Refer to the history before making a decision.Here are some general rules you should follow:</p>
<p>1.You should choose the <code>load</code>action if and only if the history is empty.2. If you choose the <code>load</code>or <code>combine</code>action, make sure the name exists in the reference.</p>
<p>Otherwise, try to update the reference with the <code>retrieve</code>action.</p>
<p>PlanAgent Answer <thought></p>
<p>The video generation part is complete, wherein a 2-second video of colorful fish swimming in an aquarium with bubbles rising is generated at 8 FPS.The next part of the task is to increase the frame rate by 3x using frame interpolation to achieve smoother motion.I need to combine the current workflow with the "video_frame_interpolation" reference to add the interpolation capability and then adjust the frame rate multiplier to 3x as per the requirements.
→ → → → → </thought> <plan>
Step 1: Combine the current workflow with the "video_frame_interpolation" reference to add the interpolation functionality.</p>
<p>→</p>
<p>Step 2: Adapt the multiplier for frame interpolation to 3x to ensure the frame rate increase.</p>
<p>Step 3: Finish the task since this will meet the stated requirements.</plan> <action>combine(name="video_frame_interpolation")</action></p>
<p>CombineAgent Prompt</p>
<h2>Task</h2>
<p>ComfyUI uses workflows to create and execute Stable Diffusion pipelines so that users can design their own workflows to generate highly customized artworks.ComfyUI workflows can be formulated into the equivalent Python code, where each statement represents the execution of a single node.You are an expert in ComfyUI, helping users to design workflows according to their requirements.First generate a 2-second video of colorful fish swimming in an aquarium with bubbles rising to the surface.Then interpolate the video to increase the frame rate by 3x.The result should be a smoother video of the swimming fish.</p>
<p>→ →</p>
<p>The core requirements and the expected paradigm are analyzed as follows:</p>
<p>The task requires generating a short 2-second video of fish swimming in an aquarium with bubbles rising using a text-to-video approach, as the primary input is textual.The video needs to depict dynamic, colorful fish movements with the additional visual element of rising bubbles.After generating this video, the frame rate needs to be increased by 3x using a video interpolation process to achieve smoother motion.Thus, the workflow core involves two stages, first, synthesizing the video from a text prompt using a text-to-video node or series of nodes, followed by applying video interpolation to enhance frame smoothness.There are no specific quality-oriented steps like upscaling or refinement beyond the interpolation requirement.
→ → → → → → → → ## Reference
The code and annotation of the current workflow you are referring to are presented as follows: <code> image_7, frame_count_7, audio_7, video_info_7, latent_7 = VHS_LoadVideo(custom_height=512, custom_width=512, force_rate=0, force_size="""Disabled""", frame_load_cap=0, select_every_nth=1, skip_first_frames=0, video="""play_guitar.gif""") → → image_10 = RIFE_VFI(ckpt_name="""rife47.pth""",clear_cache_after_n_frames=10, ensemble=True, fast_mode=True, frames=image_7, multiplier=3, scale_factor=1) → filenames_3 = VHS_VideoCombine(filename_prefix="""AnimateDiff""", format="""image/gif""", frame_rate=24, images=image_10, loop_count=0, pingpong=False, save_output=True) The workflow first loads the input video using "VHS_LoadVideo", which extracts the individual frames.The "RIFE VFI" node is then used to interpolate the frames by generating additional frames between the existing ones.In this scenario, the multiplier is set to 3x, effectively tripling the frame count and enabling a smoother video playback at 24 frames per second.Finally, the interpolated frames are combined into a video or GIF format using "VHS_VideoCombine".
→ → → → → </principle> ## Workspace
The code and annotation of the current workflow you are working on are presented as follows: <code> model_15, clip_vision_15, vae_15 = ImageOnlyCheckpointLoader(ckpt_name="""svd_xt_1_1.safetensors""")→ model_16, clip_16, vae_16 = CheckpointLoaderSimple(ckpt_name="""sd_xl_base_1.0.safetensors""") conditioning_18 = CLIPTextEncode(text="""colorful fish swimming in an aquarium with bubbles rising to the surface""", speak_and_recognation=True, clip=clip_16) → conditioning_19 = CLIPTextEncode(text="""text, watermark""", speak_and_recognation=True,</p>
<p>→ latent_22 = EmptyLatentImage(width=1024, height=576, batch_size=1) model_14 = VideoLinearCFGGuidance(min_cfg=1, model=model_15) latent_17 = KSampler(seed=307393744025667, steps=15, cfg=8, sampler_name="""uni_pc_bh2""", scheduler="""normal""", denoise=1,</p>
<p>→ → image_20 = VAEDecode(samples=latent_17, vae=vae_16) _ = PreviewImage(images=image_20) positive_12, negative_12, latent_12 = SVD_img2vid_Conditioning(width=1024, height=576, video_frames=16, motion_bucket_id=127, fps=8, augmentation_level=0, clip_vision=clip_vision_15, init_image=image_20, vae=vae_15) → → latent_3 = KSampler(seed=1103641334004632, steps=20, cfg=2.5, sampler_name="""euler""", scheduler="""karras""", denoise=1, model=model_14, positive=positive_12, negative=negative_12, latent_image=latent_12) → → image_8 = VAEDecode(samples=latent_3, vae=vae_15) filenames_23 = VHS_VideoCombine(frame_rate=8, loop_count=0, filename_prefix="""SVD_txt2vid""", format="""video/h264-mp4""", pix_fmt="""yuv420p""", crf=19, save_metadata=True, pingpong=False, save_output=True, images=image_8)
→ → </code> <function>
This workflow generates a 2-second video of colorful fish swimming in an aquarium with bubbles rising to the surface from a given text description.The video is first synthesized using a text-to-video diffusion process.Step 1: Combine the current workflow with the "video_frame_interpolation" reference to add the interpolation functionality.</p>
<p>→</p>
<p>Step 2: Adapt the multiplier for frame interpolation to 3x to ensure the frame rate increase.</p>
<p>Step 3: Finish the task since this will meet the stated requirements.</p>
<p>You are working on the first step of your schedule.In other words, you should combine the reference workflow with the current workflow according to your schedule.</p>
<p>→</p>
<p>First, you should provide your Python code to formulate the updated workflow.Each line of code should correspond to a single node, so you should avoid nested calls in a single statement.You should also avoid reusing the same variable name, even if the variable is temporary.Your code should be enclosed with "<code>" tag.For example: <code>output = node(input)</code>.
→ → → →
After that, you should provide an annotation as in the reference, including the function and principle of the updated workflow.The function should be enclosed with "<function>" tag.For example: <function>This workflow generates a high-resolution image of a running horse.</function>.The principle should be enclosed with "<principle>" tag.For example: <principle>The workflow first generates a low-resolution image using the text-to-image pipeline and then applies an upscaling module to improve the resolution.</principle>.CombineAgent Answer <code> model_15, clip_vision_15, vae_15 = ImageOnlyCheckpointLoader(ckpt_name="""svd_xt_1_1.safetensors""")→ model_16, clip_16, vae_16 = CheckpointLoaderSimple(ckpt_name="""sd_xl_base_1.0.safetensors""") conditioning_18 = CLIPTextEncode(text="""colorful fish swimming in an aquarium with bubbles rising to the surface""", speak_and_recognation=True, clip=clip_16) → conditioning_19 = CLIPTextEncode(text="""text, watermark""", speak_and_recognation=True, clip=clip_16) → latent_22 = EmptyLatentImage(width=1024, height=576, batch_size=1) model_14 = VideoLinearCFGGuidance(min_cfg=1, model=model_15) latent_17 = KSampler(seed=307393744025667, steps=15, cfg=8, sampler_name="""uni_pc_bh2""", scheduler="""normal""", denoise=1, model=model_16, positive=conditioning_18, negative=conditioning_19, latent_image=latent_22) → → image_20 = VAEDecode(samples=latent_17, vae=vae_16) _ = PreviewImage(images=image_20) positive_12, negative_12, latent_12 = SVD_img2vid_Conditioning(width=1024, height=576, video_frames=16, motion_bucket_id=127, fps=8, augmentation_level=0, clip_vision=clip_vision_15, init_image=image_20, vae=vae_15) → → latent_3 = KSampler(seed=1103641334004632, steps=20, cfg=2.5, sampler_name="""euler""", scheduler="""karras""", denoise=1, model=model_14, positive=positive_12, negative=negative_12, latent_image=latent_12) → → image_8 = VAEDecode(samples=latent_3, vae=vae_15) filenames_23 = VHS_VideoCombine(frame_rate=8, loop_count=0, filename_prefix="""SVD_txt2vid""", format="""video/h264-mp4""", pix_fmt="""yuv420p""", crf=19, save_metadata=True, pingpong=False, save_output=True, images=image_8) → → image_10 = RIFE_VFI(ckpt_name="""rife47.pth""",clear_cache_after_n_frames=10, ensemble=True, fast_mode=True, frames=image_8, multiplier=3, scale_factor=1) → filenames_24 = VHS_VideoCombine(frame_rate=24, loop_count=0, filename_prefix="""SVD_txt2vid_Interpolated""", format="""video/h264-mp4""", pix_fmt="""yuv420p""", crf=19, save_metadata=True, pingpong=False, save_output=True, images=image_10) First generate a 2-second video of colorful fish swimming in an aquarium with bubbles rising to the surface.Then interpolate the video to increase the frame rate by 3x.The result should be a smoother video of the swimming fish.</p>
<p>→ →</p>
<p>The core requirements and the expected paradigm are analyzed as follows:</p>
<p>The task requires generating a short 2-second video of fish swimming in an aquarium with bubbles rising using a text-to-video approach, as the primary input is textual.The video needs to depict dynamic, colorful fish movements with the additional visual element of rising bubbles.After generating this video, the frame rate needs to be increased by 3x using a video interpolation process to achieve smoother motion.Thus, the workflow core involves two stages, first, synthesizing the video from a text prompt using a text-to-video node or series of nodes, followed by applying video interpolation to enhance frame smoothness.There are no specific quality-oriented steps like upscaling or refinement beyond the interpolation requirement.
→ → → → → → → →
Improving the quality of the generation result with additional steps, such as upscaling and interpolation, is not recommended, unless specified in the requirements.</p>
<p>→ ## Reference</p>
<p>According to the requirements, we have retrieved some relevant workflows which may be helpful:</p>
<ul>
<li>Step 1: I will load the "text_to_video" reference workflow to use it as the base.</li>
</ul>
<p>Step 2: I will adapt the workflow to modify the prompt to match the requirements ("colorful fish swimming in an aquarium with bubbles rising").</p>
<p>→</p>
<p>Step 3: I will combine it with the "video_frame_interpolation" reference to enable interpolation for smoother video.</p>
<p>→</p>
<p>Step 4: I will adapt the interpolation multiplier to 3x to achieve the expected frame rate increase.</p>
<p>→</p>
<p>Step 5: I will finish the task by confirming the generated output meets the required specifications.The current workflow generates a 3-second video of a landscape scene, but this is not aligned with the task of generating a 2-second video of colorful fish swimming in an aquarium with bubbles rising.I need to adapt the text prompt accordingly to match the requirements.Additionally, the frame rate is currently 8 frames per second, but after generating the video, we will need to interpolate it to increase the frame rate by 3x to make the motion smoother.The basic pipeline for generating videos from a text prompt is already functional, but the correct text needs to be provided, and the next step would be to apply video frame interpolation.I will combine this workflow with the "video_frame_interpolation" to achieve smoother motion.
→ → → </thought> <plan>
Step 1: Adapt the text prompt to describe the required scene of "colorful fish swimming in an aquarium with bubbles rising".</p>
<p>→</p>
<p>Step 2: Combine the current workflow with the "video_frame_interpolation" reference workflow to add video interpolation functionality.</p>
<p>→</p>
<p>Step 3: Adapt the interpolation multiplier to 3x for smoother motion.</p>
<p>Step 4: Finish the task after confirming that the workflow meets the requirements.</plan> <action> adapt(prompt="Change the text prompt to 'colorful fish swimming in an aquarium with bubbles rising to the surface' and reduce the video duration to 2 seconds.")
→ </action> -Step: 3 <thought>
The video generation part is complete, wherein a 2-second video of colorful fish swimming in an aquarium with bubbles rising is generated at 8 FPS.The next part of the task is to increase the frame rate by 3x using frame interpolation to achieve smoother motion.I need to combine the current workflow with the "video_frame_interpolation" reference to add the interpolation capability and then adjust the frame rate multiplier to 3x as per the requirements.
→ → → → → </thought> <plan>
Step 1: Combine the current workflow with the "video_frame_interpolation" reference to add the interpolation functionality.</p>
<p>→</p>
<p>Step 2: Adapt the multiplier for frame interpolation to 3x to ensure the frame rate increase.</p>
<p>Step 3: Finish the task since this will meet the stated requirements.</plan> <action> combine(name="video_frame_interpolation") </action> ## Workspace</p>
<p>The code and annotation of the current workflow you are working on are presented as follows: <code> model_15, clip_vision_15, vae_15 = ImageOnlyCheckpointLoader(ckpt_name="""svd_xt_1_1.safetensors""")→ model_16, clip_16, vae_16 = CheckpointLoaderSimple(ckpt_name="""sd_xl_base_1.0.safetensors""") conditioning_18 = CLIPTextEncode(text="""colorful fish swimming in an aquarium with bubbles rising to the surface""", speak_and_recognation=True, clip=clip_16) → conditioning_19 = CLIPTextEncode(text="""text, watermark""", speak_and_recognation=True, clip=clip_16) → latent_22 = EmptyLatentImage(width=1024, height=576, batch_size=1) model_14 = VideoLinearCFGGuidance(min_cfg=1, model=model_15) latent_17 = KSampler(seed=307393744025667, steps=15, cfg=8, sampler_name="""uni_pc_bh2""", scheduler="""normal""", denoise=1, model=model_16, positive=conditioning_18, negative=conditioning_19, latent_image=latent_22) → → image_20 = VAEDecode(samples=latent_17, vae=vae_16) _ = PreviewImage(images=image_20) positive_12, negative_12, latent_12 = SVD_img2vid_Conditioning(width=1024, height=576, video_frames=16, motion_bucket_id=127, fps=8, augmentation_level=0, clip_vision=clip_vision_15, init_image=image_20, vae=vae_15) → → latent_3 = KSampler(seed=1103641334004632, steps=20, cfg=2.5, sampler_name="""euler""", scheduler="""karras""", denoise=1, model=model_14, positive=positive_12, negative=negative_12, latent_image=latent_12) → → image_8 = VAEDecode(samples=latent_3, vae=vae_15) filenames_23 = VHS_VideoCombine(frame_rate=8, loop_count=0, filename_prefix="""SVD_txt2vid""", format="""video/h264-mp4""", pix_fmt="""yuv420p""", crf=19, save_metadata=True, pingpong=False, save_output=True, images=image_8) → → image_10 = RIFE_VFI(ckpt_name="""rife47.pth""",clear_cache_after_n_frames=10, ensemble=True, fast_mode=True, frames=image_8, multiplier=3, scale_factor=1) → filenames_24 = VHS_VideoCombine(frame_rate=24, loop_count=0, filename_prefix="""SVD_txt2vid_Interpolated""", format="""video/h264-mp4""", pix_fmt="""yuv420p""", crf=19, save_metadata=True, pingpong=False, save_output=True, images=image_10)  After that, you should update your step-by-step plan to further modify your workflow.There are 2 steps remaining, so your plan should contain at most 2 steps.Make sure that each step is feasible to be converted into a single action.Your plan should be enclosed with "<plan>" tag.For example: <plan>Step 1: I will refer to "reference_name" to add a module.</p>
<p>Step 2: I will finish the task since the expected effects are realized.</plan>.
→ → → →
Finally, you should choose one of the following actions and specify the arguments (if required), so that the updated workflow can realize the first step in your plan.You should provide your action with the format of function calls in Python.Your action should be enclosed with "<action>" tag.For example: <action>combine(name="reference_name")</action>, <action>adapt(prompt="Change the factor to 0.5 and rewrite the prompt.")</action>,and <action>finish()</action>. -<code>adapt</code>: Adapt some parameters in the current workflow, so that the expected effects can be realized.Arguments: → -<code>prompt</code>: The prompt to specify the adaptation you want to make.-<code>retrieve</code>: Retrieve a new batch of reference workflows, so that more useful references can be found.Arguments: → -<code>prompt</code>: The prompt to describe the reference you want to retrieve.</p>
<p>"Figure 1 .
1
Figure 1.(a) ComfyBench is a comprehensive benchmark to evaluate agents's ability to design collaborative AI systems in ComfyUI.Given the task instruction, agents are required to learn from documents and create workflows to describe collaborative AI systems.The performance is measured by pass rate and resolve rate, reflecting whether the workflow can be correctly executed and whether the task requirements are realized.(b) ComfyAgent builds collaborative Al systems in ComfyUI by generating workflows.The workflows are converted into equivalent code so that LLMs can better understand them.ComfyAgent can learn from existing workflows and autonomously design new ones.The generated workflows can be interpreted as collaborative AI systems to complete given tasks.</p>
<p>Figure 2 .
2
Figure 2. ComfyBench provides annotations for 3205 nodes and 20 workflows, together with 200 task instructions categorized into three difficulty levels: vanilla, complex, and creative.</p>
<p>Figure 4 .
4
Figure 4. Examples of four common formats to represent workflows: flow graph, JSON, element list, and code.</p>
<p>Section A presents more examples of task instructions which are included in ComfyBench.• Section B summarizes the detailed implementation of GPT-4o evaluaiton for resolve rate.• Section C demonstrates the details of human evaluation, certifying the stability of our VLM evaluation system.• Section D showcases the details of prompt implementation in ComfyAgent framework.• Section E provides more examples generated by Com-fyAgent on ComfyBench, as well as a typical example trajectory of ComfyAgent.</p>
<p>Figure 5 .
5
Figure 5.A sample question selected from the created questionnaires on Google Forms in the human evaluation.</p>
<p>Figure 6 .
6
Figure 6.Correlation heatmap of the scores given by human evaluators and GPT-4o in the human evaluation.</p>
<p>Generate an image of a modern city skyline at night with illuminated skyscrapers.The result should be a high-quality image.N/A You are given an image of a scribble flower.Repaint the scribble into a realistic red flower.The result should be an image of a red flower.You are given an image of a red apple.Change it into a green apple on a table while maintaining other details.The result should be an image of a green apple.You are given an image of a sample logo containing a bird pattern.Convert it into a cubist art poster with dark colors.The result should be an image of a poster without watermark.You are given an image of a large castle standing on top of a hill.Convert the castle into the style of ice cream while maintaining its original structure.The result should be an image with the castle transformed into a colorful and fantastic ice cream castle.You are given a low-resolution photo of a crowd of people.Upscale the image by 4x.The result should be a high-resolution version of the image.</p>
<p>Figure 7 .→
7
Figure 7.A typical example for VLM-based evaluation in ComfyBench.</p>
<p>-</p>
<p><code>load</code>: Load a reference workflow into the workspace to replace the current workflow, so that you can start over.Arguments: → -<code>name</code>: The name of the reference workflow you want to load.</p>
<p>→ 3 .
3
You should not choose the <code>adapt</code>action twice in a row, because they can be simplified into a single action.</p>
<p>→ 4 .→ 5 .
45
If you choose the <code>adapt</code>or <code>retrieve</code>action, make sure the prompt is concise and contains all the necessary information.You should choose the <code>finish</code>action before the remaining steps count down to 0. Now, provide your thought, plan and action with the required format.</p>
<p>Figure 8 .→
8
Figure 8. Prompt template for PlanAgent.</p>
<p>your code and annotation with the required format.</p>
<p>Figure 9 .→
9
Figure 9. Prompt template for CombineAgent.</p>
<p>your code and annotation with the required format.</p>
<p>Figure 10 .→
10
Figure 10.Prompt template for AdaptAgent.</p>
<p>your explanation, code, and annotation with the required format.</p>
<p>Figure 11 .
11
Figure 11.Prompt template for RefineAgent.</p>
<p>→</p>
<p>to create and execute Stable Diffusion pipelines so that users can design their own workflows to generate highly customized artworks.ComfyUI workflows can be formulated into the equivalent Python code, where each statement represents the execution of a single node.You are an expert in ComfyUI, helping users to design workflows according to their requirements.Now you are required to create a ComfyUI workflow to finish the following task:</p>
<p>a basic text-to-image generation pipeline using Stable Diffusion.It requires both positive (desired traits) and negative (undesired traits) text prompts to generate an image.In this specific case, the workflow will output a high-resolution photo of a cat wearing a spacesuit inside a spaceship, avoiding blurry or illustration-like effects.by loading the "dreamshaper_8.safetensors"Stable Diffusion model.It generates a blank latent space as the starting point for the image generation.The positive prompt ("a photo of a cat wearing a spacesuit inside a spaceship") and negative prompt ("blurry, illustration") are encoded into conditioning by the CLIPTextEncode node.The KSampler node then uses these conditionings to guide the generative process over 20 sampling steps, applying denoising to the latent space.The resulting latent code is subsequently decoded into an image using the VAE and saved to disk.</p>
<p>-</p>
<p><code>load</code>: Load a reference workflow into the workspace to replace the current workflow, so that you can start over.Arguments: → -<code>name</code>: The name of the reference workflow you want to load.-<code>combine</code>: Combine the current workflow with a reference workflow, so that necessary modules can be added.Arguments: → -<code>name</code>: The name of the reference workflow you want to combine.-<code>adapt</code>: Adapt some parameters in the current workflow, so that the expected effects can be realized.Arguments: → -<code>prompt</code>: The prompt to specify the adaptation you want to make.-<code>retrieve</code>: Retrieve a new batch of reference workflows, so that more useful references can be found.Arguments: → -<code>prompt</code>: The prompt to describe the reference you want to retrieve.-<code>finish</code>: Finish the task since the current workflow can realize the expected effects.</p>
<p>→ 3 .
3
You should not choose the <code>adapt</code>action twice in a row, because they can be simplified into a single action.</p>
<p>→ 4 .→ 5 .
45
If you choose the <code>adapt</code>or <code>retrieve</code>action, make sure the prompt is concise and contains all the necessary information.You should choose the <code>finish</code>action before the remaining steps count down to 0. Now, provide your thought, plan and action with the required format.</p>
<p>→→</p>
<p><action>load(name="text_to_video")</action>PlanAgent Prompt## TaskComfyUI uses workflows to create and execute Stable Diffusion pipelines so that users can design their own workflows to generate highly customized artworks.ComfyUI workflows can be formulated into the equivalent Python code, where each statement represents the execution of a single node.You are an expert in ComfyUI, helping users to design workflows according to their requirements.Now you are required to create a ComfyUI workflow to finish the following task:</p>
<p>a basic text-to-image generation pipeline using Stable Diffusion.It requires both positive (desired traits) and negative (undesired traits) text prompts to generate an image.In this specific case, the workflow will output a high-resolution photo of a cat wearing a spacesuit inside a spaceship, avoiding blurry or illustration-like effects.by loading the "dreamshaper_8.safetensors"Stable Diffusion model.It generates a blank latent space as the starting point for the image generation.The positive prompt ("a photo of a cat wearing a spacesuit inside a spaceship") and negative prompt ("blurry, illustration") are encoded into conditioning by the CLIPTextEncode node.The KSampler node then uses these conditionings to guide the generative process over 20 sampling steps, applying denoising to the latent space.The resulting latent code is subsequently decoded into an image using the VAE and saved to disk.</p>
<p>→ 3 .
3
You should not choose the <code>adapt</code>action twice in a row, because they can be simplified into a single action.</p>
<p>→ 4 .
4
If you choose the <code>adapt</code>or <code>retrieve</code>action, make sure the prompt is concise and contains all the necessary information.</p>
<p>→ 5 .
5
You should choose the <code>finish</code>action before the remaining steps count down to 0. Now, provide your thought, plan and action with the required format.</p>
<p>→</p>
<p>Now you are required to create a ComfyUI workflow to finish the following task:</p>
<p>-</p>
<p><code>load</code>: Load a reference workflow into the workspace to replace the current workflow, so that you can start over.Arguments: → -<code>name</code>: The name of the reference workflow you want to load.</p>
<p>→ 3 .
3
You should not choose the <code>adapt</code>action twice in a row, because they can be simplified into a single action.</p>
<p>→ 4 .
4
If you choose the <code>adapt</code>or <code>retrieve</code>action, make sure the prompt is concise and contains all the necessary information.</p>
<p>→ 5 .
5
You should choose the <code>finish</code>action before the remaining steps count down to 0. Now, provide your thought, plan and action with the required format.</p>
<p>→</p>
<p>Now you are required to create a ComfyUI workflow to finish the following task:</p>
<p>loads two models: a Stable Diffusion model ("sd_xl_base_1.0.safetensors") for generating the initial image from the text description, and then a Stable Video Diffusion model ("svd_xt_1_1.safetensors")to translate that image into a dynamic video over multiple frames.Following the text input describing colorful fish and bubbles, the image is generated and expanded across 16 frames, which corresponds to a duration of 2 seconds at 8 FPS (frames per second).The final MP4 video is constructed and saved along with metadata.</p>
<p>your code and annotation with the required format.</p>
<p>-Step: 1 <thought>
1
loads and inverts the input scribble image.The inverted image is then used by a "control_v11p_sd15_scribble_fp16" ControlNet model to extract and applied to the scribble, controlling and guiding the image generation process.A pre-trained model ("dreamshaper_8.safetensors")processes the positive conditioning generated by the combination of the text prompt and the ControlNet's output, along with negative conditioning from undesired traits such as "horror" and "lowres."The latent space image is generated using a KSampler and further decoded by the VAE to form the final image.The model synthesizes a highly detailed image while adhering closely to both the shape of the scribble and the desired features implied by the text prompt.Here is a recent history of your thought, plan and action in the previous steps.The most recent record is at the bottom.→Since the history is empty, I need to start by loading a relevant reference workflow that can be used as a base for generating the 2-second video of colorful fish in an aquarium.The "text_to_video" reference seems appropriate to use as it involves generating video from text prompts, which aligns with the first part of the task.</p>
<p>history and workspace, you should first think about what functions have been implemented and what modules remain to be added.Your thought should be enclosed with "<thought>" tag.For example: <thought>The basic pipeline has been implemented, but a module is needed to improve the quality.</thought>.</p>
<p>-</p>
<p><code>load</code>: Load a reference workflow into the workspace to replace the current workflow, so that you can start over.Arguments: → -<code>name</code>: The name of the reference workflow you want to load.-<code>combine</code>: Combine the current workflow with a reference workflow, so that necessary modules can be added.Arguments: → -<code>name</code>: The name of the reference workflow you want to combine.</p>
<p>Table 1 .
1
Correlation analysis between the average scores given by human evaluators and GPT-4o indicates a strong agreement.
MetricStatisticP-valueKendall's τ0.6582.116 × 10 −8Pearson's r0.7918.374 × 10 −12Spearman's ρ0.7621.328 × 10 −10
InitYou are given an image of a scribble flower.Repaint the scribble into a realistic red flower.The result should be an image of a red flower.</p>
<p>Table 2 .
2
Evaluation results of all the baseline agents on ComfyBench.The pass rate and resolve rate of every task category, together with the summary result on ComfyBench, are reported.The best results are highlighted in bold.
AgentVanillaComplexCreativeTotal%Pass %Resolve %Pass %Resolve %Pass %Resolve %Pass %ResolveGPT-4o + Zero-shot0.00.00.00.00.00.00.00.0GPT-4o + Few-shot32.027.016.78.37.50.022.516.0GPT-4o + CoT44.029.011.78.312.50.028.017.0GPT-4o + CoT-SC45.034.011.75.015.00.029.018.5Claude-3.5-Sonnet + RAG27.013.023.06.77.50.022.08.5Llama-3.1-70B + RAG58.032.023.010.015.05.039.020.0GPT-4o + RAG62.041.045.021.740.07.552.023.0o1-mini + RAG32.016.021.78.312.57.525.012.0o1-preview + RAG70.046.048.323.330.012.555.532.5Llama-3.1-70B + ComfyAgent63.035.026.718.320.05.043.524.0GPT-4o + ComfyAgent67.046.048.321.740.015.056.032.5
modules: Memory, Planner, and Actions.</p>
<p>Table 3 .
3
Ablation results of RAG on GPT-4o with different workflow representations on ComfyBench.The pass rate and resolve rate are reported, denoted by %P and %R respectively.
RepresentationVanillaComplexCreative%P %R %P %R %P %RFlow Graph------JSON52.0 33.0 25.08.317.57.5Element List51.0 30.0 23.35.020.0 10.0Code62.0 41.0 45.0 15.0 30.07.5Table 4. Ablation results of ComfyAgent on GPT-4o with differentarchitectures on ComfyBench. Each variant is implemented by re-moving one agent from the multi-agent framework. The pass rateand resolve rate are reported, denoted by %P and %R respectively.ArchitectureVanillaComplexCreative%P %R %P %R %P %RComfyAgent67.0 46.0 48.3 21.7 40.0 15.0w/o Combine 69.0 43.0 15.05.035.0 15.0w/o Adapt52.0 26.0 20.0 10.0 17.55.0w/o Retrieve27.0 16.0 20.05.07.57.5w/o Refine65.0 42.0 43.3 21.7 35.0 12.5</p>
<p>Table 7 .
7
Continued from previous page.</p>
<p>Example: text_to_video <function> This workflow follows a text-to-video paradigm, where it generates a video from a given text prompt.It first creates an initial image based on the text description and then uses that image as the starting frame to generate a video with motion and transformation using a Stable Video Diffusion model.The output is a 3-second video at 8 frames per second, depicting beautiful scenery with mountains, rivers, and clouds.It uses the text description to create conditioning, generating an initial 1024x576 image.The image is decoded from latent space via a VAE.The video-specific node ("SVD_img2vid_Conditioning") then applies continuity and motion to this image, producing conditioned latent representations for video generation.The final video is created by sampling the latent space over multiple frames and combining them into an MP4 video using the specified frame rate and format.video frame interpolation using the RIFE VFI model.It takes an input video such as "play_guitar.gif",increases the frame rate by generating intermediate frames (interpolating) with a multiplier (in this case, 3x), and produces a smoother video with a higher frame rate (from 8 to 24 frames per second).The final output is saved as a new video or animated GIF.
<function>This workflow performs →→→→</function><principle>→→→→</function><principle>The workflow first loads two models: a Stable Diffusion model for generating the initial imagefrom the text ("sd_xl_base_1.0.safetensors") and a Stable Video Diffusion model ("svd_xt_1_1.safetensors") for video generation. → →→→→→→</principle>-Example: video_frame_interpolation</p>
<p>This workflow follows a text-to-video paradigm, where it generates a video from a given text prompt.It first creates an initial image based on the text description and then uses that image as the starting frame to generate a video with motion and transformation using a Stable Video Diffusion model.The output is a 3-second video at 8 frames per second, depicting beautiful scenery with mountains, rivers, and clouds.
→→→→</function><principle>The workflow first loads two models: a Stable Diffusion model for generating the initial imagefrom the text ("sd_xl_base_1.0.safetensors") and a Stable Video Diffusion model("svd_xt_1_1.safetensors") for video generation. It uses the text description to createconditioning, generating an initial 1024x576 image. The image is decoded from latent spacevia a VAE. The video-specific node ("SVD_img2vid_Conditioning") then applies continuity andmotion to this image, producing conditioned latent representations for video generation.→→</code><function></p>
<p>Stable Diffusion model for generating the initial image from the text ("sd_xl_base_1.0.safetensors") and a Stable Video Diffusion model ("svd_xt_1_1.safetensors")for video generation.It uses the text description to create conditioning, generating an initial 1024x576 image.The image is decoded from latent space via a VAE.The video-specific node ("SVD_img2vid_Conditioning") then applies continuity and motion to this image, producing conditioned latent representations for video generation.The final video is created by sampling the latent space over multiple frames and combining them into an MP4 video using the specified frame rate and format.
→→→→</function><principle>The workflow first loads two models: a →→→→→→→</principle>-Example: video_frame_interpolation
<function> This workflow performs video frame interpolation using the RIFE VFI model.It takes an input video such as "play_guitar.gif",increases the frame rate by generating intermediate frames (interpolating) with a multiplier (in this case, 3x), and produces a smoother video with a higher frame rate (from 8 to 24 frames per second).The final output is saved as a new video or animated GIF.</p>
<p>This workflow performs video frame interpolation using the RIFE VFI model.It takes an input video such as "play_guitar.gif",increases the frame rate by generating intermediate frames (interpolating) with a multiplier (in this case, 3x), and produces a smoother video with a higher frame rate (from 8 to 24 frames per second).The final output is saved as a new video or animated GIF.
→→→→</function><principle>
→</code> <function></p>
<p>This workflow first generates a 2-second video of colorful fish swimming in an aquarium with bubbles rising to the surface from a text description using a text-to-video approach.It then performs video interpolation to increase the frame rate by 3x, resulting in a smoother video.byloading the necessary models, including the Stable Diffusion model and the Stable Video Diffusion model.These are used to encode the input text to create a video showcasing colorful fish and bubbles.Initially, the video is generated at 8 frames per second (FPS) over 16 frames (˜2 seconds).After the video is synthesized, it is passed through a frame interpolation process using the RIFE VFI model, which increases the frame rate by generating additional frames, multiplying the frame rate by 3x (from 8 FPS to 24 FPS).Finally, the interpolated frames are combined into a smoother MP4 video output.ComfyUI uses workflows to create and execute Stable Diffusion pipelines so that users can design their own workflows to generate highly customized artworks.ComfyUI workflows can be formulated into the equivalent Python code, where each statement represents the execution of a single node.You are an expert in ComfyUI, helping users to design workflows according to their requirements.
→→→</function><principle>The workflow begins →→→→→→</principle>PlanAgent Prompt## Task→→→</code><function>
→ → → → Now you are required to create a ComfyUI workflow to finish the following task:</p>
<p>The workflow first loads two models: a Stable Diffusion model for generating the initial image from the text ("sd_xl_base_1.0.safetensors") and a Stable Video Diffusion model ("svd_xt_1_1.safetensors")forvideo generation.It uses the text description to create conditioning, generating an initial 1024x576 image.The image is decoded from latent space via a VAE.The video-specific node ("SVD_img2vid_Conditioning") then applies continuity and motion to this image, producing conditioned latent representations for video generation.The final video is created by sampling the latent space over multiple frames and combining them into an MP4 video using the specified frame rate and format.loads the input video using "VHS_LoadVideo", which extracts the individual frames.The "RIFE VFI" node is then used to interpolate the frames by generating additional frames between the existing ones.In this scenario, the multiplier is set to 3x, effectively tripling the frame count and enabling a smoother video playback at 24 frames per second.Finally, the interpolated frames are combined into a video or GIF format using "VHS_VideoCombine".
→→→→→→→</principle>-Example: video_frame_interpolation<function>This workflow performs video frame interpolation using the RIFE VFI model. It takes an input→video such as "play_guitar.gif", increases the frame rate by generating intermediate frames→(interpolating) with a multiplier (in this case, 3x), and produces a smoother video with a→higher frame rate (from 8 to 24 frames per second). The final output is saved as a new→video or animated GIF.</function><principle>The workflow first →→→→→</principle>-Example: image_to_video→→</function>→→→→→→</principle>-Example: text_to_image<function>This workflow implements a basic text-to-image generation pipeline using Stable Diffusion. It→requires both positive (desired traits) and negative (undesired traits) text prompts to→generate an image. In this specific case, the workflow will output a high-resolution photo→of a cat wearing a spacesuit inside a spaceship, avoiding blurry or illustration-like→Example: text_to_video effects.</function><function>This workflow follows a text-to-video paradigm, where it generates a video from a given text <principle>prompt. It first creates an initial image based on the text description and then uses that The workflow begins by loading the "dreamshaper_8.safetensors" Stable Diffusion model. It →→image as the starting frame to generate a video with motion and transformation using a generates a blank latent space as the starting point for the image generation. The positive→Stable Video Diffusion model. The output is a 3-second video at 8 frames per second, prompt ("a photo of a cat wearing a spacesuit inside a spaceship") and negative prompt→depicting beautiful scenery with mountains, rivers, and clouds.</function><principle>
("blurry, illustration") are encoded into conditioning by the CLIPTextEncode node.The KSampler node then uses these conditionings to guide the generative process over 20 sampling steps, applying denoising to the latent space.The resulting latent code is subsequently decoded into an image using the VAE and saved to disk.</p>
<p>generates a 2-second video of colorful fish swimming in an aquarium with bubbles rising to the surface from a text description using a text-to-video approach.It then performs video interpolation to increase the frame rate by 3x, resulting in a smoother video.The workflow begins by loading the necessary models, including the Stable Diffusion model and the Stable Video Diffusion model.These are used to encode the input text to create a video showcasing colorful fish and bubbles.Initially, the video is generated at 8 frames per second (FPS) over 16 frames (˜2 seconds).After the video is synthesized, it is passed through a frame interpolation process using the RIFE VFI model, which increases the frame rate by generating additional frames, multiplying the frame rate by 3x (from 8 FPS to 24 FPS).Finally, the interpolated frames are combined into a smoother MP4 video output.
→→→</code><function>This workflow first →→→</function><principle>
Based on the current working progress, your schedule is presented as follows:-<code>finish</code>: Finish the task since the current workflow can realize the expected effects.Refer to the history before making a decision.Here are some general rules you should follow:1.You should choose the <code>load</code>action if and only if the history is empty.2. If you choose the <code>load</code>or <code>combine</code>action, make sure the name exists in the reference.Otherwise, try to update the reference with the <code>retrieve</code>action.PlanAgent Answer<thought>The current workflow successfully generates a 2-second video of colorful fish swimming in an aquarium with bubbles rising to the surface and then applies video interpolation to enhance the frame rate by 3x.The workflow seems to fulfill both the generation and interpolation requirements.No additional modifications appear to be necessary, as the video generation part is functional and the interpolation process has been properly set to increase the frame rate from 8 FPS to 24 FPS.</thought>Step 1: I will finish the task since the expected effects have been realized.</plan> <action>finish()</action>
Salt node documentation. A I Salt, </p>
<p>Google DeepMind AlphaCode Team. Alphacode 2 technical report. 202413</p>
<p>. Anthropic, 2024Claude 3.5 sonnet</p>
<p>Human memory: A proposed system and its control processes. The psychology of learning and motivation. Richard C Atkinson, 1968</p>
<p>. Autogpt, Build, </p>
<p>Working memory. Alan Baddeley, Science. 61992</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, arXiv:2204.058622022arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, AAAI. 2024</p>
<p>Video generation models as world simulators. Tim Brooks, Bill Peebles, Connor Holmes, Will Depue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, Aditya Ramesh, 2024</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, NeurIPS. 272020</p>
<p>Spider2-v: How far are multimodal agents from automating data science and engineering workflows?. Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, arXiv:2407.109562024arXiv preprint</p>
<p>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, arXiv:2308.108482023arXiv preprint</p>
<p>Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, Feng Zhao, arXiv:2407.20183Mindsearch: Mimicking human minds elicits deep ai searcher. 2024arXiv preprint</p>
<p>Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, Yu Su, NeurIPS. 2024</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023arXiv preprint</p>
<p>Openagi: When llm meets domain experts. Yingqiang Ge, Wenyue Hua, Kai Mei, Juntao Tan, Shuyuan Xu, Zelong Li, Yongfeng Zhang, NeurIPS2024</p>
<p>Insightface: 2d and 3d face analysis project. Jia Guo, Jiankang Deng, Xiang An, Jack Yu, Baris Gecer, </p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2402.016802024arXiv preprint</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, arXiv:2308.00352Meta programming for multi-agent collaborative framework. 2023arXiv preprint</p>
<p>Automated design of agentic systems. Shengran Hu, Cong Lu, Jeff Clune, arXiv:2408.0843520242arXiv preprint</p>
<p>Ian Huang, Guandao Yang, Leonidas Guibas, arXiv:2404.17672Blenderalchemy: Editing 3d graphics with vision-language models. 2024arXiv preprint</p>
<p>Timo Kaufmann, Paul Weng, arXiv:2312.14925Viktor Bengs, and Eyke Hüllermeier. A survey of reinforcement learning from human feedback. 2023arXiv preprint</p>
<p>Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, arXiv:2310.03714Compiling declarative language model calls into self-improving pipelines. 2023arXiv preprint</p>
<p>P Diederik, Kingma, arXiv:1312.6114Auto-encoding variational bayes. 2013arXiv preprint</p>
<p>. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick, arXiv:2304.026432023Segment anything. arXiv preprint</p>
<p>Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried, arXiv:2401.13649Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. 2024arXiv preprint</p>
<p>Applications that can reason. Langchain, </p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, NeurIPS. 3272020</p>
<p>Camel: Communicative agents for" mind" exploration of large language model society. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, NeurIPS2023</p>
<p>Autoflow: Automated workflow generation for large language model agents. Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, Yongfeng Zhang, arXiv:2407.12821202415arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, NeurIPS2024</p>
<p>Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, Yongfeng Zhang, arXiv:2403.16971Llm agent operating system. 2024arXiv preprint</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Browser-assisted question-answering with human feedback. 202113arXiv preprint</p>
<p>Can generalist foundation models outcompete special-purpose tuning? case study in medicine. Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, arXiv:2311.164522023arXiv preprint</p>
<p>. Openai, Chatgpt, </p>
<p>Hello gpt-4o. Openai, 202461</p>
<p>Introducing openai o1-preview. Openai, 6</p>
<p>Openai, New embedding models and api updates. </p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, NeurIPS2022</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, ICML. 2021</p>
<p>Zero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, ICML. 2021</p>
<p>Low-rank adaptation for fast text-to-image diffusion fine-tuning. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, CVPR. 2022High-resolution image synthesis with latent diffusion models</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, NeurIPS2024</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, CVPR. 2023</p>
<p>Shunyu Theodore R Sumers, Karthik Yao, Thomas L Narasimhan, Griffiths, arXiv:2309.02427Cognitive architectures for language agents. 2023arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>A language-agent approach to formal theorem-proving. Amitayush Thakur, Yeming Wen, Swarat Chaudhuri, arXiv:2310.043532023arXiv preprint</p>
<p>Solving olympiad geometry without human demonstrations. Yuhuai Trieu H Trinh, Wu, He Quoc V Le, Thang He, Luong, Nature. 32024</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Frontiers of Computer Science. 32024</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.11171202237arXiv preprint</p>
<p>Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, Heng Ji, arXiv:2402.01030Executable code actions elicit better llm agents. 20247arXiv preprint</p>
<p>Opendevin: An open platform for ai software developers as generalist agents. Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, arXiv:2407.167412024arXiv preprint</p>
<p>Daniel Fried, and Graham Neubig. Execution-based evaluation for open-domain code generation. Zhiruo Wang, Shuyan Zhou, arXiv:2212.104812022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, NeurIPS. 3272022</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, arXiv:2308.081552023arXiv preprint</p>
<p>Os-copilot: Towards generalist computer agents with self-improvement. Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, Lingpeng Kong, arXiv:2402.074562024arXiv preprint</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, arXiv:2406.04151Evolving large language model-based agents across diverse environments. 2024arXiv preprint</p>
<p>Openagents: An open platform for language agents in the wild. Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Jing Toh, Junning Hua, Qian Zhao, Che Liu, Liu, arXiv:2310.106342023arXiv preprint</p>
<p>Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Jing Toh, Zhoujun Hua, Dongchan Cheng, Fangyu Shin, Lei, arXiv:2404.07972Benchmarking multimodal agents for open-ended tasks in real computer environments. 2024arXiv preprint</p>
<p>Interactive evolution: A neural-symbolic self-training framework for large language models. Fangzhi Xu, Qiushi Sun, Kanzhi Cheng, Jun Liu, Yu Qiao, Zhiyong Wu, arXiv:2406.117362024arXiv preprint</p>
<p>Core: Llm as interpreter for natural language programming, pseudo-code programming, and flow programming of ai agents. Shuyuan Xu, Zelong Li, Kai Mei, Yongfeng Zhang, arXiv:2405.069072024arXiv preprint</p>
<p>John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, Ofir Press, arXiv:2405.15793Swe-agent: Agent-computer interfaces enable automated software engineering. 2024arXiv preprint</p>
<p>Intercode: Standardizing and benchmarking interactive coding with execution feedback. John Yang, Akshara Prabhakar, Karthik Narasimhan, Shunyu Yao, NeurIPS. 2024</p>
<p>Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, Gang Yu, arXiv:2312.13771Appagent: Multimodal agents as smartphone users. 2023arXiv preprint</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, NeurIPS2022</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.036292022arXiv preprint</p>
<p>Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. Jun Hu Ye, Sibo Zhang, Xiao Liu, Wei Han, Yang, arXiv:2308.067212023arXiv preprint</p>
<p>Natural language to code generation in interactive data science notebooks. Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, arXiv:2212.092482022arXiv preprint</p>
<p>Enabling generalist video generation via a multi-agent framework. Zhengqing Yuan, Ruoxi Chen, Zhaoxu Li, Haolong Jia, Lifang He, Chi Wang, Lichao Sun, Mora, arXiv:2403.132482024arXiv preprint</p>
<p>The shift from models to compound ai systems. Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather Miller, Chris Potts, James Zou, Michael Carbin, Jonathan Frankle, Naveen Rao, Ali Ghodsi, 3</p>
<p>Adding conditional control to text-to-image diffusion models. Lvmin Zhang, Anyi Rao, Maneesh Agrawala, CVPR. 2023</p>
<p>Imposing consistent light. Lvmin Zhang, Anyi Rao, Maneesh Agrawala, 2024</p>
<p>Wenqi Zhang, Yongliang Shen, Weiming Lu, Yueting Zhuang, arXiv:2306.07209Data-copilot: Bridging billions of data and humans with autonomous workflow. 202335arXiv preprint</p>
<p>Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen, arXiv:2404.13501A survey on the memory mechanism of large language model based agents. 2024arXiv preprint</p>
<p>Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, arXiv:2312.07104Efficient execution of structured language model programs. 2023arXiv preprint</p>
<p>Memorybank: Enhancing large language models with long-term memory. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, AAAI. 2024</p>
<p>Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, arXiv:2307.13854A realistic web environment for building autonomous agents. 2023arXiv preprint</p>
<p>Agents: An open-source framework for autonomous language agents. Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, arXiv:2309.078702023arXiv preprint</p>
<p>Symbolic learning enables selfevolving agents. Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, arXiv:2406.185322024arXiv preprint</p>
<p>Gptswarm: Language agents as optimizable graphs. Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, Jürgen Schmidhuber, ICML. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
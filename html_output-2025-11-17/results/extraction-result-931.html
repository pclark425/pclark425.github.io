<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-931 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-931</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-931</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-352420ee61a8da783ca7750170793613b18b8d9c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/352420ee61a8da783ca7750170793613b18b8d9c" target="_blank">Tool Learning with Foundation Models</a></p>
                <p><strong>Paper Venue:</strong> ACM Computing Surveys</p>
                <p><strong>Paper TL;DR:</strong> A systematic investigation and comprehensive review of tool learning and discusses several open problems that require further investigation, such as ensuring trustworthy tool use, enabling tool creation with foundation models, and addressing personalization challenges.</p>
                <p><strong>Paper Abstract:</strong> Humans possess an extraordinary ability to create and utilize tools. With the advent of foundation models, artificial intelligence systems have the potential to be equally adept in tool use as humans. This paradigm, which is dubbed as tool learning with foundation models, combines the strengths of tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. This article presents a systematic investigation and comprehensive review of tool learning. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research and formulate a general framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate generalization in tool learning. Finally, we discuss several open problems that require further investigation, such as ensuring trustworthy tool use, enabling tool creation with foundation models, and addressing personalization challenges. Overall, we hope this article could inspire future research in integrating tools with foundation models.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e931.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e931.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT / GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT / GPT-4 (OpenAI foundation models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art conversational foundation models cited as examples of highly capable LLM controllers; discussed as able to understand intent and in the paper's experiments are reported to effectively use tools with simple prompting, but numeric interactive vs QA performance is not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ChatGPT / GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained autoregressive transformer language models optimized for instruction-following and conversational use; multimodal capability emphasized for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Tool use over 18 representative tools (paper's experiment set)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>instruction-tuned transformer; conversational interface; multimodal in GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>instruction tuning / RLHF (discussed as common for these models)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting + instruction tuning</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>The paper reports using simple prompting (zero-shot / few-shot / demonstrations) to teach ChatGPT to call APIs/tools; GPT-4 noted for multimodal grounding which aids tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Described qualitatively as effective tool use with simple prompting; no quantitative before/after metrics reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Despite strong QA/knowledge capabilities, models lack embodiment and environment grounding; context length and prompt sensitivity limit scaling to many tool descriptions; smaller or unfine-tuned models struggle to follow tool-call formats.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Foundation Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e931.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e931.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebGPT (browser-assisted QA with human feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-3â€“based system fine-tuned to interact with a web browser/search engine to improve long-form question answering via supervised demonstrations and human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>WebGPT: Browser-assisted Question-Answering with Human Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>WebGPT (fine-tuned GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 fine-tuned with human demonstrations to produce browser actions (search queries, follow links) and further refined with human preference feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Long-form question answering / multi-step QA</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Web browsing / search API manipulation for QA</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / web navigation / multi-step QA</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>LLM controller grounded to browser actions; supervised policy head mapping context to browser commands</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>supervised fine-tuning on human demonstrations + human feedback (RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (behavior cloning + RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Fine-tune a language model on recorded human browser interactions and then optimize with human preference rewards so the model learns to query and filter web content.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Reported in cited work to improve web-grounded QA and alignment with human preferences; no numeric metrics reported in this survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Vanilla LMs are not anchored to executable browser commands; supervised demonstrations and human feedback are needed to bridge language output and valid tool actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Foundation Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e931.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e931.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toolformer: Language Models Can Teach Themselves to Use Tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised method that augments pre-trained LMs by generating and filtering synthetic examples with API calls so the model learns when and how to call external tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language Models Can Teach Themselves to Use Tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Toolformer (applied to GPT-J in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pipeline that prompts a frozen LM to insert tool calls in text, validates the calls via tool execution, and fine-tunes the LM on the filtered examples to improve tool-use behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Tool use (APIs like calculator, search)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step QA</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>no architectural change to LM; training pipeline augments dataset with tool-call tokens</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>self-supervised fine-tuning with synthetic tool-use examples</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Automatically generate candidate tool-call insertions, validate them by executing tools, filter, and fine-tune the LM on the resulting corpus so it learns when to call tools.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Cited to significantly improve GPT-J's tool-use performance in the original work; this paper references that improvement but provides no numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>LMs lack exposure to explicit tool-call patterns during pretraining; augmenting training with validated tool-call examples closes gap between language-only QA and interactive tool invocation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Foundation Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e931.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e931.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reasoning + Acting prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that interleaves chain-of-thought style reasoning traces with environment actions, enabling LLMs to both reason and decide to call tools iteratively for multi-step QA and interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ReAct (prompting paradigm applied to LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt design that alternates 'Thought:' and 'Action:' tokens so the model generates internal reasoning and explicit tool actions/API calls in the same output stream.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Multi-step question answering</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Multi-step tool-augmented QA (search APIs, Wikipedia), action-oriented tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / tool use</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>no model architecture change; incorporates chain-of-thought + explicit action tokens</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting (few-shot / CoT style)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Augment CoT with explicit action specification so the model can decide when to call tools and use feedback to continue reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Cited to improve multi-step QA and interactive tool use qualitatively; no numeric metrics in this survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Standard CoT alone produces answers but lacks explicit action outputs; ReAct's coupling of reasoning and actions reduces the mismatch between QA reasoning and procedural tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Foundation Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e931.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e931.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate reasoning traces from LLMs by providing examples with step-by-step solutions, improving performance on complex reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Chain-of-Thought prompting (applied to large LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt engineering method that demonstrates intermediate reasoning steps in few-shot examples to encourage models to output stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Arithmetic, commonsense, symbolic reasoning benchmarks (general QA tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>no architectural change; relies on model capacity to produce internal reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting (few-shot CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Provide exemplars of intermediate reasoning steps to elicit similar chains in model outputs, facilitating decomposition of multi-step QA.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Reported to significantly boost reasoning-related QA tasks; the survey notes CoT helps but also that introspective CoT without environment feedback can produce unrealistic plans for interactive execution.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>CoT improves internal reasoning for QA but when plans must be executed in an environment, lack of grounding/feedback can lead to unrealistic or unexecutable plans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Foundation Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e931.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e931.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program-Aided Language Models (PAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method prompting LMs to generate programs (e.g., Python) for intermediate reasoning steps and using an interpreter as a tool to compute exact results, improving complex reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Program-Aided Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>PAL (model-as-programmer approach)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting technique where the LM outputs code that expresses reasoning steps; the code is executed to obtain precise intermediate results and then used in further reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Arithmetic, symbolic, and algorithmic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Program execution as tool (e.g., calculator via interpreter)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / tool use (programming interface)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>use of external program interpreter; 'model-as-programmer' concept</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting with program-generation examples</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural/training hybrid (use of external interpreter tool)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Leverage code-generation by LMs and execute that code to ground intermediate computations, reducing reasoning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Reported to improve arithmetic and algorithmic reasoning in cited work; in this survey highlighted as enabling generation of executable programs though lacking direct environment feedback can limit adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Transforming reasoning into executable programs grounds computations but real-world tool interactions still require environment feedback and affordance grounding for robust execution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Foundation Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e931.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e931.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SayCan (language grounding with affordance/value function)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that combines language model priors with an affordance/value function to rank feasible robot actions, grounding LMs to environment-executable actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SayCan: Grounding Language in Robotic Affordances</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines an LLM to interpret instructions and a learned value/affordance model estimating success probabilities of candidate robot actions; selects actions by multiplying language preference and affordance score.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Embodied robotic action planning / tool-use</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>embodied planning / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>value function / affordance model combined with LLM controller</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change (grounding with affordance/value model)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Estimate probability of successful execution for candidate actions to bias LLM planning toward feasible actions in the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Described as better grounding LLM decisions to physically executable actions and improving robustness in embodied tasks; no numeric metrics given in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Pure language outputs may propose actions that are infeasible; grounding via affordance/value model reduces unrealistic plans and bridges QA-style reasoning with executable behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Foundation Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e931.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e931.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Behavior Cloning / Supervised Fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Behavior cloning / supervised fine-tuning on human demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training intervention where LLMs/controllers are fine-tuned to imitate recorded human tool-use traces to map states and intents to executable tool actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Behavior cloning (applied to foundation models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Supervised learning setup where the model is trained on (query, action-trace) pairs recorded from human experts to produce action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Web browsing, web-based shopping (e.g., WebShop), robotic control</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / web navigation / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>policy mapping from observations to discrete action tokens or API calls</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>supervised fine-tuning on human demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Collect human interaction traces and train the model to imitate state-to-action mappings so the LLM becomes anchored to valid tool commands.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Cited examples (e.g., WebGPT, WebShop) show improved interactive performance after supervised fine-tuning; no numeric values reported in this survey excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Pretrained LMs lack mappings from text to concrete tool actions; behavior cloning provides direct supervision to bridge this gap but requires costly human data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Foundation Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e931.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e931.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reinforcement Learning from Human Feedback (RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning from Human Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training approach that uses human preference signals to train a reward model and then fine-tunes a policy model with RL (e.g., PPO) to align model actions with human preferences, applied to both QA and tool-use behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep Reinforcement Learning from Human Preferences</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>RLHF (as applied to LMs / controllers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Human comparisons are used to fit a reward model; an RL algorithm then optimizes the LM/controller policy to maximize this learned reward.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Tool-augmented QA, web interaction, long-form QA (e.g., WebGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step QA</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>policy network fine-tuned with reward model guidance</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>RL with learned human-preference reward (PPO commonly used)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Fit a reward model from human comparisons and optimize the controller with RL to align outputs and action sequences with human preferences for better tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Reported to align outputs with human preferences and improve tool-use/QA quality (cited WebGPT example); survey notes limitations like task-specificity and bias without numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Supervised signals alone may not capture nuanced human preferences for procedural tasks; RLHF provides preference-driven optimization to close the behavior gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Foundation Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e931.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e931.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Interface Unification (semantic / GUI / programming)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interface unification strategies: semantic, GUI, and programming interfaces</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Architectural/engineering interventions proposed to standardize how LMs interact with diverse tools (textual action tokens, simulated GUI actions, or program/API calls) to improve transfer and generalization of tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Interface unification (design concept for controllers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Three proposed unified interfaces: semantic (action names/text triggers), GUI (mouse/keyboard simulation), and programming (generate executable code/API calls) to standardize tool interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>General multi-tool interactions across many tool types</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / programming-interface mediated tool use</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>design-level choices rather than single model architecture; enables mapping LM outputs to executable actions</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change / interface design</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Standardize action spaces so models can transfer learned action semantics across tools; programming interface leverages code models, GUI interface simulates human interaction, semantic uses action labels.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Argued to facilitate generalization and reduce per-tool retraining; no experimental numeric effect reported in survey excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Diverse tool APIs and interfaces cause distributional mismatch between LM outputs and valid tool commands; unification reduces mismatch and enables transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Foundation Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e931.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e931.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-GPT (multi-step multi-tool autonomous agent project)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical system built on GPT-4 that chains reasoning, tool calls, and reflection to autonomously pursue long-horizon objectives across multiple tools; presented as an example of multi-tool long-term planning but not rigorously benchmarked in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Auto-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agentic framework that uses an LLM to decompose goals, call multiple tools over time, and reflect on past actions to refine future decisions; primarily an application/project example.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Long-horizon multi-tool planning and execution</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step multi-tool planning / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>agent loop: plan â†’ act (tool calls) â†’ observe â†’ reflect</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>largely prompt / orchestration over a base LLM (no new training reported here)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>system-level agent design (hybrid prompting + tool orchestration)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Chain-of-tasks orchestration with memory and reflection to enable long-term autonomous multi-tool workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Demonstrated capabilities in informal examples; this survey highlights Auto-GPT as pushing boundaries but provides no rigorous metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>LLMs need orchestrating logic, memory, and grounding to turn QA-style capabilities into reliable long-horizon interactive behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Foundation Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 2)</em></li>
                <li>Toolformer: Language Models Can Teach Themselves to Use Tools <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted Question-Answering with Human Feedback <em>(Rating: 2)</em></li>
                <li>Program-Aided Language Models <em>(Rating: 2)</em></li>
                <li>SayCan: Grounding Language in Robotic Affordances <em>(Rating: 2)</em></li>
                <li>Self-Ask: A Simple Question Decomposition Approach (Self-Ask / Self-Ask paper by Press et al.) <em>(Rating: 1)</em></li>
                <li>Tool learning with foundation models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-931",
    "paper_id": "paper-352420ee61a8da783ca7750170793613b18b8d9c",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "ChatGPT / GPT-4",
            "name_full": "ChatGPT / GPT-4 (OpenAI foundation models)",
            "brief_description": "State-of-the-art conversational foundation models cited as examples of highly capable LLM controllers; discussed as able to understand intent and in the paper's experiments are reported to effectively use tools with simple prompting, but numeric interactive vs QA performance is not reported.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "ChatGPT / GPT-4",
            "model_description": "Large pre-trained autoregressive transformer language models optimized for instruction-following and conversational use; multimodal capability emphasized for GPT-4.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Tool use over 18 representative tools (paper's experiment set)",
            "interactive_task_type": "tool use / multi-step reasoning",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "instruction-tuned transformer; conversational interface; multimodal in GPT-4",
            "training_method": "instruction tuning / RLHF (discussed as common for these models)",
            "intervention_type": "prompting + instruction tuning",
            "intervention_description": "The paper reports using simple prompting (zero-shot / few-shot / demonstrations) to teach ChatGPT to call APIs/tools; GPT-4 noted for multimodal grounding which aids tool use.",
            "intervention_effect": "Described qualitatively as effective tool use with simple prompting; no quantitative before/after metrics reported in this paper.",
            "hypothesized_cause_of_gap": "Despite strong QA/knowledge capabilities, models lack embodiment and environment grounding; context length and prompt sensitivity limit scaling to many tool descriptions; smaller or unfine-tuned models struggle to follow tool-call formats.",
            "uuid": "e931.0",
            "source_info": {
                "paper_title": "Tool Learning with Foundation Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "WebGPT",
            "name_full": "WebGPT (browser-assisted QA with human feedback)",
            "brief_description": "A GPT-3â€“based system fine-tuned to interact with a web browser/search engine to improve long-form question answering via supervised demonstrations and human feedback.",
            "citation_title": "WebGPT: Browser-assisted Question-Answering with Human Feedback",
            "mention_or_use": "mention",
            "model_or_agent_name": "WebGPT (fine-tuned GPT-3)",
            "model_description": "GPT-3 fine-tuned with human demonstrations to produce browser actions (search queries, follow links) and further refined with human preference feedback.",
            "model_size": null,
            "qa_task_name": "Long-form question answering / multi-step QA",
            "qa_performance": null,
            "interactive_task_name": "Web browsing / search API manipulation for QA",
            "interactive_task_type": "tool use / web navigation / multi-step QA",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "LLM controller grounded to browser actions; supervised policy head mapping context to browser commands",
            "training_method": "supervised fine-tuning on human demonstrations + human feedback (RLHF)",
            "intervention_type": "training method (behavior cloning + RLHF)",
            "intervention_description": "Fine-tune a language model on recorded human browser interactions and then optimize with human preference rewards so the model learns to query and filter web content.",
            "intervention_effect": "Reported in cited work to improve web-grounded QA and alignment with human preferences; no numeric metrics reported in this survey text.",
            "hypothesized_cause_of_gap": "Vanilla LMs are not anchored to executable browser commands; supervised demonstrations and human feedback are needed to bridge language output and valid tool actions.",
            "uuid": "e931.1",
            "source_info": {
                "paper_title": "Tool Learning with Foundation Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Toolformer",
            "name_full": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "brief_description": "A self-supervised method that augments pre-trained LMs by generating and filtering synthetic examples with API calls so the model learns when and how to call external tools.",
            "citation_title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "mention_or_use": "mention",
            "model_or_agent_name": "Toolformer (applied to GPT-J in cited work)",
            "model_description": "A pipeline that prompts a frozen LM to insert tool calls in text, validates the calls via tool execution, and fine-tunes the LM on the filtered examples to improve tool-use behavior.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Tool use (APIs like calculator, search)",
            "interactive_task_type": "tool use / multi-step QA",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "no architectural change to LM; training pipeline augments dataset with tool-call tokens",
            "training_method": "self-supervised fine-tuning with synthetic tool-use examples",
            "intervention_type": "training method",
            "intervention_description": "Automatically generate candidate tool-call insertions, validate them by executing tools, filter, and fine-tune the LM on the resulting corpus so it learns when to call tools.",
            "intervention_effect": "Cited to significantly improve GPT-J's tool-use performance in the original work; this paper references that improvement but provides no numeric metrics.",
            "hypothesized_cause_of_gap": "LMs lack exposure to explicit tool-call patterns during pretraining; augmenting training with validated tool-call examples closes gap between language-only QA and interactive tool invocation.",
            "uuid": "e931.2",
            "source_info": {
                "paper_title": "Tool Learning with Foundation Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reasoning + Acting prompts)",
            "brief_description": "A prompting technique that interleaves chain-of-thought style reasoning traces with environment actions, enabling LLMs to both reason and decide to call tools iteratively for multi-step QA and interaction.",
            "citation_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "mention_or_use": "mention",
            "model_or_agent_name": "ReAct (prompting paradigm applied to LLMs)",
            "model_description": "Prompt design that alternates 'Thought:' and 'Action:' tokens so the model generates internal reasoning and explicit tool actions/API calls in the same output stream.",
            "model_size": null,
            "qa_task_name": "Multi-step question answering",
            "qa_performance": null,
            "interactive_task_name": "Multi-step tool-augmented QA (search APIs, Wikipedia), action-oriented tasks",
            "interactive_task_type": "multi-step reasoning / tool use",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "no model architecture change; incorporates chain-of-thought + explicit action tokens",
            "training_method": "prompting (few-shot / CoT style)",
            "intervention_type": "prompting strategy",
            "intervention_description": "Augment CoT with explicit action specification so the model can decide when to call tools and use feedback to continue reasoning.",
            "intervention_effect": "Cited to improve multi-step QA and interactive tool use qualitatively; no numeric metrics in this survey text.",
            "hypothesized_cause_of_gap": "Standard CoT alone produces answers but lacks explicit action outputs; ReAct's coupling of reasoning and actions reduces the mismatch between QA reasoning and procedural tool use.",
            "uuid": "e931.3",
            "source_info": {
                "paper_title": "Tool Learning with Foundation Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits intermediate reasoning traces from LLMs by providing examples with step-by-step solutions, improving performance on complex reasoning tasks.",
            "citation_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "mention_or_use": "mention",
            "model_or_agent_name": "Chain-of-Thought prompting (applied to large LMs)",
            "model_description": "Prompt engineering method that demonstrates intermediate reasoning steps in few-shot examples to encourage models to output stepwise reasoning.",
            "model_size": null,
            "qa_task_name": "Arithmetic, commonsense, symbolic reasoning benchmarks (general QA tasks)",
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "no architectural change; relies on model capacity to produce internal reasoning",
            "training_method": "prompting (few-shot CoT)",
            "intervention_type": "prompting strategy",
            "intervention_description": "Provide exemplars of intermediate reasoning steps to elicit similar chains in model outputs, facilitating decomposition of multi-step QA.",
            "intervention_effect": "Reported to significantly boost reasoning-related QA tasks; the survey notes CoT helps but also that introspective CoT without environment feedback can produce unrealistic plans for interactive execution.",
            "hypothesized_cause_of_gap": "CoT improves internal reasoning for QA but when plans must be executed in an environment, lack of grounding/feedback can lead to unrealistic or unexecutable plans.",
            "uuid": "e931.4",
            "source_info": {
                "paper_title": "Tool Learning with Foundation Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "PAL",
            "name_full": "Program-Aided Language Models (PAL)",
            "brief_description": "A method prompting LMs to generate programs (e.g., Python) for intermediate reasoning steps and using an interpreter as a tool to compute exact results, improving complex reasoning tasks.",
            "citation_title": "Program-Aided Language Models",
            "mention_or_use": "mention",
            "model_or_agent_name": "PAL (model-as-programmer approach)",
            "model_description": "Prompting technique where the LM outputs code that expresses reasoning steps; the code is executed to obtain precise intermediate results and then used in further reasoning.",
            "model_size": null,
            "qa_task_name": "Arithmetic, symbolic, and algorithmic reasoning",
            "qa_performance": null,
            "interactive_task_name": "Program execution as tool (e.g., calculator via interpreter)",
            "interactive_task_type": "multi-step reasoning / tool use (programming interface)",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "use of external program interpreter; 'model-as-programmer' concept",
            "training_method": "prompting with program-generation examples",
            "intervention_type": "architectural/training hybrid (use of external interpreter tool)",
            "intervention_description": "Leverage code-generation by LMs and execute that code to ground intermediate computations, reducing reasoning errors.",
            "intervention_effect": "Reported to improve arithmetic and algorithmic reasoning in cited work; in this survey highlighted as enabling generation of executable programs though lacking direct environment feedback can limit adaptation.",
            "hypothesized_cause_of_gap": "Transforming reasoning into executable programs grounds computations but real-world tool interactions still require environment feedback and affordance grounding for robust execution.",
            "uuid": "e931.5",
            "source_info": {
                "paper_title": "Tool Learning with Foundation Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "SayCan",
            "name_full": "SayCan (language grounding with affordance/value function)",
            "brief_description": "A method that combines language model priors with an affordance/value function to rank feasible robot actions, grounding LMs to environment-executable actions.",
            "citation_title": "SayCan: Grounding Language in Robotic Affordances",
            "mention_or_use": "mention",
            "model_or_agent_name": "SayCan",
            "model_description": "Combines an LLM to interpret instructions and a learned value/affordance model estimating success probabilities of candidate robot actions; selects actions by multiplying language preference and affordance score.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Embodied robotic action planning / tool-use",
            "interactive_task_type": "embodied planning / sequential decision-making",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "value function / affordance model combined with LLM controller",
            "training_method": null,
            "intervention_type": "architectural change (grounding with affordance/value model)",
            "intervention_description": "Estimate probability of successful execution for candidate actions to bias LLM planning toward feasible actions in the environment.",
            "intervention_effect": "Described as better grounding LLM decisions to physically executable actions and improving robustness in embodied tasks; no numeric metrics given in the survey.",
            "hypothesized_cause_of_gap": "Pure language outputs may propose actions that are infeasible; grounding via affordance/value model reduces unrealistic plans and bridges QA-style reasoning with executable behavior.",
            "uuid": "e931.6",
            "source_info": {
                "paper_title": "Tool Learning with Foundation Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Behavior Cloning / Supervised Fine-tuning",
            "name_full": "Behavior cloning / supervised fine-tuning on human demonstrations",
            "brief_description": "A training intervention where LLMs/controllers are fine-tuned to imitate recorded human tool-use traces to map states and intents to executable tool actions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "Behavior cloning (applied to foundation models)",
            "model_description": "Supervised learning setup where the model is trained on (query, action-trace) pairs recorded from human experts to produce action sequences.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Web browsing, web-based shopping (e.g., WebShop), robotic control",
            "interactive_task_type": "tool use / web navigation / sequential decision-making",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "policy mapping from observations to discrete action tokens or API calls",
            "training_method": "supervised fine-tuning on human demonstrations",
            "intervention_type": "training method",
            "intervention_description": "Collect human interaction traces and train the model to imitate state-to-action mappings so the LLM becomes anchored to valid tool commands.",
            "intervention_effect": "Cited examples (e.g., WebGPT, WebShop) show improved interactive performance after supervised fine-tuning; no numeric values reported in this survey excerpt.",
            "hypothesized_cause_of_gap": "Pretrained LMs lack mappings from text to concrete tool actions; behavior cloning provides direct supervision to bridge this gap but requires costly human data.",
            "uuid": "e931.7",
            "source_info": {
                "paper_title": "Tool Learning with Foundation Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Reinforcement Learning from Human Feedback (RLHF)",
            "name_full": "Reinforcement Learning from Human Feedback",
            "brief_description": "A training approach that uses human preference signals to train a reward model and then fine-tunes a policy model with RL (e.g., PPO) to align model actions with human preferences, applied to both QA and tool-use behaviors.",
            "citation_title": "Deep Reinforcement Learning from Human Preferences",
            "mention_or_use": "mention",
            "model_or_agent_name": "RLHF (as applied to LMs / controllers)",
            "model_description": "Human comparisons are used to fit a reward model; an RL algorithm then optimizes the LM/controller policy to maximize this learned reward.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Tool-augmented QA, web interaction, long-form QA (e.g., WebGPT)",
            "interactive_task_type": "tool use / multi-step QA",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "policy network fine-tuned with reward model guidance",
            "training_method": "RL with learned human-preference reward (PPO commonly used)",
            "intervention_type": "training method",
            "intervention_description": "Fit a reward model from human comparisons and optimize the controller with RL to align outputs and action sequences with human preferences for better tool use.",
            "intervention_effect": "Reported to align outputs with human preferences and improve tool-use/QA quality (cited WebGPT example); survey notes limitations like task-specificity and bias without numeric metrics.",
            "hypothesized_cause_of_gap": "Supervised signals alone may not capture nuanced human preferences for procedural tasks; RLHF provides preference-driven optimization to close the behavior gap.",
            "uuid": "e931.8",
            "source_info": {
                "paper_title": "Tool Learning with Foundation Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Interface Unification (semantic / GUI / programming)",
            "name_full": "Interface unification strategies: semantic, GUI, and programming interfaces",
            "brief_description": "Architectural/engineering interventions proposed to standardize how LMs interact with diverse tools (textual action tokens, simulated GUI actions, or program/API calls) to improve transfer and generalization of tool use.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "Interface unification (design concept for controllers)",
            "model_description": "Three proposed unified interfaces: semantic (action names/text triggers), GUI (mouse/keyboard simulation), and programming (generate executable code/API calls) to standardize tool interactions.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "General multi-tool interactions across many tool types",
            "interactive_task_type": "tool use / programming-interface mediated tool use",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "design-level choices rather than single model architecture; enables mapping LM outputs to executable actions",
            "training_method": null,
            "intervention_type": "architectural change / interface design",
            "intervention_description": "Standardize action spaces so models can transfer learned action semantics across tools; programming interface leverages code models, GUI interface simulates human interaction, semantic uses action labels.",
            "intervention_effect": "Argued to facilitate generalization and reduce per-tool retraining; no experimental numeric effect reported in survey excerpt.",
            "hypothesized_cause_of_gap": "Diverse tool APIs and interfaces cause distributional mismatch between LM outputs and valid tool commands; unification reduces mismatch and enables transfer.",
            "uuid": "e931.9",
            "source_info": {
                "paper_title": "Tool Learning with Foundation Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Auto-GPT",
            "name_full": "Auto-GPT (multi-step multi-tool autonomous agent project)",
            "brief_description": "A practical system built on GPT-4 that chains reasoning, tool calls, and reflection to autonomously pursue long-horizon objectives across multiple tools; presented as an example of multi-tool long-term planning but not rigorously benchmarked in the paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "Auto-GPT",
            "model_description": "Agentic framework that uses an LLM to decompose goals, call multiple tools over time, and reflect on past actions to refine future decisions; primarily an application/project example.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Long-horizon multi-tool planning and execution",
            "interactive_task_type": "multi-step multi-tool planning / sequential decision-making",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "agent loop: plan â†’ act (tool calls) â†’ observe â†’ reflect",
            "training_method": "largely prompt / orchestration over a base LLM (no new training reported here)",
            "intervention_type": "system-level agent design (hybrid prompting + tool orchestration)",
            "intervention_description": "Chain-of-tasks orchestration with memory and reflection to enable long-term autonomous multi-tool workflows.",
            "intervention_effect": "Demonstrated capabilities in informal examples; this survey highlights Auto-GPT as pushing boundaries but provides no rigorous metrics.",
            "hypothesized_cause_of_gap": "LLMs need orchestrating logic, memory, and grounding to turn QA-style capabilities into reliable long-horizon interactive behavior.",
            "uuid": "e931.10",
            "source_info": {
                "paper_title": "Tool Learning with Foundation Models",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "rating": 2
        },
        {
            "paper_title": "WebGPT: Browser-assisted Question-Answering with Human Feedback",
            "rating": 2
        },
        {
            "paper_title": "Program-Aided Language Models",
            "rating": 2
        },
        {
            "paper_title": "SayCan: Grounding Language in Robotic Affordances",
            "rating": 2
        },
        {
            "paper_title": "Self-Ask: A Simple Question Decomposition Approach (Self-Ask / Self-Ask paper by Press et al.)",
            "rating": 1
        },
        {
            "paper_title": "Tool learning with foundation models",
            "rating": 2
        }
    ],
    "cost": 0.019798,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Tool Learning with Foundation Models</h1>
<p>Yujia Qin ${ }^{1}$, Shengding $\mathbf{H u}^{1}$, Yankai $\mathbf{L i n}^{2 <em>}$, Weize Chen ${ }^{1}$, Ning Ding ${ }^{1}$, Ganqu Cui ${ }^{1}$, Zheni Zeng ${ }^{1}$, Xuanhe Zhou ${ }^{1}$, Yufei Huang ${ }^{1}$, Chaojun Xiao ${ }^{1}$, Chi Han ${ }^{3}$, Yi Ren Fung ${ }^{3}$, Yusheng Su ${ }^{1}$, Huadong Wang ${ }^{1}$, Cheng Qian ${ }^{1}$, Runchu Tian ${ }^{1}$, Kunlun Zhu ${ }^{5}$, Shihao Liang ${ }^{8}$, Xingyu Shen ${ }^{1}$, Bokai Xu ${ }^{1}$, Zhen Zhang ${ }^{1}$, Yining Ye ${ }^{1}$, Bowen Li ${ }^{1}$, Ziwei Tang ${ }^{5}$, Jing Yi ${ }^{1}$, Yuzhang Zhu ${ }^{1}$, Zhenning Dai ${ }^{1}$, Lan Yan ${ }^{1}$, Xin Cong ${ }^{1}$, Yaxi Lu ${ }^{1}$, Weilin Zhao ${ }^{1}$, Yuxiang Huang ${ }^{1}$, Junxi Yan ${ }^{1}$, Xu Han ${ }^{1}$, Xian Sun ${ }^{7}$, Dahai $\mathbf{L i}^{7}$, Jason Phang ${ }^{4}$, Cheng Yang ${ }^{5}$, Tongshuang Wu ${ }^{6}$, Heng Ji ${ }^{3}$, Guoliang Li ${ }^{1}$, Zhiyuan Liu ${ }^{1 </em>}$, Maosong Sun ${ }^{1 *}$<br>${ }^{1}$ Tsinghua University, ${ }^{2}$ Renmin University of China, ${ }^{3}$ University of Illinois Urbana-Champaign, ${ }^{4}$ New York University, ${ }^{5}$ Beijing University of Posts and Telecommunications, ${ }^{6}$ Carnegie Mellon University, ${ }^{7}$ Zhihu Inc., ${ }^{8}$ ModelBest Inc.<br>qyj20@mails.tsinghua.edu.cn</p>
<h4>Abstract</h4>
<p>Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of recent powerful foundation models, artificial intelligence systems have the potential to be equally adept in tool use as humans. This paradigm, which is dubbed as tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation and comprehensive review of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. We recapitulate existing tool learning research and formulate a general tool learning framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate the generalization in tool learning. Considering the lack of a systematic tool learning evaluation in prior works, we experiment with 18 representative tools and show the potential of current foundation models in skillfully utilizing tools. Finally, we discuss several open problems that require further investigation for tool learning, such as ensuring safe and trustworthy tool use, enabling tool creation with foundation models, and addressing personalization challenges. Overall, we hope this paper could inspire future research in integrating tools with foundation models. Relevant codes and datasets are publicly available for further research exploration ${ }^{1}$.</p>
<p>"It is not only the violin that shapes the violinist, we are all shaped by the tools we train ourselves to use."</p>
<ul>
<li>Edsger W. Dijkstra</li>
</ul>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Contents</h1>
<p>1 Introduction ..... 4
2 Background ..... 6
2.1 Cognitive Origins of Tool Use ..... 6
2.2 Tool Categorization: A User-Interface Perspective ..... 7
2.3 Paradigm Shift of Foundation Models ..... 8
2.4 Complementary Roles of Tools and Foundation Models ..... 9
3 Tool Learning ..... 10
3.1 Components of Tool Learning ..... 10
3.1.1 Understanding the Components ..... 10
3.1.2 Connecting the Components ..... 11
3.2 The General Procedure: From Intent to Plan ..... 12
3.2.1 Understanding Intent and Tools ..... 12
3.2.2 Planning with Reasoning ..... 14
3.3 Training Models for Improved Tool Learning ..... 16
3.3.1 Learning from Demonstrations ..... 17
3.3.2 Learning from Feedback ..... 18
3.3.3 Generalizable Tool Learning ..... 19
4 Application and Experiment ..... 21
4.1 Evaluated Tools ..... 21
4.2 Experiments ..... 24
5 Discussion ..... 25
5.1 Safe and Trustworthy Tool Learning ..... 25
5.2 Tool Learning for Large Complex Systems ..... 27
5.3 From Tool User to Tool Maker: AI's Evolutionary Role ..... 27
5.4 From General Intelligence to Personalized Intelligence ..... 29
5.5 Tool Learning and Embodied Learning ..... 30
5.6 Knowledge Conflicts in Tool Augmentation ..... 31
5.7 Open Problems ..... 33
6 Conclusion ..... 34
Contributions ..... 34
A Case Study ..... 50
A. 1 3D Models ..... 50
A. 2 Stock ..... 52
A. 3 Making Slides ..... 53</p>
<p>A. 4 Movie Hunter ..... 58
A. 5 Search Engine ..... 59
A. 6 Chemicals Mining ..... 60
A. 7 Cooking Assistant ..... 61
A. 8 AI Painting ..... 62
A. 9 Navigating Knowledge Graphs ..... 63
A. 10 ALFWorld ..... 64
A. 11 Calculator ..... 66
A. 12 Weather ..... 66
A. 13 Online Shopping ..... 67
A. 14 Map ..... 68
A. 15 Processing Tables ..... 69
A. 16 Translation ..... 71
A. 17 Wikipedia ..... 72
A. 18 Database ..... 73</p>
<h1>1 Introduction</h1>
<p>Tools are extensions of human capabilities designed to enhance productivity, efficiency, and problem-solving in human activities. Since the dawn of civilization, tools have been integral to the very essence of our existence (Washburn, 1960). Tool creation and utilization are motivated by a deep-rooted desire to overcome our physical limitations and discover new territories. More specifically, with advancements in tools, we can accomplish increasingly complex tasks with ease and efficiency, liberating time and resources to pursue more ambitious ventures. As such, tools have served as the crucial foundation upon which our cultural and social practices are built, transforming our modes of learning, communication, working, and entertainment, infusing these domains with new dimensions of accessibility and interactivity (Gibson et al., 1993). Throughout history, it is undeniable that human beings have played a pivotal role in the invention and manipulation of tools, which is a striking manifestation of intelligence (Shumaker et al., 2011). Given the rise of Artificial Intelligence (AI), one natural question is, does AI possess the potential to be equally adept and capable as its creators?
The prerequisite of the manipulation of tools is a thorough comprehension of the tools' functionalities, as well as the ability to understand user intents and perform planning and reasoning for tool use. Before the advent of powerful foundation models (Bommasani et al., 2021), conducting tool-oriented AI research was exceedingly challenging. While certain basic tools could be fitted using shallow statistical models or deep neural models (Pomerleau, 1988; Mnih et al., 2013; Akkaya et al., 2019), their performance and stability remained inadequate to meet the demands of practical applications, let alone generalizing across various tools. This is due to the limitations of traditional supervised learning in capturing the complex operations essential for tool utilization and the insufficiency of trial-and-error approaches like reinforcement learning in mastering the extensive decision space associated with tool use. In a nutshell, the fundamental limitations in tool use by earlier AI lie in the insufficient capabilities of the models. Recently, the emergence of more capable foundation models, characterized by significantly improved capabilities, has rendered tool learning practicable. They have shown enormous semantic understanding capacity in diverse tasks, spanning the fields of natural language processing (NLP) (Brown et al., 2020), computer vision (CV) (Ramesh et al., 2022), biology (Jumper et al., 2021), etc. Additionally, they have demonstrated superior reasoning and decision-making abilities in complex interactive environments (Nakano et al., 2021). By harnessing the extensive world knowledge garnered during pre-training, they can perform grounded actions and interact with the real world. Notably, the emergence of ChatGPT (OpenAI, 2022) highlights the potential of foundation models to understand human intentions, automate intricate processes, and generate natural responses; the advent of GPT-4 (OpenAI, 2023) offers immense potential for multi-modal perception, which is essential to the real-world grounding ability.
Therefore, foundation models enable AI to harness tools, which can lead to more potent and streamlined solutions for real-world tasks. Foundation models are able to decipher complex data, simulate human-like planning capabilities, and generate a broad spectrum of outputs. Concurrently, specialized tools can be
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Tool learning paradigm aims to combine the strengths of specialized tools and foundation models.</p>
<p>employed to refine and target specific goals. The amalgamation of tools and models unveils vast potential where sophisticated procedures can be automated with limited human involvement. This paradigm, dubbed as tool learning with foundation models in this paper (Figure 1), aims to combine the strengths of specialized tools and foundation models, thereby culminating in greater accuracy, efficiency, and autonomy in problemsolving. Recent research has shed light on foundation models' potential to exhibit a level of dexterity and finesse in tool use (Lazaridou et al., 2022; Nakano et al., 2021; Cobbe et al., 2021; Thoppilan et al., 2022; Huang et al., 2022b; Ahn et al., 2022; Yao et al., 2022a,b; Schick et al., 2023; Wu et al., 2023; Bubeck et al., 2023). Despite these breakthroughs, the efforts mainly focus on applying foundation models to specific tasks and domains with delicate algorithm designs. The current understanding of tool learning is still not comprehensive enough to estimate its characteristics and future developments. We believe that it is crucial to examine and summarize the current progress of tool learning with foundation models to explore their potential and challenges and to better pave the way for future technological advancements.
In this paper, we conduct a systematic investigation and comprehensive review of tool learning, attempting to build a full grasp of the key challenges, opportunities, and directions in this field. Before delving into the tool learning framework, we introduce essential backgrounds (Â§ 2), covering both tools and foundation models and their interaction. Specifically, we first recapitulate the cognitive origins of tool use in human history and its potential implications for tool use in AI systems (Â§ 2.1), followed by a categorization of tools from the perspective of the user interface (Â§ 2.2). Then we review the AI paradigm shift brought by foundation models and highlight the emergence and significance of tool learning (Â§ 2.3). After that, we discuss the complementary roles of tools and foundation models, and argue that integrating both can bring various advantages, such as improving interpretability, enhancing robustness, and delivering better user experiences (Â§ 2.4).
We present a comprehensive literature review for existing exploration in tool learning. While previous works often focus on specific aspects in isolation, we strive to formulate a general tool learning framework (Â§ 3.1), which comprises the controller (typically modeled using a foundation model), tool set, environment, perceiver, and human. Based on the unified framework, we review existing works of tool learning, highlight core research problems, and introduce their existing solutions as well as future explorations. The whole procedure (Â§ 3.2) of tool learning starts with a user instruction, and models are required to make an executable plan for tool execution. To bridge user instructions with appropriate tools, models should first learn to understand the user intents underlying the instruction (i.e., intent understanding) and understand the functionalities and usage of tools (i.e., tool understanding). Models should also learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task with the appropriate tools. Regarding the training strategy (Â§ 3.3) to facilitate models for improved tool utilization, we conclude with two mainstream methods: learning from demonstrations and learning from feedback. We discuss how to construct effective training supervision under different settings. To facilitate transferring the learned tool-use skills to new tools and situations, i.e., generalizable tool learning, it is important to design a unified interface that enables the model to interact with different tools in a standardized manner.
Considering the lack of a systematic tool learning evaluation in prior works, we conduct experiments (Â§ 4) on 18 representative tools based on our framework to investigate the efficacy and limitations of foundation models in tool manipulation. We demonstrate that state-of-the-art foundation models (e.g., ChatGPT) can effectively use tools to solve tasks with simple prompting. These results highlight the potential of using the foundation model as a general agent for tool learning.
Finally, we discuss the remaining important research topics (Â§ 5) for applying our general framework to real-world scenarios, including (1) safety and trustworthiness, where we emphasize the potential risks from adversaries, governance, and trustworthiness. We contend that careful considerations are necessary before deploying tool learning models in high-stakes scenarios (Â§ 5.1); (2) tool learning for large complex systems, where we showcase the unique characters of large complex systems and discuss the challenges in applying tool learning to these systems, such as complicated knowledge and function learning, representative data sampling with privacy concerns, and the strict requirements of efficient tool learning (Â§5.2); (3) tool creation, where we discuss the possibility that AI can also create new tools, challenging the long-held beliefs about what makes humans unique (Â§ 5.3); (4) personalized tool learning, where models provide tailored assistance to users in tool use. We highlight the challenges of aligning user preference with tool manipulation and introduce the shift from reactive to proactive systems, and the privacy-preserving concerns (Â§ 5.4); (5) embodied learning, where the intersection of tool learning and embodied agent enables digital embodiment and manipulation of embodied tools (Â§ 5.5); (6) knowledge conflicts in tool augmentation, where we review how tools can be leveraged to enhance models' generation and the practical problems of knowledge conflicts, which can lead to inaccurate and unreliable model predictions (Â§ 5.6); (7) other open problems, such as viewing tool use capability as a measure for machine intelligence and tool learning for scientific discovery (Â§ 5.7). Overall, we</p>
<p>hope this paper could inspire further research in integrating tools with foundation models and developing more intelligent and capable AI systems.</p>
<h1>2 Background</h1>
<p>In this section, we first discuss the cognitive origins of human tool use (Â§ 2.1), followed by a tool categorization through the lens of the user interface (Â§ 2.2). Then we review the recent AI paradigm shift brought by foundation models (Â§ 2.3) and its significance in tool learning. After that, we examine the respective roles of specialized tools and foundation models in problem-solving, and discuss the benefits and challenges of their integration (Â§ 2.4).</p>
<h3>2.1 Cognitive Origins of Tool Use</h3>
<p>Tools have played a critical role in the long history of thousands of years of human evolution. Tools are commonly viewed as extensions of human beings, just as ancient fighting equipment and agricultural machinery were. Tool use is defined as a unique characteristic of human beings that is distinguished from other species (Von Eckardt, 1995). Throughout evolution, the ability to use tools has been essential for animals, particularly those with advanced intellectual development (Shumaker et al., 2011). For example, chimpanzees have been observed using stones or other materials to crack nuts (Boesch et al., 2019), while New Caledonian crows can craft and utilize two distinct types of hook tools to aid in capturing prey (Hunt, 1996). However, human tool behavior diverges from these observations in several ways. Humans can create much more complicated tools than other animals, such as converting our actions into fundamentally different mechanical actions in tools like hammers and pencils (Frey, 2007). Additionally, we can harness natural forces such as wind turbines to create tools (Shumaker et al., 2011). This ability may be attributed to our deep comprehension of cause-and-effect relations, which allows us to engage in technical reasoning (Osiurak \&amp; Reynaud, 2020).
Neural Basis of Tool Use. To better understand human tool use behaviors, researchers analyze the neural basis of tool observation and execution. It is verified that humans have parietal systems involved in grasping objects and using tools, and the anterior supramarginal gyrus activation of observing tool use is typical of human subjects, of which macaques do not exhibit (Orban \&amp; Caruana, 2014). This neurocognitive bases of tool observation may be related to the origins of cumulative technological evolution (e.g., the improvement in the efficiency and complexity of human tools and techniques over generations (Osiurak \&amp; Reynaud, 2020)) and some other human phenomena (Reynaud et al., 2019). While for the tool execution, researchers hold different views on manipulation-based versus reasoning-based approaches (Osiurak \&amp; Badets, 2016). The former claims that tool use has to be supported by the simulation of sensorimotor experiences, and the latter demonstrates the importance of reasoning based on mechanical knowledge in tool use. Nevertheless, the overall trend in cognitive science is understanding cognition as an enactive process that emphasizes interaction with the external world (Engel et al., 2013), and the feedback from observation, communication, and hands-on practice is important for mastering tool use.
Three Intelligence Levels of Tool Use. Besides, there are specific frameworks designed to discuss the level of intelligence represented by human tool use. For instance, "intoolligence" (Osiurak \&amp; Heinke, 2018) divides the tool use behavior into three modes: assistive tool use is usually passive and unaware (e.g., walking in the rain shelter corridor); arbitrary tool use requires active interaction (e.g., driving, using smart phones); free tool use further needs to comprehend and choose appropriate tools for the scenarios (e.g., cooking new dishes). In this framework, the three modes of tool use present a progressive relationship, and the authors assume that the key cognitive process for achieving free tool use is technical reasoning, which allows someone to learn new actions by observing others using, selecting, or making a tool instead of numerous practices.
Transition from Physical Tools to Conceptual Tools. Apart from tools in the physical world, we can also turn to more abstract tools. Take cognitive tools (Heyes, 2018) as an example: it refers to an auxiliary aid that facilitates higher-order thinking (e.g., multi-step critical analysis, the generation of creative problem-solving solutions). Cognitive tools can be classified based on the functionalities they provide (Lajoie \&amp; Derry, 2013). These include (1) supporting cognitive processes (e.g., documenting intermediate reasoning outcomes), (2) alleviating lower-level cognitive load to free up resources for advanced-level thinking, (3) enabling learners to engage in activities out of their reach and (4) allowing learners to generate and test hypotheses (e.g., simulated diagnoses for medical students).</p>
<p>Bridging the Gap between Human and Machine Tool Use. First, the abilities to manipulate tools are deeply rooted in our cognitive and perceptual systems and have evolved over millions of years. In contrast, foundation models rely primarily on statistical patterns of pre-training data, and significant gaps still exist between the tool-use capabilities of foundation models and their human counterparts. Humans can perceive the properties of tools, understand their functionalities, and identify the appropriate tools for each task. Gaining insights from this, we investigate and discuss how foundation models can learn such a process in $\S 3.2$. Second, humans excel at breaking down complex tasks into smaller, manageable subtasks and deftly manipulating tools to accomplish each sub-task. However, foundation models lack the physical embodiment and sensory experience necessary to fully understand and utilize tools. As a result, these models often struggle with tasks that require higher-order reasoning and adaptivity, and they cannot trustfully integrate multiple sources of knowledge and tools effectively. We will discuss how to better make executable plans leveraging models' reasoning abilities in $\S 3.2$. Furthermore, current algorithms for adapting foundation models to learn specific tools generally require huge amounts of supervised data (Nakano et al., 2021; Reed et al., 2022), hindering their generalization and transferability to broader types of tools or novel situations. Hence we first summarize the training strategies for tool learning (Â§ 3.3.1 and $\S 3.3$.2) and discuss how to facilitate the generalization and transferability of tool learning (Â§ 3.3.3).</p>
<h1>2.2 Tool Categorization: A User-Interface Perspective</h1>
<p>The growing number and complexity of tools in our world make it increasingly important to understand and group them in a meaningful way. A system for classifying these tools helps us better grasp their uses, benefits, and potential for growth. In this paper, our focus is particularly on those tools that can be manipulated using instructions in conjunction with foundation models. We introduce a taxonomy that sorts these tools based on their modes of expression and interaction. As depicted in Figure 2, this taxonomy incorporates three levels of interaction, arranged from the most tangible to the least. The physical level involves direct physical interactions with tools. The graphical user interface (GUI) level facilitates user interaction with visual representations of tools. The program level involves users engaging directly with the underlying source code of tools.
Physical Interaction-based Tools. We start with the most tangible genre of tools, physical interaction-based tools. As the name suggests, this class of tools involves direct interactions with the physical world, including devices like robots, sensors, and wearables that could physically impact the environment. Physical interaction tools have the capability to sense and respond to the physical environment of users, making them useful in a wide range of applications, from manufacturing to healthcare and education. Physical interaction tools are close to the real world, and they have the potential to substantially improve efficiency and productivity. For example, robots can perform from simple to intricate, even adventurous tasks to reduce human errors and labor costs. Sensors can collect valuable data, such as temperature and pressure, allowing for real-time monitoring and optimization of industrial processes. Wearables, on the other hand, provide users with a personalized experience by tracking physiological or environmental parameters, promoting health and safety. It is worth noting that although the output of such tools interacts with the real world at the physical level, users may also create the input of the tools at the GUI or source code level.</p>
<p>GUI-based Tools. Some tools allow users to manipulate them through an interactive interface, i.e., visual representations of tools, with pre-defined operations. These tools, defined as GUI-based tools, do not have a direct impact on the physical world. The GUI interface typically includes buttons, menus, text boxes, and other graphical elements that allow users to interact with the underlying system. These tools are extensively employed in various industries and applications such as software development, data analysis, and design. Particularly, GUI-based tools can improve productivity by streamlining workflows and automating repetitive tasks. GUI-based tools could considerably simplify complex tasks and reduce the learning curve for nontechnical users. From this viewpoint, tool learning with foundation models share the same primary goal, which simplifies intricate tasks to a natural language format. Representative GUI-based tools are usually well-developed software such as browsers, Microsoft Office, Adobe PhotoShop, etc. These applications showcase the versatility that graphical interfaces can provide and enable users to access and manipulate complex features within the software. On the other hand, the main limitation of GUI-based tools is that they may not provide the flexibility and customizability of command-line interfaces or APIs. In specific scenarios that require rapid and mass responses, as well as greater and flexible control, the visual interface may not always be the most effective way to interact with a system.
Program-based Tools. The innermost layer of tools that users can access is the source code, offering a high degree of flexibility for the input and output of these program-based tools. Program-based tools are software tools primarily designed for use through programming interfaces rather than visual interfaces. They can take</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Tool categorization from the perspective of the user interface: (1) physical interaction-based tools, (b) GUI-based tools, and (c) program-based tools.
various forms, including declarative languages, programming libraries, software development kits (SDKs), and even neural network-based tools. These tools are typically used by developers or technical users who possess a deeper understanding of the underlying data, system or technology, with which the users could complete complex software applications. The main advantage of program-based tools is that they provide greater flexibility and customizability than GUI-based tools, and users can build more sophisticated solutions for current problems. As a result, such tools also have a steeper learning curve than GUI-based tools, they require a greater degree of technical expertise and programming knowledge, which may not be accessible to non-technical users. For example, program-based tools can be more time-consuming to set up and configure and may require more maintenance and support in the learning process. It is noteworthy that, although these tools pose difficulties for human beings in terms of the learning curve, they may not have the same level of challenges for foundation models.
It can be seen that the above three interaction modes have varying levels of connectivity with the tool kernel. They are not strictly mutually exclusive but indicate a tendency to intermingle with each other. Human beings have the ability to deal with complex tasks by flexibly executing tools of different types. In this paper, we contend that regardless of the tool type, it is fundamentally possible to leverage foundation models to execute them by setting up intermediary interfaces. We will introduce ways to unify the interface of different tools for foundation models in Â§ 3.3.3.</p>
<h1>2.3 Paradigm Shift of Foundation Models</h1>
<p>In recent years, the field of natural language processing (NLP) has undergone a paradigm shift, marked by the advent of pre-trained language models (PLMs) (Devlin et al., 2019; Bommasani et al., 2021; Han et al., 2021). Prior to this breakthrough, NLP was a challenging field that necessitated designing separate learning objectives for distinct research domains, such as dependency parsing (KÃ¼bler et al., 2009), named entity recognition (Nadeau \&amp; Sekine, 2007), and summarization (Nenkova \&amp; McKeown, 2012). Despite the successful design of effective models and methods for these specific tasks, the separated nature of this paradigm impeded progress toward a holistic comprehension of language, thereby limiting its potential.
The invention of PLMs changes this paradigm. Building on Transformers (Vaswani et al., 2017), PLMs are trained on massive corpora, from which general linguistic ability and world knowledge are learned. This technique has expedited the unification of NLP tasks, giving rise to the pre-train-then-fine-tune paradigm, which has achieved new state-of-the-art performance on several NLP benchmarks, such as GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a). At this stage, each task shares the same starting point and only diverges as the task-specific adaptation proceeds. The fusion of task paradigms is still ongoing. T5 (Raffel et al., 2020) transforms all NLP tasks into a text-to-text format with textual descriptions, while GPT-3 (Brown et al., 2020) has discovered that introducing appropriate textual prompts can yield the desired output for specific tasks. Prompts, essentially serving as a natural language interface, are widely believed to stimulate the knowledge learned by PLMs during pre-training. Prompts can enable downstream tasks to be executed without updating model parameters for big models such as GPT-3. Research even suggests that with appropriate prompt guidance, models can perform complex reasoning tasks (Wei et al., 2022c; Wang et al., 2022a). Also, prompts formulated in a natural language format possess remarkable generalization capabilities. Specifically,</p>
<p>models that have undergone fine-tuning using diverse instructions are able to effectively generalize to new, unseen data (Wei et al., 2022a; Sanh et al., 2022). Overall, prompts demonstrate a proof-of-concept that uses PLMs as the underlying infrastructure and natural language as the medium to uniformly perform various tasks. A highly successful example is ChatGPT, where all the natural language understanding and generation processes are accomplished through conversational interactions.
Nevertheless, there exist numerous tasks that transcend the scope of purely natural language. For instance, generating presentation slides ${ }^{2}$, constructing 3D models via CAD applications, and scheduling meetings through the analysis of team member calendars are examples of complex tasks that have not been defined in traditional artificial intelligence. Fortunately, the strong generalization ability of PLM enables us to use natural language as a medium to accomplish these tasks by manipulating tools (Zeng et al., 2022). Essentially, the key to tool learning is to decompose complex tasks into sub-actions, tokenize actions in the form of natural language and convert them into executable instructions that can be understood by specific tools. Language models serve as "translators", making complex tasks more accessible to individuals without specialized technical knowledge. The potential applications of tool learning are vast and exciting, ranging from automated customer service and personal assistants to self-driving cars and even space exploration. By enabling machines to understand and interact with human language in a more natural and nuanced way, we can unlock new possibilities for collaboration and problem-solving that were previously impossible. We anticipate that tool learning will prove instrumental in facilitating the integration of diverse tasks through shared tooling. Thus, while natural language interfaces have enabled unification within the realm of language (Hao et al., 2022), the challenges posed by non-linguistic tasks necessitate a more advanced approach to leveraging both natural language and tool learning. By harnessing the power of natural language, we can create systems that are capable of understanding and adapting to the complex and dynamic world around us, opening up new avenues for innovation and discovery.</p>
<h1>2.4 Complementary Roles of Tools and Foundation Models</h1>
<p>The integration of specialized tools and foundation models represents a promising approach for harnessing the unique strengths of both. By incorporating foundation models' understanding and reasoning capabilities into specialized tools, we can create intelligent tools capable of performing more complex tasks than either specialized tools or foundation models alone. Specifically, the amalgamation of both confers a multitude of benefits as follows.</p>
<p>Benefits of Tools. Tools that are designed to streamline concrete and specific objectives bring several benefits for tool learning: (1) Mitigation for Memorization. Although foundation models have demonstrated an exceptional ability to memorize (Carlini et al., 2021, 2022, 2023), they are not capable of memorizing every piece of training data. Furthermore, foundation models are often prompted with a relatively short context during model generation, thus not all the memorized knowledge can be properly steered (Mialon et al., 2023). Additionally, memorization alone does not support the real-time coverage of up-to-date knowledge, especially in light of the potentially infinite possibilities of novel requests from users. Besides, foundation models are also criticized to hallucinate knowledge (Roller et al., 2021; Shuster et al., 2021) by generating seemingly plausible but non-factual content. This is unacceptable in applications like financial transactions that require the results are $100 \%$ correct. Given the above factors, it is necessary to augment foundation models with real-time tool execution to mitigate limitations in memorization. For instance, a significant proportion of the memorization burden can be offloaded to the search engine and database systems (of different modes) if foundation models can learn how to utilize it. (2) Enhanced Expertise. Specialized tools are designed to cater to specific domains with functionalities that are not available in foundation models. As a result, they are better suited to address the needs of domain-specific tasks, such as Wolfram ${ }^{3}$ for scientific calculation, through the utilization of tailored algorithms. Instead of solely relying on the foundation model to accomplish the task, models could invoke appropriate tools to generalize to a wider range of tasks that are beyond their capabilities. (3) Better Interpretability. Foundation models are criticized for lacking transparency in their decision-making process (Linardatos et al., 2020), which can be a significant concern in applications such as healthcare or finance, where interpretability is critical for making informed decisions. In contrast, the process of tool execution reflects the whole process of how models solve complex requests, which allows for better interpretability and transparency. Users can easily understand why certain tools are called and how they contribute to the final output, which can improve trust and facilitate human-machine collaboration. (4) Improved Robustness. Foundation models are susceptible to adversarial attacks (Wallace et al., 2019; Jin</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>et al., 2020), where slight modifications to the input can flip the model prediction. This is because these models heavily rely on statistical patterns in the training data. Conversely, tools are designed specifically for their intended use cases, which may be agnostic to the input perturbation. This makes tools more resistant to adversarial attacks. Overall, incorporating tools into the workflow of foundation models can improve the robustness of the system and reduce the risk of malicious attacks. This harmonious interplay between tools and models can enhance the reliability of the system against unpredictable real-world environments. In Â§ 4 and appendix A, we use concrete examples to show how tools can enhance the model's capabilities in various tasks.</p>
<p>Benefits of Foundation Models. Foundation models can provide a solid basis for understanding, planning, reasoning, and generation, which bring several benefits for tool learning as follows: (1) Improved DecisionMaking and Reasoning Abilities. Foundation models are trained on vast amounts of data, enabling them to acquire world knowledge across a wide range of domains. If properly steered, such knowledge can be wielded to perform decision-making and planning over prolonged time horizons Huang et al., 2022a). Besides, foundation models have demonstrated remarkable reasoning abilities (Wei et al., 2022c; Wang et al., 2022a), thereby enabling them to extrapolate the consequences of actions and make judicious decisions. These reasoning abilities are particularly useful for tasks requiring a deep understanding of cause-and-effect relations (Â§ 3.2.2). (2) Better User Experience. Benefitting from the powerful intent understanding capability of foundation models, tool learning could revolutionize the way we interact with machines and liberate users from the cognition load, allowing them to engage in higher-order thinking and decision-making processes. This, in turn, fosters a seamless and more natural language-based interaction paradigm that revolutionizes traditional graphical user interfaces (GUIs). The user only needs to provide high-level guidance and direction, and the model will seamlessly comprehend the user's intent, thereby delivering more personalized and precise responses. In addition, tool learning has the potential to democratize access to complex tools. With the aid of foundation models, even novice users can easily and quickly get started with a new tool, regardless of their prior experience or technical expertise. This not only reduces the barriers to entry for new users but also unlocks a wealth of possibilities for innovation and creativity. However, it should be noted that human-model collaboration in tool use also triggers ethical concerns, which will be discussed in Â§ 5.7.</p>
<h1>3 Tool Learning</h1>
<p>In this section, to unify existing efforts and promote a comprehensive understanding of tool learning, we first present a general framework, which encompasses four fundamental components, namely tool set, environment, controller, and perceiver, as detailed in $\S 3.1$. Subsequently, we provide an elaborate discussion of the general procedure of tool learning in $\S 3.2$. Lastly, we delve into the training methods for tool learning and discuss how to achieve generalizable tool learning in $\S 3.3$.</p>
<h3>3.1 Components of Tool Learning</h3>
<p>How can we enable foundation models to leverage the strengths of specialized tools to accomplish complex tasks? To better answer this question, we frame tool learning with four components as shown in Figure 3. Each component has its own characteristics and functions (Â§ 3.1.1), but they also interact with each other closely (Â§ 3.1.2).</p>
<h3>3.1.1 Understanding the Components</h3>
<p>We first introduce each component and explain how they contribute to the tool learning process.
Tool Set. Serving as the fundamental ingredient of tool learning, the tool set $\mathcal{T}=\left{\mathcal{T}<em 2="2">{1}, \mathcal{T}</em>$ can have different interfaces. In the following sections, we mainly take Application Programming Interface (API) as the example to illustrate how to interact with tools. Here we define an API as any function that can take the output of the foundation model as its input. For instance, for a weather API, the input to the API may be a location and time, and the output may contain the temperature or wind speed.}, \cdots\right}$ contains a collection of different tools that have different functionalities. As we have elaborated in $\S 2.2$, a tool in $\mathcal{T</p>
<p>Environment. The environment $\mathcal{E}$ is the world where the tools operate, which provides the perceiver with the execution results of tools. It provides the infrastructure necessary for tool execution, which can be either virtual or real. The former refers to a simulated environment that allows the model to interact with a digital</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of the tool learning framework, where we display the human user and four core ingredients of the framework: tool set, controller, perceiver, and environment. The user sends an instruction to the controller, which then makes decisions and executes tools in the environment. The perceiver receives feedback from both the environment and the user and summarizes them to the controller.
representation of the tool, while a real environment involves actual interaction with the physical tool. Virtual environments have the advantage of being easily accessible and replicable, allowing for more cost-effective training for models. However, virtual environments may not fully replicate the complexities of the real-world environment, leading to overfitting and poor generalization (Hansen et al., 2021). On the other hand, real environments provide a more realistic context but may be more challenging to access and involve greater costs.</p>
<p>Controller. The controller $\mathcal{C}$ serves as the "brain" for tool learning framework and is typically modeled using a foundation model. The purpose of the controller $\mathcal{C}$ is to provide a feasible and precise plan for using tools to fulfill the user's request. To this end, $\mathcal{C}$ should understand user intent as well as the relationship between the intent and available tools, and then develop a plan to select the appropriate tools for tackling tasks, which will be discussed in $\S 3.2 .1$. In cases where the query is complex and targets a high-level task, $\mathcal{C}$ may need to decompose the task into multiple sub-tasks, which requires foundational models to have powerful planning and reasoning capabilities (Â§ 3.2.2).</p>
<p>Perceiver. The perceiver $\mathcal{P}$ is responsible for processing the user's and the environment's feedback and generating a summary for the controller. Simple forms of feedback processing include concatenating the user and environment feedback or formatting the feedback using a pre-defined template. The summarized feedback is then passed to the controller to assist its decision-making. By observing this feedback, the controller can determine whether the generated plan is effective and whether there are anomalies during the execution that need to be addressed. Under more complex scenarios, the perceiver should be able to support multiple modalities, such as text, vision, and audio, to capture the diverse nature of feedback from the user and the environment.</p>
<h1>3.1.2 Connecting the Components</h1>
<p>Formally, assume we have a tool set $\mathcal{T}$, which the controller can utilize to accomplish certain tasks. At time step $t$, environment $\mathcal{E}$ provides feedback $e_{t}$ on the tool execution. The perceiver $\mathcal{P}$ receives the user feedback $f_{t}$ and the environment feedback $e_{t}$, and generates summarized feedback $x_{t}$. Typically, the perceiver can be achieved by pre-defined rules (e.g., concatenating $f_{t}$ and $e_{t}$ ) to form $x_{t}$, or modeled with complex neural models. The controller $\mathcal{C}$ generates a plan $a_{t}$, which selects and executes an appropriate tool from $\mathcal{T}$. This process can be formulated as the following probability distribution:</p>
<p>$$
p_{\mathcal{C}}\left(a_{t}\right)=p_{\theta_{\mathcal{C}}}\left(a_{t} \mid x_{t}, \mathcal{H}_{t}, q\right)
$$</p>
<p>where $\theta_{\mathcal{C}}$ denotes the parameters of $\mathcal{C}, q$ denotes the user query or instruction, and $\mathcal{H}<em s="s">{t}=\left{\left(x</em>\right)\right}}, a_{s<em t="t">{s=0}^{t-1}$ denotes the history feedback and plans. In its simplest form, a generated plan $a</em>$ can simply be a specific action for tool</p>
<p>execution. $\mathcal{C}$ can also synergize its reasoning process with the action prediction, where $a_{t}$ may additionally contain the reasoning traces that explain which sub-task should be solved next and which tool to choose for solving the sub-task. It is worth noting that if the dependence on $x_{s}$ is removed from Equation (1), the resulting probability distribution becomes equivalent to autoregressive language modeling. From this perspective, the controller additionally grounds the foundation model to the environment and the tool set. Moreover, we can factorize Equation (1) as follows:</p>
<p>$$
p_{\theta_{\mathcal{C}}}\left(a_{t} \mid x_{t}, \mathcal{H}<em _mathcal_T="\mathcal{T">{t}, q\right)=\sum</em><em _theta__mathcal_C="\theta_{\mathcal{C">{i} \in \mathcal{T}} p</em>}}}\left(a_{t} \mid \mathcal{T<em t="t">{i}, x</em>}, \mathcal{H<em _theta__mathcal_C="\theta_{\mathcal{C">{t}, q\right) \times p</em>}}}\left(\mathcal{T<em t="t">{i} \mid x</em>, q\right)
$$}, \mathcal{H}_{t</p>
<p>The decomposition reveals that the construction of the plan $a_{t}$ involves two subtasks: selecting the appropriate tool based on the user intent and deciding the actions to execute using the selected tool. For instance, given an instruction such as "I want to book a flight to Beijing next week", the controller $\mathcal{C}$ first infers that the user's goal is to reserve a flight, with Beijing as the destination and the next week as the travel time. The model then selects the airline reservation system as the tool. Finally, it inputs the time and destination as the preliminary plan. In the process of making a reservation, we may face unexpected situations such as the unavailability of flights to Beijing in the next week. To cope with these anomalies, we can further equip $\mathcal{C}$ with the ability to reason about the current context and generate alternative plans, as we will discuss in detail in Â§ 3.2.2.
After a plan $a_{t}$ is generated, it will be executed in $\mathcal{E}$, and the resulting feedback $e_{t+1}$ from $\mathcal{E}$ will be passed on to the perceiver. The above process repeats for multiple rounds until the controller accomplishes the task. The overall objective is to find an action sequence $\left{a_{t}\right}$ that ultimately fulfills the task specified by the user instruction $q$. Note after tool execution, the controller may additionally integrate the execution results into a plausible response for the user (see details in Â§ 5.6).</p>
<h1>3.2 The General Procedure: From Intent to Plan</h1>
<p>As formulated in Â§ 3.1.2, the general procedure of tool learning necessitates intricate interplay among different components. In this section, we will further elaborate on the key issues involved in this procedure.</p>
<h3>3.2.1 Understanding Intent and Tools</h3>
<p>To accurately fulfill the task specified by the user query $q$, the controller needs to understand two aspects: (1) the underlying intent of the user, which involves recognizing and formalizing the natural language $q$ as a high-level task (i.e., intent understanding); (2) the tool set $\mathcal{T}$, which entails comprehending the functionality and objective of each tool within it (i.e., tool understanding). By understanding both aspects, the controller can bridge the gap between the user's intent and the tool set, which is the pre-requisite for connecting controller $\mathcal{C}$, the user, and tool set $\mathcal{T}$ in Figure 3.</p>
<p>Intent Understanding. Understanding user intent is a long-standing research topic in NLP (Jansen et al., 2007; Sukthankar et al., 2014), which involves comprehending the underlying purpose of a user query. Intent understanding is essential in scenarios requiring human-computer interaction, such as developing advanced conversational agents capable of conducting intricate and nuanced dialogues with users. It requires learning a mapping from the instruction space to the model's cognition space. By accurately identifying the user intent, the controller can provide more personalized responses with a better user experience.
Recent explorations in instruction tuning (Wei et al., 2022a) demonstrate that foundation models can possess extraordinary proficiency in comprehending user instructions. Prior work has shown that fine-tuning large language models on a collection of datasets templated with human instructions allows models to generalize even to instructions for unseen tasks (Wei et al., 2022a; Mishra et al., 2022; Sanh et al., 2022; Bach et al., 2022; Ouyang et al., 2022). Promisingly, such generalization ability can further be enhanced by scaling up both the model size and the quantity or diversity of training instructions (Iyer et al., 2022). Despite the impressive intent understanding capabilities, challenges still exist in real-world tool learning scenarios: (1) Understanding Vague Instructions. The first challenge is dealing with the inherent vagueness and ambiguity in the user query. Many user queries are inherently imprecise and can even be polysemous, requiring the controller to rely on contextual cues and background knowledge to infer the user's intended meaning. One possible solution is to actively interact with users to clarify any ambiguity, such as asking for clarifications about a previous user query. (2) Generalization to Diverse Instructions. Another challenge is having the models generalize to more diverse user instructions. As the intent space is theoretically infinite, it is almost impractical for foundation models to be exposed to every real-world intention during training. In addition, the challenge of personalization arises from the fact that each individual has their own unique way of expressing intentions,</p>
<p>Zero-shot Prompting: Here we provide a tool (API) "forecast_weather(city:str, N:int)", which could forecast the weather about a city on a specific date (after N days from today). The returned information covers "temperature", "wind", and "precipitation".
Please write codes using this tool to answer the following question: "What's the average temperature in Beijing next week?"</p>
<p>Few-shot Prompting: We provide some examples for using a tool. Here is a tool for you to answer question:
Question: "What's the temperature in Shanghai tomorrow?"
return forecast_weather("Shanghai", 1) ["temperature"]
Question: "Will it rain in London in next two days?"
for i in range (2):
if forecast_weather("London", i+1)["precipitation"] &gt; 0:
return True
return False
Question: "What's the average temperature in San Francisco next week?"
Figure 4: Examples of zero-shot and few-shot prompting for tool understanding. The prompts are constructed by describing the functionalities (zero-shot prompting) or giving usage examples (few-shot prompting) of a weather API.
which requires the model to adapt to the diverse expressions of intent of different individuals. One solution is to incorporate more diverse training data that covers a wide range of real-world scenarios, thereby enabling the models to learn the nuances of different instructions. Another solution is to leverage user feedback and actively adapt the model to individual users, i.e., personalized tool learning (Â§ 5.4).</p>
<p>Tool Understanding. As noted by Hernik \&amp; Csibra (2009), when learning to utilize a specific tool, human beings perceive it as an object with particular functions, engaging in a cognitive process to understand its purpose and operation. By observing goal-directed demonstrations and following actions performed by other people, they gradually acquire the necessary knowledge and skills to use the tools effectively. Similarly, this understanding process is crucial for successfully solving tasks with tools. Analogously, a comprehensive understanding of the tools' functionalities is indispensable for enabling the controller to use tools proficiently. The process of tool understanding encompasses grasping what the tool is used for and how to use the tool. Take the case of a calculator: the controller needs to know that a calculator is intended for arithmetic operations, its input should be numbers and mathematical operators, and its output should be a computed value.
In real-world scenarios, tools are typically accompanied by a manual (or tutorial), which provides sufficient relevant details about their functionalities and usage. Endowed with strong few-shot learning (Brown et al., 2020) and zero-shot learning (Wei et al., 2022a) capabilities, foundation models can be prompted to unravel tools' functionalities and comprehend how to use them. To this end, we can construct suitable task-specific prompts either through manual design (Vemprala et al., 2023) or retrievial (Zhou et al., 2023). These prompts should describe the API functionalities or exemplify with demonstrations of their usage.
We categorize two prompting approaches as shown in Figure 4: (1) zero-shot prompting, which describes API functionalities, their input/output formats, possible parameters, etc. This approach allows the model to understand the tasks that each API can tackle; (2) few-shot prompting, which provides concrete tool-use demonstrations to the model. By mimicking human behaviors from these demonstrations, the model can learn how to utilize these tools. We provide experimental results of both prompting methods in $\S 4$.
Prompting has been widely adopted as a lightweight approach to teach foundation models about tools (Yao et al., 2022b; Driess et al., 2023; OpenAI, 2023) with minimum human effort. Prompts can be easily adjusted to accommodate changes of tools. For instance, when tools are modified or upgraded, we can flexibly rewrite the prompts to adapt the model behaviors. Despite these advantages, prompting methods still face several challenges. First, since the effectiveness of prompting depends a lot on the model, smaller or less capable models cannot understand prompts well. Second, prompting is restricted by input context length. Although foundation models have been shown to learn to use simple tools through prompts, the situation may be more</p>
<p>challenging with multiple complex tools with long descriptions. Especially when the tool set greatly expands, providing all possible tools within a prompt becomes infeasible given the limited context length.
A potential solution is to add an intermediate stage of tool selection, which first retrieves a small set of tools that are most suitable for the task at hand. Another solution is fine-tuning, which optimizes models with concrete tool-use examples to understand tools. This process involves leveraging the rich knowledge obtained from human tool-use experiences. When deployed in practice, a fine-tuned model alleviates the need for incorporating tool definitions in the input, which shrinks the input length and accelerates model inference. However, a major limitation of fine-tuning is that it often requires extensive human annotations or demonstrations. Additionally, tools are frequently updated, and fine-tuned models will need to be retrained with updated examples, incurring additional costs. More discussion about the tuning-based solution is left in $\S 3.3$.</p>
<h1>3.2.2 Planning with Reasoning</h1>
<p>As discussed in $\S 3.2 .1$, understanding the intent and tools lays a solid foundation for planning. Nevertheless, it is still insufficient for tackling intricate tasks. The user query $q$ often implies a complex task that should be divided into multiple sub-tasks with proper sequencing, thereby necessitating a process of reasoning.
Recent research has revealed that reasoning capabilities can emerge when foundation models are scaled up to a certain size (Wei et al., 2022b). In particular, foundation models with tens or hundreds of billions of parameters can generate intermediate reasoning traces during complex problem-solving, thereby significantly enhancing their zero-shot and few-shot performances (Nakano et al., 2021; Nye et al., 2021; Wei et al., 2022b). The reasoning ability observed in the foundation models appears to facilitate a transition from System 1 to System 2 (Kahneman, 2011), enabling the accomplishment of more complex tasks.</p>
<p>Eliciting Reasoning in Foundation Models. Despite the extensive study of the concept of reasoning in the psychology literature (Wason, 1968; Kelley, 2013), the notion of reasoning as applied to foundation models is not clearly defined. However, in general terms, the reasoning ability in the literature of foundation models can be framed as the capacity to decompose a complex problem into sub-problems and solve the sub-problems step-by-step (Wei et al., 2022c; Press et al., 2022; Khot et al., 2022). Here we keep consistent with these works and discuss reasoning in the sense of problem decomposition and sub-problem solving.
The vanilla few-shot prompt learning (Brown et al., 2020), whereby models are provided with a prompt consisting of several examples for the given task, has been shown to fail when it comes to problems that require complex reasoning (Creswell et al., 2022). To address this issue, Wei et al. (2022c) propose Chain-of-Thought (CoT) prompting. Unlike vanilla few-shot prompt learning, CoT additionally inserts the reasoning trace required to derive the final answer for each example in the prompt. In this way, CoT prompts models to generate their "thoughts" on the necessary intermediate steps before arriving at the final answer. CoT has been proven to significantly boost performance on a wide range of tasks, including arithmetic reasoning, commonsense reasoning, and symbolic reasoning (Wei et al., 2022c).
In light of the remarkable reasoning abilities of foundation models, recent research has made successful attempts to employ them in the controller in tool learning. It is demonstrated that their reasoning capabilities enable the controller to effectively decompose a complex problem into several sub-problems, and determine which tool to call upon for each sub-problem. We categorize relevant research into two streams: introspective reasoning and extrospective reasoning. The former involves generating a static plan of tool use without interacting with the environment $\mathcal{E}$, while the latter generates plans incrementally by iteratively interacting with $\mathcal{E}$ and utilizing feedback obtained from previous executions. As shown in Figure 5, the environment $\mathcal{E}$ is invisible to the controller $\mathcal{C}$ in introspective reasoning but is visible in extrospective reasoning, creating a closed-loop interaction among the four components.</p>
<p>Introspective Reasoning. This kind of reasoning directly generates multi-step plans for tool use without knowing intermediate execution results. For instance, Huang et al. (2022a) investigate the planning ability of foundation models and show that they are capable of decomposing high-level tasks into semantically plausible sub-plans. Another representative work is Program-Aided Language Models (PAL) (Gao et al., 2022), which prompts models to generate Python codes for intermediate reasoning steps. PAL uses the Python program interpreter as the tool, enabling the model to act as a programmer writing detailed comments, and achieving significant improvements in arithmetic, symbolic, and algorithmic reasoning. Notably, the idea of model-as-programmer has also been shown to be successful in embodied agents, as evidenced by ProgPrompt (Singh et al., 2022) and Code-as-Policies (Liang et al., 2022a), which prompt models to generate</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Illustration of introspective reasoning and extrospective reasoning. Extrospective reasoning requires feedback from the environment and humans to carry out iterative plan generation. We omit the perceiver in the illustration for simplicity.
executable programs for embodied agents. These studies reveal that, despite not having direct interaction with the environment, models are capable of generating executable programs for agents and anticipating possible anomalies in the plan execution. Another example is Visual ChatGPT (Wu et al., 2023), which interleaves various vision foundation models with ChatGPT to enable understanding and generating images. In their system, ChatGPT serves as the core controller and makes sequential decisions. At each step, ChatGPT might call a vision model to modify an existing image or respond to the user with plain text.</p>
<p>However, since these models are not grounded in the environment, they may generate unrealistic and nonsensical plans. To this end, SayCan (Ahn et al., 2022) emphasizes those actions that the agent is "permitted" to execute instead of those it is "willing" to perform. In practice, they employ a value function to estimate the probability of each action being successfully executed. With this function, the agent becomes more physically grounded in the environment. Overall, despite the absence of environment feedback, foundation models exhibit a remarkable ability to plan effectively in introspective reasoning. They can anticipate potential anomalies in plan execution and adjust their plans accordingly. This ability not only enables the controller to generate executable programs but also enhances its capacity to plan for a wide range of tasks.</p>
<p>Extrospective Reasoning. Despite its simplicity, introspective reasoning cannot adapt the plan in response to intermediate execution results. A more rational approach to planning is taking the environment $\mathcal{E}$ into account, and generating plans incrementally (e.g., one step at a time) with subsequent plans dependent on previous execution results. This allows the four components described in $\S 3.1$ to be well integrated and to cooperate effectively to achieve complex tasks. We refer to such an incremental reasoning strategy as retrospective reasoning. Compared to introspective reasoning, extrospective reasoning additionally considers feedback from the user and environment (Figure 5), and is thus better suited to complex tasks (Madaan et al., 2023; Wang et al., 2023b), such as multi-step QA and embodied learning, where decision-making at each step is dependent on the preceding context.
Recent works such as Self-Ask (Press et al., 2022), ReAct (Yao et al., 2022b), and ToolFormer (Schick et al., 2023) have demonstrated that by providing access to search engine APIs, models are able to achieve improved accuracy on multi-step QA. Through CoT prompting (Self-Ask and ReAct) or fine-tuning (ToolFormer), models can learn to decompose complex questions and utilize the search API to find the answer to the first sub-question. Based on the response and the question, they can then iteratively determine the subsequent question to ask or give the final answer.
For embodied learning, while introspective reasoning methods have demonstrated the ability to generate executable programs and address potential execution anomalies, direct interaction with the environment can further enhance models' planning capabilities. For instance, Inner Monologue (Huang et al., 2022b) leverages multiple sources of feedback from the environment, such as whether a task is completed successfully and the current scene information. In this way, models can generate more feasible plans and improve their ability of high-level instruction completion. LLM-Planner (Song et al., 2022) explicitly considers anomalies that may arise during plan execution and utilizes environmental feedback to regenerate the plan in case of execution failure, enabling models to handle exceptions appropriately. Additionally, ReAct (Yao et al., 2022b) grants models the autonomy to determine when to cease generating action tokens during planning, enabling them to reason about the current situation and develop more refined subsequent plans.</p>
<p>In summary, extrospective reasoning requires interaction between the controller and the environment, which is a more complex setting. However, the real-time feedback from the user and environment empowers models to have a more comprehensive understanding of the current situation, making it possible to eventually achieve long-term goals that necessitate extensive planning.</p>
<p>Challenges in Multi-Step Multi-Tool Scenario. Humans do not stick to only one single tool to complete complex tasks. Instead, we carefully decompose the task into several sub-tasks, select the most suitable tool for each sub-task, and gradually accomplish them step by step. As discussed above, current research has shown satisfactory performance in task decomposition. However, there is a lack of exploration in utilizing different tools for different sub-tasks. Most of the research mentioned in this section is limited to either multi-step single-tool or single-step multi-tool scenarios. However, there has been a recent emergence of research that addresses the multi-step multi-tool scenario. One such example is the ReAct model (Yao et al., 2022b), which integrates multiple APIs of Wikipedia and employs the foundation model to decide when to use which API. Later, MM-ReAct (Yang et al., 2023) generalizes ReAct to the multi-modal domain by including several vision experts. Furthermore, some recent projects such as Auto-GPT ${ }^{4}$ demonstrate the huge potential of GPT-4 in manipulating multiple tools and making long-term plans, pushing the boundaries of what is possible with tool learning. Given a user query, Auto-GPT will take step-by-step actions to accomplish the objective autonomously. In addition to reasoning about the current state, Auto-GPT can also reflect on past actions to refine decision-making. Although these works constitute a significant step in advancing tool learning in the multi-step multi-tool scenario, there are still several challenges and future directions that need to be investigated.</p>
<ul>
<li>Understanding the Interplay among Different Tools. The multi-step multi-tool scenario typically involves complex tasks that require a higher level of intent understanding and reasoning capability. To effectively utilize multiple tools under this scenario, models need to grasp not only the individual functionalities of tools but also their interactions and dependencies. Models should be able to sequence the tools in a logical order so that the subsequent tools can leverage the information generated by the previous tools and effectively complete the task.</li>
<li>From Sequential Execution to Parallel Execution. Tool executions do not have to be performed sequentially. In certain scenarios, parallel execution is possible for sub-tasks that do not depend on each other, which can potentially improve execution efficiency. For instance, given a user instruction "Generate two codes, one for drawing a rectangle, and one for drawing a circle.", the two tasks can be assigned to two agents, enabling the codes to be generated simultaneously. Determining the dependencies among different sub-tasks and effectively switching between parallel and sequential execution to optimize the overall process is a promising direction that merits further investigation.</li>
<li>From Single-agent Problem-Solving to Multi-agent Collaboration. Previous works typically assume that a single agent (controller) is solely responsible for the entire tool learning procedure. However, in practice, complex tasks often demand collaboration among multiple agents, each possessing unique abilities and expertise. Embracing multi-agent collaboration can unlock more effective and efficient problem-solving approaches, necessitating the design of methods for communication, coordination, and negotiation among agents to ensure seamless collaboration and optimal task execution. Notably, recent work like Park et al. (2023) demonstrates that multiple agents modeled with foundation models can simulate human behaviors (e.g., interpersonal communication) in interactive scenarios. This provides promising evidence for the adoption of multiple agents for tool learning.</li>
</ul>
<p>We look forward to more work in the future moving towards more practical multi-step multi-tool scenarios and making efforts to address these challenges. As a prior exploration, we evaluate foundation models' performance when multiple tools (APIs) are required to solve a task in Â§ 4.</p>
<h1>3.3 Training Models for Improved Tool Learning</h1>
<p>Guidance, either from humans or environments, plays a critical role in training foundation models to use tools. In contrast to the prompting-based methods mentioned in $\S 3.2 .1$ and $\S 3.2 .2$, which rely on the frozen foundation models' in-context learning abilities, the training-based method optimizes the model with supervision. As noted by Fagard et al. (2016), there are two primary ways for infants to learn a new tool, that is either from demonstration by an adult modeling the action or relying on their own exploration. Analogously, as shown in Figure 6, we categorize training strategies for tool learning into two streams: (1) learning from</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Training strategies for tool learning: (left) learning from human-annotated or model-annotated demonstrations; (right) learning from feedback, where the supervision could come from either the environment or humans.
concrete tool-use demonstrations (Nakano et al., 2021; Sasaki \&amp; Yamashina, 2021), which often requires human annotation, and (2) learning from feedback, which typically involves reinforcement learning (Reddy et al., 2020; Baker et al., 2022). Finally, considering the existence of potentially massive tools, learning each of them one by one is infeasible in practice. Hence, we emphasize the importance of generalization in tool learning and discuss potential solutions (Â§ 3.3.3).</p>
<h1>3.3.1 Learning from Demonstrations</h1>
<p>Models can be trained to mimic the behavior of human experts through imitation learning (Hussein et al., 2017; Liu et al., 2018b; Baker et al., 2022). Behavior cloning (Bain \&amp; Sammut, 1995) can be viewed as a simplistic form of imitation learning that focuses on learning policies in a supervised fashion, with the general assumption that the expert's behavior is optimal or near-optimal. The objective of behavioral cloning is to train models to imitate human experts' actions given certain inputs or conditions, and this approach is commonly adopted when the actions of an expert can be easily recorded and utilized for learning (Torabi et al., 2018).
Formally, assume that we have a dataset $\mathcal{D}$ of size $N$ consisting of pairs of user query $q$ and the human demonstration annotation $a^{<em>}$, i.e., $\mathcal{D}=\left{\left(q_{i}, a_{i}^{</em>}\right)\right}<em _mathcal_C="\mathcal{C">{i=0}^{N-1}$. Learning from human demonstrations optimizes the controller's parameters $\theta</em>$ with the following objective:}</p>
<p>$$
\theta_{\mathcal{C}}^{<em>}=\underset{\theta_{\mathcal{C}}}{\arg \max } \underset{\left(q_{i}, a_{i}^{</em>}\right) \in \mathcal{D}}{\mathbb{E}} \prod_{t=0}^{T_{i}} p_{\theta_{\mathcal{C}}}\left(a_{i, t}^{*} \mid x_{i, t}, \mathcal{H}<em i="i">{i, t}, q</em>\right)
$$</p>
<p>where $a_{i, t}^{<em>}$ is the human annotation at the $t$-th iteration for handling $q_{i}$, and $T_{i}$ is the total iteration number of $a_{i}$, other varaibles follow the notations defined in Equation (1). Based on how $a^{</em>}$ is obtained, learning from demonstration can be categorized into three streams, with human intervention gradually becoming less:</p>
<p>Supervised Learning. Traditionally, behavior cloning has been widely explored in learning end-to-end or modular perceiver-controller models for autonomous vehicles and robotic applications (Ly \&amp; Akhloufi, 2020; Codevilla et al., 2019). Recently, there has been a surge of interest in fine-tuning foundation models to perform tool-oriented tasks in a supervised way. For instance, Li et al. (2022) utilize foundation models as policy networks, whose input is the tokenized environment observations, the original goals, and action history. Benefiting from the task-general inductive bias brought by foundation models, behavior cloning using the policy network significantly improves both in-domain performance and out-of-distribution generalization. Another example is WebGPT (Nakano et al., 2021), which interacts with a search engine by iteratively refining its search queries and recording important information. To achieve this, the authors first build a search interface backed up by Bing ${ }^{2}$ and then fine-tune GPT-3 (Brown et al., 2020) to clone human web search behaviors. As a language model pre-trained on general domains, the original GPT-3 is not intrinsically anchored to valid browser commands. Therefore, it is crucial to first gather demonstrations of human interactions with the browser and then learn state-to-action mappings. After fine-tuning, the model shows exceptional capabilities in manipulating search engines for information retrieval, even surpassing human experts. Similarly, WebShop (Yao et al., 2022a) provides a web-based interactive environment where an agent could browse and</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>purchase products. Through behavior cloning, the trained agent exhibits non-trivial performance in purchasing the right product given human instructions.</p>
<p>Semi-supervised Learning. As is often the case, human behaviors cannot be easily recorded due to time and cost considerations. However, large-scale unlabeled data is often attainable, from which we could potentially construct weak, noisy supervision. Notably, recent works have shown that we could employ a less-capable model to annotate pseudo-labels on unlabeled data and convert them into weakly-supervised tool-use demonstrations. For instance, with a small amount of seed labeled data, Baker et al. (2022) train a model to predict pseudo-labels of the action taken at each timestep in a Minecraft video game. Learning from these pseudo-labels, a more powerful model can be trained without requiring the rollout of models in a target environment or large-scale gold-standard human behavior annotation.</p>
<p>Self-supervised Learning. Despite reducing the heavy requirements on human behavior annotation, semisupervised learning still requires a seed labeled dataset to attain the pseudo labels. Besides, the biases in the seed dataset may also be amplified during training, leading to poor generalization performance. To this end, researchers recently show that with a few demonstrations, foundation models can teach themselves how to utilize a tool in a self-supervised manner (Parisi et al., 2022; Schick et al., 2023). For instance, Toolformer (Schick et al., 2023) leverages the in-context learning ability of foundation models to iteratively bootstrap tool-use examples based on a handful of human-written examples. These auto-generated examples are further filtered to reduce noise. The final tool-use dataset contains sufficient supervision, significantly improving GPT-J's (Wang \&amp; Komatsuzaki, 2021) tool-use performance, highlighting the potential of selfsupervised learning for enhancing tool-use capabilities.</p>
<h1>3.3.2 Learning from Feedback</h1>
<p>Collecting manually annotated tool-use examples, which probably include complete traces of human behaviors and the final answers, is time-consuming and labor-intensive. Moreover, the learned model may not adapt effectively to new environments as it conforms to the recorded human behaviors. Besides, it is impractical to explicitly annotate every possible scenario of environment condition and agent behavior (Codevilla et al., 2019). Alternatively, humans learn from trial and error to correct and rectify their tool-use behaviors (Allen et al., 2019). Similarly, feedback from both the environment and humans can enable the model to understand the consequences of its actions and adapt its behaviors. The supervision from feedback can also enhance the capabilities of an agent trained in a supervised way (Nakano et al., 2021; Baker et al., 2022). Formally, learning from feedback can be described as optimizing the controllers' parameters $\theta_{C}$ from open explorations with query set $Q=\left{q_{i}\right}_{i}$ :</p>
<p>$$
\theta_{\mathcal{C}}^{*}=\underset{\theta_{\mathcal{C}}}{\arg \max } \underset{q_{i} \in Q}{\mathbb{E}} \underset{\left{a_{i, t}\right}<em i="i">{t=0}^{T</em>\right}}} \in p a_{C}}{ } \sum_{i}\left[R\left(\left{a_{i, t<em i="i">{t=0}^{T</em>\right)\right]
$$}</p>
<p>where $R$ is the reward estimated from the sequence of feedback and $T_{i}$ denotes the number of iterations needed for handling $q_{i}$.</p>
<p>Reinforcement Learning (RL) for Tool Learning. RL is a common solution to enabling artificial agents to learn from their environment in complex decision-making processes (Silver et al., 2018; Berner et al., 2019; Schrittwieser et al., 2020). Tool learning can be considered an RL scenario, where the action space is defined by tools, and the agent learns to select the appropriate tool and perform the correct actions that maximize the reward signal. The policy model can be initialized by a foundation model (Schulman et al., 2017). Such initialization brings the policy model abundant prior knowledge, alleviating the need for the RL agent to learn basic skills. With a reward function that quantifies the performance of the agent in achieving the task goal, RL has been successfully used in various tool learning scenarios, such as robotic grasping (Levine et al., 2018) and multi-agent autocurricula (Baker et al., 2020). By optimizing the loss function, the agent learns to reflect on the current state of the environment, select the appropriate tool, and perform the right actions that lead to the highest expected reward. In the following, we introduce two sources of feedback: environment feedback and human feedback, which can be considered sources of reward signals in the context of tool learning. These two feedbacks are complementary and can be combined with each other.</p>
<p>Environment Feedback. The controller interacts with the environment and receives feedback about the consequences of its actions. The model then updates its policy based on this feedback to improve its tool-use behavior. Environment feedback can be categorized into two forms: (1) result feedback, which is ultimate feedback returned from the environment, indicating whether the model's actions have successfully completed</p>
<p>the task or not. This type of feedback performs an overall assessment of the planning generated by the model. For instance, WebShop (Yao et al., 2022a) uses a hand-coded reward to assess the similarity between human-bought and model-bought products, which indicates whether the actions performed by the controller lead to the correct final product. By receiving feedback on the success or failure of its actions, the model can iteratively update its planning strategy, and adjust its decision-making process; (2) intermediate feedback, which refers to the state change of the environment triggered by an action. By observing the state changes, foundation models can learn whether each action is effective and appropriate, making the model better adjust its behaviors accordingly. This kind of feedback provides more detailed and timely information about the effectiveness of each tool execution. Take the case of interacting with a search engine to gather information for question-answering, models could update their policy for more efficient information retrieval by observing the rendered information of a search query.</p>
<p>Human Feedback. Humans could give the model rewards and penalties based on its generated plans to regulate its behavior. Human feedback can be explicit, which provides clear and direct insights into the model performance representing human preferences. For example, rating the quality of the model-generated action on a scale of 1 to 5 ; human feedback can also be implicit, which is not directly specified by the user but can be derived from user behavior and interactions with the model. Examples include users' comparison (Ouyang et al., 2022), response time, and actions taken after receiving a model's output (e.g., clicking on a recommended link).</p>
<p>Though human feedback is accurate and stable, it is label-intensive and has high latency. To address this issue, reinforcement learning from human feedback (RLHF) (Christiano et al., 2017) is proposed to finetune a model to imitate humans to give rewards, which are then used to optimize the policy with RL algorithms such as PPO (Schulman et al., 2017). RLHF has yielded exceptional performance in various domains such as text summarization (Ziegler et al., 2019; Stiennon et al., 2020). RLHF can also improve a model's tooluse capabilities even if it has been trained on sufficient supervised human demonstrations. For instance, WebGPT (Nakano et al., 2021) utilizes human feedback to guide a policy model to align with human preferences, which helps better manipulate search engines to answer long-form questions.
Despite its remarkable performance, RLHF still faces challenges: (1) task-specific nature: the corresponding evaluation criteria for specific tasks need to be pre-defined, and the preference data annotated for one task is hard to be transferred to other settings, which limits the applicability of RLHF to a wider range of tasks. To this end, it is critical to develop a universal reward model that generalizes to various tasks; (2) biases: RL agents optimize towards the pseudo-human reward model, thus can be up-bounded and biased by human preferences. Besides, societal biases or personal experiences may be amplified during RLHF, and it is essential to carefully evaluate the learned reward model for any biases and take measures to mitigate them.</p>
<h1>3.3.3 Generalizable Tool Learning</h1>
<p>Generalization of tool use is a key characteristic of human intelligence (Seed \&amp; Byrne, 2010; Teschke et al., 2013; Osiurak et al., 2018). The ancient human, for instance, recognized that regardless of the specific tool being used, a sharp edge was essential for achieving clean cuts and efficiently carrying out tasks. This recognition allowed them to transfer their knowledge of sharpening a knife to sharpening other tools, such as scrapers or choppers. Generalization is also a critical aspect of tool learning, especially considering the existence of a massive and rapidly expanding array of tools. Although conducting supervised fine-tuning on a vast collection of tool-use data can be a potential solution to facilitating generalization, collecting enough supervised tool-use data and ensuring its quality and diversity is time-consuming and practically infeasible.
Generalizable tool learning highlights the importance of abstraction, which is the process of identifying the essential features of a tool. Abstraction involves recognizing commonalities and patterns of tools so that models could synthesize and transfer their knowledge and skills, enabling them to use novel tools with ease. For instance, by abstracting essential features such as layers, filters, and color adjustments, users can transfer their knowledge of using Adobe Photoshop to Adobe Illustrator, even if the interface and specific tool names in these two figure-editing software are different. Abstracting these general features of tools can quickly help users learn a new tool effectively by building on previous experience.</p>
<p>Foundation of Generalization: Interface Unification. To facilitate knowledge transfer among tools, it is critical to design a unified interface that enables the model to manipulate various tools in a consistent and standardized manner, which serves as the foundation for generalizable tool learning. Through a unified interface, models can identify and abstract essential features of tools more easily in a unified tool protocol rather than grappling with the difficulty of understanding various tool interfaces. Currently, the manipulation</p>
<p>of tools is through predicting discrete action tokens, and the action space is not aligned in different scenarios, which prohibits the models from quickly adapting to new scenarios and tools. Inspired by the aspect we categorize tools in $\S 2.2$, we identify three potential ways of interface unification: the semantic interface, GUI interface, and programming interface.</p>
<ul>
<li>Semantic Interface. The semantic interface operates by utilizing a specific text span (action name) as the action trigger, which is the most intuitive and natural way for interface unification. For instance, ReAct (Yao et al., 2022b) employs Action: Search as the trigger for the function that searches for relevant passages. In robotic manipulation (Ahn et al., 2022; Liu et al., 2023), the generated natural language (e.g., pick up the sponge) is mapped to specific actions. Despite its ease of implementation, the semantic interface poses certain challenges that must be addressed. First, the mapping between the generated text and the corresponding tool action should be pre-defined individually, which is a laborious task, particularly when the tool set expands quickly. Moreover, the model may fail to accurately produce the precise form to trigger the intended action, even leading to false triggering of actions.</li>
<li>GUI Interface. Humans primarily interact with the digital world through GUI interface (e.g., mouse and keyboard), which has been extensively optimized to follow human action efficiently. Nevertheless, before robots can learn to use a GUI interface flexibly, it is necessary to establish a virtual environment that can facilitate mapping predicted tokens to human-like mouse movements and keyboard inputs. Prior research has explored providing platforms for agents to complete web-based tasks using keyboard and mouse actions (Shi et al., 2017; Liu et al., 2018a). However, these environments restrict models to a limited set of pre-defined mouse options and common keyboard actions such as copy and paste. By leveraging foundation models, it is possible to introduce prior knowledge regarding common combinations of keyword and mouse actions, thereby expanding the potential actions that a model can execute.</li>
<li>Programming Interface. This kind of interface allows the model to go beyond pure natural language and specify its action using a program. Such unification requires the model to be acquainted with the syntax of the function calls. The recent code-generating language models (CLM) such as Incoder (Fried et al., 2022) and CodeX (Chen et al., 2021) provide the possibility of such unification. The programming interface has been applied widely. For example, Code-as-Policies (Liang et al., 2022a) finds that with CLM as the backbone for robotic control, the robots can leverage the code grammar to execute complex actions, generalize to novel instructions, and give precise control with accurate parameter values to the functions. The programming interface provides promising opportunities for tool learning because (1) complex tool learning logic can be modeled using the control flow of programming language; (2) explicit calls of external APIs can be naturally implemented by executing programs.</li>
</ul>
<p>It should be noted that the interface selection should align with the capabilities and limitations of the foundation model. For instance, language foundation models are trained to generate text and may be better suited for the semantic interface. Similarly, a multimodal foundation model that combines visual and textual information may be more appropriate for the GUI interface, as it can understand and generate human-like mouse movements and keyboard inputs. On the other hand, code foundation models may be more suitable for the programming interface, as it is trained to understand code syntax and function calls.</p>
<p>Under certain cases, we may face challenges where the tool's output is not aligned with model's input format. A common practice is to compose the functionality of the model and tool in the same modality. For example, Zeng et al. (2022) chain together foundation models of various modalities by converting their outputs into natural languages. This simple method leverages prompting to compose new multimodal capabilities without fine-tuning. In contrast, another solution is to building multimodal foundation models that can perceive general modalities, based on the belief that multimodal foundation models can all be unified through a general-purpose interface (Alayrac et al., 2022; Hao et al., 2022). Gato (Reed et al., 2022) is a representative generalist multi-embodiment agent trained on tremendous datasets of agent experience. Gato can sense and act with different embodiments, such as playing Atari, captioning images, chatting, etc. Similarly, PaLM-E (Driess et al., 2023) incorporates continuous inputs from different modalities into a PLM. By joint training on multiple embodied tasks, PaLM-E could make grounded decisions in the real world.</p>
<p>Strategies of Generalizable Tool Learning. In general, a unified interface enables models to learn and transfer knowledge more easily and efficiently, but it does not guarantee optimal learning outcomes in all scenarios. Generalizable tool learning requires models to further adapt, refine, and specialize their learned knowledge to specific tasks or domains. Here, we discuss two potential approaches to achieving this goal and facilitating generalization.</p>
<ul>
<li>Meta Tool Learning. Metacognition (Clarebout et al., 2013) is a crucial aspect of human intelligence that allows individuals to reflect on their own thinking and adapt their behaviors when faced with unfamiliar situations. In the context of tool learning, metacognition refers to the ability of a model to reflect on its own learning process and adapt new tool-use strategies when necessary. With metacognition, models can identify common underlying principles or patterns in tool-use strategies and transfer them to new tasks or domains. Take the case of the web search tool, when the model trained on a source search engine (e.g., Bing Search) is transferred to a target one (e.g., Google Search), the model can leverage its metacognitive awareness to adapt its tool-use strategies based on its previous experiences. This may include identifying common underlying patterns in tool-use strategies, such as effective search queries, relevant results, and user feedback, and using this metacognitive awareness to better align with the algorithms and user interface of the new search engine.</li>
<li>Curriculum Tool Learning. Another approach to improving model generalization is through curriculum learning (Bengio et al., 2009), which starts with simple tools and gradually introduces the model to more complex tools so that it can build upon its prior knowledge and develop a deeper understanding of the tool. For instance, we could start with a curriculum of basic algorithms and operations to effectively teach a model to use Mathematica, e.g., addition and subtraction, and then gradually move on to more complex mathematical concepts like calculus and linear algebra. This training strategy ensures that the model is introduced to the simple, essential features of the tool before moving on to more complex concepts in a way that is manageable and effective. Moreover, curriculum tool learning allows the model to learn how complex tools are built upon simple tools. It provides an understanding of how a complex tool can be seen as an updated high-level version of a simple tool, and how its function is a combination of several basic tools. This understanding of the relationship between simple and complex tools facilitates the transfer of previously learned knowledge to new tools, enabling the model to more effectively identify similarities and differences between situations and adjust its approach accordingly.</li>
</ul>
<h1>4 Application and Experiment</h1>
<p>In this section, we aim to explore the applications of tool learning and investigate the efficacy and limitations of state-of-the-art foundation models in utilizing tools. We select 18 representative tools for evaluation and place the main results in this section. For more case studies of ChatGPT, please refer to Appendix A.</p>
<h3>4.1 Evaluated Tools</h3>
<p>We first briefly introduce the tools selected in experiments as follows:
Machine Translator. General-purpose language models may exhibit suboptimal proficiency when processing text from multiple linguistic domains. Machine translators can effectively alleviate this issue by enabling non-translation-dedicated language models to better comprehend multi-lingual texts. Following Toolformer, we use NLLB (Costa-jussÃ  et al., 2022) as our translator and choose MLQA (Lewis et al., 2020a), a multilingual question answering benchmark, as the testbed. Given a context in English and a question in Arabian, the task requires answering the question using English. We randomly sample 200 test instances from the original test data. For the evaluation metric, we choose F1-score.
Calculator. Following the setting of Toolformer, we conduct experiments in which language models use a simple calculator to solve math word problems. We choose a simple implementation for the calculator, which supports basic arithmetic operations (i.e.,,$+,- \times, \div$ ). We evaluate two math word problem datasets: ASDiv (Miao et al., 2020) and MathQA (Amini et al., 2019) and choose accuracy as the metric.
Map. We choose Bing Map API ${ }^{7}$ for location information retrieval, assisting in user queries related to the route, driving distance, latitude coordinates, nearby locations of interest, etc. We manually curate user queries through crowdsourcing.
Weather. We choose Weather API ${ }^{8}$ and investigate whether models could use the tool to answer weatherrelated questions, such as questions about current weather in any city, forecasting the weather within two weeks in any city, and giving suggestions based on the weather information. Two APIs are supported, the first one is GetWeatherToday<city>, which provides the current weather condition of a city; another</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://www.wolfram.com/mathematica
${ }^{7}$ https://learn.microsoft.com/en-us/bingmaps
${ }^{8}$ https://www.weatherapi.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4416 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4416</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4416</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-281204172</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.06412v1.pdf" target="_blank">Compare: A Framework for Scientific Comparisons</a></p>
                <p><strong>Paper Abstract:</strong> Navigating the vast and rapidly increasing sea of academic publications to identify institutional synergies, benchmark research contributions and pinpoint key research contributions has become an increasingly daunting task, especially with the current exponential increase in new publications. Existing tools provide useful overviews or single-document insights, but none supports structured, qualitative comparisons across institutions or publications. To address this, we demonstrate Compare, a novel framework that tackles this challenge by enabling sophisticated long-context comparisons of scientific contributions. Compare empowers users to explore and analyze research overlaps and differences at both the institutional and publication granularity, all driven by user-defined questions and automatic retrieval over online resources. For this we leverage on Retrieval-Augmented Generation over evolving data sources to foster long context knowledge synthesis. Unlike traditional scientometric tools, Compare goes beyond quantitative indicators by providing qualitative, citation-supported comparisons.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4416.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4416.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Compare</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compare framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular RAG-based framework for long-context, question-driven qualitative comparisons of scientific contributions at institutional and publication levels, retrieving from CORE and OpenAlex and synthesizing citation-supported comparative summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Compare</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>User submits a natural-language question (optionally a PDF). The system classifies the query into one of several pipelines (six use cases), retrieves documents from OpenAlex (metadata) and CORE (full-text), embeds and ranks candidate documents, organizes retrieved documents by affiliation (for institution-level analyses), generates per-entity summaries and a comparative synthesis using a Retrieval-Augmented Generation pipeline mediated by LlamaIndex, and postprocesses outputs (citation numbering, removing unused references). Implemented in Python 3.11 with Flask, Streamlit, and LlamaIndex.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified for the Compare retrieval+generation pipeline in the paper; the authors note they used GPT-4 and Gemini 2.0-flash to paraphrase manuscript text (and observed occasional incorrect citations especially for gemini-2.0-flash). The framework is described as model-agnostic via LlamaIndex.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval over CORE and OpenAlex; metadata filters (institution, country, year); ranking of candidate documents by relevance to query.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Retrieval-Augmented Generation (RAG): LLM generation conditioned on retrieved documents; generates per-entity summaries and then a comparative synthesis (question-focused multi-document summarization across retrieved long-context documents).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Variable per query; framework operates over OpenAlex subset cited in paper (61 million records with abstract+DOI used by their pipelines) and CORE full-text; per-query retrieval returns a top-K set rather than processing the entire corpus at once.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature across domains (examples in paper include COVID-19 research and NLP topics).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured, citation-supported qualitative comparisons and summaries; complementary quantitative visualizations (publication trends, key authors).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Informal expert feedback (1-5 rating scale) from four domain experts; qualitative assessment of accuracy, relevance, and clarity. No formal automatic metrics reported for the framework's outputs in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Informal user feedback was positive: participants identified ways Compare could enhance workflows. Reported issues include occasional incorrect citations and summaries sometimes lacking depth. No quantitative benchmark scores provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Existing literature discovery/summarization tools and scientometric approaches (examples discussed: CORE-GPT, OpenScholar, Consensus, Elicit, traditional scientometrics).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>No formal empirical comparison against baselines reported in the paper; comparisons are conceptual/feature-based rather than benchmarked quantitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining RAG with long-context synthesis and document-level retrieval enables flexible, question-driven qualitative comparisons across institutions and publications; grounding outputs in retrieved scholarly sources helps mitigate but does not eliminate hallucination and citation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Data quality and coverage gaps in source corpora (OpenAlex/CORE), selection bias from missing works, model bias and hallucination (occasional incorrect citations), presentation bias (uncertain claims appearing authoritative), and summaries sometimes lacking depth; reliance on open-access availability and quality of metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed to operate over very large scholarly corpora (paper reports using a 61M-record subset of OpenAlex and CORE full-text), but the paper does not provide empirical measurements of performance scaling with number of papers or detailed model-scaling experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Compare: A Framework for Scientific Comparisons', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4416.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4416.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments LLM generation with retrieved passages/documents from an external corpus, combining embedding-based retrieval and generation to answer knowledge-intensive queries or synthesize multi-document content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive NLP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-stage pipeline: (1) retrieve relevant documents or passages from an external corpus using embeddings or other retrieval methods; (2) condition an LLM on the retrieved context to generate answers/ summaries. In Compare, RAG is used in a two-step pipeline over evolving data sources to support long-context synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Method agnostic; specific LLMs are not mandated by the method. The paper references usage of LLMs generally and reports authors used GPT-4 and Gemini 2.0-flash for manuscript paraphrasing, but no single model is prescribed for RAG within Compare.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval (vector search) over indexed document chunks; metadata filtering to constrain retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM generation conditioned on retrieved documents (single-step or multi-step generation; can be used with ranking and re-ranking of candidate answers).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Variable; RAG can operate on any retrieval pool size. In context of the paper, retrieval is performed over large-scale repositories (OpenAlex and CORE) and returns a manageable top-K set per query.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General-purpose for knowledge-intensive NLP tasks and multi-document scientific QA/summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Question answers, query-focused multi-document summaries, comparative syntheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Depends on usage; typical metrics include QA accuracy, human evaluation for factuality and relevance; the referenced RAG literature includes task-specific metrics but this paper does not provide new metric values.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Pure generation without retrieval, traditional retrieval+IR pipelines, or extractive summarization baselines (not quantitatively compared in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG enables grounding of generated outputs in retrieved scholarly sources, improving traceability and factual grounding for knowledge-intensive synthesis tasks. It is a core enabler of long-context multi-document synthesis in Compare.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quality of retrieved documents and metadata directly impacts generated outputs; retrieval omissions lead to selection bias; generation can still hallucinate or misattribute content from retrieved sources.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>RAG scales with retrieval index size but per-query computational cost depends on retrieval and LLM inference; paper does not report quantitative scaling curves.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Compare: A Framework for Scientific Comparisons', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4416.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4416.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CORE-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CORE-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that combines open-access research aggregated by CORE with large language models to provide credible, trustworthy question answering over scholarly content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CORE-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Integrates the CORE aggregation of open-access scientific publications with LLM-based QA via retrieval (RAG-style) to answer user queries grounded in CORE full-text documents.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval over CORE full-text (likely embedding-based retrieval); metadata-aware retrieval inferred from description.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-conditioned generation grounded on retrieved CORE passages (RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates over CORE's open-access corpus (CORE aggregates millions of open-access documents); specific per-query counts not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General academic publications (open-access).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Question answering grounded in scholarly sources; credible/trustworthy QA.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CORE-as-corpus plus LLMs enables scholarly QA grounded in full-text open-access literature; cited as an example of prior systems that apply RAG to scientific corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed in this paper beyond general issues of data coverage and metadata quality in open repositories.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Compare: A Framework for Scientific Comparisons', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4416.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4416.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenScholar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenScholar</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system for synthesizing scientific literature using retrieval-augmented language models to support literature discovery and summarization over scholarly corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenScholar</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Applies RAG-style retrieval and LLM generation over scientific corpora to synthesize literature and support question answering / summarization tasks over scholarly content.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval-Augmented (embedding-based) retrieval from a scholarly corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-based generation conditioned on retrieved scholarly documents (query-focused summarization / synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scholarly literature across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Synthesis of scientific literature, QA, summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of recent systems (2024) that apply retrieval-augmented LMs to synthesize scholarly literature.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Compare: A Framework for Scientific Comparisons', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4416.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4416.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ai2 Scholar QA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ai2 Scholar QA (Ai2 Organized Literature Synthesis with Attribution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system developed by the Allen Institute (Singh et al.) that supports scientific question answering and organized knowledge synthesis over the Semantic Scholar corpus, with attribution to sources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ai2 Scholar QA: Organized Literature Synthesis with Attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Ai2 Scholar QA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Provides scientific question answering and knowledge synthesis over the Semantic Scholar corpus, emphasizing organized synthesis and attribution to source documents; referenced as a 2025 system addressing long-context scientific QA.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval over Semantic Scholar corpus (likely embedding-based retrieval; RAG-style approaches referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-based synthesis with attribution to supporting sources.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates over Semantic Scholar corpus (millions of works); specific per-query counts not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scholarly literature (general).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Organized literature synthesis and question answering with source attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of tools emerging (in 2025) that address scientific question answering and knowledge synthesis at scale with attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Compare: A Framework for Scientific Comparisons', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4416.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4416.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLLM / LitLLMs (LLMs for Literature Review)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured planning-and-generation pipeline (and associated line of work) that uses LLMs to generate literature-review style content, such as related-work sections, from abstracts and other paper metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LitLLMs, LLMs for Literature Review: Are we there yet?.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described in referenced work as a planning-and-generation pipeline: uses structured planning steps and LLM generation to produce related-work sections and other literature-review artifacts from paper abstracts/metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (referenced paper likely contains model details).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Document-level extraction from abstracts/metadata; structured planning to identify relevant literature.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Planned generation (multi-step LLM planning + generation) to compose related-work sections from multiple papers.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here; target is article-level literature review generation (likely dozens of related papers per output in examples).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature review writing and related-work generation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Related-work sections, literature-review style summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as recent work that structures LLMs for literature-review generation rather than generic summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed in this paper; general concerns about hallucination and depth are noted across literature-review LLM approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Compare: A Framework for Scientific Comparisons', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4416.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4416.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRUISE-Screening</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CRUISE-Screening</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system for automated literature screening from online sources, aimed at supporting living literature reviews and screening workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CRUISE-Screening: Living Literature Reviews Toolbox.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CRUISE-Screening</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Tooling for automating literature screening from online sources to support living literature reviews; presented in CRUISE project work (focus on screening pipelines and living review maintenance).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Automated retrieval and screening of literature from online sources (likely IR and rule-based/ML components; LLM usage not specified in the referenced description).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not primarily a synthesis/generation system; focused on screening and curation for review workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Literature review automation across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Screened and curated sets of candidate literature for review; tooling integration for living reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as part of the literature-review automation ecosystem; distinct from RAG-based synthesis systems because it focuses on screening.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Compare: A Framework for Scientific Comparisons', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4416.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4416.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MultiHop-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MultiHop-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmarking and method line for Retrieval-Augmented Generation on multi-hop queries, evaluating RAG approaches for queries requiring reasoning across multiple documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MultiHop-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Benchmarking suite and associated RAG approaches aimed at multi-hop queries that require retrieving and reasoning across multiple documents; used to evaluate RAG performance in multi-step information synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Multi-hop retrieval (retrieving multiple supporting documents/passage chains).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>RAG with multi-hop reasoning over retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Benchmarking datasets and retrieval pools vary; not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General multi-hop QA and multi-document reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Answers to multi-hop queries; evaluated RAG setups for multi-document reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Benchmark metrics for multi-hop QA (task-specific accuracy metrics) referenced in the cited work but not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as work probing RAG strengths and limitations on multi-hop, multi-document queries (relevant to long-context scientific synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Multi-hop document retrieval increases sensitivity to retrieval errors and selection bias; paper does not include new empirical results on this.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Compare: A Framework for Scientific Comparisons', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4416.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4416.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LeaveNoDocBehind</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leave No Document Behind (Long-Context LLM Benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark for evaluating long-context LLMs on extended multi-document question answering to test LLMs' abilities to handle very long contexts and many documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Leave No Document Behind</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Benchmark and evaluation methodology for long-context LLMs operating on extended multi-document QA scenarios, measuring the ability to retain and reason over large numbers of documents.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Long-context retrieval and indexing strategies; benchmark emphasizes extended multi-document contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Long-context LLM reasoning and multi-document QA.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Benchmark involves extended sets of documents per query (specific sizes in the referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General multi-document QA and long-context evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>QA responses over many documents; evaluation of long-context capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Benchmark accuracy and related QA metrics as defined in the referenced work (not reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as relevant work evaluating LLMs' long-context, multi-document QA abilities  relevant background for Compare's long-context synthesis goals.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Benchmarks reveal limits of current long-context LLMs for extended multi-document QA; specifics not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Compare: A Framework for Scientific Comparisons', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs <em>(Rating: 2)</em></li>
                <li>CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering <em>(Rating: 2)</em></li>
                <li>LitLLMs, LLMs for Literature Review: Are we there yet?. <em>(Rating: 2)</em></li>
                <li>Ai2 Scholar QA: Organized Literature Synthesis with Attribution <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks. <em>(Rating: 2)</em></li>
                <li>MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries <em>(Rating: 2)</em></li>
                <li>Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA <em>(Rating: 2)</em></li>
                <li>CRUISE-Screening: Living Literature Reviews Toolbox <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4416",
    "paper_id": "paper-281204172",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "Compare",
            "name_full": "Compare framework",
            "brief_description": "A modular RAG-based framework for long-context, question-driven qualitative comparisons of scientific contributions at institutional and publication levels, retrieving from CORE and OpenAlex and synthesizing citation-supported comparative summaries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Compare",
            "system_description": "User submits a natural-language question (optionally a PDF). The system classifies the query into one of several pipelines (six use cases), retrieves documents from OpenAlex (metadata) and CORE (full-text), embeds and ranks candidate documents, organizes retrieved documents by affiliation (for institution-level analyses), generates per-entity summaries and a comparative synthesis using a Retrieval-Augmented Generation pipeline mediated by LlamaIndex, and postprocesses outputs (citation numbering, removing unused references). Implemented in Python 3.11 with Flask, Streamlit, and LlamaIndex.",
            "llm_model_used": "Not specified for the Compare retrieval+generation pipeline in the paper; the authors note they used GPT-4 and Gemini 2.0-flash to paraphrase manuscript text (and observed occasional incorrect citations especially for gemini-2.0-flash). The framework is described as model-agnostic via LlamaIndex.",
            "extraction_technique": "Embedding-based retrieval over CORE and OpenAlex; metadata filters (institution, country, year); ranking of candidate documents by relevance to query.",
            "synthesis_technique": "Retrieval-Augmented Generation (RAG): LLM generation conditioned on retrieved documents; generates per-entity summaries and then a comparative synthesis (question-focused multi-document summarization across retrieved long-context documents).",
            "number_of_papers": "Variable per query; framework operates over OpenAlex subset cited in paper (61 million records with abstract+DOI used by their pipelines) and CORE full-text; per-query retrieval returns a top-K set rather than processing the entire corpus at once.",
            "domain_or_topic": "General scientific literature across domains (examples in paper include COVID-19 research and NLP topics).",
            "output_type": "Structured, citation-supported qualitative comparisons and summaries; complementary quantitative visualizations (publication trends, key authors).",
            "evaluation_metrics": "Informal expert feedback (1-5 rating scale) from four domain experts; qualitative assessment of accuracy, relevance, and clarity. No formal automatic metrics reported for the framework's outputs in this paper.",
            "performance_results": "Informal user feedback was positive: participants identified ways Compare could enhance workflows. Reported issues include occasional incorrect citations and summaries sometimes lacking depth. No quantitative benchmark scores provided.",
            "comparison_baseline": "Existing literature discovery/summarization tools and scientometric approaches (examples discussed: CORE-GPT, OpenScholar, Consensus, Elicit, traditional scientometrics).",
            "performance_vs_baseline": "No formal empirical comparison against baselines reported in the paper; comparisons are conceptual/feature-based rather than benchmarked quantitatively.",
            "key_findings": "Combining RAG with long-context synthesis and document-level retrieval enables flexible, question-driven qualitative comparisons across institutions and publications; grounding outputs in retrieved scholarly sources helps mitigate but does not eliminate hallucination and citation errors.",
            "limitations_challenges": "Data quality and coverage gaps in source corpora (OpenAlex/CORE), selection bias from missing works, model bias and hallucination (occasional incorrect citations), presentation bias (uncertain claims appearing authoritative), and summaries sometimes lacking depth; reliance on open-access availability and quality of metadata.",
            "scaling_behavior": "Designed to operate over very large scholarly corpora (paper reports using a 61M-record subset of OpenAlex and CORE full-text), but the paper does not provide empirical measurements of performance scaling with number of papers or detailed model-scaling experiments.",
            "uuid": "e4416.0",
            "source_info": {
                "paper_title": "Compare: A Framework for Scientific Comparisons",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A method that augments LLM generation with retrieved passages/documents from an external corpus, combining embedding-based retrieval and generation to answer knowledge-intensive queries or synthesize multi-document content.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks.",
            "mention_or_use": "use",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "Two-stage pipeline: (1) retrieve relevant documents or passages from an external corpus using embeddings or other retrieval methods; (2) condition an LLM on the retrieved context to generate answers/ summaries. In Compare, RAG is used in a two-step pipeline over evolving data sources to support long-context synthesis.",
            "llm_model_used": "Method agnostic; specific LLMs are not mandated by the method. The paper references usage of LLMs generally and reports authors used GPT-4 and Gemini 2.0-flash for manuscript paraphrasing, but no single model is prescribed for RAG within Compare.",
            "extraction_technique": "Embedding-based retrieval (vector search) over indexed document chunks; metadata filtering to constrain retrieval.",
            "synthesis_technique": "LLM generation conditioned on retrieved documents (single-step or multi-step generation; can be used with ranking and re-ranking of candidate answers).",
            "number_of_papers": "Variable; RAG can operate on any retrieval pool size. In context of the paper, retrieval is performed over large-scale repositories (OpenAlex and CORE) and returns a manageable top-K set per query.",
            "domain_or_topic": "General-purpose for knowledge-intensive NLP tasks and multi-document scientific QA/summarization.",
            "output_type": "Question answers, query-focused multi-document summaries, comparative syntheses.",
            "evaluation_metrics": "Depends on usage; typical metrics include QA accuracy, human evaluation for factuality and relevance; the referenced RAG literature includes task-specific metrics but this paper does not provide new metric values.",
            "performance_results": null,
            "comparison_baseline": "Pure generation without retrieval, traditional retrieval+IR pipelines, or extractive summarization baselines (not quantitatively compared in this paper).",
            "performance_vs_baseline": null,
            "key_findings": "RAG enables grounding of generated outputs in retrieved scholarly sources, improving traceability and factual grounding for knowledge-intensive synthesis tasks. It is a core enabler of long-context multi-document synthesis in Compare.",
            "limitations_challenges": "Quality of retrieved documents and metadata directly impacts generated outputs; retrieval omissions lead to selection bias; generation can still hallucinate or misattribute content from retrieved sources.",
            "scaling_behavior": "RAG scales with retrieval index size but per-query computational cost depends on retrieval and LLM inference; paper does not report quantitative scaling curves.",
            "uuid": "e4416.1",
            "source_info": {
                "paper_title": "Compare: A Framework for Scientific Comparisons",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "CORE-GPT",
            "name_full": "CORE-GPT",
            "brief_description": "A system that combines open-access research aggregated by CORE with large language models to provide credible, trustworthy question answering over scholarly content.",
            "citation_title": "CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering.",
            "mention_or_use": "mention",
            "system_name": "CORE-GPT",
            "system_description": "Integrates the CORE aggregation of open-access scientific publications with LLM-based QA via retrieval (RAG-style) to answer user queries grounded in CORE full-text documents.",
            "llm_model_used": "Not specified in this paper.",
            "extraction_technique": "Retrieval over CORE full-text (likely embedding-based retrieval); metadata-aware retrieval inferred from description.",
            "synthesis_technique": "LLM-conditioned generation grounded on retrieved CORE passages (RAG).",
            "number_of_papers": "Operates over CORE's open-access corpus (CORE aggregates millions of open-access documents); specific per-query counts not specified in this paper.",
            "domain_or_topic": "General academic publications (open-access).",
            "output_type": "Question answering grounded in scholarly sources; credible/trustworthy QA.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "CORE-as-corpus plus LLMs enables scholarly QA grounded in full-text open-access literature; cited as an example of prior systems that apply RAG to scientific corpora.",
            "limitations_challenges": "Not detailed in this paper beyond general issues of data coverage and metadata quality in open repositories.",
            "scaling_behavior": null,
            "uuid": "e4416.2",
            "source_info": {
                "paper_title": "Compare: A Framework for Scientific Comparisons",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "OpenScholar",
            "name_full": "OpenScholar",
            "brief_description": "A system for synthesizing scientific literature using retrieval-augmented language models to support literature discovery and summarization over scholarly corpora.",
            "citation_title": "OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs.",
            "mention_or_use": "mention",
            "system_name": "OpenScholar",
            "system_description": "Applies RAG-style retrieval and LLM generation over scientific corpora to synthesize literature and support question answering / summarization tasks over scholarly content.",
            "llm_model_used": "Not specified in this paper.",
            "extraction_technique": "Retrieval-Augmented (embedding-based) retrieval from a scholarly corpus.",
            "synthesis_technique": "LLM-based generation conditioned on retrieved scholarly documents (query-focused summarization / synthesis).",
            "number_of_papers": "Not specified here.",
            "domain_or_topic": "Scholarly literature across domains.",
            "output_type": "Synthesis of scientific literature, QA, summaries.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as an example of recent systems (2024) that apply retrieval-augmented LMs to synthesize scholarly literature.",
            "limitations_challenges": "Not detailed in this paper.",
            "scaling_behavior": null,
            "uuid": "e4416.3",
            "source_info": {
                "paper_title": "Compare: A Framework for Scientific Comparisons",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Ai2 Scholar QA",
            "name_full": "Ai2 Scholar QA (Ai2 Organized Literature Synthesis with Attribution)",
            "brief_description": "A system developed by the Allen Institute (Singh et al.) that supports scientific question answering and organized knowledge synthesis over the Semantic Scholar corpus, with attribution to sources.",
            "citation_title": "Ai2 Scholar QA: Organized Literature Synthesis with Attribution.",
            "mention_or_use": "mention",
            "system_name": "Ai2 Scholar QA",
            "system_description": "Provides scientific question answering and knowledge synthesis over the Semantic Scholar corpus, emphasizing organized synthesis and attribution to source documents; referenced as a 2025 system addressing long-context scientific QA.",
            "llm_model_used": "Not specified in this paper.",
            "extraction_technique": "Retrieval over Semantic Scholar corpus (likely embedding-based retrieval; RAG-style approaches referenced).",
            "synthesis_technique": "LLM-based synthesis with attribution to supporting sources.",
            "number_of_papers": "Operates over Semantic Scholar corpus (millions of works); specific per-query counts not provided here.",
            "domain_or_topic": "Scholarly literature (general).",
            "output_type": "Organized literature synthesis and question answering with source attribution.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as an example of tools emerging (in 2025) that address scientific question answering and knowledge synthesis at scale with attribution.",
            "limitations_challenges": "Not specified in this paper.",
            "scaling_behavior": null,
            "uuid": "e4416.4",
            "source_info": {
                "paper_title": "Compare: A Framework for Scientific Comparisons",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "LitLLM",
            "name_full": "LitLLM / LitLLMs (LLMs for Literature Review)",
            "brief_description": "A structured planning-and-generation pipeline (and associated line of work) that uses LLMs to generate literature-review style content, such as related-work sections, from abstracts and other paper metadata.",
            "citation_title": "LitLLMs, LLMs for Literature Review: Are we there yet?.",
            "mention_or_use": "mention",
            "system_name": "LitLLM",
            "system_description": "Described in referenced work as a planning-and-generation pipeline: uses structured planning steps and LLM generation to produce related-work sections and other literature-review artifacts from paper abstracts/metadata.",
            "llm_model_used": "Not specified in this paper (referenced paper likely contains model details).",
            "extraction_technique": "Document-level extraction from abstracts/metadata; structured planning to identify relevant literature.",
            "synthesis_technique": "Planned generation (multi-step LLM planning + generation) to compose related-work sections from multiple papers.",
            "number_of_papers": "Not specified here; target is article-level literature review generation (likely dozens of related papers per output in examples).",
            "domain_or_topic": "Scientific literature review writing and related-work generation.",
            "output_type": "Related-work sections, literature-review style summaries.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as recent work that structures LLMs for literature-review generation rather than generic summarization.",
            "limitations_challenges": "Not detailed in this paper; general concerns about hallucination and depth are noted across literature-review LLM approaches.",
            "scaling_behavior": null,
            "uuid": "e4416.5",
            "source_info": {
                "paper_title": "Compare: A Framework for Scientific Comparisons",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "CRUISE-Screening",
            "name_full": "CRUISE-Screening",
            "brief_description": "A system for automated literature screening from online sources, aimed at supporting living literature reviews and screening workflows.",
            "citation_title": "CRUISE-Screening: Living Literature Reviews Toolbox.",
            "mention_or_use": "mention",
            "system_name": "CRUISE-Screening",
            "system_description": "Tooling for automating literature screening from online sources to support living literature reviews; presented in CRUISE project work (focus on screening pipelines and living review maintenance).",
            "llm_model_used": "Not specified in this paper.",
            "extraction_technique": "Automated retrieval and screening of literature from online sources (likely IR and rule-based/ML components; LLM usage not specified in the referenced description).",
            "synthesis_technique": "Not primarily a synthesis/generation system; focused on screening and curation for review workflows.",
            "number_of_papers": "Not specified here.",
            "domain_or_topic": "Literature review automation across domains.",
            "output_type": "Screened and curated sets of candidate literature for review; tooling integration for living reviews.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Mentioned as part of the literature-review automation ecosystem; distinct from RAG-based synthesis systems because it focuses on screening.",
            "limitations_challenges": "Not detailed in this paper.",
            "scaling_behavior": null,
            "uuid": "e4416.6",
            "source_info": {
                "paper_title": "Compare: A Framework for Scientific Comparisons",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "MultiHop-RAG",
            "name_full": "MultiHop-RAG",
            "brief_description": "A benchmarking and method line for Retrieval-Augmented Generation on multi-hop queries, evaluating RAG approaches for queries requiring reasoning across multiple documents.",
            "citation_title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries.",
            "mention_or_use": "mention",
            "system_name": "MultiHop-RAG",
            "system_description": "Benchmarking suite and associated RAG approaches aimed at multi-hop queries that require retrieving and reasoning across multiple documents; used to evaluate RAG performance in multi-step information synthesis.",
            "llm_model_used": "Not specified in this paper.",
            "extraction_technique": "Multi-hop retrieval (retrieving multiple supporting documents/passage chains).",
            "synthesis_technique": "RAG with multi-hop reasoning over retrieved evidence.",
            "number_of_papers": "Benchmarking datasets and retrieval pools vary; not specified here.",
            "domain_or_topic": "General multi-hop QA and multi-document reasoning tasks.",
            "output_type": "Answers to multi-hop queries; evaluated RAG setups for multi-document reasoning.",
            "evaluation_metrics": "Benchmark metrics for multi-hop QA (task-specific accuracy metrics) referenced in the cited work but not reported here.",
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as work probing RAG strengths and limitations on multi-hop, multi-document queries (relevant to long-context scientific synthesis).",
            "limitations_challenges": "Multi-hop document retrieval increases sensitivity to retrieval errors and selection bias; paper does not include new empirical results on this.",
            "scaling_behavior": null,
            "uuid": "e4416.7",
            "source_info": {
                "paper_title": "Compare: A Framework for Scientific Comparisons",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "LeaveNoDocBehind",
            "name_full": "Leave No Document Behind (Long-Context LLM Benchmark)",
            "brief_description": "A benchmark for evaluating long-context LLMs on extended multi-document question answering to test LLMs' abilities to handle very long contexts and many documents.",
            "citation_title": "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA.",
            "mention_or_use": "mention",
            "system_name": "Leave No Document Behind",
            "system_description": "Benchmark and evaluation methodology for long-context LLMs operating on extended multi-document QA scenarios, measuring the ability to retain and reason over large numbers of documents.",
            "llm_model_used": "Not specified in this paper.",
            "extraction_technique": "Long-context retrieval and indexing strategies; benchmark emphasizes extended multi-document contexts.",
            "synthesis_technique": "Long-context LLM reasoning and multi-document QA.",
            "number_of_papers": "Benchmark involves extended sets of documents per query (specific sizes in the referenced work).",
            "domain_or_topic": "General multi-document QA and long-context evaluation.",
            "output_type": "QA responses over many documents; evaluation of long-context capabilities.",
            "evaluation_metrics": "Benchmark accuracy and related QA metrics as defined in the referenced work (not reported here).",
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Referenced as relevant work evaluating LLMs' long-context, multi-document QA abilities  relevant background for Compare's long-context synthesis goals.",
            "limitations_challenges": "Benchmarks reveal limits of current long-context LLMs for extended multi-document QA; specifics not reproduced here.",
            "scaling_behavior": null,
            "uuid": "e4416.8",
            "source_info": {
                "paper_title": "Compare: A Framework for Scientific Comparisons",
                "publication_date_yy_mm": "2025-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs",
            "rating": 2,
            "sanitized_title": "openscholar_synthesizing_scientific_literature_with_retrievalaugmented_lms"
        },
        {
            "paper_title": "CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering",
            "rating": 2,
            "sanitized_title": "coregpt_combining_open_access_research_and_large_language_models_for_credible_trustworthy_question_answering"
        },
        {
            "paper_title": "LitLLMs, LLMs for Literature Review: Are we there yet?.",
            "rating": 2,
            "sanitized_title": "litllms_llms_for_literature_review_are_we_there_yet"
        },
        {
            "paper_title": "Ai2 Scholar QA: Organized Literature Synthesis with Attribution",
            "rating": 2,
            "sanitized_title": "ai2_scholar_qa_organized_literature_synthesis_with_attribution"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks.",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries",
            "rating": 2,
            "sanitized_title": "multihoprag_benchmarking_retrievalaugmented_generation_for_multihop_queries"
        },
        {
            "paper_title": "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA",
            "rating": 2,
            "sanitized_title": "leave_no_document_behind_benchmarking_longcontext_llms_with_extended_multidoc_qa"
        },
        {
            "paper_title": "CRUISE-Screening: Living Literature Reviews Toolbox",
            "rating": 1,
            "sanitized_title": "cruisescreening_living_literature_reviews_toolbox"
        }
    ],
    "cost": 0.0169025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Compare: A Framework for Scientific Comparisons
8 Sep 2025</p>
<p>Moritz Staudinger moritz.staudinger@tuwien.ac.at 
Wojciech Kusa wojciech.kusa@nask.pl 
Allan Hanbury allan.hanbury@tuwien.ac.at 
Matteo Cancellieri matteo.cancellieri@open.ac.uk 
David Pride david.pride@open.ac.uk 
Petr Knoth petr.knoth@open.ac.uk </p>
<p>TU Wien Vienna
Austria</p>
<p>NASK National Research Institute Warsaw
Poland</p>
<p>Open University
United Kingdom</p>
<p>Compare: A Framework for Scientific Comparisons
8 Sep 2025B91B6B3B553ECD9A95025DFCD14DEAD410.1145/3746252.3761493arXiv:2509.06412v1[cs.DL]CCS ConceptsInformation systems  Data miningQuestion answeringInformation extractionWeb search engines Large Language Models, Retrieval-Augmented Generation, Scholarly Document Retrieval, summarization, question answering
Navigating the vast and rapidly increasing sea of academic publications to identify institutional synergies, benchmark research contributions and pinpoint key research contributions has become an increasingly daunting task, especially with the current exponential increase in new publications.Existing tools provide useful overviews or single-document insights, but none supports structured, qualitative comparisons across institutions or publications.To address this, we demonstrate Compare, a novel framework that tackles this challenge by enabling sophisticated long-context comparisons of scientific contributions.Compare empowers users to explore and analyze research overlaps and differences at both the institutional and publication granularity, all driven by user-defined questions and automatic retrieval over online resources.For this we leverage on Retrieval-Augmented Generation over evolving data sources to foster long context knowledge synthesis.Unlike traditional scientometric tools, Compare goes beyond quantitative indicators by providing qualitative, citation-supported comparisons.</p>
<p>Introduction</p>
<p>In today's research environment, the unprecedented growth in the number of scientific publications has created a double-edged sword.</p>
<p>While access to knowledge has never been as broad and straightforward as it is today, the ability to find, grasp, and understand research outcomes is becoming increasingly strained.Each year, tens of thousands of new articles are published in journals and conferences [4], with entire disciplines undergoing rapid transformation in the production and dissemination of knowledge.This poses significant challenges for individual researchers and research analysts -not only requiring them to stay up to date within their own fields of expertise, but also to understand broader developments across institutions and disciplines.</p>
<p>Despite a recent surge in literature discovery and summarization tools, most existing systems are geared toward analyzing individual documents or providing overviews of research topics such as CORE-GPT [18], OpenScholar [2], Consensus 1 or Elicit 2 , by applying Retrieval-Augmented Generation (RAG) [11] to a scientific corpus.What remains largely unexplored are tools that support comparative understanding and knowledge synthesis, with the tools only starting to become available in 2025, such as Ai2 by Singh et al. [21] and LitLLM [1].Addressing this gap requires tools that can analyze how research topics, methods, and contributions evolve over time.This issue becomes especially problematic when researchers aim to identify collaborators, evaluate competing approaches, or understand how their institution's research profile compares to others.In the absence of effective comparison and synthesis frameworks, researchers are left to conduct labor-intensive analyses and scoping reviews, or to rely on simplified proxies -such as institutional rankings or impact factors -that fail to capture deeper qualitative differences.</p>
<p>To address this challenge, we introduce Compare, a modular framework for long-context scientific comparisons at both the institutional and publication levels.To the best of our knowledge, Compare is the first tool to combine RAG with long-context synthesis for flexible, question-driven comparisons in the scientific domain.</p>
<p>We leverage multiple open-access resources using a two-step RAG pipeline, enabling users to pose custom comparative questions and receive synthesized insights drawn from a large pool of scholarly content.By providing comparisons and statistics based on the user input, Compare empowers researchers, research analysts and repository managers to efficiently navigate the growing sea of scientific publications.</p>
<p>Related Work 2.1 Scientific Question Answering</p>
<p>Multi-Document Question Answering (MDQA) is a well-established research area in both open-domain [22,24] and scientific settings [3,23].However, most existing resources focus on rather simple, shortcontext tasks.To our knowledge, only Wang et al. [23] explicitly address scientific question answering with long contexts of up to 200,000 characters.Other work, such as Asai et al. [2] and Auer et al. [3], focuses on multi-document QA but within shorter-contexts.</p>
<p>Query-Focused Multi-Document Summarization (QMDS) is a related subfield of question answering that aims to summarize multiple documents in response to a user's information need [20].While QMDS overlaps conceptually with our work, existing research in this area remains limited and typically focuses on short documents, such as debates [15] or webpages [9,12].</p>
<p>Compare builds on these directions but introduces a novel focus on long-context, qualitative comparisons between scientific entities (e.g., institutions or publications), grounded in scholarly sources.Rather than answering factual questions or generating summaries, Compare synthesizes similarities and differences using retrievalaugmented generation (RAG) pipelines customized for comparative research analysis.</p>
<p>Literature Review Automation</p>
<p>In recent years, various tools and datasets have been developed to automate scientific literature reviews.Lu et al. [13] introduced the Multi-XScience dataset for abstractive summarization of scientific articles, and Pilault [17] explored language model-based summarization for scientific texts.Kusa et al. [10] released CRUISE-Screening, a system for automated literature screening from online sources.</p>
<p>More recently, Agarwal et al. [1] proposed LitLLM, a structured planning-and-generation pipeline that can generate related work sections from abstracts.Similarly, Singh et al. [21] developed Ai2, a system that supports scientific question answering and knowledge synthesis over the Semantic Scholar corpus.</p>
<p>In contrast to these approaches, our goal is not to generate comprehensive related work sections or article-level summaries.Instead, we provide a flexible pipeline capable of synthesizing information at both the publication and institutional levels, with a focus on comparative insight rather than narrative review construction.</p>
<p>Institutional Comparison</p>
<p>The comparison and evaluation of scientific research has a long history, dating back at least to the establishment of peer review in 1665 [6].More systematic approaches such as citation tracking and co-authorship analysis only emerged centuries later.Today, scientometric methods are widely used to assess the impact of research and researchers.For example, Luo et al. [14] performed a scientometric analysis of construction research using co-authorship networks, citation data, and topic clustering.Similarly, Oyewola and Dada [16] explored the landscape of machine learning research through scientometric techniques.</p>
<p>Despite their prevalence, scientometric methods are often criticized for lacking depth and context.As Igi [5] notes: "Ideally, evaluating scientific impact would involve reading each publication." This sentiment highlights a fundamental challenge: while largescale quantitative metrics offer breadth, they often miss the nuanced understanding that comes from close reading.Igi further advocates for combining classical qualitative assessment with scientometrics to achieve a more comprehensive evaluation.</p>
<p>Compare addresses this challenge by enabling the automatic comparison of institutional research impact.Instead of relying solely on numerical indicators, it retrieves relevant literature and supports qualitative comparison of research contributions across institutions.</p>
<p>Compare Framework</p>
<p>Compare is a modular framework designed to assist researchers, research analysts, and repository managers in conducting qualitative comparisons of scientific contributions at the publication or institutional level.Users interact with the system by submitting a natural language question, optionally accompanied by a scientific paper.Based on the prompt's intent, the system selects and executes one of several predefined analytical pipelines to generate a structured, citation-supported response.</p>
<p>Compare is built with Python 3.11, Flask 3.1, Streamlit, and LlamaIndex, integrating both publication full-text and metadatacentric data sources to support diverse analysis scenarios.</p>
<p>Data Sources</p>
<p>Currently, Compare integrates two open-access research infrastructure APIs, from which it dynamically retrieves documents to populate its pipeline:</p>
<p> CORE [7,8] is a large-scale scholarly publications aggregation platform, offering access to the full texts of scientific publications, allowing for detailed content analysis. OpenAlex [19] is an open catalog of scholarly works, authors, institutions, and topics.It provides rich metadata (authors, affiliations, citations, etc.) for scholarly outputs, enabling high-level comparative analyses.</p>
<p>Workflow</p>
<p>Figure 1 illustrates the core workflow of the Compare framework.Users begin by submitting a natural language question, optionally accompanied by a scientific publication in PDF format.The system preprocesses the input and classifies the query into one of several supported categories displayed in Based on this classification, an appropriate pipeline is selected to fulfill the information need.</p>
<p>Each pipeline retrieves documents from the relevant data sources: OpenAlex or CORE.OpenAlex is used for metadata-level analysis, including author and institutional information, while CORE enables access to full-text content for deeper document-level comparisons.If available in the query text, metadata filters such as institution, country or publication year are applied to constrain the result set.</p>
<p>Once the relevant documents are retrieved, they are embedded using vector representations.Candidate documents are then ranked based on their relevance to the query.In institution-level pipelines,  the system organizes the documents by affiliation and generates summaries for each institution.These summaries are compared to each other, taking into account the user's original question.</p>
<p>In a final step, the system performs postprocessing to ensure output quality.This includes refining citation numbering and removing unused references from the output.The result is a structured, citation-grounded qualitative comparison tailored to the query.</p>
<p>Figures 2 and 3 illustrate the outputs for the same query, which asks to compare the contributions in COVID-19 research between Imperial College London and University College London.While Figure 2 shows the qualitative comparison, Figure 3 complements it with quantitative visualizations.</p>
<p>Together with the generated text, all the used sources are listed with their DOIs and authors, as displayed in Figure 2.</p>
<p>In Figure 3 two of the sample output plots are displayed.The first row of plots shows the publications over time for the given universities, and the second pair of plots showcases the most relevant researchers for the given topic at the university with their publication numbers.</p>
<p>In Table 1 we describe our six supported use cases, with a short description of their pipelines, as well as an example query.</p>
<p>Discussion</p>
<p>Data Quality</p>
<p>Despite ongoing efforts to collect and harmonize metadata across a wide range of publication formats and venues, significant limitations in data quality remain.Many research contributions are still not openly accessible, and even when available, their metadata is often incomplete or inconsistent.Platforms like CORE and Ope-nAlex are working to maximize data availability, but gaps persist.For example, Priem [19] reports that only around 50% of records in OpenAlex include DOIs.In our own analysis, we found that  approximately 91 million publications in OpenAlex contain both an abstract and a DOI (34% of the OpenAlex corpus), and 61 million (23% of the OpenAlex corpus) of these also include institutional Improving coverage by incorporating institutional repositories with higher-quality metadata could significantly enhance the performance and reliability of our retrieval pipelines.</p>
<p>System Bias</p>
<p>Our framework is subject to several types of bias.Selection bias is introduced when relevant works are missing due to them being excluded because of incomplete metadata or a lack of open access.Model bias is introduced to our framework as LLM occasionally hallucinate or exaggerate differences.Presentation bias occurs when generated text makes uncertain claims appear authoritative, especially if citations are imperfectly matched to the claims.</p>
<p>While these biases cannot be fully eliminated, we mitigate their impact by grounding outputs in retrieved sources and ensuring completeness of metadata.</p>
<p>Expert Feedback</p>
<p>To gain preliminary insights into the usefulness of our tool, we conducted informal evaluations with four researchers and research analysts from TU Wien, the Open University, Imperial College London, and the University of Birmingham.We demonstrated the core features of Compare, particularly the generated summaries.We invited participants to reflect on the accuracy, relevance, and clarity of summaries.After the demonstration, all participants got access to our framework, and were invited to explore the system themselves and send feedback via a 1-5 rating scale, with the option to leave textual comments.The feedback obtained so far is positive: all four participants identified potential ways in which Compare could enhance their working routines.However, they also noted limitations -in particular, occasional incorrect citations (especially for gemini-2.0-flash)and summaries that sometimes lack depth.We consider this initial feedback a valuable checkpoint and a plan to conduct more structured user studies in future work to systematically assess the effectiveness and usability of the system.</p>
<p>Conclusion</p>
<p>We presented Compare, a RAG-based framework designed to streamline the exploration and analysis of scientific publications.Specifically, Compare assists users in identifying research overlaps for scientific institutions and investigating the contributions of individual publications.Our framework provides a much-needed tool to navigate the increasingly complex landscape of academic literature.</p>
<p>A key strength of our framework is that it can be readily extended and adapted to diverse organizational needs, by integrating additional data sources and tailoring the framework to precise information requirements.</p>
<p>Future work will focus on enhancing the framework's capabilities.We plan to facilitate a more straightforward integration of additional data sources and workflows, further amplifying the benefits for researchers, research analysts and repository managers.Furthermore, we plan to refine our RAG pipeline to enable more advanced, agent and feedback-driven question answering, allowing for more nuanced and interactive explorations.Finally, we intend to contribute LlamaIndex data loaders for CORE and OpenAlex to the community, promoting broader accessibility and utility of our work.Our demo is available online 4 , as well as our source code 5 .</p>
<p>Figure 1 :
1
Figure 1: Overview of the Compare framework, providing institutional and publication-level comparisons.Dotted elements are optional.</p>
<p>Figure 2 :
2
Figure 2: A screenshot of Compare displaying qualitative analysis for a user query comparing the COVID-19 research of Imperial College London and University College London.</p>
<p>Figure 3 :
3
Figure 3: Quantitative analysis for the same COVID-19 research comparison shown in Figure 2, including publication trends and key authors per institution.</p>
<p>Table 1 ,
1
namely: University Overview, University Comparison, Multi-University Comparison, Domain Overview, Paper Comparison, or Paper QA.</p>
<p>Table 1 :
1
Overview of currently supported use cases in the Compare framework.Each use case maps to a specific query type and corresponding processing pipeline.All use cases except for 'Paper QA' support filters.Compare therefore only uses 61 million of the 267 million records in the OpenAlex corpus.
Use CaseInputSystem ActionExample QueryUniversity Overview One University + topicRetrieves publications affiliated with the uni-What has the University of Aberdeen con-versity and summarizes key contributions.tributed to Medical Natural Language Pro-cessing?University Compari-Two universities + topic Retrieves and summarizes documents per insti-What has the University of Aberdeen con-sontution, then generates a comparative synthesis.tributed to Information Retrieval com-pared to the University of Edinburgh?Multi-UniversityOne university + topicIdentifies top 3 institutions by activity, com-What has the University of Aberdeen con-Comparisonpares them with the specified one.tributed to Information Retrieval com-pared to universities in GB?Domain OverviewTopic onlyRetrieves full-text documents from CORE andWhat are the advances in multimodalsummarizes the topic across the field.learning since 2023?Paper ComparisonPDF + topic/aspectAnalyzes a paper's aspect (e.g., datasets, meth-How does the paper compare to other rel-ods) and compares with similar publications.evant literature based on its used models?Paper QAPDF + questionAnswers the question using only the contentWhat evaluation metrics are used in thisof the provided paper.paper?affiliation information 3 .
https://consensus.app/
https://elicit.com/
Retrieved via https://api.openalex.org/works?filter=has_doi:true,has_abstract:true, authorships.affiliations.institution_ids:!null
https://compare.ds-ifs.tuwien.ac.at
https://github.com/MoritzStaudinger/compare
AcknowledgmentsWe are grateful for the funding received from Open University through the REF2029 Student funding, which enabled Moritz to undertake this research in cooperation with the Open University.A special thanks also goes to Suchetha for her generosity in sharing her desk, making this work literally possible from a comfortable spot!Use of Generative AIThe authors used GPT-4 and Gemini 2.0-flash to paraphrase selected sentences and enhance the reading flow of the manuscript.All rewritten passages were carefully reviewed, edited, and approved by the authors.The core ideas, content, and interpretations remain entirely the original work of the authors.
LitLLMs, LLMs for Literature Review: Are we there yet?. Shubham Agarwal, Gaurav Sahu, Abhay Puri, H Issam, Laradji, D J Krishnamurthy, Jason Dvijotham, Laurent Stanley, Christopher Charlin, Pal, 10.48550/arXiv.2412.15249arXiv:2412.152492025</p>
<p>OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs. Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike D , David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, Graham Neubig, Dan Weld, Doug Downey, Wen-Tau Yih, 10.48550/arXiv.2411.14199Pang Wei Koh, and Hannaneh Hajishirzi. 2024</p>
<p>The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge. Sren Auer, Dante A C Barone, Cassiano Bartz, Eduardo G Cortes, Mohamad Yaser Jaradeh, Oliver Karras, Manolis Koubarakis, Dmitry Mouromtsev, Dmitrii Pliukhin, Daniil Radyush, Ivan Shilin, Markus Stocker, Eleni Tsalapati, 10.1038/s41598-023-33607-zScientific Reports. 13172402023. May 2023Nature Publishing Group</p>
<p>Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases. Robin Lutz Bornmann, Rdiger Haunschild, Mutz, 10.1057/s41599-021-00903-wHumanities and Social Sciences Communications. 812021. Oct. 2021Palgrave</p>
<p>Citation metrics and scientometrics. Rajko Igi, 10.17305/bb.2023.10233Biomolecules and Biomedicine. 2422024. April 2024</p>
<p>Editorial: What Is Peer Review?. R Amir, Mohammad H Kachooei, Ebrahimzadeh, 10.22038/abjs.2022.19585Archives of Bone and Joint Surgery. 1012022. Jan. 2022</p>
<p>CORE: A Global Aggregation Service for Open Access Papers. Petr Knoth, Drahomira Herrmannova, Matteo Cancellieri, Lucas Anastasiou, Nancy Pontika, Samuel Pearce, Bikash Gyawali, David Pride, 10.1038/s41597-023-02208-wScientific Data. 1013662023. June 2023Nature Publishing Group</p>
<p>CORE: three access levels to underpin open access. Petr Knoth, Zdenek Zdrahal, 10.1045/november2012-knothD-Lib Magazine. 18122012. 2012</p>
<p>AQua-MuSe: Automatically Generating Datasets for Query-Based Multi-Document Summarization. Sayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, Eugene Ie, 10.48550/arXiv.2010.12694arXiv:2010.126942020</p>
<p>CRUISE-Screening: Living Literature Reviews Toolbox. Wojciech Kusa, Petr Knoth, Allan Hanbury, 10.1145/3583780.3614736Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. the 32nd ACM International Conference on Information and Knowledge ManagementBirmingham United KingdomACM2023</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-Tau Yih, Tim Rocktschel, Sebastian Riedel, Douwe Kiela, Proceedings of the 34th International Conference on Neural Information Processing Systems (NIPS '20). the 34th International Conference on Neural Information Processing Systems (NIPS '20)Red Hook, NY, USACurran Associates Inc2020</p>
<p>QuerySum: A Multi-Document Query-Focused Summarization Dataset Augmented with Similar Query Clusters. Yushan Liu, Zili Wang, Ruifeng Yuan, 10.1609/aaai.v38i17.29836Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024. March 20243817</p>
<p>Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles. Yao Lu, Yue Dong, Laurent Charlin, 10.18653/v1/2020.emnlp-main.648Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Bonnie WebberOnline2020</p>
<p>A systematic review of green construction research using scientometrics methods. Wenkai Luo, Malindu Sandanayake, Lei Hou, Yongtao Tan, Guomin Zhang, 10.1016/j.jclepro.2022.132710Journal of Cleaner Production. 3661327102022. Sept. 2022</p>
<p>Diversity driven attention model for query-based abstractive summarization. Preksha Nema, M Mitesh, Anirban Khapra, Balaraman Laha, Ravindran, 10.18653/v1/P17-1098Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. Regina Barzilay, Min-Yen Kan, the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Exploring machine learning: a scientometrics approach using bibliometrix and VOSviewer. David Opeoluwa, Oyewola , Emmanuel Gbenga Dada, 10.1007/s42452-022-05027-7SN Applied Sciences. 451432022. April 2022</p>
<p>On Extractive and Abstractive Neural Document Summarization with Transformer Language Models. Jonathan Pilault, Raymond Li, Sandeep Subramanian, Chris Pal, 10.18653/v1/2020.emnlp-main.748Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Bonnie WebberOnline2020</p>
<p>CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering. David Pride, Matteo Cancellieri, Petr Knoth, ; Omar Alonso, Helena Cousijn, Gianmaria Silvello, Mnica Marrero, 10.1007/978-3-031-43849-3_13Linking Theory and Practice of Digital Libraries. Carla Teixeira, Lopes , Stefano Marchesin, Nature Switzerland, ChamSpringer2023</p>
<p>OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts. Jason Priem, Heather Piwowar, Richard Orr, 2022</p>
<p>Review on Query-focused Multidocument Summarization (QMDS) with Comparative Analysis. Prasenjeet Roy, Suman Kundu, 10.1145/3597299Comput. Surveys. 562024. Jan. 2024</p>
<p>Amanpreet Singh, Joseph Chee Chang, Chloe Anastasiades, Dany Haddad, Aakanksha Naik, Amber Tanaka, Angele Zamarron, Cecile Nguyen, Jena D Hwang, Jason Dunkleberger, Matt Latzke, Smita Rao, Jaron Lochner, Rob Evans, Rodney Kinney, Daniel S Weld, Doug Downey, Sergey Feldman, Ai2 Scholar QA: Organized Literature Synthesis with Attribution. 2025</p>
<p>MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries. Yixuan Tang, Yi Yang, 10.48550/arXiv.2401.15391arXiv:2401.153912024</p>
<p>Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA. Minzheng Wang, Longze Chen, Fu Cheng, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, Yongbin Li, 10.18653/v1/2024.emnlp-main.322Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Mohit Al-Onaizan, Yun-Nung Bansal, Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.48550/arXiv.1809.09600arXiv:1809.096002018</p>            </div>
        </div>

    </div>
</body>
</html>
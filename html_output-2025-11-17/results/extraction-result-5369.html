<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5369 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5369</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5369</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-6289471f2eca01dbde71e4832f93891f54b91cfe</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6289471f2eca01dbde71e4832f93891f54b91cfe" target="_blank">SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Inspired by Transformer and pre-trained language models from natural language processing, SMILES Transformer learns molecular fingerprints through unsupervised pre-training of the sequence-to-sequence language model using a huge corpus of SMilES, a text representation system for molecules.</p>
                <p><strong>Paper Abstract:</strong> In drug-discovery-related tasks such as virtual screening, machine learning is emerging as a promising way to predict molecular properties. Conventionally, molecular fingerprints (numerical representations of molecules) are calculated through rule-based algorithms that map molecules to a sparse discrete space. However, these algorithms perform poorly for shallow prediction models or small datasets. To address this issue, we present SMILES Transformer. Inspired by Transformer and pre-trained language models from natural language processing, SMILES Transformer learns molecular fingerprints through unsupervised pre-training of the sequence-to-sequence language model using a huge corpus of SMILES, a text representation system for molecules. We performed benchmarks on 10 datasets against existing fingerprints and graph-based methods and demonstrated the superiority of the proposed algorithms in small-data settings where pre-training facilitated good generalization. Moreover, we define a novel metric to concurrently measure model accuracy and data efficiency.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5369.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5369.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simplified Molecular Input Line Entry System</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used linear text serialization of a molecular graph where atoms, bonds, branches and ring closures are encoded as a sequence of symbol tokens (e.g., 'C', 'Br', '=', '(', '2'), enabling molecules to be treated as text for language-model training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>SMILES linearization / serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Molecular graphs are converted to a one-dimensional sequence of symbols (tokens) representing atoms, bond types, branch delimiters and ring-closure labels. In practice the paper tokenizes SMILES into symbol-level tokens (examples: 'c', 'Br', '=', '(', '2'), one-hot encodes tokens, and uses positional encodings when feeding sequences into sequence models.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>molecular graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Linear, order-dependent serialization of graph structure; compact and human-readable; enables use of standard sequence models and large unlabeled text corpora; sensitive to chosen SMILES form (canonical vs enumerated) which can bias learnt representations; token-level contextualization possible (atom-level contextual embeddings); can lose explicit graph adjacency unless model learns to reconstruct it.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Downstream molecular property prediction tasks (MoleculeNet datasets: ESOL, FreeSolv, Lipophilicity, MUV, HIV, BACE, BBBP, Tox21, SIDER, ClinTox); used as input for pretraining language models and for fingerprint extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as input representation; metrics reported for models trained on SMILES-derived fingerprints include RMSE for regression tasks (e.g., ESOL RMSE values reported) and ROC-AUC / PRC-AUC for classification. The paper reports many numeric results for models using SMILES-based representations (see SMILES Transformer entry).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared empirically (via downstream tasks) to graph-based representations (GraphConv) and rule-based fingerprints (ECFP4). Text-based SMILES approaches (SMILES Transformer and RNNS2S) outperform others on several small-data settings but are beaten by graph-based methods on some tasks (Lipophilicity, MUV, BACE, SIDER).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Order dependence and sensitivity to canonical SMILES; representation choice (canonical vs enumerated) introduces bias; length limitations for some sequence models; may require augmentation (SMILES enumeration) or specialized architectures (e.g., Transformer-XL) for very long molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5369.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5369.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES enumeration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES enumeration (randomized SMILES augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-augmentation procedure that produces different valid SMILES strings (different linearizations) for the same molecular graph to reduce bias from a single canonical SMILES and increase robustness of sequence models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Smiles enumeration as data augmentation for neural network modeling of molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Enumerated SMILES (multiple serializations per graph)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each molecule, multiple equivalent SMILES forms are generated by randomizing atom ordering / starting points so that the same graph appears as distinct sequences; used during pretraining so models see varied linearizations of identical graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>molecular graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Reduces canonical-representation bias, increases invariance of learned embeddings to linearization order, acts as data augmentation; encourages latent-space consistency across different serializations of the same graph.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used during pretraining of sequence models (SMILES Transformer) to improve representation learning; suggested to improve downstream performance and latent-space structure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No isolated quantitative ablation reported in this paper (authors state they applied random SMILES enumerator while pretraining ST). The paper reports overall downstream performance improvements for ST which used enumeration; isolated numeric effect of enumeration is not provided (null).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Presented as a remedy for SMILES canonical bias; referenced as used in prior work (Bjerrum 2017) and recommended as future improvement (also cited Bjerrum & Sattarov 2018). No direct head-to-head ablation numbers in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Paper notes enumerated SMILES should be exploited further (e.g., using sets of SMILES to constrain latent space) but does not quantify best practices; increases data volume and training variability; requires careful handling to ensure label consistency across augmented examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5369.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5369.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES Transformer (ST)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder Transformer (4 blocks per encoder/decoder) pretrained in an unsupervised seq2seq reconstruction task on ~861k SMILES to produce continuous 1024-d molecular fingerprints by pooling contextualized symbol embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>SMILES -> Transformer encoder-decoder latent fingerprint (pooled contextual embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Input: tokenized SMILES symbols one-hot encoded plus positional encodings. Model: 4 Transformer blocks each in encoder and decoder, 4 attention heads, 256 embedding dims, trained to reconstruct input SMILES (cross-entropy loss) with SMILES enumeration applied during training. Fingerprint extraction: concatenate four pooled vectors — mean-pooled and max-pooled output of last layer, first output of last layer, and first output of penultimate layer — forming a 1024-dimensional continuous fingerprint.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>molecular graph (represented as SMILES sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Contextual, continuous, 1024-d dense fingerprint; learned from large unlabeled corpora (data-driven); designed to capture chemical semantics; stable decoding (training converged to perplexity 1.0 on pretraining corpus); order-dependent but augmented with enumerated SMILES to reduce bias; intended to be task-agnostic and useful with simple downstream predictors, especially effective in low-label regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Evaluated on MoleculeNet downstream property prediction tasks (ESOL, FreeSolv, Lipophilicity, MUV, HIV, BACE, BBBP, Tox21, SIDER, ClinTox) across multiple train sizes; used with simple predictors (MLP, ridge regression, logistic regression) to test fingerprint quality; also latent-space visualization with t-SNE and decoding nearest neighbors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Downstream: RMSE for regressions, ROC-AUC / PRC-AUC for classifications. Novel Data Efficiency Metric (DEM) computed as average metric across exponentially increasing training fractions (1.25% to 80%). Representative reported numbers: DEM (ST+MLP) — ESOL 1.144 (RMSE, lower better), FreeSolv 2.246, Lipophilicity 1.169; classification DEMs: MUV 0.009 (PRC-AUC), HIV 0.683 (ROC-AUC), BACE 0.719, BBBP 0.900, Tox21 0.706, SIDER 0.559, ClinTox 0.963. In linear-model tests (ridge/logistic) ST achieved best DEM in 5/8 datasets. In max-performance (large-data) comparisons ST matched or was comparable to baselines and achieved best reported score only on ClinTox in their tuning setup. Pretraining details: 861,000 SMILES from ChEMBL24, 5 epochs, Adam optimizer, achieved perplexity 1.0 on reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to ECFP4 (hand-crafted fingerprint), RNNS2S (RNN seq2seq pretrained fingerprint), and GraphConv (graph convolutional network). Summary: ST had superior data efficiency (DEM) over baselines in 5/10 MoleculeNet tasks and worked especially well in small-data settings (ESOL, FreeSolv, BBBP, ClinTox). GraphConv or ECFP outperformed ST on some tasks (Lipophilicity, MUV, BACE, SIDER). In large-data maximum-performance comparisons ST was comparable but did not consistently beat graph-based models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>SMILES-based linearization imposes order dependence and sequence length limits; ST benefits from longer SMILES but may be limited by model sequence length; performance varies by dataset (not uniformly superior); the paper notes room for improvement via Transformer-XL for longer sequences, multi-task pretraining (predicting computed molecular descriptors), and stronger use of enumerated SMILES. Also ST used random splits for many experiments (not always scaffold splits), which affects comparability to some reported baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5369.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5369.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNNS2S</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RNN sequence-to-sequence pretrained fingerprint (RNNS2S)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RNN-based seq2seq model (encoder-decoder using GRUs) pretrained to reconstruct SMILES and used to produce continuous molecular fingerprints by concatenating encoder-layer outputs; used here as a text-based baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>SMILES -> RNN seq2seq latent fingerprint</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Authors implemented RNNS2S with a 3-layer bidirectional GRU encoder and decoder (256 hidden units). The model is pretrained to reconstruct SMILES; fingerprint extracted by concatenating outputs from the last and penultimate layers to match ST's dimensionality (1,024-d). Input SMILES are tokenized similarly to ST.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>molecular graph (represented as SMILES sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Sequential recurrent contextual embeddings at symbol/atom level; captures local sequence context but limited in handling very long sequences compared to Transformer; order-dependent; pretrained (unsupervised) to leverage unlabeled SMILES corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Same MoleculeNet downstream prediction tasks; RNNS2S used as a baseline fingerprint and fed to the same downstream predictors (MLP, linear models) to compare fingerprint quality and data efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported DEM and downstream metrics in tables: e.g., DEM RNNS2S+MLP ESOL 1.317, FreeSolv 2.987, Lipophilicity 1.219; classification DEMs similar to ST for some datasets but generally slightly lower on small-data efficiency. In linear-model tests RNNS2S DEMs: ESOL 1.288, FreeSolv 2.881, Lipophilicity 1.194, HIV 0.688, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared directly to SMILES Transformer and ECFP4 and GraphConv. RNNS2S performs comparably to ST in some tasks but ST generally shows better data efficiency and better scores with simple predictors. RNNS2S and ST share the same training data but differ in architecture (RNN vs Transformer); ST outperforms RNNS2S in several small-data settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>RNN-based seq2seq models are slower to train and less effective on long sequences than Transformers; the paper's RNNS2S implementation generally underperformed ST on data-efficiency metrics, indicating architectural limits for capturing long-range context in SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5369.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5369.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2seq / VAE SMILES models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretrained sequence-to-sequence and variational-autoencoder models decoding SMILES (prior works)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior works used RNN seq2seq and VAE architectures trained to decode SMILES from latent vectors to learn continuous molecular representations for generation and property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic chemical design using a data-driven continuous representation of molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>SMILES reconstruction via Seq2Seq / VAE latent representations</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Approaches train encoder-decoder RNNs or VAEs where latent continuous vectors are learned to reconstruct SMILES (or translate between representations); latent vectors are then used as molecular descriptors or fingerprints for downstream tasks or molecule generation. Examples cited: Gómez-Bombarelli et al. (VAE decoding SMILES), Kusner et al. (Grammar VAE), Winter et al. (translating equivalent chemical representations).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>molecular graph (represented as SMILES sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Continuous, data-driven latent descriptors; facilitate generative tasks in addition to supervised prediction; capture global molecular structure via reconstruction objective, but effectiveness depends on architecture and handling of syntax/grammar (motivating Grammar VAE).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Cited prior evaluations include molecule generation quality and downstream property prediction; in this paper these methods are referenced as related work rather than re-evaluated. The paper positions SMILES Transformer as applying more recent Transformer pretraining methods to the same paradigm.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No original numeric results reported in this paper for these prior models; referenced as prior art with claims that earlier seq2seq/ VAE methods did not demonstrate small-data generalization as analyzed here (null within-paper numeric data).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper notes that prior seq2seq / VAE approaches exist but that their small-data performance was not previously evaluated; the SMILES Transformer is presented as an advancement over earlier RNN/ VAE seq2seq work primarily due to Transformer architecture and demonstrated small-data advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prior RNN/VAE models may struggle with long-range dependencies, SMILES syntax, and small-data generalization; Grammar VAE was proposed to address syntactic validity; this paper suggests Transformer pretraining and SMILES enumeration as further improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery. <em>(Rating: 2)</em></li>
                <li>Automatic chemical design using a data-driven continuous representation of molecules. <em>(Rating: 2)</em></li>
                <li>Smiles enumeration as data augmentation for neural network modeling of molecules. <em>(Rating: 2)</em></li>
                <li>Learning continuous and data-driven molecular descriptors by translating equivalent chemical representations. <em>(Rating: 2)</em></li>
                <li>Grammar variational autoencoder. <em>(Rating: 1)</em></li>
                <li>Convolutional networks on graphs for learning molecular fingerprints. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5369",
    "paper_id": "paper-6289471f2eca01dbde71e4832f93891f54b91cfe",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "SMILES",
            "name_full": "Simplified Molecular Input Line Entry System",
            "brief_description": "A widely used linear text serialization of a molecular graph where atoms, bonds, branches and ring closures are encoded as a sequence of symbol tokens (e.g., 'C', 'Br', '=', '(', '2'), enabling molecules to be treated as text for language-model training.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "SMILES linearization / serialization",
            "representation_description": "Molecular graphs are converted to a one-dimensional sequence of symbols (tokens) representing atoms, bond types, branch delimiters and ring-closure labels. In practice the paper tokenizes SMILES into symbol-level tokens (examples: 'c', 'Br', '=', '(', '2'), one-hot encodes tokens, and uses positional encodings when feeding sequences into sequence models.",
            "graph_type": "molecular graph",
            "representation_properties": "Linear, order-dependent serialization of graph structure; compact and human-readable; enables use of standard sequence models and large unlabeled text corpora; sensitive to chosen SMILES form (canonical vs enumerated) which can bias learnt representations; token-level contextualization possible (atom-level contextual embeddings); can lose explicit graph adjacency unless model learns to reconstruct it.",
            "evaluation_task": "Downstream molecular property prediction tasks (MoleculeNet datasets: ESOL, FreeSolv, Lipophilicity, MUV, HIV, BACE, BBBP, Tox21, SIDER, ClinTox); used as input for pretraining language models and for fingerprint extraction.",
            "performance_metrics": "Used as input representation; metrics reported for models trained on SMILES-derived fingerprints include RMSE for regression tasks (e.g., ESOL RMSE values reported) and ROC-AUC / PRC-AUC for classification. The paper reports many numeric results for models using SMILES-based representations (see SMILES Transformer entry).",
            "comparison_to_other_representations": "Compared empirically (via downstream tasks) to graph-based representations (GraphConv) and rule-based fingerprints (ECFP4). Text-based SMILES approaches (SMILES Transformer and RNNS2S) outperform others on several small-data settings but are beaten by graph-based methods on some tasks (Lipophilicity, MUV, BACE, SIDER).",
            "limitations_or_challenges": "Order dependence and sensitivity to canonical SMILES; representation choice (canonical vs enumerated) introduces bias; length limitations for some sequence models; may require augmentation (SMILES enumeration) or specialized architectures (e.g., Transformer-XL) for very long molecules.",
            "uuid": "e5369.0",
            "source_info": {
                "paper_title": "SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "SMILES enumeration",
            "name_full": "SMILES enumeration (randomized SMILES augmentation)",
            "brief_description": "A data-augmentation procedure that produces different valid SMILES strings (different linearizations) for the same molecular graph to reduce bias from a single canonical SMILES and increase robustness of sequence models.",
            "citation_title": "Smiles enumeration as data augmentation for neural network modeling of molecules.",
            "mention_or_use": "use",
            "representation_name": "Enumerated SMILES (multiple serializations per graph)",
            "representation_description": "For each molecule, multiple equivalent SMILES forms are generated by randomizing atom ordering / starting points so that the same graph appears as distinct sequences; used during pretraining so models see varied linearizations of identical graphs.",
            "graph_type": "molecular graph",
            "representation_properties": "Reduces canonical-representation bias, increases invariance of learned embeddings to linearization order, acts as data augmentation; encourages latent-space consistency across different serializations of the same graph.",
            "evaluation_task": "Used during pretraining of sequence models (SMILES Transformer) to improve representation learning; suggested to improve downstream performance and latent-space structure.",
            "performance_metrics": "No isolated quantitative ablation reported in this paper (authors state they applied random SMILES enumerator while pretraining ST). The paper reports overall downstream performance improvements for ST which used enumeration; isolated numeric effect of enumeration is not provided (null).",
            "comparison_to_other_representations": "Presented as a remedy for SMILES canonical bias; referenced as used in prior work (Bjerrum 2017) and recommended as future improvement (also cited Bjerrum & Sattarov 2018). No direct head-to-head ablation numbers in this paper.",
            "limitations_or_challenges": "Paper notes enumerated SMILES should be exploited further (e.g., using sets of SMILES to constrain latent space) but does not quantify best practices; increases data volume and training variability; requires careful handling to ensure label consistency across augmented examples.",
            "uuid": "e5369.1",
            "source_info": {
                "paper_title": "SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "SMILES Transformer (ST)",
            "name_full": "SMILES Transformer",
            "brief_description": "An encoder-decoder Transformer (4 blocks per encoder/decoder) pretrained in an unsupervised seq2seq reconstruction task on ~861k SMILES to produce continuous 1024-d molecular fingerprints by pooling contextualized symbol embeddings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "SMILES -&gt; Transformer encoder-decoder latent fingerprint (pooled contextual embeddings)",
            "representation_description": "Input: tokenized SMILES symbols one-hot encoded plus positional encodings. Model: 4 Transformer blocks each in encoder and decoder, 4 attention heads, 256 embedding dims, trained to reconstruct input SMILES (cross-entropy loss) with SMILES enumeration applied during training. Fingerprint extraction: concatenate four pooled vectors — mean-pooled and max-pooled output of last layer, first output of last layer, and first output of penultimate layer — forming a 1024-dimensional continuous fingerprint.",
            "graph_type": "molecular graph (represented as SMILES sequences)",
            "representation_properties": "Contextual, continuous, 1024-d dense fingerprint; learned from large unlabeled corpora (data-driven); designed to capture chemical semantics; stable decoding (training converged to perplexity 1.0 on pretraining corpus); order-dependent but augmented with enumerated SMILES to reduce bias; intended to be task-agnostic and useful with simple downstream predictors, especially effective in low-label regimes.",
            "evaluation_task": "Evaluated on MoleculeNet downstream property prediction tasks (ESOL, FreeSolv, Lipophilicity, MUV, HIV, BACE, BBBP, Tox21, SIDER, ClinTox) across multiple train sizes; used with simple predictors (MLP, ridge regression, logistic regression) to test fingerprint quality; also latent-space visualization with t-SNE and decoding nearest neighbors.",
            "performance_metrics": "Downstream: RMSE for regressions, ROC-AUC / PRC-AUC for classifications. Novel Data Efficiency Metric (DEM) computed as average metric across exponentially increasing training fractions (1.25% to 80%). Representative reported numbers: DEM (ST+MLP) — ESOL 1.144 (RMSE, lower better), FreeSolv 2.246, Lipophilicity 1.169; classification DEMs: MUV 0.009 (PRC-AUC), HIV 0.683 (ROC-AUC), BACE 0.719, BBBP 0.900, Tox21 0.706, SIDER 0.559, ClinTox 0.963. In linear-model tests (ridge/logistic) ST achieved best DEM in 5/8 datasets. In max-performance (large-data) comparisons ST matched or was comparable to baselines and achieved best reported score only on ClinTox in their tuning setup. Pretraining details: 861,000 SMILES from ChEMBL24, 5 epochs, Adam optimizer, achieved perplexity 1.0 on reconstruction.",
            "comparison_to_other_representations": "Compared to ECFP4 (hand-crafted fingerprint), RNNS2S (RNN seq2seq pretrained fingerprint), and GraphConv (graph convolutional network). Summary: ST had superior data efficiency (DEM) over baselines in 5/10 MoleculeNet tasks and worked especially well in small-data settings (ESOL, FreeSolv, BBBP, ClinTox). GraphConv or ECFP outperformed ST on some tasks (Lipophilicity, MUV, BACE, SIDER). In large-data maximum-performance comparisons ST was comparable but did not consistently beat graph-based models.",
            "limitations_or_challenges": "SMILES-based linearization imposes order dependence and sequence length limits; ST benefits from longer SMILES but may be limited by model sequence length; performance varies by dataset (not uniformly superior); the paper notes room for improvement via Transformer-XL for longer sequences, multi-task pretraining (predicting computed molecular descriptors), and stronger use of enumerated SMILES. Also ST used random splits for many experiments (not always scaffold splits), which affects comparability to some reported baselines.",
            "uuid": "e5369.2",
            "source_info": {
                "paper_title": "SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "RNNS2S",
            "name_full": "RNN sequence-to-sequence pretrained fingerprint (RNNS2S)",
            "brief_description": "An RNN-based seq2seq model (encoder-decoder using GRUs) pretrained to reconstruct SMILES and used to produce continuous molecular fingerprints by concatenating encoder-layer outputs; used here as a text-based baseline.",
            "citation_title": "Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery.",
            "mention_or_use": "use",
            "representation_name": "SMILES -&gt; RNN seq2seq latent fingerprint",
            "representation_description": "Authors implemented RNNS2S with a 3-layer bidirectional GRU encoder and decoder (256 hidden units). The model is pretrained to reconstruct SMILES; fingerprint extracted by concatenating outputs from the last and penultimate layers to match ST's dimensionality (1,024-d). Input SMILES are tokenized similarly to ST.",
            "graph_type": "molecular graph (represented as SMILES sequences)",
            "representation_properties": "Sequential recurrent contextual embeddings at symbol/atom level; captures local sequence context but limited in handling very long sequences compared to Transformer; order-dependent; pretrained (unsupervised) to leverage unlabeled SMILES corpora.",
            "evaluation_task": "Same MoleculeNet downstream prediction tasks; RNNS2S used as a baseline fingerprint and fed to the same downstream predictors (MLP, linear models) to compare fingerprint quality and data efficiency.",
            "performance_metrics": "Reported DEM and downstream metrics in tables: e.g., DEM RNNS2S+MLP ESOL 1.317, FreeSolv 2.987, Lipophilicity 1.219; classification DEMs similar to ST for some datasets but generally slightly lower on small-data efficiency. In linear-model tests RNNS2S DEMs: ESOL 1.288, FreeSolv 2.881, Lipophilicity 1.194, HIV 0.688, etc.",
            "comparison_to_other_representations": "Compared directly to SMILES Transformer and ECFP4 and GraphConv. RNNS2S performs comparably to ST in some tasks but ST generally shows better data efficiency and better scores with simple predictors. RNNS2S and ST share the same training data but differ in architecture (RNN vs Transformer); ST outperforms RNNS2S in several small-data settings.",
            "limitations_or_challenges": "RNN-based seq2seq models are slower to train and less effective on long sequences than Transformers; the paper's RNNS2S implementation generally underperformed ST on data-efficiency metrics, indicating architectural limits for capturing long-range context in SMILES.",
            "uuid": "e5369.3",
            "source_info": {
                "paper_title": "SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Seq2seq / VAE SMILES models",
            "name_full": "Pretrained sequence-to-sequence and variational-autoencoder models decoding SMILES (prior works)",
            "brief_description": "Prior works used RNN seq2seq and VAE architectures trained to decode SMILES from latent vectors to learn continuous molecular representations for generation and property prediction.",
            "citation_title": "Automatic chemical design using a data-driven continuous representation of molecules.",
            "mention_or_use": "mention",
            "representation_name": "SMILES reconstruction via Seq2Seq / VAE latent representations",
            "representation_description": "Approaches train encoder-decoder RNNs or VAEs where latent continuous vectors are learned to reconstruct SMILES (or translate between representations); latent vectors are then used as molecular descriptors or fingerprints for downstream tasks or molecule generation. Examples cited: Gómez-Bombarelli et al. (VAE decoding SMILES), Kusner et al. (Grammar VAE), Winter et al. (translating equivalent chemical representations).",
            "graph_type": "molecular graph (represented as SMILES sequences)",
            "representation_properties": "Continuous, data-driven latent descriptors; facilitate generative tasks in addition to supervised prediction; capture global molecular structure via reconstruction objective, but effectiveness depends on architecture and handling of syntax/grammar (motivating Grammar VAE).",
            "evaluation_task": "Cited prior evaluations include molecule generation quality and downstream property prediction; in this paper these methods are referenced as related work rather than re-evaluated. The paper positions SMILES Transformer as applying more recent Transformer pretraining methods to the same paradigm.",
            "performance_metrics": "No original numeric results reported in this paper for these prior models; referenced as prior art with claims that earlier seq2seq/ VAE methods did not demonstrate small-data generalization as analyzed here (null within-paper numeric data).",
            "comparison_to_other_representations": "Paper notes that prior seq2seq / VAE approaches exist but that their small-data performance was not previously evaluated; the SMILES Transformer is presented as an advancement over earlier RNN/ VAE seq2seq work primarily due to Transformer architecture and demonstrated small-data advantages.",
            "limitations_or_challenges": "Prior RNN/VAE models may struggle with long-range dependencies, SMILES syntax, and small-data generalization; Grammar VAE was proposed to address syntactic validity; this paper suggests Transformer pretraining and SMILES enumeration as further improvements.",
            "uuid": "e5369.4",
            "source_info": {
                "paper_title": "SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery",
                "publication_date_yy_mm": "2019-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery.",
            "rating": 2
        },
        {
            "paper_title": "Automatic chemical design using a data-driven continuous representation of molecules.",
            "rating": 2
        },
        {
            "paper_title": "Smiles enumeration as data augmentation for neural network modeling of molecules.",
            "rating": 2
        },
        {
            "paper_title": "Learning continuous and data-driven molecular descriptors by translating equivalent chemical representations.",
            "rating": 2
        },
        {
            "paper_title": "Grammar variational autoencoder.",
            "rating": 1
        },
        {
            "paper_title": "Convolutional networks on graphs for learning molecular fingerprints.",
            "rating": 1
        }
    ],
    "cost": 0.013689499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery</h1>
<p>Shion Honda ${ }^{1,2,3}$, Shoi Shi ${ }^{1,2,3}$, Hiroki R. Ueda ${ }^{1,2,3}$<br>${ }^{1}$ University of Tokyo<br>${ }^{2}$ International Research Center for Neurointelligence<br>${ }^{3}$ RIKEN Center for Biosystems Dynamics Research<br>shion_honda@ipc.i.u-tokyo.ac.jp, {sshoi0322-tky,uedah-tky}@umin.ac.jp</p>
<h4>Abstract</h4>
<p>In drug-discovery-related tasks such as virtual screening, machine learning is emerging as a promising way to predict molecular properties. Conventionally, molecular fingerprints (numerical representations of molecules) are calculated through rule-based algorithms that map molecules to a sparse discrete space. However, these algorithms perform poorly for shallow prediction models or small datasets. To address this issue, we present SMILES Transformer. Inspired by Transformer and pre-trained language models from natural language processing, SMILES Transformer learns molecular fingerprints through unsupervised pre-training of the sequenceto-sequence language model using a huge corpus of SMILES, a text representation system for molecules. We performed benchmarks on 10 datasets against existing fingerprints and graph-based methods and demonstrated the superiority of the proposed algorithms in small-data settings where pre-training facilitated good generalization. Moreover, we define a novel metric to concurrently measure model accuracy and data efficiency.</p>
<h2>1 Introduction</h2>
<p>Recently, deep learning has emerged as a powerful machine learning technology. When applied to big data, deep learning can show equal or even better performance than humans in many domains such as computer vision (He et al. 2016), natural language processing (NLP) (Devlin et al. 2019; Yang et al. 2019), making decisions (Silver et al. 2017), and medicine (Jin, Barzilay, and Jaakkola 2018). Based on projected performance benchmarks, deep learning is expected to be useful a tool to handle time-consuming tasks.</p>
<p>Drug discovery is a process to find a new drug for a disease of interest from a chemical library and validate its efficacy and safety in clinical trials. This process usually takes more than a decade and is costly, and therefore may be improvable by deep learning methods. Indeed, deep learning has been applied to the process of drug discovery including quantitative structure-property relationships (QSPR) prediction (Duvenaud et al. 2015; Xu et al. 2017), molecule generation and lead optimization (Gómez-Bombarelli et al. 2018; Jin, Barzilay, and Jaakkola 2018), retrosynthesis planning (Segler, Preuss, and Waller 2018; Schwaller et al. 2018), and
compound-protein affinity prediction (Öztürk, Özgür, and Ozkirimli 2018).</p>
<p>In order to apply machine learning to drug discovery, molecular data must be transformed into a readable format for machine learning. One major approach is to transform molecular data into a simplified molecular input line entry system (SMILES), a text representation of molecules that is commonly used in many databases (Xu et al. 2017; Gómez-Bombarelli et al. 2018). Recently, graph-based approaches (Duvenaud et al. 2015; Kearnes et al. 2016) have been proposed, which usually show better performance than text-based approaches, such as SMILES, in QSPR tasks. In these studies, the models are designed for large fully-labeled training data settings, which requires huge labeled datasets and a QSPR model for one-shot learning (Altae-Tran et al. 2017). However, in most cases, it is difficult to prepare large labeled datasets of experimentally validated molecular properties or affinities to proteins, so that graph-based approaches might have limited application. Therefore, the development of a high-performing algorithm for small datasets will be required.</p>
<p>Given recent progress in the NLP field (Peters et al. 2018; Devlin et al. 2019; Yang et al. 2019), a pre-training approach may be a promising way to address this challenge. Language model pre-training can exploit huge unlabeled corpora to learn the representations of words and sentences and then the pre-trained model is fine-tuned to downstream tasks using a relatively smaller set of labeled data. Indeed, pre-training approaches have been implemented in the cheminformatics field: a pre-trained sequence-to-sequence learning models (seq2seq) composed of RNNs (Sutskever, Vinyals, and Le 2014) or variational autoencoders (VAE) (Kingma and Welling 2014) by decoding SMILES from the learned representations (Gómez-Bombarelli et al. 2018; Xu et al. 2017; Kusner, Paige, and Hernández-Lobato 2017; Goh et al. 2018; Bjerrum and Sattarov 2018; Winter et al. 2019). However, these studies did not demonstrate performance in small data settings. In other words, the performance on small data settings of pre-training approaches in the cheminformatics field has not been evaluated yet. In this study, by applying the latest pre-training method in the NLP field to cheminformatics, we propose a new approach called</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />Figure 1: The illustration of SMILES Transformer pretraining and fingerprint extraction.</p>
<p>SMILES Transformer (ST) that shows higher performance on small data settings than other approaches. ST is based on a Transformer Vaswani2017 pre-trained in an unsupervised way that produce continuous, data-driven fingerprints of molecules given SMILES. These fingerprints grasp the semantics of molecules and can be fed to arbitrary predictive models for many downstream tasks.</p>
<p>In order to evaluate the QSPR performance on small data settings, we focused on data efficiency. However, because there are few works focusing on data efficiency, which metric should be used is elusive. The most related work may be done by Wu2018, where model performance is evaluated against the size of the training set and data efficiency is emphasized as well as the best score. In this study, we propose a novel scalar metric to evaluate data efficiency. Our proposed model is described in Figure 1.</p>
<p>To sum up, our contributions include the following:</p>
<ul>
<li>We propose a data-driven fingerprinting model, SMILES Transformer, which works well with simple predictors and enables state-of-the-art data efficiency in 5 out of 10 datasets in MoleculeNet.</li>
<li>We pre-train Transformers with unlabeled SMILES to learn their representations and show the potential of text-based models compared to baseline models including graph convolutions.</li>
<li>We propose a scalar metric for data efficiency that measures model performance under different sizes of training data.</li>
</ul>
<p>In the first section, we will explain how ST is trained and the fingerprints are extracted. In the second section, we define the metric for data efficiency. In the third section, we will compare the performance of ST fingerprints against other methods using 10 different datasets from MoleculeNet and more deeply inspect the pre-trained ST including latent space visualization. Finally, we discuss possible future directions.</p>
<h2>2 Methods</h2>
<p>In this section, we introduce the SMILES Transformer architecture, pre-training settings, and how to design ST fingerprints. We then propose a novel metric for data efficiency.</p>
<h3>2.1 SMILES Transformer</h3>
<p>Model Architecture Unlike RNNs, Transformers Vaswani2017 do not have recurrent connections and are therefore more stable and faster to converge. Moreover, they empirically show better featurization performance on long sequences and complicated problems than RNNs. Hence, they are chosen as the de facto standard models in NLP Devlin2019, Yang2019.</p>
<p>We built an encoder-decoder network with 4 Transformer blocks for each with PyTorch Paszke2017. Each Transformer block has 4-head attentions with 256 embedding dimensions and 2 linear layers.</p>
<p>Pre-training settings We pre-trained ST with 861,000 unlabeled SMILES randomly sampled from ChEMBL24, a dataset of bioactive and real molecules Gaulton2016. The SMILES was split into symbols (e.g., ’c’, ’Br’, ’=’, ’(’, ’2’) and then the symbols were one-hot encoded to input to the network. To alleviate bias for the canonical representation of SMILES, we randomly transformed them every time they were used by the SMILES enumerator Bjerrum2017. Following the original paper Vaswani2017, we used the sum of token encoding and positional encoding to input to the network. The network was trained for 5 epochs to minimize the cross entropy between the input SMILES and the output probability by the Adam optimizer Kingma2015. After convergence, the network achieved a perplexity of 1.0, meaning perfect decoding from the encoded representations.</p>
<p>Fingerprint extraction As the outputs of the Transformers are contextualized word-level representations, ST outputs a sequence of symbol-level (atom-level) representations. Therefore, we need to pool them to obtain the molecule-level representations (fingerprints). We concatenated the four vectors to get the fingerprints: mean and max pooled output of the last layer, the first output of the last and the penultimate layer. Now we have a 1024-dimensional fingerprint for each molecule from ST. This fingerprint is designed to have the same dimensionality with the baseline we use for, the extended-connectivity fingerprint (ECFP) Rogers2010.</p>
<h3>2.2 Data Efficiency Metric (DEM)</h3>
<p>Here we discuss how to measure the data efficiency of a predictive model $f$ in terms of the metric $m$. Intuitively, data efficiency can be measured by averaging the metric $m$ of the model $f$ trained with different sizes of the training data.</p>
<p>More formally, let $(X, Y)$ denote the whole available dataset and $(X_{i}, Y_{i})$ denote the test data sampled from $(X, Y)$ at the rate of $1-i$. Then, the training data and the model trained with them can be represented as $(X\backslash X_{i}, Y\backslash$ $Y_{i}$ ) and $f_{i}$, respectively. The metric $m$ should be chosen to</p>
<p>Table 1: Summarized information of MoleculeNet (Wu et al. 2018). ”R” and ”C” in the type column indicates regression and classification respectively.</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Dataset</th>
<th>Tasks</th>
<th>Type</th>
<th>Mols</th>
<th>Metric</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Physical chemistry</td>
<td>ESOL</td>
<td>1</td>
<td>R</td>
<td>1128</td>
<td>RMSE</td>
<td>Aqueous solubility</td>
</tr>
<tr>
<td></td>
<td>FreeSolv</td>
<td>1</td>
<td>R</td>
<td>643</td>
<td>RMSE</td>
<td>Hydration free energy</td>
</tr>
<tr>
<td></td>
<td>Lipophilicity</td>
<td>1</td>
<td>R</td>
<td>4200</td>
<td>RMSE</td>
<td>Octanol/water distribution coefficient (logD)</td>
</tr>
<tr>
<td>Biophysics</td>
<td>MUV</td>
<td>17</td>
<td>C</td>
<td>93127</td>
<td>PRC-AUC</td>
<td>17 tasks from PubChem BioAssay</td>
</tr>
<tr>
<td></td>
<td>HIV</td>
<td>1</td>
<td>C</td>
<td>41913</td>
<td>ROC-AUC</td>
<td>Ability to inhibit HIV replication</td>
</tr>
<tr>
<td></td>
<td>BACE</td>
<td>1</td>
<td>C</td>
<td>1522</td>
<td>ROC-AUC</td>
<td>Binding results for inhibitors of human BACE-1</td>
</tr>
<tr>
<td>Physiology</td>
<td>BBBP</td>
<td>1</td>
<td>C</td>
<td>2053</td>
<td>ROC-AUC</td>
<td>Blood-brain barrier penetration</td>
</tr>
<tr>
<td></td>
<td>Tox21</td>
<td>12</td>
<td>C</td>
<td>8014</td>
<td>ROC-AUC</td>
<td>Toxicity measurements</td>
</tr>
<tr>
<td></td>
<td>SIDER</td>
<td>27</td>
<td>C</td>
<td>1427</td>
<td>ROC-AUC</td>
<td>Adverse drug reactions on 27 system organs</td>
</tr>
<tr>
<td></td>
<td>ClinTox</td>
<td>2</td>
<td>C</td>
<td>1491</td>
<td>ROC-AUC</td>
<td>Clinical trial toxicity and FDA approval status</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of data efficiency metric (DEM) with the baseline models on 10 datasets from MoleculeNet (Wu et al. 2018). The up/down arrows show that the higher/lower score is better, respectively.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>ESOL $\downarrow$</th>
<th>FrSlv $\downarrow$</th>
<th>Lipo $\downarrow$</th>
<th>MUV $\uparrow$</th>
<th>HIV $\uparrow$</th>
<th>BACE $\uparrow$</th>
<th>BBBP $\uparrow$</th>
<th>Tox21 $\uparrow$</th>
<th>Sider $\uparrow$</th>
<th>ClinTox $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td>ST+MLP (Ours)</td>
<td>1.144</td>
<td>2.246</td>
<td>1.169</td>
<td>0.009</td>
<td>0.683</td>
<td>0.719</td>
<td>0.900</td>
<td>0.706</td>
<td>0.559</td>
<td>0.963</td>
</tr>
<tr>
<td>ECFP+MLP</td>
<td>1.741</td>
<td>3.043</td>
<td>1.090</td>
<td>0.036</td>
<td>0.697</td>
<td>0.769</td>
<td>0.760</td>
<td>0.616</td>
<td>0.588</td>
<td>0.515</td>
</tr>
<tr>
<td>RNNS2S+MLP</td>
<td>1.317</td>
<td>2.987</td>
<td>1.219</td>
<td>0.010</td>
<td>0.682</td>
<td>0.717</td>
<td>0.884</td>
<td>0.702</td>
<td>0.558</td>
<td>0.904</td>
</tr>
<tr>
<td>GraphConv</td>
<td>1.673</td>
<td>3.476</td>
<td>1.062</td>
<td>0.004</td>
<td>0.723</td>
<td>0.744</td>
<td>0.795</td>
<td>0.687</td>
<td>0.557</td>
<td>0.936</td>
</tr>
</tbody>
</table>
<p>be suitable for the tasks. That is, in classification tasks $m$ should be the area under the receiver operation characteristics (ROC-AUC) or the F1 score and in regression tasks $m$ should be the R2 score or the root mean squared error (RMSE).</p>
<p>Now the proposed Data Efficiency Metric (DEM) is formulated as:</p>
<p>$M_{D E}(f,m)=\frac{1}{|I|} \sum_{i \in I} m\left(f_{i}, X_{i}, Y_{i}\right)$ (1)</p>
<p>Since we used various datasets with a wide range of sizes in the experiment described below, the percentage of the training data $i$ should be increased exponentially. Therefore, $i$ is doubly increased from 1.25% to 80%, i.e., $I=$ ${0.0125,0.025,...,0.4,0.8}$.</p>
<h2>3 Experiments</h2>
<p>We conducted five experiments to see how SMILES Transformer works from different perspectives. First, we evaluated the performance of ST against other baseline models on 10 chemical datasets. Second, we visualized the latent space to answer the question: why do ST fingerprints work well for certain datasets? Third, we applied linear models to ST and other fingerprints in order to validate that ST maps molecules to a good latent space by minimizing the contribution of the models themselves. Fourth, we evaluated our ST and baseline models on a stratified dataset by the lengths of SMILES to see when ST provides an advantage. Finally, we compared the maximum performance of ST against state-of-the-art models under large data settings.</p>
<h3>3.1 Performance on Downstream Tasks</h3>
<p>Datasets We evaluated the performance of our pre-trained SMILES Transformer on 10 datasets from MoleculeNet (Wu et al. 2018), a benchmark for molecular property prediction. These datasets were chosen because they do not use 3D information and the sizes are not too large. The datasets are different from each other in their domains, task types, and sizes.</p>
<ul>
<li>Physical chemistry: ESOL, FreeSolv, and Lipophilicity</li>
<li>Biophysics: MUV, HIV, and BACE</li>
<li>Physiology: BBBP, Tox21, SIDER, and ClinTox</li>
</ul>
<p>The information about each dataset is summarized in Table 1. For the evaluation metrics, we used the root mean squared error (RMSE) for the regression tasks and the area under the receiver operating characteristic curve (ROCAUC) or the area under the precision-recall curve (PRCAUC) for the classification tasks as suggested in (Wu et al. 2018).</p>
<p>Baseline models We compared our pre-trained SMILES Transformer to the following three baseline models for molecular property prediction tasks:</p>
<ul>
<li>ECFP4 (Rogers and Hahn 2010) is a hand-crafted fingerprint. It hashes multi-scaled substructures to integers and makes a fixed-length binary vector where 1 indicates the existence of the assigned substructure and 0 for the absence. ECFP4 counts substructures with the diameters up to 4.</li>
<li>
<p>RNNS2S (Xu et al. 2017) is another text-based pretrained fingerprint that adopts RNN Seq2seq for the model architecture.</p>
</li>
<li>
<p>GraphConv (Duvenaud et al. 2015) learns and predicts the target value directly through graph convolution operations, rather than extracting fingerprints and building another model for supervised downstream tasks. Although GraphConv is not a task-agnostic fingerprint, we include it here as the state-of-the-art model.</p>
</li>
</ul>
<p>We used RDKit (Landrum 2016) to compute ECFP4 and DeepChem (Ramsundar et al. 2019) implementation of GraphConv (with the default hyperparmeters). For RNNS2S, we implemented it with PyTorch (Paszke et al. 2017) and pre-trained it with the same dataset as ST. The encoder and the decoder are both 3-layer bidirectional gated recurrent units (GRUs) (Cho et al. 2014) with 256 hidden vector dimensions. We obtained the same dimension of fingerprint as ST by concatenating two outputs from the last and the penultimate layer.</p>
<p>Experiment settings In the downstream tasks, we used simple models, such as multilayer perceptron (MLP) classifiers and regressors with the same default hyperparameters in scikit-learn (Pedregosa et al. 2011) in order to evaluate the performance of the three fingerprints, themselves as much as possible. All of these fingerprints have 1,024 dimensions. The datasets were randomly split (stratified for classification) to train sets and test sets by the percentage $i$. Note that we did not use a scaffold split suggested in (Wu et al. 2018). We ran 20 trials for each split and report the mean score and standard deviation in Figure 2 and DEM in Table 2. The metrics were chosen as recommended in MoleculeNet.</p>
<p>Results Table 2 shows DEM of the 4 models. ST achieves the best score in 5 out of 10 datasets, followed by ECFP and GraphConv.</p>
<p>See Figure 2 for the performance change against the train size. In ESOL, FreeSolv, BBBP, and ClinTox, ST performs the best at almost all points by a significant margin and especially high scores when the train size is small compared to the other models. In Tox21, ST supports good prediction along RNNS2S, but is beaten by GraphConv as the train size increase. In Lipophilicity, MUV, BACE, and SIDER, ECFP or GraphConv can predict better than ST.</p>
<h3>3.2 Visualization of the Latent Space</h3>
<p>To inspect why our ST fingerprints lead to good predictive performance, we visualized the latent space and decode some samples from it. For each dataset in MoleculeNet, we conducted the following procedure:</p>
<ol>
<li>Calculate the ST fingerprint (1024-dimension) of each molecule.</li>
<li>Reduce their dimensions to 2 with t-SNE (Maaten and Hinton 2008).</li>
<li>Plot the reduced features into a 2-dimensional space coloring by the target value.</li>
<li>Choose a trajectory in the 2-dimensional space and divide it into 12 points.</li>
<li>Find the nearest neighbors of the 12 points and draw the corresponding molecules.</li>
</ol>
<p>We show the result of the three datasets where ST fingerprints work especially well in Figure 3, that is, FreeSolv, BBBP, and ClinTox. In FreeSolv, it can be seen that there is a clear gradation from upper left to lower right, and the molecule becomes simpler (i.e., less loops and branches) along the trajectory. In BBBP and ClinTox, the categorical target values are successfully separated, but there is no clear trends in the decoded molecules.</p>
<h3>3.3 Application of Simple Predictive Models</h3>
<p>In Section 3.1, we used MLP for the predictive model to ST, RNNS2S, and ECFP, expecting that combining it with these fingerprints would work comparably or better than GraphConv. Here we used the simplest models to measure the pure effect of the fingerprints. To be specific, adding L2 regularization to avoid overfitting, we used ridge regression for regression tasks and logistic regression with L2 penalty for classification tasks. We excluded MUV and SIDER from this experiment because their highly imbalanced columns caused errors in the solver and ROC-AUC functions implemented in scikit-learn (Pedregosa et al. 2011). We followed the same procedure as in Section 3.1 except for the model selection and datasets and the results are shown in Table 3.</p>
<p>Our ST fingerprints with linear models achieved the best scores in 5 out of 8 datasets, indicating that the ST fingerprint is a strong fingerprint that leads to the best performance regardless of model selection.</p>
<h3>3.4 Stratified Scores by the Size of Molecules</h3>
<p>We conducted another study to inspect when ST has an advantage against other models. We stratified the BBBP dataset by the lengths of SMILES (similar to the sizes of the molecules) into 5 groups and evaluated within each group. The scores and the distributions of the lengths of SMILES are shown in Figure 4.</p>
<p>Figure 4 indicates that the ROC-AUC score of ST increases along the length of SMILES, which is a similar trend to the other text-based fingerprint, RNNS2S. On the other hand, GraphConv shows more or less the same performance regardless of the SMILES lengths. These results suggest that longer SMILES give ST richer information for better discrimination.</p>
<h3>3.5 Comparison with Record Scores</h3>
<p>Finally, we compared the maximum performance of ST under the large data setting with the reported scores in MoleculeNet. Since the ST fingerprint is proven to be better than the RNNS2S fingerprint, we omitted it and instead added another graph-based model named Weave (Kearnes et al. 2016) to the baselines. In this experiment, the datasets were split into train, validation, test sets with the proportion of $80 \%$, $10 \%, 10 \%$. The validation sets were used for hyperparameter tuning and the test sets were only used for calculating the scores. To fairly compare with the reported scores, the datasets HIV, BACE, BBBP used a scaffold split and the others were split randomly. We choose the model and hyperparameter set achieving the best validation score with optuna (Akiba et al. 2019), from a linear model with L2 penalty,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparison of model performance against different train size on the 10 datasets. The top row indicates the results for the physical chemistry datasets, the second row indicates biophysics, and the two bottom rows indicate physiology, respectively. The scores were averaged over 20 trials and the error bars are the standard deviations</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Visualization of the latent space of SMILES Transformer. For three datasets, FreeSolv, BBBP, and ClinTox, the dimensions of ST fingerprints of the molecules are reduced to 2 with t-SNE (Maaten and Hinton 2008). Then, the nearest neighbors of the 12 data points on a trajectories are plotted on the latent space (left panel). The 12 points are decoded to molecules and shown in the right panel. The color bar of the top left panel indicates the standardized free energy.</p>
<p>Table 3: Comparison of data efficiency metric (DEM) with the baseline models on the 8 datasets from MoleculeNet (Wu et al. 2018). The predictive models are ridge regression and logistic regression with L2 penalty. The up/down arrows show that the higher/lower score is better, respectively.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>ESOL $\downarrow$</th>
<th>FrSlv $\downarrow$</th>
<th>Lipo $\downarrow$</th>
<th>HIV $\uparrow$</th>
<th>BACE $\uparrow$</th>
<th>BBBP $\uparrow$</th>
<th>Tox21 $\uparrow$</th>
<th>ClinTox $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td>ST (Ours)</td>
<td>1.140</td>
<td>2.452</td>
<td>1.213</td>
<td>0.696</td>
<td>0.720</td>
<td>0.895</td>
<td>0.711</td>
<td>0.958</td>
</tr>
<tr>
<td>ECFP</td>
<td>1.678</td>
<td>2.843</td>
<td>1.174</td>
<td>0.727</td>
<td>0.790</td>
<td>0.825</td>
<td>0.710</td>
<td>0.704</td>
</tr>
<tr>
<td>RNNS2S</td>
<td>1.288</td>
<td>2.881</td>
<td>1.194</td>
<td>0.688</td>
<td>0.727</td>
<td>0.884</td>
<td>0.709</td>
<td>0.915</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of the best achieved scores with the record scores on the 8 datasets from MoleculeNet (Wu et al. 2018). The scores of ECFP, GraphConv, and Weave are the reported scores in MoleculeNet. The up/down arrows show that the higher/lower score is the better, respectively.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>ESOL $\downarrow$</th>
<th>FrSlv $\downarrow$</th>
<th>Lipo $\downarrow$</th>
<th>HIV $\uparrow$</th>
<th>BACE $\uparrow$</th>
<th>BBBP $\uparrow$</th>
<th>Tox21 $\uparrow$</th>
<th>ClinTox $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Splitting</td>
<td>random</td>
<td>random</td>
<td>random</td>
<td>scaffold</td>
<td>scaffold</td>
<td>scaffold</td>
<td>random</td>
<td>random</td>
</tr>
<tr>
<td>ST (Ours)</td>
<td>0.72</td>
<td>1.65</td>
<td>0.921</td>
<td>0.729</td>
<td>0.701</td>
<td>0.704</td>
<td>0.802</td>
<td>0.954</td>
</tr>
<tr>
<td>ECFP</td>
<td>0.99</td>
<td>1.74</td>
<td>0.799</td>
<td>0.792</td>
<td>0.867</td>
<td>0.729</td>
<td>0.822</td>
<td>0.799</td>
</tr>
<tr>
<td>GraphConv</td>
<td>0.97</td>
<td>1.40</td>
<td>0.655</td>
<td>0.763</td>
<td>0.783</td>
<td>0.690</td>
<td>0.829</td>
<td>0.807</td>
</tr>
<tr>
<td>Weave</td>
<td>0.61</td>
<td>1.22</td>
<td>0.715</td>
<td>0.703</td>
<td>0.806</td>
<td>0.671</td>
<td>0.820</td>
<td>0.832</td>
</tr>
</tbody>
</table>
<p>MLP, and LightGBM (Ke et al. 2017). We conducted three independent runs and reported the average scores in Table 4</p>
<p>ST achieves first place only in ClinTox, but performs comparable to ECFP and graph-based models in the other datasets. We can conclude that our ST fingerprints, if carefully tuned, are still useful even when the large number of labels are available.</p>
<h2>4 Conclusions</h2>
<p>In this paper, we propose SMILES-Transformer, a datadriven molecular fingerprint produced by a Transformer-based seq2seq pre-trained with a huge set of unlabeled SMILES. ST fingerprints were shown to work well with any predictive model in MoleculeNet downstream tasks and is effective especially when there is not enough labeled data. When large labeled data are available, ST fingerprints work comparable to other state-of-the-art baselines such as GraphConv. We also propose DEM, a novel metric for data efficiency. In terms of DEM, the ST fingerprint is better than existing methods in 5 out of 10 downstream tasks.</p>
<p>Future work can continue in three directions. First, replacing the Transformer in ST with Transformer-XL, an extended model that can handle much longer sequences, will alleviate the length limit of ST. Second, ST will be even stronger when trained in a multi-task fashion as done in ChemNet (Goh et al. 2018): predicting automatically-calculated molecular descriptors (e.g., molecular weight, LogP) as well as decoding the input SMILES. This will help the model to learn more chemistry-relevant representations. Finally, making use of the information of enumerated SMILES is one of the keys to improving text-based molecular representations. As done in (Bjerrum and Sattarov 2018), a set of different SMILES of the same molecule can be used to restrict the latent space.</p>
<p>Our implementation for SMILES-Transformer is available at https://github.com/DSPsleeporg/smiles-transformer</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: ROC-AUC scores on each stratified group by the lengths of SMILES (left) and the distributions of the lengths of SMILES (right) of BBBP dataset.</p>
<h2>References</h2>
<p>Akiba, T.; Sano, S.; Yanase, T.; Ohta, T.; and Koyama, M. 2019. Optuna: A next-generation hyperparameter optimization framework. In ACM SIGKDD, 2623-2631. ACM.
Altae-Tran, H.; Ramsundar, B.; Pappu, A. S.; and Pande, V. 2017. Low data drug discovery with one-shot learning. ACS Cent. Sci. 3(4):283-293.
Bjerrum, E., and Sattarov, B. 2018. Improving chemical autoencoder latent space and molecular de novo generation diversity with heteroencoders. Biomolecules 8(4):131.
Bjerrum, E. J. 2017. Smiles enumeration as data augmentation for neural network modeling of molecules. arXiv preprint arXiv:1703.07076.
Cho, K.; van Merrienboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP, 1724-1734.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 4171-4186.
Duvenaud, D. K.; Maclaurin, D.; Iparraguirre, J.; Bombarell, R.; Hirzel, T.; Aspuru-Guzik, A.; and Adams, R. P. 2015. Convolutional networks on graphs for learning molecular fingerprints. In NeurIPS, 2224-2232.
Gaulton, A.; Hersey, A.; Nowotka, M.; Bento, A. P.; Chambers, J.; Mendez, D.; Mutowo, P.; Atkinson, F.; Bellis, L. J.; Cibrián-Uhalte, E.; et al. 2016. The chembl database in 2017. Nucleic acids research 45(D1):D945-D954.</p>
<p>Goh, G. B.; Siegel, C.; Vishnu, A.; and Hodas, N. 2018. Using rule-based labels for weak supervised learning: a chemnet for transferable chemical property prediction. In ACM SIGKDD, 302-310. ACM.
Gómez-Bombarelli, R.; Wei, J. N.; Duvenaud, D.; Hernández-Lobato, J. M.; Sánchez-Lengeling, B.; Sheberla, D.; Aguilera-Iparraguirre, J.; Hirzel, T. D.; Adams, R. P.; and Aspuru-Guzik, A. 2018. Automatic chemical design using a data-driven continuous representation of molecules. ACS Cent. Sci. 4(2):268-276.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning for image recognition. In CVPR, 770-778.
Jin, W.; Barzilay, R.; and Jaakkola, T. 2018. Junction tree variational autoencoder for molecular graph generation. In ICML, 2328-2337.
Ke, G.; Meng, Q.; Finley, T.; Wang, T.; Chen, W.; Ma, W.; Ye, Q.; and Liu, T.-Y. 2017. Lightgbm: A highly efficient gradient boosting decision tree. In NeurIPS, 3146-3154.
Kearnes, S.; McCloskey, K.; Berndl, M.; Pande, V.; and Riley, P. 2016. Molecular graph convolutions: moving beyond fingerprints. J. Comput. Aided Mol. Des. 30(8):595-608.
Kingma, D. P., and Ba, J. 2015. Adam: A method for stochastic optimization. In $I C L R$.
Kingma, D. P., and Welling, M. 2014. Auto-encoding variational Bayes. In $I C L R$.
Kusner, M. J.; Paige, B.; and Hernández-Lobato, J. M. 2017.</p>
<p>Grammar variational autoencoder. In ICML, 1945-1954. JMLR. org.
Landrum, G. 2016. Rdkit: Open-source cheminformatics.
Maaten, L. v. d., and Hinton, G. 2008. Visualizing data using t-sne. J. Mach. Learn. Res. 9(Nov):2579-2605.
Öztürk, H.; Özgür, A.; and Ozkirimli, E. 2018. Deepdta: deep drug-target binding affinity prediction. Bioinformatics 34(17):i821-i829.
Paszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.; DeVito, Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer, A. 2017. Automatic differentiation in pytorch. In NIPS.</p>
<p>Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.; Weiss, R.; Dubourg, V.; et al. 2011. Scikit-learn: Machine learning in python. J. Mach. Learn. Res. 12(Oct):2825-2830.
Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. In NAACL-HLT, 2227-2237.
Ramsundar, B.; Eastman, P.; Walters, P.; Pande, V.; Leswing, K.; and Wu, Z. 2019. Deep Learning for the Life Sciences. O'Reilly Media. https://www.amazon.com/Deep-Learning-Life-Sciences-Microscopy/dp/1492039837.
Rogers, D., and Hahn, M. 2010. Extended-connectivity fingerprints. J. Chem. Inf. Model. 50(5):742-754.
Schwaller, P.; Laino, T.; Gaudin, T.; Bolgar, P.; Bekas, C.; and Lee, A. A. 2018. Molecular transformer for chemical reaction prediction and uncertainty estimation. arXiv preprint arXiv:1811.02633.
Segler, M. H.; Preuss, M.; and Waller, M. P. 2018. Planning chemical syntheses with deep neural networks and symbolic ai. Nature 555(7698):604.
Silver, D.; Schrittwieser, J.; Simonyan, K.; Antonoglou, I.; Huang, A.; Guez, A.; Hubert, T.; Baker, L.; Lai, M.; Bolton, A.; et al. 2017. Mastering the game of go without human knowledge. Nature 550(7676):354.
Sutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence to sequence learning with neural networks. In NeurIPS, 31043112.</p>
<p>Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is all you need. In NeurIPS, 5998-6008.
Winter, R.; Montanari, F.; Noé, F.; and Clevert, D.-A. 2019. Learning continuous and data-driven molecular descriptors by translating equivalent chemical representations. Chem. Sci. 10(6):1692-1701.
Wu, Z.; Ramsundar, B.; Feinberg, E. N.; Gomes, J.; Geniesse, C.; Pappu, A. S.; Leswing, K.; and Pande, V. 2018. Moleculenet: a benchmark for molecular machine learning. Chem. Sci. 9(2):513-530.
Xu, Z.; Wang, S.; Zhu, F.; and Huang, J. 2017. Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery. In ACM BCB, 285-294. ACM.
Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov, R.; and Le, Q. V. 2019. Xlnet: Generalized autoregres-</p>
<p>sive pretraining for language understanding. arXiv preprint arXiv:1906.08237.</p>            </div>
        </div>

    </div>
</body>
</html>
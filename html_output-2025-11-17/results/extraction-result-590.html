<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-590 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-590</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-590</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-69ca13b8a7386f6e03a8d200d24a66509a16e6f7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/69ca13b8a7386f6e03a8d200d24a66509a16e6f7" target="_blank">Quantifying Reproducibility in NLP and ML</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is found that this paper is able to straightforwardly derive a practical framework for assessing reproducibility which has the desirable property of yielding a quantified degree of reproducecibility that is comparable across different reproduction studies.</p>
                <p><strong>Paper Abstract:</strong> Reproducibility has become an intensely debated topic in NLP and ML over recent years, but no commonly accepted way of assessing reproducibility, let alone quantifying it, has so far emerged. The assumption has been that wider scientific reproducibility terminology and definitions are not applicable to NLP/ML, with the result that many different terms and definitions have been proposed, some diametrically opposed. In this paper, we test this assumption, by taking the standard terminology and definitions from metrology and applying them directly to NLP/ML. We find that we are able to straightforwardly derive a practical framework for assessing reproducibility which has the desirable property of yielding a quantified degree of reproducibility that is comparable across different reproduction studies.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e590.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e590.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIM-framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VIM-based metrological framework for quantifying reproducibility in NLP/ML</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical framework that maps the International Vocabulary of Metrology (VIM) definitions of repeatability and reproducibility to NLP/ML, defining conditions of measurement, precision metrics and a 2-phase assessment procedure to quantify degree of reproducibility across studies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Quantifying Reproducibility in NLP and ML</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing / Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Assessment and quantification of reproducibility/variability of metric and human-evaluation measurements in NLP/ML experiments (e.g., wF1 scores, human clarity/fluency ratings, physical measurement examples used as analogues).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Multiple sources are identified and catalogued as measurement conditions: different teams/operators, different dates/times, object conditions (e.g., preprocessing, data versions, epoch of object), measurement method conditions (implementation differences, evaluation code, metric implementation), measurement procedure conditions (test set, compile-time environment, run-time environment, use of containers like Docker), random seeds and cross-validation protocols, reimplementation vs. original code, differences in rating scales or human evaluators, lack of recording of measurement conditions, and differences in measuring equipment or dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Coefficient of variation (CV, with small-sample correction CV*), unbiased sample standard deviation (sˆ), mean, 95% confidence intervals (using t-distribution), percentage of measured values within n standard deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Examples reported in the paper: Torc mass (7 measurements): CV* = 2.61, mean = 88.89 g, unbiased sˆ = 2.24 g, 95% CI for sˆ = (0.784, 3.696), n=7. If restricted to 4 measurements with known conditions (scales differ only): CV* = 0.519. Repeatability subsets: SWS scales CV* = 0.11; CBD scales CV* = 0.0. Text classifier wF1 (8 measurements): CV* = 3.818%, mean = 0.7036, unbiased sˆ = 0.0261, 95% CI [0.01, 0.04], n=8. NLG human ratings (Clarity rescaled 0..6, 2 measurements): CV* = 13.193, mean = 4.969, sˆ = 0.583, 95% CI [-2.75, 3.92], n=2. Fluency rescaled: CV* = 16.372, mean = 4.75, sˆ = 0.691, 95% CI [-3.26, 4.645], n=2.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Same precision metrics as variability_metrics (CV*, sˆ, mean, 95% CI, percent within nσ); repeatability R0 and reproducibility R computed as precision of replicate measured quantity values under same vs. varied condition sets; 2-phase assessment protocol (baseline repeatability phase then reproducibility phase) is proposed and used conceptually and with examples.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Quantified reproducibility using CV*: Torc full set under varying conditions CV* = 2.61; with better-controlled scale conditions CV* = 0.519; classifier wF1 across reproduction attempts CV* = 3.818%. Human-evaluation reproduction (two studies) produced high CV* (Clarity 13.193%, Fluency 16.372%). The paper also notes that only 14% of 513 original/reproduction score pairs in a cited survey were exactly identical (Belz et al., 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Incomplete recording of measurement conditions (lack of metadata), small sample sizes for reproducibility estimation, non-determinism from random seeds and cross-validation, environment differences (compile-time/run-time), reimplementation differences, differing test sets or preprocessing, differing evaluation code/metric implementations, human-evaluator differences and rating-scale mismatches, and residual variation even when code/data are shared; containerisation not always practical and cannot address human-evaluation variability.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Proposed and discussed methods: specify and share detailed 'conditions of measurement' (object, measurement method, measurement procedure) using checklists (e.g., ML code completeness checklist, ACL reproducibility checklist, human-evaluation datasheet); perform baseline repeatability assessment (phase 1) before reproducibility tests (phase 2); use de-biased estimators and small-sample corrections for precision metrics; report CV* and confidence intervals; share code, dependencies, pre-trained models, commands, and environment details; use containerisation (Docker) where practical; fix random seeds or run multiple seeds / cross-validation and report aggregated scores; map rating scales to a common 0-based range for comparability; document implementations of metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Illustrative quantitative outcomes: restricting torc measurements to known-scale differences reduced CV* from 2.61 to 0.519; further restricting to consistent scale subsets produced CV* = 0.11 and CV* = 0.0 for particular scale models, showing that controlling measurement conditions can substantially reduce variability. For classifier experiments the paper shows that differing CT/RT environments and use of multiple seeds (10-fold CV) account for some observed differences but does not present a controlled intervention experiment that systematically reduces CV across all cases. Containerisation is discussed as potentially reducing variation but not quantified; mapping rating scales enabled comparable CV computations for human ratings but did not reduce observed CV magnitudes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Varies by example: torc n=7 (also subsets n=4, subsets size 2-4), text classifier n=8 aggregated across reproduction studies, human-eval example n=2 (original + reproduction); the paper emphasizes small sample sizes and recommends n ≥ 3 for CV but proceeds with available small-n examples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Applying standard metrological definitions (VIM) yields a practical, comparable, and quantitative reproducibility measure (CV*) for NLP/ML; empirical examples show non-negligible residual variability even when code/data are shared (e.g., wF1 CV* = 3.818%), and controlling and reporting measurement conditions (and performing baseline repeatability tests) can substantially reduce measured variability (e.g., torc scale-controlled CV* reduced from 2.61 to 0.519).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Reproducibility in NLP and ML', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Experiments with universal CEFR classification <em>(Rating: 2)</em></li>
                <li>Reproducing monolingual, multilingual and cross-lingual CEFR predictions <em>(Rating: 1)</em></li>
                <li>REPROLANG 2020: Automatic proficiency scoring of Czech, English, German, Italian, and Spanish learner essays <em>(Rating: 1)</em></li>
                <li>Another pass: A reproduction study of the human evaluation of a football report generation system <em>(Rating: 2)</em></li>
                <li>A systematic review of reproducibility research in natural language processing <em>(Rating: 2)</em></li>
                <li>The machine learning reproducibility checklist v2.0 <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-590",
    "paper_id": "paper-69ca13b8a7386f6e03a8d200d24a66509a16e6f7",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "VIM-framework",
            "name_full": "VIM-based metrological framework for quantifying reproducibility in NLP/ML",
            "brief_description": "A practical framework that maps the International Vocabulary of Metrology (VIM) definitions of repeatability and reproducibility to NLP/ML, defining conditions of measurement, precision metrics and a 2-phase assessment procedure to quantify degree of reproducibility across studies.",
            "citation_title": "Quantifying Reproducibility in NLP and ML",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Processing / Machine Learning",
            "experimental_task": "Assessment and quantification of reproducibility/variability of metric and human-evaluation measurements in NLP/ML experiments (e.g., wF1 scores, human clarity/fluency ratings, physical measurement examples used as analogues).",
            "variability_sources": "Multiple sources are identified and catalogued as measurement conditions: different teams/operators, different dates/times, object conditions (e.g., preprocessing, data versions, epoch of object), measurement method conditions (implementation differences, evaluation code, metric implementation), measurement procedure conditions (test set, compile-time environment, run-time environment, use of containers like Docker), random seeds and cross-validation protocols, reimplementation vs. original code, differences in rating scales or human evaluators, lack of recording of measurement conditions, and differences in measuring equipment or dependencies.",
            "variability_measured": true,
            "variability_metrics": "Coefficient of variation (CV, with small-sample correction CV*), unbiased sample standard deviation (sˆ), mean, 95% confidence intervals (using t-distribution), percentage of measured values within n standard deviations.",
            "variability_results": "Examples reported in the paper: Torc mass (7 measurements): CV* = 2.61, mean = 88.89 g, unbiased sˆ = 2.24 g, 95% CI for sˆ = (0.784, 3.696), n=7. If restricted to 4 measurements with known conditions (scales differ only): CV* = 0.519. Repeatability subsets: SWS scales CV* = 0.11; CBD scales CV* = 0.0. Text classifier wF1 (8 measurements): CV* = 3.818%, mean = 0.7036, unbiased sˆ = 0.0261, 95% CI [0.01, 0.04], n=8. NLG human ratings (Clarity rescaled 0..6, 2 measurements): CV* = 13.193, mean = 4.969, sˆ = 0.583, 95% CI [-2.75, 3.92], n=2. Fluency rescaled: CV* = 16.372, mean = 4.75, sˆ = 0.691, 95% CI [-3.26, 4.645], n=2.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Same precision metrics as variability_metrics (CV*, sˆ, mean, 95% CI, percent within nσ); repeatability R0 and reproducibility R computed as precision of replicate measured quantity values under same vs. varied condition sets; 2-phase assessment protocol (baseline repeatability phase then reproducibility phase) is proposed and used conceptually and with examples.",
            "reproducibility_results": "Quantified reproducibility using CV*: Torc full set under varying conditions CV* = 2.61; with better-controlled scale conditions CV* = 0.519; classifier wF1 across reproduction attempts CV* = 3.818%. Human-evaluation reproduction (two studies) produced high CV* (Clarity 13.193%, Fluency 16.372%). The paper also notes that only 14% of 513 original/reproduction score pairs in a cited survey were exactly identical (Belz et al., 2021).",
            "reproducibility_challenges": "Incomplete recording of measurement conditions (lack of metadata), small sample sizes for reproducibility estimation, non-determinism from random seeds and cross-validation, environment differences (compile-time/run-time), reimplementation differences, differing test sets or preprocessing, differing evaluation code/metric implementations, human-evaluator differences and rating-scale mismatches, and residual variation even when code/data are shared; containerisation not always practical and cannot address human-evaluation variability.",
            "mitigation_methods": "Proposed and discussed methods: specify and share detailed 'conditions of measurement' (object, measurement method, measurement procedure) using checklists (e.g., ML code completeness checklist, ACL reproducibility checklist, human-evaluation datasheet); perform baseline repeatability assessment (phase 1) before reproducibility tests (phase 2); use de-biased estimators and small-sample corrections for precision metrics; report CV* and confidence intervals; share code, dependencies, pre-trained models, commands, and environment details; use containerisation (Docker) where practical; fix random seeds or run multiple seeds / cross-validation and report aggregated scores; map rating scales to a common 0-based range for comparability; document implementations of metrics.",
            "mitigation_effectiveness": "Illustrative quantitative outcomes: restricting torc measurements to known-scale differences reduced CV* from 2.61 to 0.519; further restricting to consistent scale subsets produced CV* = 0.11 and CV* = 0.0 for particular scale models, showing that controlling measurement conditions can substantially reduce variability. For classifier experiments the paper shows that differing CT/RT environments and use of multiple seeds (10-fold CV) account for some observed differences but does not present a controlled intervention experiment that systematically reduces CV across all cases. Containerisation is discussed as potentially reducing variation but not quantified; mapping rating scales enabled comparable CV computations for human ratings but did not reduce observed CV magnitudes.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Varies by example: torc n=7 (also subsets n=4, subsets size 2-4), text classifier n=8 aggregated across reproduction studies, human-eval example n=2 (original + reproduction); the paper emphasizes small sample sizes and recommends n ≥ 3 for CV but proceeds with available small-n examples.",
            "key_findings": "Applying standard metrological definitions (VIM) yields a practical, comparable, and quantitative reproducibility measure (CV*) for NLP/ML; empirical examples show non-negligible residual variability even when code/data are shared (e.g., wF1 CV* = 3.818%), and controlling and reporting measurement conditions (and performing baseline repeatability tests) can substantially reduce measured variability (e.g., torc scale-controlled CV* reduced from 2.61 to 0.519).",
            "uuid": "e590.0",
            "source_info": {
                "paper_title": "Quantifying Reproducibility in NLP and ML",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Experiments with universal CEFR classification",
            "rating": 2
        },
        {
            "paper_title": "Reproducing monolingual, multilingual and cross-lingual CEFR predictions",
            "rating": 1
        },
        {
            "paper_title": "REPROLANG 2020: Automatic proficiency scoring of Czech, English, German, Italian, and Spanish learner essays",
            "rating": 1
        },
        {
            "paper_title": "Another pass: A reproduction study of the human evaluation of a football report generation system",
            "rating": 2
        },
        {
            "paper_title": "A systematic review of reproducibility research in natural language processing",
            "rating": 2
        },
        {
            "paper_title": "The machine learning reproducibility checklist v2.0",
            "rating": 2
        }
    ],
    "cost": 0.009248,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Quantifying Reproducibility in NLP and ML</h1>
<p>Anya Belz<br>ADAPT Centre<br>Dublin City University, Ireland<br>anya.belz@adaptcentre.ie</p>
<h4>Abstract</h4>
<p>Reproducibility has become an intensely debated topic in NLP and ML over recent years, but no commonly accepted way of assessing reproducibility, let alone quantifying it, has so far emerged. The assumption has been that wider scientific reproducibility terminology and definitions are not applicable to NLP/ML, with the result that many different terms and definitions have been proposed, some diametrically opposed. In this paper, we test this assumption, by taking the standard terminology and definitions from metrology and applying them directly to NLP/ML. We find that we are able to straightforwardly derive a practical framework for assessing reproducibility which has the desirable property of yielding a quantified degree of reproducibility that is comparable across different reproduction studies.</p>
<h2>1 Introduction</h2>
<p>Reproducibility of results is coming under increasing scrutiny in the machine learning (ML) and natural language processing (NLP) fields, against the background of a perceived reproducibility crisis in science more widely (Baker, 2016), and NLP/ML specifically (Pedersen, 2008; Mieskes et al., 2019). There have been several workshops and checklist initiatives on the topic, ${ }^{1}$ conferences are promoting reproducibility via calls, chairs' blogs, ${ }^{2}$ and special themes, and the first shared tasks are being organised, including REPROLANG'20 (Branco et al., 2020) and ReproGen'21 (Belz et al., 2020a). The biggest impact so far has been that sharing</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>of code, data and supplementary material providing increasingly detailed information about data, systems and training regimes is now expected as standard. Yet early optimism that "[r]eproducibility would be quite easy to achieve in machine learning simply by sharing the full code used for experiments" (Sonnenburg et al., 2007) is now giving way to the realisation that even with full resource sharing and original author support the same scores can often not be obtained (see Section 2.2).</p>
<p>Despite a growing body of research, no consensus has so far emerged about standards, terminology and definitions. Particularly for the two most frequently used terms, reproducibility and replicability, divergent definitions abound, variously conditioned on same vs. different team, methods, artifacts, code, software, and data. E.g. for the ACM (Association for Computing Machinery, 2020), results have been reproduced if obtained in a different study by a different team using artifacts supplied in part by the original authors, and replicated if obtained in a different study by a different team using artifacts not supplied by the original authors. Drummond (2009) argues that what ML calls reproducibility is in fact replicability which is the ability to re-run an experiment in exactly the same way, whereas true reproducibility is the ability to obtain the same result by different means. For Rougier et al. (2017), "[r]eproducing the result of a computation means running the same software on the same input data and obtaining the same results. [...]. Replicating a published result means writing and then running new software based on the description of a computational model or method provided in the original publication". Wieling et al. (2018) tie reproducibility to "the same data and methods," and Whitaker (2017), followed by Schloss (2018), tie definitions of reproducibility, replicability, robustness and generalisability to different combinations of same vs. different data and code.</p>
<p>Underlying this diversity of definitions is the as-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A torc fragment from the British Museum and some of the information provided about it in the museum's collection catalogue ${ }^{3}$ (image © The Trustees of the British Museum).
sumption that general reproducibility terminology and definitions somehow don't apply to computer science for which different terminology and definitions are needed. To the extent that terminology and definitions have been proposed for NLP/ML, these have tended to be sketchy and incomplete, e.g. the definitions above entirely skirt the issue of how to tell if two studies are or are not 'the same' in terms of code, method, team, etc.</p>
<p>In this paper we take the general definitions (Section 3) of the International Vocabulary of Metrology (VIM) (JCGM, 2012), and explore how these can be mapped to the NLP/ML context and extended into a practical framework for reproducibility assessment capable of yielding quantified assessments of degree of reproducibility (Section 4). We start with two informal examples (Section 2), one involving weight measurements of museum artifacts, the other weighted F1 measurements of a text classifier, and conclude with a discussion of limitations and future directions (Sections 6, 7).</p>
<h2>2 Example Physical and Non-physical Measurements</h2>
<h3>2.1 Mass of a torc</h3>
<p>The first example involves physical measurement where general scientific reproducibility definitions apply entirely uncontroversially. The torc (or neck ring) shown in Figure 1 is from the British Museum collection. ${ }^{3}$ The information provided about it in the collection catalogue (some of it also shown in Figure 1) includes a measurement of its weight which is given as 87.20 g .</p>
<p>Museum records contain two other weight mea-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>surements of torc 1991,0501.129, and museum staff performed four additional weighings for this study, yielding a set of seven measured quantity values for the mass of this torc, shown in the last column of Table 1. Details such as the scales used, whether they were calibrated, placed on a flat surface, etc., are not normally recorded in a museuym context, but the general expectation is nevertheless that the same readings are obtained. The values in Table 1 range from 87.2 g to 92 g , a sizable difference that implies there must have been some differences in the conditions under which the measurements were performed that resulted in the different measured values.</p>
<p>Museum records show ${ }^{4}$ that the measurements were taken by different people (Team) at different times, up to 30 years apart (Date); staff normally place the object on ordinary electronic scales noting down the displayed number and rounding to one tenth of a gramme (Measurement method); calibration is sometimes checked with a 10 g standard weight, but scales are not recalibrated (Measurement procedure). The museum catalogue also records details of a conservation treatment in 1991 for torc 1991,0501.129 (for full details see the link in Footnote 3) which included dirt removal with a scalpel, and immersion in acid (Object conditions).</p>
<p>Table 1 provides an overview of the conditions of measurement mentioned above (Team, Date, etc.), alongside the corresponding values for condition in each of the seven measurement where available. Despite some missing information, the information indicates that the treatment for dirt and corrosion removal in 1991 led to the biggest reduction in weight, with the exact scales and use of a standard weight potentially explaining some of the remaining differences.</p>
<h3>2.2 Weighted F1-score of a text classifier</h3>
<p>The second example involves non-physical measurements in the form of a set of eight weighted F1 (wF1) scores for the same NLP system variant of which seven were obtained in four reproduction studies of Vajjala and Rama (2018). In the original paper, Vajjala and Rama report variants of a text classifier that assigns grades to essays written by second-language learners of German, Italian and Czech. One of the multilingual system variants, re-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Measurand</th>
<th style="text-align: center;">Object <br> conditions <br> Treatments</th>
<th style="text-align: center;">Measurement <br> method conditions <br> Scales</th>
<th style="text-align: center;">Measurement pro- <br> cedure conditions <br> Standard weight</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Team</th>
<th style="text-align: center;">Measured <br> quantity <br> value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1991,0501.129</td>
<td style="text-align: center;">mass</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">1991</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">92 g</td>
</tr>
<tr>
<td style="text-align: center;">1991,0501.129</td>
<td style="text-align: center;">mass</td>
<td style="text-align: center;">0 ?</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">JFT</td>
<td style="text-align: center;">92.0 g</td>
</tr>
<tr>
<td style="text-align: center;">1991,0501.129</td>
<td style="text-align: center;">mass</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">10 g</td>
<td style="text-align: center;">2012</td>
<td style="text-align: center;">JF</td>
<td style="text-align: center;">87.2 g</td>
</tr>
<tr>
<td style="text-align: center;">1991,0501.129</td>
<td style="text-align: center;">mass</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">SWS pocket scales</td>
<td style="text-align: center;">none</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">CM</td>
<td style="text-align: center;">87.47 g</td>
</tr>
<tr>
<td style="text-align: center;">1991,0501.129</td>
<td style="text-align: center;">mass</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">SWS pocket scales</td>
<td style="text-align: center;">none</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">CM</td>
<td style="text-align: center;">87.37 g</td>
</tr>
<tr>
<td style="text-align: center;">1991,0501.129</td>
<td style="text-align: center;">mass</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">CBD bench counting scales</td>
<td style="text-align: center;">none</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">CM</td>
<td style="text-align: center;">88.1 g</td>
</tr>
<tr>
<td style="text-align: center;">1991,0501.129</td>
<td style="text-align: center;">mass</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">CBD bench counting scales</td>
<td style="text-align: center;">none</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">CM</td>
<td style="text-align: center;">88.1 g</td>
</tr>
</tbody>
</table>
<p>Table 1: Some of the conditions of measurement for the seven weight measurements of torc 1991,0501.129. SWS $=$ Smart Weigh Digital Pocket scale SWS100; CBD $=$ Adam CBD 4 bench counting scales.
ferred to below as multPOS ${ }^{-}$, uses part-of-speech (POS) n-grams, and doesn't use language identity information. The information provided about it in the paper includes a wF1 score of 0.726 (Table 3 in the paper).</p>
<p>We have seven other wF1 scores for multPOS ${ }^{-}$ from subsequent research which intended, in at least the first experiment in each case, to repeat the original experiment exactly. Arhiliuc et al. (2020) report a score of 0.680 wF1 (Table 3 in the paper). Huber and Çöltekin (2020) report 0.681 (Table 3 in the paper). Bestgen (2020) reports three wF1 scores: 0.680 and 0.722 (Table 2 in the paper) and 0.728 (Table 5). Caines and Buttery (2020) report 2 scores: 0.680 and 0.732 (Table 4 in the paper). Four of the measured quantity values (those shown first for each team in Table 2) were obtained in conditions as close to identical as the teams could manage, but nevertheless range from 0.680 to 0.726 , a difference in wF1 score that would be considered an impressive improvement in many NLP contexts.</p>
<p>From the papers we know that the scores were produced by five different teams (Team), up to two years apart (Date). wF1 measurements are normally performed by computing the wF1 score over paired system and target outputs, but no information about implementation of the wF1 algorithm (Measurement method) is given. In most cases, the original system code by Vajjala \&amp; Rama (V\&amp;R) was used, but Caines \&amp; Buttery (C\&amp;B) reimplemented it, and Bestgen (B) and Huber \&amp; Çöltekin (H\&amp;C) produced cross-validated results with 10 random seeds (Object conditions), rather than a single fixed seed. The measured values were all obtained in different compile-time (CT) (Object conditions), and run-time (RT) environments (Measurement procedure), except for two results where a Docker container was used. The test set was the same as the original in all but three cases (marked as B, C\&amp;B and H\&amp;C in the Inputs column) (Mea-
surement procedure).
Table 2 provides an overview of the conditions mentioned above (Team, Date, etc.) under which the eight measurements were obtained. Conditions can be seen as attribute-value pairs where the attribute name is here shown in the (lower) column heading, and the values for it in the table cells. We know the reason for some of the differences, because the authors controlled for them: e.g. the differences in compile time and run time environments explain the difference between Bestgen's first and second results, and reimplementation of the system code in R explains the difference between Caines \&amp; Buttery's first and second results. Despite missing information, we can see that different CT/RT, and performing 10-fold cross-validation vs. a single run with fixed seed, account for some of the remaining differences.</p>
<h3>2.3 Comparison</h3>
<p>In presenting and discussing the physical and nonphysical measurements and conditions in the preceding two sections, we introduced common terminology for which full VIM definitions will be provided in the next section. The intention in using the same terms and layout in Tables 1 and 2 was to bring out the similarities between the two sets of measurements, and to show that measurements can generally, whether physical or not, be characterised in the same way on the basis of the same general scientific terms and definitions.</p>
<p>The two fields, curating of museum artifacts and NLP/ML, also have in common that (i) conditions of measurement are not traditionally recorded in any great detail, making exact repetition of a measurement virtually impossible, (ii) despite this,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Measurand</th>
<th style="text-align: center;">Object conditions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Meas. method conditions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Measurement procedure conditions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Team</th>
<th style="text-align: center;">Measured quantity value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Code*</td>
<td style="text-align: center;">Seed</td>
<td style="text-align: center;">CT env</td>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Implem.</td>
<td style="text-align: center;">Procedure</td>
<td style="text-align: center;">Inputs</td>
<td style="text-align: center;">RT Env.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">multPOS ${ }^{\sim}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">V\&amp;R ${ }^{\dagger}$</td>
<td style="text-align: center;">V\&amp;R 1 fixed</td>
<td style="text-align: center;">V\&amp;R</td>
<td style="text-align: center;">$w F 1(0,1)$</td>
<td style="text-align: center;">V\&amp;R</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">V\&amp;R</td>
<td style="text-align: center;">V\&amp;R</td>
<td style="text-align: center;">2018</td>
<td style="text-align: center;">V\&amp;R</td>
<td style="text-align: center;">0.726 wF1</td>
</tr>
<tr>
<td style="text-align: center;">multPOS ${ }^{\sim}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">V\&amp;R</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">A et al. Win</td>
<td style="text-align: center;">$w F 1(0,1)$</td>
<td style="text-align: center;">V\&amp;R?</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">V\&amp;R</td>
<td style="text-align: center;">A et al. Win</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">A et al.</td>
<td style="text-align: center;">0.680 wF1</td>
</tr>
<tr>
<td style="text-align: center;">multPOS ${ }^{\sim}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">V\&amp;R</td>
<td style="text-align: center;">V\&amp;R 1 fixed</td>
<td style="text-align: center;">B MacOS</td>
<td style="text-align: center;">$w F 1(0,1)$</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">V\&amp;R</td>
<td style="text-align: center;">B MacOS</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">0.680 wF1</td>
</tr>
<tr>
<td style="text-align: center;">multPOS ${ }^{\sim}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">V\&amp;R</td>
<td style="text-align: center;">V\&amp;R 1 fixed</td>
<td style="text-align: center;">B Docker</td>
<td style="text-align: center;">$w F 1(0,1)$</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">V\&amp;R</td>
<td style="text-align: center;">B Docker</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">0.722 wF1</td>
</tr>
<tr>
<td style="text-align: center;">multPOS ${ }^{\sim}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">V\&amp;R+s</td>
<td style="text-align: center;">B 10 avg</td>
<td style="text-align: center;">B Docker</td>
<td style="text-align: center;">$w F 1(0,1)$</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">B Docker</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">0.728 wF1</td>
</tr>
<tr>
<td style="text-align: center;">multPOS ${ }^{\sim}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">V\&amp;R</td>
<td style="text-align: center;">V\&amp;R 1 fixed</td>
<td style="text-align: center;">C\&amp;B1</td>
<td style="text-align: center;">$w F 1(0,1)$</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">V\&amp;R</td>
<td style="text-align: center;">C\&amp;B1</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">C\&amp;B</td>
<td style="text-align: center;">0.680 wF1</td>
</tr>
<tr>
<td style="text-align: center;">multPOS ${ }^{\sim}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">C\&amp;B ${ }^{\ddagger}$</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">C\&amp;B2</td>
<td style="text-align: center;">$w F 1(0,1)$</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">C\&amp;B</td>
<td style="text-align: center;">C\&amp;B2</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">C\&amp;B</td>
<td style="text-align: center;">0.732 wF1</td>
</tr>
<tr>
<td style="text-align: center;">multPOS ${ }^{\sim}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">V\&amp;R</td>
<td style="text-align: center;">H\&amp;C 10 avg</td>
<td style="text-align: center;">H\&amp;C</td>
<td style="text-align: center;">$w F 1(0,1)$</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">H\&amp;C</td>
<td style="text-align: center;">H\&amp;C</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">H\&amp;C</td>
<td style="text-align: center;">0.681 wF1</td>
</tr>
</tbody>
</table>
<p>Table 2: Some conditions of measurement for eight weighted F1 measurements of Vajjala \&amp; Rama’s multilingual POS-ngram CEFR classifier system without language category information variant (multPOS ${ }^{\sim}$ ). *Code here shown separately from random seed. OTE $=$ outputs vs. targets evaluation, i.e. the standard procedure of obtaining outputs for a set of test inputs and computing metrics over system and target outputs.
there is nevertheless an expectation that repeat measurements should yield the same results, and (iii) historically, practitioners have not cared about these issues very much.</p>
<h2>3 VIM Definitions of Repeatability and Reproducibility</h2>
<p>The International Vocabulary of Metrology (VIM) (JCGM, 2012) defines repeatability and reproducibility as follows (all defined terms in boldface, see Appendix for full set of verbatim VIM definitions including subsidiary defined terms):
2.21 measurement repeatability (or repeatability, for short) is measurement precision under a set of repeatability conditions of measurement.
2.20 a repeatability condition of measurement (repeatability condition) is a condition of measurement, out of a set of conditions that includes the same measurement procedure, same operators, same measuring system, same operating conditions and same location, and replicate measurements on the same or similar objects over a short period of time.
2.25 measurement reproducibility (reproducibility) is measurement precision under reproducibility conditions of measurement.
2.24 a reproducibility condition of measurement (reproducibility condition) is a condition of measurement, out of a set of conditions that includes different locations, operators, measuring systems, etc. A specification should give the conditions changed and unchanged, to the extent practical.
The VIM definitions are squarely focused on measurement: repeatability and reproducibility are
properties of measurements (not objects, scores, results or conclusions), and are defined as measurement precision, i.e. both are quantified by calculating the precision of a set of measured quantity values. Moreover, both concepts are defined relative to a set of conditions of measurement, in other words, the conditions have to be known and specified for assessment of either concept (repeatability, reproducibility) to be meaningful.</p>
<h2>4 Mapping the VIM Definitions to a Metrological Framework for NLP/ML</h2>
<p>The VIM definitions provide the definitional basis for reproducibility assessment. However, for a framework that tells us what to do in practice in an NLP/ML context, we also need (i) a method for computing precision, (ii) a specification of the practical steps in performing assessments, and (iii) a specific set of repeatability/reproducibility conditions of measurement. In this section, we present the fixed, discipline-independent elements of the proposed framework (Section 4.1), methods for computing precision in NLP/ML (Section 4.2), a series of steps for practical assessment of reproducibility (Section 4.3), and, utilising previous work on reproducibility in the NLP/ML field, starting points for identifying suitable conditions of measurement (Section 4.4).</p>
<h3>4.1 Fixed elements of the framework</h3>
<p>Table 2 gave a first indication what (some) conditions might look like in NLP/ML, grouped into Object conditions $C^{O}$, Measurement method conditions $C^{N}$, and Measurement procedure conditions $C^{P}$. These conceptually useful groupings are retained to yield a skeleton framework shown in diagrammatic form in Figure 2, which corresponds to</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Diagrammatic overview of repeatability assessment of measurements M1, M2, ... Mn of object O, with measurand m, and repeatability conditions of measurements C (same for all M1). Repeatability R⁰ is defined as the precision of the set of values vˆ returned by Mˆ. (For C^O, C^N, and C^P, see in text.)</p>
<p>the following definition of repeatability R⁰:</p>
<p>$$
\begin{aligned}
&amp; R^0(M_1, M_2, \dots M_n): = \text{Precision}(v_1, v_2, \dots v_n) \
&amp; \text{where } M_i : (m, O, t_i, C) \mapsto v_i
\end{aligned}
$$</p>
<p>and the Mˆ are repeat measurements for measurand m performed on object O at different times tˆ under (the same) set of conditions C = C^O ∪ C^N ∪ C^P. Below the coefficient of variation is used as the precision measure, but other measures are possible. The members of each set of conditions are attribute/value pairs each consisting of a name and a value. Reproducibility R is defined in the same way except that condition values differ for at least one condition in the Mˆ:</p>
<p>$$
\begin{aligned}
&amp; R(M_1, M_2, \dots M_n): = \text{Precision}(v_1, v_2, \dots v_n) \
&amp; \text{where } M_i : (m, O, t_i, C_i) \mapsto v_i
\end{aligned}
$$</p>
<h3>4.2 Computing precision</h3>
<p>Precision in metrological studies is reported in terms of some or all of the following: mean, standard deviation with 95% confidence intervals, coefficient of variation, percentage of measured quantity values within n standard deviations.</p>
<p>In reproducibility assessment in NLP/ML, sample sizes tend to be very small (a sample size of 8 as in Section 2.2 is currently unique). We therefore need to use de-biased sample estimators: we use the unbiased sample standard deviation, denoted sˆ, with confidence intervals calculated using a t-distribution, and standard error (of the unbiased sample standard deviation) approximated on the basis of the standard error of the unbiased sample variance se(s²) as ses²(sˆ) ≈ 1/2σse(ˆs²) (Rao, 1973). Assuming measured quantity values are normally distributed, we calculate the standard error of the sample variance in the usual way: se(s²) = √2σs²/σ1. Finally, we also use a small sample correction for the coefficient of variation: CVˆ = (1 + 1/4n)CV (Sokal and Rohlf, 1971).</p>
<p>Equipped with the above, the reproducibility of wF1 measurements of Vajjala and Rama (2018)'s MultPOS˘ system can, for example, be quantified based on the eight replicate measurements from Table 2 (disregarding the role of measurement conditions for the moment) as being CVˆ = 3.818. This and two other example applications of the framework is presented in more detail in Section 5.</p>
<h3>4.3 Steps in reproducibility assessment</h3>
<p>In order to relate multiple repeatability and reproducibility assessments of the same object and measurand to each other and compare them, they need to share the same conditions of measurement (same in terms of attribute names, not necessarily values, see next section). Repeatability is simply the special case of reproducibility where all condition values are also the same.</p>
<p>For a true estimate of the variation resulting from given differences in condition values, the baseline variation, present when all condition values are the same, needs to be known. It is therefore desirable to carry out repeatability assessment prior to reproducibility assessment in order to be able to take into account the baseline amount of variation. E.g. if the coefficient of variation is xˆC⁰ under identical conditions C⁰, and xˆC under varied conditions C, then it's the difference between xˆC⁰ and xˆC that estimates the effect of varying the conditions. Finally, for very small sample sizes, both baseline repeatability assessment, and subsequent reproducibility assessment, should use the same sample size to ensure accurate assessment of variation due to the</p>
<p><sup>7</sup>Code for computing CVˆ available here: https://github.com/asbelz/coeff-var</p>
<h2>2-PHASE REPRODUCIBILITY ASSESSMENT</h2>
<h2>REPEATABILITY PHASE</h2>
<ol>
<li>Select measurement to be assessed, identify shared object and measurand.</li>
<li>Select initial set of repeatability conditions of measurement $C^{0}$ and specify value for each condition.</li>
<li>Perform $n \geq 2$ reproduction measurements to yield measured quantity values $v_{1}^{0}, v_{2}^{0}, \ldots v_{n}^{0}$.</li>
<li>Compute precision for $v_{1}^{0}, v_{2}^{0}, \ldots v_{n}^{0}$, giving repeatability score $R^{0}$.</li>
<li>Unless precision is as small as desired, identify additional conditions that had different values in some of the reproduction measurement, and add them to the set of measurement conditions, also updating the measurements to ensure same values for the new conditions. Repeat Steps 3-5.</li>
</ol>
<h2>REPRODUCIBILITY PHASE</h2>
<ol>
<li>From the final set of repeatability conditions, select the conditions to vary, and specify the different values to test.</li>
<li>For each combination of differing condition values:
(a) Carry out $n$ reproduction tests, yielding measured quantity values $v_{1}, v_{2}, \ldots v_{n}$
(b) Compute precision for $v_{1}, v_{2}, \ldots v_{n}$, giving reproducibility score $R$.</li>
</ol>
<p>Report all resulting $R$ scores, alongside baseline $R^{0}$ score.</p>
<p>Figure 3: Steps in 2-phase reproducibility assessment with baseline repeatability assessment. Step 5 is obsolete if a field has standard conditions of measurement.
varied condition values (alone).
Figure 3 translates what we have said in this and preceding sections into a 2-phase framework for reproducibility assessment. If a field develops shared standard conditions of measurement, Step 5 is obsolete. In situations where reproduction studies have been carried out without a pre-defined, shared set of conditions of measurement, reproducibility assessment can still be carried out (as we did for the Vajjala \&amp; Rama system in Section 2.2), but in this situation a baseline assessment of variation under repeatability conditions of measurement is not possible, and the only option is to use a single-phase version of the framework as shown in Figure 4. See Section 6 for more discussion of this issue.</p>
<h3>4.4 Conditions of measurement</h3>
<p>The final component needed for a metrological framework for reproducibility assessment in NLP/ML is a specific set of conditions of measurement. As mentioned in Section 4.1, individual conditions consist of name and value, and their role is to capture those attributes of a measurement where different values may cause differences in measured values. As indicated by Step 5 in Figure 3, for repeatability, conditions should be selected with a view to reducing baseline variation. As it's not practically feasible (or, normally, theoretically possible) to specify and control for all conditions that can be the same or different in a measurement, some judgment is called for here (see Section 6 for discussion). The idea is that a discipline evolves shared standard conditions that address this.</p>
<p>It so happens that much of the reproducibility work in ML and NLP has so far focused on what standard conditions of measurement (information about system, data, dependencies, computing environment, etc.) for metric measurements need to be specified in order to enable repeatability assessment, even if it hasn't been couched in these terms. Out of all reproducibility topics, pinning down the information and resources that need to be shared to enable others to obtain the same metric scores is the one that has attracted by far the most discussion and papers. In fact, reproduction in NLP/ML has become synonymous with rerunning code and obtaining the same metric scores again as were obtained in a previous study.</p>
<p>Reproducibility checklists such as those provided by (Pineau, 2020) and the ACL ${ }^{8}$ are lists of types of information (attributes) for which authors are asked to provide information (values), and these can directly be construed as conditions of measurement. In this section, the intention is not to propose definitive sets of conditions relating to object, measurement method and measurement procedure that should be used in NLP/ML. Rather, in each subsection, we point to existing research such as the above that can be used to provide a 'starter set' of conditions of measurement.</p>
<h3>4.4.1 Object conditions</h3>
<p>The ML Code Completeness Checklist ${ }^{9}$ (adopted as part of the NeurIPS'21 guidelines) consists of</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>1-PHASE REPRODUCIBILITY ASSESSMENT</p>
<ol>
<li>For set of $n$ measurements to be assessed, identify shared object and measurand.</li>
<li>Identify all conditions of measurement $C$ for which information is available for all measurements, and specify values for each condition.</li>
<li>Gather the $n$ measured quantity values $v_{1}, v_{2}, \ldots v_{n}$</li>
<li>Compute precision for $v_{1}, v_{2}, \ldots v_{n}$, giving reproducibility score $R$.</li>
</ol>
<p>Report resulting $R$ score.</p>
<p>Figure 4: Steps in 1-phase reproducibility assessment for assessing a set of existing measurements where baseline repeatibility assessment is not possible. See also discussion in Section 6.
five items: specification of dependencies, training code, evaluation code, pre-trained models, README file including results and commands. These provide a good starting point for object conditions of measurement (note we group evaluation code under measurement method conditions, see next section):</p>
<ol>
<li>Dependencies</li>
<li>Training code</li>
<li>Pre-trained models</li>
<li>Precise commands to run code/produce results</li>
<li>Compile-time environment</li>
<li>Run-time environment</li>
</ol>
<p>In human evaluation of system outputs, object conditions don't apply, as the object of measurement is fully specified by a sample of its outputs.</p>
<h3>4.4.2 Measurement method conditions</h3>
<p>In metric-based measurement where the measurand is defined mathematically, a measurement method is an implementation of a method for (or, conceivably, a manual way of) computing a quantity value for the measurand. In this case, the method, like the Object of measurement, is a computational artifact, so the same conditions can be used as in the preceding section (albeit with different names to mark the difference; values will clearly also differ).</p>
<p>In human-evaluation-based measurement, a different set of conditions is needed. Here, the measurand is identified by (ideally standardised) name and definition for the quantity being assessed. This has
been termed 'quality criterion' ${ }^{10}$ in previous work (Howcroft et al., 2020; Belz et al., 2020b) which took a first stab at a set of standardised names and definitions. These, in conjunction with a checklist for human evaluation such as the one proposed by Shimorina and Belz (2021), can provide a starting point for measurement method conditions (as follows), and measurement procedure conditions (next section), for human evaluation.</p>
<ol>
<li>Name and definition of measurand</li>
<li>Evaluation mode ${ }^{11}$</li>
<li>Method of response elicitation</li>
<li>Method for aggregating or otherwise processing raw participant responses</li>
<li>Any code used (conditions as for Object)</li>
</ol>
<h3>4.4.3 Measurement procedure conditions</h3>
<p>Measurement procedure conditions capture any information needed to apply a given measurement method in practice. In metric-based measurement, this includes:</p>
<ol>
<li>Test set</li>
<li>Any preparatory steps taken, such as preprocessing of text</li>
<li>Any code used (conditions as for Object)</li>
</ol>
<p>In human-evaluation-based measurement, some of the remaining properties from Shimorina and Belz (2021) and Howcroft et al. (2020) can be used:</p>
<ol>
<li>If test set evaluation, test set and preprocessing code/method(s); if system interaction, specification of system set up</li>
<li>Responses collection method</li>
<li>Quality assurance code/method(s)</li>
<li>Instructions to evaluators</li>
<li>Evaluation interface</li>
<li>Any code used (conditions as for Object)</li>
</ol>
<h2>5 Examples</h2>
<p>In this section we apply the framework to the two sets of measurements from Sections 2.1 and 2.2, and an additional set of measurements from a recent reproduction study (Mille et al., 2021).</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>5.1 Mass of a Torc</h3>
<p>In the case of the torc mass measurements, the reproducibility analysis is performed post hoc, i.e. we cannot obtain more information for the three older measurements than was recorded at the time. We therefore have to use the 1-phase assessment (Figure 4), and for the complete set of seven weighings specify all conditions (Table 1) as different. $\mathrm{CV}^{*}$ results can then be reported as follows:</p>
<p>Mass measurement reproducibility under reproducibility conditions of measurement as detailed in Table 1, was assessed on the basis of seven measurements of torc 1991,0501.129 as follows: the unbiased coefficient of variation is 2.61, for a mean of 88.89, unbiased sample standard deviation of 2.24 with $95 \%$ CI (0.784, 3.696), and sample size 7. All measured values fall within two standard deviations, 71.43\% within one standard deviation.</p>
<p>If we used just the four measurements for which all condition values are known, we would get $\mathrm{CV}^{<em>}=$ 0.519 under reproducibility conditions where only the scales used differ. For a sample size of 4 we can still be reasonably confident that this is a good estimate of $\mathrm{CV}^{</em>}$ for the whole population (stdev $95 \%$ CI $[-0.04,0.90])$.</p>
<p>Repeatability assessment can be performed for the two subsets of measurements for the SDS scales and the CBD scales, which gives $\mathrm{CV}^{<em>}=0.11$ for the former and $\mathrm{CV}^{</em>}=0$ for the latter, for similar-size stdev confidence intervals.</p>
<h3>5.2 wF1 of a Text Classifier</h3>
<p>The situation is similar for the text classifier wF1 measurements (Table 2) in that we are restricted to the information made available by the authors, which is however, more complete than in the torc example. Here too we have to use the 1-phase assessment (Figure 4), and for the complete set of 8 wF1 values specify all conditions as different. $\mathrm{CV}^{<em>}$ results can then be summed up as above:
wF1 measurement reproducibility under reproducibility conditions of measurement as in Table 2, was assessed on the basis of eight measurements reported by Vajjala and Rama (2018), Arhiliuc et al. (2020), Bestgen (2020), Caines and Buttery (2020), and Huber and Çöltekin (2020) (Table 2): CV</em> $=\mathbf{3 . 8 1 8}$, mean $=0.7036$, unbiased sample standard deviation $=0.0261$, $95 \%$ CI [0.01, 0.04], sample size $=8$. All measured values fall within two standard deviations,
$87 \%$ within one standard deviation.</p>
<h3>5.3 Clarity and Fluency of an NLG system</h3>
<p>The third example comes from a recent reproduction study (Mille et al., 2021) which repeated the human evaluation of a Dutch-language football report generation system (van der Lee et al., 2017). There were two main measurands, mean Clarity ratings and mean Fluency ratings, and a single object (the report generator) was evaluated. There were two scores for each of the measurands, one from the original study, one from the reproduction study. Here the situation was that a repeatability study was intended, but not possible. ${ }^{12}$ Therefore a different set of evaluators and a different evaluation interface had to be used.</p>
<p>Both Clarity and Fluency ratings were obtained on a 7-point scale (1..7). Computed on the scores as reported, for Clarity, $\mathrm{CV}^{<em>}=10.983$, for Fluency, $\mathrm{CV}^{</em>}=13.525$. However, if the 7 -point scale had been $0 . .6$, higher $\mathrm{CV}^{*}$ values would have been obtained, i.e. results are not comparable across different rating scales. To address this, rating scales should be mapped to range with lowest score 0 . Results can then be reported as follows:</p>
<p>Clarity measurement reproducibility under reproducibility conditions of measurement as detailed in Mille et al. (2021), was assessed on the basis of 2 measurements reported by van der Lee et al. (2017) and Mille et al. (2021), rescaled to 0..6: CV* $=\mathbf{1 3 . 1 9 3}$, mean $=4.969$, unbiased sample standard deviation $=0.583$, $95 \%$ CI [-2.75, 3.92], sample size $=2$. Measured values fall within one standard deviation.</p>
<p>Fluency measurement reproducibility under reproducibility conditions of measurement as detailed in Mille et al. (2021), was assessed on the basis of 2 measurements reported by van der Lee et al. (2017) and Mille et al. (2021), rescaled to 0..6: CV* $=\mathbf{1 6 . 3 7 2}$, mean $=4.75$, unbiased sample standard deviation $=0.691$, $95 \%$ CI [-3.26, 4.645], sample size $=2$. Measured values fall within one standard deviation.</p>
<h3>5.3.1 Notes on Examples</h3>
<p>$\mathrm{CV}^{*}$ is fronted in the examples above as the 'headline' result. It can take on this role, because it is a general measure, not in the unit of the measurements (unlike mean and standard deviation),</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>providing a quantification of precision (degree of reproducibility) that is comparable across studies (Ahmed, 1995, p. 57). This also holds for percentage within $n$ standard deviations but the latter is a less recognised measure, and likely to be the less intuitive for many.</p>
<p>CV is a measure of precision and as such, sample size should be $\geq 3$. For a sample of 2 (as in the human evaluation in the last section), $\mathrm{CV}^{*}$ is still meaningful as a measure of the variation found in the sample. However, it will generally provide a less reliable estimate of population CV.</p>
<h2>6 Discussion</h2>
<p>Specifying conditions of measurement in reproducibility assessment is important so results can be compared across different measures and assessments. We have not attempted to come up with a definitive set of conditions, but pointed to other research as a starting point. One important role the conditions play is to facilitate estimation of baseline variation via repeatability testing. It could be argued that if the goal is to ensure as near as possible identical measurements in repeatability testing, then a straightforward way to achieve that is containerisation. However, firstly the purpose of repeatability testing is to assess variation under normal use and it's not realistic to always run systems in Docker containers even in a research context. Secondly, human evaluation can't be run in a container.</p>
<p>What counts as a good level of reproducibility can differ substantially between disciplines and contexts. E.g. in bio-science assays, ${ }^{13}$ precision (reported as coefficient of variation) ranges from typically $&lt;10 \%$ for enzyme assays, to $20-50 \%$ for in vivo and cell-based assays, and $&gt;300 \%$ for virus titer assays (AAH/USFWS, n.d.). For NLP, such typical CV ranges would have to be established over time, but it seems clear that we would expect typical CV for metric-based measurements to be much lower (better) than for human-assessmentbased measurements. For a set of wF1 measurements, the $3.8 \% \mathrm{CV}^{*}$ above seems high.</p>
<p>There are many ways in which results from similar studies can be compared and conclusions drawn from comparisons. For example, to make (subjective) judgments of whether the same conclusions can be drawn from a set of comparable experimen-</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tal results, or to ask a group of assessors to make such judgments, is a valid and informative thing to do, but it's not reproducibility assessment in the general scientific sense of the term. It can also be informative to consider the ease with which systems can be recreated, but reproducibility is not a property of systems. Computer science has a history of departing from standard scientific reproducibility definitions, e.g. the ACM changed its definitions after NISO asked it to "harmonize its terminology and definitions with those used in the broader scientific research community." (Association for Computing Machinery, 2020). The question is, if the standard scientific terminology and definitions work for computer science, why would we not use them exactly as they are, rather than adapt them in often fundamental ways?</p>
<h2>7 Conclusion</h2>
<p>The reproducibility debate in NLP/ML has long been framed in terms of pinning down exactly what information we need to share so that others are guaranteed to get the same metric scores (e.g. Sonnenburg et al., 2007). What is becoming clear, however, is that no matter how much of our code, data and ancillary information we share, residual amounts of variation remain that are stubbornly resistant to being eliminated. A recent survey (Belz et al., 2021) found that just $14 \%$ of the 513 original/reproduction score pairs analysed were exactly the same. Judging the remainder simply 'not reproduced' would be of limited usefulness, as some are much closer to being the same than others, while assessments of whether the same conclusions can be drawn are prone to low levels of agreement. Quantifying closeness of results, and, over time, establishing expected levels closeness, seems a better way forward.</p>
<p>In this paper our aim has been to challenge the assumption that the general scientific reproducibility terms and definitions are not applicable or suitable for NLP/ML by directly mapping them to a practical framework that yields quantified assessments of degree of reproducibility that are comparable across different studies.</p>
<p>The NLP/ML field certainly needs some way of assessing degree of reproducibility that is comparable across studies, because the ability to assess reproducibility of results, hence the trustworthiness of evaluation practices, is a cornerstone of scientific research that the field has not yet fully achieved.</p>
<h2>Acknowledgements</h2>
<p>The contribution to this work made by Dr Julia Farley, curator (European Iron Age and Roman Conquest period collections) at the British Museum, and colleagues, is gratefully acknowledged. In particular their time and patience in obtaining the seven historical and new weighings of a 2,000 year old torc, as well as providing detailed information about the weighings.</p>
<h2>References</h2>
<p>AAH/USFWS. n.d. Assay validation methods: Definitions and terms. Aquatic Animal Health Program, U.S. Fish \&amp; Wildlife Service.</p>
<p>SE Ahmed. 1995. A pooling methodology for coefficient of variation. Sankhyā: The Indian Journal of Statistics, Series B, pages 57-75.</p>
<p>Cristina Arhiliuc, Jelena Mitrović, and Michael Granitzer. 2020. Language proficiency scoring. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 5624-5630, Marseille, France. European Language Resources Association.</p>
<p>Association for Computing Machinery. 2020. Artifact review and badging Version 1.1, August 24, 2020. https://www. acm.org/publications/policies/ artifact-review-and-badging-current.</p>
<p>Monya Baker. 2016. Reproducibility crisis. Nature, 533(26):353-66.</p>
<p>Anja Belz, Shubham Agarwal, Anastasia Shimorina, and Ehud Reiter. 2021. A systematic review of reproducibility research in natural language processing. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 381-393.</p>
<p>Anya Belz, Shubham Agarwal, Anastasia Shimorina, and Ehud Reiter. 2020a. ReproGen: Proposal for a shared task on reproducibility of human evaluations in NLG. In Proceedings of the 13th International Conference on Natural Language Generation, pages 232-236, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Anya Belz, Simon Mille, and David M. Howcroft. 2020b. Disentangling the properties of human evaluation methods: A classification system to support comparability, meta-evaluation and reproducibility testing. In Proceedings of the 13th International Conference on Natural Language Generation, pages 183-194, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Yves Bestgen. 2020. Reproducing monolingual, multilingual and cross-lingual CEFR predictions. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 5595-5602, Marseille, France. European Language Resources Association.</p>
<p>António Branco, Nicoletta Calzolari, Piek Vossen, Gertjan Van Noord, Dieter van Uytvanck, João Silva, Luís Gomes, André Moreira, and Willem Elbers. 2020. A shared task of a new, collaborative type to foster reproducibility: A first exercise in the area of language science and technology with REPROLANG2020. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 5539-5545, Marseille, France. European Language Resources Association.</p>
<p>Andrew Caines and Paula Buttery. 2020. REPROLANG 2020: Automatic proficiency scoring of Czech, English, German, Italian, and Spanish learner essays. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 5614-5623, Marseille, France. European Language Resources Association.</p>
<p>Chris Drummond. 2009. Replicability is not reproducibility: not is it good science. Presented at 4th Workshop on Evaluation Methods for Machine Learning held at ICML'09.</p>
<p>David M. Howcroft, Anya Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad Mahamood, Simon Mille, Emiel van Miltenburg, Sashank Santhanam, and Verena Rieser. 2020. Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions. In Proceedings of the 13th International Conference on Natural Language Generation, pages 169-182, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Eva Huber and Çağrı Çöltekin. 2020. Reproduction and replication: A case study with automatic essay scoring. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 56035613, Marseille, France. European Language Resources Association.</p>
<p>JCGM. 2012. International vocabulary of metrologybasic and general concepts and associated terms (VIM).</p>
<p>Margot Mieskes, Karën Fort, Aurélie Névéol, Cyril Grouin, and Kevin Cohen. 2019. Community perspective on replicability in natural language processing. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019), pages 768-775, Varna, Bulgaria. INCOMA Ltd.</p>
<p>Simon Mille, Thiago Castro Ferreira, Anya Belz, and Brian Davis. 2021. Another pass: A reproduction study of the human evaluation of a football report generation system. In Proceedings of the 14th International Conference on Natural Language Generation (INLG 2021).</p>
<p>Ted Pedersen. 2008. Empiricism is not a matter of faith. Computational Linguistics, 34(3):465-470.</p>
<p>Joelle Pineau. 2020. The machine learning reproducibility checklist v2.0.</p>
<p>Calyampudi Radhakrishna Rao. 1973. Linear statistical inference and its applications. Wiley.</p>
<p>Nicolas P Rougier, Konrad Hinsen, Frédéric Alexandre, Thomas Arildsen, Lorena A Barba, Fabien CY Benureau, C Titus Brown, Pierre De Buyl, Ozan Caglayan, Andrew P Davison, et al. 2017. Sustainable computational science: the rescience initiative. PeerJ Computer Science, 3:e142.</p>
<p>Patrick D Schloss. 2018. Identifying and overcoming threats to reproducibility, replicability, robustness, and generalizability in microbiome research. MBio, $9(3)$.</p>
<p>Anastasia Shimorina and Anya Belz. 2021. The human evaluation datasheet 1.0: A template for recording details of human evaluation experiments in nlp. arXiv preprint arXiv:2103.09710.
R.R. Sokal and F.J. Rohlf. 1971. Biometry: The Principles and Practice of Statistics in Biological Research. WH Freeman.</p>
<p>Soren Sonnenburg, Mikio L Braun, Cheng Soon Ong, Samy Bengio, Leon Bottou, Geoffrey Holmes, Yann LeCunn, Klaus-Robert Muller, Fernando Pereira, Carl Edward Rasmussen, et al. 2007. The need for open source software in machine learning.</p>
<p>Sowmya Vajjala and Taraka Rama. 2018. Experiments with universal CEFR classification. In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 147-153, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Chris van der Lee, Emiel Krahmer, and Sander Wubben. 2017. Pass: A dutch data-to-text system for soccer, targeted towards specific audiences. In Proceedings of the 10th International Conference on Natural Language Generation, pages 95-104.</p>
<p>Kirstie Whitaker. 2017. The MT Reproducibility Checklist. https://www.cs.mcgill.ca/ -jpineau/ReproducibilityChecklist. pdf.</p>
<p>Martijn Wieling, Josine Rawee, and Gertjan van Noord. 2018. Reproducibility in computational linguistics: Are we willing to share? Computational Linguistics, 44(4):641-649.</p>
<h1>A Verbatim VIM Definitions</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Primary term (synonyms)</th>
<th style="text-align: center;">Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2.21 measurement repeatability (repeatability)</td>
<td style="text-align: center;">measurement precision under a set of repeatability conditions of measurement</td>
</tr>
<tr>
<td style="text-align: center;">2.20 (Note 1) repeatability condition of measurement (repeatability condition)</td>
<td style="text-align: center;">condition of measurement, out of a set of conditions that includes the same measurement procedure, same operators, same measuring system, same operating conditions and same location, and replicate measurements on the same or similar objects over a short period of time <br> NOTE 1 A condition of measurement is a repeatability condition only with respect to a specified set of repeatability conditions.</td>
</tr>
<tr>
<td style="text-align: center;">2.25 measurement reproducibility (reproducibility)</td>
<td style="text-align: center;">measurement precision under reproducibility conditions of measurement</td>
</tr>
<tr>
<td style="text-align: center;">2.24 (Note 2) reproducibility condition of measurement (reproducibility condition)</td>
<td style="text-align: center;">condition of measurement, out of a set of conditions that includes different locations, operators, measuring systems, and replicate measurements on the same or similar objects <br> NOTE 2 A specification should give the conditions changed and unchanged, to the extent practical.</td>
</tr>
<tr>
<td style="text-align: center;">1.1 quantity</td>
<td style="text-align: center;">property of a phenomenon, body, or substance, where the property has a magnitude that can be expressed as a number and a reference</td>
</tr>
<tr>
<td style="text-align: center;">1.19 quantity value (value of a quantity, value)</td>
<td style="text-align: center;">number and reference together expressing magnitude of a quantity</td>
</tr>
<tr>
<td style="text-align: center;">2.1 measurement</td>
<td style="text-align: center;">process of experimentally obtaining one or more quantity values that can reasonably be attributed to a quantity</td>
</tr>
<tr>
<td style="text-align: center;">2.3 measurand</td>
<td style="text-align: center;">quantity intended to be measured</td>
</tr>
<tr>
<td style="text-align: center;">2.5 measurement method</td>
<td style="text-align: center;">method of measurement generic description of a logical organization of operations used in a measurement</td>
</tr>
<tr>
<td style="text-align: center;">2.6 measurement procedure</td>
<td style="text-align: center;">detailed description of a measurement according to one or more measurement principles and to a given measurement method, based on a measurement model and including any calculation to obtain a measurement result <br> NOTE 1 A measurement procedure is usually documented in sufficient detail to enable an operator to perform a measurement.</td>
</tr>
<tr>
<td style="text-align: center;">2.9 measurement result (result of measurement)</td>
<td style="text-align: center;">set of quantity values being attributed to a measurand together with any other available relevant information</td>
</tr>
<tr>
<td style="text-align: center;">2.10 measured quantity value (value of a measured quantity, measured value)</td>
<td style="text-align: center;">quantity value representing a measurement result</td>
</tr>
<tr>
<td style="text-align: center;">2.15 measurement precision (precision)</td>
<td style="text-align: center;">closeness of agreement between indications or measured quantity values obtained by replicate measurements on the same or similar objects under specified conditions</td>
</tr>
<tr>
<td style="text-align: center;">4.1 indication</td>
<td style="text-align: center;">quantity value provided by a measuring instrument or a measuring system</td>
</tr>
</tbody>
</table>
<p>Table 3: Verbatim VIM definitions of repeatability, reproducibility and related concepts (JCGM, 2012). (In VIM, definitions also give the earlier definition number from the second edition in parentheses which we omit here. In Definition 2.20, Note 2 relating only to chemistry is omitted. In 2.5 and 2.6, less important notes are omitted for space reasons.)</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{13}$ An investigative analytic procedure in the physical sciences e.g. laboratory medicine.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ Information about museum records and practices in this and other sections very kindly provided by Dr Julia Farley, curator of the British Museum's European Iron Age and Roman Conquest Period collections.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
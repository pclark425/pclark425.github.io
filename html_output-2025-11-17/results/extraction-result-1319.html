<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1319 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1319</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1319</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-218763547</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2005.10614v1.pdf" target="_blank">Transfer learning based multi-fidelity physics informed deep neural network</a></p>
                <p><strong>Paper Abstract:</strong> For many systems in science and engineering, the governing differential equation is either not known or known in an approximate sense. Analyses and design of such systems are governed by data collected from the field and/or laboratory experiments. This challenging scenario is further worsened when data-collection is expensive and time-consuming. To address this issue, this paper presents a novel multi-fidelity physics informed deep neural network (MF-PIDNN). The framework proposed is particularly suitable when the physics of the problem is known in an approximate sense (low-fidelity physics) and only a few high-fidelity data are available. MF-PIDNN blends physics informed and data-driven deep learning techniques by using the concept of transfer learning. The approximate governing equation is first used to train a low-fidelity physics informed deep neural network. This is followed by transfer learning where the low-fidelity model is updated by using the available high-fidelity data. MF-PIDNN is able to encode useful information on the physics of the problem from the {\it approximate} governing differential equation and hence, provides accurate prediction even in zones with no data. Additionally, no low-fidelity data is required for training this model. Applicability and utility of MF-PIDNN are illustrated in solving four benchmark reliability analysis problems. Case studies to illustrate interesting features of the proposed approach are also presented.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1319.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1319.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FEniCS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The FEniCS Project (finite element solver)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source finite element package used in this paper to generate high-fidelity solutions/benchmarks for PDE examples (Burger's equation) and to produce high-fidelity data used for training and evaluating MF-PIDNN.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The fenics project version 1.5</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>FEniCS</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A finite-element PDE solver used to solve the full nonlinear Burgers' equation and to generate high-fidelity datasets/benchmarks (spatio-temporal solutions with boundary perturbations and viscous terms).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>fluid dynamics / PDEs</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high-fidelity PDE solver (FEM) used to produce detailed solutions of the full nonlinear Burgers' equation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Solves full nonlinear PDE including convective and viscous terms, resolves spatial transition layer, provides spatially and temporally resolved solutions (used as ground-truth HF data for reliability analysis); specifics such as mesh/time-step not reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>MF-PIDNN (and HF-DNN when trained on generated HF data)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Physics-informed and data-driven fully connected deep neural network (MF-PIDNN): FC-DNN used as PI-DNN on low-fidelity physics then updated via transfer learning on HF data; for Burger's example architecture: 6 hidden layers × 50 neurons, tanh activations (linear output).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Reliability analysis: predict transition-layer position z and compute probability of failure (P_f) at a specified time (extrapolation to t=10), i.e., scientific reasoning about PDE solution behavior under uncertain boundary perturbation δ.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>High-fidelity FEniCS-generated solutions / held-out spatio-temporal extrapolation (predict at t=10, outside observed times)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>MF-PIDNN: P_f = 0.2242 vs MCS (FEniCS benchmark) P_f = 0.2036 (error reported 8.5304%); HF-DNN and LF-PIDNN performed poorly (HF-DNN P_f = 0.932, LF-PIDNN P_f = 0). MF-PIDNN performance improved with increasing number of HF time snapshots (N_t) and showed convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Paper compares LF-PIDNN (physics-only on simplified model), HF-DNN (data-only trained on limited HF samples), and MF-PIDNN (PI-trained on low-fidelity physics then transfer-learned). MF-PIDNN produced results closest to FEniCS/MCS benchmarks; HF-DNN and LF-PIDNN produced large errors (LF often failed entirely). Increasing HF data/time-samples improved MF-PIDNN accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>No explicit quantitative minimal-fidelity threshold is given; authors demonstrate that a low-fidelity model that omits the nonlinear convective term still provides useful inductive bias when combined with limited HF data via transfer learning — i.e., some preserved physics/invariants in the low-fidelity model are sufficient for successful transfer in examples presented.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>LF-PIDNN (low-fidelity physics-only) failed to predict correct failure probability (produced P_f = 0). MF-PIDNN showed sensitivity to initialization and run-to-run variability for Burger's example; HF-DNN (data-only) failed when HF sample count was low.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer learning based multi-fidelity physics informed deep neural network', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1319.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1319.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATLAB ODE45</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MATLAB ODE45 (adaptive Runge-Kutta) solver</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MATLAB's adaptive Runge–Kutta ODE solver used in the paper to generate high-fidelity time‑series solutions for ODE examples (nonlinear oscillator and cell signaling cascade) that serve as training/validation data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MATLAB ODE45</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Adaptive Runge–Kutta (4,5) numerical integrator provided in MATLAB, used to integrate coupled nonlinear ODE systems to generate high-fidelity trajectories for the nonlinear oscillator and the biochemical (cell signaling) cascade.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics (nonlinear oscillator) and biology (cell signaling ODE model)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high-fidelity numerical ODE integrator (accurate time-integration of full nonlinear coupled ODEs used as HF reference data)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Adaptive time-stepping RK45 scheme resolving nonlinear dynamics and coupling terms; includes full nonlinearities and parameter interactions specified by the high-fidelity ODE models (no simplifications beyond numerical integration).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>MF-PIDNN (and HF-DNN as comparator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fully-connected deep neural networks used in physics-informed and transfer-learning pipeline; e.g., oscillator: 4 hidden layers × 50 neurons, tanh activations, two outputs; cell signaling: 4 hidden layers × 100 neurons, three outputs; PI loss formed with collocation points and AD; transfer learning updates last 1–2 layers on limited HF samples.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Reliability estimation (probability of failure) for dynamical systems: predict state variables at target times and compute failure probabilities (e.g., oscillator: threshold on x2 at t=5; cell-signaling: threshold on e3p at t=3), including extrapolation to times outside HF observation window.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Transfer from low-fidelity physics-informed model to ODE45-generated high-fidelity trajectories and to extrapolated time instants outside observed ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Nonlinear oscillator: MF-PIDNN P_f = 0.1576 vs MCS benchmark 0.1599 (error 0.95%). For alternative case (t=3): MF-PIDNN P_f = 0.0729 vs MCS 0.0651 (3.88% error). Cell signaling cascade: MF-PIDNN P_f = 0.17 vs MCS 0.1663 (1.52% error). HF-DNN and LF-PIDNN showed much larger errors in these cases.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Across oscillator and biochemical examples, MF-PIDNN (low-fidelity PI training + transfer learning on limited HF ODE data) consistently outperformed HF-DNN (data-only on small HF datasets) and LF-PIDNN (physics-only on approximate model). MF-PIDNN yielded near-MCS accuracy with very few HF samples (e.g., 5–10 realizations).</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors do not quantify a minimal fidelity specification numerically, but show that even decoupled or simplified low-fidelity ODEs (e.g., I=0 in cell signaling, or replacing a nonlinear term) can provide enough structure for transfer learning to succeed when combined with limited HF data.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>HF-DNN trained only on the small HF datasets produced large errors (e.g., oscillator/HF-DNN errors up to tens of percent); LF-PIDNN alone often produced erroneous reliability estimates (large biases).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer learning based multi-fidelity physics informed deep neural network', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1319.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1319.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Monte Carlo Simulation (MCS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo Simulation (numerical sampling for reliability estimates)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Used throughout the paper as the ground-truth benchmark: large-sample Monte Carlo runs (with external high-fidelity solver) generate reference failure probabilities and reliability indices against which MF-PIDNN and comparator methods are evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Simulation and the Monte Carlo method</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Monte Carlo simulation (driven by numerical solvers)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Statistical sampling procedure that repeatedly evaluates the (high-fidelity) solver across samples of stochastic inputs to estimate probabilities (used with FEniCS or MATLAB solvers to obtain reference P_f and reliability indices).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general (used for reliability quantification across PDE and ODE examples: fluid dynamics, mechanics, biology)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>empirical/reference estimator; fidelity depends on numerical solver used for each sample (treated as ground truth in the paper when using large sample counts).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Produces converged empirical failure probabilities given large numbers of HF solver runs; examples: 10^6 samples for ODE example 1, 10^4 for many PDE/ODE benchmarks (paper reports these counts explicitly).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Estimate probability of failure / reliability indices for stochastic dynamical / PDE systems by sampling the high-fidelity solver over input distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>MCS results are taken as the reference; MF-PIDNN predictions are reported relative to MCS (MF-PIDNN errors typically small: e.g., 0% in Example 1 baseline, 2.98% in extrapolation case, ~1–9% in other examples).</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>MCS is not a learning method here; used as ground truth. No failure cases reported for MCS itself, but authors highlight MCS computational expense which motivates MF-PIDNN.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer learning based multi-fidelity physics informed deep neural network', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1319.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1319.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Large Eddy Simulation (LES)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large-Eddy Simulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in the introduction as an example of a numerical solver that might be considered 'low-fidelity' compared to experiments but is still computationally expensive — illustrating that 'low-fidelity' in multifidelity contexts does not always mean cheap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large-eddy simulation: Past, present and the future</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Large Eddy Simulation (LES)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A computational fluid dynamics approach that resolves large turbulent structures and models subgrid scales; cited as an example where a so-called 'low-fidelity' solver (relative to wind tunnel experiments) can still be very costly.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>fluid dynamics / turbulence modeling</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>described as a computationally expensive 'low-fidelity' solver relative to experimental data in context — effectively medium/high fidelity numerics for turbulence with approximate subgrid modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Resolves large-scale turbulent motions, models subgrid scales; computationally expensive; likely includes full Navier–Stokes physics with modeled subgrid stresses (paper provides no additional specifics).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Mentioned as context/example (no training tasks in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Used to motivate that low-fidelity solvers can still be costly; no quantitative minimal-fidelity guidance is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No experimental usage in this paper; mentioned only as a motivating example of costliness of some 'low-fidelity' solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer learning based multi-fidelity physics informed deep neural network', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations <em>(Rating: 2)</em></li>
                <li>Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling <em>(Rating: 2)</em></li>
                <li>On transfer learning of neural networks using bi-fidelity data for uncertainty propagation <em>(Rating: 2)</em></li>
                <li>A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse pde problems <em>(Rating: 2)</em></li>
                <li>Multi-fidelity physics-constrained neural network and its application in materials modeling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1319",
    "paper_id": "paper-218763547",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "FEniCS",
            "name_full": "The FEniCS Project (finite element solver)",
            "brief_description": "Open-source finite element package used in this paper to generate high-fidelity solutions/benchmarks for PDE examples (Burger's equation) and to produce high-fidelity data used for training and evaluating MF-PIDNN.",
            "citation_title": "The fenics project version 1.5",
            "mention_or_use": "use",
            "simulator_name": "FEniCS",
            "simulator_description": "A finite-element PDE solver used to solve the full nonlinear Burgers' equation and to generate high-fidelity datasets/benchmarks (spatio-temporal solutions with boundary perturbations and viscous terms).",
            "scientific_domain": "fluid dynamics / PDEs",
            "fidelity_level": "high-fidelity PDE solver (FEM) used to produce detailed solutions of the full nonlinear Burgers' equation",
            "fidelity_characteristics": "Solves full nonlinear PDE including convective and viscous terms, resolves spatial transition layer, provides spatially and temporally resolved solutions (used as ground-truth HF data for reliability analysis); specifics such as mesh/time-step not reported in paper.",
            "model_or_agent_name": "MF-PIDNN (and HF-DNN when trained on generated HF data)",
            "model_description": "Physics-informed and data-driven fully connected deep neural network (MF-PIDNN): FC-DNN used as PI-DNN on low-fidelity physics then updated via transfer learning on HF data; for Burger's example architecture: 6 hidden layers × 50 neurons, tanh activations (linear output).",
            "reasoning_task": "Reliability analysis: predict transition-layer position z and compute probability of failure (P_f) at a specified time (extrapolation to t=10), i.e., scientific reasoning about PDE solution behavior under uncertain boundary perturbation δ.",
            "training_performance": null,
            "transfer_target": "High-fidelity FEniCS-generated solutions / held-out spatio-temporal extrapolation (predict at t=10, outside observed times)",
            "transfer_performance": "MF-PIDNN: P_f = 0.2242 vs MCS (FEniCS benchmark) P_f = 0.2036 (error reported 8.5304%); HF-DNN and LF-PIDNN performed poorly (HF-DNN P_f = 0.932, LF-PIDNN P_f = 0). MF-PIDNN performance improved with increasing number of HF time snapshots (N_t) and showed convergence.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Paper compares LF-PIDNN (physics-only on simplified model), HF-DNN (data-only trained on limited HF samples), and MF-PIDNN (PI-trained on low-fidelity physics then transfer-learned). MF-PIDNN produced results closest to FEniCS/MCS benchmarks; HF-DNN and LF-PIDNN produced large errors (LF often failed entirely). Increasing HF data/time-samples improved MF-PIDNN accuracy.",
            "minimal_fidelity_discussion": "No explicit quantitative minimal-fidelity threshold is given; authors demonstrate that a low-fidelity model that omits the nonlinear convective term still provides useful inductive bias when combined with limited HF data via transfer learning — i.e., some preserved physics/invariants in the low-fidelity model are sufficient for successful transfer in examples presented.",
            "failure_cases": "LF-PIDNN (low-fidelity physics-only) failed to predict correct failure probability (produced P_f = 0). MF-PIDNN showed sensitivity to initialization and run-to-run variability for Burger's example; HF-DNN (data-only) failed when HF sample count was low.",
            "uuid": "e1319.0",
            "source_info": {
                "paper_title": "Transfer learning based multi-fidelity physics informed deep neural network",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "MATLAB ODE45",
            "name_full": "MATLAB ODE45 (adaptive Runge-Kutta) solver",
            "brief_description": "MATLAB's adaptive Runge–Kutta ODE solver used in the paper to generate high-fidelity time‑series solutions for ODE examples (nonlinear oscillator and cell signaling cascade) that serve as training/validation data.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "MATLAB ODE45",
            "simulator_description": "Adaptive Runge–Kutta (4,5) numerical integrator provided in MATLAB, used to integrate coupled nonlinear ODE systems to generate high-fidelity trajectories for the nonlinear oscillator and the biochemical (cell signaling) cascade.",
            "scientific_domain": "mechanics (nonlinear oscillator) and biology (cell signaling ODE model)",
            "fidelity_level": "high-fidelity numerical ODE integrator (accurate time-integration of full nonlinear coupled ODEs used as HF reference data)",
            "fidelity_characteristics": "Adaptive time-stepping RK45 scheme resolving nonlinear dynamics and coupling terms; includes full nonlinearities and parameter interactions specified by the high-fidelity ODE models (no simplifications beyond numerical integration).",
            "model_or_agent_name": "MF-PIDNN (and HF-DNN as comparator)",
            "model_description": "Fully-connected deep neural networks used in physics-informed and transfer-learning pipeline; e.g., oscillator: 4 hidden layers × 50 neurons, tanh activations, two outputs; cell signaling: 4 hidden layers × 100 neurons, three outputs; PI loss formed with collocation points and AD; transfer learning updates last 1–2 layers on limited HF samples.",
            "reasoning_task": "Reliability estimation (probability of failure) for dynamical systems: predict state variables at target times and compute failure probabilities (e.g., oscillator: threshold on x2 at t=5; cell-signaling: threshold on e3p at t=3), including extrapolation to times outside HF observation window.",
            "training_performance": null,
            "transfer_target": "Transfer from low-fidelity physics-informed model to ODE45-generated high-fidelity trajectories and to extrapolated time instants outside observed ranges.",
            "transfer_performance": "Nonlinear oscillator: MF-PIDNN P_f = 0.1576 vs MCS benchmark 0.1599 (error 0.95%). For alternative case (t=3): MF-PIDNN P_f = 0.0729 vs MCS 0.0651 (3.88% error). Cell signaling cascade: MF-PIDNN P_f = 0.17 vs MCS 0.1663 (1.52% error). HF-DNN and LF-PIDNN showed much larger errors in these cases.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Across oscillator and biochemical examples, MF-PIDNN (low-fidelity PI training + transfer learning on limited HF ODE data) consistently outperformed HF-DNN (data-only on small HF datasets) and LF-PIDNN (physics-only on approximate model). MF-PIDNN yielded near-MCS accuracy with very few HF samples (e.g., 5–10 realizations).",
            "minimal_fidelity_discussion": "Authors do not quantify a minimal fidelity specification numerically, but show that even decoupled or simplified low-fidelity ODEs (e.g., I=0 in cell signaling, or replacing a nonlinear term) can provide enough structure for transfer learning to succeed when combined with limited HF data.",
            "failure_cases": "HF-DNN trained only on the small HF datasets produced large errors (e.g., oscillator/HF-DNN errors up to tens of percent); LF-PIDNN alone often produced erroneous reliability estimates (large biases).",
            "uuid": "e1319.1",
            "source_info": {
                "paper_title": "Transfer learning based multi-fidelity physics informed deep neural network",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "Monte Carlo Simulation (MCS)",
            "name_full": "Monte Carlo Simulation (numerical sampling for reliability estimates)",
            "brief_description": "Used throughout the paper as the ground-truth benchmark: large-sample Monte Carlo runs (with external high-fidelity solver) generate reference failure probabilities and reliability indices against which MF-PIDNN and comparator methods are evaluated.",
            "citation_title": "Simulation and the Monte Carlo method",
            "mention_or_use": "use",
            "simulator_name": "Monte Carlo simulation (driven by numerical solvers)",
            "simulator_description": "Statistical sampling procedure that repeatedly evaluates the (high-fidelity) solver across samples of stochastic inputs to estimate probabilities (used with FEniCS or MATLAB solvers to obtain reference P_f and reliability indices).",
            "scientific_domain": "general (used for reliability quantification across PDE and ODE examples: fluid dynamics, mechanics, biology)",
            "fidelity_level": "empirical/reference estimator; fidelity depends on numerical solver used for each sample (treated as ground truth in the paper when using large sample counts).",
            "fidelity_characteristics": "Produces converged empirical failure probabilities given large numbers of HF solver runs; examples: 10^6 samples for ODE example 1, 10^4 for many PDE/ODE benchmarks (paper reports these counts explicitly).",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "Estimate probability of failure / reliability indices for stochastic dynamical / PDE systems by sampling the high-fidelity solver over input distributions.",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "MCS results are taken as the reference; MF-PIDNN predictions are reported relative to MCS (MF-PIDNN errors typically small: e.g., 0% in Example 1 baseline, 2.98% in extrapolation case, ~1–9% in other examples).",
            "minimal_fidelity_discussion": null,
            "failure_cases": "MCS is not a learning method here; used as ground truth. No failure cases reported for MCS itself, but authors highlight MCS computational expense which motivates MF-PIDNN.",
            "uuid": "e1319.2",
            "source_info": {
                "paper_title": "Transfer learning based multi-fidelity physics informed deep neural network",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "Large Eddy Simulation (LES)",
            "name_full": "Large-Eddy Simulation",
            "brief_description": "Mentioned in the introduction as an example of a numerical solver that might be considered 'low-fidelity' compared to experiments but is still computationally expensive — illustrating that 'low-fidelity' in multifidelity contexts does not always mean cheap.",
            "citation_title": "Large-eddy simulation: Past, present and the future",
            "mention_or_use": "mention",
            "simulator_name": "Large Eddy Simulation (LES)",
            "simulator_description": "A computational fluid dynamics approach that resolves large turbulent structures and models subgrid scales; cited as an example where a so-called 'low-fidelity' solver (relative to wind tunnel experiments) can still be very costly.",
            "scientific_domain": "fluid dynamics / turbulence modeling",
            "fidelity_level": "described as a computationally expensive 'low-fidelity' solver relative to experimental data in context — effectively medium/high fidelity numerics for turbulence with approximate subgrid modeling.",
            "fidelity_characteristics": "Resolves large-scale turbulent motions, models subgrid scales; computationally expensive; likely includes full Navier–Stokes physics with modeled subgrid stresses (paper provides no additional specifics).",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "Mentioned as context/example (no training tasks in this paper).",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Used to motivate that low-fidelity solvers can still be costly; no quantitative minimal-fidelity guidance is provided.",
            "failure_cases": "No experimental usage in this paper; mentioned only as a motivating example of costliness of some 'low-fidelity' solvers.",
            "uuid": "e1319.3",
            "source_info": {
                "paper_title": "Transfer learning based multi-fidelity physics informed deep neural network",
                "publication_date_yy_mm": "2020-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "rating": 2,
            "sanitized_title": "physicsinformed_neural_networks_a_deep_learning_framework_for_solving_forward_and_inverse_problems_involving_nonlinear_partial_differential_equations"
        },
        {
            "paper_title": "Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling",
            "rating": 2,
            "sanitized_title": "nonlinear_information_fusion_algorithms_for_dataefficient_multifidelity_modelling"
        },
        {
            "paper_title": "On transfer learning of neural networks using bi-fidelity data for uncertainty propagation",
            "rating": 2,
            "sanitized_title": "on_transfer_learning_of_neural_networks_using_bifidelity_data_for_uncertainty_propagation"
        },
        {
            "paper_title": "A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse pde problems",
            "rating": 2,
            "sanitized_title": "a_composite_neural_network_that_learns_from_multifidelity_data_application_to_function_approximation_and_inverse_pde_problems"
        },
        {
            "paper_title": "Multi-fidelity physics-constrained neural network and its application in materials modeling",
            "rating": 1,
            "sanitized_title": "multifidelity_physicsconstrained_neural_network_and_its_application_in_materials_modeling"
        }
    ],
    "cost": 0.016524,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TRANSFER LEARNING BASED MULTI-FIDELITY PHYSICS INFORMED DEEP NEURAL NETWORK
May 22, 2020</p>
<p>Souvik Chakraborty csouvik41@gmail.com 
School of Engineering
University of British Columbia
KelownaCanada</p>
<p>TRANSFER LEARNING BASED MULTI-FIDELITY PHYSICS INFORMED DEEP NEURAL NETWORK
May 22, 2020multi-fidelity · deep learning · physics-informed · transfer learning · reliability
For many systems in science and engineering, the governing differential equation is either not known or known in an approximate sense. Analyses and design of such systems are governed by data collected from the field and/or laboratory experiments. This challenging scenario is further worsened when data-collection is expensive and time-consuming. To address this issue, this paper presents a novel multi-fidelity physics informed deep neural network (MF-PIDNN). The framework proposed is particularly suitable when the physics of the problem is known in an approximate sense (lowfidelity physics) and only a few high-fidelity data are available. MF-PIDNN blends physics informed and data-driven deep learning techniques by using the concept of transfer learning. The approximate governing equation is first used to train a low-fidelity physics informed deep neural network. This is followed by transfer learning where the low-fidelity model is updated by using the available highfidelity data. MF-PIDNN is able to encode useful information on the physics of the problem from the approximate governing differential equation and hence, provides accurate prediction even in zones with no data. Additionally, no low-fidelity data is required for training this model. Applicability and utility of MF-PIDNN are illustrated in solving four benchmark reliability analysis problems. Case studies to illustrate interesting features of the proposed approach are also presented.Keywords multi-fidelity · deep learning · physics-informed · transfer learning · reliability One possible solution to the difficulties raised above resides in multi-fidelity schemes[7][8][9]where data fusion techniques are used to combine high-fidelity and low-fidelity data. The most popular multi-fidelity schemes are perhaps the multi-level Monte Carlo (MLMC) methods[10][11][12][13]. The primary idea in MLMC is to accelerate the calculation of the second moments of the quantity of interests. Another popular approach for dealing with multi-fidelity data is co-Kriging[14][15][16][17]. In this method, Kriging[18][19][20][21], aka Gaussian process[22][23][24][25][26], is coupled with an auto-regressive like information fusion scheme[27][28][29]. Methods where the Gaussian process in co-Kriging is replaced by other</p>
<p>Introduction</p>
<p>The governing equations used in science and engineering are often based on certain assumptions and approximations [1]. For example, heterogeneous material properties are approximated as homogeneous [2], effect of environmental conditions are rarely considered [3] and critical parts such as joints are often ignored [4]. Naturally, results obtained by solving the governing equations only provide an approximation of the true system behavior (i.e., low-fidelity results). An alternative is to perform actual experiments in a laboratory environment. With modern experimental setups and sensors, it is possible to perform highly sophisticated experiments [5,6]. Results obtained from such experiments are generally accurate (high-fidelity results). However, experiments are expensive and time-consuming, and one can only perform a limited number of experiments (usually in the order of tens). Such small number of experiments is often not sufficient for understanding the system behavior, specifically if dealing with problems such as uncertainty quantification and reliability analysis. machine learning techniques can also be found in the literature [30][31][32][33]. The success of all these methods is already well-established in the literature [34,35]. Unfortunately, these methods only work for cases where the low-fidelity data is able to capture the trend and the models of different fidelities have a strong linear correlation Both co-Kriging motivated approaches and MLMC fails when the low-fidelity and high-fidelity data have a space-dependent, complex and nonlinear correlations. To address this issue, researchers have recently proposed methods that are rooted in Bayesian statistics [36] and nonlinear auto-regressive algorithm [37].</p>
<p>The field of artificial intelligence and machine learning has recently witnessed a huge boom [38] and its influence can also be observed in the multi-fidelity approaches. De et al. [39] developed two multi-fidelity approaches by using deep neural networks. While the first framework uses transfer learning, the second framework utilizes bi-fidelity weighted learning. Meng and Karniadakis [40], on the other hand, proposed a composite neural network that is trained based on multi-fidelity data. A physics aware component was also added to this network; although, the physics informed component is only used for solving inverse problems. Liu and Wang [41] proposed physics constrained multi-fidelity neural networks for solving partial differential equations.</p>
<p>Based on the discussion above, (at least) two salient conclusions can be drawn about the existing multi-fidelity approaches.</p>
<p>• First, the existing multi-fidelity approaches assume the low-fidelity solver to be computationally efficient so that one can generate sufficient low-fidelity data. This is not always true. For example, compared to wind tunnel test data, a large eddy simulation [42,43] solver can be treated as a low-fidelity solver. However, computational cost associated with large eddy simulation is significant, even on modern computer clusters. • Second, the physics informed multi-fidelity approaches proposed in [41] assume that the exact physics corresponding to the high-fidelity data is known. This is not necessarily true. There are problems where the underlying physics is unknown [1]. Also, the apparently known governing equations are often derived based on certain assumptions and hence, only reflect the true scenario in an approximate manner.</p>
<p>The objective of this paper is to present a multi-fidelity physics informed deep learning framework that addresses both the limitations discussed above. Unlike some of the previous studies, it is assumed that the data-generation process for the high-fidelity data is unknown. The low-fidelity model is given by ordinary/partial differential equations. The proposed model needs no low-fidelity data; instead, the initial low fidelity model is directly trained based on the (approximate) physics of the problem. This is achieved by utilizing the recently developed physics informed deep learning algorithm. [44][45][46][47]. With this setup, important physical laws such as invariance and symmetries present in the low-fidelity model will be inherently captured by the deep learning framework. Transfer learning [45] and available high-fidelity data is then used to update the trained deep learning framework. Performance of the proposed framework is illustrated on selected reliability analysis problems from the literature.</p>
<p>The rest of the paper is organized as follows. Section 2 provides details on the problem to be solved. Details about the proposed approach are presented in Section 3. Numerical results showcasing the performance of the proposed approach are presented in Section 4. Finally, Section 5 provides the concluding remarks.</p>
<p>Problem statement
Consider Ξ = (Ξ 1 , Ξ 2 , . . . , Ξ N ) : Ω → R N to be an N −dimensional stochastic vector with cumulative distribution function F Ξ (ξ) = P (Ξ ≤ ξ) ,(1)
where ξ is a realization from the random vector Ξ, P (·) represents the probability measure and Ω is the input domain. In reliability analysis, one first formulates a limit-state or performance function, J (ξ) = 0 such that J (ξ) &lt; 0 represents the failure domain (Ω f ) and J (ξ) ≥ 0 represents the safe domain. Mathematically, this can be represented as
Ω f {Ξ : J (ξ &lt; 0)} .
(2) With this consideration, the probability of failure of the system can be calculated as
P f = P (Ξ ∈ Ω f ) = Ω f dF Ξ (ξ) = Ω I Ω f dF Ξ (ξ),(3)
where I c is an indicator function,
I c (ξ) = 1 if ξ ∈ c 0 if ξ / ∈ c(4)
Although the mathematical formulation of reliability analysis discussed above is relatively simple, the difficulty arises due to the multivariate integral in Eq. (3). Almost all the time, there exists no closed-form solution for the multivariate integral and hence, one has to rely on numerical integration techniques or asymptotic approximations. A detailed account of different reliability analysis methods can be found in [48,49].</p>
<p>Another important player in reliability analysis is the limit-state function J (ξ). For computing the probability of failure in Eq. (3) using numerical integration, one needs to evaluate the limit-state function repeatedly; the inherent assumption being, the mathematical model/equation for the limit-state function is known. In this regards, the accuracy of the limit-state function is of utmost importance. However, for many systems in science and engineering, the governing equation is either not available or available in an approximate sense [1]. Under such circumstances, one has no option but to rely on data collected either from the field or from laboratory experiments. Further assuming that the system under consideration is at its design phase, the option of collecting field data becomes invalid and performing laboratory experiments is the only feasible alternative.</p>
<p>Consider, D h = [Ξ hx , u h ] to be the data available from laboratory experiments where  (5) indicates Kronecker product and u h = [u 1 , . . . u r ] , r = N h × s × n represents the responses. 'h' in the suffix indicates that the data-collected is high-fidelity. The limit-state function J (ξ) is generally expressed in terms of the response variable u and a threshold u t
Ξ hx = Ξ h ⊗ x h ⊗ t h .(5)Ξ h = ξ (1) , . . . , ξ (N h ) T inJ (ξ) = g (u(ξ, x i , t j )) − u t .(6)
In case the number of data-points N h is significant, it is possible to directly train a surrogate model, M : (ξ, x, t) → u and then use it to evaluate the probability of failure in Eq. (3). Popular surrogate models available in the literature includes Gaussian process [22,23], polynomial chaos expansion [50,51], analysis of variance decomposition [52,53], support vector machine [54] and hybrid polynomial correlated function expansion [55,56]. However, in reality, the number of laboratory experiments that can be performed is limited and hence, the number of data-points available is often not sufficient for training a surrogate model. To compensate for the fact that only a limited number of highfidelity data is available, the approximate (low-fidelity) governing equation of the system is considered,
u t + h (u, u x , u xx , . . . ; ξ) = 0.(7)
u x and u xx in Eq. (7) represent the first and second derivative of u with respect to x. As already discussed in Section 1, solving Eq. (7) to generate sufficient number of low-fidelity data can also be computationally expensive.</p>
<p>The objective of this paper is to develop a multi-fidelity deep learning framework that can be directly trained by using the low-fidelity model in Eq. (7) (without generating data from it) and the high-fidelity data, D h .</p>
<p>Multi-fidelity physics informed deep neural network</p>
<p>In this section, the proposed multi-fidelity physics informed deep neural network (MF-PIDNN) is presented. However, before proceeding to the proposed framework, details on data-driven and physics-informed deep neural networks are discussed. Data-driven and physics informed deep neural networks form the backbone of the proposed multi-fidelity approach.</p>
<p>Data-driven deep neural networks</p>
<p>One of the primary components of the proposed multi-fidelity approach is a deep neural network (DNN). In this work, a fully connected DNN (FC-DNN) is used and hence, the discussion is limited to FC-DNN only. Having said that, the framework presented is generic and can be used with convolutional [57] and other types of neural networks as well.</p>
<p>An FC-DNN with L−hidden layers can be represented by using a sequence of activation functions and linear transformations
N (·; θ) = (σ L • W L+1 ) • · · · • (σ 0 • W 1 ) ,(8)
where σ j : R → R and W j+1 respectively represents the activation function and the weight matrix associated with the edges connecting the j−th and (j + 1)−th layers. The biases of the neural network are absorbed into the weight matrix W j ; the weight matrices {W j } L+1 j=1 are the parameters of the FC-DNN and are represented using θ. '•' in Eq. (8) represents operator composition. Note that the 0−th layer in Eq. (8) represents the input and (L + 1)−th layer represents the output. For using the neural network in practice, the model parameters θ needs to be estimated. In a data-driven setting, this is achieved by minimizing a loss function. For a detailed account of different loss-functions available in the literature, interested readers may refer [38,58]. In this work, the mean-square loss function (L d ) has been used,
L d = 1 N d N d k=1 (u k −û k ) 2 .(9)
In Eq. (9), N d represents the number of data-points, u k is the observed response corresponding to the k−th input, ξ k andû k represents the neural network predicted response corresponding to ξ k ,
u k = N(ξ k ; θ).(10)
The primary challenge behind the application of the DNN for engineering applications is the need for data. It is a well-acknowledged fact that DNNs are data-hungry tools [36]. Unfortunately, for the current work, the focus is on problems where one has access to very few high-fidelity data. Therefore, the direct application of data-driven DNN is unlikely to yield satisfactory results.</p>
<p>Physics-informed deep neural networks</p>
<p>To address the over-reliance of data-driven DNNs on training data, physics informed deep neural networks (PI-DNN) was proposed in [44]. The basic idea is to compute the DNN parameters directly from the physics (governing ODE/PDE) of the problem. Since its inception, the PI-DNN has been used for solving a wide range of problems in science and engineering [40,[45][46][47].</p>
<p>Consider the governing (stochastic) differential equation in Eq. (7). The objective is to solve the stochastic differential equation so as to build a mapping from the input space (stochastic, spatial and temporal inputs) to the response space.</p>
<p>In conventional data-driven DNN, this is achieved in three simple steps
• Generate training data D = {Ξ c,i , u i } Nr i=1 , where Ξ x = ξ 1:Ns ⊗ x 1:Nx ⊗ t 1:Nt ,(11)
and
N r = N s × N x × N t .(12)
• Represent the output y using DNN,
u = N (ξ, x, t; θ) .(13)
• Compute the DNN parameters θ by minimizing the loss-function in Eq. (9),
θ * = arg min θ L d (θ) .(14)
In PI-DNN, the objective is to remove the data-generation step and compute the DNN parameters θ directly from the governing differential equation in Eq. (7). Following the method presented in [46], this is achieved in four simple steps. First, similar to the data-driven case, the response u is represented by using a DNN,
u ≈ u N N = N (ξ, x, t; θ) .(15)
Second, the neural network outputs are modified so as to automatically satisfy the initial and Dirichlet boundary conditions.û (ξ,
x, t) = u b (x b , t i ) + B · u N N (x, t, ξ),(16)
where the function B is defined in such a way that B = 0 at the boundary (x b ) and initial (t i ) points. The function
u b (x b , t i )
is defined based on the initial and boundary conditions. More details on this can be found in [45,46]. Note thatû(ξ, x, t) can also be viewed as a DNN,N(ξ, x, t; θ).</p>
<p>In the third step, collocation points for the inputs, D c = {ξ k , x k , t k } Nc k=1 are generated by using some suitable design of experiment scheme [52,59]. Using the collocation points, the physics-informed loss function is formulated as
L p (θ) = 1 N c Nc i=1 R 2 i ,(17)
where N c is the number of collocation points and R i is the residual of the governing differential equation corresponding to the i−th collocation point,
R i = (û t ) i + h ((û) i , (û x ) i , (û xx ) i , . . . ; ξ i ) . (18) (û) i in Eq. (18) is obtained by substituting the i−th collocation point into Eq. (16). (û t ) i , (û x ) i , (û xx ) i are obtained by using automatic differentiation (AD) [60],û t = ∂û ∂t =N t (ξ, x, t; θ), u x = ∂û ∂x =N x (ξ, x, t; θ), u xx = ∂ 2û ∂x 2 =N xx (ξ, x, t; θ).(19)
Note that the derivatives in Eq. (19) are also DNN. Since the DNNs in Eq. (19) are obtained by differentiating Eq. (16), they have the same architecture and same parameters; the only difference is in the form of the activation function.</p>
<p>In the fourth and final step, the loss function in Eq. (17) is minimized to compute the parameters of the DNN,
θ * = arg min θ L p (θ).(20)
For further details on PI-DNN and its application in solving reliability analysis problems, interested readers may refer [46].</p>
<p>PI-DNN has two major advantages. First, unlike other reliability analysis tools including data-driven DNN, PI-DNN needs no simulation data. This is expected to reduce the computational cost significantly. Second, PI-DNN is trained by satisfying the governing differential equation of the system. Therefore, physical properties such as invariance and symmetries are satisfied. However, despite these advantages, the whole idea of PI-DNN is hinged on the fact that the exact governing differential equation for the system under consideration is available. Unfortunately, this is not necessarily true. There exists a number of scenarios in science and engineering where the governing differential equation is not known [1]. Even if the governing equation is known, it is often based on certain assumptions and approximations. In other words, the governing differential equation only represents the reality in an approximate manner. Under such circumstances, results obtained using PI-DNN are bound to be erroneous.</p>
<p>Proposed approach</p>
<p>Neither the data-driven DNN in Section 3.1 nor the PI-DNN presented in Section 3.2 is capable of solving the reliability analysis problem defined in Section 2. The data-driven DNN fails because the number of high-fidelity data available, N h is very less. On the other hand, the PI-DNN fails as the governing differential equation in Eq. (7) only represents the actual scenario in an approximate manner. To solve the problem defined in Section 3, a multi-fidelity physics informed deep neural network (MF-PIDNN) is presented in this section. MF-PIDNN utilizes the concepts of both data-driven and physics informed DNNs. Unlike available multi-fidelity frameworks, the proposed MF-PIDNN does not assume that generating low-fidelity data is trivial. In fact, no low-fidelity data is needed for the MF-PIDNN presented here.</p>
<p>The key consideration of any multi-fidelity framework is associated with discovering and exploiting the relation between the low-fidelity and high-fidelity model/data. In most of the frameworks available in the literature, this is achieved by using two surrogates; the first surrogate is trained based on the low-fidelity data and the second surrogate is used to find the functional relation between the low-fidelity and the high-fidelity data. This paper takes a separate route; instead of using two DNNs, a single DNN is first trained for the low-fidelity model and then updated based on the high-fidelity data. For updating the DNN, the concept of transfer learning is used in this study. Note that the idea of using transfer learning in a multi-fidelity framework has previously been exploited in [39]. However, unlike the proposed framework, the algorithm presented in [39] is purely data-driven in nature.</p>
<p>MF-PIDNN solves the problem defined in Section 2 in two simple steps. In the first step, PI-DNN is used to solve the low-fidelity model. To that end, the exact procedure as discussed in Section 3.2 is followed. In the second step, the low-fidelity PI-DNN is updated based on the high-fidelity data D hx . This is achieved by utilizing the concept of datadriven DNN. However, unlike the first step, the second step is not straight-forward. More specifically, two specific factors are considered in this step. First, the training algorithm starts by setting the initial value of the neural network parameters to those obtained in step 1. Second, the parameters corresponding to all the layers are not updated. Instead, the concept of transfer learning [45] is used and the parameters corresponding to only the last one or two layers are updated. A schematic representation of MF-PIDNN is shown in Fig. 1. The advantage of transfer learning is three-folds. During the low-fidelity training phase in (a), the DNN has three building blocks. The physics induced DNN architecture is governed by the low-fidelity governing differential equation of the system. At this stage, the DNN parameters are tuned by using the collocation points D c and minimizing the residual, (R i ) (physics-informed loss). θ l (yellow box) indicates that the DNN parameters obtained at the end of the training phase. During the high-fidelity training phase in (b), the DNN parameters for all but the last one or two layers are fixed at θ l (yellow box)). The tunable parameters θ t (green box) are estimated by minimizing the mean-squared error computed using the high-fidelity data D h .</p>
<p>• First, because of transfer learning, the number of parameters to be updated is reduced. This in turn, accelerates training of the DNN. • Second, freezing the parameters of the initial layer ensures that the features learned/extracted from the lowfidelity model are retained in the network. • Thirdly, transfer learning also ensures that the DNN does not overfit the high-fidelity data, D h .</p>
<p>The steps involved in the proposed MF-PIDNN are shown in algorithm 1. For training the MF-PIDNN, RMSProp optimizer [61] followed by L-BFGS algorithm is used. Xavier initialization is used for initializing the DNN parameters. Details on the parameters settings for the optimizers are provided in Section 4. Once the MF-PIDNN is trained, it is possible to predict u corresponding to some unknown inputs by using Eq. (16).</p>
<p>Numerical illustration</p>
<p>In this section, four numerical examples are presented to illustrate the performance of the proposed approach. A wide variety of examples involving single and multiple stochastic variables, linear and non-linear problems, ordinary and partial differential equations are selected. For illustrating the performance of the proposed approach, benchmark Algorithm 1: Transfer learning based multi-fidelity physics informed deep neural network 1 Initialize: Provide high-fidelity data D h and the low-fidelity model. Also specify the architecture of the DNN and the number of tunable layers, l t during transfer learning. 2 Express the unknown response using a DNN ;</p>
<p>Eq. (15) 3 Modify the DNN to automatically satisfy the initial and boundary conditions ;</p>
<p>Eq. (16) 4 Utilize the low-fidelity physics to formulate a physics-informed loss function ;</p>
<p>Eq. (17) 5 Minimize the physics-informed loss function to compute the network parameters, θ ;</p>
<p>Eq. (20) 6 Freeze the DNN parameters for initial (L − l t + 1) layers. 7 Formulate data-driven loss function using D h ;</p>
<p>Eq. (9) 8 Minimize the loss-function to tune the tunable parameters
θ t = arg min θt L d (θ t ),
where θ t represents the tunable parameters. results using Monte Carlo simulation (MCS) [62] are generated. The software accompanying the proposed approach is developed using TenserFlow [63]. For examples 1, 2 and 4, the benchmark results are generated using MATLAB [64]. Benchmark results for example 2 are generated using the FeNICS package [65].</p>
<p>An ordinary differential equation</p>
<p>As the first example, a benchmark stochastic ordinary differential equation previously studied in [66] is considered. The low-fidelity model for this problem is given by the following stochastic ordinary differential equation,
du l dt = −Zu l ,(21)
where Z is the stochastic variable. The differential equation in Eq. (21) is subjected to the following initial condition, u l (t = 0) = 1.0.</p>
<p>The high-fidelity model, on the other hand, is represented as
u h = t sin (t) log u 4 l 2 + 15t 3 + 1.0(23)
Clearly, the relation between the high-fidelity u h and the low-fidelity u l is non-linear. The limit-state function for this problem is defined as
J (Z, t t ) = u h (Z, t t ) − u 0 ,(24)
where u 0 is the threshold, u h (Z, t t ) is the response and t t is the time at which the probability of failure is to be estimated. </p>
<p>The residual for training the low-fidelity DNN is
R i = dû i dt + Z iûi ,(26)
where R i is the residual andû i is obtained from Eq. (25). 'i in the suffix indicates the i−th collocation point. For training the low-fidelity model, 8000 collocation points is used and the RMSprop optimizer is run for 15, 000 iterations.</p>
<p>A learning rate of 0.001 is used. The other parameters of RMSprop are kept at there default values. The maximum allowable iterations for L-BFGS optimizer is set to be 10, 000.</p>
<p>After training the physics-informed low-fidelity DNN, the next step is to update the model based on the high-fidelity data by using the transfer learning. The parameters corresponding to the last two layers are only updated; parameters corresponding to all other layers are kept fixed. The RMSprop optimizer is run for 10, 000 iterations and maximum allowable iterations for the L-BFGS optimizer is set to be 10, 000. For RMSProp optimizer, a learning rate of 0.001 is used. Table 1 shows the results obtained using MCS and MF-PIDNN. Along with the probability of failure P f , the reliability index β for this problem is also reported.
β = Φ −1 (1 − P f ) ,(27)
where Φ (·) represents cumulative distribution function of standard Gaussian distribution. The results obtained using MF-PIDNN matches exactly with the MCS results. To show the utility of the proposed approach, results obtained using only the low-fidelity PI-DNN and the high-fidelity DNN are also presented. Both low-fidelity PI-DNN and high-fidelity DNN are found to yield erroneous results. To further illustrate the performance of the MF-PIDNN, two additional case studies are performed. In the first case study, the performance of the MF-PIDNN in predicting future reliability is investigated. To that end, it is assumed that for each of the 15 high-fidelity samples, observations are available t = [0.0, 0.5, 0.9], and the objective is to compute the reliability of the system at t = 1.0. The difficulty, in this case, arises from the fact that this is an extrapolation problem as no observation is available at or beyond t = 1.0. The network architecture and other parameters of MF-PIDNN are considered to be same as before; the only difference resides in the fact that the RMSProp optimizer is run for 15, 000 iterations (while updating the network using transfer learning). The results obtained are shown in Table 2.</p>
<p>Compared to the results presented in Table 1, slight deterioration in the results have been observed; this is expected because this is an extrapolation problem. Nonetheless, the results obtained are still significantly more accurate as compared to HF-DNN and LF-PIDNN.   Fig. 2). This is because, with only observations at two time-instants, the DNN fails to predict the trend of the limit-state function. MF-PIDNN, on the other hand, learns the trend from the physics of the problem and then update itself based on the high-fidelity data.</p>
<p>Burger's equation</p>
<p>As the second problem, the well-known Burger's equation is considered. The high-fidelity model for this problem is  (29) is a small perturbation that is applied to the left boundary. The problem as defined has a transition layer at z, so that u h (z) = 0. As illustrated in previous studies [67,68], the transition layer is super sensitive to δ. Details on different aspects of this problem can be found in [67,68].
(u h ) t + u h (u h ) x = ν(u h ) x x,(28)u h (t, x = −1) = 1 + δ u h (t, x = 1) = −1, (29a) u h (t = 0, x) = −1 + (1 + x) 1 + δ 2 . (29b) δ in Eq.
The low-fidelity model, on the other hand, is considered to be 
(u l ) t = ν(u l ) xx .(30)
The limit-state function for this problem is represented as
J (δ, t t ) = −z(δ, t) + z 0 ,(32)
where z represents the transition layer, t t is the time at which the reliability is to be computed and z 0 is the threshold. For this example, z 0 = 0.40 is considered. The objective is to compute the probability of failure at t t = 10. Note that solution of this problem involves extrapolation as no observation at t = 10 or beyond is available.</p>
<p>For solving the problem using the proposed MF-PIDNN, u is first represented by using a FC-DNN with 6 hidden layers. Each of the 6 hidden layers has 50 neurons. The DNN has 3 inputs, x, t and δ and one output u N N . tanh activation function is considered for all but the last layer. For the last layer, linear activation function is used. To automatically satisfy the boundary and initial conditions, the DNN output is modified aŝ
u = u h (t = 0, x) + t(1 − x)(1 + x)u N N ,(33)
where u h (t = 0, x) is obtained from Eq. (29b). Usingû and its derivatives, the residual of the low-fidelity model is formulated as
R i = (û t ) i − ν(û) xx ) i ,(34)
where R i is the residual. i in the suffix indicates that the quantities are evaluated corresponding to the i−th collocation point. The physics-informed loss-function for training the low-fidelity model is formulated by using 30, 000 collocation points and Eq. (34). Because of the simplicity of the low-fidelity model, the RMSProp optimizer is run for 500 iterations and the maximum allowable iterations for the L-BFGS optimizer is set to be 1000. The learning rate in RMSProp optimizer is set to be 0.001. Once the low-fidelity physics informed DNN is trained, the next step is to update the DNN model by using transfer learning. To retain information gained from the low-fidelity model and avoid over-fitting, parameters corresponding to only the last two layers of the DNN are allowed to update; all the other parameters are frozen. The RMSprop optimizer is run for 6000 iterations with a learning rate of 0.003. The maximum allowed iterations for the L-BFGS algorithm is set to 10,000. The L-BFGS optimizer is only allowed to update the DNN parameters corresponding to the last layer. For this problem, the MF-PIDNN is found to be highly sensitive to the initial point of the parameters and varies from run to run. Therefore, the MF-PIDNN results presented are mean predictions after running the model for 20 times.</p>
<p>For the purpose of validation, benchmark results using MCS with 10 4 simulations are generated. To that end, finite element package FeNICS [65] is used. The same-solver is used for generating the high-fidelity data as well.</p>
<p>The reliability analysis results are shown in Table 3. Along with MCS and MF-PIDNN results, LF-PIDNN and HF-DNN predicted results are also presented. Similar to the previous example, both probability of failure and reliability index are reported. It is observed that MF-PIDNN predicted results are extremely close to the MCS results. HF-DNN and LF-PIDNN, on the other hand, yields erroneous results. Fig. 3 shows the performance of MF-PIDNN with increase in N t (i.e, number of time-steps at which high-fidelity data is available). It is observed that with an increase in N t , the MF-PIDNN predicted result moves closer to the MCS results. However, at N t = 6 and 8, the probability of failure obtained is found to be similar, indicating convergence of the proposed approach.<br />
Methods P f β N h N r = |β−βe| βe × 100 MCS 0.2036 0.8288 10 4 10 4 × 33 × 10 3 - LF-PIDNN 0 ∞ 0 0 ∞ HF-DNN 0.932 -1.4909 5 120(5 × 3 × 8) 280% MF-PIDNN 0.2242 0.7581 5 120(5 × 3 × 8) 8.5304%</p>
<p>Nonlinear oscillator</p>
<p>As the third example, a nonlinear oscillator, previously studied in [69] has been considered. The high-fidelity model for this problem is given as
d(x h ) 1 dt = (x h ) 2 , d(x h ) 2 dt = −α 1 (x h ) 2 − α 2 sin ((x h ) 1 ) ,(35)
where α 1 and α 2 are the stochastic parameter. The initial conditions for the problem are
(x h ) 1 (t = 0) = −1.193, (x h ) 2 (t = 0) = −3.876.(36)
The low-fidelity model, on the other hand, is given as
d(x l ) 1 dt = (x l ) 2 , d(x l ) 2 dt = −α 1 (x l ) 2 − α 2 (x h ) 1 .(37)
The initial condition for the low-fidelity model is considered to be same as the high-fidelity model. Similar to the previous examples, the high-fidelity equation is assumed to be unknown and one only has access to the low-fidelity model and high-fidelity data. More specifically, data corresponding to five realizations of the stochastic parameters are available. For each of the realizations, the observations are available at five equally spaced time-instants in [0, 5].</p>
<p>The realizations of the stochastic parameters are obtained using Latin hypercube sampling [70]. Following [69], the stochastic parameters α 1 ∼ U (0, 0.4) and α 2 ∼ U (8. 8, 9.2) are considered to be uniformly distributed. The limitstate function for this problem is defined as
J (α 1 , α 2 , t t ) = − |x 2 (α 1 , α 2 , t t )| + x 0 ,(38)
where x 0 is the threshold and t t is the time at which the reliability is to be evaluated. For this problem, t t = 5.0 and x 0 = 4.0 is considered.</p>
<p>To solve the problem using MF-PIDNN, x i , i = 1, 2 is first represented using a FC-DNN having 4 hidden layers. Each hidden layer has 50 neurons. The DNN has three inputs, t, α 1 and α 2 and two outputs x 1 and x 2 . tanh activation function is considered for all but the last layer. Linear activation function is used for the last layer. To automatically satisfy the initial conditions, the DNN output is modified aŝ
x 1 = t · x N N,1 − 1.193, x 2 = t · x N N,2 − 3.876,(39)
where x N N,1 and x N N,2 are the DNN outputs. Usingx 1 andx 2 , the residuals are computed,
R 1,i = ((x 1 ) t ) i − (x 2 ) i , R 2,i = ((x 2 ) t ) i + (α 1 ) i (x 2 ) i + (α 2 ) i (x 1 ) i .(40)
i in Eq. (40) indicates the i−th collocation point. Using the residuals, the physics-informed loss function for training the low-fidelity model is formulated as
L p (θ l ) = 1 N c Nc i=1 R 2 1,i + R 2 2,i ,(41)
where N c is the number of collocation points. For this problem, 10000 collocation points have been used. The RMSProp optimizer is run for 15000 iterations with a learning-rate of 0.001. For L-BFGS optimizer, the maximum allowable iterations is set to 10000. The trained low-fidelity model is then updated by using transfer learning and high-fidelity data. Only the parameters corresponding to the last two layers of DNN are allowed to be updated. A learning rate of 0.001 is used and the RMSProp optimizer is run for 10000 iterations. Maximum allowable iterations for the L-BFGS optimizer is set to be 10000.</p>
<p>The benchmark results for validation are generated by using MCS with 10 4 simulations. To that end, the differential equations are solved using the ODE45 routine in MATLAB [64]. The high-fidelity data-set discussed earlier was also generated by using the same solver. Lastly, to illustrate the robustness of the proposed MF-PIDNN, the same model is used to compute the probability of failure at a t t = 3.0 and x 0 = 2.0. The results obtained are shown in Table 5. In this case also, MF-PIDNN is found to yield accurate results outperforming both HF-DNN and LF-PIDNN.   </p>
<p>Cell signaling cascade</p>
<p>As the last example, a mathematical model of autocrine cell-signaling cascade is considered.
de 1p dt = I 1 + G 4 e 3p V max,1 1 − e 1p K m,1 + (1 − e 1p ) − V max,2 e 1p K m,2 + e 1p , de 2p dt = V max,3 e 1p 1 − e 2p K m,3 + (1 − e 2p ) − V max,4 e 2p K m,4 + e 2p , de 3p dt = V max,5 e 2p 1 − e 3p K m,5 + (1 − e 3p ) − V max,6 e 3p K m,6 + e 3p , t ∈ [0, 10],(42)
where e 1p , e 2p and e 3p are the state variables and denotes concentrations of the active form of enzymes. I in Eq. (42) is the tuning parameter. The initial conditions for this problem are e 1p (t = 0) = 0, e 2p (t = 0) = 1.0, e 3p (t = 0) = 0.</p>
<p>(43) This model was first developed in [71]. Overall the model has 13 parameters, K m,1:6 , V max,1:6 and G 4 . For biological meaning and other details on the model parameters, interested readers may refer [71].</p>
<p>For reliability analysis, all the 13 parameters defined above are considered to be stochastic. The mean of the parameters are adopted from [71] and a 10% relative noise is added. For clarity of readers, the mean of the 13 parameters is presented in Table 6. The same parameter settings have previously been used in [69].</p>
<p>A low-fidelity model for this problem is set up by considering I = 0. With this, the coupled differential equations in Eq. (42) is decoupled and it becomes possible to solve the equations sequentially. Moreover, the stochastic variables The limit-state function for this problem is
J (ξ) = e 3p (ξ, t t ) − e 3,0 ,(44)
where ξ ∈ R 13 represents the stochastic variables, e 3,0 is the threshold parameter and t t is the time-instants at which the reliability is to be estimated. For this problem, t t = 3.0 is considered. Since all the high-fidelity observation are available in [4,7], this is an extrapolation problem. </p>
<p>where e 1p,N N , e 2p,N N and e 3p,N N are the DNN outputs. The residuals for formulating the physics-informed loss function are given as
R 1,i = (K m,2 ) i + (ê 1p ) i ((ê 1p ) t ) i + (V max,2 ) i (ê 1p ) i , R 2,i = (K m,4 ) i + (ê 2p ) i ((K m,3 ) i + (1 − (ê 2p ) i )) − (V max,3 ) i (ê 1p ) i (1 − (ê 2p ) i ), R 3,i = (K m,6 ) i + (ê 3p ) i ((K m,5 ) i + (1 − (ê 3p ) i )) − (V max,5 ) i (ê 2p ) i (1 − (ê 3p ) i ),(46)
where 'i' in suffix represents the i−th collocation point. The residuals in Eq. (46) corresponds to the low-fidelity model and hence, I, G 4 and V max,1 are not present. Using the residuals, the physics-informed loss function for the low-fidelity model is computed as
L p (θ l ) = 1 N c Nc i=1 3 k=1 R 2 k,i ,(47)
where N c represents the number of collocation points. For minimizing L p (θ), the RMSProp optimizer is run for 5000 iterations. A learning rate of 0.001 is used. As for the L-BFGS optimizer, the maximum allowed iterations is set to 10000. The trained low-fidelity model is then updated by using the high-fidelity data and transfer learning. At this stage, only the parameters corresponding to the last layer is allowed to be tuned. All the other parameters are fixed at θ l . A learning rate of 0.001 is used and the RMSProp optimizer is run for 5000 iterations. As for the L-BFGS optimizer, the maximum allowed iterations is set to be 10000.</p>
<p>The benchmark results for this problem are generated by using MCS with 10 4 simulations. To that end, the ODE45 routine available in MATLAB is used. The high-fidelity data discussed before were also generated by using the same procedure. Table 7 shows the reliability analysis results for the cell signaling cascade problem. Along with MCS and MF-PIDNN, results obtained using LF-PIDNN and HF-DNN are also presented. The proposed MF-PIDNN is found to yield highly accurate results with a prediction error of 1.52%. Results obtained using LF-PIDNN and HF-DNN respectively have an error of 64.35% and 98.04%. The variation of probability of failure with the change in threshold e 3,0 is shown in Fig. 5. For all the thresholds, MF-PIDNN predicted results are found to closely match with the MCS results. This indicates that MF-PIDNN is able to capture the response over the whole domain.</p>
<p>Finally, the trained MF-PIDNN is used to compute the probability of failure at different time instants. The corresponding results are illustrated in Fig. 6. To be specific, probability of failures around t = 3, 5, 7 and 9 are presented. The threshold e 3,0 for the four cases are set to be 0.40, 0.575, 0.70 and 0.78. MF-PIDNN for all the four cases is found to yield reasonably accurate results. Do note that high-fidelity data was only available at five equidistant time-instants  between t = 4.0 and t = 7.0. The fact that the proposed approach yields reasonable results outside this domain illustrates the extrapolability of the proposed approach. This capability of the MF-PIDNN is because of the fact that some physics is learnt (and retained) from the low-fidelity data.</p>
<p>Conclusions</p>
<p>In this paper, a multi-fidelity physics informed deep neural network (MF-PIDNN) is presented. The proposed approach is ideally suited for problems where the physics of the problem is known in an approximate sense (low-fidelity physics) and only a few high-fidelity data is available. MF-PIDNN blends the concepts of physics-informed and data-driven deep learning; the primary idea is to first train a low-fidelity deep learning model based on the available approximate physics and then use transfer learning to update the model based on the high-fidelity data. With this, MF-PIDNN is able to extract useful information from both the low-fidelity physics and high-fidelity data. There are two distinct advantages of MF-PIDNN. First, the low-fidelity model is directly trained from the physics of the problem and hence, no low-fidelity data is needed in this framework. Second, because of the physics-informed framework within MF-PIDNN, the proposed approach is able to capture some of the physical laws that are present in the approximate model. As a result, it provides reasonable predictions even in zones with no-data.</p>
<p>The proposed approach is used for solving benchmark reliability analysis problems from the literature. For all the problems, the proposed approach is able to correctly predict the probability of failure and the reliability index of the system. To illustrate the advantage of the proposed approach, results obtained are compared with those obtained from only the high-fidelity data-driven model and low-fidelity physics-driven model. The proposed approach is found to outperform both these approaches. Case studies are also presented to illustrate different features of MF-PIDNN.</p>
<p>Despite the several advantages of the MF-PIDNN, certain aspects can be further enhanced. For example, during updating the model using transfer learning, mean-squared loss-function with no regularization has been used. This can lead to over-fitting. One future direction is to study the effect of regularization on the results. Second, the number of tunable parameters during transfer learning are selected manually in this study. Automating the transfer learning step will be hugely beneficial. Third, the network architecture and the activation functions in this study are manually provided. Automating this will also be beneficial. In future, some of these aspects will be investigated. </p>
<p>Fig. 1 :
1Schematic representation of the proposed MF-PIDNN.</p>
<p>For this example, t t = 1.0 and u 0 = 18.0 is considered. It is assumed that 15 samples from the highfidelity model is available, and for each of the 15 high-fidelity samples, the observations are available at t = [0.0, 1.0]. Note that the data-generation process, i.e., Eq. (23) is not known. MF-PIDNN only have access to the high-fidelity data and the low-fidelity model in Eq.(21). For this particular problem, the stochastic variable Z ∼ N (µ, σ 2 ) is considered to follow Gaussian distribution with mean µ = −2.0 and standard deviation σ = 1.0. MCS with 10 6 simulations yields a probability of failure of 0.045.For solving the problem using the proposed approach, the unknown response u is first represented by using an FC-DNN with 2 inputs, 5 hidden layers and 50 neurons per hidden layer. The 2 inputs to the DNN are time t and decay parameter Z. Hyperbolic tangent (tanh) activation function is considered for all but the last layer. For the last layer, linear activation function is considered. The initial conditions in Eq.(22) is automatically satisfied by modifying the DNN output, u N N using Eq. (16), where u b = 1.0 and B = t, u = t · u N N + 1.0.</p>
<p>Finally
, the performance of the MF-PIDNN with variation in the number of high-fidelity data point, N h is investigated. For each realization of Z, the responses are observed at t = [0.0, 1.0] and the probability of failure at t t = 1.0 is computed. The variation of the MF-PIDNN predicted probability of failure is shown in Fig. 2. The benchmark result obtained using MCS is also reported. With an increase in N h , the MF-PIDNN predicted probability of failure converges to the MCS solution. The HF-DNN results, up to N h = 20, yields erroneous results (not shown in</p>
<p>Fig. 2 :
2Variation in the MF-PIDNN predicted results with increase in number of high-fidelity data points. with x ∈ [−1, 1] and t ∈ [0, 12]. ν &gt; 0 in Eq. (28) represents the viscosity. The boundary and the initial conditions for this problem are</p>
<p>Eq.(30) is obtained by ignoring the nonlinear term in the high-fidelity model. The initial and the boundary conditions are considered to be same as the high-fidelity model. The boundary perturbation δ ∼ U (0.0, 0.1) is uniformly distributed between 0.0 and 0.1. It is considered that the high-fidelity model in Eq. (23) is not known; instead, data corresponding to five realizations of δ is available. For each δ, observations at 3 spatial location and 8 temporal locations are available.Ξ hx = [0, 0.025, 0.05, 0.075, 0.1] ⊗ [−1, 0, 1] ⊗ [1, 2.14,3.29, 4.43, 5.57, 6.71, 7.86, 9]  .</p>
<p>Fig. 3 :
3Variation of MF-PIDNN predicted results with N t .</p>
<p>Fig. 4 :
4Variation of probability of failure with threshold x 0 .</p>
<p>For
reliability analysis using MF-PIDNN, the output responses are first represented by using a FC-DNN. The DNN has 14 inputs (13 stochastic variables and time), 3 outputs and 4 hidden layers. Each of the hidden layers has 100 neurons. All but the last layer of the DNN have tanh activation function. For the last layer, linear activation function is used. To automatically satisfy the initial conditions, the DNN outputs are modified aŝ e 1p = t · e 1p,N N e 2p = t · e 2p,N N + 1.0, e 3p = t · e 3p,N N ,</p>
<p>Fig. 5 :
5Variation of the probability of failure with threshold e 3,0 .</p>
<p>Fig. 6 :
6MF-PIDNN and MCS predicted results at different time-instants. The threshold e 3,0 for these four cases are set at (a) 0.40, (b) 0.575, (c) 0.70 and (d) 0.78.</p>
<p>Eq. (5) represents sample/data of the stochastic inputs, x h = [x 1 , . . . , x s ] T are the spatial locations where data is available (sensor locations) and t h = [t 1 , . . . , t n ] T are the time-steps are the times at which observations are available. The operator '⊗' in Eq.</p>
<p>Table 1 :
1Reliability analysis results for example 1.Methods 
P f 
β 
N h 
N r 
= |βe−β| </p>
<p>βe </p>
<p>× 100 
MCS 
0.045 
1.6954 
10 6 10 6 × 1001 
-
LF-PIDNN 
0.8133 -0.8901 
0 
0 
152.5% 
HF-DNN 
0.0 
∞ 
15 
30(15 × 2) 
∞ 
MF-PIDNN 
0.045 
1.6954 
15 
30(15 × 2) 
0.0% </p>
<p>Table 2 :
2Reliability analysis results for example 1. The results presented illustrate the extrapolation capability of the MF-PIDNN.Methods 
P f 
β 
N h 
N r 
= |βe−β| </p>
<p>βe </p>
<p>× 100 
MCS 
0.045 
1.6954 
10 6 10 6 × 1001 
-
LF-PIDNN 
0.8133 -0.8901 
0 
0 
152.5% 
HF-DNN 
0.014 
2.9173 
15 
45(15 × 3) 
29.60% 
MF-PIDNN 
0.05 
1.6449 
15 
45(15 × 3) 
2.98% </p>
<p>Table 3 :
3Reliability analysis results for the Burger's equation</p>
<p>Table 4
4shows the reliability analysis results for the nonlinear oscillator problem. Similar to the previous examples, 
results obtained using HF-DNN and LF-PIDNN are also presented. The MF-PIDNN is found to yield highly accurate 
results, matching closely with the MCS solutions. LF-PIDNN and HF-DNN yield erroneous results. The variation 
of probability of failure with threshold x 0 is shown in Fig. 4. Corresponding to all the thresholds, the MF-PIDNN 
predicted results matches closely with the MCS results. This indicates that the proposed MF-PIDNN is able to capture 
the response over the whole domain. </p>
<p>Table 4 :
4Reliability analysis results for nonlinear oscillator.Methods 
P f 
β 
N h 
N r 
= |β−βe| </p>
<p>βe </p>
<p>× 100 
MCS 
0.1599 0.9949 10000 10 4 × 10 3 
-
LF-PIDNN 
0.27 
0.6128 
0 
0 
38.41% 
HF-DNN 
0.19 
0.8779 
5 
5 × 5 
11.76% 
MF-PIDNN 0.1576 1.0044 
5 
5 × 5 
0.95% </p>
<p>2.5 
3 
3.5 
4 
4.5 </p>
<p>Table 5 :
5Reliability analysis results for nonlinear oscillator at t t = 3.0 and x 0 = 2.0Methods 
P f 
β 
N h 
N r 
= |β−βe| </p>
<p>βe </p>
<p>× 100 
MCS 
0.0651 
1.5133 
10000 10 4 × 10 3 
-
LF-PIDNN 
0.98 
−2.0537 
0 
0 
235.7% 
HF-DNN 
0.5955 −0.2417 
5 
5 × 5 
115.97% 
MF-PIDNN 0.0729 
1.4545 
5 
5 × 5 
3.88% </p>
<p>Table 6 :
6Mean of the parameters for the cell signaling cascade problemParameters K m,1:6 V max,1 V max,2 V max,3 V max,4 V max,5 V max,6 G 4 G 4 , V max,1 and K m,1 become inactive. This further complicates the problem. It is further assumed that the governing differential equation in Eq. (42) is not available; instead, responses corresponding to 10 realizations of the stochastic variables are available. For each of the 10 realizations, observations are available at 5 time-steps. The observation time-instants are equally spaced in[4,7] Mean 
0.2 
0.5 
0.15 
0.15 
0.15 
0.25 
0.05 
2 </p>
<p>Table 7 :
7Reliability analysis results for cell signaling cascade problemMethods 
P f 
β 
N h 
N r 
= |β−βe| </p>
<p>βe </p>
<p>× 100 
MCS 
0.1663 0.9689 10000 10 4 × 10 3 
-
LF-PIDNN 
0.3649 0.3454 
0 
0 
64.35% 
HF-DNN 
0.0275 1.9189 
10 
10 × 5 
98.04% 
MF-PIDNN 
0.17 
0.9542 
10 
10 × 5 
1.52% </p>
<p>0.39 
0.4 
0.41 
0.42 
0.43 
0.44 </p>
<p>AcknowledgementsThe author would like to thank Soumya Chakraborty for proof-reading this article and Somdatta Goswami, Tanmoy Chatterjee and Rajdip Nayek for the useful discussions during the preparation of this paper. The TensorFlow codes were run on Google Colab service.
Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Joshua L Steven L Brunton, J Nathan Proctor, Kutz, Proceedings of the national academy of sciences. the national academy of sciences113Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the national academy of sciences, 113(15):3932- 3937, 2016.</p>
<p>Material homogenization technique for composites: A meshless formulation. Des Rodrigues, Belinha, Fma Pires, Lmjs Dinis, Jorge, Science and Technology of Materials. 301DES Rodrigues, J Belinha, FMA Pires, LMJS Dinis, and RM Natal Jorge. Material homogenization technique for composites: A meshless formulation. Science and Technology of Materials, 30(1):50-59, 2018.</p>
<p>Environmental effect of structural solutions and building materials to a building. Appu Haapio, Pertti Viitaniemi, Environmental impact assessment review. 288Appu Haapio and Pertti Viitaniemi. Environmental effect of structural solutions and building materials to a building. Environmental impact assessment review, 28(8):587-600, 2008.</p>
<p>A simple homogenization scheme for 3d finite element analysis of composite bolted joints. Bibekananda Mandal, Anupam Chakrabarti, Composite Structures. 120Bibekananda Mandal and Anupam Chakrabarti. A simple homogenization scheme for 3d finite element analysis of composite bolted joints. Composite Structures, 120:1-9, 2015.</p>
<p>Digital photoelasticity of glass: A comprehensive review. K Ramesh, Vivek Ramakrishnan, Optics and Lasers in Engineering. 87K Ramesh and Vivek Ramakrishnan. Digital photoelasticity of glass: A comprehensive review. Optics and Lasers in Engineering, 87:59-74, 2016.</p>
<p>Ultrafast machine vision with 2d material neural network image sensors. Lukas Mennel, Joanna Symonowicz, Stefan Wachter, Dmitry K Polyushkin, J Aday, Thomas Molina-Mendoza, Mueller, Nature. 5797797Lukas Mennel, Joanna Symonowicz, Stefan Wachter, Dmitry K Polyushkin, Aday J Molina-Mendoza, and Thomas Mueller. Ultrafast machine vision with 2d material neural network image sensors. Nature, 579(7797):62-66, 2020.</p>
<p>Survey of multifidelity methods in uncertainty propagation, inference, and optimization. Benjamin Peherstorfer, Karen Willcox, Max Gunzburger, Siam Review. 603Benjamin Peherstorfer, Karen Willcox, and Max Gunzburger. Survey of multifidelity methods in uncertainty propagation, inference, and optimization. Siam Review, 60(3):550-591, 2018.</p>
<p>Issues in deciding whether to use multifidelity surrogates. Chanyoung M Giselle Fernández-Godino, Park, H Nam, Raphael T Kim, Haftka, AIAA Journal. 575M Giselle Fernández-Godino, Chanyoung Park, Nam H Kim, and Raphael T Haftka. Issues in deciding whether to use multifidelity surrogates. AIAA Journal, 57(5):2039-2054, 2019.</p>
<p>Rajib Chowdhury, and Sondipon Adhikari. A surrogate based multifidelity approach for robust design optimization. Souvik Chakraborty, Tanmoy Chatterjee, Applied Mathematical Modelling. 47Souvik Chakraborty, Tanmoy Chatterjee, Rajib Chowdhury, and Sondipon Adhikari. A surrogate based multi- fidelity approach for robust design optimization. Applied Mathematical Modelling, 47:726-744, 2017.</p>
<p>Approximation of probability density functions by the multilevel monte carlo maximum entropy method. Claudio Bierig, Alexey Chernov, Journal of Computational Physics. 314Claudio Bierig and Alexey Chernov. Approximation of probability density functions by the multilevel monte carlo maximum entropy method. Journal of Computational Physics, 314:661-681, 2016.</p>
<p>Multilevel monte carlo path simulation. B Michael, Giles, Operations research. 563Michael B Giles. Multilevel monte carlo path simulation. Operations research, 56(3):607-617, 2008.</p>
<p>Adaptive multilevel monte carlo approximation of distribution functions. B Mike, Tigran Giles, Klaus Nagapetyan, Ritter, arXiv:1706.06869arXiv preprintMike B Giles, Tigran Nagapetyan, and Klaus Ritter. Adaptive multilevel monte carlo approximation of distribu- tion functions. arXiv preprint arXiv:1706.06869, 2017.</p>
<p>Multilevel monte carlo methods. Stefan Heinrich, International Conference on Large-Scale Scientific Computing. SpringerStefan Heinrich. Multilevel monte carlo methods. In International Conference on Large-Scale Scientific Com- puting, pages 58-67. Springer, 2001.</p>
<p>Recursive co-kriging model for design of computer experiments with multiple levels of fidelity. Le Loic, Josselin Gratiet, Garnier, International Journal for Uncertainty Quantification. 45Loic Le Gratiet and Josselin Garnier. Recursive co-kriging model for design of computer experiments with multiple levels of fidelity. International Journal for Uncertainty Quantification, 4(5), 2014.</p>
<p>Multi-fidelity modelling via recursive co-kriging and gaussian-markov random fields. Paris Perdikaris, Daniele Venturi, O Johannes, George Em Royset, Karniadakis, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences. 471Paris Perdikaris, Daniele Venturi, Johannes O Royset, and George Em Karniadakis. Multi-fidelity modelling via recursive co-kriging and gaussian-markov random fields. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 471(2179):20150018, 2015.</p>
<p>Variable-fidelity electromagnetic simulations and co-kriging for accurate modeling of antennas. Slawomir Koziel, Stanislav Ogurtsov, Ivo Couckuyt, Tom Dhaene, IEEE transactions on antennas and propagation. 613Slawomir Koziel, Stanislav Ogurtsov, Ivo Couckuyt, and Tom Dhaene. Variable-fidelity electromagnetic sim- ulations and co-kriging for accurate modeling of antennas. IEEE transactions on antennas and propagation, 61(3):1301-1308, 2012.</p>
<p>Multi-fidelity Gaussian process regression for computer experiments. Gratiet Loic Le, Universite Paris-DiderotPhD thesisLoic Le Gratiet. Multi-fidelity Gaussian process regression for computer experiments. PhD thesis, Universite Paris-Diderot, 2013.</p>
<p>Kriging-based approach for estimation of vehicular speed and passenger car units on an urban arterial. Subhadip Biswas, Souvik Chakraborty, Satish Chandra, Indrajit Ghosh, Journal of Transportation Engineering, Part A: Systems. 14334016013Subhadip Biswas, Souvik Chakraborty, Satish Chandra, and Indrajit Ghosh. Kriging-based approach for estima- tion of vehicular speed and passenger car units on an urban arterial. Journal of Transportation Engineering, Part A: Systems, 143(3):04016013, 2017.</p>
<p>A critical assessment of kriging model variants for high-fidelity uncertainty quantification in dynamics of composite shells. Tanmoy Mukhopadhyay, S Chakraborty, Dey, R Adhikari, Chowdhury, Archives of Computational Methods in Engineering. 243Tanmoy Mukhopadhyay, S Chakraborty, S Dey, S Adhikari, and R Chowdhury. A critical assessment of kriging model variants for high-fidelity uncertainty quantification in dynamics of composite shells. Archives of Compu- tational Methods in Engineering, 24(3):495-518, 2017.</p>
<p>Kriging based saturation flow models for traffic conditions in indian cities. Arpita Saha, Souvik Chakraborty, Satish Chandra, Indrajit Ghosh, Transportation Research Part A: Policy and Practice. 118Arpita Saha, Souvik Chakraborty, Satish Chandra, and Indrajit Ghosh. Kriging based saturation flow models for traffic conditions in indian cities. Transportation Research Part A: Policy and Practice, 118:38-51, 2018.</p>
<p>Application of kriging method to structural reliability problems. Irfan Kaymaz, Structural Safety. 27Irfan Kaymaz. Application of kriging method to structural reliability problems. Structural Safety, 27(2):133-151, 2005.</p>
<p>Multi-output local Gaussian process regression: Applications to uncertainty quantification. Ilias Bilionis, Nicholas Zabaras, Journal of Computational Physics. 23117Ilias Bilionis and Nicholas Zabaras. Multi-output local Gaussian process regression: Applications to uncertainty quantification. Journal of Computational Physics, 231(17):5718-5746, 2012.</p>
<p>Multi-output separable Gaussian process: Towards an efficient, fully Bayesian paradigm for uncertainty quantification. Ilias Bilionis, Nicholas Zabaras, A Bledar, Guang Konomi, Lin, Journal of Computational Physics. 241Ilias Bilionis, Nicholas Zabaras, Bledar A. Konomi, and Guang Lin. Multi-output separable Gaussian process: Towards an efficient, fully Bayesian paradigm for uncertainty quantification. Journal of Computational Physics, 241:212-239, 2013.</p>
<p>A Gaussian process latent force model for joint input-state estimation in linear structural systems. Rajdip Nayek, Souvik Chakraborty, Sriram Narasimhan, Mechanical Systems and Signal Processing. 128Rajdip Nayek, Souvik Chakraborty, and Sriram Narasimhan. A Gaussian process latent force model for joint input-state estimation in linear structural systems. Mechanical Systems and Signal Processing, 128:497-530, 2019.</p>
<p>Graph-Theoretic-Approach-Assisted Gaussian Process for Nonlinear Stochastic Dynamic Analysis under Generalized Loading. Souvik Chakraborty, Rajib Chowdhury, Journal of Engineering Mechanics. 145124019105Souvik Chakraborty and Rajib Chowdhury. Graph-Theoretic-Approach-Assisted Gaussian Process for Non- linear Stochastic Dynamic Analysis under Generalized Loading. Journal of Engineering Mechanics, 145(12):04019105, 2019.</p>
<p>The role of surrogate models in the development of digital twins of dynamic systems. Souvik Chakraborty, Sondipon Adhikari, Ranjan Ganguli, arXiv:2001.09292arXiv preprintSouvik Chakraborty, Sondipon Adhikari, and Ranjan Ganguli. The role of surrogate models in the development of digital twins of dynamic systems. arXiv preprint arXiv:2001.09292, 2020.</p>
<p>Multi-fidelity modelling of mixed convection based on experimental correlations and numerical simulations. H Babaee, C Perdikaris, G E Chryssostomidis, Karniadakis, Journal of Fluid Mechanics. 809H Babaee, P Perdikaris, C Chryssostomidis, and GE Karniadakis. Multi-fidelity modelling of mixed convection based on experimental correlations and numerical simulations. Journal of Fluid Mechanics, 809:895-917, 2016.</p>
<p>Multifidelity information fusion with machine learning: A case study of dopant formation energies in hafnia. Rohit Batra, Ghanshyam Pilania, P Blas, Rampi Uberuaga, Ramprasad, ACS applied materials &amp; interfaces. 1128Rohit Batra, Ghanshyam Pilania, Blas P Uberuaga, and Rampi Ramprasad. Multifidelity information fusion with machine learning: A case study of dopant formation energies in hafnia. ACS applied materials &amp; interfaces, 11(28):24906-24918, 2019.</p>
<p>Multifidelity information fusion algorithms for high-dimensional systems and massive data sets. Paris Perdikaris, Daniele Venturi, George Em Karniadakis, SIAM Journal on Scientific Computing. 384Paris Perdikaris, Daniele Venturi, and George Em Karniadakis. Multifidelity information fusion algorithms for high-dimensional systems and massive data sets. SIAM Journal on Scientific Computing, 38(4):B521-B538, 2016.</p>
<p>A multi-fidelity surrogate-model-assisted evolutionary algorithm for computationally expensive optimization problems. Bo Liu, Slawomir Koziel, Qingfu Zhang, Journal of computational science. 12Bo Liu, Slawomir Koziel, and Qingfu Zhang. A multi-fidelity surrogate-model-assisted evolutionary algorithm for computationally expensive optimization problems. Journal of computational science, 12:28-37, 2016.</p>
<p>Remarks on multi-fidelity surrogates. Structural and Multidisciplinary Optimization. Chanyoung Park, T Raphael, Nam H Haftka, Kim, 55Chanyoung Park, Raphael T Haftka, and Nam H Kim. Remarks on multi-fidelity surrogates. Structural and Multidisciplinary Optimization, 55(3):1029-1050, 2017.</p>
<p>Adaptive multi-fidelity polynomial chaos approach to bayesian inference in inverse problems. Liang Yan, Tao Zhou, Journal of Computational Physics. 381Liang Yan and Tao Zhou. Adaptive multi-fidelity polynomial chaos approach to bayesian inference in inverse problems. Journal of Computational Physics, 381:110-128, 2019.</p>
<p>Multi-fidelity non-intrusive polynomial chaos based on regression. Takeshi Pramudita Satria Palar, Geoffrey Thomas Tsuchiya, Parks, Computer Methods in Applied Mechanics and Engineering. 305Pramudita Satria Palar, Takeshi Tsuchiya, and Geoffrey Thomas Parks. Multi-fidelity non-intrusive polynomial chaos based on regression. Computer Methods in Applied Mechanics and Engineering, 305:579-606, 2016.</p>
<p>A bi-fidelity surrogate modeling approach for uncertainty propagation in three-dimensional hemodynamic simulations. Han Gao, Xueyu Zhu, Jian-Xun Wang, Computer Methods in Applied Mechanics and Engineering. 366113047Han Gao, Xueyu Zhu, and Jian-Xun Wang. A bi-fidelity surrogate modeling approach for uncertainty propagation in three-dimensional hemodynamic simulations. Computer Methods in Applied Mechanics and Engineering, 366:113047, 2020.</p>
<p>Engineering design via surrogate modelling: a practical guide. Alexander Forrester, Andras Sobester, Andy Keane, John Wiley &amp; SonsAlexander Forrester, Andras Sobester, and Andy Keane. Engineering design via surrogate modelling: a practical guide. John Wiley &amp; Sons, 2008.</p>
<p>Phaedon-Stelios Koutsourelakis, and Wolfgang A Wall. A generalized probabilistic learning approach for multi-fidelity uncertainty propagation in complex physical simulations. Jonas Nitzler, Jonas Biehler, Niklas Fehn, arXiv:2001.02892arXiv preprintJonas Nitzler, Jonas Biehler, Niklas Fehn, Phaedon-Stelios Koutsourelakis, and Wolfgang A Wall. A generalized probabilistic learning approach for multi-fidelity uncertainty propagation in complex physical simulations. arXiv preprint arXiv:2001.02892, 2020.</p>
<p>Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling. Paris Perdikaris, Maziar Raissi, Andreas Damianou, George Em Lawrence, Karniadakis, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences. 47320160751Paris Perdikaris, Maziar Raissi, Andreas Damianou, ND Lawrence, and George Em Karniadakis. Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2198):20160751, 2017.</p>
<p>Deep learning. Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT pressIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.</p>
<p>On transfer learning of neural networks using bi-fidelity data for uncertainty propagation. Subhayan De, Jolene Britton, Matthew Reynolds, Ryan Skinner, Kenneth Jansen, Alireza Doostan, arXiv:2002.04495arXiv preprintSubhayan De, Jolene Britton, Matthew Reynolds, Ryan Skinner, Kenneth Jansen, and Alireza Doostan. On transfer learning of neural networks using bi-fidelity data for uncertainty propagation. arXiv preprint arXiv:2002.04495, 2020.</p>
<p>A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse pde problems. Xuhui Meng, George Em Karniadakis, Journal of Computational Physics. 401109020Xuhui Meng and George Em Karniadakis. A composite neural network that learns from multi-fidelity data: Ap- plication to function approximation and inverse pde problems. Journal of Computational Physics, 401:109020, 2020.</p>
<p>Multi-fidelity physics-constrained neural network and its application in materials modeling. Dehao Liu, Yan Wang, Journal of Mechanical Design. 14112Dehao Liu and Yan Wang. Multi-fidelity physics-constrained neural network and its application in materials modeling. Journal of Mechanical Design, 141(12), 2019.</p>
<p>Large-eddy simulation: Past, present and the future. Yang Zhiyin, Chinese journal of Aeronautics. 281Yang Zhiyin. Large-eddy simulation: Past, present and the future. Chinese journal of Aeronautics, 28(1):11-24, 2015.</p>
<p>Machine learning models of errors in large eddy simulation predictions of surface pressure fluctuations. F Matthew, Julia Barone, Kenny Ling, Warren Chowdhary, Jeffrey Davis, Fike, 47th AIAA Fluid Dynamics Conference. 3979Matthew F Barone, Julia Ling, Kenny Chowdhary, Warren Davis, and Jeffrey Fike. Machine learning models of errors in large eddy simulation predictions of surface pressure fluctuations. In 47th AIAA Fluid Dynamics Conference, page 3979, 2017.</p>
<p>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Maziar Raissi, Paris Perdikaris, George E Karniadakis, Journal of Computational Physics. 378Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686-707, 2019.</p>
<p>Transfer learning enhanced physics informed neural network for phase-field modeling of fracture. Somdatta Goswami, Cosmin Anitescu, Souvik Chakraborty, Timon Rabczuk, Theoretical and Applied Fracture Mechanics. 106102447Somdatta Goswami, Cosmin Anitescu, Souvik Chakraborty, and Timon Rabczuk. Transfer learning enhanced physics informed neural network for phase-field modeling of fracture. Theoretical and Applied Fracture Me- chanics, 106:102447, 2020.</p>
<p>Simulation free reliability analysis: A physics-informed deep learning based approach. Souvik Chakraborty, arXiv:2005.01302arXiv preprintSouvik Chakraborty. Simulation free reliability analysis: A physics-informed deep learning based approach. arXiv preprint arXiv:2005.01302, 2020.</p>
<p>Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Yinhao Zhu, Nicholas Zabaras, Journal of Computational Physics. 394Phaedon-Stelios Koutsourelakis, and Paris PerdikarisYinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris. Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Journal of Computational Physics, 394:56-81, 2019.</p>
<p>Probability, reliability, and statistical methods in engineering design. Achintya Haldar, Sankaran Mahadevan, John WileyAchintya Haldar and Sankaran Mahadevan. Probability, reliability, and statistical methods in engineering design. John Wiley, 2000.</p>
<p>Reliability assessment using stochastic finite element analysis. Achintya Haldar, Sankaran Mahadevan, John Wiley &amp; SonsAchintya Haldar and Sankaran Mahadevan. Reliability assessment using stochastic finite element analysis. John Wiley &amp; Sons, 2000.</p>
<p>The Wiener-Askey polynomial chaos for stochastic differential equations. Dongbin Xiu, George Em Karniadakis, SIAM Journal on Scientific Computing. 242Dongbin Xiu and George Em Karniadakis. The Wiener-Askey polynomial chaos for stochastic differential equa- tions. SIAM Journal on Scientific Computing, 24(2):619-644, 2002.</p>
<p>Global sensitivity analysis using polynomial chaos expansions. Bruno Sudret, Reliability Engineering &amp; System Safety. 937Bruno Sudret. Global sensitivity analysis using polynomial chaos expansions. Reliability Engineering &amp; System Safety, 93(7):964-979, 2008.</p>
<p>Sequential experimental design based generalised ANOVA. Souvik Chakraborty, Rajib Chowdhury, Journal of Computational Physics. 317Souvik Chakraborty and Rajib Chowdhury. Sequential experimental design based generalised ANOVA. Journal of Computational Physics, 317:15-32, 2016.</p>
<p>Polynomial Correlated Function Expansion. Souvik Chakraborty, Rajib Chowdhury, Modeling and Simulation Techniques in Structural Engineering. IGI GlobalSouvik Chakraborty and Rajib Chowdhury. Polynomial Correlated Function Expansion. In Modeling and Simu- lation Techniques in Structural Engineering, pages 348-373. IGI Global, 2017.</p>
<p>Support vector regression based metamodel by sequential adaptive sampling for reliability analysis of structures. Atin Roy, Subrata Chakraborty, Reliability Engineering &amp; System Safety. 106948Atin Roy and Subrata Chakraborty. Support vector regression based metamodel by sequential adaptive sampling for reliability analysis of structures. Reliability Engineering &amp; System Safety, page 106948, 2020.</p>
<p>An efficient algorithm for building locally refined hp-adaptive hpcfe: Application to uncertainty quantification. Souvik Chakraborty, Rajib Chowdhury, Journal of Computational Physics. 351Souvik Chakraborty and Rajib Chowdhury. An efficient algorithm for building locally refined hp-adaptive h- pcfe: Application to uncertainty quantification. Journal of Computational Physics, 351:59-79, 2017.</p>
<p>Hybrid framework for the estimation of rare failure event probability. Souvik Chakraborty, Rajib Chowdhury, Journal of Engineering Mechanics. 14354017010Souvik Chakraborty and Rajib Chowdhury. Hybrid framework for the estimation of rare failure event probability. Journal of Engineering Mechanics, 143(5):04017010, 2017.</p>
<p>Bayesian deep convolutional encoder-decoder networks for surrogate modeling and uncertainty quantification. Yinhao Zhu, Nicholas Zabaras, Journal of Computational Physics. 366Yinhao Zhu and Nicholas Zabaras. Bayesian deep convolutional encoder-decoder networks for surrogate mod- eling and uncertainty quantification. Journal of Computational Physics, 366:415-447, 2018.</p>
<p>. Hagan Demuth Beale, B Howard, Demuth, Hagan, Neural network design. Pws. Hagan Demuth Beale, Howard B Demuth, and MT Hagan. Neural network design. Pws, Boston, 1996.</p>
<p>A Critical Appraisal of Design of Experiments for Uncertainty Quantification. Biswarup Bhattacharyya, Archives of Computational Methods in Engineering. 253Biswarup Bhattacharyya. A Critical Appraisal of Design of Experiments for Uncertainty Quantification. Archives of Computational Methods in Engineering, 25(3):727-751, 2018.</p>
<p>Automatic differentiation in machine learning: a survey. Atılım Günes Baydin, A Barak, Alexey Pearlmutter, Jeffrey Mark Andreyevich Radul, Siskind, The Journal of Machine Learning Research. 181Atılım Günes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Research, 18(1):5595-5637, 2017.</p>
<p>Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning. Tijmen Tieleman, Geoffrey Hinton, 4Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31, 2012.</p>
<p>Simulation and the Monte Carlo method. Ry, Rubinstein, WileyNew York, U.S.A.RY. Rubinstein. Simulation and the Monte Carlo method. Wiley, New York, U.S.A., 1981.</p>
<p>Tensorflow: A system for large-scale machine learning. Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, 12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16). Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16), pages 265-283, 2016.</p>
<p>MATLAB and Statistics Toolbox Release. The Mathworks Inc. The Mathworks Inc., Natick, Massachusetts, US. MATLAB and Statistics Toolbox Release 2019b, 2019.</p>
<p>Martin Alnaes, Jan Blechta, Johan Hake, August Johansson, Benjamin Kehlet, Anders Logg, Chris Richardson, Johannes Ring, Marie E Rognes, Garth N Wells, The fenics project version 1.5. Archive of Numerical Software. 3Martin Alnaes, Jan Blechta, Johan Hake, August Johansson, Benjamin Kehlet, Anders Logg, Chris Richardson, Johannes Ring, Marie E Rognes, and Garth N Wells. The fenics project version 1.5. Archive of Numerical Software, 3(100), 2015.</p>
<p>Evaluation of failure probability via surrogate models. Jing Li, Dongbin Xiu, Journal of Computational Physics. 22923Jing Li and Dongbin Xiu. Evaluation of failure probability via surrogate models. Journal of Computational Physics, 229(23):8966-8980, 2010.</p>
<p>Supersensitivity due to uncertain boundary conditions. Dongbin Xiu, George Em Karniadakis, International journal for numerical methods in engineering. 6112Dongbin Xiu and George Em Karniadakis. Supersensitivity due to uncertain boundary conditions. International journal for numerical methods in engineering, 61(12):2114-2138, 2004.</p>
<p>Nonlinear singular perturbation problems and the Engquist-Osher difference scheme. Jens Lorenz, Katholieke Universiteit Nijmegen. Mathematisch InstituutJens Lorenz. Nonlinear singular perturbation problems and the Engquist-Osher difference scheme. Katholieke Universiteit Nijmegen. Mathematisch Instituut, 1981.</p>
<p>A neural network approach for uncertainty quantification for time-dependent problems with random parameters. Zhen Tong Qin, John Chen, Dongbin Jakeman, Xiu, arXiv:1910.07096arXiv preprintTong Qin, Zhen Chen, John Jakeman, and Dongbin Xiu. A neural network approach for uncertainty quantification for time-dependent problems with random parameters. arXiv preprint arXiv:1910.07096, 2019.</p>
<p>Latin hypercube sampling (program user's guide). R L Iman, J M Davenport, D K Zeigler, Sandia laboratoriesTechnical reportR. L. Iman, J. M. Davenport, and D. K. Zeigler. Latin hypercube sampling (program user's guide). Technical report, Sandia laboratories, 1980.</p>
<p>Autocrine loops with positive feedback enable context-dependent cell signaling. Stanislav Yefimovic Shvartsman, P Michael, Hagan, Paul Yacoub, Dent, Douglas A Wiley, Lauffenburger, American Journal of Physiology-Cell Physiology. 2823Stanislav Yefimovic Shvartsman, Michael P Hagan, A Yacoub, Paul Dent, HS Wiley, and Douglas A Lauffen- burger. Autocrine loops with positive feedback enable context-dependent cell signaling. American Journal of Physiology-Cell Physiology, 282(3):C545-C559, 2002.</p>            </div>
        </div>

    </div>
</body>
</html>